{
    "id": "hb325m23mlqv746wrepekkxqrptiylmh",
    "title": "Bottom-Up Search and Transfer Learning in SRL",
    "info": {
        "author": [
            "Raymond J. Mooney, Department of Computer Science, University of Texas at Austin"
        ],
        "published": "Sept. 18, 2009",
        "recorded": "July 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Transfer Learning"
        ]
    },
    "url": "http://videolectures.net/ilpmlgsrl09_mooney_bustl/",
    "segmentation": [
        [
            "Hey Pedro, thanks for the nice introduction and thanks to the organizers.",
            "Also for the other organizers.",
            "Also for inviting me.",
            "I'm sorry I brought the hot weather with me here from from Texas.",
            "If you think it's hot here, spend the summer in Austin.",
            "I tried to get away but I hate it when I get away and it's hot wherever I go.",
            "So I'd like to talk today about some of our recent work in statistical relational learning, particularly focusing on two topics.",
            "Bottom Up, search and transfer learning, and this is joint.",
            "Most of what I talk about is joint work with.",
            "So my graduation slowly malkova until union.",
            "They're both here so that you can talk to both of them about the various aspects of their work that I'll talk about today.",
            "And also I'll talk about some of the work from the Washington Group, and I'd like to thank Jesse Davis and Pedro and Stanley Kok for giving me some slides that I'm also using in here."
        ],
        [
            "OK, so the real problem with all this sort of work that we're doing in relation learning whether statistical relational learning, inductive logic programming are learning with graphs is those models define a very large, very complex hypothesis space, and every machine learning person knows that that makes the problem hard, because it makes it both hard for time complexity 'cause searching that space for a good hypothesis is very hard when the space is so enormous and also from learning theory, we clearly know that the larger the hypothesis space, the more examples we're going to need.",
            "To learn so without driving the right biases that give give us to the right part of the space it's going to take us an intractable number of examples to learn with such very rich hypothesis spaces."
        ],
        [
            "So the particular problem I want to talk about is structure learning in these sort of relational models.",
            "So statistical relational models usually consist of two parts.",
            "They have a structure, like a set of logical formula or some graphical model or relational model that specified the structure of the model and then they have a set of parameters or weights which might be potentials in a Markov net or probabilities are conditional probabilities that parameterized them.",
            "So parameter learning, I think, is generally agreed to be an easier problem, and then the techniques for that are much more well developed.",
            "The harder problem is learning the structure.",
            "Of the model, which is a much more difficult and less well developed problem.",
            "In fact, in a lot of papers you'll see that there is no structure learning going on that a lot of people, even in the SRL community, specify the structure upfront manually by using human engineering to specify the structure of the model.",
            "And they just learn the parameters from data.",
            "So I'm going to focus on that.",
            "What I think is the more difficult problem of actually learning the."
        ],
        [
            "Structure.",
            "And I think there are these two techniques that bottom up search and transfer.",
            "Learning can really be beneficial in making structure learning better.",
            "So I think there are two effective techniques for ameliorating the time and sample complexity problems of learning these complex relational models that I talked about.",
            "So I'll talk about both of these.",
            "So bottom up search is idea of using directly using the data to drive the formation of promising hypothesis and transfer learning.",
            "Is using knowledge previously acquired in a related domain to drive the formulation of promising hypothesis in some new domain."
        ],
        [
            "So so just tickle relation learning.",
            "I'm sure a lot of you here already quite familiar with it.",
            "There's a so called Alphabet soup of various sorts of methods.",
            "This is just a small sample of them from stochastic logic program, probabilistic relational models, Bayesian logic programs, relational Markov networks, and Markov logic networks.",
            "My talk today will focus on our work on Markov logic networks, but I think a lot of the ideas that I want to talk about in terms of the value of bottom up search and transfer learning or not at all specific to MLN's.",
            "I think those lessons apply broadly across these other.",
            "Models and that sort of some of our future work is to maybe apply some of these ideas to other models, but our current work has been focused on on LNS for various pragmatic and other reasons.",
            "They are."
        ],
        [
            "Very nice model, so just to I'm sure a lot of you have even seen talks this morning talking about MLN.",
            "So I'm going to keep my summary of MLN's extremely short.",
            "You can listen to Pedro drag on about them for all for all afternoon if you want, but I'm not going to.",
            "I'm not going to play Pedro so very quickly.",
            "Markov Logic Network is is a logical KB, so we know logical standard logical KB.",
            "You have a set of hard constraints on the set of possible worlds.",
            "The easiest intuitive way to see an MLN is it's just a set of soft constraints on a possible world.",
            "So when apart when a world a set of facts violates a formula, it becomes less probable, but not impossible like in normal logic.",
            "So you give each formula weight and then the higher the way that formula, the stronger constraint it is, the more unlikely it makes worlds that violated an.",
            "In general the probability of some world is the equation at the bottom.",
            "Here is proportional to an exponential of the weights of the formulas that it that it satisfies."
        ],
        [
            "OK, so that's the basic idea of an MLN.",
            "So you can attach weight to rule.",
            "So here is always like to use the family domain.",
            "When I talk about logic, so we might have some very definition of things like says if X is wise.",
            "Parenting wise mailed in.",
            "Why is X is son?",
            "That's sort of a definition of rule will give it an extremely high weight.",
            "Well in other rules might be a little bit more heuristics.",
            "Isaiah vexes wise parent and ex is married to Z then Z is wise parent.",
            "Well that's less and less true these days.",
            "I'll give that a little bit less weight, maybe about a 10 and then maybe another rule.",
            "This is of ex lives with Y&X is mailing.",
            "Why is female then they're married?",
            "That's becoming even less likely, so will.",
            "Just say that has even a weight of 1 and so you can specify nice logical rules that summer which are very hard constraints that are definitionally and others which are sort of juristic rules.",
            "And of course you try to learn these weights automatically from data rather than having to."
        ],
        [
            "Defy them up front.",
            "And then in Mo and also is nice because it defines a very well defined probabilistic model.",
            "Basically in MLN is a template for constructing a ground Markov net where the ground literals correspond to nodes in the ground network and a ground clause corresponds to clique connecting the ground literals in the clause.",
            "So two nodes are connected if they occur in the same ground clause, and then the probability of a word isn't world is a normalized exponential model where Z is a normalizing constant and we sum up the weights of all the true groundings of.",
            "Each of the formula so I hear ranges over the set of formulas in my MLN, and I take the wait times the number of true groundings of that formula, exponentiated and normalize it to get a probability distribution over possible worlds.",
            "So it defines a very nice probability distribution over possible."
        ],
        [
            "The other nice thing about about Emma lenses.",
            "They have nice software support, which is one of the pragmatic reasons I think why we've been one of the pragmatic reasons we've been using it.",
            "There's this package, alchemy that Pedro in his his students and other people at the University of Washington maintain, and it's a nice package of software.",
            "It's open source that has inference algorithms, weight learning, structure learning algorithms, datasets.",
            "You know everything you need to get started.",
            "So this is my ad for alchemy and all of our software extends and uses the Alchemy Alchemy software."
        ],
        [
            "OK, so that's some background.",
            "So now let's talk about the first bigger idea that I wanted to focus on today, which."
        ],
        [
            "Bottom up search.",
            "So to start with bottom of search we have to contrast it with the North, but I would say the more normal approach which is top down search where we have some method for generating hypothesis and that generates a whole bunch of different hypothesis and then we go ahead and test them on the data and keep just we test it on the training data.",
            "We keep just a few of the ones that seem to fit the training data without without overfitting."
        ],
        [
            "So that's the typical approach by which algorithms work, and particularly in statistical relational learning.",
            "It's almost all the structure learning algorithms out there, basically, or some variant of top down search where you start with an empty theory, then you repeat and with until further refinements to that theory failed to improve the fit to the training data, you generate all possible refinements of a current theory where you have some grammar that defines what the possible refinements are, such as adding a literal to a clause in a logical model.",
            "You test each of those refined theories on the training data and pick the ones that best improve the fit, possibly with a complexity penalty to prevent overfitting.",
            "And then you keep repeating that.",
            "So the problem is you get this huge branching factor 'cause these models are very complex, which means the number of possible refinements to them is huge.",
            "So you get this incredibly large branching factor just and so the search becomes very intractable.",
            "So you're forced to use some sort of greedy or beam search to control the time complexity, and then that subjects you to local Maxima.",
            "Another standard problem with those sort of juristic search methods.",
            "So these sort of top down methods, I think, are fundamentally constrained by their basic framework."
        ],
        [
            "So the alternative of bottom up search says when I generate hypothesis, I'm going to immediately start looking at the training data.",
            "Even when I start generating hypothesis, I'm going to use the data itself to generate hopefully a smaller set of good hypothesis and then only test that smaller set of reasonable hypothesis on the data, and then hopefully quite a few of them will be will be good fits to the."
        ],
        [
            "To the training data.",
            "So the whole idea of bottom up search is to use the data to directly drive the formation of a much more limited set of more promising hypothesis.",
            "And sometimes people refer to this sort of approach also as a data driven approach or a specific to general sort of search method rather than a general to specific, which is what most top down models."
        ],
        [
            "So there's a long history of work in bottom up search in IO P. In fact from the very beginning days of inductive logic programming.",
            "This these 3 letter Acronyms because if this conference is very confusing.",
            "'cause you don't know really know when someone says IOP if they mean integer linear programming or if they mean.",
            "Inductive logic programming, so this means the ladder here.",
            "So you know back in the early days of Saigal, Steve's earliest work in one of his earliest works in IO, P Universe resolution very much a bottom up sort of technique that goes back over 20 years now and then.",
            "LGG least channel generalization goes back over what, 30 or almost 40 years now?",
            "So there's a long Tran.",
            "Then Golomb was was the bottom up IO P system built on top of the concept of plaque and concept of LG.",
            "So bottom up ideas go back a very long way in IO P."
        ],
        [
            "But pretty quickly I think.",
            "It sorry.",
            "Skip one.",
            "Our.",
            "Whoops, yeah, so one idea that I'd like to talk about.",
            "That's a bottom up technique that I developed with a former student of mine, Brad Richards, back in 1992, is relational pathfinding where the idea was to learn definite clauses based on finding paths of relations, connecting the arguments of a specific positive example.",
            "So it's driven by a particular positive example, which gives it this bottom up flavor, and again using the family domain as an example.",
            "Say we have a simple family like this, where the Red Arrows denote parent and the green lines represent the married relation.",
            "So I give you the positive example that Tom is Mary's uncle.",
            "I find the path between those two items Tom and Mary in the graph, and I missed initially immediately create a clause bottom up from that single training example based on that particular example that looks like this, I take each of the constants and I verbalize it and I get a pretty good pause for uncle and one from one example very very quickly.",
            "And if I want to make that absolutely correct, I have to do a little bit of maybe top down refinement of it to add a constraint that W has to be male and I get a perfectly good single clause definition."
        ],
        [
            "Banco not only that, so I get another example that Bob is Anne's uncle where Bob is a different type of uncle right by marriage, but so that sets up a slightly different path.",
            "That gives me a slightly different clause, which I verbalize.",
            "I add a constraint to top down and I get the other clause for Uncle, so from 2 examples I've learned both the disjunctive clauses for Uncle.",
            "So it's a very powerful heuristic, bottom up method in the domains where it's applicable in family."
        ],
        [
            "Game is 1 where it works great.",
            "But after these top down methods, there was a lot of work in IOP and methods that I'm calling hybrid here.",
            "That's a term that people refer to a lot of different things with.",
            "By here I mean combining top down and bottom up techniques.",
            "So back in ACNL back in 94 Johns Ellie Ann I proposed of sort of combination at back.",
            "In those days I called it down for the head foil for the body.",
            "But it's pretty good characterization of the of the technique called chillin and then profile came along which I view very much as a hybrid technique.",
            "It has some very strong as you start up with the seed example which is very much a bottom up driven process, but then you do this sort of top down simplification of the bottom clause so that sort of top down so it's very much a hybrid and then Alex System of course is built.",
            "Built mainly off of pro."
        ],
        [
            "All ideas, so there's this rich tradition of working IO P instead of bottom up in hybrid method.",
            "But within SRO there's really not that much.",
            "Mostly algorithms seem to be influenced by graphical model learning techniques which are highly dominated by top down techniques which I sort of attribute.",
            "There's this nice Heckerman tutorial on learning graphical models, but it's totally prevents it from the presents it from the top down idea and everybody takes these top down approaches to learning graphical models.",
            "Everybody is of course maybe a little exaggerated, but not much.",
            "And I think another problem with this is I could I can be critical of American researchers 'cause I am one that many American researchers in statistical relational learning are really not sufficiently familiar with the rich history of bottom up techniques in within inductive logic programming, and I think they could be more so and they would get more ideas for developing more interesting structure learning algorithms if they were more familiar with that work myself.",
            "I've been working in IOP almost from the beginning.",
            "I didn't go to the first LP conference, but I went to the second one in Tokyo in 1992, so I hopefully am familiar with these.",
            "Ideas, and we've been very productive at exploiting these IOB ideals to build better SRL models, and I don't think enough people have been doing that."
        ],
        [
            "Is why I'm giving this talk.",
            "OK, so one of the things I'm going to talk about a few albums that use bottom up ideas and learning lens.",
            "So the first one I want to talk about is a system that my student Lilly presented at ICM Ella couple years ago called Muscle which is just a simple acronym for bottom up structure learner.",
            "It's actually not strictly bottom up.",
            "Like a lot of code, I'm calling bottom up techniques there.",
            "They have a certain amount of top down components, so strictly speaking I would actually call bustle hybrid technique, but it exploits it sort of partial proposition proposition alization of the data by using a sort of variant of relational pathfinding.",
            "And it also uses an existing Markov net structure learner to build a Markov net template that constrains the clause construction.",
            "So this sort of uses the data to build this template and then uses the the template to constrain its top down search for clauses.",
            "So it's sort of very much a hybrid, but it's using a lot of bottom up sort of information."
        ],
        [
            "So I don't have to go into the details, is just a really quick overview of how Bustle works.",
            "So for each predicate in the domain.",
            "So this assumes a sort of non discriminative case where you want to learn a set of clauses to predict all the possible predicates in your domain.",
            "So we're going to do this for every predicate we construct a set of template nodes where you're going to represent the nodes in the final template Markov network and then use them to partially proposition lies the data.",
            "And we're going to construct a Markov network template from that propositional data, sort of using a sort of.",
            "Upgrading sort of approach and then form candidate clauses based on those templates, which then constrain the clauses.",
            "So I'm proposing many fewer candidates than I do in a normal top down approach.",
            "And then you evaluate all the candidate causes on the training data and keep them."
        ],
        [
            "1.",
            "So these template node you construct these nodes based on conjunctions of one or more verbalised literals that serve as sort of building blocks for the clauses that you're eventually going to construct, and you construct them by looking for groups of true constant sharing ground literals in the data, and then verbalizing them, which is a sort of short pass, so it takes a set of short paths that exist in the actual ground data, very much.",
            "A bottom up process, and generates these so called template nodes from that, so it could hurt can be viewed as a partial relate using partial relational pasta, sort of."
        ],
        [
            "Partially proposition lies the data, so here's just a very simple example in the movie domain.",
            "Use these layers.",
            "Somebody just briefly explain so this is using a little bit of IMDb data which will talk about a little bit later, but we have predicates like someone's at actor, someones a director and then this works far means that some actor worked for some particular director.",
            "Here Brando work for Coppola and Godfather say and Pacino did.",
            "And then there's this movie predicate which means some particular movie contains some individual in its credit list and so.",
            "For if we have a current predicate actor, refined paths of relations that connect that actor to various other predicates, we might have chains of 1 here where these chain is also connected by this variable F in this one by D and then it creates instances for every value of a it creates denero in the one of these in this table, and it's sort of partially proposition lies the data based on these sort of pieces of."
        ],
        [
            "Ads.",
            "Then once we have that, we have this table of propositional data, we give that to a standard ground traditional Markov net.",
            "Learning our and we use this algorithm developed by Bloomberg Broom Bromberg a few years ago.",
            "So it takes that propositional data and generate a network where the nodes are these so-called template nodes, and then this is what we call them."
        ],
        [
            "Template Markov net.",
            "Now that we used to constrain the set of clauses that will consider instead of just compare looking at all possible clauses we look at cliques in that template network and only construct clauses and evaluate them that are consistent with that, because again, clauses correspond to clicks in the graph and we use the template network to drive the construction of a much smaller set of alternative possible clauses and then evaluate those."
        ],
        [
            "So again, that's just a real quick overview, and so we we we ran a fair number of experiments to try to show the advantage of this technique compared to standard top down structure learning."
        ],
        [
            "All three of the datasets are used throughout this talk.",
            "If you know anything about MLN's, I'm sure you've heard of this nice data set about the computer science Department at the University of Washington that Matt and Pedro put together a number of years ago, and it includes professors like Professor I mean predicates like Professor Student, advised by taught by publications, things of that sort.",
            "We created a small IMDb data.",
            "When I talk about transfer learning, the reason why we kept this data small originally is we wanted to view it as a target of transfer.",
            "Learning a domain where you have very little data but you wanted to transfer knowledge from another domain.",
            "So this is a relatively small datasets only extremely small fraction of all of IMDb, but it contains predicates like actor, director, movie, worked, far genre, etc.",
            "And then we also have a version of the web KB.",
            "It gets a little confusing 'cause there's so many variants of this.",
            "What I talk about in this talk, at least for most of 'em, doesn't use the text of the pages, it just uses the.",
            "Entities and the relations between them, and it includes predicates like faculty and student in project, and that way it's pretty similar in some of its predicates.",
            "It's sort of a subset of the predicates that occur in the UW."
        ],
        [
            "Jesse domain.",
            "So these datasets are organized by what I call we call Mega Examples, where you're doing relation learning.",
            "It's pretty tricky because if you're going to do cross validation you need independent training and test data, but the whole point is your data is rich and relationally connected, so it usually what we do is we assume the data consists of these large sets of connected objects, so it's a standard examples in the web domain they have data from 4 different universities and people each.",
            "Each University defined its own connected graph and you can train on some universities, test on others 'cause those are independent.",
            "So we call those large sets of.",
            "Relationally connected fats and mega example and here's just some statistics, just to give you some idea.",
            "I mean, once you get down to the number of ground literals, sorry, this literal stands for ground literal.",
            "It turns into a fairly large number of property propositions."
        ],
        [
            "Mom.",
            "OK, looked.",
            "Weather going backwards.",
            "Sorry.",
            "Ah OK.",
            "So then the methodology that we run our experiments.",
            "We generate learning curves using leave one Mega example out, which in the web domain people we've talked about leave one University out, which isn't leave one mega examples out.",
            "Just the generalization that you run keeping one mega example for testing train on the remaining ones in test on the on the one an average of results over that, giving it 1 Meg.",
            "Example out of time so you can plot a learning curve and then we evaluate the MLN by performing inference on the literals of each predicate, providing the rest is evident.",
            "So you test each predicate holding it out.",
            "As a test predicate training on I mean giving that data from all the other predicates and trying to predict that missing one, and we compared our technique to what at the time when we did this work was the current best structure learner out of the University of Washington, which I'm here calling T DSL for top down structure learning to counterpose it to bustle the bottom up one.",
            "I don't know why this thing started going back."
        ],
        [
            "We use the two standard metrics that the Washington Group is designed for testing MLN's.",
            "The conditional log likelihood of the of the test data averaged over all the test literals and the area under the precision recall curve, which I think is usually a better measure 'cause there's so many false literals that you can get pretty high CLL by just predicting everything is false.",
            "But AUC really PR really requires you to predict the correct positive facts.",
            "Um?"
        ],
        [
            "So here's a result of a learning curve in the IMDb domain, where blue is the top down algorithm, red is the bottom up.",
            "We do quite a bit better."
        ],
        [
            "Here are the results in the in the UW CSE domain.",
            "I've also included here.",
            "the Washington Group has developed a hand built knowledge base for this domain and that's shown in purple here, but red eventually even beats out the hand built knowledge base by using bottom up learning.",
            "But the top line structure learner does it."
        ],
        [
            "And then this is the results in the web KB after one Meg example were tide with the top down learner, but then it tends to start overfitting.",
            "It looks like or remains level and we keep keep learning.",
            "So that's sort of 1 demonstration at the bottom up technique can be a much better learner."
        ],
        [
            "Then a top down technique and it also is not only better on accuracy, it's much better on time 'cause we're reducing the number of hypothesis we need to explore quite dramatically so we can see we get dramatic reductions in training time as well.",
            "These are minutes and and the bustle is of course to lower the lower bar."
        ],
        [
            "OK, so that's a quick view of this this out bustle algorithm.",
            "I want to briefly then talk about another bottom up structure learner from lens that we worked on that was presented at ICM out last year.",
            "But my student to you and it said there were actually moving to the discriminative setting where we assume we have a target predicate that we want to predict and we're going to learn a theory pacifically for predicting that predicate from all the other background predicates and we found that existing nondiscrimination on structure learners did very, very poorly on a set of IOP benchmarks.",
            "1 problems getting into.",
            "One of our goals going into this project was to say how do I my lens compared to the best current state of the art IOP learners on some of these standard IOP benchmarks like in molecular biology.",
            "So we ran the existing structure learners and they did quite poorly.",
            "But then we built a hybrid discriminative IOP algorithm based on all if the standard IO P system to learn candidate clauses and it did much much."
        ],
        [
            "So the general approach is is we start out with a general IO P system that generates lots of possible clauses and then we wait those with the discriminative wait learner and we buy us some of that.",
            "We try to put a very high bias towards zero wait so it does very strong regularization so that a lot of the clauses are just eliminate it because they end up with zero weights during the discriminative weight training process."
        ],
        [
            "Again, you know the paper last year talks about the details of this.",
            "I'm just going to talk very briefly about it, so the structure learning part.",
            "We used a variant of all.",
            "If we called out of plus plus 'cause it just opens up Aleph and allows it to generate a lot more clauses than it normally would by loosening the restrictions on what we think is a good clause, that's good enough.",
            "So it generates again.",
            "It's the idea is it's going to be used to generate a ton of possible clauses, but those claws are being generated bottom up using the standard.",
            "All of things that rely on on the bottom claws and things of that sort."
        ],
        [
            "And then we use descriptive weight learning.",
            "To learn the weight on those maximizing the conditional log likelihood of the of the target predicate on the training data, and we another thing we did to make this thing work better was for the case of non recursive clauses.",
            "When you're doing discriminative learning with non recursive clauses you can do exact inference in MLN and we do that very efficiently.",
            "And it's much better than using the standard approximate inference algorithm.",
            "And we also wanted to highly bias it towards learning 0 weight clauses to eliminate a lot of the less useful clauses that generated during this very expensive generation of bottom up.",
            "Possibilities using olive, so we used L1 regularization, which if you don't know it just a different way of regularising that bias is more Tord 0 weight clause."
        ],
        [
            "So one of the datasets we tested it with.",
            "This is is the one of the standard biochemistry domains in used in IOP, which is this problem of comparing Alzheimer's drugs for various biochemical properties.",
            "So there are four different datasets in this general data set about Alzheimer's drug."
        ],
        [
            "And here are the the accuracy results using different techniques.",
            "So even our algorithm bustle because and doesn't work that well in this domain.",
            "So the first 2 rows here are the this previous structure learners actually just generic IOP algorithms.",
            "The yellow here is alif.",
            "Generic LPR is doing much better on this domain, but using our technique that combines ideas from my opian from discriminative weight learning from lens, we get even better results here in red."
        ],
        [
            "And then we also the nice thing about my lenses are so general that one constraint that really wasn't being applied in the IO P results for this problem is is that you're trying to predict this predicate less toxic.",
            "That says that one drug is less toxic than another drug, but it didn't even use the knowledge that that that predicate should be transitive.",
            "That if a is less toxic than B&B is less toxic than see.",
            "That obviously is less toxic than see within MLN.",
            "Just put that clause in, put an infinite wait on it so it's a hard constraint, throw it into the MLN and it adds that knowledge to making all of its prediction.",
            "So this makes it collected the normal way this data set was evaluated.",
            "It wasn't doing collective inference.",
            "We throw this background predicate in clause in.",
            "It does do collective inference and we get a little bit better.",
            "So by adding collective inference by adding this one clause, we go from the green to the blue and we improve our results quite a bit.",
            "So another nice property of melons, you have additional knowledge destroyed into the MLN, and it will hopefully improve your result."
        ],
        [
            "OK, the last bottom up technique I wanted to talk briefly about is a technique that was published this year by it by Stanley Kok in an Pedro that was just presented at ICM Ella couple of weeks ago, and it's a new sort of I would call it bottom up again.",
            "It might be more accurately characterized as a hybrid technique, and it's really even more faithful to the idea of relational pathfinding than our bus.",
            "Allow rhythm was, and it's pretty clearly the best current structure learner overall for Emma Lens, and it's actually a poster here this evening so you can talk to Stanley more about it this evening at his poster.",
            "He'll pay me $20 later for the ad."
        ],
        [
            "Kaiser.",
            "So.",
            "And the fundamental idea behind behind this technique he calls lifted hypergraph learning is as you do clustering to cluster the constants into groups and then produce a lifted graph based on that.",
            "So he has this nice.",
            "Figure in his talk where he has a set of people here and they have these relationships to classes like person can teach a class or a student can be ATA for a class or some professors can advise other students and the first clusters all the constants in the domain to get this clustered graph over here where all the teachers are here.",
            "All the students end up in this cluster and all the classes end up in that cluster and you get this set of relations that usually teachers advise graduate students graduate students.",
            "I'm sorry, got this backwards graduate students.",
            "A classic, I mean faculty teach classes, graduate students, TA classes and grain faculty advised graduate students, so he first does this lifting, and then he does relational pathfinding on this lifted."
        ],
        [
            "Graph so it has three basic steps.",
            "The first algorithm for lifting the hypergraph biclustering the constants.",
            "Then it finds a set of paths in that listed graph.",
            "And just like I showed you for relational pathfinding, it learns clauses from those paths and then test them on the data and adds the good ones to the final MLN.",
            "So it's quite.",
            "It's actually a fairly elegant and simple."
        ],
        [
            "Overall algorithm, and it gets pretty good results with it.",
            "I just in addition to some of the datasets already talked about.",
            "This is a data set that also was talked about in one of the morning sessions.",
            "Is Cora data set where you try to to solve the duping problem in computer in computer science, literature, citations and it builds a pretty."
        ],
        [
            "Pretty large model, so here are some graphs on on his technique comparing it to the previous top down structure learner that he built an plus.",
            "Our bottom up structure learner and these are an area under the PR curve for different domains, IMDb and UW and Cora and shows that his algorithm does better than the current state of the art, particularly on this."
        ],
        [
            "For a domain, and that's the conditional log likelihood there he wins.",
            "He beats us on bustle just a little bit on on those two domains, but again does very well on coral."
        ],
        [
            "R. Another nice thing that he does in his paper as he doesn't ablation test that shows what if I use the lifted graph, but I just generate clauses without using the relational pathfinding clue as a bottom up clue.",
            "How much does that relational pathfinding heuristic help me and shows that it helps quite a quite a bit, but if you take out the relational path finding that the performance goes down quite dramatically on all of the at least these two."
        ],
        [
            "OK, so that's my quick summary of some of the recent work in bottom up structure learning, and why I think it's a good approach to learn structures, particularly for for SRL models like micro Markov logic networks, and I think, But I think it's not as exploited as it as it should be.",
            "OK, so now I'm going to transit to the second topic.",
            "I wanted to talk about."
        ],
        [
            "Just transfer learning.",
            "So if you haven't heard about transfer learning, the idea is that most machine learning methods approach each new problem.",
            "You know with a blank slate from scratch, and they don't utilize what they've learned in previous domains to help learn in a new domain.",
            "And the whole idea of transfer learning is to use knowledge acquired in some previous so called source task to facilitate and improve learning in a related new target task.",
            "And of course for this to work, there has to be some sort of similarity.",
            "But as we'll see, that similarity can be extremely abstract and you can transfer between."
        ],
        [
            "Different things.",
            "So normally you assume there's a significant amount of training data that's available in the source, but you have a new domain, the target domain where you have very limited data, so you really need to exploit some other source of bias to be able to learn from such little amount of data with particular learning.",
            "Such a rich structure like you would in a relational model.",
            "So by exploiting knowledge from the source ideas to learning in the target can be more accurate by using the bias from the source domain knowledge you learn will be more accurate, and also because you already have a pretty good hypothesis from the target from the source domain.",
            "That training should time should all go, so to be dramatically reduced."
        ],
        [
            "So again, the general goal on transfer learning is you plot the learning curve and if we just learn from scratch where we don't do any transfer, we get a learning curve from that.",
            "But if we start out with knowledge we learned in the source domain, hopefully we get a much boosted learning curve by transferring knowledge from the source domain.",
            "So sometimes people call this initial difference that you get given no training data by just mapping the knowledge directly to the target problem they signs.",
            "That's called Jumpstart, but also you're interested in, particularly when there's a limited amount of training data in the target domain that you can get quite dramatic improvements in accuracy by exploiting knowledge from the source."
        ],
        [
            "So there's been quite a bit of recent work in transfer learning.",
            "Part of that is due to the fact that the US DARPA agency ran a program in transfer learning, which both Pedro and I were actually involved in.",
            "It just ended recently.",
            "In fact, I think my contract expired yesterday or the day before.",
            "So, but most of that work is focused on feature vector classification.",
            "I'm not going to go through.",
            "There's a pretty large literature now on using transfer learning.",
            "There's also a growing body of literature using transfer learning or reinforcement for reinforcement learning, but."
        ],
        [
            "There's not a lot on using transfer learning for the type of problems that you and I in this this room are interested in these relational sorts.",
            "Sorts of problems."
        ],
        [
            "One of the other interesting things when you combine the transfer learning with statistical relational learning, one of the things that I found it, I think it's sort of interesting is in standard machine learning.",
            "You make the so the famous IID assumption right that examples are independent and identically distributed.",
            "Well, when you do so just to relational learning, as we all know, the independence assumption is broken because your examples that you're trying to classify or not independent, you have to do this task of what I think Daphne originally called Collective Classification, which is one of the important tasks at this Community, has looked at in addition.",
            "When you do transfer learning, you're breaking the other part of the IID assumption, because the identicality assumption goes away because the test examples are not distributed the same way, the training examples might not even be represented in the same language.",
            "So when you combine transfer learning in SRL, I think it's sort of interesting 'cause you're now breaking both of the eyes in the IID."
        ],
        [
            "OK, so we've done a certain amount of work and over the last few years on transferring for Markov logic networks.",
            "Originally a lot of this is about present was presented in a paper triple AI by Lilly to get myself back in a couple of years ago and the idea is you start out with one domain where you have a fair bit of data.",
            "The UW CSE domain where we have all sorts of knowledge about professors and students in publications and what by who advised to.",
            "And we just start out with a little bit of data in the movie domain.",
            "And we want to transfer a Markov logic network learned here in the source to this target domain.",
            "But the problem is the predicates are completely different.",
            "The representation is completely different, so the first thing we have to do is establish some sort of mapping here and we have to realize that professors are like directors and students are like actors in publications are like movies and like advising.",
            "The student is sort of like a director advising an actor in a film, and there's actually a very nice analogy between these two domains, even though they're quite different and you can actually transfer a lot of the knowledge that you've learned in UW CSE, two IMDb.",
            "As we'll see.",
            "And that requires 2 steps.",
            "First you have to map the source predicates to the target.",
            "Realize what the correspondences are here, and then take that map knowledge and revise it."
        ],
        [
            "So we developed a system called Tamar for transfer via automatic mapping and revision.",
            "Not one of my better acronyms, but it's OK.",
            "He's like ones that make a real word.",
            "So let's say we start with a clause in the UW CSE domain where this is a very common rule in that domain that has a very high predictive accuracy, which says that if some student has a paper and their advised by some advisor, then usually the advisor puts their name on the paper, to which we've all done.",
            "It works great, multiplies your Vita quite dramatically.",
            "So, so it's pretty.",
            "It's a pretty good predictive rule that advisors put their name on their students papers, no matter if they contributed much the ideas or not.",
            "And now we get this new domain in the movie domain and we're going to transfer that now is that we've learned from the academic domain to the movie domain.",
            "So the first thing we have to do is map the predicates, and that's in a box we call EM Tomar mapping part of transfer, and we have to establish a relationship between the predicates and realize the publication is like a movie and advising the student is like.",
            "Is like a director.",
            "Advising an actor or the work for predicate in that domain.",
            "Now, once we've established that predicate mapping, we can map the predicate to get a clause in the new domain, which says if you're an actor in a movie and you've worked for some director in the past, and maybe he's the director of this movie, 'cause directors and actors do tend to establish a relationship just like students and faculty do.",
            "But it's probably not as tight, right?",
            "Because I actually I do that my students have gone off and done projects and other students for other professors, and they write papers with them, and I'm a little bit jealous, right?",
            "You know, it's like you know they should be on that paper.",
            "Is there my student?",
            "You know, in the movie domain this rule doesn't maybe work quite as well because the bond between directors and actors is not as strong as that strong bond between an advisor and his student.",
            "So maybe we have to revise this clause a little bit to work better in the movie domain.",
            "So my favorite example of doing this is, say, well, if that if the director is a relative of the actor, then it's probably more likely that they'll be the director of the film.",
            "I famous example.",
            "My favorite example of this is Clint Howard.",
            "Does anyone know who Clint Howard it so it's Ron Howard's brother?",
            "He wasn't?",
            "Child actor too, just like Ron was he starting this old show called called Gentle Ben.",
            "But since it since he did TV work as a kid, he really hasn't gotten much acting jobs except when except when his brother is directing the movie.",
            "He hires Clint for some bit part he was in Apollo 13 and all these other Ron Howard movies.",
            "So this might be a reasonable cause if we add this extra condition that the director has to be related to the act.",
            "So that's done by this other step called the revision step of trance."
        ],
        [
            "OK, so first I'm going to talk about the predicate mapping very briefly.",
            "So how do we establish a predicate mapping where we're going to map each clause independently, so each clause we've learned in the source domain we're going to take and try to find a mapping for that clause in isolation, and it does a pretty brute force technique as we found out that the space of possible consistent possible mappings is really not that large as long as my set of predicates in their arity is not too large, so each predicate is mapped to some target predicate, and we try all possible so called type consistent ways where we can map we're assuming.",
            "Every MLN has types on its arguments, and you have to respect those typing constraints.",
            "And once you do that, it actually constrains the set of mappings pretty much.",
            "And then we just evaluate each of the possible map clauses to see how well it fits the target data in the training data for the target domain, and we pick that mapping that results in the best accuracy.",
            "So we just empirically evaluate you should the possible mappings and pick the best one."
        ],
        [
            "So again, I had this example.",
            "I might come up with this mapping.",
            "That might be one of the many mappings I might try is well, publication is like movie and advising a person is like working for them and that establishes a consistent type mapping because all the titles map to names, titles of publication, map to names of movies, and actually a person Maps to a person.",
            "That's an easy type mapping, so that's one type consistent mapping.",
            "You try all these sort of tight, consistent mapping to evaluate them on the training data and pick the best one and it's sort of a brute force algorithm that we found in practice.",
            "It actually worked."
        ],
        [
            "Well, so we then pick the best.",
            "We transfer the clause based."
        ],
        [
            "On that mapping.",
            "OK, so that's a quick review of predicate mapping.",
            "Then we get to the revision part.",
            "So once we've mapped the clauses, they don't work exactly.",
            "Know the two domains are related, but they're not exactly isomorphic, so we're going to have to change some of the clauses to get them to be more accurate.",
            "So we view.",
            "This is a theory revision problem, so one of the reasons, actually the reason why I even got into this whole field of inductive logic programming I was interested in theory refinement.",
            "I did a lot of work in propositional data, and I got into interested in in doing it in first order logic about the same time that the IOP community started up, and I.",
            "One of my first IOP systems I worked on with my student, Brad Richards was this system called Forte.",
            "That's a nice acronym.",
            "First daughter.",
            "Revision of theories from examples.",
            "That's a better acronym.",
            "And we basically view the transfer problem very much as an instance of this revision.",
            "Sorts of problem where we have an initial theory and we need to now revise it to fit this new set of data.",
            "And of course being the bottom up type of guy I am.",
            "I do that revision not based on a simple top down approach right?",
            "Try all possible revisions to the source domain 'cause that leads to a huge branching factor, local minimum, all those bad problems we do more of a bottom up.",
            "Approach where you use the data to propose potentially good revisions to the theory and then just test a very small number of revisions.",
            "So again it uses this idea of using more bots."
        ],
        [
            "OP directed search.",
            "So it looks something like this.",
            "You have a set of clauses from the source domain and you have some.",
            "You look at the data for the target domain you run, the inferences there and see how well they work and you see where clauses make good predictions and where the existing clauses make bad predictions.",
            "And based on that you evaluate each of the clauses and you decide whether that clause is too long, too short, or whether it looks like it's pretty good.",
            "Which really means does it need to have extra literals added to this disjunction literals removed from the disjunction, and you can bake those decisions pretty clearly by seeing how well the initial clauses perform on the data.",
            "And then once you have that, you do a directed beam search through the possible ways of long, long lengthening the ones that need lengthening and shortening the ones that need shortened.",
            "And then you create various versions of those.",
            "Again, test them on the data course, and keep some of the best revisions.",
            "And then you also need the technique to bring in new rules, right?",
            "'cause maybe there's some rules that don't don't have any analog in the source domain, so you need some technique for just cuts covering new clauses.",
            "You put those in, then you do some weight learning on those.",
            "And then you pick out the ones that do test them on the training data and you see how well they fit the training data and you keep the clauses that fit the training data the most and put those into the final MLN.",
            "So I don't have time to go into all the details of how self diagnosis works.",
            "You can look at the paper."
        ],
        [
            "For those details, WIP.",
            "So the nice thing about this bottom up sort of approach to revision is it's very directed.",
            "It's not just trying all possible changes.",
            "Literal deletions are only attempted on clauses marked for shortening in literal additions are only attempted on clauses marked for lengthening based on the diagnosis of how those clauses performed in the new domain, and training data is much faster since the search is constrained by limiting the clauses that are considered for any possible update and then restricting the type of updates that are allowed.",
            "So as we'll see in the empirical results, it also runs a lot faster than a top down approach."
        ],
        [
            "Our new claws discovery.",
            "We basically just use a variant of the relational pathfinding algorithm from 92 that I mentioned before."
        ],
        [
            "And then of course, at the end, once we've gone through, we've mapped the predicates.",
            "We revised the clause by adding constraints to rules, removing them, adding new rules.",
            "We go ahead and then do a final step of weight learning on those to learn a final set of weight weighted clauses for MLN."
        ],
        [
            "OK, so some experiment."
        ],
        [
            "It's in this domain.",
            "We compare the number of systems the full system we took and you can take this top down algorithm of cocking Domingo's from 2005 and either learn to use it to learn a theory completely from scratch or you can use it as a top down theory refinement algorithm by just using top down search to refine that theory to fit the training."
        ],
        [
            "Another thing we tested was revising a hand built knowledge base, so so again the UW people put together this nice hand built knowledge base for this UW CSE domain.",
            "And when you use that as a source domain, you can transfer it.",
            "Sort of a form of old style of theory, refinement.",
            "Except the predicates are different, so it's sort of a form of mapped.",
            "I have to map the predicates, the new domain and then do theory refinement."
        ],
        [
            "Are to summarize all the results in our experiments that we tried.",
            "We predict we compute this thing called transfer ratio, which this DARPA program came up with, which is the area under the learning curve for the transfer.",
            "Learning over the area under the learning curve for the order from scratch.",
            "The whole idea is transfer learning boot.",
            "You know, this me up lifts up the learning curve and so the area, the ratio of the areas under those two curves is a good measure of how much we've improved things."
        ],
        [
            "OK, we tried different types of transfer from different of these three domains that I talked of each other to one of the others.",
            "We never used web KB as a target domain since the mega examples there are pretty large and you can pretty much learn a good theory for that more simplified domain just by learning from the training data.",
            "But we tried various other ways of mapping from 1."
        ],
        [
            "Main to another, and these are all the different domains, so here I'm first showing you the transfer ratio.",
            "If I measure it in terms of area under the precision recall curve and so above 1 here means I have positive transfer.",
            "In other words and look at learning knowledge from the source.",
            "Helps me learn in the target domain and then this compares a top down theory refinement algorithm to a bottom up one that Amar algorithm.",
            "So on AUC on this.",
            "To these problems we don't do too much different.",
            "Most all the domains get positive transfer, except this last one of IMDb to UW CSE.",
            "That's mainly because we have so little data in IMDb that it doesn't learn a very good theory that it can transfer.",
            "It works better the other way, as you can see."
        ],
        [
            "Over here.",
            "And if you look at the conditional log likelihood the other way of measuring accuracy we do consistently better than the top down algorithm in terms of accuracy.",
            "But the real win in this domain is in time, as we'll see in a."
        ],
        [
            "But this is just a sample learning curve where we show the number of mega examples in the target domain and then these.",
            "Is horizontal lines or what?",
            "If we come up with a mapping by hand so we don't do the algorithm for predicting the best mapping.",
            "We actually say OK. Professors are like directors and actors are like students and actually notice that.",
            "So the bottom the bottom most curve here, the red curve is learning from scratch using the top down algorithm.",
            "And if we use transfer with the top down algorithm with hand mapping we get a little bit better if we do transfer with the top down algorithm using an automated mapping, we do even better this.",
            "Aqua Blue line and then if we use the full Tamar system, of course we get the best best results.",
            "Another interesting point here is the automated mapping does much better than the manual mapping.",
            "You might think that you can you as a human can establish a good analogy and map the predicates properly, but the algorithm actually comes up with better mappings than the human expert."
        ],
        [
            "And here's the the training time.",
            "The training times dramatically reduced one of these is learning from scratch, so this this blue is learning from scratch with the top down algorithm, maroon is revising using the top down algorithm, and then this very barely visible white line is the training time for the full tomorrow algorithm, which uses bottom up search to really direct the search."
        ],
        [
            "A much better way.",
            "OK, so I just want to briefly talk about a couple of 1 extension to this work that will be talked about it inch by this summer so Lily will give a talk in a couple of weeks on this work where we did an extension of Tamar to learn with extremely little target data.",
            "It uses just very minimal target data to determine a good predicate mapping from the source less than a single mega example.",
            "In fact, we're just going to."
        ],
        [
            "But information about a single entity.",
            "So normally we trained it on one full mega example, which is a whole bunch of entities richly connected with each other.",
            "But let's say we have just very little training data in the target, so I just give you one person Bob and I tell you all the people he's related to an all the objects that is related to and his properties, but I only give you information about a single individual.",
            "Can I actually transfer with such little limited?"
        ],
        [
            "Target data solely developed a slightly alternate mapping algorithm which she called Sr to LR 'cause it takes this idea of mapping from the mapping from the short ranges to the long range clauses and the idea is we can.",
            "We can only really evaluate on this limited target data the clauses that talk about a single object, but we have other clauses that really talk about the relationships between multiple entities and we can't really evaluate those.",
            "So the idea is we can actually establish a mapping based on the chartrain clauses.",
            "And use that to infer a mapping for the long range clauses and actually."
        ],
        [
            "Get pretty good results, so we showed was given just a single entity of data in the target domain.",
            "The maroon here is that tomorrow if we use our existing mapping algorithm, we get some good results, but not as good as if we use this new enhanced our that specifically intended.",
            "When you have very limited target date."
        ],
        [
            "The last transfer in MLM thing I wanted to talk about was work that Jesse Davis presented at ICM L just a couple of weeks ago on deep transfer with second order Markov logic networks, which tries to transfer very abstract patterns between very desperate domains by learning patterns in a second order logic that verbalize."
        ],
        [
            "For predicate, so let's not start going backwards again.",
            "So the idea is to generalize to very different domains, and so one of the examples that Jesse gives in his paper is transferring from a web page domain where you have web pages linked to other web pages, transferring knowledge you learned in that domain to something completely different as Monty Python would say, where you have a protein interaction network now is there much you can actually transfer between the structure of the web and the structure of interacting proteins?",
            "Well, it turns out there is some things you can transfer.",
            "R."
        ],
        [
            "So what they do is they call this DTM deep transfer with Markov logic.",
            "They come up with second order patterns and they abstract away the predicate names and discern very high level structural regularity's and then they search through the space of the 2nd order formula to find ones that fit the data well and then they use those as a bias when they learn in the top."
        ],
        [
            "I demand.",
            "And they tested on a number of different domains of protein interaction data set a version of web KB I should mention here that this version actually does use the information from the text.",
            "Anna version of a social network based on face."
        ],
        [
            "Fuck, I'm actually show that you can learn some pretty abstract concepts that generalize between these very disparate domains.",
            "So there's this general concept that's known in the Network World called him awfully, which says if you have two entities and entity one is related to Entity 2, then a lot of times they share a property so it learns this very general pattern.",
            "Another very general pattern that learns is transitivity, which says if you have three entities and entity one is related to entity two and ending two is related in maybe 3, then frequently entity three will be related to entity want.",
            "And then Cemetery.",
            "Also, which of course says that if we have two entities, if ones related to the other than the others probably related to the one.",
            "So it learns very abstract patterns and then Jesse presents."
        ],
        [
            "Experiment showing that by using those abstract patterns to transfer knowledge from 2 very different between two very different domains, you can actually get improved results.",
            "So these are learning curves in the area under the PR curve where the bottom line here is just the standard top down learning algorithm.",
            "But if we transfer knowledge properly these are different techniques.",
            "I don't have time to go into that he talks about, but with the best technique you get quite dramatically better results and this is transferring from web to yeast example that I was in."
        ],
        [
            "Just earlier and he also gets good results transferring from Facebook Domain to the web KB domain."
        ],
        [
            "OK, so I'm starting to run out of time so just to talk a little bit about what I think are some future research issues along these lines.",
            "Obviously there's a standard one of trying to apply these techniques to bigger and more realistic application domains.",
            "Notice that when I talked about the transfer learners I always compared them to top down structure learners.",
            "If you compare him to bottom up structure learners they don't work as well.",
            "And what we really need is more bottom up structure learning algorithms that use transfer.",
            "R. Also, like I said, a lot of the ideas I've presented here, I don't think are specific to M lens we've been working with them, and so we've tested him out there first, but I think a lot of these ideas of bottom up search and transfer you could apply to other your favorite other SRL model, your favorite TLA.",
            "And our predicate mapping currently for transfer is quite limited.",
            "We would like to try to be able to do other things.",
            "It doesn't allow reordering.",
            "The arguments are the arity of predicates to change.",
            "You might want to be able to map one predicate to injunction of greater than one.",
            "So say I didn't actually have the work for predicate in the movie domain, but I do have that.",
            "Actors are in movies and directors are in movies.",
            "Well, I might find that a student advising being advised by a professor is like an actor working in.",
            "That in a movie and then that movie having a director so I have to actually map a unary predicate to a conjunction of predicates in the other domain.",
            "You might view this as transfer motivated predicate invention.",
            "So this is something that that was been on our To Do List for awhile, but Lily wants to graduate this summer, so I don't think we're going to."
        ],
        [
            "A couple other things I think are important on the transfer side.",
            "One is multiple source transfers.",
            "So far we've looked at just transferring from a single domain to a target source domain to a target domain.",
            "It would be nice to transfer from multiple source problems, determine which clauses to map from which previous domain, and revise them, and use multiple source domains when you're learning in it."
        ],
        [
            "Target domain, another obvious problem is what sometimes called source selection and transfer learning, which is I in all these cases I told it.",
            "Start with.",
            "You know UW CSE and transfer to IMDb.",
            "But in the real world I have tons of previous domains out there and I don't necessarily know which ones are going to be the best ones that transfer from it would be nice to have an automated algorithm that hopefully in time that's even sub linear in the number of domains I've previously experienced find the best ones is to use service sources for transfer in a new.",
            "In a new problem."
        ],
        [
            "OK, so to wrap up and leave some time for questions, I've tried to talk today about what I think are two important ways to improve structure learning for statistical relational models such as Markov logic networks particularly, we talked about the value of doing bottom up search and we talked about three different systems that showed the advantage of doing bottom up search and learning structures for Markov logic networks bustle.",
            "This olive combined with MLN's approach an this LHL technique.",
            "We also talked about transfer.",
            "Learning and three different systems.",
            "Tomorrow, Sartell are in the 2nd order MLN's and how they can be used to transfer knowledge from one domain to another.",
            "When you're learning complicated relational structures and I've tried to show that both of these techniques improve both the speed of training because we're doing a more effective, efficient search of the hypothesis space and the accuracy of the learn models because we're imposing a good bias on that space to prefer a good set of hypothesis by using clues bottom up from the data.",
            "The other point I wanted to try to bring home is, is that.",
            "There's a lot of work in when I'm here calling classical IOP that I think a lot of the SRL literature doesn't effectively utilized, and there are a lot of ideas there that I think can be very effective in improving the state of the art in statistical release."
        ],
        [
            "Personal learning OK thanks.",
            "OK, I think I left plenty of time for questions.",
            "Yes, Steve.",
            "Lots of interesting examples in which Constellation data positive effect on the learning curve.",
            "It's quite easy to imagine that the officer could come true.",
            "Secondly, I guess that was gonna happen.",
            "Yeah, so the first one is yes, we actually have some negative results.",
            "You can actually read the edge type paper 'cause there are some unfortunately negative results in there where transfer actually hurt at least a little bit, right?",
            "Lonely and so yes, we do have experience with negative transfer even between these three domains when you're given very limited data.",
            "So yeah, so that's the source selection problem, which I think is within transfer learning as a whole, not just whole, not just in transfer.",
            "Learning for S 1st.",
            "Relation learning, but I think transfer learning in whole this whole problem of there's been a few pieces of work that try to cluster domains where you actually cluster problems and then if 2 problems end up in the same cluster then you think you can transfer between them.",
            "Been some nice work on that, but not not a lot so I don't know if I have a lot of ideas to say about the source selection other than I agree with you.",
            "It's a very important problem here.",
            "You're relying on human ingenuity to tell you what are good problems to transfer from, and we of course would like to automate that process.",
            "So that payments for learning slideshow from from Jesse and pictures the first one I can remember seeing where the asymptotes were different, is that because there's a concept that's really important for used protein that families here?",
            "Maybe we might have a better answer which which one are you?"
        ],
        [
            "Talking about, yeah I know Stanley where is he?",
            "Concept that's that's critical for the target task.",
            "That can really only be learned from the source.",
            "I guess that's more question for Pedro or Stanley does.",
            "Research.",
            "Thanks for learning.",
            "Enables escape local office.",
            "Yeah, so actually the question I asked Stanley and his talk a couple of weeks ago was have you compared this to your new?",
            "I'm sorry this is Jesse's where I'm screwing.",
            "He's not here.",
            "Sorry getting confused.",
            "I'm jet lagged so I asked him.",
            "I asked Jesse why didn't you you compare to Stanley's new algorithm instead of Stanley's old algorithm.",
            "And he said, Oh yeah, I'd like to do that someday.",
            "Because I actually am, because that is less subject to these local minimum problems that causes.",
            "So I think this is a search what Pedro saying and I agree with him.",
            "It's a search problem.",
            "It's not a fundamental transfer problem.",
            "I'm sorry there was something up here.",
            "First, ask when you're learning of the model for the source mascot.",
            "Imagine if you're if you know you're going to be transferring it to a new target would learn very differently, or you want to learn very differently than if you just.",
            "Yeah, that's very good.",
            "I'm looking at my student Lily 'cause her tonight.",
            "She and I've talked about exactly this problem is interesting question.",
            "Sometimes the best thing to learn to transfer is not the best thing to learn.",
            "If you want to learn the source, we actually have some nice results.",
            "Actually, they're not as good as we thought they were, lilies ran some more experiments lately where we have a different version of muscle that learns a different theory that actually does less well in the source domain to start.",
            "But when you transfer from it, it actually does better.",
            "So we've actually seen that happen on occasion that it's better to learn a model in the source that's not as good in the source.",
            "But might be better for transferring to problems later on, so I think that's a very interesting point and we have just very little information.",
            "I don't know if you have anything to say about that.",
            "More general, right?",
            "So I so agree with you and I don't think that's been looked at that much that what you want to learn in the source actually depends.",
            "It actually turns it more, and if you're familiar with the term multi task learning 'cause usually transfer you assume you learn in the source just to do well in the source and then later someone pops this new domain on you multitask.",
            "Assumes you know all the domains up front and you know you're going to have to learn to solve all of them.",
            "And sometimes I think this deep transfer is actually more of a multi task type of technique rather than a quote transfer.",
            "But those two things are very close to each other.",
            "But it's a good point.",
            "The transfer is running on different where you can imagine multi task with different representations for each task right?",
            "I mean it's just a different variation of Mount Multitask.",
            "Yeah.",
            "How is it representation of it work?",
            "How much force background on the Turnpike background knowledge?",
            "Or did you use only?",
            "Mapping you're saying?",
            "Yeah, no?",
            "So there's no domain knowledge used during mapping, and that's not because I don't think that might be useful.",
            "I think it very well might be useful.",
            "I mean, it doesn't actually know that you know mapping the person type to a person type is actually a good thing.",
            "It just knows that when it does that, it gets good results in the target domain.",
            "So I think exploiting that sort of information even using the lexical items and maybe using lexical knowledge like word.",
            "Net to know that mapping of person type to another person type like mapping a director to a professor makes sense because there at least both people but mapping out.",
            "Professor to a movie probably doesn't make a lot of sense, so I think there is lots of room for using lexical knowledge and domain knowledge to help.",
            "We actually tried this right tunes.",
            "We tried this and we couldn't get it to work right.",
            "So there's a distinction between domain knowledge and mapping knowledge.",
            "We should make.",
            "I think that distinction I was talking more about maybe or closing the source domain and then on close is a target for me.",
            "And then you can just try if they look similar to each other under some mapping.",
            "So I think my question is whether you use branch of knowledge in the source and targets or within use of 1st order.",
            "Yes, I think you're gonna answer part because we use abstract knowledge in the source, but we never use abstract knowledge in the target, we just use the data in the target.",
            "You could imagine learning clauses in the target and in the source and establishing the mapping from clause to clause rather than.",
            "Mapping the clauses and testing it on the target date.",
            "Again, these are ideas that I've talked about with my students on various occasions, but we haven't tried anything like that.",
            "I think there are other ways to do mapping.",
            "I mean like I said, the way we do mapping is pretty pretty primitive.",
            "We just found that it actually works pretty well and so we actually focused on other problems.",
            "Steve had another question.",
            "Logical reasoning, sure.",
            "What's the difference between this is one of the ones where you pay the guy for having the question.",
            "So here's the slide on for that question, which is.",
            "So there's a lot of work in analogical reasoning, particularly done by actually people who were at the universal.",
            "And when I was there, so I took all their classes.",
            "I know them quite well.",
            "I think, David, you know these folks to buy Brian Falcon Heiner, and conformist Dedre Gentner.",
            "So they have various ways of mapping things so that.",
            "Key thing is, is they always base their conditions purely on sort of structural analogy.",
            "They never look at empirical adequacy of the knowledge that's map.",
            "So the key difference is they never consider the accuracy of the map knowledge in the target domain to prefer a mapping.",
            "They just say this has goods.",
            "All Systematis city is just a structural characteristic can pick the best mapping.",
            "We pick the best mapping by trying different mappings, seeing how well the map knowledge actually performs and making good predictions in the target domain, and that I think is a key difference between this approach and the traditional work that's been done in analogical reasoning in AI.",
            "It's a very good question, which is why I have a backup slide.",
            "There's no induction then running out early.",
            "There's no revision, basically.",
            "Yeah, it's just mapping and that's it.",
            "They don't have the revision step at all.",
            "So I think that's another.",
            "That's another difference.",
            "Thanks Pedro."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hey Pedro, thanks for the nice introduction and thanks to the organizers.",
                    "label": 0
                },
                {
                    "sent": "Also for the other organizers.",
                    "label": 0
                },
                {
                    "sent": "Also for inviting me.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry I brought the hot weather with me here from from Texas.",
                    "label": 0
                },
                {
                    "sent": "If you think it's hot here, spend the summer in Austin.",
                    "label": 0
                },
                {
                    "sent": "I tried to get away but I hate it when I get away and it's hot wherever I go.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to talk today about some of our recent work in statistical relational learning, particularly focusing on two topics.",
                    "label": 0
                },
                {
                    "sent": "Bottom Up, search and transfer learning, and this is joint.",
                    "label": 1
                },
                {
                    "sent": "Most of what I talk about is joint work with.",
                    "label": 0
                },
                {
                    "sent": "So my graduation slowly malkova until union.",
                    "label": 0
                },
                {
                    "sent": "They're both here so that you can talk to both of them about the various aspects of their work that I'll talk about today.",
                    "label": 0
                },
                {
                    "sent": "And also I'll talk about some of the work from the Washington Group, and I'd like to thank Jesse Davis and Pedro and Stanley Kok for giving me some slides that I'm also using in here.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the real problem with all this sort of work that we're doing in relation learning whether statistical relational learning, inductive logic programming are learning with graphs is those models define a very large, very complex hypothesis space, and every machine learning person knows that that makes the problem hard, because it makes it both hard for time complexity 'cause searching that space for a good hypothesis is very hard when the space is so enormous and also from learning theory, we clearly know that the larger the hypothesis space, the more examples we're going to need.",
                    "label": 0
                },
                {
                    "sent": "To learn so without driving the right biases that give give us to the right part of the space it's going to take us an intractable number of examples to learn with such very rich hypothesis spaces.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the particular problem I want to talk about is structure learning in these sort of relational models.",
                    "label": 0
                },
                {
                    "sent": "So statistical relational models usually consist of two parts.",
                    "label": 1
                },
                {
                    "sent": "They have a structure, like a set of logical formula or some graphical model or relational model that specified the structure of the model and then they have a set of parameters or weights which might be potentials in a Markov net or probabilities are conditional probabilities that parameterized them.",
                    "label": 0
                },
                {
                    "sent": "So parameter learning, I think, is generally agreed to be an easier problem, and then the techniques for that are much more well developed.",
                    "label": 0
                },
                {
                    "sent": "The harder problem is learning the structure.",
                    "label": 0
                },
                {
                    "sent": "Of the model, which is a much more difficult and less well developed problem.",
                    "label": 1
                },
                {
                    "sent": "In fact, in a lot of papers you'll see that there is no structure learning going on that a lot of people, even in the SRL community, specify the structure upfront manually by using human engineering to specify the structure of the model.",
                    "label": 0
                },
                {
                    "sent": "And they just learn the parameters from data.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to focus on that.",
                    "label": 0
                },
                {
                    "sent": "What I think is the more difficult problem of actually learning the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Structure.",
                    "label": 0
                },
                {
                    "sent": "And I think there are these two techniques that bottom up search and transfer.",
                    "label": 0
                },
                {
                    "sent": "Learning can really be beneficial in making structure learning better.",
                    "label": 0
                },
                {
                    "sent": "So I think there are two effective techniques for ameliorating the time and sample complexity problems of learning these complex relational models that I talked about.",
                    "label": 1
                },
                {
                    "sent": "So I'll talk about both of these.",
                    "label": 0
                },
                {
                    "sent": "So bottom up search is idea of using directly using the data to drive the formation of promising hypothesis and transfer learning.",
                    "label": 1
                },
                {
                    "sent": "Is using knowledge previously acquired in a related domain to drive the formulation of promising hypothesis in some new domain.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so just tickle relation learning.",
                    "label": 0
                },
                {
                    "sent": "I'm sure a lot of you here already quite familiar with it.",
                    "label": 0
                },
                {
                    "sent": "There's a so called Alphabet soup of various sorts of methods.",
                    "label": 0
                },
                {
                    "sent": "This is just a small sample of them from stochastic logic program, probabilistic relational models, Bayesian logic programs, relational Markov networks, and Markov logic networks.",
                    "label": 1
                },
                {
                    "sent": "My talk today will focus on our work on Markov logic networks, but I think a lot of the ideas that I want to talk about in terms of the value of bottom up search and transfer learning or not at all specific to MLN's.",
                    "label": 0
                },
                {
                    "sent": "I think those lessons apply broadly across these other.",
                    "label": 0
                },
                {
                    "sent": "Models and that sort of some of our future work is to maybe apply some of these ideas to other models, but our current work has been focused on on LNS for various pragmatic and other reasons.",
                    "label": 0
                },
                {
                    "sent": "They are.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very nice model, so just to I'm sure a lot of you have even seen talks this morning talking about MLN.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to keep my summary of MLN's extremely short.",
                    "label": 0
                },
                {
                    "sent": "You can listen to Pedro drag on about them for all for all afternoon if you want, but I'm not going to.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to play Pedro so very quickly.",
                    "label": 0
                },
                {
                    "sent": "Markov Logic Network is is a logical KB, so we know logical standard logical KB.",
                    "label": 1
                },
                {
                    "sent": "You have a set of hard constraints on the set of possible worlds.",
                    "label": 1
                },
                {
                    "sent": "The easiest intuitive way to see an MLN is it's just a set of soft constraints on a possible world.",
                    "label": 1
                },
                {
                    "sent": "So when apart when a world a set of facts violates a formula, it becomes less probable, but not impossible like in normal logic.",
                    "label": 0
                },
                {
                    "sent": "So you give each formula weight and then the higher the way that formula, the stronger constraint it is, the more unlikely it makes worlds that violated an.",
                    "label": 0
                },
                {
                    "sent": "In general the probability of some world is the equation at the bottom.",
                    "label": 0
                },
                {
                    "sent": "Here is proportional to an exponential of the weights of the formulas that it that it satisfies.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's the basic idea of an MLN.",
                    "label": 0
                },
                {
                    "sent": "So you can attach weight to rule.",
                    "label": 0
                },
                {
                    "sent": "So here is always like to use the family domain.",
                    "label": 0
                },
                {
                    "sent": "When I talk about logic, so we might have some very definition of things like says if X is wise.",
                    "label": 0
                },
                {
                    "sent": "Parenting wise mailed in.",
                    "label": 0
                },
                {
                    "sent": "Why is X is son?",
                    "label": 0
                },
                {
                    "sent": "That's sort of a definition of rule will give it an extremely high weight.",
                    "label": 0
                },
                {
                    "sent": "Well in other rules might be a little bit more heuristics.",
                    "label": 0
                },
                {
                    "sent": "Isaiah vexes wise parent and ex is married to Z then Z is wise parent.",
                    "label": 0
                },
                {
                    "sent": "Well that's less and less true these days.",
                    "label": 0
                },
                {
                    "sent": "I'll give that a little bit less weight, maybe about a 10 and then maybe another rule.",
                    "label": 0
                },
                {
                    "sent": "This is of ex lives with Y&X is mailing.",
                    "label": 0
                },
                {
                    "sent": "Why is female then they're married?",
                    "label": 0
                },
                {
                    "sent": "That's becoming even less likely, so will.",
                    "label": 0
                },
                {
                    "sent": "Just say that has even a weight of 1 and so you can specify nice logical rules that summer which are very hard constraints that are definitionally and others which are sort of juristic rules.",
                    "label": 0
                },
                {
                    "sent": "And of course you try to learn these weights automatically from data rather than having to.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Defy them up front.",
                    "label": 0
                },
                {
                    "sent": "And then in Mo and also is nice because it defines a very well defined probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "Basically in MLN is a template for constructing a ground Markov net where the ground literals correspond to nodes in the ground network and a ground clause corresponds to clique connecting the ground literals in the clause.",
                    "label": 1
                },
                {
                    "sent": "So two nodes are connected if they occur in the same ground clause, and then the probability of a word isn't world is a normalized exponential model where Z is a normalizing constant and we sum up the weights of all the true groundings of.",
                    "label": 0
                },
                {
                    "sent": "Each of the formula so I hear ranges over the set of formulas in my MLN, and I take the wait times the number of true groundings of that formula, exponentiated and normalize it to get a probability distribution over possible worlds.",
                    "label": 0
                },
                {
                    "sent": "So it defines a very nice probability distribution over possible.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other nice thing about about Emma lenses.",
                    "label": 0
                },
                {
                    "sent": "They have nice software support, which is one of the pragmatic reasons I think why we've been one of the pragmatic reasons we've been using it.",
                    "label": 0
                },
                {
                    "sent": "There's this package, alchemy that Pedro in his his students and other people at the University of Washington maintain, and it's a nice package of software.",
                    "label": 0
                },
                {
                    "sent": "It's open source that has inference algorithms, weight learning, structure learning algorithms, datasets.",
                    "label": 1
                },
                {
                    "sent": "You know everything you need to get started.",
                    "label": 0
                },
                {
                    "sent": "So this is my ad for alchemy and all of our software extends and uses the Alchemy Alchemy software.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's some background.",
                    "label": 0
                },
                {
                    "sent": "So now let's talk about the first bigger idea that I wanted to focus on today, which.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bottom up search.",
                    "label": 0
                },
                {
                    "sent": "So to start with bottom of search we have to contrast it with the North, but I would say the more normal approach which is top down search where we have some method for generating hypothesis and that generates a whole bunch of different hypothesis and then we go ahead and test them on the data and keep just we test it on the training data.",
                    "label": 0
                },
                {
                    "sent": "We keep just a few of the ones that seem to fit the training data without without overfitting.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the typical approach by which algorithms work, and particularly in statistical relational learning.",
                    "label": 0
                },
                {
                    "sent": "It's almost all the structure learning algorithms out there, basically, or some variant of top down search where you start with an empty theory, then you repeat and with until further refinements to that theory failed to improve the fit to the training data, you generate all possible refinements of a current theory where you have some grammar that defines what the possible refinements are, such as adding a literal to a clause in a logical model.",
                    "label": 1
                },
                {
                    "sent": "You test each of those refined theories on the training data and pick the ones that best improve the fit, possibly with a complexity penalty to prevent overfitting.",
                    "label": 0
                },
                {
                    "sent": "And then you keep repeating that.",
                    "label": 0
                },
                {
                    "sent": "So the problem is you get this huge branching factor 'cause these models are very complex, which means the number of possible refinements to them is huge.",
                    "label": 0
                },
                {
                    "sent": "So you get this incredibly large branching factor just and so the search becomes very intractable.",
                    "label": 1
                },
                {
                    "sent": "So you're forced to use some sort of greedy or beam search to control the time complexity, and then that subjects you to local Maxima.",
                    "label": 0
                },
                {
                    "sent": "Another standard problem with those sort of juristic search methods.",
                    "label": 0
                },
                {
                    "sent": "So these sort of top down methods, I think, are fundamentally constrained by their basic framework.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the alternative of bottom up search says when I generate hypothesis, I'm going to immediately start looking at the training data.",
                    "label": 0
                },
                {
                    "sent": "Even when I start generating hypothesis, I'm going to use the data itself to generate hopefully a smaller set of good hypothesis and then only test that smaller set of reasonable hypothesis on the data, and then hopefully quite a few of them will be will be good fits to the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the training data.",
                    "label": 0
                },
                {
                    "sent": "So the whole idea of bottom up search is to use the data to directly drive the formation of a much more limited set of more promising hypothesis.",
                    "label": 1
                },
                {
                    "sent": "And sometimes people refer to this sort of approach also as a data driven approach or a specific to general sort of search method rather than a general to specific, which is what most top down models.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's a long history of work in bottom up search in IO P. In fact from the very beginning days of inductive logic programming.",
                    "label": 1
                },
                {
                    "sent": "This these 3 letter Acronyms because if this conference is very confusing.",
                    "label": 0
                },
                {
                    "sent": "'cause you don't know really know when someone says IOP if they mean integer linear programming or if they mean.",
                    "label": 0
                },
                {
                    "sent": "Inductive logic programming, so this means the ladder here.",
                    "label": 0
                },
                {
                    "sent": "So you know back in the early days of Saigal, Steve's earliest work in one of his earliest works in IO, P Universe resolution very much a bottom up sort of technique that goes back over 20 years now and then.",
                    "label": 0
                },
                {
                    "sent": "LGG least channel generalization goes back over what, 30 or almost 40 years now?",
                    "label": 0
                },
                {
                    "sent": "So there's a long Tran.",
                    "label": 0
                },
                {
                    "sent": "Then Golomb was was the bottom up IO P system built on top of the concept of plaque and concept of LG.",
                    "label": 0
                },
                {
                    "sent": "So bottom up ideas go back a very long way in IO P.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But pretty quickly I think.",
                    "label": 0
                },
                {
                    "sent": "It sorry.",
                    "label": 0
                },
                {
                    "sent": "Skip one.",
                    "label": 0
                },
                {
                    "sent": "Our.",
                    "label": 0
                },
                {
                    "sent": "Whoops, yeah, so one idea that I'd like to talk about.",
                    "label": 0
                },
                {
                    "sent": "That's a bottom up technique that I developed with a former student of mine, Brad Richards, back in 1992, is relational pathfinding where the idea was to learn definite clauses based on finding paths of relations, connecting the arguments of a specific positive example.",
                    "label": 1
                },
                {
                    "sent": "So it's driven by a particular positive example, which gives it this bottom up flavor, and again using the family domain as an example.",
                    "label": 0
                },
                {
                    "sent": "Say we have a simple family like this, where the Red Arrows denote parent and the green lines represent the married relation.",
                    "label": 0
                },
                {
                    "sent": "So I give you the positive example that Tom is Mary's uncle.",
                    "label": 0
                },
                {
                    "sent": "I find the path between those two items Tom and Mary in the graph, and I missed initially immediately create a clause bottom up from that single training example based on that particular example that looks like this, I take each of the constants and I verbalize it and I get a pretty good pause for uncle and one from one example very very quickly.",
                    "label": 0
                },
                {
                    "sent": "And if I want to make that absolutely correct, I have to do a little bit of maybe top down refinement of it to add a constraint that W has to be male and I get a perfectly good single clause definition.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Banco not only that, so I get another example that Bob is Anne's uncle where Bob is a different type of uncle right by marriage, but so that sets up a slightly different path.",
                    "label": 0
                },
                {
                    "sent": "That gives me a slightly different clause, which I verbalize.",
                    "label": 0
                },
                {
                    "sent": "I add a constraint to top down and I get the other clause for Uncle, so from 2 examples I've learned both the disjunctive clauses for Uncle.",
                    "label": 0
                },
                {
                    "sent": "So it's a very powerful heuristic, bottom up method in the domains where it's applicable in family.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Game is 1 where it works great.",
                    "label": 0
                },
                {
                    "sent": "But after these top down methods, there was a lot of work in IOP and methods that I'm calling hybrid here.",
                    "label": 0
                },
                {
                    "sent": "That's a term that people refer to a lot of different things with.",
                    "label": 0
                },
                {
                    "sent": "By here I mean combining top down and bottom up techniques.",
                    "label": 0
                },
                {
                    "sent": "So back in ACNL back in 94 Johns Ellie Ann I proposed of sort of combination at back.",
                    "label": 0
                },
                {
                    "sent": "In those days I called it down for the head foil for the body.",
                    "label": 0
                },
                {
                    "sent": "But it's pretty good characterization of the of the technique called chillin and then profile came along which I view very much as a hybrid technique.",
                    "label": 0
                },
                {
                    "sent": "It has some very strong as you start up with the seed example which is very much a bottom up driven process, but then you do this sort of top down simplification of the bottom clause so that sort of top down so it's very much a hybrid and then Alex System of course is built.",
                    "label": 0
                },
                {
                    "sent": "Built mainly off of pro.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All ideas, so there's this rich tradition of working IO P instead of bottom up in hybrid method.",
                    "label": 0
                },
                {
                    "sent": "But within SRO there's really not that much.",
                    "label": 0
                },
                {
                    "sent": "Mostly algorithms seem to be influenced by graphical model learning techniques which are highly dominated by top down techniques which I sort of attribute.",
                    "label": 0
                },
                {
                    "sent": "There's this nice Heckerman tutorial on learning graphical models, but it's totally prevents it from the presents it from the top down idea and everybody takes these top down approaches to learning graphical models.",
                    "label": 0
                },
                {
                    "sent": "Everybody is of course maybe a little exaggerated, but not much.",
                    "label": 0
                },
                {
                    "sent": "And I think another problem with this is I could I can be critical of American researchers 'cause I am one that many American researchers in statistical relational learning are really not sufficiently familiar with the rich history of bottom up techniques in within inductive logic programming, and I think they could be more so and they would get more ideas for developing more interesting structure learning algorithms if they were more familiar with that work myself.",
                    "label": 1
                },
                {
                    "sent": "I've been working in IOP almost from the beginning.",
                    "label": 0
                },
                {
                    "sent": "I didn't go to the first LP conference, but I went to the second one in Tokyo in 1992, so I hopefully am familiar with these.",
                    "label": 0
                },
                {
                    "sent": "Ideas, and we've been very productive at exploiting these IOB ideals to build better SRL models, and I don't think enough people have been doing that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is why I'm giving this talk.",
                    "label": 0
                },
                {
                    "sent": "OK, so one of the things I'm going to talk about a few albums that use bottom up ideas and learning lens.",
                    "label": 0
                },
                {
                    "sent": "So the first one I want to talk about is a system that my student Lilly presented at ICM Ella couple years ago called Muscle which is just a simple acronym for bottom up structure learner.",
                    "label": 0
                },
                {
                    "sent": "It's actually not strictly bottom up.",
                    "label": 0
                },
                {
                    "sent": "Like a lot of code, I'm calling bottom up techniques there.",
                    "label": 0
                },
                {
                    "sent": "They have a certain amount of top down components, so strictly speaking I would actually call bustle hybrid technique, but it exploits it sort of partial proposition proposition alization of the data by using a sort of variant of relational pathfinding.",
                    "label": 0
                },
                {
                    "sent": "And it also uses an existing Markov net structure learner to build a Markov net template that constrains the clause construction.",
                    "label": 1
                },
                {
                    "sent": "So this sort of uses the data to build this template and then uses the the template to constrain its top down search for clauses.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of very much a hybrid, but it's using a lot of bottom up sort of information.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I don't have to go into the details, is just a really quick overview of how Bustle works.",
                    "label": 0
                },
                {
                    "sent": "So for each predicate in the domain.",
                    "label": 0
                },
                {
                    "sent": "So this assumes a sort of non discriminative case where you want to learn a set of clauses to predict all the possible predicates in your domain.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do this for every predicate we construct a set of template nodes where you're going to represent the nodes in the final template Markov network and then use them to partially proposition lies the data.",
                    "label": 1
                },
                {
                    "sent": "And we're going to construct a Markov network template from that propositional data, sort of using a sort of.",
                    "label": 0
                },
                {
                    "sent": "Upgrading sort of approach and then form candidate clauses based on those templates, which then constrain the clauses.",
                    "label": 0
                },
                {
                    "sent": "So I'm proposing many fewer candidates than I do in a normal top down approach.",
                    "label": 0
                },
                {
                    "sent": "And then you evaluate all the candidate causes on the training data and keep them.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "So these template node you construct these nodes based on conjunctions of one or more verbalised literals that serve as sort of building blocks for the clauses that you're eventually going to construct, and you construct them by looking for groups of true constant sharing ground literals in the data, and then verbalizing them, which is a sort of short pass, so it takes a set of short paths that exist in the actual ground data, very much.",
                    "label": 1
                },
                {
                    "sent": "A bottom up process, and generates these so called template nodes from that, so it could hurt can be viewed as a partial relate using partial relational pasta, sort of.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Partially proposition lies the data, so here's just a very simple example in the movie domain.",
                    "label": 0
                },
                {
                    "sent": "Use these layers.",
                    "label": 0
                },
                {
                    "sent": "Somebody just briefly explain so this is using a little bit of IMDb data which will talk about a little bit later, but we have predicates like someone's at actor, someones a director and then this works far means that some actor worked for some particular director.",
                    "label": 0
                },
                {
                    "sent": "Here Brando work for Coppola and Godfather say and Pacino did.",
                    "label": 0
                },
                {
                    "sent": "And then there's this movie predicate which means some particular movie contains some individual in its credit list and so.",
                    "label": 0
                },
                {
                    "sent": "For if we have a current predicate actor, refined paths of relations that connect that actor to various other predicates, we might have chains of 1 here where these chain is also connected by this variable F in this one by D and then it creates instances for every value of a it creates denero in the one of these in this table, and it's sort of partially proposition lies the data based on these sort of pieces of.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ads.",
                    "label": 0
                },
                {
                    "sent": "Then once we have that, we have this table of propositional data, we give that to a standard ground traditional Markov net.",
                    "label": 0
                },
                {
                    "sent": "Learning our and we use this algorithm developed by Bloomberg Broom Bromberg a few years ago.",
                    "label": 0
                },
                {
                    "sent": "So it takes that propositional data and generate a network where the nodes are these so-called template nodes, and then this is what we call them.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Template Markov net.",
                    "label": 0
                },
                {
                    "sent": "Now that we used to constrain the set of clauses that will consider instead of just compare looking at all possible clauses we look at cliques in that template network and only construct clauses and evaluate them that are consistent with that, because again, clauses correspond to clicks in the graph and we use the template network to drive the construction of a much smaller set of alternative possible clauses and then evaluate those.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, that's just a real quick overview, and so we we we ran a fair number of experiments to try to show the advantage of this technique compared to standard top down structure learning.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All three of the datasets are used throughout this talk.",
                    "label": 0
                },
                {
                    "sent": "If you know anything about MLN's, I'm sure you've heard of this nice data set about the computer science Department at the University of Washington that Matt and Pedro put together a number of years ago, and it includes professors like Professor I mean predicates like Professor Student, advised by taught by publications, things of that sort.",
                    "label": 0
                },
                {
                    "sent": "We created a small IMDb data.",
                    "label": 0
                },
                {
                    "sent": "When I talk about transfer learning, the reason why we kept this data small originally is we wanted to view it as a target of transfer.",
                    "label": 0
                },
                {
                    "sent": "Learning a domain where you have very little data but you wanted to transfer knowledge from another domain.",
                    "label": 0
                },
                {
                    "sent": "So this is a relatively small datasets only extremely small fraction of all of IMDb, but it contains predicates like actor, director, movie, worked, far genre, etc.",
                    "label": 1
                },
                {
                    "sent": "And then we also have a version of the web KB.",
                    "label": 0
                },
                {
                    "sent": "It gets a little confusing 'cause there's so many variants of this.",
                    "label": 0
                },
                {
                    "sent": "What I talk about in this talk, at least for most of 'em, doesn't use the text of the pages, it just uses the.",
                    "label": 0
                },
                {
                    "sent": "Entities and the relations between them, and it includes predicates like faculty and student in project, and that way it's pretty similar in some of its predicates.",
                    "label": 1
                },
                {
                    "sent": "It's sort of a subset of the predicates that occur in the UW.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jesse domain.",
                    "label": 0
                },
                {
                    "sent": "So these datasets are organized by what I call we call Mega Examples, where you're doing relation learning.",
                    "label": 0
                },
                {
                    "sent": "It's pretty tricky because if you're going to do cross validation you need independent training and test data, but the whole point is your data is rich and relationally connected, so it usually what we do is we assume the data consists of these large sets of connected objects, so it's a standard examples in the web domain they have data from 4 different universities and people each.",
                    "label": 0
                },
                {
                    "sent": "Each University defined its own connected graph and you can train on some universities, test on others 'cause those are independent.",
                    "label": 0
                },
                {
                    "sent": "So we call those large sets of.",
                    "label": 0
                },
                {
                    "sent": "Relationally connected fats and mega example and here's just some statistics, just to give you some idea.",
                    "label": 0
                },
                {
                    "sent": "I mean, once you get down to the number of ground literals, sorry, this literal stands for ground literal.",
                    "label": 0
                },
                {
                    "sent": "It turns into a fairly large number of property propositions.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mom.",
                    "label": 0
                },
                {
                    "sent": "OK, looked.",
                    "label": 0
                },
                {
                    "sent": "Weather going backwards.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Ah OK.",
                    "label": 0
                },
                {
                    "sent": "So then the methodology that we run our experiments.",
                    "label": 0
                },
                {
                    "sent": "We generate learning curves using leave one Mega example out, which in the web domain people we've talked about leave one University out, which isn't leave one mega examples out.",
                    "label": 1
                },
                {
                    "sent": "Just the generalization that you run keeping one mega example for testing train on the remaining ones in test on the on the one an average of results over that, giving it 1 Meg.",
                    "label": 0
                },
                {
                    "sent": "Example out of time so you can plot a learning curve and then we evaluate the MLN by performing inference on the literals of each predicate, providing the rest is evident.",
                    "label": 1
                },
                {
                    "sent": "So you test each predicate holding it out.",
                    "label": 0
                },
                {
                    "sent": "As a test predicate training on I mean giving that data from all the other predicates and trying to predict that missing one, and we compared our technique to what at the time when we did this work was the current best structure learner out of the University of Washington, which I'm here calling T DSL for top down structure learning to counterpose it to bustle the bottom up one.",
                    "label": 0
                },
                {
                    "sent": "I don't know why this thing started going back.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use the two standard metrics that the Washington Group is designed for testing MLN's.",
                    "label": 0
                },
                {
                    "sent": "The conditional log likelihood of the of the test data averaged over all the test literals and the area under the precision recall curve, which I think is usually a better measure 'cause there's so many false literals that you can get pretty high CLL by just predicting everything is false.",
                    "label": 1
                },
                {
                    "sent": "But AUC really PR really requires you to predict the correct positive facts.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a result of a learning curve in the IMDb domain, where blue is the top down algorithm, red is the bottom up.",
                    "label": 0
                },
                {
                    "sent": "We do quite a bit better.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are the results in the in the UW CSE domain.",
                    "label": 0
                },
                {
                    "sent": "I've also included here.",
                    "label": 0
                },
                {
                    "sent": "the Washington Group has developed a hand built knowledge base for this domain and that's shown in purple here, but red eventually even beats out the hand built knowledge base by using bottom up learning.",
                    "label": 0
                },
                {
                    "sent": "But the top line structure learner does it.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then this is the results in the web KB after one Meg example were tide with the top down learner, but then it tends to start overfitting.",
                    "label": 0
                },
                {
                    "sent": "It looks like or remains level and we keep keep learning.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of 1 demonstration at the bottom up technique can be a much better learner.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then a top down technique and it also is not only better on accuracy, it's much better on time 'cause we're reducing the number of hypothesis we need to explore quite dramatically so we can see we get dramatic reductions in training time as well.",
                    "label": 0
                },
                {
                    "sent": "These are minutes and and the bustle is of course to lower the lower bar.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's a quick view of this this out bustle algorithm.",
                    "label": 0
                },
                {
                    "sent": "I want to briefly then talk about another bottom up structure learner from lens that we worked on that was presented at ICM out last year.",
                    "label": 0
                },
                {
                    "sent": "But my student to you and it said there were actually moving to the discriminative setting where we assume we have a target predicate that we want to predict and we're going to learn a theory pacifically for predicting that predicate from all the other background predicates and we found that existing nondiscrimination on structure learners did very, very poorly on a set of IOP benchmarks.",
                    "label": 1
                },
                {
                    "sent": "1 problems getting into.",
                    "label": 0
                },
                {
                    "sent": "One of our goals going into this project was to say how do I my lens compared to the best current state of the art IOP learners on some of these standard IOP benchmarks like in molecular biology.",
                    "label": 0
                },
                {
                    "sent": "So we ran the existing structure learners and they did quite poorly.",
                    "label": 1
                },
                {
                    "sent": "But then we built a hybrid discriminative IOP algorithm based on all if the standard IO P system to learn candidate clauses and it did much much.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the general approach is is we start out with a general IO P system that generates lots of possible clauses and then we wait those with the discriminative wait learner and we buy us some of that.",
                    "label": 0
                },
                {
                    "sent": "We try to put a very high bias towards zero wait so it does very strong regularization so that a lot of the clauses are just eliminate it because they end up with zero weights during the discriminative weight training process.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, you know the paper last year talks about the details of this.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to talk very briefly about it, so the structure learning part.",
                    "label": 1
                },
                {
                    "sent": "We used a variant of all.",
                    "label": 1
                },
                {
                    "sent": "If we called out of plus plus 'cause it just opens up Aleph and allows it to generate a lot more clauses than it normally would by loosening the restrictions on what we think is a good clause, that's good enough.",
                    "label": 0
                },
                {
                    "sent": "So it generates again.",
                    "label": 0
                },
                {
                    "sent": "It's the idea is it's going to be used to generate a ton of possible clauses, but those claws are being generated bottom up using the standard.",
                    "label": 0
                },
                {
                    "sent": "All of things that rely on on the bottom claws and things of that sort.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we use descriptive weight learning.",
                    "label": 1
                },
                {
                    "sent": "To learn the weight on those maximizing the conditional log likelihood of the of the target predicate on the training data, and we another thing we did to make this thing work better was for the case of non recursive clauses.",
                    "label": 1
                },
                {
                    "sent": "When you're doing discriminative learning with non recursive clauses you can do exact inference in MLN and we do that very efficiently.",
                    "label": 0
                },
                {
                    "sent": "And it's much better than using the standard approximate inference algorithm.",
                    "label": 0
                },
                {
                    "sent": "And we also wanted to highly bias it towards learning 0 weight clauses to eliminate a lot of the less useful clauses that generated during this very expensive generation of bottom up.",
                    "label": 0
                },
                {
                    "sent": "Possibilities using olive, so we used L1 regularization, which if you don't know it just a different way of regularising that bias is more Tord 0 weight clause.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the datasets we tested it with.",
                    "label": 0
                },
                {
                    "sent": "This is is the one of the standard biochemistry domains in used in IOP, which is this problem of comparing Alzheimer's drugs for various biochemical properties.",
                    "label": 1
                },
                {
                    "sent": "So there are four different datasets in this general data set about Alzheimer's drug.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here are the the accuracy results using different techniques.",
                    "label": 1
                },
                {
                    "sent": "So even our algorithm bustle because and doesn't work that well in this domain.",
                    "label": 0
                },
                {
                    "sent": "So the first 2 rows here are the this previous structure learners actually just generic IOP algorithms.",
                    "label": 0
                },
                {
                    "sent": "The yellow here is alif.",
                    "label": 0
                },
                {
                    "sent": "Generic LPR is doing much better on this domain, but using our technique that combines ideas from my opian from discriminative weight learning from lens, we get even better results here in red.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we also the nice thing about my lenses are so general that one constraint that really wasn't being applied in the IO P results for this problem is is that you're trying to predict this predicate less toxic.",
                    "label": 0
                },
                {
                    "sent": "That says that one drug is less toxic than another drug, but it didn't even use the knowledge that that that predicate should be transitive.",
                    "label": 0
                },
                {
                    "sent": "That if a is less toxic than B&B is less toxic than see.",
                    "label": 0
                },
                {
                    "sent": "That obviously is less toxic than see within MLN.",
                    "label": 0
                },
                {
                    "sent": "Just put that clause in, put an infinite wait on it so it's a hard constraint, throw it into the MLN and it adds that knowledge to making all of its prediction.",
                    "label": 0
                },
                {
                    "sent": "So this makes it collected the normal way this data set was evaluated.",
                    "label": 0
                },
                {
                    "sent": "It wasn't doing collective inference.",
                    "label": 1
                },
                {
                    "sent": "We throw this background predicate in clause in.",
                    "label": 0
                },
                {
                    "sent": "It does do collective inference and we get a little bit better.",
                    "label": 0
                },
                {
                    "sent": "So by adding collective inference by adding this one clause, we go from the green to the blue and we improve our results quite a bit.",
                    "label": 0
                },
                {
                    "sent": "So another nice property of melons, you have additional knowledge destroyed into the MLN, and it will hopefully improve your result.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the last bottom up technique I wanted to talk briefly about is a technique that was published this year by it by Stanley Kok in an Pedro that was just presented at ICM Ella couple of weeks ago, and it's a new sort of I would call it bottom up again.",
                    "label": 0
                },
                {
                    "sent": "It might be more accurately characterized as a hybrid technique, and it's really even more faithful to the idea of relational pathfinding than our bus.",
                    "label": 1
                },
                {
                    "sent": "Allow rhythm was, and it's pretty clearly the best current structure learner overall for Emma Lens, and it's actually a poster here this evening so you can talk to Stanley more about it this evening at his poster.",
                    "label": 0
                },
                {
                    "sent": "He'll pay me $20 later for the ad.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kaiser.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And the fundamental idea behind behind this technique he calls lifted hypergraph learning is as you do clustering to cluster the constants into groups and then produce a lifted graph based on that.",
                    "label": 0
                },
                {
                    "sent": "So he has this nice.",
                    "label": 0
                },
                {
                    "sent": "Figure in his talk where he has a set of people here and they have these relationships to classes like person can teach a class or a student can be ATA for a class or some professors can advise other students and the first clusters all the constants in the domain to get this clustered graph over here where all the teachers are here.",
                    "label": 0
                },
                {
                    "sent": "All the students end up in this cluster and all the classes end up in that cluster and you get this set of relations that usually teachers advise graduate students graduate students.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, got this backwards graduate students.",
                    "label": 0
                },
                {
                    "sent": "A classic, I mean faculty teach classes, graduate students, TA classes and grain faculty advised graduate students, so he first does this lifting, and then he does relational pathfinding on this lifted.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Graph so it has three basic steps.",
                    "label": 1
                },
                {
                    "sent": "The first algorithm for lifting the hypergraph biclustering the constants.",
                    "label": 0
                },
                {
                    "sent": "Then it finds a set of paths in that listed graph.",
                    "label": 0
                },
                {
                    "sent": "And just like I showed you for relational pathfinding, it learns clauses from those paths and then test them on the data and adds the good ones to the final MLN.",
                    "label": 1
                },
                {
                    "sent": "So it's quite.",
                    "label": 0
                },
                {
                    "sent": "It's actually a fairly elegant and simple.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Overall algorithm, and it gets pretty good results with it.",
                    "label": 0
                },
                {
                    "sent": "I just in addition to some of the datasets already talked about.",
                    "label": 0
                },
                {
                    "sent": "This is a data set that also was talked about in one of the morning sessions.",
                    "label": 0
                },
                {
                    "sent": "Is Cora data set where you try to to solve the duping problem in computer in computer science, literature, citations and it builds a pretty.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty large model, so here are some graphs on on his technique comparing it to the previous top down structure learner that he built an plus.",
                    "label": 0
                },
                {
                    "sent": "Our bottom up structure learner and these are an area under the PR curve for different domains, IMDb and UW and Cora and shows that his algorithm does better than the current state of the art, particularly on this.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For a domain, and that's the conditional log likelihood there he wins.",
                    "label": 0
                },
                {
                    "sent": "He beats us on bustle just a little bit on on those two domains, but again does very well on coral.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "R. Another nice thing that he does in his paper as he doesn't ablation test that shows what if I use the lifted graph, but I just generate clauses without using the relational pathfinding clue as a bottom up clue.",
                    "label": 0
                },
                {
                    "sent": "How much does that relational pathfinding heuristic help me and shows that it helps quite a quite a bit, but if you take out the relational path finding that the performance goes down quite dramatically on all of the at least these two.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's my quick summary of some of the recent work in bottom up structure learning, and why I think it's a good approach to learn structures, particularly for for SRL models like micro Markov logic networks, and I think, But I think it's not as exploited as it as it should be.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm going to transit to the second topic.",
                    "label": 0
                },
                {
                    "sent": "I wanted to talk about.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just transfer learning.",
                    "label": 0
                },
                {
                    "sent": "So if you haven't heard about transfer learning, the idea is that most machine learning methods approach each new problem.",
                    "label": 1
                },
                {
                    "sent": "You know with a blank slate from scratch, and they don't utilize what they've learned in previous domains to help learn in a new domain.",
                    "label": 0
                },
                {
                    "sent": "And the whole idea of transfer learning is to use knowledge acquired in some previous so called source task to facilitate and improve learning in a related new target task.",
                    "label": 1
                },
                {
                    "sent": "And of course for this to work, there has to be some sort of similarity.",
                    "label": 0
                },
                {
                    "sent": "But as we'll see, that similarity can be extremely abstract and you can transfer between.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Different things.",
                    "label": 0
                },
                {
                    "sent": "So normally you assume there's a significant amount of training data that's available in the source, but you have a new domain, the target domain where you have very limited data, so you really need to exploit some other source of bias to be able to learn from such little amount of data with particular learning.",
                    "label": 0
                },
                {
                    "sent": "Such a rich structure like you would in a relational model.",
                    "label": 0
                },
                {
                    "sent": "So by exploiting knowledge from the source ideas to learning in the target can be more accurate by using the bias from the source domain knowledge you learn will be more accurate, and also because you already have a pretty good hypothesis from the target from the source domain.",
                    "label": 1
                },
                {
                    "sent": "That training should time should all go, so to be dramatically reduced.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, the general goal on transfer learning is you plot the learning curve and if we just learn from scratch where we don't do any transfer, we get a learning curve from that.",
                    "label": 0
                },
                {
                    "sent": "But if we start out with knowledge we learned in the source domain, hopefully we get a much boosted learning curve by transferring knowledge from the source domain.",
                    "label": 0
                },
                {
                    "sent": "So sometimes people call this initial difference that you get given no training data by just mapping the knowledge directly to the target problem they signs.",
                    "label": 0
                },
                {
                    "sent": "That's called Jumpstart, but also you're interested in, particularly when there's a limited amount of training data in the target domain that you can get quite dramatic improvements in accuracy by exploiting knowledge from the source.",
                    "label": 1
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's been quite a bit of recent work in transfer learning.",
                    "label": 1
                },
                {
                    "sent": "Part of that is due to the fact that the US DARPA agency ran a program in transfer learning, which both Pedro and I were actually involved in.",
                    "label": 0
                },
                {
                    "sent": "It just ended recently.",
                    "label": 0
                },
                {
                    "sent": "In fact, I think my contract expired yesterday or the day before.",
                    "label": 0
                },
                {
                    "sent": "So, but most of that work is focused on feature vector classification.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go through.",
                    "label": 1
                },
                {
                    "sent": "There's a pretty large literature now on using transfer learning.",
                    "label": 1
                },
                {
                    "sent": "There's also a growing body of literature using transfer learning or reinforcement for reinforcement learning, but.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's not a lot on using transfer learning for the type of problems that you and I in this this room are interested in these relational sorts.",
                    "label": 0
                },
                {
                    "sent": "Sorts of problems.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the other interesting things when you combine the transfer learning with statistical relational learning, one of the things that I found it, I think it's sort of interesting is in standard machine learning.",
                    "label": 0
                },
                {
                    "sent": "You make the so the famous IID assumption right that examples are independent and identically distributed.",
                    "label": 1
                },
                {
                    "sent": "Well, when you do so just to relational learning, as we all know, the independence assumption is broken because your examples that you're trying to classify or not independent, you have to do this task of what I think Daphne originally called Collective Classification, which is one of the important tasks at this Community, has looked at in addition.",
                    "label": 0
                },
                {
                    "sent": "When you do transfer learning, you're breaking the other part of the IID assumption, because the identicality assumption goes away because the test examples are not distributed the same way, the training examples might not even be represented in the same language.",
                    "label": 0
                },
                {
                    "sent": "So when you combine transfer learning in SRL, I think it's sort of interesting 'cause you're now breaking both of the eyes in the IID.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we've done a certain amount of work and over the last few years on transferring for Markov logic networks.",
                    "label": 0
                },
                {
                    "sent": "Originally a lot of this is about present was presented in a paper triple AI by Lilly to get myself back in a couple of years ago and the idea is you start out with one domain where you have a fair bit of data.",
                    "label": 0
                },
                {
                    "sent": "The UW CSE domain where we have all sorts of knowledge about professors and students in publications and what by who advised to.",
                    "label": 0
                },
                {
                    "sent": "And we just start out with a little bit of data in the movie domain.",
                    "label": 0
                },
                {
                    "sent": "And we want to transfer a Markov logic network learned here in the source to this target domain.",
                    "label": 1
                },
                {
                    "sent": "But the problem is the predicates are completely different.",
                    "label": 0
                },
                {
                    "sent": "The representation is completely different, so the first thing we have to do is establish some sort of mapping here and we have to realize that professors are like directors and students are like actors in publications are like movies and like advising.",
                    "label": 0
                },
                {
                    "sent": "The student is sort of like a director advising an actor in a film, and there's actually a very nice analogy between these two domains, even though they're quite different and you can actually transfer a lot of the knowledge that you've learned in UW CSE, two IMDb.",
                    "label": 0
                },
                {
                    "sent": "As we'll see.",
                    "label": 0
                },
                {
                    "sent": "And that requires 2 steps.",
                    "label": 1
                },
                {
                    "sent": "First you have to map the source predicates to the target.",
                    "label": 0
                },
                {
                    "sent": "Realize what the correspondences are here, and then take that map knowledge and revise it.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we developed a system called Tamar for transfer via automatic mapping and revision.",
                    "label": 1
                },
                {
                    "sent": "Not one of my better acronyms, but it's OK.",
                    "label": 0
                },
                {
                    "sent": "He's like ones that make a real word.",
                    "label": 0
                },
                {
                    "sent": "So let's say we start with a clause in the UW CSE domain where this is a very common rule in that domain that has a very high predictive accuracy, which says that if some student has a paper and their advised by some advisor, then usually the advisor puts their name on the paper, to which we've all done.",
                    "label": 0
                },
                {
                    "sent": "It works great, multiplies your Vita quite dramatically.",
                    "label": 0
                },
                {
                    "sent": "So, so it's pretty.",
                    "label": 0
                },
                {
                    "sent": "It's a pretty good predictive rule that advisors put their name on their students papers, no matter if they contributed much the ideas or not.",
                    "label": 0
                },
                {
                    "sent": "And now we get this new domain in the movie domain and we're going to transfer that now is that we've learned from the academic domain to the movie domain.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we have to do is map the predicates, and that's in a box we call EM Tomar mapping part of transfer, and we have to establish a relationship between the predicates and realize the publication is like a movie and advising the student is like.",
                    "label": 0
                },
                {
                    "sent": "Is like a director.",
                    "label": 0
                },
                {
                    "sent": "Advising an actor or the work for predicate in that domain.",
                    "label": 0
                },
                {
                    "sent": "Now, once we've established that predicate mapping, we can map the predicate to get a clause in the new domain, which says if you're an actor in a movie and you've worked for some director in the past, and maybe he's the director of this movie, 'cause directors and actors do tend to establish a relationship just like students and faculty do.",
                    "label": 0
                },
                {
                    "sent": "But it's probably not as tight, right?",
                    "label": 0
                },
                {
                    "sent": "Because I actually I do that my students have gone off and done projects and other students for other professors, and they write papers with them, and I'm a little bit jealous, right?",
                    "label": 0
                },
                {
                    "sent": "You know, it's like you know they should be on that paper.",
                    "label": 0
                },
                {
                    "sent": "Is there my student?",
                    "label": 0
                },
                {
                    "sent": "You know, in the movie domain this rule doesn't maybe work quite as well because the bond between directors and actors is not as strong as that strong bond between an advisor and his student.",
                    "label": 0
                },
                {
                    "sent": "So maybe we have to revise this clause a little bit to work better in the movie domain.",
                    "label": 0
                },
                {
                    "sent": "So my favorite example of doing this is, say, well, if that if the director is a relative of the actor, then it's probably more likely that they'll be the director of the film.",
                    "label": 0
                },
                {
                    "sent": "I famous example.",
                    "label": 0
                },
                {
                    "sent": "My favorite example of this is Clint Howard.",
                    "label": 0
                },
                {
                    "sent": "Does anyone know who Clint Howard it so it's Ron Howard's brother?",
                    "label": 0
                },
                {
                    "sent": "He wasn't?",
                    "label": 0
                },
                {
                    "sent": "Child actor too, just like Ron was he starting this old show called called Gentle Ben.",
                    "label": 0
                },
                {
                    "sent": "But since it since he did TV work as a kid, he really hasn't gotten much acting jobs except when except when his brother is directing the movie.",
                    "label": 0
                },
                {
                    "sent": "He hires Clint for some bit part he was in Apollo 13 and all these other Ron Howard movies.",
                    "label": 0
                },
                {
                    "sent": "So this might be a reasonable cause if we add this extra condition that the director has to be related to the act.",
                    "label": 0
                },
                {
                    "sent": "So that's done by this other step called the revision step of trance.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so first I'm going to talk about the predicate mapping very briefly.",
                    "label": 0
                },
                {
                    "sent": "So how do we establish a predicate mapping where we're going to map each clause independently, so each clause we've learned in the source domain we're going to take and try to find a mapping for that clause in isolation, and it does a pretty brute force technique as we found out that the space of possible consistent possible mappings is really not that large as long as my set of predicates in their arity is not too large, so each predicate is mapped to some target predicate, and we try all possible so called type consistent ways where we can map we're assuming.",
                    "label": 1
                },
                {
                    "sent": "Every MLN has types on its arguments, and you have to respect those typing constraints.",
                    "label": 0
                },
                {
                    "sent": "And once you do that, it actually constrains the set of mappings pretty much.",
                    "label": 1
                },
                {
                    "sent": "And then we just evaluate each of the possible map clauses to see how well it fits the target data in the training data for the target domain, and we pick that mapping that results in the best accuracy.",
                    "label": 0
                },
                {
                    "sent": "So we just empirically evaluate you should the possible mappings and pick the best one.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, I had this example.",
                    "label": 0
                },
                {
                    "sent": "I might come up with this mapping.",
                    "label": 0
                },
                {
                    "sent": "That might be one of the many mappings I might try is well, publication is like movie and advising a person is like working for them and that establishes a consistent type mapping because all the titles map to names, titles of publication, map to names of movies, and actually a person Maps to a person.",
                    "label": 0
                },
                {
                    "sent": "That's an easy type mapping, so that's one type consistent mapping.",
                    "label": 1
                },
                {
                    "sent": "You try all these sort of tight, consistent mapping to evaluate them on the training data and pick the best one and it's sort of a brute force algorithm that we found in practice.",
                    "label": 0
                },
                {
                    "sent": "It actually worked.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, so we then pick the best.",
                    "label": 0
                },
                {
                    "sent": "We transfer the clause based.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On that mapping.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a quick review of predicate mapping.",
                    "label": 0
                },
                {
                    "sent": "Then we get to the revision part.",
                    "label": 0
                },
                {
                    "sent": "So once we've mapped the clauses, they don't work exactly.",
                    "label": 0
                },
                {
                    "sent": "Know the two domains are related, but they're not exactly isomorphic, so we're going to have to change some of the clauses to get them to be more accurate.",
                    "label": 0
                },
                {
                    "sent": "So we view.",
                    "label": 0
                },
                {
                    "sent": "This is a theory revision problem, so one of the reasons, actually the reason why I even got into this whole field of inductive logic programming I was interested in theory refinement.",
                    "label": 0
                },
                {
                    "sent": "I did a lot of work in propositional data, and I got into interested in in doing it in first order logic about the same time that the IOP community started up, and I.",
                    "label": 0
                },
                {
                    "sent": "One of my first IOP systems I worked on with my student, Brad Richards was this system called Forte.",
                    "label": 0
                },
                {
                    "sent": "That's a nice acronym.",
                    "label": 0
                },
                {
                    "sent": "First daughter.",
                    "label": 0
                },
                {
                    "sent": "Revision of theories from examples.",
                    "label": 0
                },
                {
                    "sent": "That's a better acronym.",
                    "label": 0
                },
                {
                    "sent": "And we basically view the transfer problem very much as an instance of this revision.",
                    "label": 0
                },
                {
                    "sent": "Sorts of problem where we have an initial theory and we need to now revise it to fit this new set of data.",
                    "label": 0
                },
                {
                    "sent": "And of course being the bottom up type of guy I am.",
                    "label": 0
                },
                {
                    "sent": "I do that revision not based on a simple top down approach right?",
                    "label": 0
                },
                {
                    "sent": "Try all possible revisions to the source domain 'cause that leads to a huge branching factor, local minimum, all those bad problems we do more of a bottom up.",
                    "label": 0
                },
                {
                    "sent": "Approach where you use the data to propose potentially good revisions to the theory and then just test a very small number of revisions.",
                    "label": 0
                },
                {
                    "sent": "So again it uses this idea of using more bots.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OP directed search.",
                    "label": 0
                },
                {
                    "sent": "So it looks something like this.",
                    "label": 0
                },
                {
                    "sent": "You have a set of clauses from the source domain and you have some.",
                    "label": 0
                },
                {
                    "sent": "You look at the data for the target domain you run, the inferences there and see how well they work and you see where clauses make good predictions and where the existing clauses make bad predictions.",
                    "label": 0
                },
                {
                    "sent": "And based on that you evaluate each of the clauses and you decide whether that clause is too long, too short, or whether it looks like it's pretty good.",
                    "label": 1
                },
                {
                    "sent": "Which really means does it need to have extra literals added to this disjunction literals removed from the disjunction, and you can bake those decisions pretty clearly by seeing how well the initial clauses perform on the data.",
                    "label": 1
                },
                {
                    "sent": "And then once you have that, you do a directed beam search through the possible ways of long, long lengthening the ones that need lengthening and shortening the ones that need shortened.",
                    "label": 0
                },
                {
                    "sent": "And then you create various versions of those.",
                    "label": 0
                },
                {
                    "sent": "Again, test them on the data course, and keep some of the best revisions.",
                    "label": 0
                },
                {
                    "sent": "And then you also need the technique to bring in new rules, right?",
                    "label": 0
                },
                {
                    "sent": "'cause maybe there's some rules that don't don't have any analog in the source domain, so you need some technique for just cuts covering new clauses.",
                    "label": 0
                },
                {
                    "sent": "You put those in, then you do some weight learning on those.",
                    "label": 1
                },
                {
                    "sent": "And then you pick out the ones that do test them on the training data and you see how well they fit the training data and you keep the clauses that fit the training data the most and put those into the final MLN.",
                    "label": 0
                },
                {
                    "sent": "So I don't have time to go into all the details of how self diagnosis works.",
                    "label": 0
                },
                {
                    "sent": "You can look at the paper.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For those details, WIP.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing about this bottom up sort of approach to revision is it's very directed.",
                    "label": 0
                },
                {
                    "sent": "It's not just trying all possible changes.",
                    "label": 0
                },
                {
                    "sent": "Literal deletions are only attempted on clauses marked for shortening in literal additions are only attempted on clauses marked for lengthening based on the diagnosis of how those clauses performed in the new domain, and training data is much faster since the search is constrained by limiting the clauses that are considered for any possible update and then restricting the type of updates that are allowed.",
                    "label": 1
                },
                {
                    "sent": "So as we'll see in the empirical results, it also runs a lot faster than a top down approach.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our new claws discovery.",
                    "label": 0
                },
                {
                    "sent": "We basically just use a variant of the relational pathfinding algorithm from 92 that I mentioned before.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then of course, at the end, once we've gone through, we've mapped the predicates.",
                    "label": 0
                },
                {
                    "sent": "We revised the clause by adding constraints to rules, removing them, adding new rules.",
                    "label": 0
                },
                {
                    "sent": "We go ahead and then do a final step of weight learning on those to learn a final set of weight weighted clauses for MLN.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so some experiment.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's in this domain.",
                    "label": 0
                },
                {
                    "sent": "We compare the number of systems the full system we took and you can take this top down algorithm of cocking Domingo's from 2005 and either learn to use it to learn a theory completely from scratch or you can use it as a top down theory refinement algorithm by just using top down search to refine that theory to fit the training.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another thing we tested was revising a hand built knowledge base, so so again the UW people put together this nice hand built knowledge base for this UW CSE domain.",
                    "label": 0
                },
                {
                    "sent": "And when you use that as a source domain, you can transfer it.",
                    "label": 1
                },
                {
                    "sent": "Sort of a form of old style of theory, refinement.",
                    "label": 1
                },
                {
                    "sent": "Except the predicates are different, so it's sort of a form of mapped.",
                    "label": 0
                },
                {
                    "sent": "I have to map the predicates, the new domain and then do theory refinement.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are to summarize all the results in our experiments that we tried.",
                    "label": 1
                },
                {
                    "sent": "We predict we compute this thing called transfer ratio, which this DARPA program came up with, which is the area under the learning curve for the transfer.",
                    "label": 1
                },
                {
                    "sent": "Learning over the area under the learning curve for the order from scratch.",
                    "label": 0
                },
                {
                    "sent": "The whole idea is transfer learning boot.",
                    "label": 0
                },
                {
                    "sent": "You know, this me up lifts up the learning curve and so the area, the ratio of the areas under those two curves is a good measure of how much we've improved things.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we tried different types of transfer from different of these three domains that I talked of each other to one of the others.",
                    "label": 0
                },
                {
                    "sent": "We never used web KB as a target domain since the mega examples there are pretty large and you can pretty much learn a good theory for that more simplified domain just by learning from the training data.",
                    "label": 1
                },
                {
                    "sent": "But we tried various other ways of mapping from 1.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Main to another, and these are all the different domains, so here I'm first showing you the transfer ratio.",
                    "label": 0
                },
                {
                    "sent": "If I measure it in terms of area under the precision recall curve and so above 1 here means I have positive transfer.",
                    "label": 0
                },
                {
                    "sent": "In other words and look at learning knowledge from the source.",
                    "label": 0
                },
                {
                    "sent": "Helps me learn in the target domain and then this compares a top down theory refinement algorithm to a bottom up one that Amar algorithm.",
                    "label": 0
                },
                {
                    "sent": "So on AUC on this.",
                    "label": 0
                },
                {
                    "sent": "To these problems we don't do too much different.",
                    "label": 0
                },
                {
                    "sent": "Most all the domains get positive transfer, except this last one of IMDb to UW CSE.",
                    "label": 1
                },
                {
                    "sent": "That's mainly because we have so little data in IMDb that it doesn't learn a very good theory that it can transfer.",
                    "label": 0
                },
                {
                    "sent": "It works better the other way, as you can see.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over here.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the conditional log likelihood the other way of measuring accuracy we do consistently better than the top down algorithm in terms of accuracy.",
                    "label": 0
                },
                {
                    "sent": "But the real win in this domain is in time, as we'll see in a.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But this is just a sample learning curve where we show the number of mega examples in the target domain and then these.",
                    "label": 1
                },
                {
                    "sent": "Is horizontal lines or what?",
                    "label": 0
                },
                {
                    "sent": "If we come up with a mapping by hand so we don't do the algorithm for predicting the best mapping.",
                    "label": 0
                },
                {
                    "sent": "We actually say OK. Professors are like directors and actors are like students and actually notice that.",
                    "label": 0
                },
                {
                    "sent": "So the bottom the bottom most curve here, the red curve is learning from scratch using the top down algorithm.",
                    "label": 0
                },
                {
                    "sent": "And if we use transfer with the top down algorithm with hand mapping we get a little bit better if we do transfer with the top down algorithm using an automated mapping, we do even better this.",
                    "label": 0
                },
                {
                    "sent": "Aqua Blue line and then if we use the full Tamar system, of course we get the best best results.",
                    "label": 0
                },
                {
                    "sent": "Another interesting point here is the automated mapping does much better than the manual mapping.",
                    "label": 0
                },
                {
                    "sent": "You might think that you can you as a human can establish a good analogy and map the predicates properly, but the algorithm actually comes up with better mappings than the human expert.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's the the training time.",
                    "label": 0
                },
                {
                    "sent": "The training times dramatically reduced one of these is learning from scratch, so this this blue is learning from scratch with the top down algorithm, maroon is revising using the top down algorithm, and then this very barely visible white line is the training time for the full tomorrow algorithm, which uses bottom up search to really direct the search.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A much better way.",
                    "label": 0
                },
                {
                    "sent": "OK, so I just want to briefly talk about a couple of 1 extension to this work that will be talked about it inch by this summer so Lily will give a talk in a couple of weeks on this work where we did an extension of Tamar to learn with extremely little target data.",
                    "label": 0
                },
                {
                    "sent": "It uses just very minimal target data to determine a good predicate mapping from the source less than a single mega example.",
                    "label": 1
                },
                {
                    "sent": "In fact, we're just going to.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But information about a single entity.",
                    "label": 0
                },
                {
                    "sent": "So normally we trained it on one full mega example, which is a whole bunch of entities richly connected with each other.",
                    "label": 0
                },
                {
                    "sent": "But let's say we have just very little training data in the target, so I just give you one person Bob and I tell you all the people he's related to an all the objects that is related to and his properties, but I only give you information about a single individual.",
                    "label": 0
                },
                {
                    "sent": "Can I actually transfer with such little limited?",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Target data solely developed a slightly alternate mapping algorithm which she called Sr to LR 'cause it takes this idea of mapping from the mapping from the short ranges to the long range clauses and the idea is we can.",
                    "label": 1
                },
                {
                    "sent": "We can only really evaluate on this limited target data the clauses that talk about a single object, but we have other clauses that really talk about the relationships between multiple entities and we can't really evaluate those.",
                    "label": 0
                },
                {
                    "sent": "So the idea is we can actually establish a mapping based on the chartrain clauses.",
                    "label": 0
                },
                {
                    "sent": "And use that to infer a mapping for the long range clauses and actually.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get pretty good results, so we showed was given just a single entity of data in the target domain.",
                    "label": 1
                },
                {
                    "sent": "The maroon here is that tomorrow if we use our existing mapping algorithm, we get some good results, but not as good as if we use this new enhanced our that specifically intended.",
                    "label": 0
                },
                {
                    "sent": "When you have very limited target date.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last transfer in MLM thing I wanted to talk about was work that Jesse Davis presented at ICM L just a couple of weeks ago on deep transfer with second order Markov logic networks, which tries to transfer very abstract patterns between very desperate domains by learning patterns in a second order logic that verbalize.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For predicate, so let's not start going backwards again.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to generalize to very different domains, and so one of the examples that Jesse gives in his paper is transferring from a web page domain where you have web pages linked to other web pages, transferring knowledge you learned in that domain to something completely different as Monty Python would say, where you have a protein interaction network now is there much you can actually transfer between the structure of the web and the structure of interacting proteins?",
                    "label": 1
                },
                {
                    "sent": "Well, it turns out there is some things you can transfer.",
                    "label": 0
                },
                {
                    "sent": "R.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what they do is they call this DTM deep transfer with Markov logic.",
                    "label": 0
                },
                {
                    "sent": "They come up with second order patterns and they abstract away the predicate names and discern very high level structural regularity's and then they search through the space of the 2nd order formula to find ones that fit the data well and then they use those as a bias when they learn in the top.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I demand.",
                    "label": 0
                },
                {
                    "sent": "And they tested on a number of different domains of protein interaction data set a version of web KB I should mention here that this version actually does use the information from the text.",
                    "label": 0
                },
                {
                    "sent": "Anna version of a social network based on face.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fuck, I'm actually show that you can learn some pretty abstract concepts that generalize between these very disparate domains.",
                    "label": 0
                },
                {
                    "sent": "So there's this general concept that's known in the Network World called him awfully, which says if you have two entities and entity one is related to Entity 2, then a lot of times they share a property so it learns this very general pattern.",
                    "label": 0
                },
                {
                    "sent": "Another very general pattern that learns is transitivity, which says if you have three entities and entity one is related to entity two and ending two is related in maybe 3, then frequently entity three will be related to entity want.",
                    "label": 0
                },
                {
                    "sent": "And then Cemetery.",
                    "label": 0
                },
                {
                    "sent": "Also, which of course says that if we have two entities, if ones related to the other than the others probably related to the one.",
                    "label": 0
                },
                {
                    "sent": "So it learns very abstract patterns and then Jesse presents.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experiment showing that by using those abstract patterns to transfer knowledge from 2 very different between two very different domains, you can actually get improved results.",
                    "label": 0
                },
                {
                    "sent": "So these are learning curves in the area under the PR curve where the bottom line here is just the standard top down learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "But if we transfer knowledge properly these are different techniques.",
                    "label": 0
                },
                {
                    "sent": "I don't have time to go into that he talks about, but with the best technique you get quite dramatically better results and this is transferring from web to yeast example that I was in.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just earlier and he also gets good results transferring from Facebook Domain to the web KB domain.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm starting to run out of time so just to talk a little bit about what I think are some future research issues along these lines.",
                    "label": 1
                },
                {
                    "sent": "Obviously there's a standard one of trying to apply these techniques to bigger and more realistic application domains.",
                    "label": 1
                },
                {
                    "sent": "Notice that when I talked about the transfer learners I always compared them to top down structure learners.",
                    "label": 0
                },
                {
                    "sent": "If you compare him to bottom up structure learners they don't work as well.",
                    "label": 0
                },
                {
                    "sent": "And what we really need is more bottom up structure learning algorithms that use transfer.",
                    "label": 0
                },
                {
                    "sent": "R. Also, like I said, a lot of the ideas I've presented here, I don't think are specific to M lens we've been working with them, and so we've tested him out there first, but I think a lot of these ideas of bottom up search and transfer you could apply to other your favorite other SRL model, your favorite TLA.",
                    "label": 1
                },
                {
                    "sent": "And our predicate mapping currently for transfer is quite limited.",
                    "label": 0
                },
                {
                    "sent": "We would like to try to be able to do other things.",
                    "label": 0
                },
                {
                    "sent": "It doesn't allow reordering.",
                    "label": 1
                },
                {
                    "sent": "The arguments are the arity of predicates to change.",
                    "label": 0
                },
                {
                    "sent": "You might want to be able to map one predicate to injunction of greater than one.",
                    "label": 1
                },
                {
                    "sent": "So say I didn't actually have the work for predicate in the movie domain, but I do have that.",
                    "label": 0
                },
                {
                    "sent": "Actors are in movies and directors are in movies.",
                    "label": 0
                },
                {
                    "sent": "Well, I might find that a student advising being advised by a professor is like an actor working in.",
                    "label": 0
                },
                {
                    "sent": "That in a movie and then that movie having a director so I have to actually map a unary predicate to a conjunction of predicates in the other domain.",
                    "label": 0
                },
                {
                    "sent": "You might view this as transfer motivated predicate invention.",
                    "label": 0
                },
                {
                    "sent": "So this is something that that was been on our To Do List for awhile, but Lily wants to graduate this summer, so I don't think we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A couple other things I think are important on the transfer side.",
                    "label": 0
                },
                {
                    "sent": "One is multiple source transfers.",
                    "label": 0
                },
                {
                    "sent": "So far we've looked at just transferring from a single domain to a target source domain to a target domain.",
                    "label": 0
                },
                {
                    "sent": "It would be nice to transfer from multiple source problems, determine which clauses to map from which previous domain, and revise them, and use multiple source domains when you're learning in it.",
                    "label": 1
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Target domain, another obvious problem is what sometimes called source selection and transfer learning, which is I in all these cases I told it.",
                    "label": 0
                },
                {
                    "sent": "Start with.",
                    "label": 0
                },
                {
                    "sent": "You know UW CSE and transfer to IMDb.",
                    "label": 0
                },
                {
                    "sent": "But in the real world I have tons of previous domains out there and I don't necessarily know which ones are going to be the best ones that transfer from it would be nice to have an automated algorithm that hopefully in time that's even sub linear in the number of domains I've previously experienced find the best ones is to use service sources for transfer in a new.",
                    "label": 1
                },
                {
                    "sent": "In a new problem.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so to wrap up and leave some time for questions, I've tried to talk today about what I think are two important ways to improve structure learning for statistical relational models such as Markov logic networks particularly, we talked about the value of doing bottom up search and we talked about three different systems that showed the advantage of doing bottom up search and learning structures for Markov logic networks bustle.",
                    "label": 1
                },
                {
                    "sent": "This olive combined with MLN's approach an this LHL technique.",
                    "label": 0
                },
                {
                    "sent": "We also talked about transfer.",
                    "label": 0
                },
                {
                    "sent": "Learning and three different systems.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow, Sartell are in the 2nd order MLN's and how they can be used to transfer knowledge from one domain to another.",
                    "label": 0
                },
                {
                    "sent": "When you're learning complicated relational structures and I've tried to show that both of these techniques improve both the speed of training because we're doing a more effective, efficient search of the hypothesis space and the accuracy of the learn models because we're imposing a good bias on that space to prefer a good set of hypothesis by using clues bottom up from the data.",
                    "label": 1
                },
                {
                    "sent": "The other point I wanted to try to bring home is, is that.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of work in when I'm here calling classical IOP that I think a lot of the SRL literature doesn't effectively utilized, and there are a lot of ideas there that I think can be very effective in improving the state of the art in statistical release.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Personal learning OK thanks.",
                    "label": 0
                },
                {
                    "sent": "OK, I think I left plenty of time for questions.",
                    "label": 0
                },
                {
                    "sent": "Yes, Steve.",
                    "label": 0
                },
                {
                    "sent": "Lots of interesting examples in which Constellation data positive effect on the learning curve.",
                    "label": 0
                },
                {
                    "sent": "It's quite easy to imagine that the officer could come true.",
                    "label": 0
                },
                {
                    "sent": "Secondly, I guess that was gonna happen.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the first one is yes, we actually have some negative results.",
                    "label": 0
                },
                {
                    "sent": "You can actually read the edge type paper 'cause there are some unfortunately negative results in there where transfer actually hurt at least a little bit, right?",
                    "label": 0
                },
                {
                    "sent": "Lonely and so yes, we do have experience with negative transfer even between these three domains when you're given very limited data.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so that's the source selection problem, which I think is within transfer learning as a whole, not just whole, not just in transfer.",
                    "label": 0
                },
                {
                    "sent": "Learning for S 1st.",
                    "label": 0
                },
                {
                    "sent": "Relation learning, but I think transfer learning in whole this whole problem of there's been a few pieces of work that try to cluster domains where you actually cluster problems and then if 2 problems end up in the same cluster then you think you can transfer between them.",
                    "label": 0
                },
                {
                    "sent": "Been some nice work on that, but not not a lot so I don't know if I have a lot of ideas to say about the source selection other than I agree with you.",
                    "label": 0
                },
                {
                    "sent": "It's a very important problem here.",
                    "label": 0
                },
                {
                    "sent": "You're relying on human ingenuity to tell you what are good problems to transfer from, and we of course would like to automate that process.",
                    "label": 0
                },
                {
                    "sent": "So that payments for learning slideshow from from Jesse and pictures the first one I can remember seeing where the asymptotes were different, is that because there's a concept that's really important for used protein that families here?",
                    "label": 0
                },
                {
                    "sent": "Maybe we might have a better answer which which one are you?",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talking about, yeah I know Stanley where is he?",
                    "label": 0
                },
                {
                    "sent": "Concept that's that's critical for the target task.",
                    "label": 0
                },
                {
                    "sent": "That can really only be learned from the source.",
                    "label": 0
                },
                {
                    "sent": "I guess that's more question for Pedro or Stanley does.",
                    "label": 0
                },
                {
                    "sent": "Research.",
                    "label": 0
                },
                {
                    "sent": "Thanks for learning.",
                    "label": 0
                },
                {
                    "sent": "Enables escape local office.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so actually the question I asked Stanley and his talk a couple of weeks ago was have you compared this to your new?",
                    "label": 0
                },
                {
                    "sent": "I'm sorry this is Jesse's where I'm screwing.",
                    "label": 0
                },
                {
                    "sent": "He's not here.",
                    "label": 0
                },
                {
                    "sent": "Sorry getting confused.",
                    "label": 0
                },
                {
                    "sent": "I'm jet lagged so I asked him.",
                    "label": 0
                },
                {
                    "sent": "I asked Jesse why didn't you you compare to Stanley's new algorithm instead of Stanley's old algorithm.",
                    "label": 0
                },
                {
                    "sent": "And he said, Oh yeah, I'd like to do that someday.",
                    "label": 0
                },
                {
                    "sent": "Because I actually am, because that is less subject to these local minimum problems that causes.",
                    "label": 0
                },
                {
                    "sent": "So I think this is a search what Pedro saying and I agree with him.",
                    "label": 0
                },
                {
                    "sent": "It's a search problem.",
                    "label": 0
                },
                {
                    "sent": "It's not a fundamental transfer problem.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry there was something up here.",
                    "label": 0
                },
                {
                    "sent": "First, ask when you're learning of the model for the source mascot.",
                    "label": 0
                },
                {
                    "sent": "Imagine if you're if you know you're going to be transferring it to a new target would learn very differently, or you want to learn very differently than if you just.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's very good.",
                    "label": 0
                },
                {
                    "sent": "I'm looking at my student Lily 'cause her tonight.",
                    "label": 0
                },
                {
                    "sent": "She and I've talked about exactly this problem is interesting question.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the best thing to learn to transfer is not the best thing to learn.",
                    "label": 0
                },
                {
                    "sent": "If you want to learn the source, we actually have some nice results.",
                    "label": 0
                },
                {
                    "sent": "Actually, they're not as good as we thought they were, lilies ran some more experiments lately where we have a different version of muscle that learns a different theory that actually does less well in the source domain to start.",
                    "label": 0
                },
                {
                    "sent": "But when you transfer from it, it actually does better.",
                    "label": 0
                },
                {
                    "sent": "So we've actually seen that happen on occasion that it's better to learn a model in the source that's not as good in the source.",
                    "label": 0
                },
                {
                    "sent": "But might be better for transferring to problems later on, so I think that's a very interesting point and we have just very little information.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you have anything to say about that.",
                    "label": 0
                },
                {
                    "sent": "More general, right?",
                    "label": 0
                },
                {
                    "sent": "So I so agree with you and I don't think that's been looked at that much that what you want to learn in the source actually depends.",
                    "label": 0
                },
                {
                    "sent": "It actually turns it more, and if you're familiar with the term multi task learning 'cause usually transfer you assume you learn in the source just to do well in the source and then later someone pops this new domain on you multitask.",
                    "label": 0
                },
                {
                    "sent": "Assumes you know all the domains up front and you know you're going to have to learn to solve all of them.",
                    "label": 0
                },
                {
                    "sent": "And sometimes I think this deep transfer is actually more of a multi task type of technique rather than a quote transfer.",
                    "label": 0
                },
                {
                    "sent": "But those two things are very close to each other.",
                    "label": 0
                },
                {
                    "sent": "But it's a good point.",
                    "label": 0
                },
                {
                    "sent": "The transfer is running on different where you can imagine multi task with different representations for each task right?",
                    "label": 0
                },
                {
                    "sent": "I mean it's just a different variation of Mount Multitask.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "How is it representation of it work?",
                    "label": 0
                },
                {
                    "sent": "How much force background on the Turnpike background knowledge?",
                    "label": 0
                },
                {
                    "sent": "Or did you use only?",
                    "label": 0
                },
                {
                    "sent": "Mapping you're saying?",
                    "label": 0
                },
                {
                    "sent": "Yeah, no?",
                    "label": 0
                },
                {
                    "sent": "So there's no domain knowledge used during mapping, and that's not because I don't think that might be useful.",
                    "label": 0
                },
                {
                    "sent": "I think it very well might be useful.",
                    "label": 0
                },
                {
                    "sent": "I mean, it doesn't actually know that you know mapping the person type to a person type is actually a good thing.",
                    "label": 0
                },
                {
                    "sent": "It just knows that when it does that, it gets good results in the target domain.",
                    "label": 0
                },
                {
                    "sent": "So I think exploiting that sort of information even using the lexical items and maybe using lexical knowledge like word.",
                    "label": 0
                },
                {
                    "sent": "Net to know that mapping of person type to another person type like mapping a director to a professor makes sense because there at least both people but mapping out.",
                    "label": 0
                },
                {
                    "sent": "Professor to a movie probably doesn't make a lot of sense, so I think there is lots of room for using lexical knowledge and domain knowledge to help.",
                    "label": 0
                },
                {
                    "sent": "We actually tried this right tunes.",
                    "label": 0
                },
                {
                    "sent": "We tried this and we couldn't get it to work right.",
                    "label": 0
                },
                {
                    "sent": "So there's a distinction between domain knowledge and mapping knowledge.",
                    "label": 0
                },
                {
                    "sent": "We should make.",
                    "label": 0
                },
                {
                    "sent": "I think that distinction I was talking more about maybe or closing the source domain and then on close is a target for me.",
                    "label": 0
                },
                {
                    "sent": "And then you can just try if they look similar to each other under some mapping.",
                    "label": 0
                },
                {
                    "sent": "So I think my question is whether you use branch of knowledge in the source and targets or within use of 1st order.",
                    "label": 0
                },
                {
                    "sent": "Yes, I think you're gonna answer part because we use abstract knowledge in the source, but we never use abstract knowledge in the target, we just use the data in the target.",
                    "label": 0
                },
                {
                    "sent": "You could imagine learning clauses in the target and in the source and establishing the mapping from clause to clause rather than.",
                    "label": 0
                },
                {
                    "sent": "Mapping the clauses and testing it on the target date.",
                    "label": 0
                },
                {
                    "sent": "Again, these are ideas that I've talked about with my students on various occasions, but we haven't tried anything like that.",
                    "label": 0
                },
                {
                    "sent": "I think there are other ways to do mapping.",
                    "label": 0
                },
                {
                    "sent": "I mean like I said, the way we do mapping is pretty pretty primitive.",
                    "label": 0
                },
                {
                    "sent": "We just found that it actually works pretty well and so we actually focused on other problems.",
                    "label": 0
                },
                {
                    "sent": "Steve had another question.",
                    "label": 0
                },
                {
                    "sent": "Logical reasoning, sure.",
                    "label": 0
                },
                {
                    "sent": "What's the difference between this is one of the ones where you pay the guy for having the question.",
                    "label": 0
                },
                {
                    "sent": "So here's the slide on for that question, which is.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of work in analogical reasoning, particularly done by actually people who were at the universal.",
                    "label": 0
                },
                {
                    "sent": "And when I was there, so I took all their classes.",
                    "label": 0
                },
                {
                    "sent": "I know them quite well.",
                    "label": 0
                },
                {
                    "sent": "I think, David, you know these folks to buy Brian Falcon Heiner, and conformist Dedre Gentner.",
                    "label": 0
                },
                {
                    "sent": "So they have various ways of mapping things so that.",
                    "label": 0
                },
                {
                    "sent": "Key thing is, is they always base their conditions purely on sort of structural analogy.",
                    "label": 0
                },
                {
                    "sent": "They never look at empirical adequacy of the knowledge that's map.",
                    "label": 0
                },
                {
                    "sent": "So the key difference is they never consider the accuracy of the map knowledge in the target domain to prefer a mapping.",
                    "label": 0
                },
                {
                    "sent": "They just say this has goods.",
                    "label": 0
                },
                {
                    "sent": "All Systematis city is just a structural characteristic can pick the best mapping.",
                    "label": 0
                },
                {
                    "sent": "We pick the best mapping by trying different mappings, seeing how well the map knowledge actually performs and making good predictions in the target domain, and that I think is a key difference between this approach and the traditional work that's been done in analogical reasoning in AI.",
                    "label": 0
                },
                {
                    "sent": "It's a very good question, which is why I have a backup slide.",
                    "label": 0
                },
                {
                    "sent": "There's no induction then running out early.",
                    "label": 0
                },
                {
                    "sent": "There's no revision, basically.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's just mapping and that's it.",
                    "label": 0
                },
                {
                    "sent": "They don't have the revision step at all.",
                    "label": 0
                },
                {
                    "sent": "So I think that's another.",
                    "label": 0
                },
                {
                    "sent": "That's another difference.",
                    "label": 0
                },
                {
                    "sent": "Thanks Pedro.",
                    "label": 0
                }
            ]
        }
    }
}