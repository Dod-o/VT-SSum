{
    "id": "de7lq23zhjqx4sheskijfjb6rigu7ed4",
    "title": "Towards Linked Data Fact Validation through Measuring Consensus",
    "info": {
        "author": [
            "Shuangyan Liu, Knowledge Media Institute (KMI), Open University (OU)"
        ],
        "published": "July 15, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2015_liu_linked_data/",
    "segmentation": [
        [
            "Hello everyone, my name is strong in you.",
            "I work in the Knowledge Media Institute at the Open University in the UK and this work.",
            "Today.",
            "I'm going to talk about the my paper on the topic of linked data quality assessment to value essentially to evaluate the correctly solve accuracy of RDF triples and this work is collaborative with my advisor method acting.",
            "And also in Ryco professor in vehicle motor."
        ],
        [
            "First, I will just give a quick review of the fields, give some essential terminologies that used in this field.",
            "So what it what is a data quality it contains contains 2 main points so it's data quality is multidimensional construct with a purpose.",
            "With a focus on the fitness for purpose, so it's so when people talking about quality of data, people are main interest in the application fields, so whether it's fit for that problem area and also it's multidimensional, so are there existing methodologies in in other fields.",
            "In general, data quality assessment can be applicable to the to the data.",
            "Pretty assessment for linked data.",
            "I think the answer is not straightforward because there are because link the data.",
            "There are new challenges in this field because the features of data like the open links of link the data and also the diversity of the data.",
            "Because data can come from different sources, the values of data can become conflict and also the dynamic of the link, the data and also in this.",
            "Carry out by the referee in it or into 2012.",
            "It has summarized dimensions of data, link data quality, and is mainly grouped into four groups.",
            "It includes the Accessibility dimension, which talks about the availability of data, the security performance of link, the data, and the representational dementia mainly concerns the problem of how the data structures under the context dimensions mainly concerns the timeliness of the data.",
            "Issues like that and the interesting dimension mainly concerns of the complete list and consistency.",
            "Brands of linked the data so.",
            "The dimension of this paper carespot is semantic accuracy dimension.",
            "So how the degree of of the data that actually reflects the actual state of the data?",
            "That's the dimension.",
            "This work is really relevant."
        ],
        [
            "OK, so the problem our problem.",
            "This paper is is trying to address this.",
            "How to actually identify inaccurate RDF?",
            "RDF triples and we have a review.",
            "Some of the existing approaches.",
            "We want to know whether this approach is.",
            "Is generally applicable to this problem.",
            "So at least some of the.",
            "Approaches, the first one is called Squeaker, so it's basically want to identify the literal values of properties based on functional dependency rules, so they define some rules based on the dependencies between the two or more properties.",
            "And limitation of this work is that you need that you need the dependencies exist so that you can define the rules and.",
            "You can actually apply rely on this dependency rules and to check the accuracy of the triples, but without the dependencies you can do so it cannot be applied to arbitrary arbitrary statements.",
            "The second one is called defective, which is mainly approach applied.",
            "The bootstrapping link the data framework to verbalize RDF triples.",
            "RDF triples.",
            "The training set like DBPR training set and then these patterns.",
            "For example, I can give you a simple example.",
            "For example the movie Mad Max was written and directed by.",
            "George Miller so this pattern was further as a query used as a query made to our regular search engines and then the search engines return the relevant pages and they are fine.",
            "They will find proofs in this web pages and to actually 2 two giving compute the confidence scores of these patterns and the limitation of this work is because they rely on the.",
            "Aborah framework and the training set.",
            "The training of the both framework focused on mainly object properties, not data type properties, so surely they need some other company approaches for support data type properties."
        ],
        [
            "So our approach is try.",
            "Focus on measuring Constance is based on evidenced.",
            "Repost collected from other linked datasets, and it follows the over same as links to gathering the relevant idea of triples from other sources, and then it performs semantic relatives.",
            "Relatedly, based predicate matching, and then it calling finds.",
            "Tries to according quantify the agreement among the sources as a aggregated confidence score and compare to the exact existing approaches.",
            "It doesn't need to rely on some specific dependency rules and between values of properties and also it has its focal sound data type properties, so it can complete the approaches like the defector approaches."
        ],
        [
            "So our approach is so this graph actually shows the ideas.",
            "The main ideas of our approach it so it starts from the target triples.",
            "So that's the triple.",
            "So you want to evaluate the whether it's true or not, and then you are.",
            "It collects on the cleans the collector relevant subject links from from photos.",
            "Our same as links from other data sources and then it will retrieve all the predicates and objects of that subject.",
            "Links from all other data sources, and this collected triples will be used.",
            "As the basis based sources to fill out the evidence triples.",
            "And then finally we will calculate the confidence scores be based on.",
            "The evidence triples finally get the results of the aggregated scores."
        ],
        [
            "So for the first phrase is about the subject, links collecting and cleaning.",
            "There are main main methods used, one in stock over same as links that the target data source provided.",
            "So like Pete Wikipedia, when every resource for each resource it has, it has all same several or same as links to attached to a specific resource.",
            "And you can carry this properties and to get relevant subject links and also.",
            "The other method is by querying the same as orgy services, which many provide all the relevant subject links attached to a specific subject.",
            "So because there are two sources used, so there are duplicate subject links and also some some diet links so long resolve resolve links.",
            "So want to we want to clean this duplicate links and also long resolved reservable links so we we propose to.",
            "They supposed to come through the some tests to actually do the data cleaning on the subject.",
            "This, for example by painting the corresponding URLs of the subject links and also to removing the identical URLs.",
            "And also because this approach focuses on the English English version of the triple, so it doesn't.",
            "Compel.",
            "Doesn't compare the one triple in English with another triple in other languages."
        ],
        [
            "OK, so.",
            "So we would get the subject links we want to retrieve all the predicates and objects we we may maintain self because giving a URL of a subject list is.",
            "In some cases the URLs are not different, different symbol.",
            "So for example, some biology datasets you cannot difference the predicate repost from the URL, so it needs some other approaches.",
            "So we maintain some map of between the domain names which from the different sources and the modules to actually to retrieve the predicate object so.",
            "For, for example, for sources like jewel names and Ordnance Service and BBC Things we use Gina API to actually retrieve the triples and four YAGO knowledge, which actually you can difference the difference that you are about.",
            "It actually do not provide the the active data you want, so we use sparkle in the point instead and also the week data we use with some toolkit for week data too.",
            "Which actually provide provides the object triples predicates by analyzing the dump of weak data."
        ],
        [
            "OK.",
            "So the predicate matching is based on the node based on stream.",
            "Similarity matching because there are often the case is there are there are properties that from the string similarity perspective they are not the same.",
            "For example, the DB pedia or population total is no different from the Jago has number of people, but semantically they are the same properties.",
            "So the values they show are identical.",
            "So we instead we use semantic related.",
            "Relatedly, similarity matching the swap method is measuring is applied, so the web method basically can gives your score normalized between zero and one, and we also tested it.",
            "Compare it with some modest like name method, but it gives the higher rate of more reliable results, so we use that.",
            "It's basically considers the.",
            "Capsule two, since there's two concepts in the world that taxonomic's, so we use swap method and we use words similarity for Java API to actually use to generate the pairwise similarity metrics for two input predicates."
        ],
        [
            "OK, so let this shows just the formulas.",
            "The mathematical formulas we defined to actually to calculate the political similarities.",
            "So from from the metrics of pairwise similarity semantic similarities, we got the individual scores for two input sentences.",
            "So that's splitted from 2 compound properties and then we use.",
            "Formula in the middle to actually compute the similarity between each word.",
            "And finally we use the buttons formula to actually to to aggregate the similarity scores for one properties OK."
        ],
        [
            "And then we come to the stage of confidence measuring because the the proposed approach focused on main.",
            "Mainly the literal values.",
            "So there are two the string and numerical numerical values of the objects are considered.",
            "If the object of the target table is a string, we computed the aggregated score as the weighted average of the.",
            "Weighted average of the string similarity between the top object values of the target triple under also the the sources that ripples from the sources and if the object the tag triple is numerical we.",
            "We will also use the aggregated score, but we will subject Subs substructure.",
            "The ratio between the differences between the weighted average and the.",
            "The numerical value of the target target triple and the rich score from one so.",
            "OK."
        ],
        [
            "So we have carry out some experiments at this stage they experiment was a main try to test the feasibility of our approach.",
            "So we want to say each component of our approach is work not under real world datasets.",
            "So we collect, test set from DB pedia and using this barcode query and mainly trying to retrieve the towns in Milton Keynes cities.",
            "Which has a population larger than 10,000, so it gives us a test.",
            "Some target triples as showed."
        ],
        [
            "And initially we collected 1300 subject links and after the cleaning process we there are 170 remained as the subject list.",
            "The reason for that is most of the links are long reasonable so there are issues with like 500 errors and the server was crashed.",
            "So we will load.",
            "We were not able to actually get to the subject.",
            "When the we we carry out the experiment and also there are 18 duplicated duplicated links and also 252 links in other languages.",
            "Also, we examined also examine the causes of the.",
            "The sub jailings process whether we want to improve the cleaning process."
        ],
        [
            "OK, so after the predicates of objects between process the totally there are.",
            "1700 triples collected for 172 subject lines for each of the target triple.",
            "The number of the number of relevant triple collected are various, so some for some of the triple has hundreds.",
            "So larger number relevant, triple collected.",
            "But for some of them is just a few dozens of triples.",
            "So by the algorithm there are 60% not matched by the algorithm we so we cannot could not produce the confidence score on this on this part of data.",
            "But 40 are matched actually used by the algorithms."
        ],
        [
            "So this diagram shows the distribution of the predicate similarity that given by the algorithms.",
            "So we see there are 60% which got which are not matched.",
            "So and there are like 12% is matched by the by the algorithm."
        ],
        [
            "OK, for the confidence measurement we we also we found some errors in the result.",
            "We found some errors in the results.",
            "We act."
        ],
        [
            "Only this for the middle one the 004 is is the error so caused by the fake submitting.",
            "So during the subject link links we follows the same as link and it taught us is identical.",
            "But actually we checked this link and it's it's wrong, so it's pointed to some other links which caused the school is very low.",
            "So I think the.",
            "The method need to extend it to also consider the reliability of the subject links."
        ],
        [
            "OK, so our main contribution is that we propose a novel approach for measuring consciousness of the identify verify artifi.",
            "RDF triples correctly based on the.",
            "Based on the evidence triples collected from other data sources and also there.",
            "Incorporates a method to identify match that ripples from the to the target triple, and this approach would become increasingly important due to the growth of LD Cloud and for the evaluation of the work we're going to demonstrate the.",
            "The approach can be applied to opportunely predicates and also we are trying to do doing.",
            "Solution on the predicate similarity matching method with standard evaluation and measures like precision.",
            "Recall on some other well known datasets and also we trying to explore the correlation between the confidence score aside with the proposed method and the accuracy of the input affect.",
            "So that's the end of my presentation.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everyone, my name is strong in you.",
                    "label": 0
                },
                {
                    "sent": "I work in the Knowledge Media Institute at the Open University in the UK and this work.",
                    "label": 1
                },
                {
                    "sent": "Today.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about the my paper on the topic of linked data quality assessment to value essentially to evaluate the correctly solve accuracy of RDF triples and this work is collaborative with my advisor method acting.",
                    "label": 0
                },
                {
                    "sent": "And also in Ryco professor in vehicle motor.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, I will just give a quick review of the fields, give some essential terminologies that used in this field.",
                    "label": 0
                },
                {
                    "sent": "So what it what is a data quality it contains contains 2 main points so it's data quality is multidimensional construct with a purpose.",
                    "label": 1
                },
                {
                    "sent": "With a focus on the fitness for purpose, so it's so when people talking about quality of data, people are main interest in the application fields, so whether it's fit for that problem area and also it's multidimensional, so are there existing methodologies in in other fields.",
                    "label": 1
                },
                {
                    "sent": "In general, data quality assessment can be applicable to the to the data.",
                    "label": 0
                },
                {
                    "sent": "Pretty assessment for linked data.",
                    "label": 0
                },
                {
                    "sent": "I think the answer is not straightforward because there are because link the data.",
                    "label": 0
                },
                {
                    "sent": "There are new challenges in this field because the features of data like the open links of link the data and also the diversity of the data.",
                    "label": 0
                },
                {
                    "sent": "Because data can come from different sources, the values of data can become conflict and also the dynamic of the link, the data and also in this.",
                    "label": 0
                },
                {
                    "sent": "Carry out by the referee in it or into 2012.",
                    "label": 0
                },
                {
                    "sent": "It has summarized dimensions of data, link data quality, and is mainly grouped into four groups.",
                    "label": 0
                },
                {
                    "sent": "It includes the Accessibility dimension, which talks about the availability of data, the security performance of link, the data, and the representational dementia mainly concerns the problem of how the data structures under the context dimensions mainly concerns the timeliness of the data.",
                    "label": 1
                },
                {
                    "sent": "Issues like that and the interesting dimension mainly concerns of the complete list and consistency.",
                    "label": 0
                },
                {
                    "sent": "Brands of linked the data so.",
                    "label": 0
                },
                {
                    "sent": "The dimension of this paper carespot is semantic accuracy dimension.",
                    "label": 0
                },
                {
                    "sent": "So how the degree of of the data that actually reflects the actual state of the data?",
                    "label": 1
                },
                {
                    "sent": "That's the dimension.",
                    "label": 0
                },
                {
                    "sent": "This work is really relevant.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the problem our problem.",
                    "label": 0
                },
                {
                    "sent": "This paper is is trying to address this.",
                    "label": 0
                },
                {
                    "sent": "How to actually identify inaccurate RDF?",
                    "label": 0
                },
                {
                    "sent": "RDF triples and we have a review.",
                    "label": 0
                },
                {
                    "sent": "Some of the existing approaches.",
                    "label": 0
                },
                {
                    "sent": "We want to know whether this approach is.",
                    "label": 0
                },
                {
                    "sent": "Is generally applicable to this problem.",
                    "label": 0
                },
                {
                    "sent": "So at least some of the.",
                    "label": 0
                },
                {
                    "sent": "Approaches, the first one is called Squeaker, so it's basically want to identify the literal values of properties based on functional dependency rules, so they define some rules based on the dependencies between the two or more properties.",
                    "label": 0
                },
                {
                    "sent": "And limitation of this work is that you need that you need the dependencies exist so that you can define the rules and.",
                    "label": 0
                },
                {
                    "sent": "You can actually apply rely on this dependency rules and to check the accuracy of the triples, but without the dependencies you can do so it cannot be applied to arbitrary arbitrary statements.",
                    "label": 1
                },
                {
                    "sent": "The second one is called defective, which is mainly approach applied.",
                    "label": 0
                },
                {
                    "sent": "The bootstrapping link the data framework to verbalize RDF triples.",
                    "label": 0
                },
                {
                    "sent": "RDF triples.",
                    "label": 1
                },
                {
                    "sent": "The training set like DBPR training set and then these patterns.",
                    "label": 0
                },
                {
                    "sent": "For example, I can give you a simple example.",
                    "label": 0
                },
                {
                    "sent": "For example the movie Mad Max was written and directed by.",
                    "label": 0
                },
                {
                    "sent": "George Miller so this pattern was further as a query used as a query made to our regular search engines and then the search engines return the relevant pages and they are fine.",
                    "label": 0
                },
                {
                    "sent": "They will find proofs in this web pages and to actually 2 two giving compute the confidence scores of these patterns and the limitation of this work is because they rely on the.",
                    "label": 1
                },
                {
                    "sent": "Aborah framework and the training set.",
                    "label": 0
                },
                {
                    "sent": "The training of the both framework focused on mainly object properties, not data type properties, so surely they need some other company approaches for support data type properties.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our approach is try.",
                    "label": 0
                },
                {
                    "sent": "Focus on measuring Constance is based on evidenced.",
                    "label": 1
                },
                {
                    "sent": "Repost collected from other linked datasets, and it follows the over same as links to gathering the relevant idea of triples from other sources, and then it performs semantic relatives.",
                    "label": 1
                },
                {
                    "sent": "Relatedly, based predicate matching, and then it calling finds.",
                    "label": 1
                },
                {
                    "sent": "Tries to according quantify the agreement among the sources as a aggregated confidence score and compare to the exact existing approaches.",
                    "label": 0
                },
                {
                    "sent": "It doesn't need to rely on some specific dependency rules and between values of properties and also it has its focal sound data type properties, so it can complete the approaches like the defector approaches.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our approach is so this graph actually shows the ideas.",
                    "label": 0
                },
                {
                    "sent": "The main ideas of our approach it so it starts from the target triples.",
                    "label": 1
                },
                {
                    "sent": "So that's the triple.",
                    "label": 0
                },
                {
                    "sent": "So you want to evaluate the whether it's true or not, and then you are.",
                    "label": 0
                },
                {
                    "sent": "It collects on the cleans the collector relevant subject links from from photos.",
                    "label": 1
                },
                {
                    "sent": "Our same as links from other data sources and then it will retrieve all the predicates and objects of that subject.",
                    "label": 1
                },
                {
                    "sent": "Links from all other data sources, and this collected triples will be used.",
                    "label": 0
                },
                {
                    "sent": "As the basis based sources to fill out the evidence triples.",
                    "label": 0
                },
                {
                    "sent": "And then finally we will calculate the confidence scores be based on.",
                    "label": 0
                },
                {
                    "sent": "The evidence triples finally get the results of the aggregated scores.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the first phrase is about the subject, links collecting and cleaning.",
                    "label": 1
                },
                {
                    "sent": "There are main main methods used, one in stock over same as links that the target data source provided.",
                    "label": 0
                },
                {
                    "sent": "So like Pete Wikipedia, when every resource for each resource it has, it has all same several or same as links to attached to a specific resource.",
                    "label": 0
                },
                {
                    "sent": "And you can carry this properties and to get relevant subject links and also.",
                    "label": 0
                },
                {
                    "sent": "The other method is by querying the same as orgy services, which many provide all the relevant subject links attached to a specific subject.",
                    "label": 1
                },
                {
                    "sent": "So because there are two sources used, so there are duplicate subject links and also some some diet links so long resolve resolve links.",
                    "label": 0
                },
                {
                    "sent": "So want to we want to clean this duplicate links and also long resolved reservable links so we we propose to.",
                    "label": 0
                },
                {
                    "sent": "They supposed to come through the some tests to actually do the data cleaning on the subject.",
                    "label": 0
                },
                {
                    "sent": "This, for example by painting the corresponding URLs of the subject links and also to removing the identical URLs.",
                    "label": 1
                },
                {
                    "sent": "And also because this approach focuses on the English English version of the triple, so it doesn't.",
                    "label": 1
                },
                {
                    "sent": "Compel.",
                    "label": 0
                },
                {
                    "sent": "Doesn't compare the one triple in English with another triple in other languages.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So we would get the subject links we want to retrieve all the predicates and objects we we may maintain self because giving a URL of a subject list is.",
                    "label": 1
                },
                {
                    "sent": "In some cases the URLs are not different, different symbol.",
                    "label": 0
                },
                {
                    "sent": "So for example, some biology datasets you cannot difference the predicate repost from the URL, so it needs some other approaches.",
                    "label": 0
                },
                {
                    "sent": "So we maintain some map of between the domain names which from the different sources and the modules to actually to retrieve the predicate object so.",
                    "label": 0
                },
                {
                    "sent": "For, for example, for sources like jewel names and Ordnance Service and BBC Things we use Gina API to actually retrieve the triples and four YAGO knowledge, which actually you can difference the difference that you are about.",
                    "label": 0
                },
                {
                    "sent": "It actually do not provide the the active data you want, so we use sparkle in the point instead and also the week data we use with some toolkit for week data too.",
                    "label": 0
                },
                {
                    "sent": "Which actually provide provides the object triples predicates by analyzing the dump of weak data.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the predicate matching is based on the node based on stream.",
                    "label": 1
                },
                {
                    "sent": "Similarity matching because there are often the case is there are there are properties that from the string similarity perspective they are not the same.",
                    "label": 0
                },
                {
                    "sent": "For example, the DB pedia or population total is no different from the Jago has number of people, but semantically they are the same properties.",
                    "label": 0
                },
                {
                    "sent": "So the values they show are identical.",
                    "label": 0
                },
                {
                    "sent": "So we instead we use semantic related.",
                    "label": 0
                },
                {
                    "sent": "Relatedly, similarity matching the swap method is measuring is applied, so the web method basically can gives your score normalized between zero and one, and we also tested it.",
                    "label": 0
                },
                {
                    "sent": "Compare it with some modest like name method, but it gives the higher rate of more reliable results, so we use that.",
                    "label": 0
                },
                {
                    "sent": "It's basically considers the.",
                    "label": 0
                },
                {
                    "sent": "Capsule two, since there's two concepts in the world that taxonomic's, so we use swap method and we use words similarity for Java API to actually use to generate the pairwise similarity metrics for two input predicates.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let this shows just the formulas.",
                    "label": 0
                },
                {
                    "sent": "The mathematical formulas we defined to actually to calculate the political similarities.",
                    "label": 0
                },
                {
                    "sent": "So from from the metrics of pairwise similarity semantic similarities, we got the individual scores for two input sentences.",
                    "label": 0
                },
                {
                    "sent": "So that's splitted from 2 compound properties and then we use.",
                    "label": 0
                },
                {
                    "sent": "Formula in the middle to actually compute the similarity between each word.",
                    "label": 0
                },
                {
                    "sent": "And finally we use the buttons formula to actually to to aggregate the similarity scores for one properties OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we come to the stage of confidence measuring because the the proposed approach focused on main.",
                    "label": 0
                },
                {
                    "sent": "Mainly the literal values.",
                    "label": 0
                },
                {
                    "sent": "So there are two the string and numerical numerical values of the objects are considered.",
                    "label": 1
                },
                {
                    "sent": "If the object of the target table is a string, we computed the aggregated score as the weighted average of the.",
                    "label": 1
                },
                {
                    "sent": "Weighted average of the string similarity between the top object values of the target triple under also the the sources that ripples from the sources and if the object the tag triple is numerical we.",
                    "label": 1
                },
                {
                    "sent": "We will also use the aggregated score, but we will subject Subs substructure.",
                    "label": 0
                },
                {
                    "sent": "The ratio between the differences between the weighted average and the.",
                    "label": 1
                },
                {
                    "sent": "The numerical value of the target target triple and the rich score from one so.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have carry out some experiments at this stage they experiment was a main try to test the feasibility of our approach.",
                    "label": 0
                },
                {
                    "sent": "So we want to say each component of our approach is work not under real world datasets.",
                    "label": 0
                },
                {
                    "sent": "So we collect, test set from DB pedia and using this barcode query and mainly trying to retrieve the towns in Milton Keynes cities.",
                    "label": 0
                },
                {
                    "sent": "Which has a population larger than 10,000, so it gives us a test.",
                    "label": 0
                },
                {
                    "sent": "Some target triples as showed.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And initially we collected 1300 subject links and after the cleaning process we there are 170 remained as the subject list.",
                    "label": 1
                },
                {
                    "sent": "The reason for that is most of the links are long reasonable so there are issues with like 500 errors and the server was crashed.",
                    "label": 0
                },
                {
                    "sent": "So we will load.",
                    "label": 0
                },
                {
                    "sent": "We were not able to actually get to the subject.",
                    "label": 1
                },
                {
                    "sent": "When the we we carry out the experiment and also there are 18 duplicated duplicated links and also 252 links in other languages.",
                    "label": 1
                },
                {
                    "sent": "Also, we examined also examine the causes of the.",
                    "label": 0
                },
                {
                    "sent": "The sub jailings process whether we want to improve the cleaning process.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so after the predicates of objects between process the totally there are.",
                    "label": 0
                },
                {
                    "sent": "1700 triples collected for 172 subject lines for each of the target triple.",
                    "label": 1
                },
                {
                    "sent": "The number of the number of relevant triple collected are various, so some for some of the triple has hundreds.",
                    "label": 0
                },
                {
                    "sent": "So larger number relevant, triple collected.",
                    "label": 0
                },
                {
                    "sent": "But for some of them is just a few dozens of triples.",
                    "label": 1
                },
                {
                    "sent": "So by the algorithm there are 60% not matched by the algorithm we so we cannot could not produce the confidence score on this on this part of data.",
                    "label": 0
                },
                {
                    "sent": "But 40 are matched actually used by the algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this diagram shows the distribution of the predicate similarity that given by the algorithms.",
                    "label": 0
                },
                {
                    "sent": "So we see there are 60% which got which are not matched.",
                    "label": 0
                },
                {
                    "sent": "So and there are like 12% is matched by the by the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, for the confidence measurement we we also we found some errors in the result.",
                    "label": 1
                },
                {
                    "sent": "We found some errors in the results.",
                    "label": 0
                },
                {
                    "sent": "We act.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only this for the middle one the 004 is is the error so caused by the fake submitting.",
                    "label": 0
                },
                {
                    "sent": "So during the subject link links we follows the same as link and it taught us is identical.",
                    "label": 0
                },
                {
                    "sent": "But actually we checked this link and it's it's wrong, so it's pointed to some other links which caused the school is very low.",
                    "label": 0
                },
                {
                    "sent": "So I think the.",
                    "label": 0
                },
                {
                    "sent": "The method need to extend it to also consider the reliability of the subject links.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so our main contribution is that we propose a novel approach for measuring consciousness of the identify verify artifi.",
                    "label": 0
                },
                {
                    "sent": "RDF triples correctly based on the.",
                    "label": 0
                },
                {
                    "sent": "Based on the evidence triples collected from other data sources and also there.",
                    "label": 0
                },
                {
                    "sent": "Incorporates a method to identify match that ripples from the to the target triple, and this approach would become increasingly important due to the growth of LD Cloud and for the evaluation of the work we're going to demonstrate the.",
                    "label": 1
                },
                {
                    "sent": "The approach can be applied to opportunely predicates and also we are trying to do doing.",
                    "label": 1
                },
                {
                    "sent": "Solution on the predicate similarity matching method with standard evaluation and measures like precision.",
                    "label": 0
                },
                {
                    "sent": "Recall on some other well known datasets and also we trying to explore the correlation between the confidence score aside with the proposed method and the accuracy of the input affect.",
                    "label": 1
                },
                {
                    "sent": "So that's the end of my presentation.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}