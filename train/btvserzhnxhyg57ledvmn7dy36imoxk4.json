{
    "id": "btvserzhnxhyg57ledvmn7dy36imoxk4",
    "title": "Using OpenACC With CUDA Libraries (Part 4)",
    "info": {
        "author": [
            "John Urbanic, Pittsburgh Supercomputing Center"
        ],
        "published": "Sept. 19, 2016",
        "recorded": "June 2016",
        "category": [
            "Top->Computer Science",
            "Top->Computers->Programming"
        ]
    },
    "url": "http://videolectures.net/ihpcss2016_urbanic_using_openACC/",
    "segmentation": [
        [
            "The next thing I want to talk about is using openings to see with CUDA libraries, which if I were inclined to have really long titles, will also say and also interfacing with CUDA code itself.",
            "So I'll show you right now how to actually allow yourself if you really want to write your own CUDA code, you work with a colleague.",
            "It writes CUDA code you want to.",
            "At some point you decide open ACC isn't cutting it in this particular piece of the code, you can write a little bit of CUDA code and interface with it very easily with open ACC.",
            "As a matter of fact, that's essentially what we're doing.",
            "We're using libraries.",
            "The vast majority of openev.",
            "Of libraries for GPU's were painstakingly optimized by somebody writing in CUDA at a low level, and so you get the benefit of all of that.",
            "So, as I mentioned before, there are three different ways you can use a GPU, so we can either program at the low level we can program at high level with open ACC, which is what this is all about.",
            "We can use libraries like this, or we can interface to CUDA code at a low level.",
            "So let's take first of all the opportunity here for me to not give you a comprehensive survey of CUDA libraries.",
            "Because they are many, there is an immense amount of of the whole ecosystem for for NVIDIA in particular is large, and by that I'm including debugging tools and performance profiling tools and whatnot, but most important is the library base, which is enormous.",
            "There are CUDA libraries to do all of the obvious things like you might think of like matrix multiplies and image recognition.",
            "Things that you might think that or image enhancement and processing that you might think of GPU would be good at.",
            "There are also.",
            "Large amounts of their large numbers of libraries out there to do things that should impress me.",
            "At least that somebody managed to get the GPU to do effectively.",
            "Knowing what I do about it, their architecture and whatnot.",
            "So the very first thing you should do before you start writing code in a particular application is you should Google and again, Google is your friend on these kind of things.",
            "You should Google and say is there a library that does this already or does a big does a large chunk of this?",
            "And if there's a library that accomplishes.",
            "Most of what you want to do, you'd be foolish not to use it.",
            "That's general programming, right?",
            "Not just open ACC.",
            "And don't you can go to say NVIDIA.",
            "NVIDIA does a nice job of highlighting all of the CUDA libraries that are out there, but they're not entirely comprehensive and thorough, and so there are lots more things hiding out there on GitHub and whatnot.",
            "So again, Google will find most of those things, so Google before you start writing code, and that's what should be obvious to you if you're writing serial code and you found yourself writing some kind of matrix factorization routine, you probably take hesitate for a second ago.",
            "Hasn't somebody already done this for me?",
            "Well, you should absolutely do that with CUDA.",
            "So I won't go through all of these.",
            "There's a little appendix on this on this talk that has some of the things that are out there, but again, it's it's not remotely comprehensive.",
            "So let's instead look at what is interesting here, which is how to use those libraries, and basically all of the magic is in the fact that using a clause that will find device pointer.",
            "Here we can tell CUDA or excuse me, we can tell open ACC that this is a pointer to some data on the GPU, so it's already on the GPU.",
            "And now in the data is probably on the GPU 'cause you called some CUDA library and it left some process data there.",
            "Likewise, you can tell some CUDA library.",
            "Hey, here's some stuff I'm working with an open ACC and I would like you to operate in that data.",
            "So what we'll do here is a simple example that does a.",
            "So here's the pragma and we'll just see it in action right here.",
            "We've got a simple example that does a convolution.",
            "A convolution is basically when you're just, you're mathematically you're integrating over the composition of some functions, but in.",
            "Digital, it's discrete form.",
            "Usually what you're doing is you're taking some bunch of data that you have in your applying a small transform to it across the entire data set.",
            "An very common way of doing that is taking FFT of some data, transform it into its frequency space, apply your own manipulations to it, and then transform it back.",
            "That's a very common thing to do for many, many things, not just image processing, but also lots of things that involve differential equations can be done this way for reasons that are mathematically interesting.",
            "However, for those of you that don't use out of teas and don't care about them at all, think of this just simply as a very popular math library which is written in CUDA runs very effectively on GPU's and we are now going to utilized for more open ACC code.",
            "We're going to interact with our open ACC code, so the steps of a convolution or do an FFT.",
            "That's step one here #2, which is to do some monkeying with the data, which in this case we're going to apply our data and scale it some, and that's a useful thing to do.",
            "For convolution we're going to open ACC and then we're going to send it back for another 50, yes?",
            "Oh, what is happening here?",
            "We probably have not started the slideshow and.",
            "I regret.",
            "That this is odd.",
            "We are completely decoupled between what my laptop is doing and what is on the screen.",
            "Let's try that again.",
            "And I'm gonna guess that's been the case since I started talking.",
            "Is that right?",
            "Yeah, OK. You are way too accommodating of a group."
        ],
        [
            "OK, this will maybe have a little more context to it so.",
            "It's a relief."
        ],
        [
            "Fortunately, you didn't miss anything too critical here.",
            "This was just backing up my point that we have now with open ACC."
        ],
        [
            "We're going to be able to interface between libraries as well as CUDA code in CUDA routine."
        ],
        [
            "This was when I was talking about the immense number of libraries that are out there available.",
            "This is what we've got just as a sampler here, but again."
        ],
        [
            "Google is your friend and will show you all kinds of stuff out there.",
            "As a matter of fact, a good thing.",
            "I think I didn't have on here is the latest version of the CUDA capabilities comes along with a tool called Envy Graphene Videos Graph library.",
            "So there are all kinds of graph oriented retains and this came up over lunch just out how many people in this room do do graph type operations and use graph libraries and whatnot?",
            "Yeah, there's there's always a decent minority of people out there, so graph operations or something that seems to those of you that are used to the bigger raise we've been talking about bigger raise so far of data to apply or.",
            "R do loop so we got to do in the for loops.",
            "Graph operations are more like chasing pointers through memory and so there are very different way of organizing data and usually scare people that are used to working with big arrays of data and they should because it's sometimes very difficult to optimize those kind of operations.",
            "That's the kind of thing that knowing what you know so far about GPU's you might go of course graph operations are not a thing to do on GPU's.",
            "All these cores that are synchronized and doing the same things will clearly not be effective on graph operations.",
            "Turns out that's not the case at all.",
            "GPU's are very effective.",
            "And the envy graph library, which has just been released is good because not only does it pull together a lot of the spirit graph routines that are floating around out there, but they're very well optimized.",
            "So for those of you that Dordogne graph operations, yes, GPU's are for you as well, and actually they're very very effective at a wide classes of graphs."
        ],
        [
            "Operations so."
        ],
        [
            "Let's get onto."
        ],
        [
            "So these are the claws."
        ],
        [
            "Here, as I said, there's nothing magic about them, but here's the example that we're going to do.",
            "We're going to take an FFT operation again.",
            "A very common and useful math operation that you may or may not use.",
            "We're going to do an FFT on some data.",
            "Then we're going to mess with the date a little bit, which is what is going to make this actually a convolution, and we'll do that with their own open ACC code, and then will call an FFT routine to transform the data back.",
            "So again, we're using the."
        ],
        [
            "Extremely effective and fast FFT routines with our own our own open ACC code mixed in.",
            "So here's what's going to happen at a high level and it's fairly straightforward to use.",
            "The only the only thing that we have to dive down and get a little bit messy with and there's just a little bit of messing us here and the messiness that I'm showing you here is the same messiness will apply to any problem.",
            "So if you're not comfortable if you're not, if you're a C programmer, I hope that the Malik type routines or memory allocation are second nature to you.",
            "If you're a Fortran programmer, maybe you've managed to skirt these issues.",
            "And perhaps not, perhaps you had to deal with memory allocation not in in the elegant new four tramway, but in dealing with C routines.",
            "At any rate, all we're doing is we're telling we're setting up our data structures before hand, with memory allocation routines, and they're fairly straightforward.",
            "Here we have a couple of different we have.",
            "In this case, we have a data that lives on the host, and the data lives on the device.",
            "Again, host being the generic term for CPU device being the generic term for accelerator or GPU.",
            "In this case.",
            "Here we have our complex whoops we have our.",
            "We have our come on don't fight me on this.",
            "Pardon me while I fight with.",
            "This thing once again.",
            "Oh"
        ],
        [
            "OK.",
            "Yes, OK, so in this case here."
        ],
        [
            "We have first a couple of allocation routines were on the host.",
            "We need to set up our memory so when the data lives on the host it's dynamically allocated memory that we set up in advance, and we're using a couple of Malik routines against any C programmer.",
            "This is very familiar to you.",
            "Fortran programmers you have to deal with with this very simple and straightforward way.",
            "We're just saying give me a couple of buffers in memory.",
            "Then we also do a very similar looking thing on the GPU, which is new to you.",
            "If you are a C programmer.",
            "Again, looks kind of familiar, but you know it's a variation on it were doing.",
            "The CUDA memory allocate with the CUDA malloc routine right there, so with CUDA Malik we're going on the device.",
            "They set up the data.",
            "This is where our data is going to live.",
            "So once we do this now, we can point to these regions at the appropriate time when we want to refer to the data that's in these regions where."
        ],
        [
            "It's on the host of the GPU, so here's the actual part of the source code that does all the fun stuff and you have in your exercises directory the full source code.",
            "Now I got this excerpt from some of my buddies at NVIDIA and they have a slight different philosophy about training in me.",
            "They like to do a lot of training with excerpts from big complicated codes.",
            "I'd like to be able to make sure everybody understands every line of code or loss code.",
            "Again, you are capable of understanding every line in this case here.",
            "This code is a couple 100 lines long, even though this is really this another excerpt I'll show you where the interesting parts.",
            "They write examples.",
            "In this case, the thing runs the CUDA version and then it also runs a serial version and compares both to make sure that you get the right result and now it's kind of sophisticated, but at the expense of being a little bit bigger.",
            "So the example sits in your exercise directory, it's called CU FFT, something subdirectory you can go and look at it and play with it.",
            "It's just a little bit bigger than I feel comfortable going over line by line in a class, so here's the excerpt where the action happens in this case.",
            "Here we are calling the CUDA FFT routine.",
            "It happens to have this name.",
            "They've all got long, funny names that kind of cram all the information and so the CUDA FFT library has a routine called CUDA FFT.",
            "And then if this is a complex to complex FFT and so we're calling it, we call this on the data.",
            "That's our signal.",
            "So we're doing convolution got signal data and then we've got our filter kernel data that we're applying.",
            "That's what are filters applying to our main data.",
            "This is what makes you a convolution again.",
            "So these are two things that live on the device.",
            "This is the data when it lives on the device.",
            "And we say in this case here it's already on the device the data is on the device at this point in the code, and we call the FFT, but we want to point it today that lives on the device.",
            "So this says when you look at when you get this variable right there these signal.",
            "This is a pointer to stuff that's already on the device.",
            "Don't worry about migrating data back and forth.",
            "This is out there when you come back from the FFT routine.",
            "At this point in the code here we now have transformed data.",
            "Now we call our own routine and it's written with the.",
            "The The Nice Compact name of complex pointwise multiply and scale, so that's what are we actually doing.",
            "It will look at that source code in a second, so this is our own CUDA Skoda torrox use me open ACC code that we've written right here and this manipulates the data a little bit in the way we'll see and then once again at the end we transform the data back.",
            "So this is a very very straightforward use of an existing CUDA library that we know nothing about.",
            "So this is a black box library right here that we're calling, but we just need to make sure we call it that.",
            "It knows that the date is.",
            "Already on the device, don't try to do anything for us."
        ],
        [
            "Now the data that's on the device, we can refer to it out there without doing any data management.",
            "We say don't try to move the stuff onto or off the GPU.",
            "This is a pointer to stuff that's out there, so we use the clause device pointer clause and say this is a pointer to stuff that's already out there.",
            "So going to play with it on the GPU, but don't do any data management and so we play with the data here in this routine we can do all kinds of stuff right here, and this is where this could be any open ACC code.",
            "And it's nice.",
            "Regular C serial code, right?",
            "Here we can do whatever we want.",
            "Works with Fortran too.",
            "We can play with the data all we want right here and then once again we."
        ],
        [
            "To go to the previous slide, we returned from this monkeying with the data, and then could call the next FFT, and that is the loop that we do here to the data, so it's very."
        ],
        [
            "Straightforward access any CUDA library in equivalent fashion and that could also be CUDA code, so if you're working with some colleague and they've written code code or you as a CUDA programmer either at existing CUDA programmer have code or you need to do to do it for some reason, you can interface with your own field very easily, yes.",
            "Open CL now so hope it's yeah, we'll we'll talk about open CL a little bit later.",
            "I think in the advanced or not needed and stuff we covered.",
            "That must be the.",
            "No, it's it's still to come somewhere to open it.",
            "I'll mention it briefly how many open CL programmers do we have here, by the way.",
            "Or people who deal with open CL a couple.",
            "OK, so open CL is a thing is an issue in this world, but gotten increasingly unattractive.",
            "One to deal with so.",
            "We'll talk about it."
        ],
        [
            "Anyway, so linking this if you actually want to run the example yourself or play around with it linking this is like any other library and that it's always got some complex nonsense to worry about, but it's not particularly profound, so if you want to link with this and run it, you can do it.",
            "It's very straightforward.",
            "Feel free to."
        ],
        [
            "With it and you run the code and it gives you some output and like I say, this code is is kind of what I think is a little unduly complex.",
            "It runs a serial version and it runs the non serial version in comparison to make sure they match etc etc, but in essence it's a nice example of just how easy it is to use any of the existing CUDA libraries and interfacing with your own open ACC code so."
        ],
        [
            "That's really all I want to say about this because there isn't much more to it.",
            "It's not the profound.",
            "The little bit of boilerplate code code you have to include is slightly ugly, but it's minimal.",
            "And what we've done right there with those memory allocations.",
            "That's about it.",
            "So you don't have to spend a lot of time trying to figure out what these kudo routines are about.",
            "Those are the only ones you really have to see, so I've got."
        ],
        [
            "Appendix on here, which used to be kind of interesting to Wade through."
        ],
        [
            "Of the kind of."
        ],
        [
            "Routines that are out there a lot of anything that's in MKL orb last kind of linear algebra library."
        ],
        [
            "Says they've got versions."
        ],
        [
            "So all this stuff you'd expect."
        ],
        [
            "Things out there, but again, increasingly things that you wouldn't expect would."
        ],
        [
            "So well are also out there as well."
        ],
        [
            "So it's kind of silly for me to go over these."
        ],
        [
            "Is this some kind of comprehensive survey of the field?",
            "Look through the appendix if you want, but more importantly again just say hey what do I wish was out there that there's a serial version of and Google for it and it may be out there and it may not be, but it's not hard to ascertain.",
            "OK with that.",
            "I don't want to like I said, well I'm at a whole lot because it's not all that interesting is compared to.",
            "Yes.",
            "I.",
            "Let me you probably have to load a module.",
            "The right module on this, since this is a slightly old version, would be let me go back and look at it.",
            "Yeah.",
            "Um, load the koetzle.",
            "That sounds that sounds like the right thing to do, but there are different versions up, so module.",
            "By the way, modules on Bridge is an on and hopefully most modern platforms is a nice way to manage all these different version and environment issues.",
            "Module is simply a way it brings it a bunch of passive environment variables.",
            "Primarily it could be slightly clever, but most of the time it just sets up all the path environment variables that you have associated with the compiler or library or whatnot, and then also equally importantly, when you unload the module one sets all of those who don't have conflicts, so modules is a great environment.",
            "I hope again many of you that work on big platforms have been exposed to him already because it beats the alternative on bridges.",
            "We try to do everything with modules.",
            "One of the things modules does is have a lot of versions though.",
            "So in this age especially the age of Python And whatnot, it is important to have every version that's ever existed sitting around because everything is incompatible with other versions.",
            "In this case here with CUDA versions, if you type, if you do a module avail.",
            "Did you try that?",
            "Do a module, just do module avail in general and see which ones of CUDA.",
            "Are sitting around there.",
            "If you type one of the commands you can do with modules, say module vouchers.",
            "You all the modules on the machine and very often for CUDA for example, there should be several versions.",
            "There's a default one if you just say module load CUDA and then there should be, I hope a couple of alternatives.",
            "Yes, the 7.5 as default in 8.0 RC also is on there so so I will be glad to help you debug this and anybody else that wants to run this particular example.",
            "Which modules need to be loaded because I will admit that I haven't actually tried to execute this thing in.",
            "I months, which is now probably several versions of the PGI compiler and one version of CUDA Library or something.",
            "So I hope you sort it out there.",
            "So if you want to run this example, see me in a little bit after we have some free exercise time and I'll tell you what the right modules to load are.",
            "But yeah, so with with that linkage there having all those versions in it.",
            "Yeah, inevitably you have to find the right ones, although as I said, with C and Fortran standard libraries, it's usually it's just a matter of finding the right version with typical Python programming stack anymore it could be.",
            "Two day adventure so OK so the next thing I want to talk about is.",
            "Right here.",
            "The.",
            "Open.",
            "Empyre stick pick yes.",
            "So this one is more, I think, interesting to us here.",
            "Let's make sure I Max."
        ],
        [
            "Doing something."
        ],
        [
            "Yes, OK, this is a topic it's showing up continually, not just in here but many.",
            "Many of my discussions with you guys independently have revolved around this because open ACC as I mentioned you early on open ACC arose because the need GPU computing came to be and the need for some higher level approach then came to be recognized and recognizing the Open MP's directive based approach is not only had has a great history of affecting this but seems to be exactly the right thing to do which I think you're in a position to judge.",
            "It's pretty elegant way to do this stuff right?",
            "So open MP has had history at that point and the committee said, well, let's us do this, but because the open MP standard is a very good one of the very serious one, we can't frivolously just start adding stuff into it.",
            "Let's fork intentionally to open ACC, which will be very quick moving, fast paced, new standard.",
            "We can adapt and refine things and figure out how to do this stuff.",
            "And then we're going to merge it back together, probably around open MP 4.0, so we'll see.",
            "Gone off this new fork for directives called Open ACC will get stuff working and then will merging back together.",
            "And for those of you that are open MP programmers.",
            "By the way, this is all seem very very familiar.",
            "Have gone along right because it is just.",
            "It's the same type of approach in philosophy.",
            "So what's the difference?",
            "Well, the difference is one of the things that I've mentioned several times.",
            "Is that with the benefit of a decade of experience over open MP, they were able to assume that the compiler could do some smarter things, and so hence the fact that it will take responsibility for not.",
            "Doing things with the Kernel Command as opposed to open MP, it will always do your directives and do and very often that's the source of problems with open MP code is that you did something you ignore dependency.",
            "For example open MP does not care that you overlooked.",
            "Independency will go ahead and paralyze a loop and give you incorrect results open.",
            "ACC will balk at these things, so it had the benefit of saying let's assume the compiler is a little bit smarter about doing things and that was a philosophy that was more of a philosophical kind of thing, but more.",
            "Directly open ACC was built to deal with devices that could spawn thousands of threads very easily."
        ],
        [
            "And each thread there's a tiny bit of work open.",
            "MP is meant to deal with a handful of threads which you hope live pretty long time because starting a thread on a core takes many thousands of cycles just to start it up.",
            "That's the overhead of creating a threat so you don't create an close threads Willy nilly on a processor.",
            "As I said, the appropriate place for a thread with the browser might be a tab where you open a tab it lives for awhile and then you close a tab.",
            "That's a great use of a threat.",
            "A good use of a thread on an open ACC.",
            "Is for one iteration of a for loop.",
            "You know one inside loop on a for loop.",
            "That's a great threat, so there's a very big difference there in the way you approach things.",
            "So these things show up at in practicality as they were implementing standards, however, allow."
        ],
        [
            "The way something important happened and that is that Intel decided that accelerators were actually real and we're not a fat or a joke, just some videos.",
            "Some heroic programmers using video game cards, but this was an important part of the future and actually a threat to the way people were doing standard.",
            "You know, multicore computing and Intel's answer to a lot of technological challenges is how do we do this with X86?",
            "Whatever somebody else is doing, how can we do this with our our our processor?",
            "For those of you run along time back in the 90s when it was first threatened by RISC processors coming on the scene.",
            "Intel said, oh, this is, you know, new and effective way of doing stuff.",
            "Well.",
            "They just rolled all the risk into Intel and now Intel processors recession risk processes behind the scene.",
            "In this case here.",
            "They said, how can we respond to these thousands of cheap cores that Invidia knows how to produce and they're very good at?",
            "And that's not our game.",
            "How can we just say six?",
            "Well, how about if instead of giving you thousands of cores, what if we gave you 50 or 60 cores on a single chip?",
            "They're not.",
            "They're not as many cores on a GPU, but they're smarter.",
            "These cores look a lot more like a CPU core.",
            "As a matter of fact, they are a CPU core and the original way that people usually represented.",
            "As they said, they're like a Pentium core from back in the late 90s Pentium processors, so we're going to give you 60 Pentium processors on a single chip, and that's what Knights Corner became, so that's a nice corner right there that you will find in Stampede these days."
        ],
        [
            "And it is essentially 60 Pentium processors on a device.",
            "Now, that's not really a fair representation of it.",
            "We shouldn't say that their Pentium processors Pentium processors were not particularly effective at math, whereas these things were built to address the HPC community first and foremost.",
            "So they really beefed up the math processor a whole lot.",
            "They put a very wide vector processor in them.",
            "Much better vector instructions than an old Pentium chips, and as a result one of these things can do about a teraflop altogether amongst all of the.",
            "The."
        ],
        [
            "First, they are in originally hooked together.",
            "In the existing version, the kind you'll find in Stampede have a ring bus connecting all these processors, so they share their memory.",
            "So there really a little shared memory device shared memory node with the ring bus connecting everything with 60 of these independently.",
            "Not impressive processors, but hopefully all 16 gang together are very effective now than."
        ],
        [
            "Version of this that just was announced that has rolled out and we got our first one at Pittsburgh Supercomputing Center last week and I can't wait to get back and unblocks the thing because it is interesting now has a fairly different actual architecture than the 1st generation.",
            "It's got a grid based connection Alligatoring bus.",
            "It uses 3D memory which means you stack memory on top of memory which gives you with the vias that allow you to the memory to connect through to the bottom.",
            "It now gives you much much.",
            "Better bandwidth and latency than having a bunch of memory spread out over a border around the edges, and it is also designed to have on the path go directly through the chip eventually.",
            "On the past the network like we have on bridges, that being the newest kind of Intel's challenge to InfiniBand InfiniBand is previously as a commodity network, kind of the network of choice for building a big shared memory machine, so your choices were essentially InfiniBand, or a proprietary network like Cray might put in the machine, while Intel bought crazed technology.",
            "Off of them areas technology while you want to turn it on the path so bridges is actually machine.",
            "Using bridge is the first world's first large scale deployment of Omni Path actually rolled out there, yes?",
            "Is.",
            "Connection between the internal visit only author.",
            "You can imagine it's on the path so bridges is on the path all through.",
            "It is the first big machine built with Omni path, yeah.",
            "Since since then a few more have come online because Intel is aggressively pushing this and InfiniBand is pushing back, which is always good to have competition, but at any rate the Knights landing is built to take InfiniBand right to the chip, so this is their new version of their accelerator.",
            "So now why is this of such great importance to the open ACC versus Open MP issue?"
        ],
        [
            "The reason is that open MP was again intended to merge open ACC and open and Open MP back together in the 4.0 version.",
            "However, the feud which is the most polite way to put it between Intel and NVIDIA, you know they're competitive haggling meant that they both had an incentive to kind of push the spec their own way, and video was happy to make sure that open ACC first and foremost accommodates GPU's with their small simple cores.",
            "But many thousands of them.",
            "Off Intel was much more interested in making sure that things reflected there are open MP model a fairly complex scores were fairly complex threads and so they both done a good job of trying not to make things compatible, and so they have not come back together.",
            "So with open MP4, what you have now are directives that look like this one right here, which looks well.",
            "Explain it to you and then you'll see the great similarity of what you already know.",
            "So this right here is the open MP4 directive for doing data management.",
            "Up until now, open MP is no data management because open MP was built to address shared memory nodes.",
            "There was nowhere for data move.",
            "Open MP was built for course sharing memory, so why would you move data that all shared memory?",
            "Now we've got these accelerators where data needs to move so with open MP4 we introduced data management directives and here is the data management directive that corresponds basically to our our copy directive.",
            "It's a target directive, so these are the open it whoops, these are the open MP pragmas.",
            "That we've.",
            "That look again, weather you know.",
            "Whether you know open MP or not, this looks very similar to open ACC Directive.",
            "It's a pragma OMP instead of pragma ACC.",
            "This says we're using open MP now.",
            "We have a target at device that's a little bit more elaborate than what open ACC does.",
            "Open ACC.",
            "Again, it's kind of assumes it's modern, so it assumes that the device you're using is is just the accelerator you got plugged in.",
            "So by default we don't have to specify every single time we're moving data to an accelerator and open MP you targeted device.",
            "Device Zero is the zero with accelerated got plugged in.",
            "But here is the great.",
            "Here is the great similarity between open ACC, an open MP, the ridiculous.",
            "I should say similarity between the two.",
            "That shows you this is more of technical, more political feud, the technical feud.",
            "This right here is equivalent of a copy command instead of having copy two and copy from, we have our copy and copy out.",
            "Excuse me, things are so similar that they confuse me since I start talking about him having copying and copy out.",
            "We have moved or should be mapped.",
            "Two an map from there, exactly the same thing.",
            "And by the way, instead of copy which does a copy in a copy out, we have two front, so this map to from with the array B.",
            "That's would be isn't there is just a variable this map to from would be is the exact same thing is saying copy B, so the similarities in the analogs are just simple minded and."
        ],
        [
            "Here's an example of a code that runs the Saxby loop again that we've all seen with open MP4 on an NVIDIA device.",
            "This looks a little bit more complicated than just using Kernels Command an it is because kernels takes responsibility for doing everything in open MP.",
            "Instead, we have to take responsibility for a bit of it and more importantly and more to the point, we have a whole lot of this messy stuff here about teams and how to distribute them, and this is the equivalent of open ACC.",
            "More advanced stuff that we've looked at with worker Vector gang, right with worker Vector Gang we've looked at.",
            "We recognize that we're responsible for work taking responsibility.",
            "We can neglect it and let it do the default thing, but we're taking responsibility for how to break the work up into pieces with open MP 4.0, it becomes kind of necessary on a GPU to start to say how to distribute things in pieces.",
            "So that's where these things show up."
        ],
        [
            "Here's a little bit better compare and contrast example right here.",
            "Here's open MP4405, which is not too bad, right?",
            "This is pretty clean right here.",
            "We're saying target device zero and Arabie gets a copy in a copy or its equivalent.",
            "Just copy B and then we say here's a parallel for loop and this is standard open MP, so that does the right thing.",
            "Here is open ACC doing the right thing here.",
            "It just says hey, here's a kernel.",
            "Do the right things and it works fine.",
            "Here is when we start trying to apply open MP to an NVIDIA GPU.",
            "Things get a little bit messy here needlessly so."
        ],
        [
            "You might think here is one way you can do it.",
            "Here's the ifdef Ugly Ifdef solution to doing this where you're trying to support multiple architectures with open MP, where again the fact that Intel and NVIDIA have pride things apart as much as possible shows up in terms of the directive so."
        ],
        [
            "Which way to go these days?",
            "It is still the slide I keep waiting for the slide to become obsolete to where things finally come together and work.",
            "But at this point in time open MP4 is what Intel very much prefers you to support their zone 5 devices with and hence the Intel compiler works just great with open MP4 and 4.5.",
            "The latest version of the spec, which isn't nearly as interesting and increment, but they hope with open MP 4.5 in their compiler that you see on Fi.",
            "Processors NVIDIA instead and hopes that the open ACC compilers and there are multiple companies working powers that you use them to support to use NVIDIA process."
        ],
        [
            "So at this point in time, it simply comes down to which device is most interesting to you right now.",
            "So if you really interested in Scion five, you think your problems can work there well, or you've got access to them or whatever, then you're going to be using open MP4 if you think that NVIDIA GPU's are most interesting to you, because again you got access to them, or you just want the best odds of just landing on a random machine out there with accelerators, it's going to be NVIDIA.",
            "Then you're going to use open ACC and that sounds awful that I've got to make this choice based on no good technical merit.",
            "But at the end of the day, the two are such simple minded mappings from one to the other that it's more an exercise in editing to move back and forth.",
            "And it is like I've gotta change my algorithms or rethink things.",
            "It's mostly editing as a matter of fact.",
            "Where is Galen?",
            "There he is in the back.",
            "So Galen is maybe the world expert are certainly part of the team of world experts at this point in time.",
            "And what the state of affairs is.",
            "These guys just wrote a paper which I guess won't won't be publicly available until when.",
            "Exceed 6, which is right around the corner.",
            "So in a couple of weeks that exceed 16 their paper, which involves automatically converting between one to the other, or I guess going mostly in the open ACC to open MP direct direction, will explain the actual practical situation at the moment in doing that and making that work with various compilers and whatnot, and that is wonderfully enlightening because, again, the situation is needlessly complex and messy at this point in time, but not hopeless in the sense that.",
            "Again, you make this huge investment one way, and then you're screwed, yes.",
            "Yeah, maybe we use.",
            "You can use use for open MP as well, right?",
            "Yes, absolutely open MP is meant to target accelerators and so absolutely it's meant to support GPU's.",
            "But as a practical matter you're going to get much better support out of an open ACC compiler and again, get Galen will just can discuss the exact details of the current state of various compilers and how well they support different things.",
            "He's going to right now as a matter of fact please.",
            "So the only current open MP4.",
            "Compiler that supports NVIDIA is the one that you get when you buy a Cray.",
            "It's really cheap, it's almost free.",
            "It's just Cray dollars that you have spent so.",
            "So it turns out it's really expensive if you happen to own a Cray urine.",
            "I've got one.",
            "He's got one.",
            "You're in good shape.",
            "Tyler is looking to break the bank agreement.",
            "So you playing with.",
            "I don't know about claims work for that yet, but I do know that Cray is supporting it and they are.",
            "Socrate whether they made the right decision or not, they have decided to go with open MP4.",
            "And they they support open ACC at its current standard.",
            "But they're not going to do the next standard.",
            "I don't know why that's just what they decided.",
            "So you have, like I say, the practical consideration that if you're going to use NVIDIA GPU's, you should, if that's if that's where you think you're most likely to land in the immediate future, you should probably go with the flow you know which at this point in time is in video saying use open ACC if you're going to use Intel devices, probably go with the flow and not that you have actually an option.",
            "This point, it's open MP and again it's aggravating to me from a technical perspective because they're so similar that there should be one stage.",
            "The standard should have been merged together.",
            "The flip side is they're so similar that it's not a nightmare.",
            "Like I say, where if you invest going one way and then decide that you need to support the other as well, jumping onto the other track is mostly again and exercising and compiling, and these guys have done an automatic conversion thing themselves already, so it gives you some idea that it's again it's not reach.",
            "It's not changing your algorithms up and rethinking the entire problem.",
            "It still comes down to data management and avoiding dependencies and loops.",
            "So it's it's again.",
            "It's interesting enough discussion that those of you that have specific concerns again talk to me and best thing of all is to talk to GAIL in particular, 'cause he can.",
            "He can tell you the state of the art right now in various compilers doing various things, but be aware of the fact that it's in flux, but so if you'll confuse yourself, you just say well, yes, open MP supports everything and open ACC supports everything.",
            "That is true on paper.",
            "Reality is a little bit."
        ],
        [
            "A little bit different, so one of the things that I keep mentioning is that both companies hope that we're going to go hostless becausw of.",
            "Obviously, Intel would prefer you spend all of their money with your money with them.",
            "An NVIDIA resents you having to buy Intel processors to use your NVIDIA cards, so they would like you to just buy their devices and that your computer is just a room full of racks, full event video cards or racks full of Zion, Fives and they both have very serious commitments to this strategy.",
            "And they both have Rd Maps that are making this happen in the near future with Intel.",
            "They just publicly announced with Knights Landing.",
            "How that's going to happen with the next generation ship where it will be hostless with NVIDIA.",
            "I'm under NDA still so I can't say anything, but I I I can't tell you it's on the road map 'cause that's public and I can tell you that they're not going to let Intel get very far ahead of them.",
            "So NVIDIA likewise has a plan that has involves no CPU at all.",
            "And as you've seen here with open ACC, the idea is that you can move everything onto the GPU and.",
            "And minimize the amount of CPU time that you have the so that's."
        ],
        [
            "That is the new that's near in the future than you might think.",
            "Some things we didn't mention here.",
            "We did not talk about open CL whole lot here.",
            "One of the reasons is that if you don't know anything about an open CL might have given you the impression that it's a high level.",
            "It's some kind of alternative to open AC or open up here.",
            "It's not.",
            "It is an alternative to CUDA Open CL was the response to the community of saying Nvidia's CUDA is great, but it is owned lock, stock and barrel by NVIDIA.",
            "It is a proprietary standard that makes people uncomfortable.",
            "So open CL emerged as an alternative to the CUDA Praya Terry Standard.",
            "Now Interestingly, NVIDIA supports it pretty well.",
            "Actually they have supported it pretty well.",
            "It's never worked as well as as CUDA at.",
            "You know, for efficiency, but it's not been orders of magnitude off either, so NVIDIA hasn't treated terribly, but the two big supporters for it were in the Khronos Group, actually deficient supporters for it in the Khronos Group, which formed the standard included NVIDIA as well.",
            "So in videos actual official supporter of it, Intel at AMD and they all had this.",
            "Those other two parties you can imagine back when CUDA was the only game in town at a very strong interest in supporting open CL.",
            "Because it kind of kept them in the game.",
            "However, things have changed, and in particular Intel for reasons that you now completely understand has decided that open MP is the way forward and using accelerators because they want you to use their accelerators and open MP matches their accelerators so they're not huge supporters of open CL AMD, which you might have imagined as the odd man out were certainly big fans of open CL.",
            "It kept them in the game.",
            "However they have a very different idea here, which is a future.",
            "Where a Fusion architecture in the future and actually just now.",
            "And this is actually an existing hardware.",
            "Let's because normal AMD would like to have your memory and your graphics processor in your processor all residing in the same place, so there's no memory management or movement issues.",
            "And you might say, well, that's obvious design.",
            "Everybody should be doing that.",
            "Why does anybody have this?",
            "Remember, the reason that GPU's have graphics memory in them instead of regular memory is that the very very different architecture of the graphics memory.",
            "Allows you to have extremely high bandwidth, latency tolerant memory versus the kind of memory that you want to put in the CPU, where high latency sitting there waiting on memory accesses.",
            "Stall everything out and so there's a reason that there are two different types of memory.",
            "AMD just kind of take to approach it.",
            "We're going to ignore that reason and just put everything in the same place and as a result they pay a penalty in that they're floating point performance in their bandwidth to accesses the memory are greatly handicapped by the fact that the processor is sharing the same memory, so.",
            "They have this Fusion architecture whether or not you think it's a good compromise or trade off isn't as germane.",
            "Here is the fact that the cause of that they don't care about open CL now either.",
            "'cause open CL's and what in concern with memory management, moving data back and forth.",
            "They prefer now that use their new interface H essay, which is all about disputing architecture, so AMD is no longer that.",
            "They're all still official supporters, but AMD is no longer really concerned about spending a lot of resources in open CL.",
            "Intel's no longer concerned about it.",
            "NVIDIA was, you know, did an admirable job, but probably we just assumed that the thing didn't exist and as a consequence open CL isn't getting a lot of resources at the device driver and engineering level from the big hardware companies.",
            "And while that's not fatal to a lot of open source projects like say, good new compilers, becausw Intel architecture isn't radically changing every 14 months.",
            "So the open source community can support compilers pretty well even if you don't have engineers and better than Intel, you can kind of respond after the fact and keep up with things.",
            "And yeah, the Intel compiler is always a little bit better and more efficient, but good news not.",
            "Completely moot, so that works doesn't work so well in an environment where every 14 months drastic new hardware is coming out.",
            "It's all patented and proprietary and you're dependent on device drivers.",
            "So if the companies aren't supporting it with engineers in House, it's hard to keep up.",
            "And so open CL has had a performance gap that's this widen considerably and I don't personally see why that's not going to continue given again that the current circumstances, so it makes it increasingly bleak long-term picture for open CL.",
            "And that's that's just the way it is.",
            "The Direct Compute Library from Microsoft is an alternative, but it's not really HPC oriented, so it's not really interesting for us.",
            "For straight thread use, so not so concerned with accelerators, but still since I'm talking about these alternative things, if you're opening programmers, you've got thread libraries like C++ app or threading building blocks, or silk from Intel.",
            "They're very C++ oriented anyway.",
            "They're not really Excel your libraries, but so I just stick them in here because there are there terms that come up in the context of this discussion you might think are they alternatives?",
            "So they're not.",
            "So again we have.",
            "Is anybody in the room here using something else?",
            "You're wondering how it fits in the picture.",
            "This is a good time to ask those questions.",
            "I think I've covered most of the bases here, Yep.",
            "Some time ago.",
            "Of what we have we have we have we wish to capture all of your questions on our microphone here so.",
            "So some time ago I heard about Spur, which was supposed to be an intermediate language which promised to be very universal and to allow even open CL to reach almost the performance of CUDA.",
            "Do you have any updated news about it?",
            "Say that?",
            "Again, what was it?",
            "If I remember correctly, it was spur SPIRSPIR.",
            "No that is not ringing a Bell with me.",
            "Anybody else in this room here of this so well by the current group I think.",
            "So I can come, but I think the big thing of difference there is Opus.",
            "Heel is actually fine.",
            "But the big problem with challenges we've all spent so much time optimizing our codes to the hardware because they were earlier.",
            "We typically get 7580% the performance of CUDA when we use open CL.",
            "And again this is on NVIDIA optimized hardware.",
            "So typically we have better value for money and name the hardware.",
            "So if I were to start over today, I would probably get open.",
            "Seal is probably the better framework for long-term.",
            "The only sad thing is that there is so much better documentation for CUDA if you're getting started with programming.",
            "Can I add there is 1 unit where are go bolts?",
            "We were we actually tried TBD quite a lot of years and the same thing with TB is that it doesn't come close to open MP.",
            "Likely because Intel is optimized TV before Photoshop for web servers or whatever to get responsiveness not scaling, but arguments look really interesting.",
            "I don't think it has officially been released yet.",
            "Yeah, I mean, they're targeting an application space and desktop application world has very different patterns of usage than what we're talking about here.",
            "It's much less numerically oriented, much more towards the kind of things you see in.",
            "You know video game or a word processor?",
            "OK, well then let's move onto.",
            "Question.",
            "Yeah, this may be just a formality, but other intentions do at some point stop calling GPU's GPU's which they are not.",
            "Balance the the actual terminology which you see sprinkled throughout.",
            "My talk is to call everything accelerator, so that's what I should be calling everything just an accelerator.",
            "But now and NVIDIA engineers who are very well aware.",
            "The guys who make the NVIDIA stuff which they know isn't the GPU still constantly call them GPU's.",
            "And these are the guys who should be offended by the use of the term 'cause they spent all this time making this number crunching double precision device and they call it GPS casually.",
            "So I think that term is here to stay until.",
            "And just becomes probably well after it's obsolete.",
            "Nobody can even remember GPU stands for will still be calling GPU's.",
            "OK, another question.",
            "Yeah, so regarding open.",
            "Please correct me if I'm wrong, but as far as I know is the only one that allows to have it.",
            "Russia news devices that run at the same time without changing the cover, anything right.",
            "Also regarding open CL, AMD has initiative called Boltzmann Initiative, so each product, CUDA program or whatever you can translate into opens here.",
            "Finally someone is interested in running the cooler.",
            "50 exercise I think.",
            "I found a way to fix this.",
            "It was just a matter of finding the right modules.",
            "I hope nothing more common.",
            "Yeah yeah.",
            "And then you need to export the LD library fast.",
            "OK, well thank you for for that effort.",
            "OK, yes.",
            "Are they not in the same spot?",
            "OK, they they will.",
            "The slides for this will appear in the same spot.",
            "Then I'm not sure why they aren't, but I'll make sure that they do OK. Well, Fortunately there's been a lot of dense coding in the stuff that's ahead of us here, so forgive me for not having him out there.",
            "You won't miss a whole lot.",
            "We won't have to refer them for exercises or anything.",
            "OK?",
            "Well then I'll move on to the hybrid programming, although this discussion again, there's no reason not if you.",
            "If you think of points relevant to your code later, just jump in.",
            "We're not.",
            "We're not segmenting our topics here formally, so let's talk about hybrid programming though, because that is, I hope of great interest to many of you because of modern machines are built up of these heterogeneous pieces.",
            "They've got accelerators, and they've got processors which always have multiple cores, and for most of you sticking, a lot of those together in a room or using a machine where somebody's done that in great expense is interesting.",
            "So we should know how all these things interoperate and the good news is that these standards committees open ACC, Open MP, and MPI committees.",
            "Recognize and have always recognized this fact, so they meant things to interoperate very well, and they do in the most logical way.",
            "If you understand how these pieces work separately, the way they fit together makes perfect sense.",
            "You don't need a separate document to describe how this interacts with that.",
            "I mean, it's actually is in the spec, but it all works exactly how you should think it works.",
            "If you understand the pieces, but will step through some examples here, I am assuming that you guys all know basic MPI because that was kind of a prerequisite for this whole thing, although after.",
            "Yesterday's quiz, it is clear that that is not entirely true, and that's not really a surprise to me.",
            "Although this this group Interestingly scored better than the MPI Group on the MPI quiz, so I'm not sure what to make of that.",
            "But at any rate, I will assume that you know basic MPI at least, and we're not doing anything too fancy here, and this will also be for those of you that are interested in doing the hybrid challenge.",
            "Pay close attention, and I know that's a fair fraction of this group, which I'm glad to see last year was a lot of fun.",
            "And by the way, over lunch I just realized that since David just grab David's running it this year, so he's the absolute guy to talk to about the rules, but he just grabbed my rules from last year was the first year we did it, and I see grab my rules verbatim, which means collaboration is also perfectly legal.",
            "I might even say encouraged.",
            "So if you're somebody that doesn't know MPI really well, and you think that's going to be a handicap to you, because certainly whatever the winning solution is, it's going to use the rules.",
            "Or you can use up to four nodes of bridges.",
            "It's going to use for nodes.",
            "Bridges now whether it uses 4 nodes, well with open ACC or with open MP or whatever, that's an interesting an outstanding question, but you're definitely not going to win the competition.",
            "Running on one node.",
            "So MPI is going to be the basis for whatever the winning solution is.",
            "If you don't really feel comfortable with MPI and you think that's going to disqualify you from having fun with the competition, find somebody that does.",
            "As a matter of fact, if you feel like you really are getting the gist of open ACC, find some MPI person.",
            "It doesn't.",
            "And two of you may well be the winning team, so collaboration again is.",
            "Is not just encouraged, but it's the solution for any of you that would like to participate, but just don't think your MPI skills are up to par.",
            "Feel free to collaborate, but I'm going to assume you know at least basic MPI, which is nice because it's rarely that I get to make that assumption.",
            "As a matter of fact, the one time a year besides this that I really can make.",
            "That assumption is when I teach everybody MPI first on boot camp.",
            "So the only time I ever get to talk about hybrid programming and depth is our boot camp where I teach everybody open, MP open ACC and MPI in the same week, and then we can really get into.",
            "Hybrid programming in this case I can, I'm going to make that assumption.",
            "So here is, oh, geez, I just violated my what exactly what I told you that not having the slides in front of you will be not a huge handicap.",
            "Well, it might be a little bit.",
            "And I'm guessing that nobody can.",
            "Probably.",
            "How far back can we read this card?",
            "Seeing squinting halfway back already?",
            "So we might?",
            "We might have an issue here.",
            "As a matter of fact, I do have some code from here on out, but we don't have hands on and we're not in a huge rush.",
            "My solution to this looking at this room would be why don't we move down to the front?",
            "If you really, if you if you want to follow along, can we take two minutes and move down to the front?",
            "Don't we don't have to do every other row thing at all, so if you if you really want to know what's going on, move, move yourselves down to the frontier and you'll be able to see."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next thing I want to talk about is using openings to see with CUDA libraries, which if I were inclined to have really long titles, will also say and also interfacing with CUDA code itself.",
                    "label": 0
                },
                {
                    "sent": "So I'll show you right now how to actually allow yourself if you really want to write your own CUDA code, you work with a colleague.",
                    "label": 0
                },
                {
                    "sent": "It writes CUDA code you want to.",
                    "label": 0
                },
                {
                    "sent": "At some point you decide open ACC isn't cutting it in this particular piece of the code, you can write a little bit of CUDA code and interface with it very easily with open ACC.",
                    "label": 0
                },
                {
                    "sent": "As a matter of fact, that's essentially what we're doing.",
                    "label": 0
                },
                {
                    "sent": "We're using libraries.",
                    "label": 0
                },
                {
                    "sent": "The vast majority of openev.",
                    "label": 0
                },
                {
                    "sent": "Of libraries for GPU's were painstakingly optimized by somebody writing in CUDA at a low level, and so you get the benefit of all of that.",
                    "label": 0
                },
                {
                    "sent": "So, as I mentioned before, there are three different ways you can use a GPU, so we can either program at the low level we can program at high level with open ACC, which is what this is all about.",
                    "label": 0
                },
                {
                    "sent": "We can use libraries like this, or we can interface to CUDA code at a low level.",
                    "label": 0
                },
                {
                    "sent": "So let's take first of all the opportunity here for me to not give you a comprehensive survey of CUDA libraries.",
                    "label": 0
                },
                {
                    "sent": "Because they are many, there is an immense amount of of the whole ecosystem for for NVIDIA in particular is large, and by that I'm including debugging tools and performance profiling tools and whatnot, but most important is the library base, which is enormous.",
                    "label": 0
                },
                {
                    "sent": "There are CUDA libraries to do all of the obvious things like you might think of like matrix multiplies and image recognition.",
                    "label": 0
                },
                {
                    "sent": "Things that you might think that or image enhancement and processing that you might think of GPU would be good at.",
                    "label": 0
                },
                {
                    "sent": "There are also.",
                    "label": 0
                },
                {
                    "sent": "Large amounts of their large numbers of libraries out there to do things that should impress me.",
                    "label": 0
                },
                {
                    "sent": "At least that somebody managed to get the GPU to do effectively.",
                    "label": 0
                },
                {
                    "sent": "Knowing what I do about it, their architecture and whatnot.",
                    "label": 0
                },
                {
                    "sent": "So the very first thing you should do before you start writing code in a particular application is you should Google and again, Google is your friend on these kind of things.",
                    "label": 0
                },
                {
                    "sent": "You should Google and say is there a library that does this already or does a big does a large chunk of this?",
                    "label": 0
                },
                {
                    "sent": "And if there's a library that accomplishes.",
                    "label": 0
                },
                {
                    "sent": "Most of what you want to do, you'd be foolish not to use it.",
                    "label": 0
                },
                {
                    "sent": "That's general programming, right?",
                    "label": 0
                },
                {
                    "sent": "Not just open ACC.",
                    "label": 0
                },
                {
                    "sent": "And don't you can go to say NVIDIA.",
                    "label": 0
                },
                {
                    "sent": "NVIDIA does a nice job of highlighting all of the CUDA libraries that are out there, but they're not entirely comprehensive and thorough, and so there are lots more things hiding out there on GitHub and whatnot.",
                    "label": 0
                },
                {
                    "sent": "So again, Google will find most of those things, so Google before you start writing code, and that's what should be obvious to you if you're writing serial code and you found yourself writing some kind of matrix factorization routine, you probably take hesitate for a second ago.",
                    "label": 0
                },
                {
                    "sent": "Hasn't somebody already done this for me?",
                    "label": 0
                },
                {
                    "sent": "Well, you should absolutely do that with CUDA.",
                    "label": 1
                },
                {
                    "sent": "So I won't go through all of these.",
                    "label": 0
                },
                {
                    "sent": "There's a little appendix on this on this talk that has some of the things that are out there, but again, it's it's not remotely comprehensive.",
                    "label": 0
                },
                {
                    "sent": "So let's instead look at what is interesting here, which is how to use those libraries, and basically all of the magic is in the fact that using a clause that will find device pointer.",
                    "label": 0
                },
                {
                    "sent": "Here we can tell CUDA or excuse me, we can tell open ACC that this is a pointer to some data on the GPU, so it's already on the GPU.",
                    "label": 0
                },
                {
                    "sent": "And now in the data is probably on the GPU 'cause you called some CUDA library and it left some process data there.",
                    "label": 0
                },
                {
                    "sent": "Likewise, you can tell some CUDA library.",
                    "label": 0
                },
                {
                    "sent": "Hey, here's some stuff I'm working with an open ACC and I would like you to operate in that data.",
                    "label": 0
                },
                {
                    "sent": "So what we'll do here is a simple example that does a.",
                    "label": 0
                },
                {
                    "sent": "So here's the pragma and we'll just see it in action right here.",
                    "label": 0
                },
                {
                    "sent": "We've got a simple example that does a convolution.",
                    "label": 0
                },
                {
                    "sent": "A convolution is basically when you're just, you're mathematically you're integrating over the composition of some functions, but in.",
                    "label": 0
                },
                {
                    "sent": "Digital, it's discrete form.",
                    "label": 0
                },
                {
                    "sent": "Usually what you're doing is you're taking some bunch of data that you have in your applying a small transform to it across the entire data set.",
                    "label": 0
                },
                {
                    "sent": "An very common way of doing that is taking FFT of some data, transform it into its frequency space, apply your own manipulations to it, and then transform it back.",
                    "label": 0
                },
                {
                    "sent": "That's a very common thing to do for many, many things, not just image processing, but also lots of things that involve differential equations can be done this way for reasons that are mathematically interesting.",
                    "label": 0
                },
                {
                    "sent": "However, for those of you that don't use out of teas and don't care about them at all, think of this just simply as a very popular math library which is written in CUDA runs very effectively on GPU's and we are now going to utilized for more open ACC code.",
                    "label": 0
                },
                {
                    "sent": "We're going to interact with our open ACC code, so the steps of a convolution or do an FFT.",
                    "label": 0
                },
                {
                    "sent": "That's step one here #2, which is to do some monkeying with the data, which in this case we're going to apply our data and scale it some, and that's a useful thing to do.",
                    "label": 0
                },
                {
                    "sent": "For convolution we're going to open ACC and then we're going to send it back for another 50, yes?",
                    "label": 0
                },
                {
                    "sent": "Oh, what is happening here?",
                    "label": 0
                },
                {
                    "sent": "We probably have not started the slideshow and.",
                    "label": 0
                },
                {
                    "sent": "I regret.",
                    "label": 0
                },
                {
                    "sent": "That this is odd.",
                    "label": 0
                },
                {
                    "sent": "We are completely decoupled between what my laptop is doing and what is on the screen.",
                    "label": 0
                },
                {
                    "sent": "Let's try that again.",
                    "label": 0
                },
                {
                    "sent": "And I'm gonna guess that's been the case since I started talking.",
                    "label": 0
                },
                {
                    "sent": "Is that right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK. You are way too accommodating of a group.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this will maybe have a little more context to it so.",
                    "label": 0
                },
                {
                    "sent": "It's a relief.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fortunately, you didn't miss anything too critical here.",
                    "label": 0
                },
                {
                    "sent": "This was just backing up my point that we have now with open ACC.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to be able to interface between libraries as well as CUDA code in CUDA routine.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This was when I was talking about the immense number of libraries that are out there available.",
                    "label": 0
                },
                {
                    "sent": "This is what we've got just as a sampler here, but again.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Google is your friend and will show you all kinds of stuff out there.",
                    "label": 0
                },
                {
                    "sent": "As a matter of fact, a good thing.",
                    "label": 0
                },
                {
                    "sent": "I think I didn't have on here is the latest version of the CUDA capabilities comes along with a tool called Envy Graphene Videos Graph library.",
                    "label": 0
                },
                {
                    "sent": "So there are all kinds of graph oriented retains and this came up over lunch just out how many people in this room do do graph type operations and use graph libraries and whatnot?",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's there's always a decent minority of people out there, so graph operations or something that seems to those of you that are used to the bigger raise we've been talking about bigger raise so far of data to apply or.",
                    "label": 0
                },
                {
                    "sent": "R do loop so we got to do in the for loops.",
                    "label": 0
                },
                {
                    "sent": "Graph operations are more like chasing pointers through memory and so there are very different way of organizing data and usually scare people that are used to working with big arrays of data and they should because it's sometimes very difficult to optimize those kind of operations.",
                    "label": 0
                },
                {
                    "sent": "That's the kind of thing that knowing what you know so far about GPU's you might go of course graph operations are not a thing to do on GPU's.",
                    "label": 0
                },
                {
                    "sent": "All these cores that are synchronized and doing the same things will clearly not be effective on graph operations.",
                    "label": 0
                },
                {
                    "sent": "Turns out that's not the case at all.",
                    "label": 0
                },
                {
                    "sent": "GPU's are very effective.",
                    "label": 0
                },
                {
                    "sent": "And the envy graph library, which has just been released is good because not only does it pull together a lot of the spirit graph routines that are floating around out there, but they're very well optimized.",
                    "label": 0
                },
                {
                    "sent": "So for those of you that Dordogne graph operations, yes, GPU's are for you as well, and actually they're very very effective at a wide classes of graphs.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Operations so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's get onto.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the claws.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, as I said, there's nothing magic about them, but here's the example that we're going to do.",
                    "label": 0
                },
                {
                    "sent": "We're going to take an FFT operation again.",
                    "label": 0
                },
                {
                    "sent": "A very common and useful math operation that you may or may not use.",
                    "label": 0
                },
                {
                    "sent": "We're going to do an FFT on some data.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to mess with the date a little bit, which is what is going to make this actually a convolution, and we'll do that with their own open ACC code, and then will call an FFT routine to transform the data back.",
                    "label": 0
                },
                {
                    "sent": "So again, we're using the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Extremely effective and fast FFT routines with our own our own open ACC code mixed in.",
                    "label": 0
                },
                {
                    "sent": "So here's what's going to happen at a high level and it's fairly straightforward to use.",
                    "label": 0
                },
                {
                    "sent": "The only the only thing that we have to dive down and get a little bit messy with and there's just a little bit of messing us here and the messiness that I'm showing you here is the same messiness will apply to any problem.",
                    "label": 0
                },
                {
                    "sent": "So if you're not comfortable if you're not, if you're a C programmer, I hope that the Malik type routines or memory allocation are second nature to you.",
                    "label": 0
                },
                {
                    "sent": "If you're a Fortran programmer, maybe you've managed to skirt these issues.",
                    "label": 0
                },
                {
                    "sent": "And perhaps not, perhaps you had to deal with memory allocation not in in the elegant new four tramway, but in dealing with C routines.",
                    "label": 0
                },
                {
                    "sent": "At any rate, all we're doing is we're telling we're setting up our data structures before hand, with memory allocation routines, and they're fairly straightforward.",
                    "label": 0
                },
                {
                    "sent": "Here we have a couple of different we have.",
                    "label": 0
                },
                {
                    "sent": "In this case, we have a data that lives on the host, and the data lives on the device.",
                    "label": 0
                },
                {
                    "sent": "Again, host being the generic term for CPU device being the generic term for accelerator or GPU.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "Here we have our complex whoops we have our.",
                    "label": 0
                },
                {
                    "sent": "We have our come on don't fight me on this.",
                    "label": 0
                },
                {
                    "sent": "Pardon me while I fight with.",
                    "label": 0
                },
                {
                    "sent": "This thing once again.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, so in this case here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have first a couple of allocation routines were on the host.",
                    "label": 0
                },
                {
                    "sent": "We need to set up our memory so when the data lives on the host it's dynamically allocated memory that we set up in advance, and we're using a couple of Malik routines against any C programmer.",
                    "label": 0
                },
                {
                    "sent": "This is very familiar to you.",
                    "label": 0
                },
                {
                    "sent": "Fortran programmers you have to deal with with this very simple and straightforward way.",
                    "label": 0
                },
                {
                    "sent": "We're just saying give me a couple of buffers in memory.",
                    "label": 0
                },
                {
                    "sent": "Then we also do a very similar looking thing on the GPU, which is new to you.",
                    "label": 0
                },
                {
                    "sent": "If you are a C programmer.",
                    "label": 0
                },
                {
                    "sent": "Again, looks kind of familiar, but you know it's a variation on it were doing.",
                    "label": 0
                },
                {
                    "sent": "The CUDA memory allocate with the CUDA malloc routine right there, so with CUDA Malik we're going on the device.",
                    "label": 0
                },
                {
                    "sent": "They set up the data.",
                    "label": 0
                },
                {
                    "sent": "This is where our data is going to live.",
                    "label": 0
                },
                {
                    "sent": "So once we do this now, we can point to these regions at the appropriate time when we want to refer to the data that's in these regions where.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's on the host of the GPU, so here's the actual part of the source code that does all the fun stuff and you have in your exercises directory the full source code.",
                    "label": 0
                },
                {
                    "sent": "Now I got this excerpt from some of my buddies at NVIDIA and they have a slight different philosophy about training in me.",
                    "label": 0
                },
                {
                    "sent": "They like to do a lot of training with excerpts from big complicated codes.",
                    "label": 0
                },
                {
                    "sent": "I'd like to be able to make sure everybody understands every line of code or loss code.",
                    "label": 0
                },
                {
                    "sent": "Again, you are capable of understanding every line in this case here.",
                    "label": 0
                },
                {
                    "sent": "This code is a couple 100 lines long, even though this is really this another excerpt I'll show you where the interesting parts.",
                    "label": 0
                },
                {
                    "sent": "They write examples.",
                    "label": 0
                },
                {
                    "sent": "In this case, the thing runs the CUDA version and then it also runs a serial version and compares both to make sure that you get the right result and now it's kind of sophisticated, but at the expense of being a little bit bigger.",
                    "label": 0
                },
                {
                    "sent": "So the example sits in your exercise directory, it's called CU FFT, something subdirectory you can go and look at it and play with it.",
                    "label": 0
                },
                {
                    "sent": "It's just a little bit bigger than I feel comfortable going over line by line in a class, so here's the excerpt where the action happens in this case.",
                    "label": 0
                },
                {
                    "sent": "Here we are calling the CUDA FFT routine.",
                    "label": 0
                },
                {
                    "sent": "It happens to have this name.",
                    "label": 0
                },
                {
                    "sent": "They've all got long, funny names that kind of cram all the information and so the CUDA FFT library has a routine called CUDA FFT.",
                    "label": 0
                },
                {
                    "sent": "And then if this is a complex to complex FFT and so we're calling it, we call this on the data.",
                    "label": 0
                },
                {
                    "sent": "That's our signal.",
                    "label": 0
                },
                {
                    "sent": "So we're doing convolution got signal data and then we've got our filter kernel data that we're applying.",
                    "label": 0
                },
                {
                    "sent": "That's what are filters applying to our main data.",
                    "label": 0
                },
                {
                    "sent": "This is what makes you a convolution again.",
                    "label": 0
                },
                {
                    "sent": "So these are two things that live on the device.",
                    "label": 0
                },
                {
                    "sent": "This is the data when it lives on the device.",
                    "label": 0
                },
                {
                    "sent": "And we say in this case here it's already on the device the data is on the device at this point in the code, and we call the FFT, but we want to point it today that lives on the device.",
                    "label": 0
                },
                {
                    "sent": "So this says when you look at when you get this variable right there these signal.",
                    "label": 0
                },
                {
                    "sent": "This is a pointer to stuff that's already on the device.",
                    "label": 0
                },
                {
                    "sent": "Don't worry about migrating data back and forth.",
                    "label": 0
                },
                {
                    "sent": "This is out there when you come back from the FFT routine.",
                    "label": 0
                },
                {
                    "sent": "At this point in the code here we now have transformed data.",
                    "label": 0
                },
                {
                    "sent": "Now we call our own routine and it's written with the.",
                    "label": 0
                },
                {
                    "sent": "The The Nice Compact name of complex pointwise multiply and scale, so that's what are we actually doing.",
                    "label": 0
                },
                {
                    "sent": "It will look at that source code in a second, so this is our own CUDA Skoda torrox use me open ACC code that we've written right here and this manipulates the data a little bit in the way we'll see and then once again at the end we transform the data back.",
                    "label": 0
                },
                {
                    "sent": "So this is a very very straightforward use of an existing CUDA library that we know nothing about.",
                    "label": 0
                },
                {
                    "sent": "So this is a black box library right here that we're calling, but we just need to make sure we call it that.",
                    "label": 0
                },
                {
                    "sent": "It knows that the date is.",
                    "label": 0
                },
                {
                    "sent": "Already on the device, don't try to do anything for us.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the data that's on the device, we can refer to it out there without doing any data management.",
                    "label": 0
                },
                {
                    "sent": "We say don't try to move the stuff onto or off the GPU.",
                    "label": 0
                },
                {
                    "sent": "This is a pointer to stuff that's out there, so we use the clause device pointer clause and say this is a pointer to stuff that's already out there.",
                    "label": 0
                },
                {
                    "sent": "So going to play with it on the GPU, but don't do any data management and so we play with the data here in this routine we can do all kinds of stuff right here, and this is where this could be any open ACC code.",
                    "label": 0
                },
                {
                    "sent": "And it's nice.",
                    "label": 0
                },
                {
                    "sent": "Regular C serial code, right?",
                    "label": 0
                },
                {
                    "sent": "Here we can do whatever we want.",
                    "label": 0
                },
                {
                    "sent": "Works with Fortran too.",
                    "label": 0
                },
                {
                    "sent": "We can play with the data all we want right here and then once again we.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To go to the previous slide, we returned from this monkeying with the data, and then could call the next FFT, and that is the loop that we do here to the data, so it's very.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Straightforward access any CUDA library in equivalent fashion and that could also be CUDA code, so if you're working with some colleague and they've written code code or you as a CUDA programmer either at existing CUDA programmer have code or you need to do to do it for some reason, you can interface with your own field very easily, yes.",
                    "label": 0
                },
                {
                    "sent": "Open CL now so hope it's yeah, we'll we'll talk about open CL a little bit later.",
                    "label": 0
                },
                {
                    "sent": "I think in the advanced or not needed and stuff we covered.",
                    "label": 0
                },
                {
                    "sent": "That must be the.",
                    "label": 0
                },
                {
                    "sent": "No, it's it's still to come somewhere to open it.",
                    "label": 0
                },
                {
                    "sent": "I'll mention it briefly how many open CL programmers do we have here, by the way.",
                    "label": 0
                },
                {
                    "sent": "Or people who deal with open CL a couple.",
                    "label": 0
                },
                {
                    "sent": "OK, so open CL is a thing is an issue in this world, but gotten increasingly unattractive.",
                    "label": 0
                },
                {
                    "sent": "One to deal with so.",
                    "label": 0
                },
                {
                    "sent": "We'll talk about it.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway, so linking this if you actually want to run the example yourself or play around with it linking this is like any other library and that it's always got some complex nonsense to worry about, but it's not particularly profound, so if you want to link with this and run it, you can do it.",
                    "label": 0
                },
                {
                    "sent": "It's very straightforward.",
                    "label": 0
                },
                {
                    "sent": "Feel free to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With it and you run the code and it gives you some output and like I say, this code is is kind of what I think is a little unduly complex.",
                    "label": 0
                },
                {
                    "sent": "It runs a serial version and it runs the non serial version in comparison to make sure they match etc etc, but in essence it's a nice example of just how easy it is to use any of the existing CUDA libraries and interfacing with your own open ACC code so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's really all I want to say about this because there isn't much more to it.",
                    "label": 0
                },
                {
                    "sent": "It's not the profound.",
                    "label": 0
                },
                {
                    "sent": "The little bit of boilerplate code code you have to include is slightly ugly, but it's minimal.",
                    "label": 0
                },
                {
                    "sent": "And what we've done right there with those memory allocations.",
                    "label": 0
                },
                {
                    "sent": "That's about it.",
                    "label": 0
                },
                {
                    "sent": "So you don't have to spend a lot of time trying to figure out what these kudo routines are about.",
                    "label": 0
                },
                {
                    "sent": "Those are the only ones you really have to see, so I've got.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Appendix on here, which used to be kind of interesting to Wade through.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the kind of.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Routines that are out there a lot of anything that's in MKL orb last kind of linear algebra library.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Says they've got versions.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So all this stuff you'd expect.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things out there, but again, increasingly things that you wouldn't expect would.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So well are also out there as well.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's kind of silly for me to go over these.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is this some kind of comprehensive survey of the field?",
                    "label": 0
                },
                {
                    "sent": "Look through the appendix if you want, but more importantly again just say hey what do I wish was out there that there's a serial version of and Google for it and it may be out there and it may not be, but it's not hard to ascertain.",
                    "label": 0
                },
                {
                    "sent": "OK with that.",
                    "label": 0
                },
                {
                    "sent": "I don't want to like I said, well I'm at a whole lot because it's not all that interesting is compared to.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Let me you probably have to load a module.",
                    "label": 0
                },
                {
                    "sent": "The right module on this, since this is a slightly old version, would be let me go back and look at it.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Um, load the koetzle.",
                    "label": 0
                },
                {
                    "sent": "That sounds that sounds like the right thing to do, but there are different versions up, so module.",
                    "label": 0
                },
                {
                    "sent": "By the way, modules on Bridge is an on and hopefully most modern platforms is a nice way to manage all these different version and environment issues.",
                    "label": 0
                },
                {
                    "sent": "Module is simply a way it brings it a bunch of passive environment variables.",
                    "label": 0
                },
                {
                    "sent": "Primarily it could be slightly clever, but most of the time it just sets up all the path environment variables that you have associated with the compiler or library or whatnot, and then also equally importantly, when you unload the module one sets all of those who don't have conflicts, so modules is a great environment.",
                    "label": 0
                },
                {
                    "sent": "I hope again many of you that work on big platforms have been exposed to him already because it beats the alternative on bridges.",
                    "label": 0
                },
                {
                    "sent": "We try to do everything with modules.",
                    "label": 0
                },
                {
                    "sent": "One of the things modules does is have a lot of versions though.",
                    "label": 0
                },
                {
                    "sent": "So in this age especially the age of Python And whatnot, it is important to have every version that's ever existed sitting around because everything is incompatible with other versions.",
                    "label": 0
                },
                {
                    "sent": "In this case here with CUDA versions, if you type, if you do a module avail.",
                    "label": 0
                },
                {
                    "sent": "Did you try that?",
                    "label": 0
                },
                {
                    "sent": "Do a module, just do module avail in general and see which ones of CUDA.",
                    "label": 0
                },
                {
                    "sent": "Are sitting around there.",
                    "label": 0
                },
                {
                    "sent": "If you type one of the commands you can do with modules, say module vouchers.",
                    "label": 0
                },
                {
                    "sent": "You all the modules on the machine and very often for CUDA for example, there should be several versions.",
                    "label": 0
                },
                {
                    "sent": "There's a default one if you just say module load CUDA and then there should be, I hope a couple of alternatives.",
                    "label": 0
                },
                {
                    "sent": "Yes, the 7.5 as default in 8.0 RC also is on there so so I will be glad to help you debug this and anybody else that wants to run this particular example.",
                    "label": 0
                },
                {
                    "sent": "Which modules need to be loaded because I will admit that I haven't actually tried to execute this thing in.",
                    "label": 0
                },
                {
                    "sent": "I months, which is now probably several versions of the PGI compiler and one version of CUDA Library or something.",
                    "label": 0
                },
                {
                    "sent": "So I hope you sort it out there.",
                    "label": 0
                },
                {
                    "sent": "So if you want to run this example, see me in a little bit after we have some free exercise time and I'll tell you what the right modules to load are.",
                    "label": 0
                },
                {
                    "sent": "But yeah, so with with that linkage there having all those versions in it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, inevitably you have to find the right ones, although as I said, with C and Fortran standard libraries, it's usually it's just a matter of finding the right version with typical Python programming stack anymore it could be.",
                    "label": 0
                },
                {
                    "sent": "Two day adventure so OK so the next thing I want to talk about is.",
                    "label": 0
                },
                {
                    "sent": "Right here.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Open.",
                    "label": 0
                },
                {
                    "sent": "Empyre stick pick yes.",
                    "label": 0
                },
                {
                    "sent": "So this one is more, I think, interesting to us here.",
                    "label": 0
                },
                {
                    "sent": "Let's make sure I Max.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing something.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, OK, this is a topic it's showing up continually, not just in here but many.",
                    "label": 0
                },
                {
                    "sent": "Many of my discussions with you guys independently have revolved around this because open ACC as I mentioned you early on open ACC arose because the need GPU computing came to be and the need for some higher level approach then came to be recognized and recognizing the Open MP's directive based approach is not only had has a great history of affecting this but seems to be exactly the right thing to do which I think you're in a position to judge.",
                    "label": 0
                },
                {
                    "sent": "It's pretty elegant way to do this stuff right?",
                    "label": 0
                },
                {
                    "sent": "So open MP has had history at that point and the committee said, well, let's us do this, but because the open MP standard is a very good one of the very serious one, we can't frivolously just start adding stuff into it.",
                    "label": 0
                },
                {
                    "sent": "Let's fork intentionally to open ACC, which will be very quick moving, fast paced, new standard.",
                    "label": 0
                },
                {
                    "sent": "We can adapt and refine things and figure out how to do this stuff.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to merge it back together, probably around open MP 4.0, so we'll see.",
                    "label": 0
                },
                {
                    "sent": "Gone off this new fork for directives called Open ACC will get stuff working and then will merging back together.",
                    "label": 0
                },
                {
                    "sent": "And for those of you that are open MP programmers.",
                    "label": 0
                },
                {
                    "sent": "By the way, this is all seem very very familiar.",
                    "label": 0
                },
                {
                    "sent": "Have gone along right because it is just.",
                    "label": 0
                },
                {
                    "sent": "It's the same type of approach in philosophy.",
                    "label": 0
                },
                {
                    "sent": "So what's the difference?",
                    "label": 0
                },
                {
                    "sent": "Well, the difference is one of the things that I've mentioned several times.",
                    "label": 0
                },
                {
                    "sent": "Is that with the benefit of a decade of experience over open MP, they were able to assume that the compiler could do some smarter things, and so hence the fact that it will take responsibility for not.",
                    "label": 0
                },
                {
                    "sent": "Doing things with the Kernel Command as opposed to open MP, it will always do your directives and do and very often that's the source of problems with open MP code is that you did something you ignore dependency.",
                    "label": 0
                },
                {
                    "sent": "For example open MP does not care that you overlooked.",
                    "label": 0
                },
                {
                    "sent": "Independency will go ahead and paralyze a loop and give you incorrect results open.",
                    "label": 0
                },
                {
                    "sent": "ACC will balk at these things, so it had the benefit of saying let's assume the compiler is a little bit smarter about doing things and that was a philosophy that was more of a philosophical kind of thing, but more.",
                    "label": 0
                },
                {
                    "sent": "Directly open ACC was built to deal with devices that could spawn thousands of threads very easily.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And each thread there's a tiny bit of work open.",
                    "label": 0
                },
                {
                    "sent": "MP is meant to deal with a handful of threads which you hope live pretty long time because starting a thread on a core takes many thousands of cycles just to start it up.",
                    "label": 0
                },
                {
                    "sent": "That's the overhead of creating a threat so you don't create an close threads Willy nilly on a processor.",
                    "label": 0
                },
                {
                    "sent": "As I said, the appropriate place for a thread with the browser might be a tab where you open a tab it lives for awhile and then you close a tab.",
                    "label": 0
                },
                {
                    "sent": "That's a great use of a threat.",
                    "label": 0
                },
                {
                    "sent": "A good use of a thread on an open ACC.",
                    "label": 0
                },
                {
                    "sent": "Is for one iteration of a for loop.",
                    "label": 0
                },
                {
                    "sent": "You know one inside loop on a for loop.",
                    "label": 0
                },
                {
                    "sent": "That's a great threat, so there's a very big difference there in the way you approach things.",
                    "label": 0
                },
                {
                    "sent": "So these things show up at in practicality as they were implementing standards, however, allow.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way something important happened and that is that Intel decided that accelerators were actually real and we're not a fat or a joke, just some videos.",
                    "label": 0
                },
                {
                    "sent": "Some heroic programmers using video game cards, but this was an important part of the future and actually a threat to the way people were doing standard.",
                    "label": 0
                },
                {
                    "sent": "You know, multicore computing and Intel's answer to a lot of technological challenges is how do we do this with X86?",
                    "label": 0
                },
                {
                    "sent": "Whatever somebody else is doing, how can we do this with our our our processor?",
                    "label": 0
                },
                {
                    "sent": "For those of you run along time back in the 90s when it was first threatened by RISC processors coming on the scene.",
                    "label": 0
                },
                {
                    "sent": "Intel said, oh, this is, you know, new and effective way of doing stuff.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "They just rolled all the risk into Intel and now Intel processors recession risk processes behind the scene.",
                    "label": 0
                },
                {
                    "sent": "In this case here.",
                    "label": 0
                },
                {
                    "sent": "They said, how can we respond to these thousands of cheap cores that Invidia knows how to produce and they're very good at?",
                    "label": 0
                },
                {
                    "sent": "And that's not our game.",
                    "label": 0
                },
                {
                    "sent": "How can we just say six?",
                    "label": 0
                },
                {
                    "sent": "Well, how about if instead of giving you thousands of cores, what if we gave you 50 or 60 cores on a single chip?",
                    "label": 0
                },
                {
                    "sent": "They're not.",
                    "label": 0
                },
                {
                    "sent": "They're not as many cores on a GPU, but they're smarter.",
                    "label": 0
                },
                {
                    "sent": "These cores look a lot more like a CPU core.",
                    "label": 0
                },
                {
                    "sent": "As a matter of fact, they are a CPU core and the original way that people usually represented.",
                    "label": 0
                },
                {
                    "sent": "As they said, they're like a Pentium core from back in the late 90s Pentium processors, so we're going to give you 60 Pentium processors on a single chip, and that's what Knights Corner became, so that's a nice corner right there that you will find in Stampede these days.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it is essentially 60 Pentium processors on a device.",
                    "label": 0
                },
                {
                    "sent": "Now, that's not really a fair representation of it.",
                    "label": 0
                },
                {
                    "sent": "We shouldn't say that their Pentium processors Pentium processors were not particularly effective at math, whereas these things were built to address the HPC community first and foremost.",
                    "label": 0
                },
                {
                    "sent": "So they really beefed up the math processor a whole lot.",
                    "label": 0
                },
                {
                    "sent": "They put a very wide vector processor in them.",
                    "label": 0
                },
                {
                    "sent": "Much better vector instructions than an old Pentium chips, and as a result one of these things can do about a teraflop altogether amongst all of the.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, they are in originally hooked together.",
                    "label": 0
                },
                {
                    "sent": "In the existing version, the kind you'll find in Stampede have a ring bus connecting all these processors, so they share their memory.",
                    "label": 0
                },
                {
                    "sent": "So there really a little shared memory device shared memory node with the ring bus connecting everything with 60 of these independently.",
                    "label": 0
                },
                {
                    "sent": "Not impressive processors, but hopefully all 16 gang together are very effective now than.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Version of this that just was announced that has rolled out and we got our first one at Pittsburgh Supercomputing Center last week and I can't wait to get back and unblocks the thing because it is interesting now has a fairly different actual architecture than the 1st generation.",
                    "label": 0
                },
                {
                    "sent": "It's got a grid based connection Alligatoring bus.",
                    "label": 0
                },
                {
                    "sent": "It uses 3D memory which means you stack memory on top of memory which gives you with the vias that allow you to the memory to connect through to the bottom.",
                    "label": 0
                },
                {
                    "sent": "It now gives you much much.",
                    "label": 0
                },
                {
                    "sent": "Better bandwidth and latency than having a bunch of memory spread out over a border around the edges, and it is also designed to have on the path go directly through the chip eventually.",
                    "label": 0
                },
                {
                    "sent": "On the past the network like we have on bridges, that being the newest kind of Intel's challenge to InfiniBand InfiniBand is previously as a commodity network, kind of the network of choice for building a big shared memory machine, so your choices were essentially InfiniBand, or a proprietary network like Cray might put in the machine, while Intel bought crazed technology.",
                    "label": 0
                },
                {
                    "sent": "Off of them areas technology while you want to turn it on the path so bridges is actually machine.",
                    "label": 0
                },
                {
                    "sent": "Using bridge is the first world's first large scale deployment of Omni Path actually rolled out there, yes?",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Connection between the internal visit only author.",
                    "label": 0
                },
                {
                    "sent": "You can imagine it's on the path so bridges is on the path all through.",
                    "label": 0
                },
                {
                    "sent": "It is the first big machine built with Omni path, yeah.",
                    "label": 0
                },
                {
                    "sent": "Since since then a few more have come online because Intel is aggressively pushing this and InfiniBand is pushing back, which is always good to have competition, but at any rate the Knights landing is built to take InfiniBand right to the chip, so this is their new version of their accelerator.",
                    "label": 0
                },
                {
                    "sent": "So now why is this of such great importance to the open ACC versus Open MP issue?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The reason is that open MP was again intended to merge open ACC and open and Open MP back together in the 4.0 version.",
                    "label": 0
                },
                {
                    "sent": "However, the feud which is the most polite way to put it between Intel and NVIDIA, you know they're competitive haggling meant that they both had an incentive to kind of push the spec their own way, and video was happy to make sure that open ACC first and foremost accommodates GPU's with their small simple cores.",
                    "label": 0
                },
                {
                    "sent": "But many thousands of them.",
                    "label": 0
                },
                {
                    "sent": "Off Intel was much more interested in making sure that things reflected there are open MP model a fairly complex scores were fairly complex threads and so they both done a good job of trying not to make things compatible, and so they have not come back together.",
                    "label": 0
                },
                {
                    "sent": "So with open MP4, what you have now are directives that look like this one right here, which looks well.",
                    "label": 0
                },
                {
                    "sent": "Explain it to you and then you'll see the great similarity of what you already know.",
                    "label": 0
                },
                {
                    "sent": "So this right here is the open MP4 directive for doing data management.",
                    "label": 0
                },
                {
                    "sent": "Up until now, open MP is no data management because open MP was built to address shared memory nodes.",
                    "label": 0
                },
                {
                    "sent": "There was nowhere for data move.",
                    "label": 0
                },
                {
                    "sent": "Open MP was built for course sharing memory, so why would you move data that all shared memory?",
                    "label": 0
                },
                {
                    "sent": "Now we've got these accelerators where data needs to move so with open MP4 we introduced data management directives and here is the data management directive that corresponds basically to our our copy directive.",
                    "label": 0
                },
                {
                    "sent": "It's a target directive, so these are the open it whoops, these are the open MP pragmas.",
                    "label": 0
                },
                {
                    "sent": "That we've.",
                    "label": 0
                },
                {
                    "sent": "That look again, weather you know.",
                    "label": 0
                },
                {
                    "sent": "Whether you know open MP or not, this looks very similar to open ACC Directive.",
                    "label": 0
                },
                {
                    "sent": "It's a pragma OMP instead of pragma ACC.",
                    "label": 0
                },
                {
                    "sent": "This says we're using open MP now.",
                    "label": 0
                },
                {
                    "sent": "We have a target at device that's a little bit more elaborate than what open ACC does.",
                    "label": 0
                },
                {
                    "sent": "Open ACC.",
                    "label": 0
                },
                {
                    "sent": "Again, it's kind of assumes it's modern, so it assumes that the device you're using is is just the accelerator you got plugged in.",
                    "label": 0
                },
                {
                    "sent": "So by default we don't have to specify every single time we're moving data to an accelerator and open MP you targeted device.",
                    "label": 0
                },
                {
                    "sent": "Device Zero is the zero with accelerated got plugged in.",
                    "label": 0
                },
                {
                    "sent": "But here is the great.",
                    "label": 0
                },
                {
                    "sent": "Here is the great similarity between open ACC, an open MP, the ridiculous.",
                    "label": 0
                },
                {
                    "sent": "I should say similarity between the two.",
                    "label": 0
                },
                {
                    "sent": "That shows you this is more of technical, more political feud, the technical feud.",
                    "label": 0
                },
                {
                    "sent": "This right here is equivalent of a copy command instead of having copy two and copy from, we have our copy and copy out.",
                    "label": 0
                },
                {
                    "sent": "Excuse me, things are so similar that they confuse me since I start talking about him having copying and copy out.",
                    "label": 0
                },
                {
                    "sent": "We have moved or should be mapped.",
                    "label": 0
                },
                {
                    "sent": "Two an map from there, exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "And by the way, instead of copy which does a copy in a copy out, we have two front, so this map to from with the array B.",
                    "label": 0
                },
                {
                    "sent": "That's would be isn't there is just a variable this map to from would be is the exact same thing is saying copy B, so the similarities in the analogs are just simple minded and.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example of a code that runs the Saxby loop again that we've all seen with open MP4 on an NVIDIA device.",
                    "label": 0
                },
                {
                    "sent": "This looks a little bit more complicated than just using Kernels Command an it is because kernels takes responsibility for doing everything in open MP.",
                    "label": 0
                },
                {
                    "sent": "Instead, we have to take responsibility for a bit of it and more importantly and more to the point, we have a whole lot of this messy stuff here about teams and how to distribute them, and this is the equivalent of open ACC.",
                    "label": 0
                },
                {
                    "sent": "More advanced stuff that we've looked at with worker Vector gang, right with worker Vector Gang we've looked at.",
                    "label": 0
                },
                {
                    "sent": "We recognize that we're responsible for work taking responsibility.",
                    "label": 0
                },
                {
                    "sent": "We can neglect it and let it do the default thing, but we're taking responsibility for how to break the work up into pieces with open MP 4.0, it becomes kind of necessary on a GPU to start to say how to distribute things in pieces.",
                    "label": 0
                },
                {
                    "sent": "So that's where these things show up.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's a little bit better compare and contrast example right here.",
                    "label": 0
                },
                {
                    "sent": "Here's open MP4405, which is not too bad, right?",
                    "label": 0
                },
                {
                    "sent": "This is pretty clean right here.",
                    "label": 0
                },
                {
                    "sent": "We're saying target device zero and Arabie gets a copy in a copy or its equivalent.",
                    "label": 0
                },
                {
                    "sent": "Just copy B and then we say here's a parallel for loop and this is standard open MP, so that does the right thing.",
                    "label": 0
                },
                {
                    "sent": "Here is open ACC doing the right thing here.",
                    "label": 0
                },
                {
                    "sent": "It just says hey, here's a kernel.",
                    "label": 0
                },
                {
                    "sent": "Do the right things and it works fine.",
                    "label": 0
                },
                {
                    "sent": "Here is when we start trying to apply open MP to an NVIDIA GPU.",
                    "label": 0
                },
                {
                    "sent": "Things get a little bit messy here needlessly so.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You might think here is one way you can do it.",
                    "label": 0
                },
                {
                    "sent": "Here's the ifdef Ugly Ifdef solution to doing this where you're trying to support multiple architectures with open MP, where again the fact that Intel and NVIDIA have pride things apart as much as possible shows up in terms of the directive so.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which way to go these days?",
                    "label": 0
                },
                {
                    "sent": "It is still the slide I keep waiting for the slide to become obsolete to where things finally come together and work.",
                    "label": 0
                },
                {
                    "sent": "But at this point in time open MP4 is what Intel very much prefers you to support their zone 5 devices with and hence the Intel compiler works just great with open MP4 and 4.5.",
                    "label": 0
                },
                {
                    "sent": "The latest version of the spec, which isn't nearly as interesting and increment, but they hope with open MP 4.5 in their compiler that you see on Fi.",
                    "label": 0
                },
                {
                    "sent": "Processors NVIDIA instead and hopes that the open ACC compilers and there are multiple companies working powers that you use them to support to use NVIDIA process.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at this point in time, it simply comes down to which device is most interesting to you right now.",
                    "label": 0
                },
                {
                    "sent": "So if you really interested in Scion five, you think your problems can work there well, or you've got access to them or whatever, then you're going to be using open MP4 if you think that NVIDIA GPU's are most interesting to you, because again you got access to them, or you just want the best odds of just landing on a random machine out there with accelerators, it's going to be NVIDIA.",
                    "label": 0
                },
                {
                    "sent": "Then you're going to use open ACC and that sounds awful that I've got to make this choice based on no good technical merit.",
                    "label": 0
                },
                {
                    "sent": "But at the end of the day, the two are such simple minded mappings from one to the other that it's more an exercise in editing to move back and forth.",
                    "label": 0
                },
                {
                    "sent": "And it is like I've gotta change my algorithms or rethink things.",
                    "label": 0
                },
                {
                    "sent": "It's mostly editing as a matter of fact.",
                    "label": 0
                },
                {
                    "sent": "Where is Galen?",
                    "label": 0
                },
                {
                    "sent": "There he is in the back.",
                    "label": 0
                },
                {
                    "sent": "So Galen is maybe the world expert are certainly part of the team of world experts at this point in time.",
                    "label": 0
                },
                {
                    "sent": "And what the state of affairs is.",
                    "label": 0
                },
                {
                    "sent": "These guys just wrote a paper which I guess won't won't be publicly available until when.",
                    "label": 0
                },
                {
                    "sent": "Exceed 6, which is right around the corner.",
                    "label": 0
                },
                {
                    "sent": "So in a couple of weeks that exceed 16 their paper, which involves automatically converting between one to the other, or I guess going mostly in the open ACC to open MP direct direction, will explain the actual practical situation at the moment in doing that and making that work with various compilers and whatnot, and that is wonderfully enlightening because, again, the situation is needlessly complex and messy at this point in time, but not hopeless in the sense that.",
                    "label": 0
                },
                {
                    "sent": "Again, you make this huge investment one way, and then you're screwed, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe we use.",
                    "label": 0
                },
                {
                    "sent": "You can use use for open MP as well, right?",
                    "label": 0
                },
                {
                    "sent": "Yes, absolutely open MP is meant to target accelerators and so absolutely it's meant to support GPU's.",
                    "label": 0
                },
                {
                    "sent": "But as a practical matter you're going to get much better support out of an open ACC compiler and again, get Galen will just can discuss the exact details of the current state of various compilers and how well they support different things.",
                    "label": 0
                },
                {
                    "sent": "He's going to right now as a matter of fact please.",
                    "label": 0
                },
                {
                    "sent": "So the only current open MP4.",
                    "label": 0
                },
                {
                    "sent": "Compiler that supports NVIDIA is the one that you get when you buy a Cray.",
                    "label": 0
                },
                {
                    "sent": "It's really cheap, it's almost free.",
                    "label": 0
                },
                {
                    "sent": "It's just Cray dollars that you have spent so.",
                    "label": 0
                },
                {
                    "sent": "So it turns out it's really expensive if you happen to own a Cray urine.",
                    "label": 0
                },
                {
                    "sent": "I've got one.",
                    "label": 0
                },
                {
                    "sent": "He's got one.",
                    "label": 0
                },
                {
                    "sent": "You're in good shape.",
                    "label": 0
                },
                {
                    "sent": "Tyler is looking to break the bank agreement.",
                    "label": 0
                },
                {
                    "sent": "So you playing with.",
                    "label": 0
                },
                {
                    "sent": "I don't know about claims work for that yet, but I do know that Cray is supporting it and they are.",
                    "label": 0
                },
                {
                    "sent": "Socrate whether they made the right decision or not, they have decided to go with open MP4.",
                    "label": 0
                },
                {
                    "sent": "And they they support open ACC at its current standard.",
                    "label": 0
                },
                {
                    "sent": "But they're not going to do the next standard.",
                    "label": 0
                },
                {
                    "sent": "I don't know why that's just what they decided.",
                    "label": 0
                },
                {
                    "sent": "So you have, like I say, the practical consideration that if you're going to use NVIDIA GPU's, you should, if that's if that's where you think you're most likely to land in the immediate future, you should probably go with the flow you know which at this point in time is in video saying use open ACC if you're going to use Intel devices, probably go with the flow and not that you have actually an option.",
                    "label": 0
                },
                {
                    "sent": "This point, it's open MP and again it's aggravating to me from a technical perspective because they're so similar that there should be one stage.",
                    "label": 0
                },
                {
                    "sent": "The standard should have been merged together.",
                    "label": 0
                },
                {
                    "sent": "The flip side is they're so similar that it's not a nightmare.",
                    "label": 0
                },
                {
                    "sent": "Like I say, where if you invest going one way and then decide that you need to support the other as well, jumping onto the other track is mostly again and exercising and compiling, and these guys have done an automatic conversion thing themselves already, so it gives you some idea that it's again it's not reach.",
                    "label": 0
                },
                {
                    "sent": "It's not changing your algorithms up and rethinking the entire problem.",
                    "label": 0
                },
                {
                    "sent": "It still comes down to data management and avoiding dependencies and loops.",
                    "label": 0
                },
                {
                    "sent": "So it's it's again.",
                    "label": 0
                },
                {
                    "sent": "It's interesting enough discussion that those of you that have specific concerns again talk to me and best thing of all is to talk to GAIL in particular, 'cause he can.",
                    "label": 0
                },
                {
                    "sent": "He can tell you the state of the art right now in various compilers doing various things, but be aware of the fact that it's in flux, but so if you'll confuse yourself, you just say well, yes, open MP supports everything and open ACC supports everything.",
                    "label": 0
                },
                {
                    "sent": "That is true on paper.",
                    "label": 0
                },
                {
                    "sent": "Reality is a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A little bit different, so one of the things that I keep mentioning is that both companies hope that we're going to go hostless becausw of.",
                    "label": 1
                },
                {
                    "sent": "Obviously, Intel would prefer you spend all of their money with your money with them.",
                    "label": 0
                },
                {
                    "sent": "An NVIDIA resents you having to buy Intel processors to use your NVIDIA cards, so they would like you to just buy their devices and that your computer is just a room full of racks, full event video cards or racks full of Zion, Fives and they both have very serious commitments to this strategy.",
                    "label": 0
                },
                {
                    "sent": "And they both have Rd Maps that are making this happen in the near future with Intel.",
                    "label": 0
                },
                {
                    "sent": "They just publicly announced with Knights Landing.",
                    "label": 0
                },
                {
                    "sent": "How that's going to happen with the next generation ship where it will be hostless with NVIDIA.",
                    "label": 1
                },
                {
                    "sent": "I'm under NDA still so I can't say anything, but I I I can't tell you it's on the road map 'cause that's public and I can tell you that they're not going to let Intel get very far ahead of them.",
                    "label": 0
                },
                {
                    "sent": "So NVIDIA likewise has a plan that has involves no CPU at all.",
                    "label": 0
                },
                {
                    "sent": "And as you've seen here with open ACC, the idea is that you can move everything onto the GPU and.",
                    "label": 0
                },
                {
                    "sent": "And minimize the amount of CPU time that you have the so that's.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That is the new that's near in the future than you might think.",
                    "label": 0
                },
                {
                    "sent": "Some things we didn't mention here.",
                    "label": 0
                },
                {
                    "sent": "We did not talk about open CL whole lot here.",
                    "label": 0
                },
                {
                    "sent": "One of the reasons is that if you don't know anything about an open CL might have given you the impression that it's a high level.",
                    "label": 0
                },
                {
                    "sent": "It's some kind of alternative to open AC or open up here.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It is an alternative to CUDA Open CL was the response to the community of saying Nvidia's CUDA is great, but it is owned lock, stock and barrel by NVIDIA.",
                    "label": 0
                },
                {
                    "sent": "It is a proprietary standard that makes people uncomfortable.",
                    "label": 0
                },
                {
                    "sent": "So open CL emerged as an alternative to the CUDA Praya Terry Standard.",
                    "label": 0
                },
                {
                    "sent": "Now Interestingly, NVIDIA supports it pretty well.",
                    "label": 0
                },
                {
                    "sent": "Actually they have supported it pretty well.",
                    "label": 0
                },
                {
                    "sent": "It's never worked as well as as CUDA at.",
                    "label": 0
                },
                {
                    "sent": "You know, for efficiency, but it's not been orders of magnitude off either, so NVIDIA hasn't treated terribly, but the two big supporters for it were in the Khronos Group, actually deficient supporters for it in the Khronos Group, which formed the standard included NVIDIA as well.",
                    "label": 0
                },
                {
                    "sent": "So in videos actual official supporter of it, Intel at AMD and they all had this.",
                    "label": 0
                },
                {
                    "sent": "Those other two parties you can imagine back when CUDA was the only game in town at a very strong interest in supporting open CL.",
                    "label": 0
                },
                {
                    "sent": "Because it kind of kept them in the game.",
                    "label": 0
                },
                {
                    "sent": "However, things have changed, and in particular Intel for reasons that you now completely understand has decided that open MP is the way forward and using accelerators because they want you to use their accelerators and open MP matches their accelerators so they're not huge supporters of open CL AMD, which you might have imagined as the odd man out were certainly big fans of open CL.",
                    "label": 0
                },
                {
                    "sent": "It kept them in the game.",
                    "label": 0
                },
                {
                    "sent": "However they have a very different idea here, which is a future.",
                    "label": 0
                },
                {
                    "sent": "Where a Fusion architecture in the future and actually just now.",
                    "label": 0
                },
                {
                    "sent": "And this is actually an existing hardware.",
                    "label": 0
                },
                {
                    "sent": "Let's because normal AMD would like to have your memory and your graphics processor in your processor all residing in the same place, so there's no memory management or movement issues.",
                    "label": 0
                },
                {
                    "sent": "And you might say, well, that's obvious design.",
                    "label": 0
                },
                {
                    "sent": "Everybody should be doing that.",
                    "label": 0
                },
                {
                    "sent": "Why does anybody have this?",
                    "label": 0
                },
                {
                    "sent": "Remember, the reason that GPU's have graphics memory in them instead of regular memory is that the very very different architecture of the graphics memory.",
                    "label": 0
                },
                {
                    "sent": "Allows you to have extremely high bandwidth, latency tolerant memory versus the kind of memory that you want to put in the CPU, where high latency sitting there waiting on memory accesses.",
                    "label": 0
                },
                {
                    "sent": "Stall everything out and so there's a reason that there are two different types of memory.",
                    "label": 1
                },
                {
                    "sent": "AMD just kind of take to approach it.",
                    "label": 0
                },
                {
                    "sent": "We're going to ignore that reason and just put everything in the same place and as a result they pay a penalty in that they're floating point performance in their bandwidth to accesses the memory are greatly handicapped by the fact that the processor is sharing the same memory, so.",
                    "label": 0
                },
                {
                    "sent": "They have this Fusion architecture whether or not you think it's a good compromise or trade off isn't as germane.",
                    "label": 0
                },
                {
                    "sent": "Here is the fact that the cause of that they don't care about open CL now either.",
                    "label": 0
                },
                {
                    "sent": "'cause open CL's and what in concern with memory management, moving data back and forth.",
                    "label": 0
                },
                {
                    "sent": "They prefer now that use their new interface H essay, which is all about disputing architecture, so AMD is no longer that.",
                    "label": 0
                },
                {
                    "sent": "They're all still official supporters, but AMD is no longer really concerned about spending a lot of resources in open CL.",
                    "label": 0
                },
                {
                    "sent": "Intel's no longer concerned about it.",
                    "label": 0
                },
                {
                    "sent": "NVIDIA was, you know, did an admirable job, but probably we just assumed that the thing didn't exist and as a consequence open CL isn't getting a lot of resources at the device driver and engineering level from the big hardware companies.",
                    "label": 0
                },
                {
                    "sent": "And while that's not fatal to a lot of open source projects like say, good new compilers, becausw Intel architecture isn't radically changing every 14 months.",
                    "label": 0
                },
                {
                    "sent": "So the open source community can support compilers pretty well even if you don't have engineers and better than Intel, you can kind of respond after the fact and keep up with things.",
                    "label": 0
                },
                {
                    "sent": "And yeah, the Intel compiler is always a little bit better and more efficient, but good news not.",
                    "label": 0
                },
                {
                    "sent": "Completely moot, so that works doesn't work so well in an environment where every 14 months drastic new hardware is coming out.",
                    "label": 0
                },
                {
                    "sent": "It's all patented and proprietary and you're dependent on device drivers.",
                    "label": 0
                },
                {
                    "sent": "So if the companies aren't supporting it with engineers in House, it's hard to keep up.",
                    "label": 0
                },
                {
                    "sent": "And so open CL has had a performance gap that's this widen considerably and I don't personally see why that's not going to continue given again that the current circumstances, so it makes it increasingly bleak long-term picture for open CL.",
                    "label": 0
                },
                {
                    "sent": "And that's that's just the way it is.",
                    "label": 0
                },
                {
                    "sent": "The Direct Compute Library from Microsoft is an alternative, but it's not really HPC oriented, so it's not really interesting for us.",
                    "label": 0
                },
                {
                    "sent": "For straight thread use, so not so concerned with accelerators, but still since I'm talking about these alternative things, if you're opening programmers, you've got thread libraries like C++ app or threading building blocks, or silk from Intel.",
                    "label": 0
                },
                {
                    "sent": "They're very C++ oriented anyway.",
                    "label": 0
                },
                {
                    "sent": "They're not really Excel your libraries, but so I just stick them in here because there are there terms that come up in the context of this discussion you might think are they alternatives?",
                    "label": 0
                },
                {
                    "sent": "So they're not.",
                    "label": 0
                },
                {
                    "sent": "So again we have.",
                    "label": 0
                },
                {
                    "sent": "Is anybody in the room here using something else?",
                    "label": 0
                },
                {
                    "sent": "You're wondering how it fits in the picture.",
                    "label": 0
                },
                {
                    "sent": "This is a good time to ask those questions.",
                    "label": 0
                },
                {
                    "sent": "I think I've covered most of the bases here, Yep.",
                    "label": 0
                },
                {
                    "sent": "Some time ago.",
                    "label": 0
                },
                {
                    "sent": "Of what we have we have we have we wish to capture all of your questions on our microphone here so.",
                    "label": 0
                },
                {
                    "sent": "So some time ago I heard about Spur, which was supposed to be an intermediate language which promised to be very universal and to allow even open CL to reach almost the performance of CUDA.",
                    "label": 0
                },
                {
                    "sent": "Do you have any updated news about it?",
                    "label": 0
                },
                {
                    "sent": "Say that?",
                    "label": 0
                },
                {
                    "sent": "Again, what was it?",
                    "label": 0
                },
                {
                    "sent": "If I remember correctly, it was spur SPIRSPIR.",
                    "label": 0
                },
                {
                    "sent": "No that is not ringing a Bell with me.",
                    "label": 0
                },
                {
                    "sent": "Anybody else in this room here of this so well by the current group I think.",
                    "label": 0
                },
                {
                    "sent": "So I can come, but I think the big thing of difference there is Opus.",
                    "label": 0
                },
                {
                    "sent": "Heel is actually fine.",
                    "label": 0
                },
                {
                    "sent": "But the big problem with challenges we've all spent so much time optimizing our codes to the hardware because they were earlier.",
                    "label": 0
                },
                {
                    "sent": "We typically get 7580% the performance of CUDA when we use open CL.",
                    "label": 0
                },
                {
                    "sent": "And again this is on NVIDIA optimized hardware.",
                    "label": 0
                },
                {
                    "sent": "So typically we have better value for money and name the hardware.",
                    "label": 0
                },
                {
                    "sent": "So if I were to start over today, I would probably get open.",
                    "label": 0
                },
                {
                    "sent": "Seal is probably the better framework for long-term.",
                    "label": 0
                },
                {
                    "sent": "The only sad thing is that there is so much better documentation for CUDA if you're getting started with programming.",
                    "label": 0
                },
                {
                    "sent": "Can I add there is 1 unit where are go bolts?",
                    "label": 0
                },
                {
                    "sent": "We were we actually tried TBD quite a lot of years and the same thing with TB is that it doesn't come close to open MP.",
                    "label": 0
                },
                {
                    "sent": "Likely because Intel is optimized TV before Photoshop for web servers or whatever to get responsiveness not scaling, but arguments look really interesting.",
                    "label": 0
                },
                {
                    "sent": "I don't think it has officially been released yet.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, they're targeting an application space and desktop application world has very different patterns of usage than what we're talking about here.",
                    "label": 0
                },
                {
                    "sent": "It's much less numerically oriented, much more towards the kind of things you see in.",
                    "label": 0
                },
                {
                    "sent": "You know video game or a word processor?",
                    "label": 0
                },
                {
                    "sent": "OK, well then let's move onto.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this may be just a formality, but other intentions do at some point stop calling GPU's GPU's which they are not.",
                    "label": 0
                },
                {
                    "sent": "Balance the the actual terminology which you see sprinkled throughout.",
                    "label": 0
                },
                {
                    "sent": "My talk is to call everything accelerator, so that's what I should be calling everything just an accelerator.",
                    "label": 0
                },
                {
                    "sent": "But now and NVIDIA engineers who are very well aware.",
                    "label": 0
                },
                {
                    "sent": "The guys who make the NVIDIA stuff which they know isn't the GPU still constantly call them GPU's.",
                    "label": 0
                },
                {
                    "sent": "And these are the guys who should be offended by the use of the term 'cause they spent all this time making this number crunching double precision device and they call it GPS casually.",
                    "label": 0
                },
                {
                    "sent": "So I think that term is here to stay until.",
                    "label": 0
                },
                {
                    "sent": "And just becomes probably well after it's obsolete.",
                    "label": 0
                },
                {
                    "sent": "Nobody can even remember GPU stands for will still be calling GPU's.",
                    "label": 0
                },
                {
                    "sent": "OK, another question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so regarding open.",
                    "label": 0
                },
                {
                    "sent": "Please correct me if I'm wrong, but as far as I know is the only one that allows to have it.",
                    "label": 0
                },
                {
                    "sent": "Russia news devices that run at the same time without changing the cover, anything right.",
                    "label": 0
                },
                {
                    "sent": "Also regarding open CL, AMD has initiative called Boltzmann Initiative, so each product, CUDA program or whatever you can translate into opens here.",
                    "label": 0
                },
                {
                    "sent": "Finally someone is interested in running the cooler.",
                    "label": 0
                },
                {
                    "sent": "50 exercise I think.",
                    "label": 0
                },
                {
                    "sent": "I found a way to fix this.",
                    "label": 0
                },
                {
                    "sent": "It was just a matter of finding the right modules.",
                    "label": 0
                },
                {
                    "sent": "I hope nothing more common.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "And then you need to export the LD library fast.",
                    "label": 0
                },
                {
                    "sent": "OK, well thank you for for that effort.",
                    "label": 0
                },
                {
                    "sent": "OK, yes.",
                    "label": 0
                },
                {
                    "sent": "Are they not in the same spot?",
                    "label": 0
                },
                {
                    "sent": "OK, they they will.",
                    "label": 0
                },
                {
                    "sent": "The slides for this will appear in the same spot.",
                    "label": 0
                },
                {
                    "sent": "Then I'm not sure why they aren't, but I'll make sure that they do OK. Well, Fortunately there's been a lot of dense coding in the stuff that's ahead of us here, so forgive me for not having him out there.",
                    "label": 0
                },
                {
                    "sent": "You won't miss a whole lot.",
                    "label": 0
                },
                {
                    "sent": "We won't have to refer them for exercises or anything.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "Well then I'll move on to the hybrid programming, although this discussion again, there's no reason not if you.",
                    "label": 0
                },
                {
                    "sent": "If you think of points relevant to your code later, just jump in.",
                    "label": 0
                },
                {
                    "sent": "We're not.",
                    "label": 0
                },
                {
                    "sent": "We're not segmenting our topics here formally, so let's talk about hybrid programming though, because that is, I hope of great interest to many of you because of modern machines are built up of these heterogeneous pieces.",
                    "label": 0
                },
                {
                    "sent": "They've got accelerators, and they've got processors which always have multiple cores, and for most of you sticking, a lot of those together in a room or using a machine where somebody's done that in great expense is interesting.",
                    "label": 0
                },
                {
                    "sent": "So we should know how all these things interoperate and the good news is that these standards committees open ACC, Open MP, and MPI committees.",
                    "label": 0
                },
                {
                    "sent": "Recognize and have always recognized this fact, so they meant things to interoperate very well, and they do in the most logical way.",
                    "label": 0
                },
                {
                    "sent": "If you understand how these pieces work separately, the way they fit together makes perfect sense.",
                    "label": 0
                },
                {
                    "sent": "You don't need a separate document to describe how this interacts with that.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's actually is in the spec, but it all works exactly how you should think it works.",
                    "label": 0
                },
                {
                    "sent": "If you understand the pieces, but will step through some examples here, I am assuming that you guys all know basic MPI because that was kind of a prerequisite for this whole thing, although after.",
                    "label": 0
                },
                {
                    "sent": "Yesterday's quiz, it is clear that that is not entirely true, and that's not really a surprise to me.",
                    "label": 1
                },
                {
                    "sent": "Although this this group Interestingly scored better than the MPI Group on the MPI quiz, so I'm not sure what to make of that.",
                    "label": 0
                },
                {
                    "sent": "But at any rate, I will assume that you know basic MPI at least, and we're not doing anything too fancy here, and this will also be for those of you that are interested in doing the hybrid challenge.",
                    "label": 0
                },
                {
                    "sent": "Pay close attention, and I know that's a fair fraction of this group, which I'm glad to see last year was a lot of fun.",
                    "label": 0
                },
                {
                    "sent": "And by the way, over lunch I just realized that since David just grab David's running it this year, so he's the absolute guy to talk to about the rules, but he just grabbed my rules from last year was the first year we did it, and I see grab my rules verbatim, which means collaboration is also perfectly legal.",
                    "label": 0
                },
                {
                    "sent": "I might even say encouraged.",
                    "label": 0
                },
                {
                    "sent": "So if you're somebody that doesn't know MPI really well, and you think that's going to be a handicap to you, because certainly whatever the winning solution is, it's going to use the rules.",
                    "label": 1
                },
                {
                    "sent": "Or you can use up to four nodes of bridges.",
                    "label": 0
                },
                {
                    "sent": "It's going to use for nodes.",
                    "label": 0
                },
                {
                    "sent": "Bridges now whether it uses 4 nodes, well with open ACC or with open MP or whatever, that's an interesting an outstanding question, but you're definitely not going to win the competition.",
                    "label": 0
                },
                {
                    "sent": "Running on one node.",
                    "label": 1
                },
                {
                    "sent": "So MPI is going to be the basis for whatever the winning solution is.",
                    "label": 0
                },
                {
                    "sent": "If you don't really feel comfortable with MPI and you think that's going to disqualify you from having fun with the competition, find somebody that does.",
                    "label": 0
                },
                {
                    "sent": "As a matter of fact, if you feel like you really are getting the gist of open ACC, find some MPI person.",
                    "label": 0
                },
                {
                    "sent": "It doesn't.",
                    "label": 0
                },
                {
                    "sent": "And two of you may well be the winning team, so collaboration again is.",
                    "label": 0
                },
                {
                    "sent": "Is not just encouraged, but it's the solution for any of you that would like to participate, but just don't think your MPI skills are up to par.",
                    "label": 0
                },
                {
                    "sent": "Feel free to collaborate, but I'm going to assume you know at least basic MPI, which is nice because it's rarely that I get to make that assumption.",
                    "label": 0
                },
                {
                    "sent": "As a matter of fact, the one time a year besides this that I really can make.",
                    "label": 0
                },
                {
                    "sent": "That assumption is when I teach everybody MPI first on boot camp.",
                    "label": 0
                },
                {
                    "sent": "So the only time I ever get to talk about hybrid programming and depth is our boot camp where I teach everybody open, MP open ACC and MPI in the same week, and then we can really get into.",
                    "label": 0
                },
                {
                    "sent": "Hybrid programming in this case I can, I'm going to make that assumption.",
                    "label": 0
                },
                {
                    "sent": "So here is, oh, geez, I just violated my what exactly what I told you that not having the slides in front of you will be not a huge handicap.",
                    "label": 0
                },
                {
                    "sent": "Well, it might be a little bit.",
                    "label": 0
                },
                {
                    "sent": "And I'm guessing that nobody can.",
                    "label": 0
                },
                {
                    "sent": "Probably.",
                    "label": 0
                },
                {
                    "sent": "How far back can we read this card?",
                    "label": 0
                },
                {
                    "sent": "Seeing squinting halfway back already?",
                    "label": 0
                },
                {
                    "sent": "So we might?",
                    "label": 0
                },
                {
                    "sent": "We might have an issue here.",
                    "label": 0
                },
                {
                    "sent": "As a matter of fact, I do have some code from here on out, but we don't have hands on and we're not in a huge rush.",
                    "label": 0
                },
                {
                    "sent": "My solution to this looking at this room would be why don't we move down to the front?",
                    "label": 0
                },
                {
                    "sent": "If you really, if you if you want to follow along, can we take two minutes and move down to the front?",
                    "label": 0
                },
                {
                    "sent": "Don't we don't have to do every other row thing at all, so if you if you really want to know what's going on, move, move yourselves down to the frontier and you'll be able to see.",
                    "label": 0
                }
            ]
        }
    }
}