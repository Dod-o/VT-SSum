{
    "id": "q2bmr3glm4wtib5gevuebjy2cazuoefi",
    "title": "On a Theory of Similarity Functions for Learning and Clustering",
    "info": {
        "author": [
            "Avrim Blum, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/mlss09us_blum_tsflc/",
    "segmentation": [
        [
            "Alright, so I'm going to be talking about theory of similarity functions for learning and clustering and.",
            "Actually everything I'm talking about is joint work with Nina Balcan and also portions are joint work with Nati Srebro and Santosh Kampala."
        ],
        [
            "Alright, so let me start with kind of a 2 minute overview version of the talk and then I'll go on to the the remaining 58 minute version.",
            "So suppose we're given the typical machine learning problem, say given the collection of images we want to learn a rule to distinguish men from women.",
            "The problem, though, is that our representation were given, say, in terms of pixels, is not so good.",
            "So for a situation like this, one powerful technique that's come about is to use a kernel.",
            "OK, so what's a kernel?",
            "It's a special kind of pairwise similarity measure.",
            "It takes into objects, and it outputs a number.",
            "And very useful in many different kinds of applications.",
            "And also there's a very powerful theory about kernels.",
            "But one thing about.",
            "About the theory in terms of what is it that you want out of your kernel with you?",
            "What are the properties you want your kernel to have?",
            "The theories talks about viewing them as implicit mappings into some potentially high dimensional space and."
        ],
        [
            "Question that started this.",
            "This work is can we develop a theory that just views a kernel as just a measure of similarity that talks in terms of natural properties?",
            "Thinking of it as a measure of similarity?",
            "And that's ideally even more general than the standard theory of kernels in not requiring it to have some of the mathematical properties you need in order to view them as implicit mappings.",
            "And if we satisfy these conditions, that would still be useful for learning.",
            "And then in the second part of my talk, when we talk about the problem well, what if actually we don't have any labeled data at all?",
            "We only have unlabeled data, so we have.",
            "We still want to learn well, but we don't have any labeled data, so we have effectively a clustering problem.",
            "And can we develop a theory of properties of a similarity function that would be sufficient to be able to cluster well to learn?",
            "Well when you don't have any label data at all?",
            "And presumably you're going to need stronger properties when you have no labeled data.",
            "And so we'll get to that.",
            "OK, so that's the that."
        ],
        [
            "Human version in doing this will develop a kind of a pack model.",
            "Physical learning theory model for cluster.",
            "OK, so let me."
        ],
        [
            "Start with part one.",
            "Alright, so first of all, so the theme of this part, so we want to talk about."
        ],
        [
            "Theory of natural sufficient conditions for similarity function to be useful for binary classification.",
            "Learning problems.",
            "We don't want to require that our similarity function be positive semidefinite to satisfy the properties that you want for kernel.",
            "Don't have implicit spaces, but we do want to include the notion of a large margin kernel in terms of these conditions.",
            "And so we're going to develop some conditions, and actually the formal level will even allow you to learn more, so we can define classes of functions that don't have large margin kernels, even if you allow yourself substantial hinge loss and yet do have good similarity functions under this notion.",
            "So I'll get to that later.",
            "OK."
        ],
        [
            "So let me start, sorry at the beginning.",
            "So let me start with kernels.",
            "OK, so here's my.",
            "62nd introduction to kernels.",
            "So we have a lot of great algorithms for learning linear separators, so perceptron, SVM, so linear separators are really one of the things in learning that we use all the time, but unfortunately a lot of time we have data that's not linearly separable.",
            "We need some sort of decision surface like this.",
            "So what do we do so they have all the answers.",
            "When I was in Graduate School, was to use a multi layer in their own network and that was the thing you would do if you.",
            "Linear separate wasn't good enough and kind of newer answers.",
            "Well, how about using a kernel function and a kernel function as a way of getting your linear separator algorithm to learn a nonlinear decision surface and the point of kernels is that while many algorithms and you look at how do they see the data, they only interact with the data by taking dot products of pairs of examples.",
            "So they take their data and if you look at how they interact, you can write the algorithm in such a way that the only thing they do with their data as they ask for dot products of pairs, and that's the only way they look at their data.",
            "So that's the interface.",
            "So if that's the interface they have with data, if you just redefine that product, you'll get the algorithm do something different.",
            "And for example, if you define K of XY, this is we are kernel of XY to be something else.",
            "That's not that product is some other function.",
            "This corresponds to taking a dot product, but in some other space, so kernel is just a redefinition of dot product, it's something it's a function that can be viewed as a dot product in some implicit space, and so for example, in this case 1 + X dot Y to the D is a legal definition of dot product, but if your original data was an end dimensional space, this corresponds to a dot product and end to the D dimensional space where this fight, some implicit mapping taking your end dimensional data into some implicit end to the D dimensional space.",
            "And so your algorithm, since it only is interacting with the data via dot product, is acting as if data was in this much higher dimensional space and maybe in that higher dimensional space your data is linearly separable.",
            "Great and that can allow your algorithm producing nonlinear curve in your original space."
        ],
        [
            "OK, and this here's a little example of, well, X dot Y to the D where D = 2 and your original space is a 2 dimensional space, and if your data looked like this, so this is one class, another class.",
            "It's a non linear separator in the implicit space is 3 dimensional space and there you have a linear separator.",
            "Great and the other thing it's not."
        ],
        [
            "This is a more over if it's not.",
            "If data has, so if your kernel has the property that the implicit space, not only is the data linearly separable, but it's linearly separable by some large margin gamma, then you get you can generalize well as a function of this margin.",
            "So if you have margin gamma in this implicit space, you only need sample size roughly one over gamma squared to have confidence in your ability to do well on new data.",
            "Now we have to talk about normalization, normalize everything to be inside the unit ball.",
            "Otherwise, talk about whether units.",
            "Otherwise the radius squared over gamma squared, so normalize everything, radius one and then you need only order one over gamma squared examples to be able to generalize well, and there's no direct dependence on the dimension great.",
            "OK, so this is this is the standard theory of colonels, and colonels are useful in practice for dealing with many, many different kinds of data.",
            "OK."
        ],
        [
            "So the issue I want to get out is that in practice, if you're thinking about some learning problem and trying to come up with what you think might be a good kernel, you're thinking about it by viewing it as a measure of similarity between your data object.",
            "You're thinking, Gee, what would be a good way of measuring how similar I don't know?",
            "Two DNA sequences are two images are, but our theory is talking about margins in some implicit space, and so it may not be the best for intuition and trying to decide.",
            "Gee, you know whether I think would be a.",
            "A good kernel to use.",
            "Also we have this technical requirement that our kernel has to be something you can view as an implicit dot product and that might rule out the most natural similarity function for your domain.",
            "Now you could take that similarity function kind of coerce it into, you know, bang into a legal kernel, but maybe you'd like to use it directly.",
            "OK, so.",
            "So coming up with some alternative, maybe more general theoretical explanation about what it is that we want in our similarity measure that would allow us to use it for learning OK?"
        ],
        [
            "So.",
            "So I'm going to talk about is a notion of what what we might want in a good similarity function.",
            "OK, so I want so a sufficient condition for a similarity function to be allow us to learn well with the following properties.",
            "So we'd like to talk in terms of natural direct quantities.",
            "Without talking about an implicit space without requiring that our similarity function be of this special type that you can view it as an implicit dot product, and if you satisfy these conditions we then we'd like to be able to conclude that we can use it to learn well, so we'd like to have an algorithm that, if you have a similarity function, has these properties, and we can use it to learn well.",
            "We'd like our death our notion to be broad, broad enough.",
            "That includes the usual notion of a good kernel, one that has a large margin in its implicit space.",
            "And even will see in a formal sense of a notion, even allows you to do do some things that you can't do with large margin kernels.",
            "OK, So what I'm going to do is I'm going to start with the notion that.",
            "Would be sort of nice intuitive, but won't be broad enough to include the.",
            "All large margin kernel so it will include things that aren't legal Colonel, so so it's not contained inside the notion of large margin kernels, but it doesn't include all of them.",
            "And then once we see that then we'll see how to generalize that into our main notion, which will include.",
            "Large margin kernel functions.",
            "So let me start with the first one, so we'll see that it's.",
            "The notion that's intuitive, and then we'll see that if we satisfy that condition, we can use to use it to learn.",
            "Well, I guess I won't prove it's intuitive.",
            "I can't do that, but I can prove that if you have 5, the condition that you can use to learn, well, OK?"
        ],
        [
            "So.",
            "OK, so we have, so we have a learning classification problem.",
            "So we have a distribution over labeled examples, positive and negative examples, and our goal is to output a good classification rule, one that has low air.",
            "And let's say that our similarity function is good if most examples are on average more similar to random examples of their own class than to random examples of the other class.",
            "OK, so we have positive and negative examples and will say our similarity function is good for this problem.",
            "If most examples are on average more similar, their average similarity to examples of their label is larger than their average similarity to examples of the other label OK or in math.",
            "Like this we will say that kernel are similarity function.",
            "Is epsilon gamma good?",
            "If most 1 minus epsilon fraction of examples satisfy the condition that their average similarity 2 examples why of their own label is larger and not just a tiny tiny exponentially small amount larger but by some gap gamma larger than their average similarity 2 examples of the other label?",
            "Seems like a natural condition you might ask of a notion of similarity between points.",
            "You say?",
            "Well, I've got this problem distinguishing.",
            "You know cats from dogs and and well, I want some measure of similarity so that most of the cats are on average more similar to cats and dogs, and most the dogs are in average more similar dogs and cats good.",
            "OK, so it's a reasonably intuitive condition to ask for, and if you satisfy this condition, it's actually also easy to learn well, so let's just take a look at that.",
            "Suppose I were to give you a similarity notion between between our data objects that had this property.",
            "Well, how could we use it to learn well?",
            "Well, actually it's not so hard.",
            "OK."
        ],
        [
            "So very simple.",
            "This is actually a pretty strong condition and very simple algorithm that we could use to learn well if somebody handed us a measure of similarity between data objects that had this property.",
            "So how could we do it?",
            "All we have to do is the following.",
            "We just do average nearest neighbor.",
            "We take a bunch of random positive examples, S plus take a bunch of random negative examples S minus and now just classify or new example X that we see based on whether it's on average more similar to the points in S plus or an average more similar to the points in S minus.",
            "OK. That makes sense.",
            "I guess I should have said just also I said sort of implicitly.",
            "I'm assuming our notion of similarity that the scores are between plus one and minus one.",
            "So you think A plus one is being really similar and minus one is being really different.",
            "That corresponds to for kernel saying that in the implicit space everything is inside the unit ball, because there you have dot products and so you've got dot product one dot product minus one dot product zero.",
            "If you're 90 degrees, so I'm assuming, like otherwise, the gamma would be sort of unitless.",
            "Quantity wouldn't make sense.",
            "So I'm assuming similarity is something between one and minus one.",
            "Great so.",
            "So I'll just draw two sets.",
            "Bunch of positives, bunch of negatives, classifying new example based on.",
            "Are you on average more similar?",
            "Test plus minus?",
            "And if you're more similar, plus you call a positive more somewhere else minus call it negative OK, and we could analyze this.",
            "OK, so there's a bunch of points."
        ],
        [
            "And so.",
            "And we can show the following so.",
            "So if our always to do is take S plus must be big enough, roughly one over gamma squared times some log factors, then with high probability this rule will have only error epsilon prime more than our base error epsilon.",
            "So we're already losing epsilon was so defined so that there could be an epsilon fraction of points.",
            "It just don't behave fine.",
            "You'll lose them, and then you'll only lose a little bit more.",
            "OK, and I didn't write out the proof of this theorem, but the proof is actually.",
            "Pretty straightforward.",
            "If you.",
            "Fix a point access.",
            "Find that condition.",
            "This is a big enough draw.",
            "So what are we doing?",
            "RASTA meeting that expectation from samples estimating that expectation from samples and this is a big enough sample so that way if you fix X first and then draw your S plus and S minus by huffing bounds with very high probability, you are going to estimate your expectations close enough to get the right answer.",
            "OK, and your failure probability will be only Delta times epsilon prime.",
            "So if you pick and accessing this condition.",
            "Then you draw your S plus minus with high probability.",
            "You'll get the right answer on X and then you just use Markov inequality to flip this around to say that if S plus and S minus are the size and then with high probability, they're only going to fail on some epsilon prime fractional points.",
            "OK, and that gives us the proof.",
            "Great, so if we have a similarity function satisfying this condition, then there's a very simple algorithm that we can use to learn well.",
            "Just take a bunch of random positives, a bunch of random negatives classifying new point based on.",
            "Are you on average more similar to these positives?",
            "Oregon average more similar?",
            "These negatives, 'cause we're estimating these expectations when you can see where the one over gamma squared is coming from?",
            "Very simply here 'cause what's happening.",
            "Just trying to estimate you got, you know, like a coin of 1 bias account of another bias.",
            "There's a little gap gamma.",
            "You gotta flip it one over gamma squared times times some log factors.",
            "Alright.",
            "UNF."
        ],
        [
            "Fortunately.",
            "This condition is not broad enough to include all large margin kernels.",
            "And you might suspect that because if you satisfy this condition, we had a really simple algorithm, namely average nearest neighbor, whereas for a large margin kernel we use something like SVM's you if this included with all the large margin kernels, you wouldn't need SVM, you could just use average nearest neighbor.",
            "Yep."
        ],
        [
            "Oh yeah, this is a constant, so just just take your huffing bound and you say.",
            "You know, just want with high probability you just wanna know how how many times you flip these coins and the constant.",
            "Just because this is a random variable that's bounded between plus 1 -- 1, not exactly a coin.",
            "It's random variable T + 1 -- 1.",
            "You gotta estimate that up to plus or minus like gamma over 2.",
            "In order for that expectation to be on the correct side of that expectation.",
            "Good."
        ],
        [
            "Alright, so let's take an example of why this is not broad enough, and then how do we fix that?",
            "OK, so here's a simple example.",
            "Consider the case of.",
            "Data, so we've got all of our negative examples.",
            "Is the unit circle.",
            "All are negative examples are here.",
            "This is a 30 degree angle.",
            "The positives are split.",
            "Half of them up there and half of them over there.",
            "That's also 30 degree angle, and our kernel is just regular dot product, so the implicit space is the actual space, which is just this screen here.",
            "OK, so it's 2 dimensional problem on this screen.",
            "Now this is a large margin kernel because there's a nice large margin there.",
            "The margin is 1/2, it's the.",
            "Um?",
            "It's a margin 1/2 it's the sign of 30 degrees, but it doesn't satisfy our condition, so let's see why those points in the upper right are actually more similar.",
            "On average to the negatives, and they are the positives.",
            "Well, why is that?",
            "Well, their average similarity to the negatives by dot product.",
            "It's so this is our notion of similarity.",
            "The 60 degree angle here that dot product is 1/2.",
            "So the average similarity of those guys to the negative 1/2 the average similarity, the positives well, half of the positives are also over.",
            "There.",
            "They have similarity.",
            "One the other half of the positives, right angle 120 degrees dot product negative 1/2.",
            "So they get the average of one negative half is only 1/4.",
            "So on average there only 1/4 similar to the positives.",
            "But there have similar the negatives.",
            "So according to this measure, some are the those.",
            "No, cats are more similar to the dogs and then then then the typical cat.",
            "I guess this is where there's two kinds of cats anyway, OK?",
            "So it does have a large margin, doesn't satisfy our condition, so it's not brought up.",
            "It's sufficient for learning, but it's not broad enough to capture the notion of large margin kernels.",
            "So."
        ],
        [
            "Here's an idea for broadening the definition.",
            "You notice if we.",
            "If we didn't take our points, why?",
            "As long as we didn't pick them from over there, we would've been alright.",
            "So let's broaden our condition to say that.",
            "For our our notion of similarity good, it's sufficient for there to be nonnegligible region are of reasonable points.",
            "Let's call them so that most acts are on average more similar to the reasonable points of their label then to the reasonable points of the other label.",
            "Alright, so most of the cats are on average more similar to the reasonable cats in the reasonable dogs, and most of the dogs are on average more similar to the reasonable dogs and reasonable cats.",
            "OK, and now if our algorithm knew what this like, if you had to say oh and Furthermore, these are the ones that are reasonable.",
            "And then you could just use the previous approach.",
            "But if you don't know, so we'll say that it's a good similarity function.",
            "If this set are exists, even if our algorithm doesn't know in advance.",
            "So our algorithms now a tougher time, 'cause we're going to have to somehow learn and figure out this reasonable set together.",
            "OK, but that's going to be our notion.",
            "OK."
        ],
        [
            "So here is the broader definition.",
            "Will say that our similarity function and now we can if we want to parameterize it out of epsilon, gamma, Tau good.",
            "If there exists a set R of reasonable points, why such that most exo one of mice epsilon fraction are on average more similar to the reasonable points of their label than to the reasonable points of the other label by some gap gamma, and Furthermore reasonable points should exist.",
            "They shouldn't be like you know, vanishingly small probability mass.",
            "There should be at least a Tau probability mass of reasonable positives and reasonable negative, so you should be able to see them in a random sample.",
            "OK, so that's the property we're going to want.",
            "Alright, so that's just like the previous one, but we're now weakening our requirement by allowing the previous thing to be violated on on points that are not reasonable.",
            "OK, so that's only the reason you have to be more similar to the reasonable guys of your label then to the reasonable ones the other way.",
            "Yeah.",
            "Life.",
            "Just just for the so I want most exit, so this is a single reasonable set.",
            "You could partition it into reasonable positives in the reasonable negatives, and I want is most, let's say, oh, let's say ideally like we talked about large margin separator use of 1st talk about separating all the points by a large margin.",
            "Then you say, well, it's OK.",
            "If you have some hinge loss, so the zero hinge loss version would be all axes should be on average more similar to the reasonable.",
            "Points of their label then the reason points the other label by some amount.",
            "Kim, that's that's the zero hinge loss version.",
            "Yeah, in there like the support vectors, but instead normally we think of SVM.",
            "You think of wanting a few support vectors.",
            "Here we want to we want a lot.",
            "We want to be able to see them.",
            "We want a large probability mass of things and.",
            "Yeah.",
            "That's right.",
            "And if if the distribution has these point masses will need the reasonable missed itself be a probabilistic function.",
            "Otherwise you can just have it be binary.",
            "And actually, as mentioned, matching in the answer to your question, technically in the same way that if you want SVM to work, if you say well, I couldn't separate all the points you know by Gap gamma only, most of them.",
            "What you really want is to have small hinge loss.",
            "It's not.",
            "You don't say I want to have most of the points separated by Gap game, you say, well, I want my total hinge loss to be most some some small value.",
            "And similarly here what we really want is that.",
            "When I say most of the exercises for this condition technically what I really want is at most some small hinge loss by hinge lost.",
            "So what do I mean by hinge loss here?",
            "Is we want back quality there be bigger than that quantity, thereby gamma?",
            "That's great if you don't satisfy the condition.",
            "Like maybe you have the same similarity to your label and the other label that counts hinge loss of one.",
            "Well, if you're actually gamma more similar to the other labels in your label, that counts is hinge loss of two.",
            "If you're 10 gamma more similar to the other label, then your label, then you're really bad.",
            "That's bigger hinge loss.",
            "OK, so.",
            "So we really want the hinge loss version.",
            "OK so I click this is a broader condition, but I claim you can still learn with this even if you don't know the set art and learn by a very kind of a natural kind of algorithm.",
            "And notice one thing here is that there's no requirement.",
            "Are similarity measure be positive semidefinite?",
            "They have this dot product.",
            "Relation fact doesn't need to be symmetric function, so you know.",
            "It just has to be bounded.",
            "That's the only thing.",
            "So here's the algorithm you can use will do.",
            "It will just take a.",
            "You will draw a set of random, unlabeled examples.",
            "They could have labels too.",
            "I guess that's actually for this version, maybe I do know.",
            "OK they can be unlabeled.",
            "Take a bunch of random points.",
            "I can ignore their labels and I'll call them landmarks.",
            "Their random points, call them landmarks and now will explicitly represent our data, so this is.",
            "But what you might call an empirical similarity map will just take each example and explicitly represented according to features where the first feature is.",
            "How similar are you the landmark one?",
            "The second feature is how similarity to landmark two that they thought the D features.",
            "How similar are you the landmark D sweet Aicardi landmarks were kind of triangulate Ng the points.",
            "How similar you did?",
            "This has some argue this, just write it out like that.",
            "OK so that will do that.",
            "So now we have an explicit representation of our data.",
            "Alright, so pictorially.",
            "We have this weird space and we've mapped it into this into some other space here.",
            "And now the following, I claim the following property, that is, if we take enough landmarks and roughly we're going to need one over gamma squared tell.",
            "So we're just coming from.",
            "I said there's at least our probability mass of reasonable positives and reasonable negatives, so this is enough points that with high probability we've got our one over gamma squared reasonable positive.",
            "Somewhere in here we don't know where they are.",
            "One over gamma squared.",
            "Reasonable negatives.",
            "Somewhere in here we don't know where they are, but somewhere there.",
            "Alright, so we take enough landmarks and with high probability in this space there is a large.",
            "There's a good separator, in fact, not only a good separator, but a separator with a large L1 margin.",
            "OK so well, So what do I mean?",
            "So in particular, consider the following weight vector in this space.",
            "Consider the wavevector.",
            "OK, so remember this dimensional space that gives away 0.",
            "So I'm just talking about something that exists, will talk later about what the fine but just would exist.",
            "Consider the weight factor that gives weight 0 to the points.",
            "Why that were not reasonable?",
            "OK, wait zero.",
            "Consider it will give way 1 / N plus, where N plus is the number of reasonable positives in here.",
            "OK, so the 57 reasonable pauses in here they get wait 1 / 57 gives those to the reasonable positives and gives weight negative 1 / N minus to the reasonable negatives in here where N minus the number of reasonable negative.",
            "So if they were 62 reasonable magazine here they will get wait negative 1 / 62.",
            "What is what is it doing?",
            "What is www.fofxdoing?",
            "Tell me the F of X is doing the exact same thing that previous average nearest neighbor was doing over the reasonable points that we have X is saying.",
            "Take your average similarity to the reasonable positives in here.",
            "Compare that to the average similarity, reasonable negatives, and then just see if that's great.",
            "Uncle is here or not is saying well, which is bigger.",
            "So this weight vector, which we which we're saying exists, it's just doing.",
            "This weight vector is just doing the same as our previous algorithm over the reasonable points and so the analysis we just had implies that if we take enough landmarks so that.",
            "There should be a little~ over here, 'cause I'm ignoring the log factors if we.",
            "If we take enough landmarks with high probability, we have enough reasonable pauses, reasonable negatives to satisfy that bound.",
            "We had two slides ago.",
            "This way vector will with high probability, separate most of the points, and Furthermore will actually do so by a good margin.",
            "We didn't analyze the margin before, but it will also have a good margin.",
            "It'll have margin, say gamma over 2.",
            "OK, and Furthermore this weight vector has Lowell one length, it's L1 length is just two because all these 1 / N plus is add up to one and all these negative one over and minuses add up to negative one.",
            "So the magnitude is 1.",
            "So they want like this too.",
            "So we have a small L1 length vector.",
            "They give us a nice margin.",
            "Great, now we don't know what it is, that's unfortunate, but the good thing is we have nice algorithms for learning.",
            "When you have a linear separator of large L1 margin, so all you have to do is now that you've done this, take a fresh set of labeled examples, project them into this space, and run your favorite algorithm for learning.",
            "When there's a lol one separator.",
            "Or large L1 margin linear separator like window or something like that.",
            "OK, and if you do that, your standard bounds for a safer window will say so you take your data."
        ],
        [
            "Protecting the space.",
            "Run your algorithm, get your separator and your standard bounds for an algorithm like we know will say that with high probability you'll get to error your base error epsilon plus some extra epsilon sub accuracy.",
            "If you have how much unlabeled unit you need, we need this roughly one over gamma squared.",
            "Tell how much label data do we need?",
            "Need one over gamma squared times, epsilon accuracy times?",
            "Log the dimension of the space 'cause the L1 algorithms only have to pay login dimension of the space.",
            "And so that so with that much data, we can then learn well.",
            "So this is the kind of.",
            "In the same way that something like SVM is a natural algorithm, you think about having a large margin Colonel in the usual sense.",
            "This is the become sort of a natural algorithm for similarity function satisfying this definition.",
            "This is really going to L1 type of type.",
            "This gamma here is really an L1 type of margin.",
            "OK, so so so the other thing it's nice is that."
        ],
        [
            "You can show that.",
            "So what we've shown, we've shown that if you satisfy this property, we have an algorithm.",
            "We can use an.",
            "You can also show that if your similarity function really is a legal kernel, and it really does have some large margin gamma, then they will also satisfy this condition.",
            "Oh"
        ],
        [
            "So the algorithm doesn't know the reasonable set, just comes in and improving it.",
            "There exists a weight vector and you need.",
            "There to be enough reasonable points."
        ],
        [
            "So where do you get in trouble?",
            "If there's not very many of them?",
            "Remember if you think about what this weight vector is doing, it's doing this empirical vote over the reasonable pauses and reasonable negatives, so you need enough points in your data set so that the previous algorithm has enough.",
            "You know you're taking a good vote.",
            "You got your one over gamma squared positive to vote, and your one over gamma squared negative to vote.",
            "So will you just need is is in the set of landmarks you don't know where they are.",
            "There's enough enough reasonable points that they can have a vote so that they're there.",
            "Their vote is able to estimate these expectations well.",
            "That's all good.",
            "Ality it would change it.",
            "I thought that your picture here would have some linear separator maybe.",
            "So right so?",
            "In some sense here, there's nothing about where they have to be, you know.",
            "Oh, they're independent test sample, so if I could be that there's multiple like in the previous case, I said, well, if you only pick the points from over here, things work.",
            "You could also only pick the points mean there's a different possible you can have set to work.",
            "This would work, or that would work in the same way that for SVM you can have different kinds of sets of support vectors, and that's independent test sample.",
            "Right, so that in fact, what's happening is that in this space there exists a separator that zero in most of the coordinates.",
            "So most of these are totally irrelevant, and there exists a separate that uses only a small number of coordinates in this space and is able to separate there.",
            "Oh so so how do you get good generalization?",
            "So that's where the gamma comes in.",
            "So the the.",
            "The.",
            "Let me go here good so."
        ],
        [
            "In order to be able to generalize well, you need to have an unlabeled sample of that size, one over gamma squared tail, so it's gamma goes down.",
            "You need more unlabeled data as to how it goes down anymore.",
            "Unlabeled data.",
            "You're labeled data also has one over gamma squared, so so gamma is really controlling.",
            "You know if Gamma is small, you need more labeled data, be able to generalize well, and now.",
            "The question maybe is a function of how many landmarks, so the more landmarks you have, the more labeled data points you need, but it's only log rhythmic because of the bounds you get for L1 margins so.",
            "So I guess what's controlling?",
            "I guess the way I would say this way a small gamma is like a large complexity in a large game is like a small complexity.",
            "You can think of gamma or one over gamma as sort of your complexity term.",
            "Is that goes to 0.",
            "This similarity function becoming worse and worse and worse, and your.",
            "Needing more data in order to be able to get that generalization, if it's an answer.",
            "Press.",
            "'cause in the end what you're doing is you're learning a linear separator of of this margin, roughly gamma in this space.",
            "Opening OK."
        ],
        [
            "Good.",
            "So.",
            "So we can show that if a similar function is illegal kernel and it has a large margin property in its implicit space, it also satisfies this condition, although there's a loss in translation.",
            "So if you have margin gamma in the usual L2 cents in your implicit space, that gamma get squared.",
            "And then there's some towels that crop in into this definition.",
            "So there's there is some amount of loss in translating this way, But if you have.",
            "I.",
            "But in terms of right?",
            "So if you have a large margin kernel, it also is a good similarity function of the parameters get a little bit worse.",
            "On the other hand, you can also show that."
        ],
        [
            "You can have cases where you can have good similarity functions where you don't have large margin kernels, so one interesting thing is you can show that there exist classes in distributions where there's a similarity function that has a large large gamma in this sense for all functions in the class, but there's no kernel that has a large margin for all functions in the."
        ],
        [
            "Laugh.",
            "OK, and in particular it turns out and there's a couple of ways to show this that for any class of pairwise uncorrelated functions.",
            "So if you have some distribution of data and I give you a list of functions that pairwise are uncorrelated, so to say they reach 5050 positive and negative, and the probability of being positive for one given that you're positive for the other is also 5050 for all pairs, then there is no kernel that can have a large margin for all functions in the class, so that's something you can show.",
            "In fact, even if you allow yourself to have substantial hinge loss.",
            "You still can't get a kernel with a large margin, but you can construct a similarity function with a large gamma.",
            "OK, so in a setting like this for a class of pairwise uncorrelated functions, in principle you should be able to learn from to get down to error epsilon with log of the size of the class times one over epsilon roughly examples and you can define a generic similarity function which has no error perfect margin.",
            "Small towel seen a lot of unlabeled data, but if we don't count the cost of unlabeled data, if that counts, is free, then this will achieve this bound in terms of how many labeled examples you need.",
            "But on the other hand, you can show that there is no good kernel in hinge loss, even if you allow yourself substantial hinge loss like 1/2, you still need margin roughly one over the square root of the size of the class, which means from your standard margin bounds you would need linear.",
            "A number of functions in class examples rather than log the number of functions examples.",
            "So you can show a separation.",
            "Yeah.",
            "Yeah like say yeah, like I had a hard matrix right so?",
            "Good.",
            "Say bad example.",
            "Oh yeah, so you can get in the same kind of trouble here as you get with kernels, where if you separate most of the data correctly, but the ones you make a mistake on our way off, they can kill you here.",
            "Just like with Colonel, so they you know most of the data is more similar to their type.",
            "The other type, but a few ones are really more similar, wrong kinds.",
            "It's the same hinge loss thing.",
            "Yeah, we don't get around the hinge loss."
        ],
        [
            "Another nice thing, let me just mention one more nice thing about this, so an issue that's come up in learning with kernels that you can port over to.",
            "Here is what if you have a collection of similarity functions and none of them individually is good for your problem, but you expect there's you think there's some convex combination of them that you think would be good for your problem.",
            "So how can you use that so?",
            "There's been work in learning with kernels, which is a very nice on making a semidefinite programming problem out of this problem of trying to find a convex combination of your kernels.",
            "That's good, but here's a really simple thing you can do in this framework.",
            "OK, really, simple.",
            "If someone hands you a collection of similarity functions and they tell you, I think some convex combination of them is good.",
            "I just don't know which one will you can do.",
            "Take your landmarks as before and just do like the stupidest thing possible.",
            "Just create one feature for every landmark similarity function pair.",
            "So, given a new example, how similar my the landmark one using similarity function one?",
            "How similar my to landmark one using similar function to how someone would love it?",
            "OK, make this R * D length vector.",
            "Run the same L1 optimization or window that you had before."
        ],
        [
            "And the point is that.",
            "That the number of similarity functions is only going to come in this log.",
            "It only comes in the log because.",
            "The L1 margin doesn't change, so.",
            "You still have a separator here of large L1 margin.",
            "The only thing that's growing with our is the dimension of your space and the ZL1 algorithms.",
            "They only have a log rhythmic dependence on dimension of the space, so you get the really nicely and not very much penalty for throwing a whole bunch of similarity functions in the same way that the ZL1 algorithms like we know not much penalty for throwing a lot of features at the problem, OK?"
        ],
        [
            "In the region, let me just skip the proof, but if you look at the map and you're really wanted, which is this unknown convex combination?",
            "Yeah, that's the one I really wanted and there's some weight vector like weather wanted there.",
            "If you break it out into what it's looking at in this space, you're splitting it out, but the norm hasn't changed.",
            "You're just taking taking weights and just chopping in full pieces.",
            "But if you add up their magnitudes, it stays the same.",
            "That's too fast, but them let me.",
            "That's what's happening here, so that's nice."
        ],
        [
            "So not too much penalty because we're looking at L1.",
            "OK. Another algorithm you could do is try to do some joint optimization.",
            "There's trying to learn the convex combination the same time you're doing it there.",
            "You would potentially get, maybe even a better bound 'cause you have a.",
            "You have a smaller capacity, but we don't know how to do that efficiently.",
            "It's a nice open problem.",
            "This is something that we know it's known how to do this efficiently for kernels, but we don't know how to do this efficiently for general similarity functions that are not necessarily legal, kernels are not necessarily positive semidefinite.",
            "That's open.",
            "Alright."
        ],
        [
            "So let me now switch to the second part, which is so.",
            "Can we use this angle to think about clustering?",
            "So Sandra talk this morning about clustering and we'll see from this may be a little bit different way of looking at at at some clustering problems."
        ],
        [
            "OK, so.",
            "Alright, so I have to say your clustering example of different places.",
            "You know, given a bunch of documents or search results you want to cluster them by topic.",
            "Given a bunch of protein sequences, cluster them by function given images of people cluster bye bye who's in them?",
            "And.",
            "So we want to look at a framework for.",
            "Can you see this to see if smells?",
            "So I think of a framework for analyzing problems of this type."
        ],
        [
            "OK, so I want to model with clustering like this.",
            "We're given the data set of objects.",
            "An objects.",
            "Say maybe news are the calls an.",
            "I'm going to assume that there is some unknown correct answer.",
            "OK, so we're clustering proteins by functional.",
            "They each one has a function images by who's in them?",
            "I mean, there's a person in them, so there's some right answer.",
            "News articles you want to cluster by subject that there is some subject.",
            "If you ask somebody, they would tell you it's just nobody wants to answer the question.",
            "So we're given a bunch of objects, totally unlabeled.",
            "There's some ground truth, correct answer."
        ],
        [
            "OK, and our job is an algorithm is to try to find it.",
            "So given this unlabeled data, our goal is to produce.",
            "So say this K clusters in the ground truth, I'll call him see one star up to seek a star.",
            "Our goals produce the hypothesis clustering.",
            "See when the CK that as much as possible matches the right answer.",
            "OK, so the notion of error I want to look at is not.",
            "How well do we do buy some.",
            "Distances of.",
            "K means distance or something.",
            "I don't know how many points do we get right?",
            "How many points do we get wrong?",
            "So we'd like to get most of the points right, so our goals produce hypothesis clustering that if you were to take that overlaid on top of the correct answer that we would get most of the points right there.",
            "We would have small error.",
            "So I mean by having small error, we want to have a small number of mistakes, up to renumbering of the indices.",
            "Alright, so the sports is cluster C, One star in politics is C2 star and we call politics C1 and sports see two big deal.",
            "Let's just re number.",
            "OK, that's fine, but.",
            "You know, but if our clusters, instead of looking like this, look like that, that's bad.",
            "So what do we have to work with?",
            "We have no labels.",
            "Alright, so that's bad.",
            "But we do have a measure of similarity.",
            "So someone's hand I said look.",
            "This is a measure of similarity between documents that I think is going to be useful for clustering news articles and say, OK, great, that's nice.",
            "What conditions do we want that similarity measure dissatisfied?",
            "What conditions would be enough to allow us to cluster, well, OK, so that's the question, why?",
            "So basically the same question we're looking at before, and presumably we're going stronger properties since we don't have any labeled data.",
            "OK, and."
        ],
        [
            "Just to contrast us a little bit with some of the more standard approaches to clustering this kind of these two 2 standard approaches.",
            "One is you view your similarity information as your ground truth in some sense, and you look at your ability of your algorithms to achieve different optimization criteria.",
            "So you might look at how well can you approximate the K means objective K meeting objective case center objective.",
            "OK, so we don't want to look at because we're thinking of their measure of similarity is just a heuristic.",
            "Someone had this to us.",
            "Our goal is not to get to K means optimal solution.",
            "Our goal is to get the points right.",
            "Another standard approach is you make a generative model, you say.",
            "Well, I assume that my data comes from mixture of Gaussians again.",
            "I don't want to assume our data is a mixture of Gaussians that the documents are picked in some random process.",
            "The documents of the documents someone is handing us a measure of similarity and I really want to look at the question of what properties should that measure of similarity have.",
            "So I don't want to make a generative assumption, just want to look at the question just like we do with kernels given some data, how powerful similarity function do we need to be able to cluster well?"
        ],
        [
            "Now just to get started, here's a condition that trivially works alright.",
            "Suppose I give you a similarity function such that for any two objects of the same type.",
            "Alright in the same cluster there are similarities bigger than zero, and Frank two objects in different clusters in the ground truth their similarities less than 0.",
            "If you gave me that that be easy, I wouldn't need.",
            "You know any sort of fancy algorithm get taken acts like everybody is more than zero.",
            "Similar to it, that's its class.",
            "St pulls away.",
            "Take another point, everything more than zero somewhere, so it's another cluster for the way right?",
            "Really easy.",
            "Great 'cause this is an extremely strong property.",
            "Fine.",
            "Well, alright, So what happens if we just make this?",
            "Let's make this a little weaker and then we're going to immediately see we get into some trouble, which is part of the."
        ],
        [
            "Reason why people tend not to analyze clustering this way.",
            "OK, but we'll see a way around it.",
            "OK, so suppose our measure of similarity is the following also really strong property?",
            "Suppose I tell you that I promise you the following all access all documents are strictly more similar to all other documents of their own.",
            "Type of their own cluster, then to any document of any other cluster.",
            "OK, it's still pretty strong property but but but we immediately get into the following issue.",
            "It's still a strong condition, but here's a problem."
        ],
        [
            "So.",
            "Imagine that I've got, you know my documents look like this.",
            "I've got documents about baseball, basketball, math and physics.",
            "And imagine I have a measure of similarity that does the thing you'd expect.",
            "The baseball documents are all really similar to each other.",
            "The basketballs are really similar to each other.",
            "The math to each other, the physics to each other.",
            "Baseball and basketball are medium similarity.",
            "Math and physics medium and really low across here.",
            "Great.",
            "So it could be this is 4 clusters, baseball, basketball, math and physics.",
            "It could be this is 2 clusters, sports and science.",
            "It could be if you ask me to cluster this.",
            "This would look like 3 clusters.",
            "To me this would look like sports math and physics and it's totally consistent with this property because every every sports document is more similar to every other sports document than to any non sports one.",
            "The maths are more similar.",
            "The maths and physics or the sports and the physics are more similar to the physics and the mass of the sports.",
            "OK, you go on the street and ask.",
            "Somebody is going to look like this.",
            "Alright, you have baseball, basketball and then the stuff with equations in it?",
            "And that's a totally consistent clustering with this property.",
            "Alright, everything is more similar to everybody else in its own cluster than anybody in any other cluster.",
            "So clearly this condition.",
            "How's our poor algorithm supposed to know whether we were looking for this answer or that answer?",
            "OK, even if I tell you I'm looking for three clusters.",
            "OK, so the condition is kind of not enough information.",
            "So the way we're going to get around."
        ],
        [
            "And it is by relaxing our goals.",
            "OK, we're going to say in a way that I think is natural, so we're going to say that it's OK for our algorithm to produce a hierarchical clustering.",
            "So a tree on subsets that will call successful if the right answer.",
            "The target clustering is approximately equal to some pruning of this tree.",
            "OK, so in this case there's a really natural tree which is all document splits into sports and science sports, plus splits into baseball, basketball, science into math and physics.",
            "And then.",
            "And then we our algorithm doesn't have to figure out.",
            "Which pruning we were thinking we'll just call it outputs this and we'll call it successful.",
            "If the thing we were looking for is approximately some printing of the tree so you can view this is saying mean one way to view this is just think about putting the top level.",
            "You know sports and science, and then you're telling the user look if any of these is too broad.",
            "You know I didn't really know how specific you wanted to be, if it's.",
            "Not specific enough, just click on and I'll just put it for you and this is telling you how to split it.",
            "OK, so where we're offloading to the user a little bit, which is well, how specific did you want to be in different parts of the space?",
            "But basically what's happening is where our loss function is a little bit relaxed.",
            "Rather than forcing the algorithm for the partition, the algorithm is allowed to have the hierarchy and the score is so our notion of being a successful hierarchy is if the right answer, the target clustering is approximately some pruning of this tree, then we'll call it successful.",
            "OK, so now another another another thing that we also look up.",
            "I'm not going to talk about here is it's a rather than outputting a single clustering.",
            "You're out allowed out with a small list of clustering is kind of like what you doin list decoding, but I'm not going to talk about this one.",
            "I think in Nina's talk she talked about about this in the context of some approximation assumptions.",
            "OK."
        ],
        [
            "Given this is going to focus on the tree one, so now we can start getting somewhere.",
            "So for example, this strong condition that felt like you know that's a pretty strong condition, although be good, the cluster in fact is sufficient to get a hierarchal clustering.",
            "With the target being appearing in the tree, in fact by very simple algorithm, so Kruskal's algorithm are single linkage, so single linkage algorithm you take all the points like Sandra talked about, take all the points, just merge the two that are most similar.",
            "Most similar, most similar, and you get this tree OK.",
            "It's not hard to show that if your similarity function has this condition, then.",
            "The tree you output will have the correct answer is some pruning of the tree.",
            "OK, I'm not going to prove that 'cause I want to show you something a little more interest."
        ],
        [
            "So here's a weaker condition.",
            "The weaker condition is maybe don't satisfy that property, but so here's a weaker property, and in fact people have looked at that property is very much like this for problems in mathematical biology and the properties that you want.",
            "Your clusters have the following condition.",
            "So for all clusters, CNC prime for all subsets A of C and a primacy prime, you want it to be that it's not the case that the egg and a primer both more similar to each other on average.",
            "Then each is to the rest of its own cluster.",
            "So if you think about like the stable marriage property, you know that you don't have like a you know husband and wife.",
            "They want to run off together.",
            "This is the clustering analogue of that.",
            "OK, so you don't have a so you think of the similarity is how attractive they are.",
            "You don't have a subset of 1 cluster in the subset of the other that are on average more attracted to each other than the rest of their own clusters may go run off together.",
            "So suppose the ground truth has the stability condition.",
            "Then you can show that this is also sufficient to get a good tree not using single linkage but using average single linkage.",
            "In fact, as we talked about 3 versions of single linkage, it was the middle, the middle one you need, the others don't work for this, but the middle one does OK and so."
        ],
        [
            "So in particular.",
            "Yeah, let me just go through how you can show this.",
            "I'm going to look at a simpler version.",
            "So remember stable marriage.",
            "You said it's stable.",
            "If for any piece of 1 cluster piece of the other they don't.",
            "It's OK if one of them likes the other one better, as long as the other one doesn't like reciprocate.",
            "OK, but let's look at the stronger version that's called strongly stable.",
            "If neither one wants to jump ship.",
            "OK, so.",
            "So let's assume that for all clusters, CNC prime for all ANC a prime in C prime A is more similar to the rest of its own cluster that a prime and then need to switching in a prime the other way too.",
            "Let's also assume our similarity function is symmetric.",
            "Everything we've been talking about actually works for asymmetric functions, but let's assume symmetric, so then.",
            "The claim is the average single linkage will succeed.",
            "What is average single linkage like Kruskal's algorithm?",
            "Every step you're merging the pair of clusters whose average similarity average pairwise similarity is highest OK. And how can we analyze this is a nice way to analyze this, which is if you think about it for a bit.",
            "You realize that what you want is, as your algorithm is glomming things together, you want the all the clusters you've produced are Lamon are with respect to the target.",
            "By that I mean that every cluster you've created is equal a subset of a target cluster equal to a target cluster or a union of target clusters.",
            "If you can maintain tap through the whole process, that'll guarantee you that somewhere along the line you've produced, the target is some printing of your tree.",
            "You guys think about that a little bit, but but it turns out to be true, so all we have to do is just show by induction that if the current state of our algorithm.",
            "Is has this property, so let's say the current state.",
            "Let's say the right answer is those two big big ovals there and the current state of our algorithm is.",
            "Is these other little pieces that after each step so each piece that we have is either a piece of attire cluster equal to entire cluster or union entire clusters.",
            "But that's also true after one step too OK."
        ],
        [
            "So that's actually not so hard to show if you look at you realize the only way that you can fail is if you if your algorithm ever takes a subset, say C1 of some cluster C and and some other things C2 which is disjoint from C and merges them together.",
            "But if you think about that, Member Algorithm merges the pair of highest average similarity, and I promised you that C one is more similar on average, the rest of its cluster than to any other piece of another cluster.",
            "So C2 is taking the role of this a prime over here.",
            "So we know that C one is more similar to the rest of its own cluster, and so this guy.",
            "That's one of our clusters that we have produced so far.",
            "That's that's at least as good as the average.",
            "So it's going to have to be some C3 there so that the similar you can C3 and C1C3 is at least as good as the average similarity of C1 to the rest of big cluster, and so that will be bigger.",
            "Better than the similarity of C1C2.",
            "So our algorithm wouldn't merge C1C2.",
            "It'll instead merge C1C3.",
            "You can show this by induction.",
            "The algorithm will do the right OK, just examples of."
        ],
        [
            "Yeah, OK so.",
            "Perfect, you can.",
            "Also another thing you can do is take some of these conditions, like say the strong one.",
            "Everybody is more similar to everybody in its own cluster than anybody in the other cluster, and you can say, well, what if that's not true for all the data.",
            "Whether it's true for most of the data, but you added in some extra data points that are like acting in a crazy way now that can totally kill a bottom up algorithm.",
            "I mean, just take one add-in.",
            "One point that thinks it's like 100% similar to everybody and all of a sudden you're single link is going to connect it to everybody.",
            "Like that, OK, so you can.",
            "You can really mess up bottom up algorithms by adding in extra noisy so maliciously noisy data or some crazy data.",
            "OK, but you can show a different kind of algorithm works where you start by creating a collection of plausable clusters.",
            "In this case what you would do is for every data point in every radius just look at the cluster of everybody is at least that similar to that point and under this condition that will include all the correct answers, all the correct clusters plus a bunch of other stuff too.",
            "And then you gotta figure out which ones to throw away.",
            "And so you can use a series of pairwise tests.",
            "Look at, let's say, well look, you know these guys are OK, but these two there's no way that both this cluster here in this cluster.",
            "They're not both allowed to exist.",
            "'cause one is not a subset of the other and are not disjoint.",
            "So you can do a pairwise test that allows you to throw out one of them, and so you can prove that a certain pairwise test to allow you to throw out everything until what's left is consistent with the tree.",
            "So you can look.",
            "So these are the kind of natural algorithms that you get under."
        ],
        [
            "Missions like this.",
            "You can also connect this to some implicit assumptions made by optimization approach to clustering, so this is with Nina had talked about on Saturday, which is if you're using an approximation algorithm for something like, say, the K median problem or K means.",
            "You're somehow if there's some right answer there, you're hoping anyway that the optimal solution to this objective is actually close to the right answer, not only optimal solution, since you only have an approximation that any approximately optimal solution is also close to the right answer, and.",
            "It turns out that you can just say, well, similarity function is good, if any, approximately optimal solution is close.",
            "The right answer and you can use that property to cluster well even if your notion of approximately optimal is better than what you actually know.",
            "How to get algorithmically, even if it's maybe even NP hard to get that that close to optimal for at least certain objectives like a median, K means min sum is a few that we know how you can use that that property to get close to the right answer."
        ],
        [
            "OK, then one can also analyze an inductive setting where everything I'm talking about is been transductive.",
            "You just have your data.",
            "You can imagine a property defined over distribution and then you draw data from the distribution and then you want to argue if this property is true over the whole distribution.",
            "Well, if I take my sample that it's true over my sample, the usual sort of thing you'd want to do so for here you need kind of different sort of.",
            "The sample complexity results seem to rely on some other other kinds of tools.",
            "Is regularity type results that in order to argue with high probability, if some condition you had was true over the whole distribution, high probability or reasonable size sample will mimic that and have the property be true over the sample.",
            "OK."
        ],
        [
            "Let me just end by saying that in a way.",
            "So I said this is a little bit like a pack model for clustering, and what I mean by that is that what we're talking about properties?",
            "What's a property, the relation between the target clustering and similarity information, and so this is this relation.",
            "It's like a data dependent concept class.",
            "OK, in particular, given data in a similarity function of property induces the concept class, the set of all clusterings consistent with the property and what we're asking in this tree model is we want to produce a tree such that the set of prunings of this tree form an epsilon cover.",
            "Of these things, so whatever the target is, we've got some close clustering to it as a printing of our tree.",
            "That's one way to view the view.",
            "What we're asking for in this model.",
            "OK."
        ],
        [
            "Good.",
            "Alright, so.",
            "OK, so we're looking at the what natural properties allow a similarity measure to be useful for clustering in order to get a good theory this, we had to relax.",
            "We mean by useful for clustering and then given this week and we were then able to analyze a number of properties and show guarantees.",
            "An algorithm is able to."
        ],
        [
            "System.",
            "Let me just.",
            "I want to get to it.",
            "Open problem.",
            "Good so.",
            "OK, so so our goal here is to be able to say alright, what if you're coming with a measure of similarity?",
            "What?",
            "What kind of properties might you want to shoot for?",
            "And given the property believer similarity measure has what's a good algorithm to use for that?",
            "I guess have a bunch of problems maybe, maybe just two?",
            "Mention one the property that any approximately optimal K median solution is close to the target that we know how to deal with the property that any approximately optimal graph separator according to the balance separator notion is closely target that we don't know how to deal with, and that would be really nice to be able to say I give you data, I promise you any good approximations.",
            "The balance separator problem is close to target, great under that condition.",
            "Can you find some close to the target?",
            "We don't have to deal with that.",
            "Island.",
            "Question.",
            "OK, well then I will be here during the breakfast with you.",
            "Wanna look into dating sites and will take a break for half an hour."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I'm going to be talking about theory of similarity functions for learning and clustering and.",
                    "label": 0
                },
                {
                    "sent": "Actually everything I'm talking about is joint work with Nina Balcan and also portions are joint work with Nati Srebro and Santosh Kampala.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let me start with kind of a 2 minute overview version of the talk and then I'll go on to the the remaining 58 minute version.",
                    "label": 0
                },
                {
                    "sent": "So suppose we're given the typical machine learning problem, say given the collection of images we want to learn a rule to distinguish men from women.",
                    "label": 1
                },
                {
                    "sent": "The problem, though, is that our representation were given, say, in terms of pixels, is not so good.",
                    "label": 1
                },
                {
                    "sent": "So for a situation like this, one powerful technique that's come about is to use a kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's a kernel?",
                    "label": 1
                },
                {
                    "sent": "It's a special kind of pairwise similarity measure.",
                    "label": 0
                },
                {
                    "sent": "It takes into objects, and it outputs a number.",
                    "label": 0
                },
                {
                    "sent": "And very useful in many different kinds of applications.",
                    "label": 0
                },
                {
                    "sent": "And also there's a very powerful theory about kernels.",
                    "label": 0
                },
                {
                    "sent": "But one thing about.",
                    "label": 0
                },
                {
                    "sent": "About the theory in terms of what is it that you want out of your kernel with you?",
                    "label": 0
                },
                {
                    "sent": "What are the properties you want your kernel to have?",
                    "label": 0
                },
                {
                    "sent": "The theories talks about viewing them as implicit mappings into some potentially high dimensional space and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Question that started this.",
                    "label": 0
                },
                {
                    "sent": "This work is can we develop a theory that just views a kernel as just a measure of similarity that talks in terms of natural properties?",
                    "label": 0
                },
                {
                    "sent": "Thinking of it as a measure of similarity?",
                    "label": 0
                },
                {
                    "sent": "And that's ideally even more general than the standard theory of kernels in not requiring it to have some of the mathematical properties you need in order to view them as implicit mappings.",
                    "label": 0
                },
                {
                    "sent": "And if we satisfy these conditions, that would still be useful for learning.",
                    "label": 0
                },
                {
                    "sent": "And then in the second part of my talk, when we talk about the problem well, what if actually we don't have any labeled data at all?",
                    "label": 0
                },
                {
                    "sent": "We only have unlabeled data, so we have.",
                    "label": 1
                },
                {
                    "sent": "We still want to learn well, but we don't have any labeled data, so we have effectively a clustering problem.",
                    "label": 0
                },
                {
                    "sent": "And can we develop a theory of properties of a similarity function that would be sufficient to be able to cluster well to learn?",
                    "label": 1
                },
                {
                    "sent": "Well when you don't have any label data at all?",
                    "label": 0
                },
                {
                    "sent": "And presumably you're going to need stronger properties when you have no labeled data.",
                    "label": 0
                },
                {
                    "sent": "And so we'll get to that.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Human version in doing this will develop a kind of a pack model.",
                    "label": 1
                },
                {
                    "sent": "Physical learning theory model for cluster.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Start with part one.",
                    "label": 0
                },
                {
                    "sent": "Alright, so first of all, so the theme of this part, so we want to talk about.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Theory of natural sufficient conditions for similarity function to be useful for binary classification.",
                    "label": 1
                },
                {
                    "sent": "Learning problems.",
                    "label": 0
                },
                {
                    "sent": "We don't want to require that our similarity function be positive semidefinite to satisfy the properties that you want for kernel.",
                    "label": 0
                },
                {
                    "sent": "Don't have implicit spaces, but we do want to include the notion of a large margin kernel in terms of these conditions.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to develop some conditions, and actually the formal level will even allow you to learn more, so we can define classes of functions that don't have large margin kernels, even if you allow yourself substantial hinge loss and yet do have good similarity functions under this notion.",
                    "label": 1
                },
                {
                    "sent": "So I'll get to that later.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me start, sorry at the beginning.",
                    "label": 0
                },
                {
                    "sent": "So let me start with kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's my.",
                    "label": 0
                },
                {
                    "sent": "62nd introduction to kernels.",
                    "label": 0
                },
                {
                    "sent": "So we have a lot of great algorithms for learning linear separators, so perceptron, SVM, so linear separators are really one of the things in learning that we use all the time, but unfortunately a lot of time we have data that's not linearly separable.",
                    "label": 1
                },
                {
                    "sent": "We need some sort of decision surface like this.",
                    "label": 0
                },
                {
                    "sent": "So what do we do so they have all the answers.",
                    "label": 0
                },
                {
                    "sent": "When I was in Graduate School, was to use a multi layer in their own network and that was the thing you would do if you.",
                    "label": 0
                },
                {
                    "sent": "Linear separate wasn't good enough and kind of newer answers.",
                    "label": 0
                },
                {
                    "sent": "Well, how about using a kernel function and a kernel function as a way of getting your linear separator algorithm to learn a nonlinear decision surface and the point of kernels is that while many algorithms and you look at how do they see the data, they only interact with the data by taking dot products of pairs of examples.",
                    "label": 0
                },
                {
                    "sent": "So they take their data and if you look at how they interact, you can write the algorithm in such a way that the only thing they do with their data as they ask for dot products of pairs, and that's the only way they look at their data.",
                    "label": 0
                },
                {
                    "sent": "So that's the interface.",
                    "label": 0
                },
                {
                    "sent": "So if that's the interface they have with data, if you just redefine that product, you'll get the algorithm do something different.",
                    "label": 0
                },
                {
                    "sent": "And for example, if you define K of XY, this is we are kernel of XY to be something else.",
                    "label": 0
                },
                {
                    "sent": "That's not that product is some other function.",
                    "label": 0
                },
                {
                    "sent": "This corresponds to taking a dot product, but in some other space, so kernel is just a redefinition of dot product, it's something it's a function that can be viewed as a dot product in some implicit space, and so for example, in this case 1 + X dot Y to the D is a legal definition of dot product, but if your original data was an end dimensional space, this corresponds to a dot product and end to the D dimensional space where this fight, some implicit mapping taking your end dimensional data into some implicit end to the D dimensional space.",
                    "label": 1
                },
                {
                    "sent": "And so your algorithm, since it only is interacting with the data via dot product, is acting as if data was in this much higher dimensional space and maybe in that higher dimensional space your data is linearly separable.",
                    "label": 1
                },
                {
                    "sent": "Great and that can allow your algorithm producing nonlinear curve in your original space.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and this here's a little example of, well, X dot Y to the D where D = 2 and your original space is a 2 dimensional space, and if your data looked like this, so this is one class, another class.",
                    "label": 0
                },
                {
                    "sent": "It's a non linear separator in the implicit space is 3 dimensional space and there you have a linear separator.",
                    "label": 0
                },
                {
                    "sent": "Great and the other thing it's not.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a more over if it's not.",
                    "label": 0
                },
                {
                    "sent": "If data has, so if your kernel has the property that the implicit space, not only is the data linearly separable, but it's linearly separable by some large margin gamma, then you get you can generalize well as a function of this margin.",
                    "label": 0
                },
                {
                    "sent": "So if you have margin gamma in this implicit space, you only need sample size roughly one over gamma squared to have confidence in your ability to do well on new data.",
                    "label": 0
                },
                {
                    "sent": "Now we have to talk about normalization, normalize everything to be inside the unit ball.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, talk about whether units.",
                    "label": 0
                },
                {
                    "sent": "Otherwise the radius squared over gamma squared, so normalize everything, radius one and then you need only order one over gamma squared examples to be able to generalize well, and there's no direct dependence on the dimension great.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is the standard theory of colonels, and colonels are useful in practice for dealing with many, many different kinds of data.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the issue I want to get out is that in practice, if you're thinking about some learning problem and trying to come up with what you think might be a good kernel, you're thinking about it by viewing it as a measure of similarity between your data object.",
                    "label": 1
                },
                {
                    "sent": "You're thinking, Gee, what would be a good way of measuring how similar I don't know?",
                    "label": 0
                },
                {
                    "sent": "Two DNA sequences are two images are, but our theory is talking about margins in some implicit space, and so it may not be the best for intuition and trying to decide.",
                    "label": 1
                },
                {
                    "sent": "Gee, you know whether I think would be a.",
                    "label": 0
                },
                {
                    "sent": "A good kernel to use.",
                    "label": 0
                },
                {
                    "sent": "Also we have this technical requirement that our kernel has to be something you can view as an implicit dot product and that might rule out the most natural similarity function for your domain.",
                    "label": 0
                },
                {
                    "sent": "Now you could take that similarity function kind of coerce it into, you know, bang into a legal kernel, but maybe you'd like to use it directly.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 1
                },
                {
                    "sent": "So coming up with some alternative, maybe more general theoretical explanation about what it is that we want in our similarity measure that would allow us to use it for learning OK?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about is a notion of what what we might want in a good similarity function.",
                    "label": 1
                },
                {
                    "sent": "OK, so I want so a sufficient condition for a similarity function to be allow us to learn well with the following properties.",
                    "label": 0
                },
                {
                    "sent": "So we'd like to talk in terms of natural direct quantities.",
                    "label": 1
                },
                {
                    "sent": "Without talking about an implicit space without requiring that our similarity function be of this special type that you can view it as an implicit dot product, and if you satisfy these conditions we then we'd like to be able to conclude that we can use it to learn well, so we'd like to have an algorithm that, if you have a similarity function, has these properties, and we can use it to learn well.",
                    "label": 0
                },
                {
                    "sent": "We'd like our death our notion to be broad, broad enough.",
                    "label": 0
                },
                {
                    "sent": "That includes the usual notion of a good kernel, one that has a large margin in its implicit space.",
                    "label": 1
                },
                {
                    "sent": "And even will see in a formal sense of a notion, even allows you to do do some things that you can't do with large margin kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm going to do is I'm going to start with the notion that.",
                    "label": 0
                },
                {
                    "sent": "Would be sort of nice intuitive, but won't be broad enough to include the.",
                    "label": 0
                },
                {
                    "sent": "All large margin kernel so it will include things that aren't legal Colonel, so so it's not contained inside the notion of large margin kernels, but it doesn't include all of them.",
                    "label": 0
                },
                {
                    "sent": "And then once we see that then we'll see how to generalize that into our main notion, which will include.",
                    "label": 0
                },
                {
                    "sent": "Large margin kernel functions.",
                    "label": 0
                },
                {
                    "sent": "So let me start with the first one, so we'll see that it's.",
                    "label": 0
                },
                {
                    "sent": "The notion that's intuitive, and then we'll see that if we satisfy that condition, we can use to use it to learn.",
                    "label": 0
                },
                {
                    "sent": "Well, I guess I won't prove it's intuitive.",
                    "label": 0
                },
                {
                    "sent": "I can't do that, but I can prove that if you have 5, the condition that you can use to learn, well, OK?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have, so we have a learning classification problem.",
                    "label": 0
                },
                {
                    "sent": "So we have a distribution over labeled examples, positive and negative examples, and our goal is to output a good classification rule, one that has low air.",
                    "label": 1
                },
                {
                    "sent": "And let's say that our similarity function is good if most examples are on average more similar to random examples of their own class than to random examples of the other class.",
                    "label": 1
                },
                {
                    "sent": "OK, so we have positive and negative examples and will say our similarity function is good for this problem.",
                    "label": 1
                },
                {
                    "sent": "If most examples are on average more similar, their average similarity to examples of their label is larger than their average similarity to examples of the other label OK or in math.",
                    "label": 0
                },
                {
                    "sent": "Like this we will say that kernel are similarity function.",
                    "label": 0
                },
                {
                    "sent": "Is epsilon gamma good?",
                    "label": 0
                },
                {
                    "sent": "If most 1 minus epsilon fraction of examples satisfy the condition that their average similarity 2 examples why of their own label is larger and not just a tiny tiny exponentially small amount larger but by some gap gamma larger than their average similarity 2 examples of the other label?",
                    "label": 0
                },
                {
                    "sent": "Seems like a natural condition you might ask of a notion of similarity between points.",
                    "label": 0
                },
                {
                    "sent": "You say?",
                    "label": 0
                },
                {
                    "sent": "Well, I've got this problem distinguishing.",
                    "label": 0
                },
                {
                    "sent": "You know cats from dogs and and well, I want some measure of similarity so that most of the cats are on average more similar to cats and dogs, and most the dogs are in average more similar dogs and cats good.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a reasonably intuitive condition to ask for, and if you satisfy this condition, it's actually also easy to learn well, so let's just take a look at that.",
                    "label": 0
                },
                {
                    "sent": "Suppose I were to give you a similarity notion between between our data objects that had this property.",
                    "label": 0
                },
                {
                    "sent": "Well, how could we use it to learn well?",
                    "label": 0
                },
                {
                    "sent": "Well, actually it's not so hard.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So very simple.",
                    "label": 0
                },
                {
                    "sent": "This is actually a pretty strong condition and very simple algorithm that we could use to learn well if somebody handed us a measure of similarity between data objects that had this property.",
                    "label": 0
                },
                {
                    "sent": "So how could we do it?",
                    "label": 0
                },
                {
                    "sent": "All we have to do is the following.",
                    "label": 0
                },
                {
                    "sent": "We just do average nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "We take a bunch of random positive examples, S plus take a bunch of random negative examples S minus and now just classify or new example X that we see based on whether it's on average more similar to the points in S plus or an average more similar to the points in S minus.",
                    "label": 1
                },
                {
                    "sent": "OK. That makes sense.",
                    "label": 0
                },
                {
                    "sent": "I guess I should have said just also I said sort of implicitly.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming our notion of similarity that the scores are between plus one and minus one.",
                    "label": 0
                },
                {
                    "sent": "So you think A plus one is being really similar and minus one is being really different.",
                    "label": 0
                },
                {
                    "sent": "That corresponds to for kernel saying that in the implicit space everything is inside the unit ball, because there you have dot products and so you've got dot product one dot product minus one dot product zero.",
                    "label": 0
                },
                {
                    "sent": "If you're 90 degrees, so I'm assuming, like otherwise, the gamma would be sort of unitless.",
                    "label": 0
                },
                {
                    "sent": "Quantity wouldn't make sense.",
                    "label": 0
                },
                {
                    "sent": "So I'm assuming similarity is something between one and minus one.",
                    "label": 0
                },
                {
                    "sent": "Great so.",
                    "label": 0
                },
                {
                    "sent": "So I'll just draw two sets.",
                    "label": 0
                },
                {
                    "sent": "Bunch of positives, bunch of negatives, classifying new example based on.",
                    "label": 0
                },
                {
                    "sent": "Are you on average more similar?",
                    "label": 0
                },
                {
                    "sent": "Test plus minus?",
                    "label": 0
                },
                {
                    "sent": "And if you're more similar, plus you call a positive more somewhere else minus call it negative OK, and we could analyze this.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a bunch of points.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "And we can show the following so.",
                    "label": 0
                },
                {
                    "sent": "So if our always to do is take S plus must be big enough, roughly one over gamma squared times some log factors, then with high probability this rule will have only error epsilon prime more than our base error epsilon.",
                    "label": 0
                },
                {
                    "sent": "So we're already losing epsilon was so defined so that there could be an epsilon fraction of points.",
                    "label": 0
                },
                {
                    "sent": "It just don't behave fine.",
                    "label": 0
                },
                {
                    "sent": "You'll lose them, and then you'll only lose a little bit more.",
                    "label": 0
                },
                {
                    "sent": "OK, and I didn't write out the proof of this theorem, but the proof is actually.",
                    "label": 0
                },
                {
                    "sent": "Pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                },
                {
                    "sent": "Fix a point access.",
                    "label": 0
                },
                {
                    "sent": "Find that condition.",
                    "label": 0
                },
                {
                    "sent": "This is a big enough draw.",
                    "label": 0
                },
                {
                    "sent": "So what are we doing?",
                    "label": 0
                },
                {
                    "sent": "RASTA meeting that expectation from samples estimating that expectation from samples and this is a big enough sample so that way if you fix X first and then draw your S plus and S minus by huffing bounds with very high probability, you are going to estimate your expectations close enough to get the right answer.",
                    "label": 0
                },
                {
                    "sent": "OK, and your failure probability will be only Delta times epsilon prime.",
                    "label": 0
                },
                {
                    "sent": "So if you pick and accessing this condition.",
                    "label": 0
                },
                {
                    "sent": "Then you draw your S plus minus with high probability.",
                    "label": 0
                },
                {
                    "sent": "You'll get the right answer on X and then you just use Markov inequality to flip this around to say that if S plus and S minus are the size and then with high probability, they're only going to fail on some epsilon prime fractional points.",
                    "label": 1
                },
                {
                    "sent": "OK, and that gives us the proof.",
                    "label": 0
                },
                {
                    "sent": "Great, so if we have a similarity function satisfying this condition, then there's a very simple algorithm that we can use to learn well.",
                    "label": 0
                },
                {
                    "sent": "Just take a bunch of random positives, a bunch of random negatives classifying new point based on.",
                    "label": 0
                },
                {
                    "sent": "Are you on average more similar to these positives?",
                    "label": 0
                },
                {
                    "sent": "Oregon average more similar?",
                    "label": 0
                },
                {
                    "sent": "These negatives, 'cause we're estimating these expectations when you can see where the one over gamma squared is coming from?",
                    "label": 0
                },
                {
                    "sent": "Very simply here 'cause what's happening.",
                    "label": 0
                },
                {
                    "sent": "Just trying to estimate you got, you know, like a coin of 1 bias account of another bias.",
                    "label": 0
                },
                {
                    "sent": "There's a little gap gamma.",
                    "label": 0
                },
                {
                    "sent": "You gotta flip it one over gamma squared times times some log factors.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "UNF.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fortunately.",
                    "label": 0
                },
                {
                    "sent": "This condition is not broad enough to include all large margin kernels.",
                    "label": 1
                },
                {
                    "sent": "And you might suspect that because if you satisfy this condition, we had a really simple algorithm, namely average nearest neighbor, whereas for a large margin kernel we use something like SVM's you if this included with all the large margin kernels, you wouldn't need SVM, you could just use average nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh yeah, this is a constant, so just just take your huffing bound and you say.",
                    "label": 0
                },
                {
                    "sent": "You know, just want with high probability you just wanna know how how many times you flip these coins and the constant.",
                    "label": 0
                },
                {
                    "sent": "Just because this is a random variable that's bounded between plus 1 -- 1, not exactly a coin.",
                    "label": 0
                },
                {
                    "sent": "It's random variable T + 1 -- 1.",
                    "label": 0
                },
                {
                    "sent": "You gotta estimate that up to plus or minus like gamma over 2.",
                    "label": 0
                },
                {
                    "sent": "In order for that expectation to be on the correct side of that expectation.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let's take an example of why this is not broad enough, and then how do we fix that?",
                    "label": 0
                },
                {
                    "sent": "OK, so here's a simple example.",
                    "label": 0
                },
                {
                    "sent": "Consider the case of.",
                    "label": 0
                },
                {
                    "sent": "Data, so we've got all of our negative examples.",
                    "label": 0
                },
                {
                    "sent": "Is the unit circle.",
                    "label": 0
                },
                {
                    "sent": "All are negative examples are here.",
                    "label": 0
                },
                {
                    "sent": "This is a 30 degree angle.",
                    "label": 0
                },
                {
                    "sent": "The positives are split.",
                    "label": 0
                },
                {
                    "sent": "Half of them up there and half of them over there.",
                    "label": 0
                },
                {
                    "sent": "That's also 30 degree angle, and our kernel is just regular dot product, so the implicit space is the actual space, which is just this screen here.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's 2 dimensional problem on this screen.",
                    "label": 0
                },
                {
                    "sent": "Now this is a large margin kernel because there's a nice large margin there.",
                    "label": 0
                },
                {
                    "sent": "The margin is 1/2, it's the.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It's a margin 1/2 it's the sign of 30 degrees, but it doesn't satisfy our condition, so let's see why those points in the upper right are actually more similar.",
                    "label": 0
                },
                {
                    "sent": "On average to the negatives, and they are the positives.",
                    "label": 0
                },
                {
                    "sent": "Well, why is that?",
                    "label": 0
                },
                {
                    "sent": "Well, their average similarity to the negatives by dot product.",
                    "label": 0
                },
                {
                    "sent": "It's so this is our notion of similarity.",
                    "label": 0
                },
                {
                    "sent": "The 60 degree angle here that dot product is 1/2.",
                    "label": 0
                },
                {
                    "sent": "So the average similarity of those guys to the negative 1/2 the average similarity, the positives well, half of the positives are also over.",
                    "label": 0
                },
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "They have similarity.",
                    "label": 0
                },
                {
                    "sent": "One the other half of the positives, right angle 120 degrees dot product negative 1/2.",
                    "label": 0
                },
                {
                    "sent": "So they get the average of one negative half is only 1/4.",
                    "label": 0
                },
                {
                    "sent": "So on average there only 1/4 similar to the positives.",
                    "label": 0
                },
                {
                    "sent": "But there have similar the negatives.",
                    "label": 0
                },
                {
                    "sent": "So according to this measure, some are the those.",
                    "label": 0
                },
                {
                    "sent": "No, cats are more similar to the dogs and then then then the typical cat.",
                    "label": 1
                },
                {
                    "sent": "I guess this is where there's two kinds of cats anyway, OK?",
                    "label": 1
                },
                {
                    "sent": "So it does have a large margin, doesn't satisfy our condition, so it's not brought up.",
                    "label": 1
                },
                {
                    "sent": "It's sufficient for learning, but it's not broad enough to capture the notion of large margin kernels.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's an idea for broadening the definition.",
                    "label": 0
                },
                {
                    "sent": "You notice if we.",
                    "label": 0
                },
                {
                    "sent": "If we didn't take our points, why?",
                    "label": 0
                },
                {
                    "sent": "As long as we didn't pick them from over there, we would've been alright.",
                    "label": 0
                },
                {
                    "sent": "So let's broaden our condition to say that.",
                    "label": 0
                },
                {
                    "sent": "For our our notion of similarity good, it's sufficient for there to be nonnegligible region are of reasonable points.",
                    "label": 0
                },
                {
                    "sent": "Let's call them so that most acts are on average more similar to the reasonable points of their label then to the reasonable points of the other label.",
                    "label": 1
                },
                {
                    "sent": "Alright, so most of the cats are on average more similar to the reasonable cats in the reasonable dogs, and most of the dogs are on average more similar to the reasonable dogs and reasonable cats.",
                    "label": 0
                },
                {
                    "sent": "OK, and now if our algorithm knew what this like, if you had to say oh and Furthermore, these are the ones that are reasonable.",
                    "label": 0
                },
                {
                    "sent": "And then you could just use the previous approach.",
                    "label": 0
                },
                {
                    "sent": "But if you don't know, so we'll say that it's a good similarity function.",
                    "label": 1
                },
                {
                    "sent": "If this set are exists, even if our algorithm doesn't know in advance.",
                    "label": 0
                },
                {
                    "sent": "So our algorithms now a tougher time, 'cause we're going to have to somehow learn and figure out this reasonable set together.",
                    "label": 0
                },
                {
                    "sent": "OK, but that's going to be our notion.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the broader definition.",
                    "label": 1
                },
                {
                    "sent": "Will say that our similarity function and now we can if we want to parameterize it out of epsilon, gamma, Tau good.",
                    "label": 0
                },
                {
                    "sent": "If there exists a set R of reasonable points, why such that most exo one of mice epsilon fraction are on average more similar to the reasonable points of their label than to the reasonable points of the other label by some gap gamma, and Furthermore reasonable points should exist.",
                    "label": 1
                },
                {
                    "sent": "They shouldn't be like you know, vanishingly small probability mass.",
                    "label": 0
                },
                {
                    "sent": "There should be at least a Tau probability mass of reasonable positives and reasonable negative, so you should be able to see them in a random sample.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the property we're going to want.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's just like the previous one, but we're now weakening our requirement by allowing the previous thing to be violated on on points that are not reasonable.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's only the reason you have to be more similar to the reasonable guys of your label then to the reasonable ones the other way.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Life.",
                    "label": 0
                },
                {
                    "sent": "Just just for the so I want most exit, so this is a single reasonable set.",
                    "label": 0
                },
                {
                    "sent": "You could partition it into reasonable positives in the reasonable negatives, and I want is most, let's say, oh, let's say ideally like we talked about large margin separator use of 1st talk about separating all the points by a large margin.",
                    "label": 0
                },
                {
                    "sent": "Then you say, well, it's OK.",
                    "label": 0
                },
                {
                    "sent": "If you have some hinge loss, so the zero hinge loss version would be all axes should be on average more similar to the reasonable.",
                    "label": 1
                },
                {
                    "sent": "Points of their label then the reason points the other label by some amount.",
                    "label": 0
                },
                {
                    "sent": "Kim, that's that's the zero hinge loss version.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in there like the support vectors, but instead normally we think of SVM.",
                    "label": 0
                },
                {
                    "sent": "You think of wanting a few support vectors.",
                    "label": 0
                },
                {
                    "sent": "Here we want to we want a lot.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to see them.",
                    "label": 0
                },
                {
                    "sent": "We want a large probability mass of things and.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "And if if the distribution has these point masses will need the reasonable missed itself be a probabilistic function.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you can just have it be binary.",
                    "label": 0
                },
                {
                    "sent": "And actually, as mentioned, matching in the answer to your question, technically in the same way that if you want SVM to work, if you say well, I couldn't separate all the points you know by Gap gamma only, most of them.",
                    "label": 0
                },
                {
                    "sent": "What you really want is to have small hinge loss.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "You don't say I want to have most of the points separated by Gap game, you say, well, I want my total hinge loss to be most some some small value.",
                    "label": 0
                },
                {
                    "sent": "And similarly here what we really want is that.",
                    "label": 0
                },
                {
                    "sent": "When I say most of the exercises for this condition technically what I really want is at most some small hinge loss by hinge lost.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by hinge loss here?",
                    "label": 0
                },
                {
                    "sent": "Is we want back quality there be bigger than that quantity, thereby gamma?",
                    "label": 0
                },
                {
                    "sent": "That's great if you don't satisfy the condition.",
                    "label": 0
                },
                {
                    "sent": "Like maybe you have the same similarity to your label and the other label that counts hinge loss of one.",
                    "label": 0
                },
                {
                    "sent": "Well, if you're actually gamma more similar to the other labels in your label, that counts is hinge loss of two.",
                    "label": 0
                },
                {
                    "sent": "If you're 10 gamma more similar to the other label, then your label, then you're really bad.",
                    "label": 0
                },
                {
                    "sent": "That's bigger hinge loss.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So we really want the hinge loss version.",
                    "label": 0
                },
                {
                    "sent": "OK so I click this is a broader condition, but I claim you can still learn with this even if you don't know the set art and learn by a very kind of a natural kind of algorithm.",
                    "label": 0
                },
                {
                    "sent": "And notice one thing here is that there's no requirement.",
                    "label": 0
                },
                {
                    "sent": "Are similarity measure be positive semidefinite?",
                    "label": 0
                },
                {
                    "sent": "They have this dot product.",
                    "label": 0
                },
                {
                    "sent": "Relation fact doesn't need to be symmetric function, so you know.",
                    "label": 0
                },
                {
                    "sent": "It just has to be bounded.",
                    "label": 0
                },
                {
                    "sent": "That's the only thing.",
                    "label": 1
                },
                {
                    "sent": "So here's the algorithm you can use will do.",
                    "label": 0
                },
                {
                    "sent": "It will just take a.",
                    "label": 0
                },
                {
                    "sent": "You will draw a set of random, unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "They could have labels too.",
                    "label": 0
                },
                {
                    "sent": "I guess that's actually for this version, maybe I do know.",
                    "label": 0
                },
                {
                    "sent": "OK they can be unlabeled.",
                    "label": 0
                },
                {
                    "sent": "Take a bunch of random points.",
                    "label": 0
                },
                {
                    "sent": "I can ignore their labels and I'll call them landmarks.",
                    "label": 0
                },
                {
                    "sent": "Their random points, call them landmarks and now will explicitly represent our data, so this is.",
                    "label": 0
                },
                {
                    "sent": "But what you might call an empirical similarity map will just take each example and explicitly represented according to features where the first feature is.",
                    "label": 0
                },
                {
                    "sent": "How similar are you the landmark one?",
                    "label": 0
                },
                {
                    "sent": "The second feature is how similarity to landmark two that they thought the D features.",
                    "label": 0
                },
                {
                    "sent": "How similar are you the landmark D sweet Aicardi landmarks were kind of triangulate Ng the points.",
                    "label": 0
                },
                {
                    "sent": "How similar you did?",
                    "label": 0
                },
                {
                    "sent": "This has some argue this, just write it out like that.",
                    "label": 0
                },
                {
                    "sent": "OK so that will do that.",
                    "label": 0
                },
                {
                    "sent": "So now we have an explicit representation of our data.",
                    "label": 0
                },
                {
                    "sent": "Alright, so pictorially.",
                    "label": 0
                },
                {
                    "sent": "We have this weird space and we've mapped it into this into some other space here.",
                    "label": 0
                },
                {
                    "sent": "And now the following, I claim the following property, that is, if we take enough landmarks and roughly we're going to need one over gamma squared tell.",
                    "label": 0
                },
                {
                    "sent": "So we're just coming from.",
                    "label": 1
                },
                {
                    "sent": "I said there's at least our probability mass of reasonable positives and reasonable negatives, so this is enough points that with high probability we've got our one over gamma squared reasonable positive.",
                    "label": 0
                },
                {
                    "sent": "Somewhere in here we don't know where they are.",
                    "label": 0
                },
                {
                    "sent": "One over gamma squared.",
                    "label": 0
                },
                {
                    "sent": "Reasonable negatives.",
                    "label": 0
                },
                {
                    "sent": "Somewhere in here we don't know where they are, but somewhere there.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we take enough landmarks and with high probability in this space there is a large.",
                    "label": 0
                },
                {
                    "sent": "There's a good separator, in fact, not only a good separator, but a separator with a large L1 margin.",
                    "label": 0
                },
                {
                    "sent": "OK so well, So what do I mean?",
                    "label": 0
                },
                {
                    "sent": "So in particular, consider the following weight vector in this space.",
                    "label": 0
                },
                {
                    "sent": "Consider the wavevector.",
                    "label": 0
                },
                {
                    "sent": "OK, so remember this dimensional space that gives away 0.",
                    "label": 0
                },
                {
                    "sent": "So I'm just talking about something that exists, will talk later about what the fine but just would exist.",
                    "label": 0
                },
                {
                    "sent": "Consider the weight factor that gives weight 0 to the points.",
                    "label": 0
                },
                {
                    "sent": "Why that were not reasonable?",
                    "label": 0
                },
                {
                    "sent": "OK, wait zero.",
                    "label": 0
                },
                {
                    "sent": "Consider it will give way 1 / N plus, where N plus is the number of reasonable positives in here.",
                    "label": 0
                },
                {
                    "sent": "OK, so the 57 reasonable pauses in here they get wait 1 / 57 gives those to the reasonable positives and gives weight negative 1 / N minus to the reasonable negatives in here where N minus the number of reasonable negative.",
                    "label": 0
                },
                {
                    "sent": "So if they were 62 reasonable magazine here they will get wait negative 1 / 62.",
                    "label": 0
                },
                {
                    "sent": "What is what is it doing?",
                    "label": 0
                },
                {
                    "sent": "What is www.fofxdoing?",
                    "label": 0
                },
                {
                    "sent": "Tell me the F of X is doing the exact same thing that previous average nearest neighbor was doing over the reasonable points that we have X is saying.",
                    "label": 0
                },
                {
                    "sent": "Take your average similarity to the reasonable positives in here.",
                    "label": 0
                },
                {
                    "sent": "Compare that to the average similarity, reasonable negatives, and then just see if that's great.",
                    "label": 0
                },
                {
                    "sent": "Uncle is here or not is saying well, which is bigger.",
                    "label": 0
                },
                {
                    "sent": "So this weight vector, which we which we're saying exists, it's just doing.",
                    "label": 0
                },
                {
                    "sent": "This weight vector is just doing the same as our previous algorithm over the reasonable points and so the analysis we just had implies that if we take enough landmarks so that.",
                    "label": 0
                },
                {
                    "sent": "There should be a little~ over here, 'cause I'm ignoring the log factors if we.",
                    "label": 0
                },
                {
                    "sent": "If we take enough landmarks with high probability, we have enough reasonable pauses, reasonable negatives to satisfy that bound.",
                    "label": 0
                },
                {
                    "sent": "We had two slides ago.",
                    "label": 0
                },
                {
                    "sent": "This way vector will with high probability, separate most of the points, and Furthermore will actually do so by a good margin.",
                    "label": 0
                },
                {
                    "sent": "We didn't analyze the margin before, but it will also have a good margin.",
                    "label": 0
                },
                {
                    "sent": "It'll have margin, say gamma over 2.",
                    "label": 0
                },
                {
                    "sent": "OK, and Furthermore this weight vector has Lowell one length, it's L1 length is just two because all these 1 / N plus is add up to one and all these negative one over and minuses add up to negative one.",
                    "label": 0
                },
                {
                    "sent": "So the magnitude is 1.",
                    "label": 0
                },
                {
                    "sent": "So they want like this too.",
                    "label": 0
                },
                {
                    "sent": "So we have a small L1 length vector.",
                    "label": 0
                },
                {
                    "sent": "They give us a nice margin.",
                    "label": 0
                },
                {
                    "sent": "Great, now we don't know what it is, that's unfortunate, but the good thing is we have nice algorithms for learning.",
                    "label": 0
                },
                {
                    "sent": "When you have a linear separator of large L1 margin, so all you have to do is now that you've done this, take a fresh set of labeled examples, project them into this space, and run your favorite algorithm for learning.",
                    "label": 1
                },
                {
                    "sent": "When there's a lol one separator.",
                    "label": 0
                },
                {
                    "sent": "Or large L1 margin linear separator like window or something like that.",
                    "label": 0
                },
                {
                    "sent": "OK, and if you do that, your standard bounds for a safer window will say so you take your data.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Protecting the space.",
                    "label": 0
                },
                {
                    "sent": "Run your algorithm, get your separator and your standard bounds for an algorithm like we know will say that with high probability you'll get to error your base error epsilon plus some extra epsilon sub accuracy.",
                    "label": 0
                },
                {
                    "sent": "If you have how much unlabeled unit you need, we need this roughly one over gamma squared.",
                    "label": 0
                },
                {
                    "sent": "Tell how much label data do we need?",
                    "label": 0
                },
                {
                    "sent": "Need one over gamma squared times, epsilon accuracy times?",
                    "label": 0
                },
                {
                    "sent": "Log the dimension of the space 'cause the L1 algorithms only have to pay login dimension of the space.",
                    "label": 0
                },
                {
                    "sent": "And so that so with that much data, we can then learn well.",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of.",
                    "label": 0
                },
                {
                    "sent": "In the same way that something like SVM is a natural algorithm, you think about having a large margin Colonel in the usual sense.",
                    "label": 0
                },
                {
                    "sent": "This is the become sort of a natural algorithm for similarity function satisfying this definition.",
                    "label": 0
                },
                {
                    "sent": "This is really going to L1 type of type.",
                    "label": 0
                },
                {
                    "sent": "This gamma here is really an L1 type of margin.",
                    "label": 0
                },
                {
                    "sent": "OK, so so so the other thing it's nice is that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can show that.",
                    "label": 0
                },
                {
                    "sent": "So what we've shown, we've shown that if you satisfy this property, we have an algorithm.",
                    "label": 0
                },
                {
                    "sent": "We can use an.",
                    "label": 0
                },
                {
                    "sent": "You can also show that if your similarity function really is a legal kernel, and it really does have some large margin gamma, then they will also satisfy this condition.",
                    "label": 1
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the algorithm doesn't know the reasonable set, just comes in and improving it.",
                    "label": 0
                },
                {
                    "sent": "There exists a weight vector and you need.",
                    "label": 0
                },
                {
                    "sent": "There to be enough reasonable points.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So where do you get in trouble?",
                    "label": 0
                },
                {
                    "sent": "If there's not very many of them?",
                    "label": 0
                },
                {
                    "sent": "Remember if you think about what this weight vector is doing, it's doing this empirical vote over the reasonable pauses and reasonable negatives, so you need enough points in your data set so that the previous algorithm has enough.",
                    "label": 0
                },
                {
                    "sent": "You know you're taking a good vote.",
                    "label": 1
                },
                {
                    "sent": "You got your one over gamma squared positive to vote, and your one over gamma squared negative to vote.",
                    "label": 0
                },
                {
                    "sent": "So will you just need is is in the set of landmarks you don't know where they are.",
                    "label": 1
                },
                {
                    "sent": "There's enough enough reasonable points that they can have a vote so that they're there.",
                    "label": 0
                },
                {
                    "sent": "Their vote is able to estimate these expectations well.",
                    "label": 0
                },
                {
                    "sent": "That's all good.",
                    "label": 1
                },
                {
                    "sent": "Ality it would change it.",
                    "label": 0
                },
                {
                    "sent": "I thought that your picture here would have some linear separator maybe.",
                    "label": 0
                },
                {
                    "sent": "So right so?",
                    "label": 0
                },
                {
                    "sent": "In some sense here, there's nothing about where they have to be, you know.",
                    "label": 0
                },
                {
                    "sent": "Oh, they're independent test sample, so if I could be that there's multiple like in the previous case, I said, well, if you only pick the points from over here, things work.",
                    "label": 0
                },
                {
                    "sent": "You could also only pick the points mean there's a different possible you can have set to work.",
                    "label": 0
                },
                {
                    "sent": "This would work, or that would work in the same way that for SVM you can have different kinds of sets of support vectors, and that's independent test sample.",
                    "label": 1
                },
                {
                    "sent": "Right, so that in fact, what's happening is that in this space there exists a separator that zero in most of the coordinates.",
                    "label": 0
                },
                {
                    "sent": "So most of these are totally irrelevant, and there exists a separate that uses only a small number of coordinates in this space and is able to separate there.",
                    "label": 0
                },
                {
                    "sent": "Oh so so how do you get good generalization?",
                    "label": 0
                },
                {
                    "sent": "So that's where the gamma comes in.",
                    "label": 0
                },
                {
                    "sent": "So the the.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Let me go here good so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In order to be able to generalize well, you need to have an unlabeled sample of that size, one over gamma squared tail, so it's gamma goes down.",
                    "label": 0
                },
                {
                    "sent": "You need more unlabeled data as to how it goes down anymore.",
                    "label": 0
                },
                {
                    "sent": "Unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "You're labeled data also has one over gamma squared, so so gamma is really controlling.",
                    "label": 0
                },
                {
                    "sent": "You know if Gamma is small, you need more labeled data, be able to generalize well, and now.",
                    "label": 0
                },
                {
                    "sent": "The question maybe is a function of how many landmarks, so the more landmarks you have, the more labeled data points you need, but it's only log rhythmic because of the bounds you get for L1 margins so.",
                    "label": 0
                },
                {
                    "sent": "So I guess what's controlling?",
                    "label": 0
                },
                {
                    "sent": "I guess the way I would say this way a small gamma is like a large complexity in a large game is like a small complexity.",
                    "label": 0
                },
                {
                    "sent": "You can think of gamma or one over gamma as sort of your complexity term.",
                    "label": 0
                },
                {
                    "sent": "Is that goes to 0.",
                    "label": 0
                },
                {
                    "sent": "This similarity function becoming worse and worse and worse, and your.",
                    "label": 0
                },
                {
                    "sent": "Needing more data in order to be able to get that generalization, if it's an answer.",
                    "label": 0
                },
                {
                    "sent": "Press.",
                    "label": 0
                },
                {
                    "sent": "'cause in the end what you're doing is you're learning a linear separator of of this margin, roughly gamma in this space.",
                    "label": 0
                },
                {
                    "sent": "Opening OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we can show that if a similar function is illegal kernel and it has a large margin property in its implicit space, it also satisfies this condition, although there's a loss in translation.",
                    "label": 0
                },
                {
                    "sent": "So if you have margin gamma in the usual L2 cents in your implicit space, that gamma get squared.",
                    "label": 0
                },
                {
                    "sent": "And then there's some towels that crop in into this definition.",
                    "label": 0
                },
                {
                    "sent": "So there's there is some amount of loss in translating this way, But if you have.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "But in terms of right?",
                    "label": 0
                },
                {
                    "sent": "So if you have a large margin kernel, it also is a good similarity function of the parameters get a little bit worse.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, you can also show that.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can have cases where you can have good similarity functions where you don't have large margin kernels, so one interesting thing is you can show that there exist classes in distributions where there's a similarity function that has a large large gamma in this sense for all functions in the class, but there's no kernel that has a large margin for all functions in the.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Laugh.",
                    "label": 0
                },
                {
                    "sent": "OK, and in particular it turns out and there's a couple of ways to show this that for any class of pairwise uncorrelated functions.",
                    "label": 0
                },
                {
                    "sent": "So if you have some distribution of data and I give you a list of functions that pairwise are uncorrelated, so to say they reach 5050 positive and negative, and the probability of being positive for one given that you're positive for the other is also 5050 for all pairs, then there is no kernel that can have a large margin for all functions in the class, so that's something you can show.",
                    "label": 0
                },
                {
                    "sent": "In fact, even if you allow yourself to have substantial hinge loss.",
                    "label": 0
                },
                {
                    "sent": "You still can't get a kernel with a large margin, but you can construct a similarity function with a large gamma.",
                    "label": 0
                },
                {
                    "sent": "OK, so in a setting like this for a class of pairwise uncorrelated functions, in principle you should be able to learn from to get down to error epsilon with log of the size of the class times one over epsilon roughly examples and you can define a generic similarity function which has no error perfect margin.",
                    "label": 1
                },
                {
                    "sent": "Small towel seen a lot of unlabeled data, but if we don't count the cost of unlabeled data, if that counts, is free, then this will achieve this bound in terms of how many labeled examples you need.",
                    "label": 1
                },
                {
                    "sent": "But on the other hand, you can show that there is no good kernel in hinge loss, even if you allow yourself substantial hinge loss like 1/2, you still need margin roughly one over the square root of the size of the class, which means from your standard margin bounds you would need linear.",
                    "label": 0
                },
                {
                    "sent": "A number of functions in class examples rather than log the number of functions examples.",
                    "label": 0
                },
                {
                    "sent": "So you can show a separation.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah like say yeah, like I had a hard matrix right so?",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "Say bad example.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, so you can get in the same kind of trouble here as you get with kernels, where if you separate most of the data correctly, but the ones you make a mistake on our way off, they can kill you here.",
                    "label": 0
                },
                {
                    "sent": "Just like with Colonel, so they you know most of the data is more similar to their type.",
                    "label": 0
                },
                {
                    "sent": "The other type, but a few ones are really more similar, wrong kinds.",
                    "label": 0
                },
                {
                    "sent": "It's the same hinge loss thing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we don't get around the hinge loss.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another nice thing, let me just mention one more nice thing about this, so an issue that's come up in learning with kernels that you can port over to.",
                    "label": 0
                },
                {
                    "sent": "Here is what if you have a collection of similarity functions and none of them individually is good for your problem, but you expect there's you think there's some convex combination of them that you think would be good for your problem.",
                    "label": 0
                },
                {
                    "sent": "So how can you use that so?",
                    "label": 0
                },
                {
                    "sent": "There's been work in learning with kernels, which is a very nice on making a semidefinite programming problem out of this problem of trying to find a convex combination of your kernels.",
                    "label": 0
                },
                {
                    "sent": "That's good, but here's a really simple thing you can do in this framework.",
                    "label": 0
                },
                {
                    "sent": "OK, really, simple.",
                    "label": 0
                },
                {
                    "sent": "If someone hands you a collection of similarity functions and they tell you, I think some convex combination of them is good.",
                    "label": 1
                },
                {
                    "sent": "I just don't know which one will you can do.",
                    "label": 0
                },
                {
                    "sent": "Take your landmarks as before and just do like the stupidest thing possible.",
                    "label": 0
                },
                {
                    "sent": "Just create one feature for every landmark similarity function pair.",
                    "label": 0
                },
                {
                    "sent": "So, given a new example, how similar my the landmark one using similarity function one?",
                    "label": 0
                },
                {
                    "sent": "How similar my to landmark one using similar function to how someone would love it?",
                    "label": 0
                },
                {
                    "sent": "OK, make this R * D length vector.",
                    "label": 1
                },
                {
                    "sent": "Run the same L1 optimization or window that you had before.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the point is that.",
                    "label": 0
                },
                {
                    "sent": "That the number of similarity functions is only going to come in this log.",
                    "label": 0
                },
                {
                    "sent": "It only comes in the log because.",
                    "label": 0
                },
                {
                    "sent": "The L1 margin doesn't change, so.",
                    "label": 0
                },
                {
                    "sent": "You still have a separator here of large L1 margin.",
                    "label": 1
                },
                {
                    "sent": "The only thing that's growing with our is the dimension of your space and the ZL1 algorithms.",
                    "label": 0
                },
                {
                    "sent": "They only have a log rhythmic dependence on dimension of the space, so you get the really nicely and not very much penalty for throwing a whole bunch of similarity functions in the same way that the ZL1 algorithms like we know not much penalty for throwing a lot of features at the problem, OK?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the region, let me just skip the proof, but if you look at the map and you're really wanted, which is this unknown convex combination?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's the one I really wanted and there's some weight vector like weather wanted there.",
                    "label": 0
                },
                {
                    "sent": "If you break it out into what it's looking at in this space, you're splitting it out, but the norm hasn't changed.",
                    "label": 0
                },
                {
                    "sent": "You're just taking taking weights and just chopping in full pieces.",
                    "label": 0
                },
                {
                    "sent": "But if you add up their magnitudes, it stays the same.",
                    "label": 0
                },
                {
                    "sent": "That's too fast, but them let me.",
                    "label": 0
                },
                {
                    "sent": "That's what's happening here, so that's nice.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So not too much penalty because we're looking at L1.",
                    "label": 0
                },
                {
                    "sent": "OK. Another algorithm you could do is try to do some joint optimization.",
                    "label": 1
                },
                {
                    "sent": "There's trying to learn the convex combination the same time you're doing it there.",
                    "label": 0
                },
                {
                    "sent": "You would potentially get, maybe even a better bound 'cause you have a.",
                    "label": 0
                },
                {
                    "sent": "You have a smaller capacity, but we don't know how to do that efficiently.",
                    "label": 0
                },
                {
                    "sent": "It's a nice open problem.",
                    "label": 0
                },
                {
                    "sent": "This is something that we know it's known how to do this efficiently for kernels, but we don't know how to do this efficiently for general similarity functions that are not necessarily legal, kernels are not necessarily positive semidefinite.",
                    "label": 1
                },
                {
                    "sent": "That's open.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me now switch to the second part, which is so.",
                    "label": 0
                },
                {
                    "sent": "Can we use this angle to think about clustering?",
                    "label": 1
                },
                {
                    "sent": "So Sandra talk this morning about clustering and we'll see from this may be a little bit different way of looking at at at some clustering problems.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I have to say your clustering example of different places.",
                    "label": 0
                },
                {
                    "sent": "You know, given a bunch of documents or search results you want to cluster them by topic.",
                    "label": 1
                },
                {
                    "sent": "Given a bunch of protein sequences, cluster them by function given images of people cluster bye bye who's in them?",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So we want to look at a framework for.",
                    "label": 0
                },
                {
                    "sent": "Can you see this to see if smells?",
                    "label": 0
                },
                {
                    "sent": "So I think of a framework for analyzing problems of this type.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I want to model with clustering like this.",
                    "label": 0
                },
                {
                    "sent": "We're given the data set of objects.",
                    "label": 1
                },
                {
                    "sent": "An objects.",
                    "label": 0
                },
                {
                    "sent": "Say maybe news are the calls an.",
                    "label": 0
                },
                {
                    "sent": "I'm going to assume that there is some unknown correct answer.",
                    "label": 1
                },
                {
                    "sent": "OK, so we're clustering proteins by functional.",
                    "label": 0
                },
                {
                    "sent": "They each one has a function images by who's in them?",
                    "label": 1
                },
                {
                    "sent": "I mean, there's a person in them, so there's some right answer.",
                    "label": 0
                },
                {
                    "sent": "News articles you want to cluster by subject that there is some subject.",
                    "label": 0
                },
                {
                    "sent": "If you ask somebody, they would tell you it's just nobody wants to answer the question.",
                    "label": 0
                },
                {
                    "sent": "So we're given a bunch of objects, totally unlabeled.",
                    "label": 1
                },
                {
                    "sent": "There's some ground truth, correct answer.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and our job is an algorithm is to try to find it.",
                    "label": 0
                },
                {
                    "sent": "So given this unlabeled data, our goal is to produce.",
                    "label": 0
                },
                {
                    "sent": "So say this K clusters in the ground truth, I'll call him see one star up to seek a star.",
                    "label": 0
                },
                {
                    "sent": "Our goals produce the hypothesis clustering.",
                    "label": 1
                },
                {
                    "sent": "See when the CK that as much as possible matches the right answer.",
                    "label": 1
                },
                {
                    "sent": "OK, so the notion of error I want to look at is not.",
                    "label": 0
                },
                {
                    "sent": "How well do we do buy some.",
                    "label": 0
                },
                {
                    "sent": "Distances of.",
                    "label": 0
                },
                {
                    "sent": "K means distance or something.",
                    "label": 0
                },
                {
                    "sent": "I don't know how many points do we get right?",
                    "label": 0
                },
                {
                    "sent": "How many points do we get wrong?",
                    "label": 0
                },
                {
                    "sent": "So we'd like to get most of the points right, so our goals produce hypothesis clustering that if you were to take that overlaid on top of the correct answer that we would get most of the points right there.",
                    "label": 0
                },
                {
                    "sent": "We would have small error.",
                    "label": 1
                },
                {
                    "sent": "So I mean by having small error, we want to have a small number of mistakes, up to renumbering of the indices.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the sports is cluster C, One star in politics is C2 star and we call politics C1 and sports see two big deal.",
                    "label": 0
                },
                {
                    "sent": "Let's just re number.",
                    "label": 0
                },
                {
                    "sent": "OK, that's fine, but.",
                    "label": 0
                },
                {
                    "sent": "You know, but if our clusters, instead of looking like this, look like that, that's bad.",
                    "label": 0
                },
                {
                    "sent": "So what do we have to work with?",
                    "label": 0
                },
                {
                    "sent": "We have no labels.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's bad.",
                    "label": 0
                },
                {
                    "sent": "But we do have a measure of similarity.",
                    "label": 1
                },
                {
                    "sent": "So someone's hand I said look.",
                    "label": 0
                },
                {
                    "sent": "This is a measure of similarity between documents that I think is going to be useful for clustering news articles and say, OK, great, that's nice.",
                    "label": 0
                },
                {
                    "sent": "What conditions do we want that similarity measure dissatisfied?",
                    "label": 0
                },
                {
                    "sent": "What conditions would be enough to allow us to cluster, well, OK, so that's the question, why?",
                    "label": 1
                },
                {
                    "sent": "So basically the same question we're looking at before, and presumably we're going stronger properties since we don't have any labeled data.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to contrast us a little bit with some of the more standard approaches to clustering this kind of these two 2 standard approaches.",
                    "label": 0
                },
                {
                    "sent": "One is you view your similarity information as your ground truth in some sense, and you look at your ability of your algorithms to achieve different optimization criteria.",
                    "label": 1
                },
                {
                    "sent": "So you might look at how well can you approximate the K means objective K meeting objective case center objective.",
                    "label": 0
                },
                {
                    "sent": "OK, so we don't want to look at because we're thinking of their measure of similarity is just a heuristic.",
                    "label": 0
                },
                {
                    "sent": "Someone had this to us.",
                    "label": 0
                },
                {
                    "sent": "Our goal is not to get to K means optimal solution.",
                    "label": 0
                },
                {
                    "sent": "Our goal is to get the points right.",
                    "label": 1
                },
                {
                    "sent": "Another standard approach is you make a generative model, you say.",
                    "label": 0
                },
                {
                    "sent": "Well, I assume that my data comes from mixture of Gaussians again.",
                    "label": 0
                },
                {
                    "sent": "I don't want to assume our data is a mixture of Gaussians that the documents are picked in some random process.",
                    "label": 0
                },
                {
                    "sent": "The documents of the documents someone is handing us a measure of similarity and I really want to look at the question of what properties should that measure of similarity have.",
                    "label": 0
                },
                {
                    "sent": "So I don't want to make a generative assumption, just want to look at the question just like we do with kernels given some data, how powerful similarity function do we need to be able to cluster well?",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now just to get started, here's a condition that trivially works alright.",
                    "label": 1
                },
                {
                    "sent": "Suppose I give you a similarity function such that for any two objects of the same type.",
                    "label": 0
                },
                {
                    "sent": "Alright in the same cluster there are similarities bigger than zero, and Frank two objects in different clusters in the ground truth their similarities less than 0.",
                    "label": 0
                },
                {
                    "sent": "If you gave me that that be easy, I wouldn't need.",
                    "label": 0
                },
                {
                    "sent": "You know any sort of fancy algorithm get taken acts like everybody is more than zero.",
                    "label": 0
                },
                {
                    "sent": "Similar to it, that's its class.",
                    "label": 0
                },
                {
                    "sent": "St pulls away.",
                    "label": 0
                },
                {
                    "sent": "Take another point, everything more than zero somewhere, so it's another cluster for the way right?",
                    "label": 0
                },
                {
                    "sent": "Really easy.",
                    "label": 0
                },
                {
                    "sent": "Great 'cause this is an extremely strong property.",
                    "label": 0
                },
                {
                    "sent": "Fine.",
                    "label": 1
                },
                {
                    "sent": "Well, alright, So what happens if we just make this?",
                    "label": 1
                },
                {
                    "sent": "Let's make this a little weaker and then we're going to immediately see we get into some trouble, which is part of the.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reason why people tend not to analyze clustering this way.",
                    "label": 0
                },
                {
                    "sent": "OK, but we'll see a way around it.",
                    "label": 0
                },
                {
                    "sent": "OK, so suppose our measure of similarity is the following also really strong property?",
                    "label": 0
                },
                {
                    "sent": "Suppose I tell you that I promise you the following all access all documents are strictly more similar to all other documents of their own.",
                    "label": 1
                },
                {
                    "sent": "Type of their own cluster, then to any document of any other cluster.",
                    "label": 0
                },
                {
                    "sent": "OK, it's still pretty strong property but but but we immediately get into the following issue.",
                    "label": 1
                },
                {
                    "sent": "It's still a strong condition, but here's a problem.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Imagine that I've got, you know my documents look like this.",
                    "label": 0
                },
                {
                    "sent": "I've got documents about baseball, basketball, math and physics.",
                    "label": 1
                },
                {
                    "sent": "And imagine I have a measure of similarity that does the thing you'd expect.",
                    "label": 0
                },
                {
                    "sent": "The baseball documents are all really similar to each other.",
                    "label": 0
                },
                {
                    "sent": "The basketballs are really similar to each other.",
                    "label": 0
                },
                {
                    "sent": "The math to each other, the physics to each other.",
                    "label": 0
                },
                {
                    "sent": "Baseball and basketball are medium similarity.",
                    "label": 0
                },
                {
                    "sent": "Math and physics medium and really low across here.",
                    "label": 0
                },
                {
                    "sent": "Great.",
                    "label": 0
                },
                {
                    "sent": "So it could be this is 4 clusters, baseball, basketball, math and physics.",
                    "label": 0
                },
                {
                    "sent": "It could be this is 2 clusters, sports and science.",
                    "label": 1
                },
                {
                    "sent": "It could be if you ask me to cluster this.",
                    "label": 0
                },
                {
                    "sent": "This would look like 3 clusters.",
                    "label": 0
                },
                {
                    "sent": "To me this would look like sports math and physics and it's totally consistent with this property because every every sports document is more similar to every other sports document than to any non sports one.",
                    "label": 1
                },
                {
                    "sent": "The maths are more similar.",
                    "label": 0
                },
                {
                    "sent": "The maths and physics or the sports and the physics are more similar to the physics and the mass of the sports.",
                    "label": 0
                },
                {
                    "sent": "OK, you go on the street and ask.",
                    "label": 0
                },
                {
                    "sent": "Somebody is going to look like this.",
                    "label": 0
                },
                {
                    "sent": "Alright, you have baseball, basketball and then the stuff with equations in it?",
                    "label": 0
                },
                {
                    "sent": "And that's a totally consistent clustering with this property.",
                    "label": 0
                },
                {
                    "sent": "Alright, everything is more similar to everybody else in its own cluster than anybody in any other cluster.",
                    "label": 1
                },
                {
                    "sent": "So clearly this condition.",
                    "label": 0
                },
                {
                    "sent": "How's our poor algorithm supposed to know whether we were looking for this answer or that answer?",
                    "label": 0
                },
                {
                    "sent": "OK, even if I tell you I'm looking for three clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, so the condition is kind of not enough information.",
                    "label": 0
                },
                {
                    "sent": "So the way we're going to get around.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it is by relaxing our goals.",
                    "label": 1
                },
                {
                    "sent": "OK, we're going to say in a way that I think is natural, so we're going to say that it's OK for our algorithm to produce a hierarchical clustering.",
                    "label": 1
                },
                {
                    "sent": "So a tree on subsets that will call successful if the right answer.",
                    "label": 0
                },
                {
                    "sent": "The target clustering is approximately equal to some pruning of this tree.",
                    "label": 1
                },
                {
                    "sent": "OK, so in this case there's a really natural tree which is all document splits into sports and science sports, plus splits into baseball, basketball, science into math and physics.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "And then we our algorithm doesn't have to figure out.",
                    "label": 0
                },
                {
                    "sent": "Which pruning we were thinking we'll just call it outputs this and we'll call it successful.",
                    "label": 0
                },
                {
                    "sent": "If the thing we were looking for is approximately some printing of the tree so you can view this is saying mean one way to view this is just think about putting the top level.",
                    "label": 0
                },
                {
                    "sent": "You know sports and science, and then you're telling the user look if any of these is too broad.",
                    "label": 1
                },
                {
                    "sent": "You know I didn't really know how specific you wanted to be, if it's.",
                    "label": 0
                },
                {
                    "sent": "Not specific enough, just click on and I'll just put it for you and this is telling you how to split it.",
                    "label": 0
                },
                {
                    "sent": "OK, so where we're offloading to the user a little bit, which is well, how specific did you want to be in different parts of the space?",
                    "label": 0
                },
                {
                    "sent": "But basically what's happening is where our loss function is a little bit relaxed.",
                    "label": 0
                },
                {
                    "sent": "Rather than forcing the algorithm for the partition, the algorithm is allowed to have the hierarchy and the score is so our notion of being a successful hierarchy is if the right answer, the target clustering is approximately some pruning of this tree, then we'll call it successful.",
                    "label": 1
                },
                {
                    "sent": "OK, so now another another another thing that we also look up.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about here is it's a rather than outputting a single clustering.",
                    "label": 0
                },
                {
                    "sent": "You're out allowed out with a small list of clustering is kind of like what you doin list decoding, but I'm not going to talk about this one.",
                    "label": 0
                },
                {
                    "sent": "I think in Nina's talk she talked about about this in the context of some approximation assumptions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Given this is going to focus on the tree one, so now we can start getting somewhere.",
                    "label": 0
                },
                {
                    "sent": "So for example, this strong condition that felt like you know that's a pretty strong condition, although be good, the cluster in fact is sufficient to get a hierarchal clustering.",
                    "label": 1
                },
                {
                    "sent": "With the target being appearing in the tree, in fact by very simple algorithm, so Kruskal's algorithm are single linkage, so single linkage algorithm you take all the points like Sandra talked about, take all the points, just merge the two that are most similar.",
                    "label": 0
                },
                {
                    "sent": "Most similar, most similar, and you get this tree OK.",
                    "label": 0
                },
                {
                    "sent": "It's not hard to show that if your similarity function has this condition, then.",
                    "label": 0
                },
                {
                    "sent": "The tree you output will have the correct answer is some pruning of the tree.",
                    "label": 1
                },
                {
                    "sent": "OK, I'm not going to prove that 'cause I want to show you something a little more interest.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a weaker condition.",
                    "label": 1
                },
                {
                    "sent": "The weaker condition is maybe don't satisfy that property, but so here's a weaker property, and in fact people have looked at that property is very much like this for problems in mathematical biology and the properties that you want.",
                    "label": 0
                },
                {
                    "sent": "Your clusters have the following condition.",
                    "label": 0
                },
                {
                    "sent": "So for all clusters, CNC prime for all subsets A of C and a primacy prime, you want it to be that it's not the case that the egg and a primer both more similar to each other on average.",
                    "label": 1
                },
                {
                    "sent": "Then each is to the rest of its own cluster.",
                    "label": 0
                },
                {
                    "sent": "So if you think about like the stable marriage property, you know that you don't have like a you know husband and wife.",
                    "label": 0
                },
                {
                    "sent": "They want to run off together.",
                    "label": 0
                },
                {
                    "sent": "This is the clustering analogue of that.",
                    "label": 0
                },
                {
                    "sent": "OK, so you don't have a so you think of the similarity is how attractive they are.",
                    "label": 0
                },
                {
                    "sent": "You don't have a subset of 1 cluster in the subset of the other that are on average more attracted to each other than the rest of their own clusters may go run off together.",
                    "label": 0
                },
                {
                    "sent": "So suppose the ground truth has the stability condition.",
                    "label": 0
                },
                {
                    "sent": "Then you can show that this is also sufficient to get a good tree not using single linkage but using average single linkage.",
                    "label": 1
                },
                {
                    "sent": "In fact, as we talked about 3 versions of single linkage, it was the middle, the middle one you need, the others don't work for this, but the middle one does OK and so.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in particular.",
                    "label": 0
                },
                {
                    "sent": "Yeah, let me just go through how you can show this.",
                    "label": 0
                },
                {
                    "sent": "I'm going to look at a simpler version.",
                    "label": 1
                },
                {
                    "sent": "So remember stable marriage.",
                    "label": 0
                },
                {
                    "sent": "You said it's stable.",
                    "label": 0
                },
                {
                    "sent": "If for any piece of 1 cluster piece of the other they don't.",
                    "label": 0
                },
                {
                    "sent": "It's OK if one of them likes the other one better, as long as the other one doesn't like reciprocate.",
                    "label": 0
                },
                {
                    "sent": "OK, but let's look at the stronger version that's called strongly stable.",
                    "label": 0
                },
                {
                    "sent": "If neither one wants to jump ship.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So let's assume that for all clusters, CNC prime for all ANC a prime in C prime A is more similar to the rest of its own cluster that a prime and then need to switching in a prime the other way too.",
                    "label": 1
                },
                {
                    "sent": "Let's also assume our similarity function is symmetric.",
                    "label": 0
                },
                {
                    "sent": "Everything we've been talking about actually works for asymmetric functions, but let's assume symmetric, so then.",
                    "label": 0
                },
                {
                    "sent": "The claim is the average single linkage will succeed.",
                    "label": 0
                },
                {
                    "sent": "What is average single linkage like Kruskal's algorithm?",
                    "label": 0
                },
                {
                    "sent": "Every step you're merging the pair of clusters whose average similarity average pairwise similarity is highest OK. And how can we analyze this is a nice way to analyze this, which is if you think about it for a bit.",
                    "label": 1
                },
                {
                    "sent": "You realize that what you want is, as your algorithm is glomming things together, you want the all the clusters you've produced are Lamon are with respect to the target.",
                    "label": 0
                },
                {
                    "sent": "By that I mean that every cluster you've created is equal a subset of a target cluster equal to a target cluster or a union of target clusters.",
                    "label": 0
                },
                {
                    "sent": "If you can maintain tap through the whole process, that'll guarantee you that somewhere along the line you've produced, the target is some printing of your tree.",
                    "label": 0
                },
                {
                    "sent": "You guys think about that a little bit, but but it turns out to be true, so all we have to do is just show by induction that if the current state of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "Is has this property, so let's say the current state.",
                    "label": 0
                },
                {
                    "sent": "Let's say the right answer is those two big big ovals there and the current state of our algorithm is.",
                    "label": 0
                },
                {
                    "sent": "Is these other little pieces that after each step so each piece that we have is either a piece of attire cluster equal to entire cluster or union entire clusters.",
                    "label": 0
                },
                {
                    "sent": "But that's also true after one step too OK.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's actually not so hard to show if you look at you realize the only way that you can fail is if you if your algorithm ever takes a subset, say C1 of some cluster C and and some other things C2 which is disjoint from C and merges them together.",
                    "label": 0
                },
                {
                    "sent": "But if you think about that, Member Algorithm merges the pair of highest average similarity, and I promised you that C one is more similar on average, the rest of its cluster than to any other piece of another cluster.",
                    "label": 0
                },
                {
                    "sent": "So C2 is taking the role of this a prime over here.",
                    "label": 0
                },
                {
                    "sent": "So we know that C one is more similar to the rest of its own cluster, and so this guy.",
                    "label": 0
                },
                {
                    "sent": "That's one of our clusters that we have produced so far.",
                    "label": 1
                },
                {
                    "sent": "That's that's at least as good as the average.",
                    "label": 1
                },
                {
                    "sent": "So it's going to have to be some C3 there so that the similar you can C3 and C1C3 is at least as good as the average similarity of C1 to the rest of big cluster, and so that will be bigger.",
                    "label": 0
                },
                {
                    "sent": "Better than the similarity of C1C2.",
                    "label": 0
                },
                {
                    "sent": "So our algorithm wouldn't merge C1C2.",
                    "label": 0
                },
                {
                    "sent": "It'll instead merge C1C3.",
                    "label": 0
                },
                {
                    "sent": "You can show this by induction.",
                    "label": 0
                },
                {
                    "sent": "The algorithm will do the right OK, just examples of.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, OK so.",
                    "label": 0
                },
                {
                    "sent": "Perfect, you can.",
                    "label": 0
                },
                {
                    "sent": "Also another thing you can do is take some of these conditions, like say the strong one.",
                    "label": 0
                },
                {
                    "sent": "Everybody is more similar to everybody in its own cluster than anybody in the other cluster, and you can say, well, what if that's not true for all the data.",
                    "label": 1
                },
                {
                    "sent": "Whether it's true for most of the data, but you added in some extra data points that are like acting in a crazy way now that can totally kill a bottom up algorithm.",
                    "label": 0
                },
                {
                    "sent": "I mean, just take one add-in.",
                    "label": 0
                },
                {
                    "sent": "One point that thinks it's like 100% similar to everybody and all of a sudden you're single link is going to connect it to everybody.",
                    "label": 0
                },
                {
                    "sent": "Like that, OK, so you can.",
                    "label": 0
                },
                {
                    "sent": "You can really mess up bottom up algorithms by adding in extra noisy so maliciously noisy data or some crazy data.",
                    "label": 1
                },
                {
                    "sent": "OK, but you can show a different kind of algorithm works where you start by creating a collection of plausable clusters.",
                    "label": 0
                },
                {
                    "sent": "In this case what you would do is for every data point in every radius just look at the cluster of everybody is at least that similar to that point and under this condition that will include all the correct answers, all the correct clusters plus a bunch of other stuff too.",
                    "label": 1
                },
                {
                    "sent": "And then you gotta figure out which ones to throw away.",
                    "label": 0
                },
                {
                    "sent": "And so you can use a series of pairwise tests.",
                    "label": 0
                },
                {
                    "sent": "Look at, let's say, well look, you know these guys are OK, but these two there's no way that both this cluster here in this cluster.",
                    "label": 0
                },
                {
                    "sent": "They're not both allowed to exist.",
                    "label": 0
                },
                {
                    "sent": "'cause one is not a subset of the other and are not disjoint.",
                    "label": 0
                },
                {
                    "sent": "So you can do a pairwise test that allows you to throw out one of them, and so you can prove that a certain pairwise test to allow you to throw out everything until what's left is consistent with the tree.",
                    "label": 0
                },
                {
                    "sent": "So you can look.",
                    "label": 0
                },
                {
                    "sent": "So these are the kind of natural algorithms that you get under.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Missions like this.",
                    "label": 0
                },
                {
                    "sent": "You can also connect this to some implicit assumptions made by optimization approach to clustering, so this is with Nina had talked about on Saturday, which is if you're using an approximation algorithm for something like, say, the K median problem or K means.",
                    "label": 1
                },
                {
                    "sent": "You're somehow if there's some right answer there, you're hoping anyway that the optimal solution to this objective is actually close to the right answer, not only optimal solution, since you only have an approximation that any approximately optimal solution is also close to the right answer, and.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you can just say, well, similarity function is good, if any, approximately optimal solution is close.",
                    "label": 0
                },
                {
                    "sent": "The right answer and you can use that property to cluster well even if your notion of approximately optimal is better than what you actually know.",
                    "label": 0
                },
                {
                    "sent": "How to get algorithmically, even if it's maybe even NP hard to get that that close to optimal for at least certain objectives like a median, K means min sum is a few that we know how you can use that that property to get close to the right answer.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, then one can also analyze an inductive setting where everything I'm talking about is been transductive.",
                    "label": 1
                },
                {
                    "sent": "You just have your data.",
                    "label": 0
                },
                {
                    "sent": "You can imagine a property defined over distribution and then you draw data from the distribution and then you want to argue if this property is true over the whole distribution.",
                    "label": 0
                },
                {
                    "sent": "Well, if I take my sample that it's true over my sample, the usual sort of thing you'd want to do so for here you need kind of different sort of.",
                    "label": 0
                },
                {
                    "sent": "The sample complexity results seem to rely on some other other kinds of tools.",
                    "label": 0
                },
                {
                    "sent": "Is regularity type results that in order to argue with high probability, if some condition you had was true over the whole distribution, high probability or reasonable size sample will mimic that and have the property be true over the sample.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me just end by saying that in a way.",
                    "label": 0
                },
                {
                    "sent": "So I said this is a little bit like a pack model for clustering, and what I mean by that is that what we're talking about properties?",
                    "label": 0
                },
                {
                    "sent": "What's a property, the relation between the target clustering and similarity information, and so this is this relation.",
                    "label": 1
                },
                {
                    "sent": "It's like a data dependent concept class.",
                    "label": 1
                },
                {
                    "sent": "OK, in particular, given data in a similarity function of property induces the concept class, the set of all clusterings consistent with the property and what we're asking in this tree model is we want to produce a tree such that the set of prunings of this tree form an epsilon cover.",
                    "label": 1
                },
                {
                    "sent": "Of these things, so whatever the target is, we've got some close clustering to it as a printing of our tree.",
                    "label": 0
                },
                {
                    "sent": "That's one way to view the view.",
                    "label": 0
                },
                {
                    "sent": "What we're asking for in this model.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're looking at the what natural properties allow a similarity measure to be useful for clustering in order to get a good theory this, we had to relax.",
                    "label": 1
                },
                {
                    "sent": "We mean by useful for clustering and then given this week and we were then able to analyze a number of properties and show guarantees.",
                    "label": 0
                },
                {
                    "sent": "An algorithm is able to.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "System.",
                    "label": 0
                },
                {
                    "sent": "Let me just.",
                    "label": 0
                },
                {
                    "sent": "I want to get to it.",
                    "label": 0
                },
                {
                    "sent": "Open problem.",
                    "label": 0
                },
                {
                    "sent": "Good so.",
                    "label": 0
                },
                {
                    "sent": "OK, so so our goal here is to be able to say alright, what if you're coming with a measure of similarity?",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "What kind of properties might you want to shoot for?",
                    "label": 1
                },
                {
                    "sent": "And given the property believer similarity measure has what's a good algorithm to use for that?",
                    "label": 0
                },
                {
                    "sent": "I guess have a bunch of problems maybe, maybe just two?",
                    "label": 0
                },
                {
                    "sent": "Mention one the property that any approximately optimal K median solution is close to the target that we know how to deal with the property that any approximately optimal graph separator according to the balance separator notion is closely target that we don't know how to deal with, and that would be really nice to be able to say I give you data, I promise you any good approximations.",
                    "label": 0
                },
                {
                    "sent": "The balance separator problem is close to target, great under that condition.",
                    "label": 0
                },
                {
                    "sent": "Can you find some close to the target?",
                    "label": 0
                },
                {
                    "sent": "We don't have to deal with that.",
                    "label": 0
                },
                {
                    "sent": "Island.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "OK, well then I will be here during the breakfast with you.",
                    "label": 0
                },
                {
                    "sent": "Wanna look into dating sites and will take a break for half an hour.",
                    "label": 0
                }
            ]
        }
    }
}