{
    "id": "l363xro3dgcwqvvjq7ylpwgjmd72o3hl",
    "title": "Preference elicitation and inverse reinforcement learning",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Constantin A. Rothkopf, Frankfurt Institute for Advanced Studies"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_rothkopf_elicitation/",
    "segmentation": [
        [
            "You are in the lobby and you think, well, where am I going to go next right?",
            "Maybe at the program you have a sequence of talks there and you may say, well, maybe I should go into this room here, but you may also decide to go down the steps over here in here I have to go downstairs and you have to pass the food.",
            "Maybe that's tempting to you and then you have to enter the room and then you stay at the food on all you have.",
            "Also the possibility of going to these other talks or you decide I'll just go to the to the swimming pool.",
            "OK, that seems to be a reasonable problem too.",
            "Book."
        ],
        [
            "So.",
            "Preference elicitation is a problem that has been.",
            "Formulated and worked with for quite some time.",
            "So here is 1 reference I think.",
            "As an important one, and the idea is OK, you just have a decision maker.",
            "You observe that decision maker and then I certain events that he prefers.",
            "OK, I only want to be somehow quantities of that.",
            "So what do you do while you use the two assumptions?",
            "First one would be there's some partial ordering of these events in terms of the preferences, and then you want to get to something numeric you want to attach an American utility to the partial ordering, and so the problem here is.",
            "To find the course that you determine the Merkle utilities for the decision maker based on these preferences, OK?"
        ],
        [
            "And so the problem of inverse reinforcement lying.",
            "Is you have a specific agent you have a reinforcement learning agent.",
            "And he's carrying out some task in a particular environment, and this is attributed to an unknown Russell who formulated this as the industry enforcement learning problem.",
            "And what are the assumptions here?",
            "Well, the first assumption is that the transition function is known.",
            "You know the dynamics of the environment.",
            "And the agent is following a policy which maximizes the cumulative total.",
            "This kind of reward.",
            "And so the corresponding problem here, which is that, well, you want to infer the reward functions that the reinforcement learning agent is implicitly maximizing.",
            "Should be obvious that you can connect these two.",
            "From this by comparing."
        ],
        [
            "These two.",
            "So here is the formalization of this.",
            "So we consider consider controlled Markov process.",
            "States actions transitions.",
            "We assume that the transition function is known to us.",
            "And then we have some demonstrations from the agent.",
            "So these are the demonstrations consistent of the state sequence and the action sequence.",
            "OK, now.",
            "Because we said something about utilities and not necessarily started from the point that this must be a reinforcement learning agent.",
            "The question is what should the utility's be?",
            "And to re connect this to the industry involvement approach are the obvious choices to say.",
            "Well, it's the return in this particular case.",
            "Note that here we use stochastic rewards so they come from some distribution that we have to choose, and we also have a discount factor here.",
            "Um?",
            "The main point though, is that we could have chosen something else for the utility.",
            "It doesn't have to be this particular function form you could pick, say, the log of the number of gold coins collected in amaze.",
            "You could use different types of discounting for animals and humans.",
            "You can show that have abolished discounting fits for certain tasks better, so this is a possibility."
        ],
        [
            "Alright.",
            "So then we can look at the statistical model.",
            "As a generative model of the state action is.",
            "So you have some prior over the reward functions you have some prior over policies condition on the reward functions and then you obtain these sequences of state action pairs.",
            "So let's be informed about this.",
            "So we have the space of what functions, basic policies, and now we define these reward functions.",
            "Given what we know about the NDP and condition on the reward function, we can get a policy.",
            "And we can find an expression for the joint distribution over reward functions and policies.",
            "OK, so let's also get the point where we can choose what?",
            "What are policies should be and most of previous reinforcement learning work assumes that the agent actually maximizes the utility's, but is obviously more interesting to look at cases where we don't know whether he does this, so he could be sub optimal.",
            "And natural choice also again looking back at the reinforcement learning literature is to use stationary softmax policies.",
            "Um?",
            "Note again though that we don't have to pick that.",
            "You could pick something else.",
            "It will affect what you want.",
            "Your calculations will be concretely afterwards, but you could in principle with something else."
        ],
        [
            "So how do we do inference?",
            "We can simply apply Bayes theorem.",
            "Then we can find the expression for the model and likelihood and we can then build the recursion on the state transitions.",
            "Details are in the paper and you can find them.",
            "Expression here or computing posterior over your reward functions OK. And maybe I don't know, maybe this surprising, so an expression for the state transitions actually is not in this expression anymore.",
            "Alright."
        ],
        [
            "So how do we go about computing this?",
            "One way is to use.",
            "The column.",
            "Think of just using straightforward Metropolis Hastings where you want samples from a particular distribution, and you have a proposal distribution, and then you have.",
            "Mixing up the.",
            "You can have the decision whether you want to keep it.",
            "The next sample so we can identify now the quantities in our problem here where we have this drug distribution of a reward, functions and policies, then we can condition on the state and action sequences and what we know about the MDP.",
            "Again, the transition function is given to us.",
            "And then we can use independent proposals here and then we have an expression for our sampling procedure."
        ],
        [
            "So now the question is.",
            "How to apply this to a?",
            "Concrete example, right?",
            "So you have to come up with your description of the problem, so let's assume.",
            "The agent visit these States and their rewards there are there normally so we can use product of beta prior on the reward function and we set those parameters say from a gamma distribution.",
            "So what do we do?",
            "We sample parameters for what function we sample, in this case a temperature for the softmax.",
            "And then we evaluate the likelihood of.",
            "Of the samples keeper rejected and do this.",
            "And hope that everything goes well."
        ],
        [
            "Um, so we also came up with an alternative way with a second way, which is Gibbs sampler, so it might be interesting to actually also get a sample of the actual reward sequence that the agent might have seen.",
            "That allows you to actually condition on the reward sequence so you can get a sample of the reward function, and then you can then update your parameters of the reward function later.",
            "OK."
        ],
        [
            "OK, so let's look at the empirical evaluation.",
            "So far here at two different problems, one is to consider just random DPS.",
            "And domain is to look at random based tasks.",
            "And how do we evaluate it?",
            "The fairest way of actually evaluating these is to look at the L1 loss, so we have an agent.",
            "It might be acting sub optimally, but we know what the true reward function is.",
            "Transition function is.",
            "We find the optimal policy and we can evaluate the optimal values and then we can compare this to the values based on the reward function that the agent is estimating.",
            "I shouldn't say the agents that allow algorithms are estimating.",
            "And also compare this to what the agent is actually doing.",
            "So if the agent was suboptimal, of course.",
            "It also encourage some loss if we can quantify this this way.",
            "OK, we use mass comparisons.",
            "The Onion Russell linear program.",
            "We use the promotion and EMEA version of Vision in this reinforcement learning.",
            "And we also look and well, algorithm by side control here."
        ],
        [
            "See.",
            "So here first we look at again says the total loss as a function of the temperature of the softmax.",
            "So the agent becomes less stochastic in this direction of the axis, and this is for the random MVP's and this is for the random maze tasks.",
            "The black line shows you the loss encouraged by the agent.",
            "By not being not following the optimum policy.",
            "Similar here.",
            "And then you can see here.",
            "The losses incurred by the linear program based algorithm and the.",
            "Invasion.",
            "It is difficult to see, but here you have the enuol.",
            "Phones and you can see that.",
            "The proposed algorithm outperforms these quite well.",
            "Similarly, here what you can see is that.",
            "The linear programs based I wasn't done.",
            "Quite a bit better, so maybe the intuition that is that because you have this linear program that just tries to.",
            "Maximize this distance to the next best action from a particular state.",
            "You do much better in tasks where from one computer stage you don't have the possibility of going to several other states where their rewards are pretty close.",
            "That is related, so that's mainly the difference here."
        ],
        [
            "K and he another evaluation.",
            "We looked at random NPS.",
            "And here we just look at the number of demonstrations that you have.",
            "So the length of the sequence.",
            "Um?",
            "And as you can see here.",
            "Of really does quite well with just few data.",
            "Here we looked again at these random NDPS such changed the size of the same space.",
            "And again we outperformed.",
            "Previous algorithms.",
            "Alright."
        ],
        [
            "So in conclusion, represented in unified framework or preference elicitation, animal enforcement learning.",
            "Presented to statistical inference models and sampling procedures.",
            "The formalisation population is quite general.",
            "The nice thing about that is you can really use alternative priors on every word functions on your policy's adults on the preferences, so the preferences could be.",
            "Actually have they could have different functional form.",
            "And in the experiments we showed that.",
            "This is.",
            "Can do quite well, especially when the agent obviously is not performing."
        ],
        [
            "Thanks and please also if you double URL check out this other contribution we have there.",
            "So you have to leave right after the talk.",
            "But I think we have time for questions, right?",
            "So.",
            "Asian.",
            "Yeah, so we don't assume we don't assume necessarily that the agent has to be performing optimally with, respectively, so you might be saying exploring, or you might be choosing some actions of the degree of some optionality of demonstrating agent affect your algorithm.",
            "Well, if we have the right if we have the correct functional performance of what he's doing, we can get posterior over the parameters going that so.",
            "I would say no.",
            "Obviously, if you don't know exactly the functional form, you might have, you might have to compare different functional forms and you have to get posterior windows, but principles the answer.",
            "Similar to the Clean up effect.",
            "What?",
            "In behavioral cloning, you can help inform your teacher if he sometimes makes mistakes, but they disappear as noise when we try to predict this next move.",
            "It's called the cleanup effect.",
            "If you make some mistakes, but there are random enough so that it doesn't make the same mistake over and over, But then we will just do the tape that mistake and that seems similar.",
            "Yeah, so I told you that I'm not an expert in this.",
            "But it's all the way you formulated.",
            "It sounded like it averages out.",
            "This is a little bit.",
            "Different from that in that we have a specific statistical model.",
            "So even if we have a good model about about how he does mistakes, then we can do most probably even better, because we not we do not average out, but we get a posterior again over the parameters graveling is being off.",
            "And is there a reasonable assumption that you know how he makes mistakes?",
            "Well, I think it's a reasonable assumption to try to figure that out, and testing things and getting posterior.",
            "Someone else.",
            "Yeah, I think that's quite a reasonable thing to do.",
            "Obviously if you know it's something more about the agent that you're looking at, say he's using a soft flex exploration with particular schedule, how we adjust its temperature.",
            "You'll do extremely well.",
            "OK, more questions.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You are in the lobby and you think, well, where am I going to go next right?",
                    "label": 0
                },
                {
                    "sent": "Maybe at the program you have a sequence of talks there and you may say, well, maybe I should go into this room here, but you may also decide to go down the steps over here in here I have to go downstairs and you have to pass the food.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's tempting to you and then you have to enter the room and then you stay at the food on all you have.",
                    "label": 0
                },
                {
                    "sent": "Also the possibility of going to these other talks or you decide I'll just go to the to the swimming pool.",
                    "label": 0
                },
                {
                    "sent": "OK, that seems to be a reasonable problem too.",
                    "label": 0
                },
                {
                    "sent": "Book.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Preference elicitation is a problem that has been.",
                    "label": 1
                },
                {
                    "sent": "Formulated and worked with for quite some time.",
                    "label": 0
                },
                {
                    "sent": "So here is 1 reference I think.",
                    "label": 0
                },
                {
                    "sent": "As an important one, and the idea is OK, you just have a decision maker.",
                    "label": 0
                },
                {
                    "sent": "You observe that decision maker and then I certain events that he prefers.",
                    "label": 0
                },
                {
                    "sent": "OK, I only want to be somehow quantities of that.",
                    "label": 0
                },
                {
                    "sent": "So what do you do while you use the two assumptions?",
                    "label": 0
                },
                {
                    "sent": "First one would be there's some partial ordering of these events in terms of the preferences, and then you want to get to something numeric you want to attach an American utility to the partial ordering, and so the problem here is.",
                    "label": 0
                },
                {
                    "sent": "To find the course that you determine the Merkle utilities for the decision maker based on these preferences, OK?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so the problem of inverse reinforcement lying.",
                    "label": 1
                },
                {
                    "sent": "Is you have a specific agent you have a reinforcement learning agent.",
                    "label": 0
                },
                {
                    "sent": "And he's carrying out some task in a particular environment, and this is attributed to an unknown Russell who formulated this as the industry enforcement learning problem.",
                    "label": 0
                },
                {
                    "sent": "And what are the assumptions here?",
                    "label": 0
                },
                {
                    "sent": "Well, the first assumption is that the transition function is known.",
                    "label": 0
                },
                {
                    "sent": "You know the dynamics of the environment.",
                    "label": 0
                },
                {
                    "sent": "And the agent is following a policy which maximizes the cumulative total.",
                    "label": 0
                },
                {
                    "sent": "This kind of reward.",
                    "label": 0
                },
                {
                    "sent": "And so the corresponding problem here, which is that, well, you want to infer the reward functions that the reinforcement learning agent is implicitly maximizing.",
                    "label": 1
                },
                {
                    "sent": "Should be obvious that you can connect these two.",
                    "label": 0
                },
                {
                    "sent": "From this by comparing.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These two.",
                    "label": 0
                },
                {
                    "sent": "So here is the formalization of this.",
                    "label": 0
                },
                {
                    "sent": "So we consider consider controlled Markov process.",
                    "label": 0
                },
                {
                    "sent": "States actions transitions.",
                    "label": 0
                },
                {
                    "sent": "We assume that the transition function is known to us.",
                    "label": 1
                },
                {
                    "sent": "And then we have some demonstrations from the agent.",
                    "label": 0
                },
                {
                    "sent": "So these are the demonstrations consistent of the state sequence and the action sequence.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                },
                {
                    "sent": "Because we said something about utilities and not necessarily started from the point that this must be a reinforcement learning agent.",
                    "label": 1
                },
                {
                    "sent": "The question is what should the utility's be?",
                    "label": 0
                },
                {
                    "sent": "And to re connect this to the industry involvement approach are the obvious choices to say.",
                    "label": 0
                },
                {
                    "sent": "Well, it's the return in this particular case.",
                    "label": 0
                },
                {
                    "sent": "Note that here we use stochastic rewards so they come from some distribution that we have to choose, and we also have a discount factor here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The main point though, is that we could have chosen something else for the utility.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have to be this particular function form you could pick, say, the log of the number of gold coins collected in amaze.",
                    "label": 0
                },
                {
                    "sent": "You could use different types of discounting for animals and humans.",
                    "label": 0
                },
                {
                    "sent": "You can show that have abolished discounting fits for certain tasks better, so this is a possibility.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So then we can look at the statistical model.",
                    "label": 0
                },
                {
                    "sent": "As a generative model of the state action is.",
                    "label": 1
                },
                {
                    "sent": "So you have some prior over the reward functions you have some prior over policies condition on the reward functions and then you obtain these sequences of state action pairs.",
                    "label": 0
                },
                {
                    "sent": "So let's be informed about this.",
                    "label": 0
                },
                {
                    "sent": "So we have the space of what functions, basic policies, and now we define these reward functions.",
                    "label": 0
                },
                {
                    "sent": "Given what we know about the NDP and condition on the reward function, we can get a policy.",
                    "label": 1
                },
                {
                    "sent": "And we can find an expression for the joint distribution over reward functions and policies.",
                    "label": 1
                },
                {
                    "sent": "OK, so let's also get the point where we can choose what?",
                    "label": 0
                },
                {
                    "sent": "What are policies should be and most of previous reinforcement learning work assumes that the agent actually maximizes the utility's, but is obviously more interesting to look at cases where we don't know whether he does this, so he could be sub optimal.",
                    "label": 1
                },
                {
                    "sent": "And natural choice also again looking back at the reinforcement learning literature is to use stationary softmax policies.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Note again though that we don't have to pick that.",
                    "label": 0
                },
                {
                    "sent": "You could pick something else.",
                    "label": 0
                },
                {
                    "sent": "It will affect what you want.",
                    "label": 0
                },
                {
                    "sent": "Your calculations will be concretely afterwards, but you could in principle with something else.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we do inference?",
                    "label": 0
                },
                {
                    "sent": "We can simply apply Bayes theorem.",
                    "label": 0
                },
                {
                    "sent": "Then we can find the expression for the model and likelihood and we can then build the recursion on the state transitions.",
                    "label": 0
                },
                {
                    "sent": "Details are in the paper and you can find them.",
                    "label": 0
                },
                {
                    "sent": "Expression here or computing posterior over your reward functions OK. And maybe I don't know, maybe this surprising, so an expression for the state transitions actually is not in this expression anymore.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we go about computing this?",
                    "label": 0
                },
                {
                    "sent": "One way is to use.",
                    "label": 0
                },
                {
                    "sent": "The column.",
                    "label": 0
                },
                {
                    "sent": "Think of just using straightforward Metropolis Hastings where you want samples from a particular distribution, and you have a proposal distribution, and then you have.",
                    "label": 0
                },
                {
                    "sent": "Mixing up the.",
                    "label": 0
                },
                {
                    "sent": "You can have the decision whether you want to keep it.",
                    "label": 0
                },
                {
                    "sent": "The next sample so we can identify now the quantities in our problem here where we have this drug distribution of a reward, functions and policies, then we can condition on the state and action sequences and what we know about the MDP.",
                    "label": 1
                },
                {
                    "sent": "Again, the transition function is given to us.",
                    "label": 0
                },
                {
                    "sent": "And then we can use independent proposals here and then we have an expression for our sampling procedure.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now the question is.",
                    "label": 0
                },
                {
                    "sent": "How to apply this to a?",
                    "label": 0
                },
                {
                    "sent": "Concrete example, right?",
                    "label": 0
                },
                {
                    "sent": "So you have to come up with your description of the problem, so let's assume.",
                    "label": 1
                },
                {
                    "sent": "The agent visit these States and their rewards there are there normally so we can use product of beta prior on the reward function and we set those parameters say from a gamma distribution.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 1
                },
                {
                    "sent": "We sample parameters for what function we sample, in this case a temperature for the softmax.",
                    "label": 0
                },
                {
                    "sent": "And then we evaluate the likelihood of.",
                    "label": 0
                },
                {
                    "sent": "Of the samples keeper rejected and do this.",
                    "label": 0
                },
                {
                    "sent": "And hope that everything goes well.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, so we also came up with an alternative way with a second way, which is Gibbs sampler, so it might be interesting to actually also get a sample of the actual reward sequence that the agent might have seen.",
                    "label": 0
                },
                {
                    "sent": "That allows you to actually condition on the reward sequence so you can get a sample of the reward function, and then you can then update your parameters of the reward function later.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's look at the empirical evaluation.",
                    "label": 0
                },
                {
                    "sent": "So far here at two different problems, one is to consider just random DPS.",
                    "label": 0
                },
                {
                    "sent": "And domain is to look at random based tasks.",
                    "label": 0
                },
                {
                    "sent": "And how do we evaluate it?",
                    "label": 0
                },
                {
                    "sent": "The fairest way of actually evaluating these is to look at the L1 loss, so we have an agent.",
                    "label": 0
                },
                {
                    "sent": "It might be acting sub optimally, but we know what the true reward function is.",
                    "label": 0
                },
                {
                    "sent": "Transition function is.",
                    "label": 0
                },
                {
                    "sent": "We find the optimal policy and we can evaluate the optimal values and then we can compare this to the values based on the reward function that the agent is estimating.",
                    "label": 0
                },
                {
                    "sent": "I shouldn't say the agents that allow algorithms are estimating.",
                    "label": 0
                },
                {
                    "sent": "And also compare this to what the agent is actually doing.",
                    "label": 0
                },
                {
                    "sent": "So if the agent was suboptimal, of course.",
                    "label": 0
                },
                {
                    "sent": "It also encourage some loss if we can quantify this this way.",
                    "label": 0
                },
                {
                    "sent": "OK, we use mass comparisons.",
                    "label": 0
                },
                {
                    "sent": "The Onion Russell linear program.",
                    "label": 0
                },
                {
                    "sent": "We use the promotion and EMEA version of Vision in this reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "And we also look and well, algorithm by side control here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "So here first we look at again says the total loss as a function of the temperature of the softmax.",
                    "label": 0
                },
                {
                    "sent": "So the agent becomes less stochastic in this direction of the axis, and this is for the random MVP's and this is for the random maze tasks.",
                    "label": 1
                },
                {
                    "sent": "The black line shows you the loss encouraged by the agent.",
                    "label": 0
                },
                {
                    "sent": "By not being not following the optimum policy.",
                    "label": 0
                },
                {
                    "sent": "Similar here.",
                    "label": 0
                },
                {
                    "sent": "And then you can see here.",
                    "label": 1
                },
                {
                    "sent": "The losses incurred by the linear program based algorithm and the.",
                    "label": 0
                },
                {
                    "sent": "Invasion.",
                    "label": 0
                },
                {
                    "sent": "It is difficult to see, but here you have the enuol.",
                    "label": 0
                },
                {
                    "sent": "Phones and you can see that.",
                    "label": 0
                },
                {
                    "sent": "The proposed algorithm outperforms these quite well.",
                    "label": 0
                },
                {
                    "sent": "Similarly, here what you can see is that.",
                    "label": 0
                },
                {
                    "sent": "The linear programs based I wasn't done.",
                    "label": 0
                },
                {
                    "sent": "Quite a bit better, so maybe the intuition that is that because you have this linear program that just tries to.",
                    "label": 0
                },
                {
                    "sent": "Maximize this distance to the next best action from a particular state.",
                    "label": 0
                },
                {
                    "sent": "You do much better in tasks where from one computer stage you don't have the possibility of going to several other states where their rewards are pretty close.",
                    "label": 0
                },
                {
                    "sent": "That is related, so that's mainly the difference here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "K and he another evaluation.",
                    "label": 0
                },
                {
                    "sent": "We looked at random NPS.",
                    "label": 0
                },
                {
                    "sent": "And here we just look at the number of demonstrations that you have.",
                    "label": 0
                },
                {
                    "sent": "So the length of the sequence.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And as you can see here.",
                    "label": 0
                },
                {
                    "sent": "Of really does quite well with just few data.",
                    "label": 0
                },
                {
                    "sent": "Here we looked again at these random NDPS such changed the size of the same space.",
                    "label": 0
                },
                {
                    "sent": "And again we outperformed.",
                    "label": 0
                },
                {
                    "sent": "Previous algorithms.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in conclusion, represented in unified framework or preference elicitation, animal enforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Presented to statistical inference models and sampling procedures.",
                    "label": 0
                },
                {
                    "sent": "The formalisation population is quite general.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about that is you can really use alternative priors on every word functions on your policy's adults on the preferences, so the preferences could be.",
                    "label": 0
                },
                {
                    "sent": "Actually have they could have different functional form.",
                    "label": 0
                },
                {
                    "sent": "And in the experiments we showed that.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Can do quite well, especially when the agent obviously is not performing.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks and please also if you double URL check out this other contribution we have there.",
                    "label": 0
                },
                {
                    "sent": "So you have to leave right after the talk.",
                    "label": 0
                },
                {
                    "sent": "But I think we have time for questions, right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Asian.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we don't assume we don't assume necessarily that the agent has to be performing optimally with, respectively, so you might be saying exploring, or you might be choosing some actions of the degree of some optionality of demonstrating agent affect your algorithm.",
                    "label": 0
                },
                {
                    "sent": "Well, if we have the right if we have the correct functional performance of what he's doing, we can get posterior over the parameters going that so.",
                    "label": 0
                },
                {
                    "sent": "I would say no.",
                    "label": 0
                },
                {
                    "sent": "Obviously, if you don't know exactly the functional form, you might have, you might have to compare different functional forms and you have to get posterior windows, but principles the answer.",
                    "label": 0
                },
                {
                    "sent": "Similar to the Clean up effect.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "In behavioral cloning, you can help inform your teacher if he sometimes makes mistakes, but they disappear as noise when we try to predict this next move.",
                    "label": 0
                },
                {
                    "sent": "It's called the cleanup effect.",
                    "label": 0
                },
                {
                    "sent": "If you make some mistakes, but there are random enough so that it doesn't make the same mistake over and over, But then we will just do the tape that mistake and that seems similar.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I told you that I'm not an expert in this.",
                    "label": 0
                },
                {
                    "sent": "But it's all the way you formulated.",
                    "label": 0
                },
                {
                    "sent": "It sounded like it averages out.",
                    "label": 0
                },
                {
                    "sent": "This is a little bit.",
                    "label": 0
                },
                {
                    "sent": "Different from that in that we have a specific statistical model.",
                    "label": 0
                },
                {
                    "sent": "So even if we have a good model about about how he does mistakes, then we can do most probably even better, because we not we do not average out, but we get a posterior again over the parameters graveling is being off.",
                    "label": 0
                },
                {
                    "sent": "And is there a reasonable assumption that you know how he makes mistakes?",
                    "label": 0
                },
                {
                    "sent": "Well, I think it's a reasonable assumption to try to figure that out, and testing things and getting posterior.",
                    "label": 0
                },
                {
                    "sent": "Someone else.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that's quite a reasonable thing to do.",
                    "label": 0
                },
                {
                    "sent": "Obviously if you know it's something more about the agent that you're looking at, say he's using a soft flex exploration with particular schedule, how we adjust its temperature.",
                    "label": 0
                },
                {
                    "sent": "You'll do extremely well.",
                    "label": 0
                },
                {
                    "sent": "OK, more questions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}