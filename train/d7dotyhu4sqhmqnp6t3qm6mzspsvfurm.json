{
    "id": "d7dotyhu4sqhmqnp6t3qm6mzspsvfurm",
    "title": "Kernel Methods for Learning Motion Patterns",
    "info": {
        "author": [
            "Lachlan McCalman, Australian Centre for Field Robotics, University of Sydney"
        ],
        "published": "Jan. 31, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_mccalman_kernel/",
    "segmentation": [
        [
            "I'm locked in the common I'm at the Australian Centre of Field Robotics and I'm just going to give a quick talk today about some of the work we've done building on Kenji for Kameezes work with Kernel Bayes rule.",
            "So.",
            "In Rome."
        ],
        [
            "Alex, we use the Gaussian process a lot, so sort of bread and butter for us for nonparametric estimation.",
            "You know the way we think about it is observations along the X and then we have some estimate of a district posterior distribution along Y.",
            "And of course marginals along.",
            "Sorry conditionals along X are Gaussian.",
            "Now what we were thinking is there's quite a lot of situations in robotics where that is not a valid assumption.",
            "We want to move beyond that too."
        ],
        [
            "Distributions that might be multimodal, where the the conditional distribution has is a more complex structure than the Gaussian, so."
        ],
        [
            "An overview of the algorithm that we used.",
            "We took Kernel Bayes rule which we've heard about this morning, so I won't go into much detail there.",
            "And what we did is we assumed fixed Gaussian mixtures for the posterior.",
            "So rather than try and recover a map estimate from the posterior embedding from Colonel Bayes rule, we thought let's see if we can recover a Gaussian mixture which gives us a little bit more information about the.",
            "Posterior distribution, so to do that, we use a quadratic program to find the coefficients of the Gaussian mixtures and then we have applied that to a few problems in robotics, which I'll give you a quick overview of."
        ],
        [
            "So posterior recovery.",
            "This is sort of a slight generalization of some work that Alex smaller did in 2008 where we're essentially just assuming a fixed Gaussian mixture distribution here.",
            "So we choose we choose mu and Sigma, or we can learn these.",
            "Beforehand and then what we do is we try and find the coefficients or the mixture coefficients beta the minimize the distance between the embedding of this in the Hilbert space and the embedding of the posterior that we get from Colonel Bayes rule.",
            "So chugging through the algebra on that one, we get a.",
            "What turns out to be a convex quadratic program here for the coefficients of beta.",
            "So in this case we've got G&H are both matrices which represent DOT product terms between those two embeddings.",
            "So we can further case of a Gaussian.",
            "We can calculate those analytically, so and then every time we want to get a posterior estimate from Kernel Bayes rule, either in the filtering or the regression case.",
            "We perform this convex optimization.",
            "Here we've got a regularizer term as well Lambda, which we have to train.",
            "So."
        ],
        [
            "So training is well, we found it quite challenging to train this sort of setup.",
            "We use K fold cross validation and the cost function we use is the negative log likelihood of the test data.",
            "So the holdout data we see what's the probability of that data occurring in the Gaussian mixture that we we come up with.",
            "So that's that's here we.",
            "Now that means overall we have to train both the parameters of Kernel Bayes rule, of which there are two regularizers and then the kernel width parameter.",
            "And then we also have to train this regularizer term from the from the convex program so.",
            "We played around with this quite a bit and what we ended up coming up with is a sort of multistart convex optimizer that reinitializes with different parameters and we essentially alternate between fixing the hyperparameters and training the pre image parameters and then fixing the pre image and training the hyperparameters.",
            "Yeah, it's quite sensitive to like choosing reasonable initial parameters, but we can get some decent results.",
            "Once we've got that training right.",
            "So."
        ],
        [
            "I'll give you a quick example.",
            "This is a filter really simple filtering simulation that we did so we had like a particle that that goes around in these rings an.",
            "At these two points, the particle essentially tosses a coin and either goes around the long way or it goes around the short way.",
            "So this sort of might be a robot that's like avoiding a pole or some sort of thing like that.",
            "And what we did was we used the filtering version of the KBR, and but we only did an observation update every five or so time steps.",
            "So what that meant was that when you run just the prediction operator, you see a bimodal distribution like this, and there are quite noisy measurements as well.",
            "By the way, the observations.",
            "So we can see."
        ],
        [
            "Like that, updating as as the filter progress."
        ],
        [
            "This is.",
            "And then."
        ],
        [
            "And we take a measurement.",
            "We can.",
            "We've taken a measurement at that cross.",
            "There we can see that the."
        ],
        [
            "Posterior distribution collapses down to the small ring and it then stays there until the next point where same thing happens again."
        ],
        [
            "And we compared this with particle filter, both trained using a Gaussian process to learn the prediction and observation models and also one actually given the exact.",
            "Knowledge of the particles motion function and you can see, so this is this is our algorithm here in 2D and 3D and you can see this is the KL divergent.",
            "So because we've got a quite a general distribution, we can't use like minimum distance or any of those metrics, so we use KL divergent and you can see that I only really when you use a large number of particles and you give the particle filter exact knowledge of the transition model to really beat this."
        ],
        [
            "So another one another application which we decided to try out this work on was a regression problem called motion field estimation so.",
            "The idea here is that you have a robot that's navigating an indoor space, which might be quite complex.",
            "But there are also sort of unseen obstacles which we want to understand things like we don't want to go through someone else's cubical.",
            "We don't want to like impinge on areas that might, you know where we might get in the way.",
            "So one way to sort of learn this.",
            "These sort of.",
            "Motion paths is to actually look at where people go, so you can put down a sick laser and then you can track people as they move around and what we've got here is arrows representing people's velocity that we've tracked just going through this particular region of the of the office, and so this is the kitchen area here.",
            "So what we thought was like can we?",
            "Can we learn motion field from this data that can?",
            "That can get a robot to sort of, navigate to the kitchen essentially now.",
            "What's interesting is that this is building on some work by Simon O'Callaghan, who used a Gaussian process to map this motion field.",
            "But what's interesting here is that we've got arrows going in both directions, so people are coming from this way going down into the kitchen and also from the other direction.",
            "And a Gaussian process."
        ],
        [
            "Learns that's this region.",
            "Here Gaussian process is learning a unimodal distribution, so it essentially learns them the average, which happens to be kind of pointing down, which is not really what we want.",
            "We want to actually.",
            "Avoid the wall and this is the this is the distribution the KBR with the GM estimate learned so you can see what we did was we use 20 coefficients values of Theta, evenly spaced, 20 Gaussian mixtures, and these arrows represent the weights of the mixtures based on direction.",
            "So you can see that.",
            "The robot queries is able to go either to the left or to the right, and with a simple motion model you can you can program."
        ],
        [
            "Robot to do just that, so these are some robots that are using the Gaussian process and you can see there sort of unsuccessfully navigate and.",
            "But then when we go to."
        ],
        [
            "The KBR you can see these two robots are crossing direction.",
            "They're both able to.",
            "Navigate successfully here so."
        ],
        [
            "So just quickly some lots of issues and further work that we'd like to explore.",
            "The first is this method sort of breaks down far from the training points, which is kind of what you'd expect.",
            "But what we'd really like is behavior a bit more like the Gaussian process, where far away from training data you kind of go back to the prior.",
            "That would be really cool.",
            "So I'm sort of thinking about how to do that, but any suggestions are definitely welcome.",
            "The training, as I mentioned, is quite difficult and also fairly slow, because every time we evaluate the posterior we have to do that convex optimization.",
            "Choosing the pre image means is another sort of problem where I could conceive we can maybe optimize those jointly with the other parameters, But then that makes the optimization a lot more difficult.",
            "We'd love to be able to do multi task learning as well, so that means like regression where we have multiple parameters but they're not all measured at the same location.",
            "So like a joint distribution over say pressure and temperature where our pressure and temperature sensors are Co located.",
            "And finally, for the purpose of doing like exploratory optimization, it would be lovely to be able to get some kind of entropy estimate from the posterior embedding, so you can essentially know how.",
            "How broad is my distribution without having to do a full?",
            "Prepare image solution so you can then plan motion paths more effectively.",
            "Yeah, so that's it really."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm locked in the common I'm at the Australian Centre of Field Robotics and I'm just going to give a quick talk today about some of the work we've done building on Kenji for Kameezes work with Kernel Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In Rome.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alex, we use the Gaussian process a lot, so sort of bread and butter for us for nonparametric estimation.",
                    "label": 0
                },
                {
                    "sent": "You know the way we think about it is observations along the X and then we have some estimate of a district posterior distribution along Y.",
                    "label": 0
                },
                {
                    "sent": "And of course marginals along.",
                    "label": 0
                },
                {
                    "sent": "Sorry conditionals along X are Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Now what we were thinking is there's quite a lot of situations in robotics where that is not a valid assumption.",
                    "label": 0
                },
                {
                    "sent": "We want to move beyond that too.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distributions that might be multimodal, where the the conditional distribution has is a more complex structure than the Gaussian, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An overview of the algorithm that we used.",
                    "label": 0
                },
                {
                    "sent": "We took Kernel Bayes rule which we've heard about this morning, so I won't go into much detail there.",
                    "label": 0
                },
                {
                    "sent": "And what we did is we assumed fixed Gaussian mixtures for the posterior.",
                    "label": 1
                },
                {
                    "sent": "So rather than try and recover a map estimate from the posterior embedding from Colonel Bayes rule, we thought let's see if we can recover a Gaussian mixture which gives us a little bit more information about the.",
                    "label": 0
                },
                {
                    "sent": "Posterior distribution, so to do that, we use a quadratic program to find the coefficients of the Gaussian mixtures and then we have applied that to a few problems in robotics, which I'll give you a quick overview of.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So posterior recovery.",
                    "label": 0
                },
                {
                    "sent": "This is sort of a slight generalization of some work that Alex smaller did in 2008 where we're essentially just assuming a fixed Gaussian mixture distribution here.",
                    "label": 0
                },
                {
                    "sent": "So we choose we choose mu and Sigma, or we can learn these.",
                    "label": 0
                },
                {
                    "sent": "Beforehand and then what we do is we try and find the coefficients or the mixture coefficients beta the minimize the distance between the embedding of this in the Hilbert space and the embedding of the posterior that we get from Colonel Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "So chugging through the algebra on that one, we get a.",
                    "label": 0
                },
                {
                    "sent": "What turns out to be a convex quadratic program here for the coefficients of beta.",
                    "label": 0
                },
                {
                    "sent": "So in this case we've got G&H are both matrices which represent DOT product terms between those two embeddings.",
                    "label": 0
                },
                {
                    "sent": "So we can further case of a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "We can calculate those analytically, so and then every time we want to get a posterior estimate from Kernel Bayes rule, either in the filtering or the regression case.",
                    "label": 0
                },
                {
                    "sent": "We perform this convex optimization.",
                    "label": 0
                },
                {
                    "sent": "Here we've got a regularizer term as well Lambda, which we have to train.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So training is well, we found it quite challenging to train this sort of setup.",
                    "label": 0
                },
                {
                    "sent": "We use K fold cross validation and the cost function we use is the negative log likelihood of the test data.",
                    "label": 1
                },
                {
                    "sent": "So the holdout data we see what's the probability of that data occurring in the Gaussian mixture that we we come up with.",
                    "label": 0
                },
                {
                    "sent": "So that's that's here we.",
                    "label": 0
                },
                {
                    "sent": "Now that means overall we have to train both the parameters of Kernel Bayes rule, of which there are two regularizers and then the kernel width parameter.",
                    "label": 0
                },
                {
                    "sent": "And then we also have to train this regularizer term from the from the convex program so.",
                    "label": 0
                },
                {
                    "sent": "We played around with this quite a bit and what we ended up coming up with is a sort of multistart convex optimizer that reinitializes with different parameters and we essentially alternate between fixing the hyperparameters and training the pre image parameters and then fixing the pre image and training the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's quite sensitive to like choosing reasonable initial parameters, but we can get some decent results.",
                    "label": 0
                },
                {
                    "sent": "Once we've got that training right.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll give you a quick example.",
                    "label": 0
                },
                {
                    "sent": "This is a filter really simple filtering simulation that we did so we had like a particle that that goes around in these rings an.",
                    "label": 1
                },
                {
                    "sent": "At these two points, the particle essentially tosses a coin and either goes around the long way or it goes around the short way.",
                    "label": 0
                },
                {
                    "sent": "So this sort of might be a robot that's like avoiding a pole or some sort of thing like that.",
                    "label": 0
                },
                {
                    "sent": "And what we did was we used the filtering version of the KBR, and but we only did an observation update every five or so time steps.",
                    "label": 0
                },
                {
                    "sent": "So what that meant was that when you run just the prediction operator, you see a bimodal distribution like this, and there are quite noisy measurements as well.",
                    "label": 0
                },
                {
                    "sent": "By the way, the observations.",
                    "label": 0
                },
                {
                    "sent": "So we can see.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like that, updating as as the filter progress.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we take a measurement.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "We've taken a measurement at that cross.",
                    "label": 0
                },
                {
                    "sent": "There we can see that the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Posterior distribution collapses down to the small ring and it then stays there until the next point where same thing happens again.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we compared this with particle filter, both trained using a Gaussian process to learn the prediction and observation models and also one actually given the exact.",
                    "label": 0
                },
                {
                    "sent": "Knowledge of the particles motion function and you can see, so this is this is our algorithm here in 2D and 3D and you can see this is the KL divergent.",
                    "label": 0
                },
                {
                    "sent": "So because we've got a quite a general distribution, we can't use like minimum distance or any of those metrics, so we use KL divergent and you can see that I only really when you use a large number of particles and you give the particle filter exact knowledge of the transition model to really beat this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another one another application which we decided to try out this work on was a regression problem called motion field estimation so.",
                    "label": 1
                },
                {
                    "sent": "The idea here is that you have a robot that's navigating an indoor space, which might be quite complex.",
                    "label": 0
                },
                {
                    "sent": "But there are also sort of unseen obstacles which we want to understand things like we don't want to go through someone else's cubical.",
                    "label": 0
                },
                {
                    "sent": "We don't want to like impinge on areas that might, you know where we might get in the way.",
                    "label": 0
                },
                {
                    "sent": "So one way to sort of learn this.",
                    "label": 0
                },
                {
                    "sent": "These sort of.",
                    "label": 0
                },
                {
                    "sent": "Motion paths is to actually look at where people go, so you can put down a sick laser and then you can track people as they move around and what we've got here is arrows representing people's velocity that we've tracked just going through this particular region of the of the office, and so this is the kitchen area here.",
                    "label": 0
                },
                {
                    "sent": "So what we thought was like can we?",
                    "label": 0
                },
                {
                    "sent": "Can we learn motion field from this data that can?",
                    "label": 0
                },
                {
                    "sent": "That can get a robot to sort of, navigate to the kitchen essentially now.",
                    "label": 0
                },
                {
                    "sent": "What's interesting is that this is building on some work by Simon O'Callaghan, who used a Gaussian process to map this motion field.",
                    "label": 0
                },
                {
                    "sent": "But what's interesting here is that we've got arrows going in both directions, so people are coming from this way going down into the kitchen and also from the other direction.",
                    "label": 0
                },
                {
                    "sent": "And a Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learns that's this region.",
                    "label": 0
                },
                {
                    "sent": "Here Gaussian process is learning a unimodal distribution, so it essentially learns them the average, which happens to be kind of pointing down, which is not really what we want.",
                    "label": 0
                },
                {
                    "sent": "We want to actually.",
                    "label": 0
                },
                {
                    "sent": "Avoid the wall and this is the this is the distribution the KBR with the GM estimate learned so you can see what we did was we use 20 coefficients values of Theta, evenly spaced, 20 Gaussian mixtures, and these arrows represent the weights of the mixtures based on direction.",
                    "label": 0
                },
                {
                    "sent": "So you can see that.",
                    "label": 0
                },
                {
                    "sent": "The robot queries is able to go either to the left or to the right, and with a simple motion model you can you can program.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Robot to do just that, so these are some robots that are using the Gaussian process and you can see there sort of unsuccessfully navigate and.",
                    "label": 0
                },
                {
                    "sent": "But then when we go to.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The KBR you can see these two robots are crossing direction.",
                    "label": 0
                },
                {
                    "sent": "They're both able to.",
                    "label": 0
                },
                {
                    "sent": "Navigate successfully here so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just quickly some lots of issues and further work that we'd like to explore.",
                    "label": 1
                },
                {
                    "sent": "The first is this method sort of breaks down far from the training points, which is kind of what you'd expect.",
                    "label": 1
                },
                {
                    "sent": "But what we'd really like is behavior a bit more like the Gaussian process, where far away from training data you kind of go back to the prior.",
                    "label": 0
                },
                {
                    "sent": "That would be really cool.",
                    "label": 0
                },
                {
                    "sent": "So I'm sort of thinking about how to do that, but any suggestions are definitely welcome.",
                    "label": 0
                },
                {
                    "sent": "The training, as I mentioned, is quite difficult and also fairly slow, because every time we evaluate the posterior we have to do that convex optimization.",
                    "label": 0
                },
                {
                    "sent": "Choosing the pre image means is another sort of problem where I could conceive we can maybe optimize those jointly with the other parameters, But then that makes the optimization a lot more difficult.",
                    "label": 0
                },
                {
                    "sent": "We'd love to be able to do multi task learning as well, so that means like regression where we have multiple parameters but they're not all measured at the same location.",
                    "label": 0
                },
                {
                    "sent": "So like a joint distribution over say pressure and temperature where our pressure and temperature sensors are Co located.",
                    "label": 0
                },
                {
                    "sent": "And finally, for the purpose of doing like exploratory optimization, it would be lovely to be able to get some kind of entropy estimate from the posterior embedding, so you can essentially know how.",
                    "label": 0
                },
                {
                    "sent": "How broad is my distribution without having to do a full?",
                    "label": 0
                },
                {
                    "sent": "Prepare image solution so you can then plan motion paths more effectively.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's it really.",
                    "label": 0
                }
            ]
        }
    }
}