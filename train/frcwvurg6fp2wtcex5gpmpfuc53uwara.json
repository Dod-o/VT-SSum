{
    "id": "frcwvurg6fp2wtcex5gpmpfuc53uwara",
    "title": "Complexity of Inference in Latent Dirichlet Allocation",
    "info": {
        "author": [
            "David Sontag, Computer Science Department, New York University (NYU)"
        ],
        "published": "Sept. 6, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_sontag_allocation/",
    "segmentation": [
        [
            "Topic models such as latent dearsley allocation are powerful tools for exploring large toolsets and for making inferences about the contents of documents.",
            "For example, if we had a collection of news articles, we might want to discover what are the topics that are present in this collection and what words are most likely to be associated with each topic, then giving a new given a new document we could hope to infer what are the topics at this new document is about.",
            "Despite Layton Dearsley allocation being one of the most popular topic models used with overhead with hundreds of papers written about it up to date, we know extremely little about the complexity of exact inference and also about how to do learning in these models.",
            "Typically, the papers currently that exist on this use approximate inference methods such as Gibbs sampling, variational inference such as mean field or their collapse variance, and learning typically proceeds by other sampling or expectation maximization.",
            "In this paper, we take a step back from the approximate inference zoo and try to ask what is it that makes the complexity of inference so difficult?"
        ],
        [
            "To be concrete, we study the computational complexity complexity of probabilistic inference in Layton Dearsley allocation.",
            "We assume that were given a new document with the words W one through WN with Arity topics, and we assume that there that we already finished with learning.",
            "So we have topic word distributions beta and also the dearsley hyperparameters Alpha given to us.",
            "Linder Slagle Kacian specifies a generative model for new documents as follows.",
            "First, a distribution over the topics is sampled from a deer sleigh with hyperparameters Alpha.",
            "Then, for each word I we sample the topic for that word.",
            "Zi from the multiple from a multinomial distribution given by Theta.",
            "Then for that.",
            "Then we sample the word WI from the topic from the topic word distribution beta specified by the actual topic Zi.",
            "When the hyperparameters Alpha are substantially smaller than one, the dearsley prior specifies that the Theta will be concentrated on a walk with port on only a small number of topics.",
            "So we consider 3 popular inference problems for latest allocation.",
            "First we look at the problem of finding the most likely assignment of topics towards where we've marginalized out the topic distribution Theta.",
            "This is the inference problem which is or is very close to the inference problem, But that's typically solved by using Gibbs sampling, collapsed Gibbs sampling and variation in France, and it has a nice advantage that it involves simply discrete variables.",
            "Second, we look at the problem of finding the most likely topic distribution Theta, given the.",
            "Given the document where we've now marginalized out instead of the topic word assignments, this is very useful for coming up with a concise representation for the document, which, among other applications could be used for information retrieval.",
            "Finally, we look at the question of sampling from the full full posterior distribution."
        ],
        [
            "Our theoretical analysis in our theoretical analysis, we tried as much as possible to focus on inference settings which are likely to arise in practical applications.",
            "So for example, whereas the model might have a large number such as thousands of different topics, typically, in practice we expect that each document may be only about three or four topics, or some small number.",
            "So in this scenario, we show that it's actually possible in polynomial time to find the map assignment of the of the topics two to the words.",
            "In contrast, in the setting where the number of topics that could be used for document or arbitrary, we show that the problem of finding the map assignment topics towards is NP hard.",
            "Next we looked at the question of finding the most likely topic distribution where now we've marginalized out the actual assignment of the topic to the words here.",
            "Now the actual setting of the digital hyperparameters turns out to be a critical quantity.",
            "When the digital hyperparameters are greater than or equal to 1, it can be shown that the posterior distribution is log concave and as a result, both the problem of finding the most likely topic distribution and also the problem of sampling from the posterior can be shown to be done in polynomial time.",
            "In contrast, when the hyperparameters are substantially smaller than one with this correspondence, two is a sparsity constraint.",
            "Saying that we want as few topics to be used to describe the document well.",
            "That's partially constraint is a source of hardness and results in both of those problems being NP hard an we show that by reduction from set cover.",
            "Please come to our poster and W 66 for more details."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Topic models such as latent dearsley allocation are powerful tools for exploring large toolsets and for making inferences about the contents of documents.",
                    "label": 1
                },
                {
                    "sent": "For example, if we had a collection of news articles, we might want to discover what are the topics that are present in this collection and what words are most likely to be associated with each topic, then giving a new given a new document we could hope to infer what are the topics at this new document is about.",
                    "label": 0
                },
                {
                    "sent": "Despite Layton Dearsley allocation being one of the most popular topic models used with overhead with hundreds of papers written about it up to date, we know extremely little about the complexity of exact inference and also about how to do learning in these models.",
                    "label": 0
                },
                {
                    "sent": "Typically, the papers currently that exist on this use approximate inference methods such as Gibbs sampling, variational inference such as mean field or their collapse variance, and learning typically proceeds by other sampling or expectation maximization.",
                    "label": 0
                },
                {
                    "sent": "In this paper, we take a step back from the approximate inference zoo and try to ask what is it that makes the complexity of inference so difficult?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To be concrete, we study the computational complexity complexity of probabilistic inference in Layton Dearsley allocation.",
                    "label": 1
                },
                {
                    "sent": "We assume that were given a new document with the words W one through WN with Arity topics, and we assume that there that we already finished with learning.",
                    "label": 1
                },
                {
                    "sent": "So we have topic word distributions beta and also the dearsley hyperparameters Alpha given to us.",
                    "label": 0
                },
                {
                    "sent": "Linder Slagle Kacian specifies a generative model for new documents as follows.",
                    "label": 0
                },
                {
                    "sent": "First, a distribution over the topics is sampled from a deer sleigh with hyperparameters Alpha.",
                    "label": 0
                },
                {
                    "sent": "Then, for each word I we sample the topic for that word.",
                    "label": 1
                },
                {
                    "sent": "Zi from the multiple from a multinomial distribution given by Theta.",
                    "label": 0
                },
                {
                    "sent": "Then for that.",
                    "label": 1
                },
                {
                    "sent": "Then we sample the word WI from the topic from the topic word distribution beta specified by the actual topic Zi.",
                    "label": 0
                },
                {
                    "sent": "When the hyperparameters Alpha are substantially smaller than one, the dearsley prior specifies that the Theta will be concentrated on a walk with port on only a small number of topics.",
                    "label": 0
                },
                {
                    "sent": "So we consider 3 popular inference problems for latest allocation.",
                    "label": 0
                },
                {
                    "sent": "First we look at the problem of finding the most likely assignment of topics towards where we've marginalized out the topic distribution Theta.",
                    "label": 0
                },
                {
                    "sent": "This is the inference problem which is or is very close to the inference problem, But that's typically solved by using Gibbs sampling, collapsed Gibbs sampling and variation in France, and it has a nice advantage that it involves simply discrete variables.",
                    "label": 0
                },
                {
                    "sent": "Second, we look at the problem of finding the most likely topic distribution Theta, given the.",
                    "label": 0
                },
                {
                    "sent": "Given the document where we've now marginalized out instead of the topic word assignments, this is very useful for coming up with a concise representation for the document, which, among other applications could be used for information retrieval.",
                    "label": 0
                },
                {
                    "sent": "Finally, we look at the question of sampling from the full full posterior distribution.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our theoretical analysis in our theoretical analysis, we tried as much as possible to focus on inference settings which are likely to arise in practical applications.",
                    "label": 0
                },
                {
                    "sent": "So for example, whereas the model might have a large number such as thousands of different topics, typically, in practice we expect that each document may be only about three or four topics, or some small number.",
                    "label": 0
                },
                {
                    "sent": "So in this scenario, we show that it's actually possible in polynomial time to find the map assignment of the of the topics two to the words.",
                    "label": 0
                },
                {
                    "sent": "In contrast, in the setting where the number of topics that could be used for document or arbitrary, we show that the problem of finding the map assignment topics towards is NP hard.",
                    "label": 0
                },
                {
                    "sent": "Next we looked at the question of finding the most likely topic distribution where now we've marginalized out the actual assignment of the topic to the words here.",
                    "label": 0
                },
                {
                    "sent": "Now the actual setting of the digital hyperparameters turns out to be a critical quantity.",
                    "label": 0
                },
                {
                    "sent": "When the digital hyperparameters are greater than or equal to 1, it can be shown that the posterior distribution is log concave and as a result, both the problem of finding the most likely topic distribution and also the problem of sampling from the posterior can be shown to be done in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "In contrast, when the hyperparameters are substantially smaller than one with this correspondence, two is a sparsity constraint.",
                    "label": 0
                },
                {
                    "sent": "Saying that we want as few topics to be used to describe the document well.",
                    "label": 0
                },
                {
                    "sent": "That's partially constraint is a source of hardness and results in both of those problems being NP hard an we show that by reduction from set cover.",
                    "label": 0
                },
                {
                    "sent": "Please come to our poster and W 66 for more details.",
                    "label": 0
                }
            ]
        }
    }
}