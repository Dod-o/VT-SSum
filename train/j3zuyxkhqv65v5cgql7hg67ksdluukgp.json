{
    "id": "j3zuyxkhqv65v5cgql7hg67ksdluukgp",
    "title": "MPI & OpenMP (Part 1)",
    "info": {
        "author": [
            "David Henty, EPCC, University of Edinburgh"
        ],
        "published": "Sept. 19, 2016",
        "recorded": "June 2016",
        "category": [
            "Top->Computer Science",
            "Top->Computers->Programming"
        ]
    },
    "url": "http://videolectures.net/ihpcss2016_henty_MPI_openMP_part1/",
    "segmentation": [
        [
            "So just briefly, who am I?",
            "My name is David Hentai.",
            "I'm from EPCC which is the Edinburgh Parallel Computing Center in Scotland and my background is in theoretical particle physics computational, but that was along time ago.",
            "I've been at UPC for over 20 years and I do a lot of training and EPC runs this the UK national supercomputer called Archer which we run and it's also a price system as well which is the link to this to this meeting."
        ],
        [
            "It's picture of Edinburgh.",
            "It's very nice.",
            "You want to visit.",
            "Very nice place.",
            "That's a picture of."
        ],
        [
            "Not sure this is, we run an MSE program or one year Masters and this is some of our students who who used this system were not using Archer, but I thought I just put."
        ],
        [
            "Up there, So what I mean, what am I going to do?",
            "I'm going to do an introduction to message passing program with MPI, and I'll tell you what I mean by an introduction and an introduction to share my program with open MP and then I'll do a bit of hybrid which is both MPI and open MP.",
            "At the same time, the assumptions are that you've used MPI a bit before.",
            "You have some knowledge of open MP.",
            "Hopefully you've looked at the background material which I was sent round earlier on.",
            "It's on the prerequisites.",
            "But all my exercises are based around this parallel traffic model, which I'll introduce and talk about.",
            "So the idea is that.",
            "Yeah, so I'm just checking up at the right side, so I'm going to introduce this traffic model and that's what we'll use to do the exercises.",
            "The slides are all done at the Xceed Wiki, so if I. Hopefully if you go too.",
            "You just go to the Xceed Wiki.",
            "Should I have a log onto?",
            "Under presentations there's a link here presentations.",
            "And then the MPI and open MP materials there.",
            "So you can't see that link, sorry.",
            "MPI and Open MP materials.",
            "Should go to a page which has all this this stuff on it.",
            "There is also.",
            "So these are arranged in order that they are also on this page.",
            "Test.",
            "Tinyurl.com/I HPCSS dash MPI Dash Open MP that will just give you a bit.",
            "They're all LinkedIn.",
            "It just in case you have problem finding the wiki page so, but the slides aren't ordered here, it's just a dumping ground so but on the wiki there ordered, so maybe it makes a bit more sense, but just in case you that there's that.",
            "They should all be there and that just goes straight through to her.",
            "A dump of all this stuff.",
            "But I would recommend that you get stuff from the from the xceed by by navigating the Xceed with its own people.",
            "Managed to find that on the Xceed Wiki.",
            "Is it there somebody managed to find it?",
            "Yeah, OK.",
            "So as I said, it's under."
        ],
        [
            "OK, there it is right there is also at tinyurl.com.",
            "There's some additional material.",
            "He is mostly slides, but I've got a new instructions for running on bridges, bridges, crib sheet PDF, which is the top.",
            "There's a small correction we're going to have to do to the exercises 'cause the way I've set them up.",
            "You won't be direct, they've reserved a whole bunch of nodes on bridges for exclusive use, But the way I've set up the the back scripts they won't by default go there, so I'll have to.",
            "We'll just do one quick correction there.",
            "There's the exercise sheet, which is the traffic model sheet.",
            "There's the codes I give you baseline code for the traffic model, but I'm going to also do a walkthrough of this more simple pie example, and this stuff isn't up there yet, but I will put up this stuff up that like.",
            "Jaune talked about this chat list this this challenge this this hybrid challenge that will go up there as well."
        ],
        [
            "So the timetable.",
            "Orbit behind already, but I'll give introduction recap.",
            "I'll do a log on and walk through the pie example and hopefully you can download and run the PY example at the same time, so I'll be able to talk about it and you'll have the code, and that will just bring us up to speed, and then I'll do a brief some stuff which picks up from concepts here, communicators, times, and modes.",
            "Then we'll have a break.",
            "Maybe make this only 15 minutes.",
            "Then I'll talk about non blocking communications.",
            "Which is you might call asynchronous communication is technically an MPI there they're called non blocking this subtle distinction there.",
            "And then we'll do the traffic model for the rest of the day."
        ],
        [
            "And tomorrow well tomorrow."
        ],
        [
            "Tomorrow, as I said, it's a challenge to teach people with such a wide Verity experiences.",
            "So the idea is that I give you a working code.",
            "The traffic model I give you at least the MPI version compiles and runs in parallel.",
            "You can play around with it, but hopefully.",
            "And you see the exercise sheet that lots of suggestions for things that you could do to work on it.",
            "Based on what we've covered in the lectures.",
            "So it's up to you what you do the it's a very, very simple model.",
            "It might seem fairly trivial, but the parallelization of this model is identical to the HPC challenge.",
            "Conceptually, the parallelization here is identical to parallelization in this this challenge code, we're going to give out, see how fast you can make it go.",
            "So hopefully you can try things out in the in the traffic model in a much simpler way than you would when you work with the slightly more.",
            "What looks more complex, but conceptually is not any more complex challenge code.",
            "I'm happy to cover whatever you want to know, so please let me know if the brakes or just you know I've got stacks of material which I can bring in and haven't got here.",
            "But you know, if there's something you want to know about this, please talk to me because.",
            "So what I'm going to do is, first of all, I'm going to do a quick recap.",
            "Of.",
            "MPI so this is."
        ],
        [
            "Stuff that I suspect you probably know.",
            "But but but really, this is just to put things in context to cover.",
            "Hopefully most of what I'd expected you to do as the prereading, but maybe bring you up to speed.",
            "So message passing is parallel programming using processes.",
            "What that means is that it allows us to write a parallel program which runs on a cluster allowed supercomputer.",
            "So the best mental model you have a large supercomputer is all the laptops in this room, connected by cables.",
            "OK, that that's what a large supercomputer is.",
            "A large number of individual computers, each of which is a shared memory processor connected by some network, each running their own operating system.",
            "So the world largest supercomputer in the world, conceptually conceptually.",
            "Is the same as all the laptops in this room?"
        ],
        [
            "And so I'll talk a bit about message passing parallelism, some practicalities here."
        ],
        [
            "So I think that I'll come back to this analogy for when we talk about open MP, but my my analogy for MPI programming is 2 whiteboards in different single person offices.",
            "The whiteboards are your memory, so there's a white one.",
            "White Board is the memory in that laptop, and the other white board is the memory in that laptop is memory, 'cause you can read and write to it.",
            "Two people are working on the same problem.",
            "Which of the processes on different nodes attached by the interconnect?",
            "How do they collaborate?",
            "The only way you can collaborate so this person is in Sydney, Australia and you're here in Lubiana.",
            "How do you collaborate?",
            "You can't see each others whiteboards that computer can't read that computers memory.",
            "It's impossible.",
            "Well, maybe the FBI can, but you can't.",
            "You can't read the memory of other peoples computers, so the only way to collaborate is by explicit communication.",
            "So you have to you have to where you want to collaborate.",
            "You have to explicitly communicate with the person.",
            "With by making a phone call or sending an email and an MPI, this is called message passing.",
            "There is no shared data, will come back to this when we do open MP where you can have shared data share open MP corresponds to four people in the same office or writing to the same whiteboard.",
            "You can share data you can say I just wrote it there go and read it.",
            "You can't do that in MPI.",
            "All your data is local, other processes are.",
            "The MPI processes don't have access to it."
        ],
        [
            "So the way it works is an MPI.",
            "If I have two processes, process one and process too and I want data to go from process wonder process to the think of process one being running on that computer and process 2 running on that computer.",
            "What I have to do is I have to explicitly do something so in NP."
        ],
        [
            "Right, right, I might say a = 23 they want.",
            "I want to communicate A to this other process will see that message."
        ],
        [
            "Passing well, so in my memory here A is set to 23.",
            "I have some magic routine."
        ],
        [
            "Possibly called send which says I want to send this data to process too, so we'll see MPI provides you a bunch of library calls, just just library functions, which allows you to do something like this.",
            "This has send the data in this in this abbreviated syntax to process too, and I want you to send a so that data is sent OK."
        ],
        [
            "But that's not enough, OK.",
            "So I said an analogy to message passing is sending an email.",
            "If you send an email to somebody, have you communicated the information?",
            "No, why not?",
            "They haven't read it, so message passing is fundamentally 2 sided.",
            "It needs the active the active participation of both ends so the sender has to actively send and this cannot go into this process's memory.",
            "So I've I've imagined that you have an inbox like an email inbox or somewhere where the data is stored, but it's not in your memory.",
            "The only way the data is transferred in standard message passing is the receiver has to actively caller receive so they receive."
        ],
        [
            "I have to say I want to receive data from process one and 'cause the receiver is total control.",
            "The receiver decides where to put the message.",
            "The sender can't tell you where to put the message.",
            "You decide when you receive so that says."
        ],
        [
            "Receive a message from from process one and put it into my variable B.",
            "So be on process two, is now the same as a on process one 'cause you've done this and you've done a receive, but then process two can do.",
            "Hey."
        ],
        [
            "Equals B + 1 OK."
        ],
        [
            "Processes choose A is 24 and process once a is 23 and this is this is a.",
            "An example of the SPMD programming model.",
            "Single program multiple data.",
            "The value right one program.",
            "So the variable A exists on every MPI process, but it can have different values.",
            "So in MPI and message passing you cannot ask what is the value of a.",
            "It's a meaningless question.",
            "You have to say what is the?",
            "What is the value of a on process one?",
            "What is the value of a on process 990?",
            "Nine 1999?",
            "Now they might be the same.",
            "You might have arranged for them to be the same, but there by default they are not the same.",
            "So so you think of process one running on that laptop.",
            "Process to running on my laptop.",
            "And then the question is what to send do OK?",
            "OK, well we'll come back to that."
        ],
        [
            "So synchronization this will maybe make more sense later on, but synchronization is automatic and message passing a lot.",
            "I'll try a lot of people get this wrong.",
            "Messages do it for you.",
            "What I mean is you never have to worry.",
            "You might say well if process one is running on that computer and process to it running on that computer.",
            "What happens with that process goes to sleep for an hour and this one keeps running or that once twice as fast as that one in message passing.",
            "You don't care because whenever you communicate with each other the synchronization happens.",
            "By default, so for example, when I make a phone call to somebody.",
            "I wait for them to pick up five, come home early, and I make a phone call.",
            "I just wait for them to come and pick up the synchronization.",
            "Happens when you receive a phone call, you going well, you go and stand by the phone and wait for it to ring.",
            "OK, so so it may not become obvious that we look at a bit more deeply, but message passing you don't have to worry about mutual synchronization of processes in the standard way of programming becausw.",
            "The synchronization is done for a lot of people.",
            "Get this wrong.",
            "You see an awful lot of MPI programs which have barriers in them which are not necessary.",
            "OK, so we'll come back to that and there's no danger corrupting someone elses data.",
            "You never have to worry.",
            "Oh no, he night I'm going to send a message to somebody, and that person might not be ready.",
            "Doesn't matter that message cannot appear in that other processes memory until they actively quarter received, at which point they have decided what to do with it.",
            "So you cannot.",
            "You cannot corrupt someone elses data.",
            "In message passing it is impossible, so the receive the way receive works is quite critical."
        ],
        [
            "So actually communication.",
            "Most sending a message could be either be synchronous or asynchronous.",
            "Synchronous end is not completed.",
            "All the messages started to be received.",
            "An asynchronous send completes as soon as the message has gone, so this is I'll come back to this.",
            "This is like making phone phone call or I got another example where if you phone somebody and they're not there, you wait for them to pick up OK, so then they pick up.",
            "This is like sending an email or sending a letter.",
            "You just stick it in the post you hit send and it disappears and you never know what happened to it.",
            "OK, so they're conceptually two ways of sending a message at MPI.",
            "We'll see you can do both received it, usually synchronous.",
            "When you issue receive, you sit and you wait for a message to come in.",
            "OK, so there are other forms of that.",
            "The standard receive when you say receive you just you say I want to receive a message and I will just.",
            "I'm going to wait for it to come in.",
            "So there."
        ],
        [
            "Biology for a synchronous send.",
            "Well, I actually think that the making a phone call is a better analogy, but you can think of maybe sending a fax that basically the important point about sending a fax as you go to the fax machine you put that you put the thing in.",
            "It said at some point you probably don't use fax machines anymore.",
            "She's update this, but you get a beep, but this beep is critical.",
            "Beep says.",
            "I know the data has been arrived has arrived or like making a phone call.",
            "You know that somebody else has picked up the phone 'cause you're on the line together, so that's called synchronous.",
            "Sending there's a synchronization in time async."
        ],
        [
            "The stand is like posting natural setting an email.",
            "You put the message in the in the letterbox.",
            "The postman delivers the letter hopefully, but you have no information as to whether it got there or not and MPI allows you to do with St both synchronous and asynchronous send and I'll come back to but the most misunderstood the thing about MPI is that when you call MPI send, which is the send routine, you do not know if it will be synchronous or asynchronous.",
            "And that has huge ramifications for writing correct codes, and I'll come back to that and I'll try and explain why they thought that was a good idea.",
            "Probably is a good idea, but it causes endless confusion.",
            "OK, MPI send the standard send could be either synchronous or asynchronous.",
            "There's no guarantee which it does.",
            "MPI could decide if it's a Wednesday.",
            "I'll do it synchronously if it's a Thursday, I'll do it asynchronously 'cause it has other mechanisms for choosing that.",
            "I may have a talk which I'll try and explain why that is."
        ],
        [
            "So we consider point to .1 Sender 1 receiver and it's like sending personal emails.",
            "I'll I will go through it in a lot of detail here, but another important concept in MPI and message passing in general are collective communications.",
            "So for example.",
            "I I could deliver this this lecture through point to point communications every time I wanted to say the next line, I could whisper in your in your ear and whisper in your ear and whisper in your ear.",
            "Go round case.",
            "It would take me an hour to do the whole one line of a lecture that would work functionally, but clearly it's a crazy thing to do.",
            "OK, so collective communications are a higher level, higher level of communication than then.",
            "Point to point because they involve more than point to point communication.",
            "Involves one Sender 1 receiver collective.",
            "Communication involves a whole group."
        ],
        [
            "So a simple message communicate between two processes.",
            "There are many instances where communication between groups is required.",
            "They could be built from simple messages, but they could be implemented separately for efficiency and so the obvious one."
        ],
        [
            "Did you get the moment is a broadcast I'm talking in your perhaps this thing.",
            "It's a one to many, many communication, but again, this is commonly misunderstood.",
            "In this example we are all in MPI speak.",
            "We are all participating with the broadcast.",
            "We are all actively doing a broadcast.",
            "Within that we have different roles.",
            "I'm the sender.",
            "You are the receiver, but we all have to call the broadcast routine OK. Will come back you so so we are all actively participating in a broadcast and MPI program.",
            "We all have to have called the broadcast routine within that one person sends and many people receive, but it's just one person calls the broadcast.",
            "In MPI, it is impossible for me if I want to transmit data to you.",
            "If I'm the only person doing something, then nothing can happen 'cause the only way data can go from one process to another is if one process is sending and the other one is actively receiving or in collective communications, it's done under the hood, but everyone needs to be calling the broadcast."
        ],
        [
            "Broadcast said."
        ],
        [
            "Data, for example, we might and will.",
            "I don't do this in the traffic model, but I use.",
            "You might have data."
        ],
        [
            "And it's just copy to everybody.",
            "You often do this at."
        ],
        [
            "Starter simulation we did the temperature or the pressure and tell everybody what that what what.",
            "It is just just just just scattered around."
        ],
        [
            "Skechers light broadcast.",
            "But instead of sending the same data, Trevor, you chunk it up so I might have data Note 12345 and I I have a block of data and I want you to get the first block.",
            "You get the second block, you get the third block, rather do it laboriously by hand.",
            "There's the MPI routine that says scattered this data up, and it automatically takes an array and splits up into chunks."
        ],
        [
            "This is scattered.",
            "It's."
        ],
        [
            "To some.",
            "Into some in some way, and we'll see the traffic model is a 1D cellular automaton.",
            "We have a long Rd which might be 10,000 or 100,000 cells long.",
            "What you'll see the program does is 1 process initializes the road and then it scatters the road to the road to the other processes.",
            "So the road is split up into chunks and distributed at the end you might want."
        ],
        [
            "To bring it back together, you might want to gather it."
        ],
        [
            "Together again, they gather and scatter are inverses of each other."
        ],
        [
            "Having said that, the most common operation you do in parallel programming."
        ],
        [
            "Is a reduction operation.",
            "So if you if you think back to John's model of four people collectively doing a weather forecast simulation on the United States, they each have different regions of the United States.",
            "So each person can compute the rainfall over the Northwest, Northeast, Southwest, and southeast corner of the United States.",
            "That's a meaningless figure.",
            "Who cares what the rainfall is in the bottom left hand corner of the United States?",
            "The only meaningful figure is the global rain form.",
            "And So what you often do in message passing.",
            "Is programs.",
            "A process computes a number and then you have to you have to sum it up globally.",
            "So for example, if I wanted to compute average age OK, I need to sum up all the ages and then divide by the number of people and come up with one answer.",
            "So there's multiple input data.",
            "Your own ages, but there's only one output.",
            "The average at MPI calls that a reduction operating it's reducing data, which is distributed into a single answer.",
            "So you could have you could have data from several party.",
            "Might want to strike or not.",
            "Everyone has their own vote, but there's a majority outcome.",
            "But in fact, adding adding in real scientific and technical programs, it's adding numbers together, which is the most common reduction operation, almost almost exclusively, and we'll see that in the traffic model."
        ],
        [
            "So reduction from the global sum product, Max, Min etc from a bunch of.",
            "So if I wanted to."
        ],
        [
            "Add these together.",
            "I could call a reduction operation.",
            "It would add note 1234 and five together to give me 50K.",
            "So that's that's really just a brief recap of sort of where the introductory material that I don't hopefully maybe got a chance to look at, took you up to.",
            "Are there any questions about any of that?",
            "If it's familiar, anything that's unclear.",
            "I don't have any, so."
        ],
        [
            "Hardware, the reason the MPI is popular message parts being popular for over 20 years is it naturally Maps to distributed memory.",
            "So as I said, if a modern supercomputers, a bunch of individual computers running some operating system that you don't want to pay for to use Linux, 'cause it's free and use GCC 'cause they're all free.",
            "OK, and and basically it's a very not the model and message passing is a model for parallel computing, a conceptual model, but it Maps very well onto the hardware because these individual processes running on different processors in different computers.",
            "There will be some high-end supercomputer.",
            "There will be a very fast interconnect between them, so sending messages is quick.",
            "And So what you do is you run one process per processor core and messages go over the interconnect between the nodes.",
            "And when I say no, a node is a single computer will see on bridges they have 28 cores, but that that might have four cores that might have four cores there physically distinct 'cause they're physically different machines.",
            "And that also means they're running different operating well.",
            "They're running different copies of the same operating system.",
            "OK, they're both.",
            "They both were running different operating systems, but but anyway, each node is a separate computer running its own operating system, so on bridges will be running ten 10s of thousands of thousands of copies of Linux.",
            "So."
        ],
        [
            "With practicality, might save a wait a second.",
            "That laptops got four cores that laptops got four cores.",
            "There's an interconnect between them.",
            "But isn't it crazy what, what?",
            "How am I going to do this because?",
            "MPI's model is is processes talking to each other, but we know in practice that four of the processes are on the same computer they're actually running under the same operating system on the same computer.",
            "Isn't it crazy that those two processes send messages to each other?",
            "When in fact it might seem crazy, but in most MPI situations we ignore the architecture.",
            "We just imagine that these are eight individual processors, so we run four processes here, 4 processes here, and we just forget about the fact that actually these are on the same node and these are on the same node and that works.",
            "You can you can go beyond that.",
            "You can say, well, actually I'll only one one process here in one process here and then exploit this multiple.",
            "This shared memory architecture.",
            "Through something like threading through open MP and you can do that and that's what's called hybrid programming, but in practice you have to really think hard whether that's worthwhile, because this seemingly crazy model works really well.",
            "It works really well becausw, anything, anything is only as fast as its slowest part.",
            "The slowest part here is sending a message from here to here over the interconnect that slow OK when this guy talks to this guy.",
            "It's quick.",
            "OK, because they're just on the same node, it's just a memory copy.",
            "So although it might seem crazy, it doesn't slow the program down because.",
            "You still got this this this link here.",
            "Now there are situations where you can argue that too naive, but most Yep.",
            "Yep.",
            "Question.",
            "So that's a bit.",
            "So the question is, does it make sense to do the reduction here and then the reduction here?",
            "So in the old days, yes, with MPI was new and hadn't been optimized for shared memory systems.",
            "It sometimes made sense to actually call a reduction here.",
            "Reduction here and then a reduction across the mall.",
            "But modern MPI implementations are very clever, so they know these four guys know.",
            "So you say they can't share memory?",
            "OK, of course they can.",
            "I mean, it's just you know the operating system to do anything.",
            "So under the hood the operating system just.",
            "Get to talk to each other directly.",
            "You can't do it easily in MPI or not easily anyway, but yes.",
            "So that's the kind of optimization which is, which is done for you.",
            "So maybe 10 years ago that was worthwhile, but but not now.",
            "So.",
            "Message between the processes on the same node.",
            "Fast, but you might say there is one disadvantage to this, though if I have four MPI processes here and four here, each of these are setting.",
            "We're getting lots of messages going over the network, getting lots of small messages, and if you don't have a very good network, that can be a problem, But that's that's that's really for later."
        ],
        [
            "So um message passing with shared memory, we typically want one process per core.",
            "We don't exploit the shared memory.",
            "It's like finding your office mate.",
            "OK, it's like being in the same office as somebody having a whiteboard, but saying, look, we split the whiteboard into.",
            "I'm not going to talk to you.",
            "We were not.",
            "I don't don't touch my whiteboard if we want to talk to each other, I'm going to phone your email, you OK?",
            "Clearly it's not a very healthy situation, but in this amount, but it actually works surprisingly well.",
            "And the other thing is message passing programs run by some special job launch and we'll come back to that."
        ],
        [
            "So issues.",
            "Sends and receives must match, so the danger of deadlock.",
            "So the biggest problem in MPI is if you issue a send without a receive or receive without ascend then they don't match up, and that's very easy to do.",
            "So MPI programs are quite hard to write, but typically they either run or stop completely.",
            "You almost never have it relatively rare to have subtle at least in simple MPI programs.",
            "It's relatively rare to have subtle bugs.",
            "Bugs are normally.",
            "Pretty castrator code just stops working.",
            "It's possible to write very complicated programs, but actually most scientific programs have a have a relatively simple structure, and people ride.",
            "Being more people are writing more complicated things now, but but a lot of scientific codes are very simple.",
            "Comp structure often results in simple communications patterns, and often you can use a collective communication, so collective communications are often very there.",
            "In MPI was designed for scientific and technical programming, so the collective communications in their other kind of ones that they expect people would want to do so.",
            "There's a question, yeah?",
            "Received so I'll come back to that.",
            "One of the exercises to look at MPI send receive.",
            "So using MPI send, MPI Recv is possibly incorrect.",
            "But well, the other thing is you can download the NPC H implementation.",
            "Look at MPI send receive.",
            "I think it's just I send or receive.",
            "Wait, I mean you know 3 lines.",
            "But anyway look at it.",
            "But again we could.",
            "That's one of the things we can transform the exercises.",
            "Actually to try that."
        ],
        [
            "Summary messages the only form of communication or communications explicit you know if there isn't an MPI calling the program, it's it's when they're not communicating.",
            "MPI processes.",
            "Just programs running on your laptop, and it's important to know the operating system.",
            "Linux does not know that their MPI programs you know this thing is, you know it's running fire Fox and email client.",
            "A couple of MPI programs that the operating system, they're just processes the operating system is not optimized for message passing.",
            "OK, we used Linux, 'cause it's free.",
            "OK, it wasn't written for high performing.",
            "It maybe originate from iPhone computer, but Linux is this huge, vast lumbering beast of an operating system, so on a lot of a lot of high performance supercomputers you spend most your time turning, turning all the bits off that you don't don't want in the next.",
            "Both systems use the SPMD model called single program.",
            "Multiple data or processes run exactly the same code.",
            "You might say if all processes run the same code, why don't they do the same thing well?",
            "Well, in MPI you have this magic thing called your rank, so once you know you run exactly the same code.",
            "So you say, well, if you're running the same code, surely the same thing.",
            "No, 'cause you have this magic variable which you and unique ID.",
            "And once you know that you can do different things so you can say what's my ID OK if I'm ID 7 do X else do Y and communications is basic is point to point and collect communication in place more complicated parents off to the current many codes.",
            "So what I want to say."
        ],
        [
            "So message passing is a programming model, but John said it in his introductory talk.",
            "MPI is really the only message passing library that anyone uses at the moment.",
            "It's been dominant.",
            "It's been around for 20 odd years.",
            "It's been dominant for over 10 years.",
            "The basic concept of relative straightforward.",
            "You need to understand them and actually the message passing model can be surprisingly, surprisingly difficult.",
            "So my example.",
            "I mean, some people, some people find it natural and it's not a problem.",
            "Some people have difficulty grasping it, but this code here, which is a perfectly reasonable bit of serial code.",
            "OK fix is less than zero.",
            "Print error, exit OK. That's fine, and cereal, but that's a very dangerous piece of code to write in a parallel program and message passing what, why?",
            "Why is that one?",
            "No, no, well no I don't.",
            "Yeah.",
            "Yeah.",
            "So your ex is minus five.",
            "My ex is 7.",
            "OK, you quit OK, I don't know you've quit.",
            "How do I know you've quit you dead?",
            "All that happens is a while later I try and send your message and you and nothing.",
            "I try and receive a message from you.",
            "I think he's he's a bit quiet so you know.",
            "Because messages are the only the only way of checking.",
            "She say OK, what I'll do is if X is less than zero, I'll send him a message to tell him I'm quitting, but he has to actively receive it.",
            "He knows he's got a message.",
            "If he checks for it.",
            "OK, so if you get into this weird catch 22, there are ways round it, but that things like this.",
            "Are are are subtly wrong in can cause problems in message passing programs.",
            "So simple things can be quite difficult to implement.",
            "So what I wanted to do is to start off with this pie example and if you go to the.",
            "Survive."
        ],
        [
            "And Job actually had this on this.",
            "It's a very standard example.",
            "Um approximations of value of Pi can be obtained from the following series, so this is just the integral of 1 / 1 + X ^2 is arctan, whatever, which is arctan.",
            "One is powered by 4, doesn't matter how this works, but this is a very naive expansion.",
            "Pie by 4.",
            "Is this someone over and some might want to end 1 / 1 plus I'm as 1/2 / N ^2.",
            "It's just drawing rectangles under the curve.",
            "It's incredibly naive.",
            "But this summation is independent, so if I wanted to sum from N = 1 to 1000, you could do one to 100.",
            "You could do 101 to 200.",
            "You could do 100 and 200 and 300 because split into 10 blocks.",
            "We could all computer separate contribution to PIE and then we could add it up at the end.",
            "OK, because addition is associative, at least mathematically.",
            "Tradition doesn't matter, so we can split it up so we can so we can trivially paralyze this calculation.",
            "So we'll use this as simple example for MPI open MP.",
            "It's the traffic models are much better analogue of real codes, but this illustrates the basic concept.",
            "So what I wanted you to do was to if you go to the sheet.",
            "Which is.",
            "Up, that's what I want to do it so I can't do.",
            "If you go to the excitement here, the bridge is reference sheet.",
            "Is is is is.",
            "Basically I'm just going to check here.",
            "You can run something on bridges.",
            "So what this is really just saying.",
            "Log on, obtain the source code which is actually I want you to the pie example.",
            "For this you can also get the traffic example.",
            "For this example we get the pie example.",
            "You can grab it, you could unpack it, you can compile it, and then you should be able to run it.",
            "And if you just follow these instructions, we should be running OK.",
            "There are three ways of running.",
            "One is submitting a batch job to the bridge is compute nodes one is running on the login nodes and one is interactive access to computers.",
            "I I have specified the mall there but I didn't realize.",
            "So what I'll do is I will.",
            "I'll just live.",
            "This is always dangerous, but I'll try and show you live on the screen.",
            "What I want you to do, but hopefully you should be able to do the same thing.",
            "Um?",
            "Yep, reservation.",
            "He has batch.",
            "OK. OKOK so well?",
            "If So what I'll do is I'll just what you're so if I I I'm going to log into bridges.",
            "That's that's just go.",
            "Even without reservation, but.",
            "So what you just ridges.psc.edu?",
            "Dot Edu.",
            "We are on.",
            "We we W get that that that stuff.",
            "We unpack it.",
            "This is all in the sheet.",
            "And if you see it's called pie example.",
            "So it's made maybe not a very good name, but it's called pie example.",
            "So if you go to pie example, you'll see that there's.",
            "Um?",
            "Serial code in C. And Fortran and parallel, so I would probably do the parallel MPI code only for do C or Fortran.",
            "But if I do the seat the C1.",
            "I should be able to compile it and I should be able to run it well.",
            "What are filter settings?",
            "Batch.",
            "Everyone on the board here, so it's just double dash res equals H PCs, so I've given you.",
            "You'll see in the sheet.",
            "I'm really think we have 10 minutes to go through I think with MPI backstop job which you should just be able to use as a template to run any of your own programs.",
            "What you need to add to that and I didn't have time to fix it.",
            "OK but I would.",
            "I would do this.",
            "I'll leave this up.",
            "You should do S tax.",
            "What is it minus minus?",
            "As.",
            "No space.",
            "Sorry, I've.",
            "I hate the font lock mode, so if you if you correct that in your MPI batch dot job, you will there be able to use that for everything and then you copy copy this.",
            "So apologies, I didn't get a chance to put that in, but you just need to put minus minus hash S batch minus minus rate equals IHP CSS.",
            "So I just but that's the apology.",
            "You can have to make that change, but you should just fail to follow through the examples that sheet to get this pie example running.",
            "So that was the first thing I wanted people to do to check that they could.",
            "They could get it running.",
            "So if anyone managed to other people did it before hand.",
            "If you have any problems, so will now go around and just wanted to check everyone can compile and submit and run this parallel pie job, which is a real MPI program.",
            "And then I'm going to go through it on the screen to try and try to illustrate that things aren't as simple, and MPO offers people.",
            "People think, well, NPI.",
            "The thing is, MPI is really simple to write.",
            "Then if you think about it or why you think wait a second.",
            "How does that work?",
            "And then you understand how it works.",
            "It becomes simple again, but it's that kind of two stage process.",
            "So stick your hand up.",
            "You have any problems or if I've made a typo, there's easily at probably a typo here.",
            "So just to say you can track your jobs using SQ, but actually they finish really quickly and then there's a maybe a 10 second delay to your output.",
            "There's this sort of void zone when your job has finished, but you don't have any output, but it's just a very small delay.",
            "Then you'll see the dot out file so.",
            "So you do need to make a copy of the batch of the batch job runs.",
            "It's kind of the backdrop runs if the bank job is called Hello Dot Job.",
            "It runs a job called Hello.",
            "You need to.",
            "You may need the copy, so did it work or?",
            "Yes.",
            "So like the copy is significant descriptor generic script which runs a job of its own name.",
            "So if you call the script Lubiana dot job, it will run at executing called Libyana.",
            "So what I'm going to do now is just.",
            "Basically I wanted to walk through that code, but I actually had.",
            "So what I'd like to do is.",
            "If I've got a kind of a quick quiz here, this is really just for fun, but if people could go to www.sockrative.com.",
            "So if I do.",
            "If people go to www.socksocrativeaccountsbelsakrative.com.",
            "So people would go to that website.",
            "OK.",
            "So just go to www.socrative.com K. Undo student login.",
            "Then you just want to specify.",
            "This.",
            "This is the quiz KE0T3 YBU.",
            "So when you do the student login, just give that give that.",
            "Give that name.",
            "And it will ask you for a name, but it does anything.",
            "I don't care what you put in just doesn't matter.",
            "OK, So what you should get so you go to www.socrative.you.",
            "I just I went to WWW.",
            "Dot socrative.com.",
            "Student login.",
            "That's the room name and then you should get join room and you should get this question.",
            "Oh OK, so I'm just testing this out.",
            "You should click all.",
            "There's more than one answer.",
            "Some of the questions have more than one answer, so click on all the apply so I don't know.",
            "This is just a test question, but let's just see how we're getting on.",
            "So we've got 21, so there's 28 people logged in and 21 of answered.",
            "If people could keep logging in so it's WWW.",
            "Sorry.",
            "Go to www.socrative.com student login.",
            "And the room name is.",
            "That Kate Miller KE0T3 YBUK.",
            "Kate.",
            "OK, so we've got 26.",
            "I'll just wait a bit more while people answer and the Thunder goes on, so there's there's there's some of the questions have more than one right answer so.",
            "I.",
            "Problem.",
            "No, I mean there's a Oh yeah it might be.",
            "Yes, yeah, that's right.",
            "So that's about.",
            "Let's see how did we do.",
            "OK, so.",
            "Everybody got it right, but I don't.",
            "People didn't maybe use the multiple answers, So what I'm going to do now is I'm going to go to the next question.",
            "OK.",
            "So this should.",
            "This should update and you should get this question now to run at MPI program across special compiler special libraries, special parallel computer or special operating system so people could click all they think applies.",
            "Again, it's all I'm just trying to get an idea of some of these questions are quite easy and some are not so easy at all.",
            "If anyone still wanting to join, that's the.",
            "That's the log.",
            "That's the room this KE0T3 YBU.",
            "So we're getting up some numbers, OK?",
            "Yeah.",
            "Oh OK, right so OK, so this I've not used this system before 'cause there's so if there's.",
            "If there's one answer only lets you set one if there's more than armed answer.",
            "If there's any answers, I think it lets you set N, doesn't it?",
            "I think that's how it works.",
            "It's clever.",
            "OK, sorry so.",
            "Well, you can guess there's only one answer then if you OK.",
            "So yeah, so the system is is clever.",
            "When I entered the question 'cause it notice there was only one answer, it it only let me set one option.",
            "So I there must be some way around that but I've not.",
            "So that's a mistake I should have.",
            "So let's see how we did.",
            "Got pretty good feedback, so I'll just do it.",
            "How did we do?",
            "OK, so.",
            "So this is this is this is a non this is a non trivial question.",
            "OK this is what I'm hoping to do in this.",
            "In this course is really take fairly simple questions but tell you there so.",
            "You need special libraries, that's correct.",
            "You don't need special compilers, OK, MPI does not require special compilers.",
            "The MPI compiler sorry.",
            "The program you used to compile the compiler used to compile.",
            "Your MPI program has no idea about MPI.",
            "OK, when you call MPI send, it doesn't know it might be turning the screen pink or making your laptop beep.",
            "It has no idea whatsoever.",
            "Now that's not obvious because when you compile an MPI program, you type MP IF90 or MPI CC.",
            "Sounds like a wonderful compiler.",
            "All that is doing is calling a normal compiler, GCC or the Intel compiler and linking libraries and stuff so so the compiler knows zero about MPI, nothing.",
            "It's just a library with some weird function calls.",
            "You don't need a special parallel computer that, again, that's a perfectly reasonable thing to say, but you can run MPI programs on your laptop 'cause MPI is based on processes OK.",
            "Processors have been around for decades.",
            "OK, processes are not special.",
            "For parallel computing, we just hijacked them 'cause they were there.",
            "Any laptop can run hundreds of my laptops running hundreds of processes at the moment.",
            "What's it doing?",
            "I could pick up a.",
            "What's my laptop doing at the moment?",
            "OK, it's running loads of stuff.",
            "You know what's it running now?",
            "Root messed, it's running hundreds of processes.",
            "OK, how many is running?",
            "I can't see there also running hundreds and hundreds of processes.",
            "OK, and all we do is some of them are MPI processes, but any computer can run a parallel program 'cause anymore any modern modern operating system can.",
            "Can support multiple processes now there's no point for performance in running 100 MPI processes on your laptop, 'cause it won't have 100 processors.",
            "But for debugging it's perfectly fine.",
            "OK, so OK for performance, you'd never run more processes than you had processors or physical cores, but for debugging it's fine, and running 100 process MPI program on your laptop is a very well good way of debugging a program.",
            "OK, my next question is.",
            "And then I'll start.",
            "Hopefully I'll start jumping.",
            "After initiating the MPI program with MPI Run by then for my MPI program, what does the call to MPI init do?",
            "Create the full parallel process is start program execution enabled for dependent programs separately to communicate with each other.",
            "Create the four parallel threads.",
            "So people can have a bash at what they think the answer is.",
            "I'll just wait a bit till this is working quite well actually.",
            "How far is it?",
            "So I could talk relatively straightforward.",
            "I can talk about that later.",
            "So.",
            "Once we get to 40 are probably OK. How do we do?",
            "OK so um.",
            "OK, so this is a common misconception.",
            "And I'll go doesn't start programming question, it doesn't create the full pallet, doesn't create the four parallel processes, it enables the four independent program to communicate with each other.",
            "This is not obvious, but it's true, and the of I've lost my glasses.",
            "Are they?",
            "I only started wearing glasses a year ago and I've still not got used to the fact that I'm blind.",
            "So if I go to the code.",
            "Which I've got somewhere.",
            "I'll use the C version for the sake of it.",
            "So this is the MPI program and I'll take the.",
            "Is that a bit too big or is that is a bit big, isn't it?",
            "It's a bit.",
            "It's a bit too.",
            "That's a bit better.",
            "Thank.",
            "Sorry.",
            "So if I.",
            "So this is an MPI program.",
            "There's a bunch of hash includes at the top left, so it's a bit.",
            "We have this is all just stuff this stuff.",
            "OK so you'll notice this print here.",
            "Is just about the first thing in the program OK?",
            "It's there OK?",
            "And the the reason I put the print statement there it's it's before MPI init OK?",
            "And if you run it, if I do MPI run minus N 2 dot slash pipe I can run.",
            "You'll see that.",
            "We get we get two people computing a problem.",
            "We get that print statement twice.",
            "OK, the the parallel processes are created by this MPI run command, the job launcher.",
            "They're not create there up.",
            "There are four programs running right from the start.",
            "For independent processes are already running.",
            "You type MPI, run the operating system.",
            "I create four.",
            "I run the same program 4 times \u03c0 parallel pipe parallel pipe parallel pipe parallel.",
            "They go off and they run OK.",
            "They're quite happy up there running.",
            "However.",
            "When you call MPI init, they go and talk to each other.",
            "Then they find out all this four of us OK and you're you #1 #2, #3 and #4.",
            "But the parallel processes are running right from the outset and you can see that you can actually if I go to the serial program.",
            "This pyserial dot C is just a very trivial code with no MPI in at all, OK. Like if I make it and run it.",
            "OK, very well I can run MPI run minus N 5.",
            "Of them OK, it just ran five times.",
            "It just ran the code 5 times.",
            "I never called MPI in it and they never talk to each other.",
            "But who cares?",
            "OK so all the MPI exact does.",
            "It runs multiple copies of a program if within that program you call MPI and it they will talk to each other and you will then be allowed to communicate with each other.",
            "But it.",
            "But MPI init does not create the parallel processes.",
            "While this is important to understand, is that.",
            "When we do open MP programming.",
            "The equivalent construction open MP, so I hate this font discoloring.",
            "An equivalent construct, an open MP.",
            "Is called a parallel region and an open MP.",
            "The parallel region does create parallel threads, so an open MP will get one print there and multiple prints here.",
            "But open MPI you have multiple processes running right from the outset.",
            "OK, so that is an important concept.",
            "The next question is OK Block.",
            "She won't do that.",
            "I won't do that one quite yet.",
            "I'll look at the.",
            "I'll just go through the pipe, the parallel code so.",
            "We'll come back to that.",
            "But basically what it does is they get together then OK.",
            "They've got together, they're all in there, all in the same room together.",
            "Then you need to know two things you need to know how many of there are you and who are you.",
            "And when you write MPI program you should.",
            "An MPI program should be written to run on any number of processes runtime.",
            "You do NPI run minus N 5.",
            "You run five of them.",
            "You have to inquire within the program how many of us are there and that is called MPI Comm size and MPI Comm rank.",
            "In MPI you can have communicated with groups of processes, so I could split you into two groups.",
            "The front of the room is one group in the back of the room is another group, and they're called communicators.",
            "In MPI, communicators are groups of processes, but by default.",
            "There's what there's a communication with everyone belongs to, which is MPI Comm world.",
            "So this is saying how many processes are there in a communicator?",
            "But that communicator is everybody, so it's saying how many processors are there is saying how many people are there in the room?",
            "OK, you can do more clever things and split communicators up into subgroups and then you would get a different number here.",
            "And this is saying who am I?",
            "OK, so if there's 50 of us you get back a number and that number is unique and it's it's zero, based from zero to end if I do.",
            "If I do run this, you'll see.",
            "We saw that we got hello from Rank Zero and hello from rank one.",
            "There were two processes and there's hello Frank O inhaler from rank one so it's zero.",
            "But even for Fortran programmers I'm sorry the first rank is zero.",
            "That's a killer.",
            "If you're a Fortran programmer, but it's true because the MPI libraries written in CI mean don't don't fool yourself.",
            "So that's what it does.",
            "So then you might worry well way to setups.",
            "While you dine, Eddie, why did I only get one print saying running in NPI?",
            "If you want to do something on one process, you have to do explicit.",
            "I just see if rank equals 0, so MPI programs are full of these kind of things.",
            "If rank equals zero and there's nothing magic about that statement, every process executes this statement.",
            "It's just that on only one processes rank equal to 0 and then the other thing is an MPI.",
            "There's nothing special about Rank zero to almost in almost all situations, all ranks are equal.",
            "The only important point about rank O, as we know that.",
            "Rank zero always exists, even if I only run the MPI program.",
            "One process there will always be a rank 0, so that's why conventionally people rank pick rank O as the boss.",
            "But it, and that's again different from MPI, Open MP and Open Open MP threads.",
            "Zeros are special thread.",
            "In MPI process there is not special process, it's just one of many.",
            "And then what we do is you have to do everything by hand in it MPI.",
            "I have this partial \u03c0.",
            "All my doing is I'm saying if I have N terms I picked N = 840 by default 'cause it has a lot of prime factors.",
            "I split the iterations based off of the blocks on four processors N oversizes.",
            "210 and then I have and I started and I stop and I started that plus one.",
            "I end.",
            "So all I've done is split the iteration space off into blocks.",
            "OK, there's other ways of doing it and.",
            "And then all I do is this is the SPMD model, single program, multiple data.",
            "Every process executes the same code OK, every processes independently at the same time.",
            "Executing this code.",
            "However we have arranged for this data.",
            "The I start on the I stop to be different on different processes, so we've arranged for processes to do different things by having different data.",
            "And here the data or the start and stop some people would write if rank equals 0 loop from I equals North to 200 F. Say Frank equals lump move from my equals 201.",
            "That will work, but it has two downsides.",
            "One is, you've written a program which is explicit for particular processes, and two it's very wasteful if you were to write that for a million processes, you have a very large file, so standard MPI programs.",
            "All have the same structure, but you mess around with loot limits and things like this based on the rank.",
            "And then what we're going to do here is now everybody has their own partial value of \u03c0.",
            "We're going to add it up.",
            "We're going to do something you shouldn't do in in reality, but as it is as a training exercise, it's useful if I'm rank zero, you're all going to send me your value, and I'm going to get them altogether.",
            "Add them up.",
            "OK, so in MPI the way you do it is we have to split into two groups.",
            "We have to say if I'm rank 0.",
            "Then I initialize Python, my partial value, and then I.",
            "Then I loop over everybody and say I want to receive from you receive from you received from you receive when you receive from you.",
            "So Rank zero has an explicit loop for source equals one source lesson size, source double plus I do receive into this variable of 1 double and this is a wild card I'm receiving from anybody.",
            "And I'll come back to this stuff, but this is just saying if I'm running on.",
            "100 processes I asked for 99 messages.",
            "And why does that work?",
            "Because one process is issuing 99 receives.",
            "Uh, 99 processes whoops.",
            "Are in the other branch of the F. And they're doing a send.",
            "They're sending their value of partial pie, which is 1 double to me.",
            "OK, so some people sometimes ask, why aren't there analysis tools which can tell me if my MPI program is correct?",
            "It's completely impossible to do this.",
            "Code only works because there's a different branch and one process executes one if branch and does 99 receives and 99 processes enter the other if grant in each do 1 cent.",
            "And remember it's based on the side if rank equals 0.",
            "Ranks just a variable that could have called it Fred or the Beyond.",
            "I mean there is no.",
            "It's almost impossible for static analysis tools to give you give you a statement about correctness of MPI programs and you just even almost trivial program.",
            "If I change anything here.",
            "If I change that to 1:00, it won't work.",
            "If I loop over not 99 receives, but 98 receives, it won't work.",
            "I mean MPI programs either work or if anything is wrong they fall over.",
            "OK, so I'm going to ask a few more questions.",
            "This is the receiving the send.",
            "At the 4th trial, just very briefly sorry.",
            "For Fortran FORTRAN, there really is very little difference between.",
            "C and fortran.",
            "For NPR, MPI, it's really just.",
            "Just.",
            "You call the functions because Fortran programmers don't like functions.",
            "They like subroutines, so for some reason we we called things and I've put in capital letters clam old-fashioned, but the code is effectively the same.",
            "It's very, very similar.",
            "Really no difference.",
            "So that works.",
            "So here's the question, OK?",
            "If you call MPI Recv and there's no incoming message, what happens?",
            "It's not obvious.",
            "Different message passing systems have taken different taking different approaches on this.",
            "Have a guess.",
            "I think what we're up to 40 will maybe have a look at the answer.",
            "Let's do it now.",
            "We've got a. OK, so you're right, the receive waits to the message drive, potentially waiting forever, which.",
            "Now this this is a perfectly rational answer.",
            "The received times after some system specific delay, some message passing systems used to do that, but MPI doesn't.",
            "MPI assumes.",
            "This within the if you're writing a message passing system for a cluster where you're worried about reliability, you're worried that that laptop might die.",
            "You have to always wonder.",
            "Well, maybe I've not got a received from that laptop 'cause that laptops he switched it off or something.",
            "We were playing some computer gaming, but MPI assumes that everything is up all the time.",
            "There was no fault tolerance in in the current version of MPI and so it says you've written a correct program.",
            "If there's no if I issue a receive, I will wait forever until it until it.",
            "Standing by the phone for a phone call till you starve to death is the somewhat sad analogy.",
            "Again, the receive report says no incoming message.",
            "There are ways of doing that with non blocking communications.",
            "You can in MPI ask is there a message?",
            "OK but I I meant standard receiver will come back to that.",
            "But again, the reason that the receive can't do that is there's no.",
            "Well, you can do it, but by default when you say you receive a message you don't know if the person you're receiving from his fast or slow.",
            "So you think there might be a bit slow.",
            "I'm expecting a message from them.",
            "Maybe they are a bit slow today.",
            "I better wait for it to come in and there are no timeouts in MPI so so people are doing well.",
            "The next question is.",
            "If you call MPI synchronous send.",
            "And there is no receive posted.",
            "OK, this is maybe.",
            "Easier, I don't know.",
            "If you know the answer, try clicking on another one, because there might be more than one answer, so I don't have to come over this question, but some of the few that are more than one answer.",
            "So try and click on more than one box.",
            "I think it will only allow you to click end boxes if there are any options.",
            "I didn't realize it would do that, but so, so please if you if you think you know the answer, click on another option and see if it moves and if it moves, go back.",
            "There's a submit when you click on an answer you click submit, don't you?",
            "Is there a click submit or is it just clicked out case you can play around?",
            "Yeah, OK, so we've got most people.",
            "How do we do?",
            "So.",
            "The send wait to receive it, potentially waiting forever.",
            "Again.",
            "This is the inverse of the of the receive.",
            "The sand doesn't fail, but there is a way of sending messages and piece of the message disappears.",
            "You can send a message in MPI and if there's no receipt, that is madness that you should never use that.",
            "OK, that kind of thing is used for video or audio processing.",
            "You know if you send a frame too, if you're running a computer game, when you send a frame to the to the graphics processor and the graphics processor is running slow, it will just ignore it go away.",
            "I'm running slow.",
            "I can't do another frame jitters for a bit, so there are systems where you want there are.",
            "There are real systems where if a message isn't received you wanted to disappear.",
            "It's like UDP versus TCP but not in MPI.",
            "The send doesn't fit.",
            "The send doesn't fail.",
            "It will wait until receives posted potential.",
            "Waiting forever.",
            "You can do this in MPI, but it's not synchronous.",
            "Send OK so that they're all.",
            "I mean, they're all none of these answers are stupid.",
            "You know they're all perfectly reasonable things to say.",
            "I'm just trying to say that MPI takes particular and I'll just do one more here.",
            "I think.",
            "What is it?",
            "This is if you call MPI asynchronous end, which is called the MPI B stand.",
            "In MPI, there's no receive posted.",
            "What happens?",
            "I think there might be more than what I can't remember if there's more than one answer here, so try and see if it.",
            "OK, so this looks like there's two answers here.",
            "You should be able to do this from a phone or something.",
            "I don't know why I've not used it before.",
            "We've got a colleague who used to be a teacher, as in the school and she introduced it to me.",
            "She said it was quite a useful thing to do.",
            "So I'll just wait a few more.",
            "It's a few more people.",
            "Have answered.",
            "OK, so that's maybe.",
            "OK, so so that's why we did very well.",
            "OK, so again the message doesn't disappear.",
            "Well actually, actually build the message does in some sense disappear, but I actually maybe of mean better there.",
            "The send doesn't fail, the send doesn't wait to receive it.",
            "Posted that synchronous sense.",
            "It's asynchronous.",
            "Synchronous senders like phoning somebody asynchronous tenders like sending an email or posting a letter so you don't need to wait to receive is posted.",
            "The message is stored and delivered later on if possible, so that's how that's how that's how asynchronous send works.",
            "If you think about it.",
            "I want MPI to an asynchronous sense.",
            "I want to be able to, say, send this data to somebody.",
            "I don't care, you know.",
            "So this is an array.",
            "This is where analogies break.",
            "But this is an array X. OK, I tell the MPI send the array X off to somebody OK. And because it's a synchronous, you don't know if or when it's received OK. What's the next thing you want to do?",
            "We've got an array.",
            "Storage is memory is valuable.",
            "Or or reuse it.",
            "Yeah you can't, so I don't know if and when this message is delivered, but to write a useful program I have to be able to reuse that memory.",
            "I can't reserve memory for every outstanding, so the only way that a message passing system could implement asynchronous ends is by copying and that's why MPI calls them be.",
            "Send B stands for buffered.",
            "This says MPI will take a copy of this message and buffer it and send it later.",
            "So if you think about it is impossible to implement asynchronous sense without copying, so both these are true.",
            "The messages stored and delivered later on, but the program continues execution regardless of whether messages received and what.",
            "I haven't put on here is even more importantly.",
            "You can reuse the storage.",
            "You can reuse that buffer.",
            "That you are a.",
            "You can overwrite it 'cause it's gone.",
            "OK so if you do if you do synchronous send when synchronous end completes, you can reuse the data because you know it's been transferred.",
            "When you do asynchronous sandwich in MPI is B said you can reuse the data 'cause you know it's been copied.",
            "So in MPI when when a message passing operation is complete you can overwrite the data.",
            "They might be able to overwrite it for different reasons, but you can overwrite the data OK.",
            "Non blocking communications which come back to bit more bit more subtle, but so I forgot anymore.",
            "OK, this is another one.",
            "The MPI Recv routine has a parameter count.",
            "So I if you look at the maybe just.",
            "Look at the.",
            "I'll go by the sea.",
            "I didn't receive.",
            "So I said I want to receive.",
            "From any source, I want to receive 1 double into this, so this this is one.",
            "OK, let's say receive 1 double well.",
            "OK so everyone sending 1 double and I'm receiving 1 double.",
            "What does that one mean?",
            "It's called account in in an MPI recv.",
            "It's called the count, so the MPI recv routine has account.",
            "What does this mean?",
            "So even if you're getting these questions right, it's making you think about about about how MPI works.",
            "So people are racing through this.",
            "That's pretty good.",
            "I'll go up when I get 40, then I'll.",
            "OK, so.",
            "NPI whoops.",
            "NPI tries not to talk about bites.",
            "NPI track because MPI is in principle can be implemented on heterogeneous architectures, there could in principle there could be 2 machines in the same MPI program which were on one machine and integer is 4 bytes, another machine integers 8 bytes.",
            "Now, in practice we don't do that, but in principle you can do it.",
            "So MPI like Fortran is have tries to be very pure and doesn't talk about bites.",
            "So on the.",
            "So C programmers love bites, they automate bites, but MPI counts in integers, reals, doubles, objects, so NPI tries.",
            "So it's not OK.",
            "It is not.",
            "The size of the incoming message in terms of integers.",
            "This is the most common one.",
            "The most common is the size available.",
            "Storing the message.",
            "What you're saying is I want to receive a message from you, and I have count.",
            "Storage to put it in.",
            "That's what you're saying.",
            "You don't specify anything about the size of the incoming message.",
            "OK, the receivers actually set it off, and in that case it was one I knew the message would be one.",
            "So OK, but but but in the MPI receivers say I want to receive a message from somebody and this is how much space I have to store it in, which is not necessarily the same as the incoming message.",
            "OK, Yep.",
            "Becausw for example.",
            "If you have.",
            "Well, it's not true in the car, but it in a molecular dynamics example which has particles basing around you might want to send all the particles which were within.",
            "1 centimeter of the boundary to your neighbor.",
            "You don't know how many that is, OK, I could send you a message saying about it, but you don't.",
            "So for regular problems, yes, at setup time regular domain decomposition you know everything but a lot of programs you don't.",
            "You save someone is going to send me some beta.",
            "I don't know how big it's going to be so you don't, so you would potentially reserve a lot of speed.",
            "Have some maximum size possibly and then and then.",
            "Yes, the message off for example.",
            "You might be sending data, but you might have a special message which says this is the last message I'm going to send.",
            "You might not want to send the data, I mean that starts, but the main thing is that you there are cases where you don't know how much data you're getting, 'cause you don't know how many points somebody is sending sending you.",
            "So the next question is, and this is all point point, send received.",
            "What happens if the incoming messages larger than count?",
            "So I said I want to receive a message from you.",
            "I've got 10 integers storage space for it, and the incoming messages 12 OK. What happens then?",
            "So you actually write most MPI programs, you'll see.",
            "That is, the size of the incoming message, but that's just because you've got a simple case where you know globally that these match up.",
            "Disrupt to quite a few.",
            "I'll wait till I get to 40 then we're doing really well, OK?",
            "So OK, so again it's not obvious.",
            "So the message beyond available storage MPI.",
            "Does try and be reasonably safe, so this is this is this is this is a mean?",
            "If you're if you're a security person, this is a buffer overflow.",
            "It's the kind of thing you should never allow, should never allow somebody to over write your own story.",
            "You never write, you never have a virus in an MPI program, heaven forbid, but so no, it doesn't do that because it doesn't want you to corrupt the data received.",
            "This is this is a perfectly reasonable assumption, but no, this is not true.",
            "They receive fails with an error.",
            "MPI is very rarely.",
            "Gives you useful error messages, but here it does.",
            "Here it says incoming it will say something like incoming message.",
            "Too large and it will report an error.",
            "What do you think MPI does when it kept an error if you say?",
            "I want to send to process 6 and there isn't a process 6.",
            "There's only five or you try and receive an incoming messages too large.",
            "It could cause the default error routine.",
            "What do you think that does?",
            "So yes, so I've been naughty here, but if I go back every MPI routine returns an error message.",
            "So in the seat in the Fortran.",
            "You'll see that.",
            "There's my receive in Fortran.",
            "I have to give and I are a variable.",
            "And this if this error is MPI success, then it was successful.",
            "Is this variable to anything other than MPI success, which is probably 0?",
            "But anyway there was an error in C. See being see Casillas hockey.",
            "Horrible language.",
            "You can define a routine.",
            "In CI should have said this.",
            "OK, I should have done that, but nobody does.",
            "OK, everyone does that.",
            "Why do they do that?",
            "What do you think MPI does on error?",
            "Well, no.",
            "It crashes, dumps core, every explodes completely at the slightest mistake.",
            "The slightest error.",
            "MPI just nukes.",
            "Everything falls over OK, so the default behavior for MPI is an error handler called MPI.",
            "Errors are fatal, which means every dies and dump score.",
            "So you can check the error variable but it never catch there 'cause it crashes.",
            "Now you can you can override the error handler to install your own error handler which is.",
            "Slightly more advanced, but by MPI program is a sloppy.",
            "They never checked the error routine because in standard configuration, MPI crashes and dies before it ever gets there.",
            "That's not true of 1 section of MPI, which is.",
            "MPI file IO.",
            "That is not true there.",
            "It gracefully handled errors there, but.",
            "So so.",
            "The correct answer is.",
            "The receive fails with an error and what an error means.",
            "This the whole program just dies and everything collapses and every dumps a huge core and you run out of disk space and it's just a nightmare.",
            "OK so but NPI when it fails it just crashes and burns, which is what you want.",
            "You've submitted a job to bridges.",
            "You've requested 1000 processors for 1000 hours.",
            "It's going to cost you 10s of thousands of dollars.",
            "You don't want it just to sit there and you know there was a little error there.",
            "I better wait, you know.",
            "If you want it just to die, so that means debugging is really hard.",
            "So what about this one?",
            "What happens if the incoming message of size N is smaller than count?",
            "So I said I want to, you know, I want to receive a message and I've got 10 integers, but I get four.",
            "OK, what happens there again?",
            "I said none of these answers are obvious.",
            "Different message passing systems have done different things, but MPI has made its choice.",
            "So I planned to take a coffee break at quarter past and then start again, maybe just about half past.",
            "So we're just about on time if.",
            "OK, so we'll see how we're doing.",
            "OK so um.",
            "The receive fails when there are no, it doesn't actually, so it doesn't receive.",
            "This would be if MPI was had graceful error messages.",
            "This is what it would do.",
            "It would say, or the income stream is too big and I didn't receive any what it actually does is the first 10 items are received, but it doesn't zero.",
            "The rest of the storage.",
            "So in MPI you know you can allow you to only overwrites the stuff that came in, so this is this is the correct answer.",
            "The first N items are received, which again is not completely obvious.",
            "But the rest of the storage is not zeroed.",
            "Um?",
            "Yeah, you could have defined it like that possibly.",
            "But it doesn't, it doesn't, it just leaves it untouched yet.",
            "So.",
            "Next question.",
            "How is the actual size of the incoming messages reported?",
            "OK, so that's the.",
            "So I should say I'm recording this.",
            "So it's possible it's possible, but very unlikely.",
            "Your audio appears on the on the OR just be my stupid voice on the audio, not your.",
            "Your voice is.",
            "So when it gets to 40 or maybe just.",
            "This is the last question so.",
            "So, um.",
            "Right, so the account receives updated.",
            "It's not now, so in fact, so it doesn't do that because you might, you know, that's something you don't want it fiddling with your data.",
            "MPI does know, and it's stored in the status parameter, so there's this thing.",
            "So again, these are perfectly reasonable.",
            "These are perfectly reasonable things to say.",
            "That would be the obvious thing, but that's not true.",
            "That you could do manually, but it's not by it's this one, and so if you look at the way I send receive works.",
            "If I go to the South look, I've done this.",
            "Receive OK.",
            "When I do a send where send.",
            "All I do when I give a send is.",
            "I say this is my data.",
            "But I'm sending it it's 1 double OK. And I specify a pointer just because you need it.",
            "Need a pointer in C. So I just say I am this is the data I want to send.",
            "So you just send the data.",
            "But but when you do receive so this, this MPI calls us to send buffer.",
            "So MPs and the send buffer count data type.",
            "I want to send this data and it's 1 double.",
            "OK that's the send buffer.",
            "So it sends side you specify the send buffer but on receive side you have to specify two separate storage spaces.",
            "You specify the receive buffer but you also specify status.",
            "So an MPI creates a message.",
            "It creates the data and some metadata like the envelope information OK. That's created for you when you receive the message, you receive the data into the receive buffer, but you have to receive the metadata separately.",
            "It's always the same size, so every MPI recv has to have a metadata which is MPI called status.",
            "In Fortran, it's a little array.",
            "In C, it's a little structure, and that's where the metadata goes, and for example.",
            "I didn't receive from any source OK.",
            "But I need to know who sent me the message because maybe I want to get back to you and say, well, here's some more work to do if you finished.",
            "Status dot MPI source is set to the to the source and in here is also the site of the incoming message.",
            "Now it's not stored directly in the status for technical reasons.",
            "You have to pass the status to a helper function called MPI get count.",
            "It's a bit horrible but it is, but the size of the messages stored in the status and the somewhat encrypted way, but that's where it's stored, so this stores where it came from, how long it was, what is tag was, which is some other data, benefits, or something else.",
            "Doesn't store the type.",
            "That doesn't make sense in MPI, but so so, so that's why I asked that question so I could have.",
            "I could have put the.",
            "So here I showed an example where I wildcarded on the the source and I gave, but again I could have asked for the size here because it would have been one, but in the general program it might not have been so as I said the what I hope to do here will show you that even with simple point to point messaging.",
            "Things aren't as you know, possibly as straightforward as you might have thought, so I hope you hope you find that useful.",
            "So what I'm going to do?",
            "In the next session is.",
            "Cover very briefly the different people.",
            "What MPI send does this is the biggest problem in MPI.",
            "The default send routine MPI send can be synchronous or asynchronous.",
            "You do not know which which is a bit of a.",
            "Problem, I'll try and explain why that is.",
            "I'll then talk about non blocking communications and then I'll introduce the traffic model so hopefully we'll get onto a reason amount of programming time.",
            "So if we could start again at.",
            "25 to 4 give you 15 minute break.",
            "Is that OK?",
            "Is that fine and start again?",
            "I hope you find that useful.",
            "I enjoyed doing the writing the questions.",
            "Anyway, the technology worked, which is amazing, so I'll see you back here in 15 minutes and so I'll do a brief talk and then we'll get onto the exercise."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just briefly, who am I?",
                    "label": 1
                },
                {
                    "sent": "My name is David Hentai.",
                    "label": 0
                },
                {
                    "sent": "I'm from EPCC which is the Edinburgh Parallel Computing Center in Scotland and my background is in theoretical particle physics computational, but that was along time ago.",
                    "label": 1
                },
                {
                    "sent": "I've been at UPC for over 20 years and I do a lot of training and EPC runs this the UK national supercomputer called Archer which we run and it's also a price system as well which is the link to this to this meeting.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's picture of Edinburgh.",
                    "label": 0
                },
                {
                    "sent": "It's very nice.",
                    "label": 0
                },
                {
                    "sent": "You want to visit.",
                    "label": 0
                },
                {
                    "sent": "Very nice place.",
                    "label": 0
                },
                {
                    "sent": "That's a picture of.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not sure this is, we run an MSE program or one year Masters and this is some of our students who who used this system were not using Archer, but I thought I just put.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Up there, So what I mean, what am I going to do?",
                    "label": 0
                },
                {
                    "sent": "I'm going to do an introduction to message passing program with MPI, and I'll tell you what I mean by an introduction and an introduction to share my program with open MP and then I'll do a bit of hybrid which is both MPI and open MP.",
                    "label": 1
                },
                {
                    "sent": "At the same time, the assumptions are that you've used MPI a bit before.",
                    "label": 1
                },
                {
                    "sent": "You have some knowledge of open MP.",
                    "label": 1
                },
                {
                    "sent": "Hopefully you've looked at the background material which I was sent round earlier on.",
                    "label": 0
                },
                {
                    "sent": "It's on the prerequisites.",
                    "label": 0
                },
                {
                    "sent": "But all my exercises are based around this parallel traffic model, which I'll introduce and talk about.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I'm just checking up at the right side, so I'm going to introduce this traffic model and that's what we'll use to do the exercises.",
                    "label": 0
                },
                {
                    "sent": "The slides are all done at the Xceed Wiki, so if I. Hopefully if you go too.",
                    "label": 0
                },
                {
                    "sent": "You just go to the Xceed Wiki.",
                    "label": 1
                },
                {
                    "sent": "Should I have a log onto?",
                    "label": 0
                },
                {
                    "sent": "Under presentations there's a link here presentations.",
                    "label": 0
                },
                {
                    "sent": "And then the MPI and open MP materials there.",
                    "label": 0
                },
                {
                    "sent": "So you can't see that link, sorry.",
                    "label": 0
                },
                {
                    "sent": "MPI and Open MP materials.",
                    "label": 0
                },
                {
                    "sent": "Should go to a page which has all this this stuff on it.",
                    "label": 0
                },
                {
                    "sent": "There is also.",
                    "label": 0
                },
                {
                    "sent": "So these are arranged in order that they are also on this page.",
                    "label": 0
                },
                {
                    "sent": "Test.",
                    "label": 0
                },
                {
                    "sent": "Tinyurl.com/I HPCSS dash MPI Dash Open MP that will just give you a bit.",
                    "label": 0
                },
                {
                    "sent": "They're all LinkedIn.",
                    "label": 0
                },
                {
                    "sent": "It just in case you have problem finding the wiki page so, but the slides aren't ordered here, it's just a dumping ground so but on the wiki there ordered, so maybe it makes a bit more sense, but just in case you that there's that.",
                    "label": 0
                },
                {
                    "sent": "They should all be there and that just goes straight through to her.",
                    "label": 0
                },
                {
                    "sent": "A dump of all this stuff.",
                    "label": 0
                },
                {
                    "sent": "But I would recommend that you get stuff from the from the xceed by by navigating the Xceed with its own people.",
                    "label": 0
                },
                {
                    "sent": "Managed to find that on the Xceed Wiki.",
                    "label": 0
                },
                {
                    "sent": "Is it there somebody managed to find it?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK.",
                    "label": 0
                },
                {
                    "sent": "So as I said, it's under.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, there it is right there is also at tinyurl.com.",
                    "label": 1
                },
                {
                    "sent": "There's some additional material.",
                    "label": 1
                },
                {
                    "sent": "He is mostly slides, but I've got a new instructions for running on bridges, bridges, crib sheet PDF, which is the top.",
                    "label": 0
                },
                {
                    "sent": "There's a small correction we're going to have to do to the exercises 'cause the way I've set them up.",
                    "label": 0
                },
                {
                    "sent": "You won't be direct, they've reserved a whole bunch of nodes on bridges for exclusive use, But the way I've set up the the back scripts they won't by default go there, so I'll have to.",
                    "label": 1
                },
                {
                    "sent": "We'll just do one quick correction there.",
                    "label": 0
                },
                {
                    "sent": "There's the exercise sheet, which is the traffic model sheet.",
                    "label": 0
                },
                {
                    "sent": "There's the codes I give you baseline code for the traffic model, but I'm going to also do a walkthrough of this more simple pie example, and this stuff isn't up there yet, but I will put up this stuff up that like.",
                    "label": 0
                },
                {
                    "sent": "Jaune talked about this chat list this this challenge this this hybrid challenge that will go up there as well.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the timetable.",
                    "label": 0
                },
                {
                    "sent": "Orbit behind already, but I'll give introduction recap.",
                    "label": 1
                },
                {
                    "sent": "I'll do a log on and walk through the pie example and hopefully you can download and run the PY example at the same time, so I'll be able to talk about it and you'll have the code, and that will just bring us up to speed, and then I'll do a brief some stuff which picks up from concepts here, communicators, times, and modes.",
                    "label": 1
                },
                {
                    "sent": "Then we'll have a break.",
                    "label": 0
                },
                {
                    "sent": "Maybe make this only 15 minutes.",
                    "label": 1
                },
                {
                    "sent": "Then I'll talk about non blocking communications.",
                    "label": 0
                },
                {
                    "sent": "Which is you might call asynchronous communication is technically an MPI there they're called non blocking this subtle distinction there.",
                    "label": 0
                },
                {
                    "sent": "And then we'll do the traffic model for the rest of the day.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And tomorrow well tomorrow.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tomorrow, as I said, it's a challenge to teach people with such a wide Verity experiences.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that I give you a working code.",
                    "label": 0
                },
                {
                    "sent": "The traffic model I give you at least the MPI version compiles and runs in parallel.",
                    "label": 0
                },
                {
                    "sent": "You can play around with it, but hopefully.",
                    "label": 0
                },
                {
                    "sent": "And you see the exercise sheet that lots of suggestions for things that you could do to work on it.",
                    "label": 0
                },
                {
                    "sent": "Based on what we've covered in the lectures.",
                    "label": 0
                },
                {
                    "sent": "So it's up to you what you do the it's a very, very simple model.",
                    "label": 0
                },
                {
                    "sent": "It might seem fairly trivial, but the parallelization of this model is identical to the HPC challenge.",
                    "label": 0
                },
                {
                    "sent": "Conceptually, the parallelization here is identical to parallelization in this this challenge code, we're going to give out, see how fast you can make it go.",
                    "label": 0
                },
                {
                    "sent": "So hopefully you can try things out in the in the traffic model in a much simpler way than you would when you work with the slightly more.",
                    "label": 0
                },
                {
                    "sent": "What looks more complex, but conceptually is not any more complex challenge code.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to cover whatever you want to know, so please let me know if the brakes or just you know I've got stacks of material which I can bring in and haven't got here.",
                    "label": 1
                },
                {
                    "sent": "But you know, if there's something you want to know about this, please talk to me because.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is, first of all, I'm going to do a quick recap.",
                    "label": 0
                },
                {
                    "sent": "Of.",
                    "label": 0
                },
                {
                    "sent": "MPI so this is.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stuff that I suspect you probably know.",
                    "label": 0
                },
                {
                    "sent": "But but but really, this is just to put things in context to cover.",
                    "label": 0
                },
                {
                    "sent": "Hopefully most of what I'd expected you to do as the prereading, but maybe bring you up to speed.",
                    "label": 0
                },
                {
                    "sent": "So message passing is parallel programming using processes.",
                    "label": 1
                },
                {
                    "sent": "What that means is that it allows us to write a parallel program which runs on a cluster allowed supercomputer.",
                    "label": 0
                },
                {
                    "sent": "So the best mental model you have a large supercomputer is all the laptops in this room, connected by cables.",
                    "label": 0
                },
                {
                    "sent": "OK, that that's what a large supercomputer is.",
                    "label": 0
                },
                {
                    "sent": "A large number of individual computers, each of which is a shared memory processor connected by some network, each running their own operating system.",
                    "label": 0
                },
                {
                    "sent": "So the world largest supercomputer in the world, conceptually conceptually.",
                    "label": 0
                },
                {
                    "sent": "Is the same as all the laptops in this room?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so I'll talk a bit about message passing parallelism, some practicalities here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I think that I'll come back to this analogy for when we talk about open MP, but my my analogy for MPI programming is 2 whiteboards in different single person offices.",
                    "label": 0
                },
                {
                    "sent": "The whiteboards are your memory, so there's a white one.",
                    "label": 0
                },
                {
                    "sent": "White Board is the memory in that laptop, and the other white board is the memory in that laptop is memory, 'cause you can read and write to it.",
                    "label": 0
                },
                {
                    "sent": "Two people are working on the same problem.",
                    "label": 1
                },
                {
                    "sent": "Which of the processes on different nodes attached by the interconnect?",
                    "label": 1
                },
                {
                    "sent": "How do they collaborate?",
                    "label": 0
                },
                {
                    "sent": "The only way you can collaborate so this person is in Sydney, Australia and you're here in Lubiana.",
                    "label": 0
                },
                {
                    "sent": "How do you collaborate?",
                    "label": 0
                },
                {
                    "sent": "You can't see each others whiteboards that computer can't read that computers memory.",
                    "label": 0
                },
                {
                    "sent": "It's impossible.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe the FBI can, but you can't.",
                    "label": 0
                },
                {
                    "sent": "You can't read the memory of other peoples computers, so the only way to collaborate is by explicit communication.",
                    "label": 0
                },
                {
                    "sent": "So you have to you have to where you want to collaborate.",
                    "label": 0
                },
                {
                    "sent": "You have to explicitly communicate with the person.",
                    "label": 0
                },
                {
                    "sent": "With by making a phone call or sending an email and an MPI, this is called message passing.",
                    "label": 0
                },
                {
                    "sent": "There is no shared data, will come back to this when we do open MP where you can have shared data share open MP corresponds to four people in the same office or writing to the same whiteboard.",
                    "label": 0
                },
                {
                    "sent": "You can share data you can say I just wrote it there go and read it.",
                    "label": 0
                },
                {
                    "sent": "You can't do that in MPI.",
                    "label": 0
                },
                {
                    "sent": "All your data is local, other processes are.",
                    "label": 0
                },
                {
                    "sent": "The MPI processes don't have access to it.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way it works is an MPI.",
                    "label": 0
                },
                {
                    "sent": "If I have two processes, process one and process too and I want data to go from process wonder process to the think of process one being running on that computer and process 2 running on that computer.",
                    "label": 0
                },
                {
                    "sent": "What I have to do is I have to explicitly do something so in NP.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, right, I might say a = 23 they want.",
                    "label": 0
                },
                {
                    "sent": "I want to communicate A to this other process will see that message.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Passing well, so in my memory here A is set to 23.",
                    "label": 0
                },
                {
                    "sent": "I have some magic routine.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Possibly called send which says I want to send this data to process too, so we'll see MPI provides you a bunch of library calls, just just library functions, which allows you to do something like this.",
                    "label": 0
                },
                {
                    "sent": "This has send the data in this in this abbreviated syntax to process too, and I want you to send a so that data is sent OK.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But that's not enough, OK.",
                    "label": 0
                },
                {
                    "sent": "So I said an analogy to message passing is sending an email.",
                    "label": 0
                },
                {
                    "sent": "If you send an email to somebody, have you communicated the information?",
                    "label": 0
                },
                {
                    "sent": "No, why not?",
                    "label": 0
                },
                {
                    "sent": "They haven't read it, so message passing is fundamentally 2 sided.",
                    "label": 0
                },
                {
                    "sent": "It needs the active the active participation of both ends so the sender has to actively send and this cannot go into this process's memory.",
                    "label": 0
                },
                {
                    "sent": "So I've I've imagined that you have an inbox like an email inbox or somewhere where the data is stored, but it's not in your memory.",
                    "label": 0
                },
                {
                    "sent": "The only way the data is transferred in standard message passing is the receiver has to actively caller receive so they receive.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have to say I want to receive data from process one and 'cause the receiver is total control.",
                    "label": 0
                },
                {
                    "sent": "The receiver decides where to put the message.",
                    "label": 0
                },
                {
                    "sent": "The sender can't tell you where to put the message.",
                    "label": 0
                },
                {
                    "sent": "You decide when you receive so that says.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Receive a message from from process one and put it into my variable B.",
                    "label": 0
                },
                {
                    "sent": "So be on process two, is now the same as a on process one 'cause you've done this and you've done a receive, but then process two can do.",
                    "label": 0
                },
                {
                    "sent": "Hey.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equals B + 1 OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Processes choose A is 24 and process once a is 23 and this is this is a.",
                    "label": 0
                },
                {
                    "sent": "An example of the SPMD programming model.",
                    "label": 0
                },
                {
                    "sent": "Single program multiple data.",
                    "label": 0
                },
                {
                    "sent": "The value right one program.",
                    "label": 0
                },
                {
                    "sent": "So the variable A exists on every MPI process, but it can have different values.",
                    "label": 0
                },
                {
                    "sent": "So in MPI and message passing you cannot ask what is the value of a.",
                    "label": 0
                },
                {
                    "sent": "It's a meaningless question.",
                    "label": 0
                },
                {
                    "sent": "You have to say what is the?",
                    "label": 0
                },
                {
                    "sent": "What is the value of a on process one?",
                    "label": 0
                },
                {
                    "sent": "What is the value of a on process 990?",
                    "label": 0
                },
                {
                    "sent": "Nine 1999?",
                    "label": 0
                },
                {
                    "sent": "Now they might be the same.",
                    "label": 0
                },
                {
                    "sent": "You might have arranged for them to be the same, but there by default they are not the same.",
                    "label": 0
                },
                {
                    "sent": "So so you think of process one running on that laptop.",
                    "label": 0
                },
                {
                    "sent": "Process to running on my laptop.",
                    "label": 0
                },
                {
                    "sent": "And then the question is what to send do OK?",
                    "label": 0
                },
                {
                    "sent": "OK, well we'll come back to that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So synchronization this will maybe make more sense later on, but synchronization is automatic and message passing a lot.",
                    "label": 1
                },
                {
                    "sent": "I'll try a lot of people get this wrong.",
                    "label": 0
                },
                {
                    "sent": "Messages do it for you.",
                    "label": 1
                },
                {
                    "sent": "What I mean is you never have to worry.",
                    "label": 0
                },
                {
                    "sent": "You might say well if process one is running on that computer and process to it running on that computer.",
                    "label": 0
                },
                {
                    "sent": "What happens with that process goes to sleep for an hour and this one keeps running or that once twice as fast as that one in message passing.",
                    "label": 0
                },
                {
                    "sent": "You don't care because whenever you communicate with each other the synchronization happens.",
                    "label": 1
                },
                {
                    "sent": "By default, so for example, when I make a phone call to somebody.",
                    "label": 0
                },
                {
                    "sent": "I wait for them to pick up five, come home early, and I make a phone call.",
                    "label": 1
                },
                {
                    "sent": "I just wait for them to come and pick up the synchronization.",
                    "label": 0
                },
                {
                    "sent": "Happens when you receive a phone call, you going well, you go and stand by the phone and wait for it to ring.",
                    "label": 0
                },
                {
                    "sent": "OK, so so it may not become obvious that we look at a bit more deeply, but message passing you don't have to worry about mutual synchronization of processes in the standard way of programming becausw.",
                    "label": 0
                },
                {
                    "sent": "The synchronization is done for a lot of people.",
                    "label": 0
                },
                {
                    "sent": "Get this wrong.",
                    "label": 0
                },
                {
                    "sent": "You see an awful lot of MPI programs which have barriers in them which are not necessary.",
                    "label": 1
                },
                {
                    "sent": "OK, so we'll come back to that and there's no danger corrupting someone elses data.",
                    "label": 0
                },
                {
                    "sent": "You never have to worry.",
                    "label": 0
                },
                {
                    "sent": "Oh no, he night I'm going to send a message to somebody, and that person might not be ready.",
                    "label": 0
                },
                {
                    "sent": "Doesn't matter that message cannot appear in that other processes memory until they actively quarter received, at which point they have decided what to do with it.",
                    "label": 0
                },
                {
                    "sent": "So you cannot.",
                    "label": 0
                },
                {
                    "sent": "You cannot corrupt someone elses data.",
                    "label": 0
                },
                {
                    "sent": "In message passing it is impossible, so the receive the way receive works is quite critical.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So actually communication.",
                    "label": 0
                },
                {
                    "sent": "Most sending a message could be either be synchronous or asynchronous.",
                    "label": 1
                },
                {
                    "sent": "Synchronous end is not completed.",
                    "label": 0
                },
                {
                    "sent": "All the messages started to be received.",
                    "label": 1
                },
                {
                    "sent": "An asynchronous send completes as soon as the message has gone, so this is I'll come back to this.",
                    "label": 0
                },
                {
                    "sent": "This is like making phone phone call or I got another example where if you phone somebody and they're not there, you wait for them to pick up OK, so then they pick up.",
                    "label": 0
                },
                {
                    "sent": "This is like sending an email or sending a letter.",
                    "label": 0
                },
                {
                    "sent": "You just stick it in the post you hit send and it disappears and you never know what happened to it.",
                    "label": 0
                },
                {
                    "sent": "OK, so they're conceptually two ways of sending a message at MPI.",
                    "label": 0
                },
                {
                    "sent": "We'll see you can do both received it, usually synchronous.",
                    "label": 0
                },
                {
                    "sent": "When you issue receive, you sit and you wait for a message to come in.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are other forms of that.",
                    "label": 0
                },
                {
                    "sent": "The standard receive when you say receive you just you say I want to receive a message and I will just.",
                    "label": 0
                },
                {
                    "sent": "I'm going to wait for it to come in.",
                    "label": 0
                },
                {
                    "sent": "So there.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Biology for a synchronous send.",
                    "label": 1
                },
                {
                    "sent": "Well, I actually think that the making a phone call is a better analogy, but you can think of maybe sending a fax that basically the important point about sending a fax as you go to the fax machine you put that you put the thing in.",
                    "label": 0
                },
                {
                    "sent": "It said at some point you probably don't use fax machines anymore.",
                    "label": 0
                },
                {
                    "sent": "She's update this, but you get a beep, but this beep is critical.",
                    "label": 0
                },
                {
                    "sent": "Beep says.",
                    "label": 0
                },
                {
                    "sent": "I know the data has been arrived has arrived or like making a phone call.",
                    "label": 0
                },
                {
                    "sent": "You know that somebody else has picked up the phone 'cause you're on the line together, so that's called synchronous.",
                    "label": 0
                },
                {
                    "sent": "Sending there's a synchronization in time async.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The stand is like posting natural setting an email.",
                    "label": 0
                },
                {
                    "sent": "You put the message in the in the letterbox.",
                    "label": 0
                },
                {
                    "sent": "The postman delivers the letter hopefully, but you have no information as to whether it got there or not and MPI allows you to do with St both synchronous and asynchronous send and I'll come back to but the most misunderstood the thing about MPI is that when you call MPI send, which is the send routine, you do not know if it will be synchronous or asynchronous.",
                    "label": 0
                },
                {
                    "sent": "And that has huge ramifications for writing correct codes, and I'll come back to that and I'll try and explain why they thought that was a good idea.",
                    "label": 0
                },
                {
                    "sent": "Probably is a good idea, but it causes endless confusion.",
                    "label": 0
                },
                {
                    "sent": "OK, MPI send the standard send could be either synchronous or asynchronous.",
                    "label": 1
                },
                {
                    "sent": "There's no guarantee which it does.",
                    "label": 0
                },
                {
                    "sent": "MPI could decide if it's a Wednesday.",
                    "label": 0
                },
                {
                    "sent": "I'll do it synchronously if it's a Thursday, I'll do it asynchronously 'cause it has other mechanisms for choosing that.",
                    "label": 0
                },
                {
                    "sent": "I may have a talk which I'll try and explain why that is.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we consider point to .1 Sender 1 receiver and it's like sending personal emails.",
                    "label": 1
                },
                {
                    "sent": "I'll I will go through it in a lot of detail here, but another important concept in MPI and message passing in general are collective communications.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "I I could deliver this this lecture through point to point communications every time I wanted to say the next line, I could whisper in your in your ear and whisper in your ear and whisper in your ear.",
                    "label": 0
                },
                {
                    "sent": "Go round case.",
                    "label": 0
                },
                {
                    "sent": "It would take me an hour to do the whole one line of a lecture that would work functionally, but clearly it's a crazy thing to do.",
                    "label": 0
                },
                {
                    "sent": "OK, so collective communications are a higher level, higher level of communication than then.",
                    "label": 1
                },
                {
                    "sent": "Point to point because they involve more than point to point communication.",
                    "label": 1
                },
                {
                    "sent": "Involves one Sender 1 receiver collective.",
                    "label": 0
                },
                {
                    "sent": "Communication involves a whole group.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a simple message communicate between two processes.",
                    "label": 1
                },
                {
                    "sent": "There are many instances where communication between groups is required.",
                    "label": 1
                },
                {
                    "sent": "They could be built from simple messages, but they could be implemented separately for efficiency and so the obvious one.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Did you get the moment is a broadcast I'm talking in your perhaps this thing.",
                    "label": 0
                },
                {
                    "sent": "It's a one to many, many communication, but again, this is commonly misunderstood.",
                    "label": 1
                },
                {
                    "sent": "In this example we are all in MPI speak.",
                    "label": 0
                },
                {
                    "sent": "We are all participating with the broadcast.",
                    "label": 0
                },
                {
                    "sent": "We are all actively doing a broadcast.",
                    "label": 0
                },
                {
                    "sent": "Within that we have different roles.",
                    "label": 0
                },
                {
                    "sent": "I'm the sender.",
                    "label": 0
                },
                {
                    "sent": "You are the receiver, but we all have to call the broadcast routine OK. Will come back you so so we are all actively participating in a broadcast and MPI program.",
                    "label": 0
                },
                {
                    "sent": "We all have to have called the broadcast routine within that one person sends and many people receive, but it's just one person calls the broadcast.",
                    "label": 0
                },
                {
                    "sent": "In MPI, it is impossible for me if I want to transmit data to you.",
                    "label": 0
                },
                {
                    "sent": "If I'm the only person doing something, then nothing can happen 'cause the only way data can go from one process to another is if one process is sending and the other one is actively receiving or in collective communications, it's done under the hood, but everyone needs to be calling the broadcast.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Broadcast said.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data, for example, we might and will.",
                    "label": 0
                },
                {
                    "sent": "I don't do this in the traffic model, but I use.",
                    "label": 0
                },
                {
                    "sent": "You might have data.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's just copy to everybody.",
                    "label": 0
                },
                {
                    "sent": "You often do this at.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Starter simulation we did the temperature or the pressure and tell everybody what that what what.",
                    "label": 0
                },
                {
                    "sent": "It is just just just just scattered around.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Skechers light broadcast.",
                    "label": 0
                },
                {
                    "sent": "But instead of sending the same data, Trevor, you chunk it up so I might have data Note 12345 and I I have a block of data and I want you to get the first block.",
                    "label": 0
                },
                {
                    "sent": "You get the second block, you get the third block, rather do it laboriously by hand.",
                    "label": 0
                },
                {
                    "sent": "There's the MPI routine that says scattered this data up, and it automatically takes an array and splits up into chunks.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is scattered.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To some.",
                    "label": 0
                },
                {
                    "sent": "Into some in some way, and we'll see the traffic model is a 1D cellular automaton.",
                    "label": 0
                },
                {
                    "sent": "We have a long Rd which might be 10,000 or 100,000 cells long.",
                    "label": 0
                },
                {
                    "sent": "What you'll see the program does is 1 process initializes the road and then it scatters the road to the road to the other processes.",
                    "label": 0
                },
                {
                    "sent": "So the road is split up into chunks and distributed at the end you might want.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To bring it back together, you might want to gather it.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Together again, they gather and scatter are inverses of each other.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Having said that, the most common operation you do in parallel programming.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a reduction operation.",
                    "label": 0
                },
                {
                    "sent": "So if you if you think back to John's model of four people collectively doing a weather forecast simulation on the United States, they each have different regions of the United States.",
                    "label": 0
                },
                {
                    "sent": "So each person can compute the rainfall over the Northwest, Northeast, Southwest, and southeast corner of the United States.",
                    "label": 0
                },
                {
                    "sent": "That's a meaningless figure.",
                    "label": 0
                },
                {
                    "sent": "Who cares what the rainfall is in the bottom left hand corner of the United States?",
                    "label": 0
                },
                {
                    "sent": "The only meaningful figure is the global rain form.",
                    "label": 0
                },
                {
                    "sent": "And So what you often do in message passing.",
                    "label": 0
                },
                {
                    "sent": "Is programs.",
                    "label": 0
                },
                {
                    "sent": "A process computes a number and then you have to you have to sum it up globally.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I wanted to compute average age OK, I need to sum up all the ages and then divide by the number of people and come up with one answer.",
                    "label": 0
                },
                {
                    "sent": "So there's multiple input data.",
                    "label": 0
                },
                {
                    "sent": "Your own ages, but there's only one output.",
                    "label": 0
                },
                {
                    "sent": "The average at MPI calls that a reduction operating it's reducing data, which is distributed into a single answer.",
                    "label": 0
                },
                {
                    "sent": "So you could have you could have data from several party.",
                    "label": 1
                },
                {
                    "sent": "Might want to strike or not.",
                    "label": 0
                },
                {
                    "sent": "Everyone has their own vote, but there's a majority outcome.",
                    "label": 0
                },
                {
                    "sent": "But in fact, adding adding in real scientific and technical programs, it's adding numbers together, which is the most common reduction operation, almost almost exclusively, and we'll see that in the traffic model.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So reduction from the global sum product, Max, Min etc from a bunch of.",
                    "label": 0
                },
                {
                    "sent": "So if I wanted to.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Add these together.",
                    "label": 0
                },
                {
                    "sent": "I could call a reduction operation.",
                    "label": 0
                },
                {
                    "sent": "It would add note 1234 and five together to give me 50K.",
                    "label": 0
                },
                {
                    "sent": "So that's that's really just a brief recap of sort of where the introductory material that I don't hopefully maybe got a chance to look at, took you up to.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions about any of that?",
                    "label": 0
                },
                {
                    "sent": "If it's familiar, anything that's unclear.",
                    "label": 0
                },
                {
                    "sent": "I don't have any, so.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hardware, the reason the MPI is popular message parts being popular for over 20 years is it naturally Maps to distributed memory.",
                    "label": 0
                },
                {
                    "sent": "So as I said, if a modern supercomputers, a bunch of individual computers running some operating system that you don't want to pay for to use Linux, 'cause it's free and use GCC 'cause they're all free.",
                    "label": 0
                },
                {
                    "sent": "OK, and and basically it's a very not the model and message passing is a model for parallel computing, a conceptual model, but it Maps very well onto the hardware because these individual processes running on different processors in different computers.",
                    "label": 0
                },
                {
                    "sent": "There will be some high-end supercomputer.",
                    "label": 0
                },
                {
                    "sent": "There will be a very fast interconnect between them, so sending messages is quick.",
                    "label": 1
                },
                {
                    "sent": "And So what you do is you run one process per processor core and messages go over the interconnect between the nodes.",
                    "label": 1
                },
                {
                    "sent": "And when I say no, a node is a single computer will see on bridges they have 28 cores, but that that might have four cores that might have four cores there physically distinct 'cause they're physically different machines.",
                    "label": 0
                },
                {
                    "sent": "And that also means they're running different operating well.",
                    "label": 0
                },
                {
                    "sent": "They're running different copies of the same operating system.",
                    "label": 0
                },
                {
                    "sent": "OK, they're both.",
                    "label": 0
                },
                {
                    "sent": "They both were running different operating systems, but but anyway, each node is a separate computer running its own operating system, so on bridges will be running ten 10s of thousands of thousands of copies of Linux.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With practicality, might save a wait a second.",
                    "label": 0
                },
                {
                    "sent": "That laptops got four cores that laptops got four cores.",
                    "label": 0
                },
                {
                    "sent": "There's an interconnect between them.",
                    "label": 1
                },
                {
                    "sent": "But isn't it crazy what, what?",
                    "label": 0
                },
                {
                    "sent": "How am I going to do this because?",
                    "label": 0
                },
                {
                    "sent": "MPI's model is is processes talking to each other, but we know in practice that four of the processes are on the same computer they're actually running under the same operating system on the same computer.",
                    "label": 0
                },
                {
                    "sent": "Isn't it crazy that those two processes send messages to each other?",
                    "label": 0
                },
                {
                    "sent": "When in fact it might seem crazy, but in most MPI situations we ignore the architecture.",
                    "label": 0
                },
                {
                    "sent": "We just imagine that these are eight individual processors, so we run four processes here, 4 processes here, and we just forget about the fact that actually these are on the same node and these are on the same node and that works.",
                    "label": 0
                },
                {
                    "sent": "You can you can go beyond that.",
                    "label": 0
                },
                {
                    "sent": "You can say, well, actually I'll only one one process here in one process here and then exploit this multiple.",
                    "label": 0
                },
                {
                    "sent": "This shared memory architecture.",
                    "label": 0
                },
                {
                    "sent": "Through something like threading through open MP and you can do that and that's what's called hybrid programming, but in practice you have to really think hard whether that's worthwhile, because this seemingly crazy model works really well.",
                    "label": 0
                },
                {
                    "sent": "It works really well becausw, anything, anything is only as fast as its slowest part.",
                    "label": 0
                },
                {
                    "sent": "The slowest part here is sending a message from here to here over the interconnect that slow OK when this guy talks to this guy.",
                    "label": 0
                },
                {
                    "sent": "It's quick.",
                    "label": 0
                },
                {
                    "sent": "OK, because they're just on the same node, it's just a memory copy.",
                    "label": 0
                },
                {
                    "sent": "So although it might seem crazy, it doesn't slow the program down because.",
                    "label": 0
                },
                {
                    "sent": "You still got this this this link here.",
                    "label": 0
                },
                {
                    "sent": "Now there are situations where you can argue that too naive, but most Yep.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "So that's a bit.",
                    "label": 0
                },
                {
                    "sent": "So the question is, does it make sense to do the reduction here and then the reduction here?",
                    "label": 0
                },
                {
                    "sent": "So in the old days, yes, with MPI was new and hadn't been optimized for shared memory systems.",
                    "label": 0
                },
                {
                    "sent": "It sometimes made sense to actually call a reduction here.",
                    "label": 0
                },
                {
                    "sent": "Reduction here and then a reduction across the mall.",
                    "label": 0
                },
                {
                    "sent": "But modern MPI implementations are very clever, so they know these four guys know.",
                    "label": 0
                },
                {
                    "sent": "So you say they can't share memory?",
                    "label": 0
                },
                {
                    "sent": "OK, of course they can.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's just you know the operating system to do anything.",
                    "label": 0
                },
                {
                    "sent": "So under the hood the operating system just.",
                    "label": 0
                },
                {
                    "sent": "Get to talk to each other directly.",
                    "label": 0
                },
                {
                    "sent": "You can't do it easily in MPI or not easily anyway, but yes.",
                    "label": 0
                },
                {
                    "sent": "So that's the kind of optimization which is, which is done for you.",
                    "label": 0
                },
                {
                    "sent": "So maybe 10 years ago that was worthwhile, but but not now.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Message between the processes on the same node.",
                    "label": 1
                },
                {
                    "sent": "Fast, but you might say there is one disadvantage to this, though if I have four MPI processes here and four here, each of these are setting.",
                    "label": 0
                },
                {
                    "sent": "We're getting lots of messages going over the network, getting lots of small messages, and if you don't have a very good network, that can be a problem, But that's that's that's really for later.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So um message passing with shared memory, we typically want one process per core.",
                    "label": 1
                },
                {
                    "sent": "We don't exploit the shared memory.",
                    "label": 1
                },
                {
                    "sent": "It's like finding your office mate.",
                    "label": 0
                },
                {
                    "sent": "OK, it's like being in the same office as somebody having a whiteboard, but saying, look, we split the whiteboard into.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk to you.",
                    "label": 0
                },
                {
                    "sent": "We were not.",
                    "label": 0
                },
                {
                    "sent": "I don't don't touch my whiteboard if we want to talk to each other, I'm going to phone your email, you OK?",
                    "label": 0
                },
                {
                    "sent": "Clearly it's not a very healthy situation, but in this amount, but it actually works surprisingly well.",
                    "label": 1
                },
                {
                    "sent": "And the other thing is message passing programs run by some special job launch and we'll come back to that.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So issues.",
                    "label": 0
                },
                {
                    "sent": "Sends and receives must match, so the danger of deadlock.",
                    "label": 1
                },
                {
                    "sent": "So the biggest problem in MPI is if you issue a send without a receive or receive without ascend then they don't match up, and that's very easy to do.",
                    "label": 0
                },
                {
                    "sent": "So MPI programs are quite hard to write, but typically they either run or stop completely.",
                    "label": 0
                },
                {
                    "sent": "You almost never have it relatively rare to have subtle at least in simple MPI programs.",
                    "label": 0
                },
                {
                    "sent": "It's relatively rare to have subtle bugs.",
                    "label": 0
                },
                {
                    "sent": "Bugs are normally.",
                    "label": 0
                },
                {
                    "sent": "Pretty castrator code just stops working.",
                    "label": 0
                },
                {
                    "sent": "It's possible to write very complicated programs, but actually most scientific programs have a have a relatively simple structure, and people ride.",
                    "label": 1
                },
                {
                    "sent": "Being more people are writing more complicated things now, but but a lot of scientific codes are very simple.",
                    "label": 0
                },
                {
                    "sent": "Comp structure often results in simple communications patterns, and often you can use a collective communication, so collective communications are often very there.",
                    "label": 0
                },
                {
                    "sent": "In MPI was designed for scientific and technical programming, so the collective communications in their other kind of ones that they expect people would want to do so.",
                    "label": 0
                },
                {
                    "sent": "There's a question, yeah?",
                    "label": 0
                },
                {
                    "sent": "Received so I'll come back to that.",
                    "label": 0
                },
                {
                    "sent": "One of the exercises to look at MPI send receive.",
                    "label": 0
                },
                {
                    "sent": "So using MPI send, MPI Recv is possibly incorrect.",
                    "label": 0
                },
                {
                    "sent": "But well, the other thing is you can download the NPC H implementation.",
                    "label": 0
                },
                {
                    "sent": "Look at MPI send receive.",
                    "label": 0
                },
                {
                    "sent": "I think it's just I send or receive.",
                    "label": 0
                },
                {
                    "sent": "Wait, I mean you know 3 lines.",
                    "label": 0
                },
                {
                    "sent": "But anyway look at it.",
                    "label": 0
                },
                {
                    "sent": "But again we could.",
                    "label": 0
                },
                {
                    "sent": "That's one of the things we can transform the exercises.",
                    "label": 0
                },
                {
                    "sent": "Actually to try that.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Summary messages the only form of communication or communications explicit you know if there isn't an MPI calling the program, it's it's when they're not communicating.",
                    "label": 0
                },
                {
                    "sent": "MPI processes.",
                    "label": 0
                },
                {
                    "sent": "Just programs running on your laptop, and it's important to know the operating system.",
                    "label": 0
                },
                {
                    "sent": "Linux does not know that their MPI programs you know this thing is, you know it's running fire Fox and email client.",
                    "label": 0
                },
                {
                    "sent": "A couple of MPI programs that the operating system, they're just processes the operating system is not optimized for message passing.",
                    "label": 0
                },
                {
                    "sent": "OK, we used Linux, 'cause it's free.",
                    "label": 0
                },
                {
                    "sent": "OK, it wasn't written for high performing.",
                    "label": 0
                },
                {
                    "sent": "It maybe originate from iPhone computer, but Linux is this huge, vast lumbering beast of an operating system, so on a lot of a lot of high performance supercomputers you spend most your time turning, turning all the bits off that you don't don't want in the next.",
                    "label": 0
                },
                {
                    "sent": "Both systems use the SPMD model called single program.",
                    "label": 1
                },
                {
                    "sent": "Multiple data or processes run exactly the same code.",
                    "label": 1
                },
                {
                    "sent": "You might say if all processes run the same code, why don't they do the same thing well?",
                    "label": 0
                },
                {
                    "sent": "Well, in MPI you have this magic thing called your rank, so once you know you run exactly the same code.",
                    "label": 0
                },
                {
                    "sent": "So you say, well, if you're running the same code, surely the same thing.",
                    "label": 0
                },
                {
                    "sent": "No, 'cause you have this magic variable which you and unique ID.",
                    "label": 0
                },
                {
                    "sent": "And once you know that you can do different things so you can say what's my ID OK if I'm ID 7 do X else do Y and communications is basic is point to point and collect communication in place more complicated parents off to the current many codes.",
                    "label": 0
                },
                {
                    "sent": "So what I want to say.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So message passing is a programming model, but John said it in his introductory talk.",
                    "label": 1
                },
                {
                    "sent": "MPI is really the only message passing library that anyone uses at the moment.",
                    "label": 0
                },
                {
                    "sent": "It's been dominant.",
                    "label": 0
                },
                {
                    "sent": "It's been around for 20 odd years.",
                    "label": 0
                },
                {
                    "sent": "It's been dominant for over 10 years.",
                    "label": 1
                },
                {
                    "sent": "The basic concept of relative straightforward.",
                    "label": 1
                },
                {
                    "sent": "You need to understand them and actually the message passing model can be surprisingly, surprisingly difficult.",
                    "label": 0
                },
                {
                    "sent": "So my example.",
                    "label": 0
                },
                {
                    "sent": "I mean, some people, some people find it natural and it's not a problem.",
                    "label": 0
                },
                {
                    "sent": "Some people have difficulty grasping it, but this code here, which is a perfectly reasonable bit of serial code.",
                    "label": 0
                },
                {
                    "sent": "OK fix is less than zero.",
                    "label": 0
                },
                {
                    "sent": "Print error, exit OK. That's fine, and cereal, but that's a very dangerous piece of code to write in a parallel program and message passing what, why?",
                    "label": 0
                },
                {
                    "sent": "Why is that one?",
                    "label": 0
                },
                {
                    "sent": "No, no, well no I don't.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So your ex is minus five.",
                    "label": 0
                },
                {
                    "sent": "My ex is 7.",
                    "label": 0
                },
                {
                    "sent": "OK, you quit OK, I don't know you've quit.",
                    "label": 0
                },
                {
                    "sent": "How do I know you've quit you dead?",
                    "label": 0
                },
                {
                    "sent": "All that happens is a while later I try and send your message and you and nothing.",
                    "label": 0
                },
                {
                    "sent": "I try and receive a message from you.",
                    "label": 0
                },
                {
                    "sent": "I think he's he's a bit quiet so you know.",
                    "label": 0
                },
                {
                    "sent": "Because messages are the only the only way of checking.",
                    "label": 0
                },
                {
                    "sent": "She say OK, what I'll do is if X is less than zero, I'll send him a message to tell him I'm quitting, but he has to actively receive it.",
                    "label": 0
                },
                {
                    "sent": "He knows he's got a message.",
                    "label": 0
                },
                {
                    "sent": "If he checks for it.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you get into this weird catch 22, there are ways round it, but that things like this.",
                    "label": 0
                },
                {
                    "sent": "Are are are subtly wrong in can cause problems in message passing programs.",
                    "label": 0
                },
                {
                    "sent": "So simple things can be quite difficult to implement.",
                    "label": 0
                },
                {
                    "sent": "So what I wanted to do is to start off with this pie example and if you go to the.",
                    "label": 0
                },
                {
                    "sent": "Survive.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And Job actually had this on this.",
                    "label": 0
                },
                {
                    "sent": "It's a very standard example.",
                    "label": 0
                },
                {
                    "sent": "Um approximations of value of Pi can be obtained from the following series, so this is just the integral of 1 / 1 + X ^2 is arctan, whatever, which is arctan.",
                    "label": 0
                },
                {
                    "sent": "One is powered by 4, doesn't matter how this works, but this is a very naive expansion.",
                    "label": 0
                },
                {
                    "sent": "Pie by 4.",
                    "label": 0
                },
                {
                    "sent": "Is this someone over and some might want to end 1 / 1 plus I'm as 1/2 / N ^2.",
                    "label": 0
                },
                {
                    "sent": "It's just drawing rectangles under the curve.",
                    "label": 0
                },
                {
                    "sent": "It's incredibly naive.",
                    "label": 0
                },
                {
                    "sent": "But this summation is independent, so if I wanted to sum from N = 1 to 1000, you could do one to 100.",
                    "label": 0
                },
                {
                    "sent": "You could do 101 to 200.",
                    "label": 0
                },
                {
                    "sent": "You could do 100 and 200 and 300 because split into 10 blocks.",
                    "label": 0
                },
                {
                    "sent": "We could all computer separate contribution to PIE and then we could add it up at the end.",
                    "label": 0
                },
                {
                    "sent": "OK, because addition is associative, at least mathematically.",
                    "label": 0
                },
                {
                    "sent": "Tradition doesn't matter, so we can split it up so we can so we can trivially paralyze this calculation.",
                    "label": 0
                },
                {
                    "sent": "So we'll use this as simple example for MPI open MP.",
                    "label": 1
                },
                {
                    "sent": "It's the traffic models are much better analogue of real codes, but this illustrates the basic concept.",
                    "label": 0
                },
                {
                    "sent": "So what I wanted you to do was to if you go to the sheet.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "Up, that's what I want to do it so I can't do.",
                    "label": 0
                },
                {
                    "sent": "If you go to the excitement here, the bridge is reference sheet.",
                    "label": 0
                },
                {
                    "sent": "Is is is is.",
                    "label": 0
                },
                {
                    "sent": "Basically I'm just going to check here.",
                    "label": 0
                },
                {
                    "sent": "You can run something on bridges.",
                    "label": 0
                },
                {
                    "sent": "So what this is really just saying.",
                    "label": 0
                },
                {
                    "sent": "Log on, obtain the source code which is actually I want you to the pie example.",
                    "label": 0
                },
                {
                    "sent": "For this you can also get the traffic example.",
                    "label": 0
                },
                {
                    "sent": "For this example we get the pie example.",
                    "label": 0
                },
                {
                    "sent": "You can grab it, you could unpack it, you can compile it, and then you should be able to run it.",
                    "label": 0
                },
                {
                    "sent": "And if you just follow these instructions, we should be running OK.",
                    "label": 0
                },
                {
                    "sent": "There are three ways of running.",
                    "label": 0
                },
                {
                    "sent": "One is submitting a batch job to the bridge is compute nodes one is running on the login nodes and one is interactive access to computers.",
                    "label": 0
                },
                {
                    "sent": "I I have specified the mall there but I didn't realize.",
                    "label": 0
                },
                {
                    "sent": "So what I'll do is I will.",
                    "label": 0
                },
                {
                    "sent": "I'll just live.",
                    "label": 0
                },
                {
                    "sent": "This is always dangerous, but I'll try and show you live on the screen.",
                    "label": 0
                },
                {
                    "sent": "What I want you to do, but hopefully you should be able to do the same thing.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yep, reservation.",
                    "label": 0
                },
                {
                    "sent": "He has batch.",
                    "label": 0
                },
                {
                    "sent": "OK. OKOK so well?",
                    "label": 0
                },
                {
                    "sent": "If So what I'll do is I'll just what you're so if I I I'm going to log into bridges.",
                    "label": 0
                },
                {
                    "sent": "That's that's just go.",
                    "label": 0
                },
                {
                    "sent": "Even without reservation, but.",
                    "label": 0
                },
                {
                    "sent": "So what you just ridges.psc.edu?",
                    "label": 0
                },
                {
                    "sent": "Dot Edu.",
                    "label": 0
                },
                {
                    "sent": "We are on.",
                    "label": 0
                },
                {
                    "sent": "We we W get that that that stuff.",
                    "label": 0
                },
                {
                    "sent": "We unpack it.",
                    "label": 0
                },
                {
                    "sent": "This is all in the sheet.",
                    "label": 0
                },
                {
                    "sent": "And if you see it's called pie example.",
                    "label": 0
                },
                {
                    "sent": "So it's made maybe not a very good name, but it's called pie example.",
                    "label": 0
                },
                {
                    "sent": "So if you go to pie example, you'll see that there's.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Serial code in C. And Fortran and parallel, so I would probably do the parallel MPI code only for do C or Fortran.",
                    "label": 0
                },
                {
                    "sent": "But if I do the seat the C1.",
                    "label": 0
                },
                {
                    "sent": "I should be able to compile it and I should be able to run it well.",
                    "label": 0
                },
                {
                    "sent": "What are filter settings?",
                    "label": 0
                },
                {
                    "sent": "Batch.",
                    "label": 0
                },
                {
                    "sent": "Everyone on the board here, so it's just double dash res equals H PCs, so I've given you.",
                    "label": 0
                },
                {
                    "sent": "You'll see in the sheet.",
                    "label": 0
                },
                {
                    "sent": "I'm really think we have 10 minutes to go through I think with MPI backstop job which you should just be able to use as a template to run any of your own programs.",
                    "label": 0
                },
                {
                    "sent": "What you need to add to that and I didn't have time to fix it.",
                    "label": 0
                },
                {
                    "sent": "OK but I would.",
                    "label": 0
                },
                {
                    "sent": "I would do this.",
                    "label": 0
                },
                {
                    "sent": "I'll leave this up.",
                    "label": 0
                },
                {
                    "sent": "You should do S tax.",
                    "label": 0
                },
                {
                    "sent": "What is it minus minus?",
                    "label": 0
                },
                {
                    "sent": "As.",
                    "label": 0
                },
                {
                    "sent": "No space.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I've.",
                    "label": 0
                },
                {
                    "sent": "I hate the font lock mode, so if you if you correct that in your MPI batch dot job, you will there be able to use that for everything and then you copy copy this.",
                    "label": 0
                },
                {
                    "sent": "So apologies, I didn't get a chance to put that in, but you just need to put minus minus hash S batch minus minus rate equals IHP CSS.",
                    "label": 0
                },
                {
                    "sent": "So I just but that's the apology.",
                    "label": 0
                },
                {
                    "sent": "You can have to make that change, but you should just fail to follow through the examples that sheet to get this pie example running.",
                    "label": 0
                },
                {
                    "sent": "So that was the first thing I wanted people to do to check that they could.",
                    "label": 0
                },
                {
                    "sent": "They could get it running.",
                    "label": 0
                },
                {
                    "sent": "So if anyone managed to other people did it before hand.",
                    "label": 0
                },
                {
                    "sent": "If you have any problems, so will now go around and just wanted to check everyone can compile and submit and run this parallel pie job, which is a real MPI program.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to go through it on the screen to try and try to illustrate that things aren't as simple, and MPO offers people.",
                    "label": 0
                },
                {
                    "sent": "People think, well, NPI.",
                    "label": 0
                },
                {
                    "sent": "The thing is, MPI is really simple to write.",
                    "label": 0
                },
                {
                    "sent": "Then if you think about it or why you think wait a second.",
                    "label": 0
                },
                {
                    "sent": "How does that work?",
                    "label": 0
                },
                {
                    "sent": "And then you understand how it works.",
                    "label": 0
                },
                {
                    "sent": "It becomes simple again, but it's that kind of two stage process.",
                    "label": 0
                },
                {
                    "sent": "So stick your hand up.",
                    "label": 0
                },
                {
                    "sent": "You have any problems or if I've made a typo, there's easily at probably a typo here.",
                    "label": 0
                },
                {
                    "sent": "So just to say you can track your jobs using SQ, but actually they finish really quickly and then there's a maybe a 10 second delay to your output.",
                    "label": 0
                },
                {
                    "sent": "There's this sort of void zone when your job has finished, but you don't have any output, but it's just a very small delay.",
                    "label": 0
                },
                {
                    "sent": "Then you'll see the dot out file so.",
                    "label": 0
                },
                {
                    "sent": "So you do need to make a copy of the batch of the batch job runs.",
                    "label": 0
                },
                {
                    "sent": "It's kind of the backdrop runs if the bank job is called Hello Dot Job.",
                    "label": 0
                },
                {
                    "sent": "It runs a job called Hello.",
                    "label": 0
                },
                {
                    "sent": "You need to.",
                    "label": 0
                },
                {
                    "sent": "You may need the copy, so did it work or?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So like the copy is significant descriptor generic script which runs a job of its own name.",
                    "label": 0
                },
                {
                    "sent": "So if you call the script Lubiana dot job, it will run at executing called Libyana.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do now is just.",
                    "label": 0
                },
                {
                    "sent": "Basically I wanted to walk through that code, but I actually had.",
                    "label": 0
                },
                {
                    "sent": "So what I'd like to do is.",
                    "label": 0
                },
                {
                    "sent": "If I've got a kind of a quick quiz here, this is really just for fun, but if people could go to www.sockrative.com.",
                    "label": 0
                },
                {
                    "sent": "So if I do.",
                    "label": 0
                },
                {
                    "sent": "If people go to www.socksocrativeaccountsbelsakrative.com.",
                    "label": 0
                },
                {
                    "sent": "So people would go to that website.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So just go to www.socrative.com K. Undo student login.",
                    "label": 0
                },
                {
                    "sent": "Then you just want to specify.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "This is the quiz KE0T3 YBU.",
                    "label": 0
                },
                {
                    "sent": "So when you do the student login, just give that give that.",
                    "label": 0
                },
                {
                    "sent": "Give that name.",
                    "label": 0
                },
                {
                    "sent": "And it will ask you for a name, but it does anything.",
                    "label": 0
                },
                {
                    "sent": "I don't care what you put in just doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "OK, So what you should get so you go to www.socrative.you.",
                    "label": 0
                },
                {
                    "sent": "I just I went to WWW.",
                    "label": 0
                },
                {
                    "sent": "Dot socrative.com.",
                    "label": 0
                },
                {
                    "sent": "Student login.",
                    "label": 0
                },
                {
                    "sent": "That's the room name and then you should get join room and you should get this question.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, so I'm just testing this out.",
                    "label": 0
                },
                {
                    "sent": "You should click all.",
                    "label": 0
                },
                {
                    "sent": "There's more than one answer.",
                    "label": 0
                },
                {
                    "sent": "Some of the questions have more than one answer, so click on all the apply so I don't know.",
                    "label": 0
                },
                {
                    "sent": "This is just a test question, but let's just see how we're getting on.",
                    "label": 0
                },
                {
                    "sent": "So we've got 21, so there's 28 people logged in and 21 of answered.",
                    "label": 0
                },
                {
                    "sent": "If people could keep logging in so it's WWW.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Go to www.socrative.com student login.",
                    "label": 0
                },
                {
                    "sent": "And the room name is.",
                    "label": 0
                },
                {
                    "sent": "That Kate Miller KE0T3 YBUK.",
                    "label": 0
                },
                {
                    "sent": "Kate.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've got 26.",
                    "label": 0
                },
                {
                    "sent": "I'll just wait a bit more while people answer and the Thunder goes on, so there's there's there's some of the questions have more than one right answer so.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "No, I mean there's a Oh yeah it might be.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "So that's about.",
                    "label": 0
                },
                {
                    "sent": "Let's see how did we do.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Everybody got it right, but I don't.",
                    "label": 0
                },
                {
                    "sent": "People didn't maybe use the multiple answers, So what I'm going to do now is I'm going to go to the next question.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this should.",
                    "label": 0
                },
                {
                    "sent": "This should update and you should get this question now to run at MPI program across special compiler special libraries, special parallel computer or special operating system so people could click all they think applies.",
                    "label": 0
                },
                {
                    "sent": "Again, it's all I'm just trying to get an idea of some of these questions are quite easy and some are not so easy at all.",
                    "label": 0
                },
                {
                    "sent": "If anyone still wanting to join, that's the.",
                    "label": 0
                },
                {
                    "sent": "That's the log.",
                    "label": 0
                },
                {
                    "sent": "That's the room this KE0T3 YBU.",
                    "label": 0
                },
                {
                    "sent": "So we're getting up some numbers, OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, right so OK, so this I've not used this system before 'cause there's so if there's.",
                    "label": 0
                },
                {
                    "sent": "If there's one answer only lets you set one if there's more than armed answer.",
                    "label": 0
                },
                {
                    "sent": "If there's any answers, I think it lets you set N, doesn't it?",
                    "label": 0
                },
                {
                    "sent": "I think that's how it works.",
                    "label": 0
                },
                {
                    "sent": "It's clever.",
                    "label": 0
                },
                {
                    "sent": "OK, sorry so.",
                    "label": 0
                },
                {
                    "sent": "Well, you can guess there's only one answer then if you OK.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so the system is is clever.",
                    "label": 0
                },
                {
                    "sent": "When I entered the question 'cause it notice there was only one answer, it it only let me set one option.",
                    "label": 0
                },
                {
                    "sent": "So I there must be some way around that but I've not.",
                    "label": 0
                },
                {
                    "sent": "So that's a mistake I should have.",
                    "label": 0
                },
                {
                    "sent": "So let's see how we did.",
                    "label": 0
                },
                {
                    "sent": "Got pretty good feedback, so I'll just do it.",
                    "label": 0
                },
                {
                    "sent": "How did we do?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So this is this is this is a non this is a non trivial question.",
                    "label": 0
                },
                {
                    "sent": "OK this is what I'm hoping to do in this.",
                    "label": 0
                },
                {
                    "sent": "In this course is really take fairly simple questions but tell you there so.",
                    "label": 0
                },
                {
                    "sent": "You need special libraries, that's correct.",
                    "label": 0
                },
                {
                    "sent": "You don't need special compilers, OK, MPI does not require special compilers.",
                    "label": 0
                },
                {
                    "sent": "The MPI compiler sorry.",
                    "label": 0
                },
                {
                    "sent": "The program you used to compile the compiler used to compile.",
                    "label": 0
                },
                {
                    "sent": "Your MPI program has no idea about MPI.",
                    "label": 0
                },
                {
                    "sent": "OK, when you call MPI send, it doesn't know it might be turning the screen pink or making your laptop beep.",
                    "label": 0
                },
                {
                    "sent": "It has no idea whatsoever.",
                    "label": 0
                },
                {
                    "sent": "Now that's not obvious because when you compile an MPI program, you type MP IF90 or MPI CC.",
                    "label": 0
                },
                {
                    "sent": "Sounds like a wonderful compiler.",
                    "label": 0
                },
                {
                    "sent": "All that is doing is calling a normal compiler, GCC or the Intel compiler and linking libraries and stuff so so the compiler knows zero about MPI, nothing.",
                    "label": 0
                },
                {
                    "sent": "It's just a library with some weird function calls.",
                    "label": 0
                },
                {
                    "sent": "You don't need a special parallel computer that, again, that's a perfectly reasonable thing to say, but you can run MPI programs on your laptop 'cause MPI is based on processes OK.",
                    "label": 0
                },
                {
                    "sent": "Processors have been around for decades.",
                    "label": 0
                },
                {
                    "sent": "OK, processes are not special.",
                    "label": 0
                },
                {
                    "sent": "For parallel computing, we just hijacked them 'cause they were there.",
                    "label": 0
                },
                {
                    "sent": "Any laptop can run hundreds of my laptops running hundreds of processes at the moment.",
                    "label": 0
                },
                {
                    "sent": "What's it doing?",
                    "label": 0
                },
                {
                    "sent": "I could pick up a.",
                    "label": 0
                },
                {
                    "sent": "What's my laptop doing at the moment?",
                    "label": 0
                },
                {
                    "sent": "OK, it's running loads of stuff.",
                    "label": 0
                },
                {
                    "sent": "You know what's it running now?",
                    "label": 0
                },
                {
                    "sent": "Root messed, it's running hundreds of processes.",
                    "label": 0
                },
                {
                    "sent": "OK, how many is running?",
                    "label": 0
                },
                {
                    "sent": "I can't see there also running hundreds and hundreds of processes.",
                    "label": 0
                },
                {
                    "sent": "OK, and all we do is some of them are MPI processes, but any computer can run a parallel program 'cause anymore any modern modern operating system can.",
                    "label": 0
                },
                {
                    "sent": "Can support multiple processes now there's no point for performance in running 100 MPI processes on your laptop, 'cause it won't have 100 processors.",
                    "label": 0
                },
                {
                    "sent": "But for debugging it's perfectly fine.",
                    "label": 0
                },
                {
                    "sent": "OK, so OK for performance, you'd never run more processes than you had processors or physical cores, but for debugging it's fine, and running 100 process MPI program on your laptop is a very well good way of debugging a program.",
                    "label": 0
                },
                {
                    "sent": "OK, my next question is.",
                    "label": 0
                },
                {
                    "sent": "And then I'll start.",
                    "label": 0
                },
                {
                    "sent": "Hopefully I'll start jumping.",
                    "label": 0
                },
                {
                    "sent": "After initiating the MPI program with MPI Run by then for my MPI program, what does the call to MPI init do?",
                    "label": 0
                },
                {
                    "sent": "Create the full parallel process is start program execution enabled for dependent programs separately to communicate with each other.",
                    "label": 0
                },
                {
                    "sent": "Create the four parallel threads.",
                    "label": 0
                },
                {
                    "sent": "So people can have a bash at what they think the answer is.",
                    "label": 0
                },
                {
                    "sent": "I'll just wait a bit till this is working quite well actually.",
                    "label": 0
                },
                {
                    "sent": "How far is it?",
                    "label": 0
                },
                {
                    "sent": "So I could talk relatively straightforward.",
                    "label": 0
                },
                {
                    "sent": "I can talk about that later.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Once we get to 40 are probably OK. How do we do?",
                    "label": 0
                },
                {
                    "sent": "OK so um.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a common misconception.",
                    "label": 0
                },
                {
                    "sent": "And I'll go doesn't start programming question, it doesn't create the full pallet, doesn't create the four parallel processes, it enables the four independent program to communicate with each other.",
                    "label": 0
                },
                {
                    "sent": "This is not obvious, but it's true, and the of I've lost my glasses.",
                    "label": 0
                },
                {
                    "sent": "Are they?",
                    "label": 0
                },
                {
                    "sent": "I only started wearing glasses a year ago and I've still not got used to the fact that I'm blind.",
                    "label": 0
                },
                {
                    "sent": "So if I go to the code.",
                    "label": 0
                },
                {
                    "sent": "Which I've got somewhere.",
                    "label": 0
                },
                {
                    "sent": "I'll use the C version for the sake of it.",
                    "label": 0
                },
                {
                    "sent": "So this is the MPI program and I'll take the.",
                    "label": 0
                },
                {
                    "sent": "Is that a bit too big or is that is a bit big, isn't it?",
                    "label": 0
                },
                {
                    "sent": "It's a bit.",
                    "label": 0
                },
                {
                    "sent": "It's a bit too.",
                    "label": 0
                },
                {
                    "sent": "That's a bit better.",
                    "label": 0
                },
                {
                    "sent": "Thank.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So if I.",
                    "label": 0
                },
                {
                    "sent": "So this is an MPI program.",
                    "label": 0
                },
                {
                    "sent": "There's a bunch of hash includes at the top left, so it's a bit.",
                    "label": 0
                },
                {
                    "sent": "We have this is all just stuff this stuff.",
                    "label": 0
                },
                {
                    "sent": "OK so you'll notice this print here.",
                    "label": 0
                },
                {
                    "sent": "Is just about the first thing in the program OK?",
                    "label": 0
                },
                {
                    "sent": "It's there OK?",
                    "label": 0
                },
                {
                    "sent": "And the the reason I put the print statement there it's it's before MPI init OK?",
                    "label": 0
                },
                {
                    "sent": "And if you run it, if I do MPI run minus N 2 dot slash pipe I can run.",
                    "label": 0
                },
                {
                    "sent": "You'll see that.",
                    "label": 0
                },
                {
                    "sent": "We get we get two people computing a problem.",
                    "label": 0
                },
                {
                    "sent": "We get that print statement twice.",
                    "label": 0
                },
                {
                    "sent": "OK, the the parallel processes are created by this MPI run command, the job launcher.",
                    "label": 0
                },
                {
                    "sent": "They're not create there up.",
                    "label": 0
                },
                {
                    "sent": "There are four programs running right from the start.",
                    "label": 0
                },
                {
                    "sent": "For independent processes are already running.",
                    "label": 0
                },
                {
                    "sent": "You type MPI, run the operating system.",
                    "label": 0
                },
                {
                    "sent": "I create four.",
                    "label": 0
                },
                {
                    "sent": "I run the same program 4 times \u03c0 parallel pipe parallel pipe parallel pipe parallel.",
                    "label": 0
                },
                {
                    "sent": "They go off and they run OK.",
                    "label": 0
                },
                {
                    "sent": "They're quite happy up there running.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "When you call MPI init, they go and talk to each other.",
                    "label": 0
                },
                {
                    "sent": "Then they find out all this four of us OK and you're you #1 #2, #3 and #4.",
                    "label": 0
                },
                {
                    "sent": "But the parallel processes are running right from the outset and you can see that you can actually if I go to the serial program.",
                    "label": 0
                },
                {
                    "sent": "This pyserial dot C is just a very trivial code with no MPI in at all, OK. Like if I make it and run it.",
                    "label": 0
                },
                {
                    "sent": "OK, very well I can run MPI run minus N 5.",
                    "label": 0
                },
                {
                    "sent": "Of them OK, it just ran five times.",
                    "label": 0
                },
                {
                    "sent": "It just ran the code 5 times.",
                    "label": 0
                },
                {
                    "sent": "I never called MPI in it and they never talk to each other.",
                    "label": 0
                },
                {
                    "sent": "But who cares?",
                    "label": 0
                },
                {
                    "sent": "OK so all the MPI exact does.",
                    "label": 0
                },
                {
                    "sent": "It runs multiple copies of a program if within that program you call MPI and it they will talk to each other and you will then be allowed to communicate with each other.",
                    "label": 0
                },
                {
                    "sent": "But it.",
                    "label": 0
                },
                {
                    "sent": "But MPI init does not create the parallel processes.",
                    "label": 0
                },
                {
                    "sent": "While this is important to understand, is that.",
                    "label": 0
                },
                {
                    "sent": "When we do open MP programming.",
                    "label": 0
                },
                {
                    "sent": "The equivalent construction open MP, so I hate this font discoloring.",
                    "label": 0
                },
                {
                    "sent": "An equivalent construct, an open MP.",
                    "label": 0
                },
                {
                    "sent": "Is called a parallel region and an open MP.",
                    "label": 0
                },
                {
                    "sent": "The parallel region does create parallel threads, so an open MP will get one print there and multiple prints here.",
                    "label": 0
                },
                {
                    "sent": "But open MPI you have multiple processes running right from the outset.",
                    "label": 0
                },
                {
                    "sent": "OK, so that is an important concept.",
                    "label": 0
                },
                {
                    "sent": "The next question is OK Block.",
                    "label": 0
                },
                {
                    "sent": "She won't do that.",
                    "label": 0
                },
                {
                    "sent": "I won't do that one quite yet.",
                    "label": 0
                },
                {
                    "sent": "I'll look at the.",
                    "label": 0
                },
                {
                    "sent": "I'll just go through the pipe, the parallel code so.",
                    "label": 0
                },
                {
                    "sent": "We'll come back to that.",
                    "label": 0
                },
                {
                    "sent": "But basically what it does is they get together then OK.",
                    "label": 0
                },
                {
                    "sent": "They've got together, they're all in there, all in the same room together.",
                    "label": 0
                },
                {
                    "sent": "Then you need to know two things you need to know how many of there are you and who are you.",
                    "label": 0
                },
                {
                    "sent": "And when you write MPI program you should.",
                    "label": 0
                },
                {
                    "sent": "An MPI program should be written to run on any number of processes runtime.",
                    "label": 0
                },
                {
                    "sent": "You do NPI run minus N 5.",
                    "label": 0
                },
                {
                    "sent": "You run five of them.",
                    "label": 0
                },
                {
                    "sent": "You have to inquire within the program how many of us are there and that is called MPI Comm size and MPI Comm rank.",
                    "label": 0
                },
                {
                    "sent": "In MPI you can have communicated with groups of processes, so I could split you into two groups.",
                    "label": 0
                },
                {
                    "sent": "The front of the room is one group in the back of the room is another group, and they're called communicators.",
                    "label": 0
                },
                {
                    "sent": "In MPI, communicators are groups of processes, but by default.",
                    "label": 0
                },
                {
                    "sent": "There's what there's a communication with everyone belongs to, which is MPI Comm world.",
                    "label": 0
                },
                {
                    "sent": "So this is saying how many processes are there in a communicator?",
                    "label": 0
                },
                {
                    "sent": "But that communicator is everybody, so it's saying how many processors are there is saying how many people are there in the room?",
                    "label": 0
                },
                {
                    "sent": "OK, you can do more clever things and split communicators up into subgroups and then you would get a different number here.",
                    "label": 0
                },
                {
                    "sent": "And this is saying who am I?",
                    "label": 0
                },
                {
                    "sent": "OK, so if there's 50 of us you get back a number and that number is unique and it's it's zero, based from zero to end if I do.",
                    "label": 0
                },
                {
                    "sent": "If I do run this, you'll see.",
                    "label": 0
                },
                {
                    "sent": "We saw that we got hello from Rank Zero and hello from rank one.",
                    "label": 0
                },
                {
                    "sent": "There were two processes and there's hello Frank O inhaler from rank one so it's zero.",
                    "label": 0
                },
                {
                    "sent": "But even for Fortran programmers I'm sorry the first rank is zero.",
                    "label": 0
                },
                {
                    "sent": "That's a killer.",
                    "label": 0
                },
                {
                    "sent": "If you're a Fortran programmer, but it's true because the MPI libraries written in CI mean don't don't fool yourself.",
                    "label": 0
                },
                {
                    "sent": "So that's what it does.",
                    "label": 0
                },
                {
                    "sent": "So then you might worry well way to setups.",
                    "label": 0
                },
                {
                    "sent": "While you dine, Eddie, why did I only get one print saying running in NPI?",
                    "label": 0
                },
                {
                    "sent": "If you want to do something on one process, you have to do explicit.",
                    "label": 0
                },
                {
                    "sent": "I just see if rank equals 0, so MPI programs are full of these kind of things.",
                    "label": 0
                },
                {
                    "sent": "If rank equals zero and there's nothing magic about that statement, every process executes this statement.",
                    "label": 0
                },
                {
                    "sent": "It's just that on only one processes rank equal to 0 and then the other thing is an MPI.",
                    "label": 0
                },
                {
                    "sent": "There's nothing special about Rank zero to almost in almost all situations, all ranks are equal.",
                    "label": 0
                },
                {
                    "sent": "The only important point about rank O, as we know that.",
                    "label": 0
                },
                {
                    "sent": "Rank zero always exists, even if I only run the MPI program.",
                    "label": 0
                },
                {
                    "sent": "One process there will always be a rank 0, so that's why conventionally people rank pick rank O as the boss.",
                    "label": 0
                },
                {
                    "sent": "But it, and that's again different from MPI, Open MP and Open Open MP threads.",
                    "label": 0
                },
                {
                    "sent": "Zeros are special thread.",
                    "label": 0
                },
                {
                    "sent": "In MPI process there is not special process, it's just one of many.",
                    "label": 0
                },
                {
                    "sent": "And then what we do is you have to do everything by hand in it MPI.",
                    "label": 0
                },
                {
                    "sent": "I have this partial \u03c0.",
                    "label": 0
                },
                {
                    "sent": "All my doing is I'm saying if I have N terms I picked N = 840 by default 'cause it has a lot of prime factors.",
                    "label": 0
                },
                {
                    "sent": "I split the iterations based off of the blocks on four processors N oversizes.",
                    "label": 0
                },
                {
                    "sent": "210 and then I have and I started and I stop and I started that plus one.",
                    "label": 0
                },
                {
                    "sent": "I end.",
                    "label": 0
                },
                {
                    "sent": "So all I've done is split the iteration space off into blocks.",
                    "label": 0
                },
                {
                    "sent": "OK, there's other ways of doing it and.",
                    "label": 0
                },
                {
                    "sent": "And then all I do is this is the SPMD model, single program, multiple data.",
                    "label": 0
                },
                {
                    "sent": "Every process executes the same code OK, every processes independently at the same time.",
                    "label": 0
                },
                {
                    "sent": "Executing this code.",
                    "label": 0
                },
                {
                    "sent": "However we have arranged for this data.",
                    "label": 0
                },
                {
                    "sent": "The I start on the I stop to be different on different processes, so we've arranged for processes to do different things by having different data.",
                    "label": 0
                },
                {
                    "sent": "And here the data or the start and stop some people would write if rank equals 0 loop from I equals North to 200 F. Say Frank equals lump move from my equals 201.",
                    "label": 0
                },
                {
                    "sent": "That will work, but it has two downsides.",
                    "label": 0
                },
                {
                    "sent": "One is, you've written a program which is explicit for particular processes, and two it's very wasteful if you were to write that for a million processes, you have a very large file, so standard MPI programs.",
                    "label": 0
                },
                {
                    "sent": "All have the same structure, but you mess around with loot limits and things like this based on the rank.",
                    "label": 0
                },
                {
                    "sent": "And then what we're going to do here is now everybody has their own partial value of \u03c0.",
                    "label": 0
                },
                {
                    "sent": "We're going to add it up.",
                    "label": 0
                },
                {
                    "sent": "We're going to do something you shouldn't do in in reality, but as it is as a training exercise, it's useful if I'm rank zero, you're all going to send me your value, and I'm going to get them altogether.",
                    "label": 0
                },
                {
                    "sent": "Add them up.",
                    "label": 0
                },
                {
                    "sent": "OK, so in MPI the way you do it is we have to split into two groups.",
                    "label": 0
                },
                {
                    "sent": "We have to say if I'm rank 0.",
                    "label": 0
                },
                {
                    "sent": "Then I initialize Python, my partial value, and then I.",
                    "label": 0
                },
                {
                    "sent": "Then I loop over everybody and say I want to receive from you receive from you received from you receive when you receive from you.",
                    "label": 0
                },
                {
                    "sent": "So Rank zero has an explicit loop for source equals one source lesson size, source double plus I do receive into this variable of 1 double and this is a wild card I'm receiving from anybody.",
                    "label": 0
                },
                {
                    "sent": "And I'll come back to this stuff, but this is just saying if I'm running on.",
                    "label": 0
                },
                {
                    "sent": "100 processes I asked for 99 messages.",
                    "label": 0
                },
                {
                    "sent": "And why does that work?",
                    "label": 0
                },
                {
                    "sent": "Because one process is issuing 99 receives.",
                    "label": 0
                },
                {
                    "sent": "Uh, 99 processes whoops.",
                    "label": 0
                },
                {
                    "sent": "Are in the other branch of the F. And they're doing a send.",
                    "label": 0
                },
                {
                    "sent": "They're sending their value of partial pie, which is 1 double to me.",
                    "label": 0
                },
                {
                    "sent": "OK, so some people sometimes ask, why aren't there analysis tools which can tell me if my MPI program is correct?",
                    "label": 0
                },
                {
                    "sent": "It's completely impossible to do this.",
                    "label": 0
                },
                {
                    "sent": "Code only works because there's a different branch and one process executes one if branch and does 99 receives and 99 processes enter the other if grant in each do 1 cent.",
                    "label": 0
                },
                {
                    "sent": "And remember it's based on the side if rank equals 0.",
                    "label": 0
                },
                {
                    "sent": "Ranks just a variable that could have called it Fred or the Beyond.",
                    "label": 0
                },
                {
                    "sent": "I mean there is no.",
                    "label": 0
                },
                {
                    "sent": "It's almost impossible for static analysis tools to give you give you a statement about correctness of MPI programs and you just even almost trivial program.",
                    "label": 0
                },
                {
                    "sent": "If I change anything here.",
                    "label": 0
                },
                {
                    "sent": "If I change that to 1:00, it won't work.",
                    "label": 0
                },
                {
                    "sent": "If I loop over not 99 receives, but 98 receives, it won't work.",
                    "label": 0
                },
                {
                    "sent": "I mean MPI programs either work or if anything is wrong they fall over.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to ask a few more questions.",
                    "label": 0
                },
                {
                    "sent": "This is the receiving the send.",
                    "label": 0
                },
                {
                    "sent": "At the 4th trial, just very briefly sorry.",
                    "label": 0
                },
                {
                    "sent": "For Fortran FORTRAN, there really is very little difference between.",
                    "label": 0
                },
                {
                    "sent": "C and fortran.",
                    "label": 0
                },
                {
                    "sent": "For NPR, MPI, it's really just.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "You call the functions because Fortran programmers don't like functions.",
                    "label": 0
                },
                {
                    "sent": "They like subroutines, so for some reason we we called things and I've put in capital letters clam old-fashioned, but the code is effectively the same.",
                    "label": 0
                },
                {
                    "sent": "It's very, very similar.",
                    "label": 0
                },
                {
                    "sent": "Really no difference.",
                    "label": 0
                },
                {
                    "sent": "So that works.",
                    "label": 0
                },
                {
                    "sent": "So here's the question, OK?",
                    "label": 0
                },
                {
                    "sent": "If you call MPI Recv and there's no incoming message, what happens?",
                    "label": 0
                },
                {
                    "sent": "It's not obvious.",
                    "label": 0
                },
                {
                    "sent": "Different message passing systems have taken different taking different approaches on this.",
                    "label": 0
                },
                {
                    "sent": "Have a guess.",
                    "label": 0
                },
                {
                    "sent": "I think what we're up to 40 will maybe have a look at the answer.",
                    "label": 0
                },
                {
                    "sent": "Let's do it now.",
                    "label": 0
                },
                {
                    "sent": "We've got a. OK, so you're right, the receive waits to the message drive, potentially waiting forever, which.",
                    "label": 0
                },
                {
                    "sent": "Now this this is a perfectly rational answer.",
                    "label": 0
                },
                {
                    "sent": "The received times after some system specific delay, some message passing systems used to do that, but MPI doesn't.",
                    "label": 0
                },
                {
                    "sent": "MPI assumes.",
                    "label": 0
                },
                {
                    "sent": "This within the if you're writing a message passing system for a cluster where you're worried about reliability, you're worried that that laptop might die.",
                    "label": 0
                },
                {
                    "sent": "You have to always wonder.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe I've not got a received from that laptop 'cause that laptops he switched it off or something.",
                    "label": 0
                },
                {
                    "sent": "We were playing some computer gaming, but MPI assumes that everything is up all the time.",
                    "label": 0
                },
                {
                    "sent": "There was no fault tolerance in in the current version of MPI and so it says you've written a correct program.",
                    "label": 0
                },
                {
                    "sent": "If there's no if I issue a receive, I will wait forever until it until it.",
                    "label": 0
                },
                {
                    "sent": "Standing by the phone for a phone call till you starve to death is the somewhat sad analogy.",
                    "label": 0
                },
                {
                    "sent": "Again, the receive report says no incoming message.",
                    "label": 0
                },
                {
                    "sent": "There are ways of doing that with non blocking communications.",
                    "label": 0
                },
                {
                    "sent": "You can in MPI ask is there a message?",
                    "label": 0
                },
                {
                    "sent": "OK but I I meant standard receiver will come back to that.",
                    "label": 0
                },
                {
                    "sent": "But again, the reason that the receive can't do that is there's no.",
                    "label": 0
                },
                {
                    "sent": "Well, you can do it, but by default when you say you receive a message you don't know if the person you're receiving from his fast or slow.",
                    "label": 0
                },
                {
                    "sent": "So you think there might be a bit slow.",
                    "label": 0
                },
                {
                    "sent": "I'm expecting a message from them.",
                    "label": 0
                },
                {
                    "sent": "Maybe they are a bit slow today.",
                    "label": 0
                },
                {
                    "sent": "I better wait for it to come in and there are no timeouts in MPI so so people are doing well.",
                    "label": 0
                },
                {
                    "sent": "The next question is.",
                    "label": 0
                },
                {
                    "sent": "If you call MPI synchronous send.",
                    "label": 0
                },
                {
                    "sent": "And there is no receive posted.",
                    "label": 0
                },
                {
                    "sent": "OK, this is maybe.",
                    "label": 0
                },
                {
                    "sent": "Easier, I don't know.",
                    "label": 0
                },
                {
                    "sent": "If you know the answer, try clicking on another one, because there might be more than one answer, so I don't have to come over this question, but some of the few that are more than one answer.",
                    "label": 0
                },
                {
                    "sent": "So try and click on more than one box.",
                    "label": 0
                },
                {
                    "sent": "I think it will only allow you to click end boxes if there are any options.",
                    "label": 0
                },
                {
                    "sent": "I didn't realize it would do that, but so, so please if you if you think you know the answer, click on another option and see if it moves and if it moves, go back.",
                    "label": 0
                },
                {
                    "sent": "There's a submit when you click on an answer you click submit, don't you?",
                    "label": 0
                },
                {
                    "sent": "Is there a click submit or is it just clicked out case you can play around?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so we've got most people.",
                    "label": 0
                },
                {
                    "sent": "How do we do?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The send wait to receive it, potentially waiting forever.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "This is the inverse of the of the receive.",
                    "label": 0
                },
                {
                    "sent": "The sand doesn't fail, but there is a way of sending messages and piece of the message disappears.",
                    "label": 0
                },
                {
                    "sent": "You can send a message in MPI and if there's no receipt, that is madness that you should never use that.",
                    "label": 0
                },
                {
                    "sent": "OK, that kind of thing is used for video or audio processing.",
                    "label": 0
                },
                {
                    "sent": "You know if you send a frame too, if you're running a computer game, when you send a frame to the to the graphics processor and the graphics processor is running slow, it will just ignore it go away.",
                    "label": 0
                },
                {
                    "sent": "I'm running slow.",
                    "label": 0
                },
                {
                    "sent": "I can't do another frame jitters for a bit, so there are systems where you want there are.",
                    "label": 0
                },
                {
                    "sent": "There are real systems where if a message isn't received you wanted to disappear.",
                    "label": 0
                },
                {
                    "sent": "It's like UDP versus TCP but not in MPI.",
                    "label": 0
                },
                {
                    "sent": "The send doesn't fit.",
                    "label": 0
                },
                {
                    "sent": "The send doesn't fail.",
                    "label": 0
                },
                {
                    "sent": "It will wait until receives posted potential.",
                    "label": 0
                },
                {
                    "sent": "Waiting forever.",
                    "label": 0
                },
                {
                    "sent": "You can do this in MPI, but it's not synchronous.",
                    "label": 0
                },
                {
                    "sent": "Send OK so that they're all.",
                    "label": 0
                },
                {
                    "sent": "I mean, they're all none of these answers are stupid.",
                    "label": 0
                },
                {
                    "sent": "You know they're all perfectly reasonable things to say.",
                    "label": 0
                },
                {
                    "sent": "I'm just trying to say that MPI takes particular and I'll just do one more here.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                },
                {
                    "sent": "This is if you call MPI asynchronous end, which is called the MPI B stand.",
                    "label": 0
                },
                {
                    "sent": "In MPI, there's no receive posted.",
                    "label": 0
                },
                {
                    "sent": "What happens?",
                    "label": 0
                },
                {
                    "sent": "I think there might be more than what I can't remember if there's more than one answer here, so try and see if it.",
                    "label": 0
                },
                {
                    "sent": "OK, so this looks like there's two answers here.",
                    "label": 0
                },
                {
                    "sent": "You should be able to do this from a phone or something.",
                    "label": 0
                },
                {
                    "sent": "I don't know why I've not used it before.",
                    "label": 0
                },
                {
                    "sent": "We've got a colleague who used to be a teacher, as in the school and she introduced it to me.",
                    "label": 0
                },
                {
                    "sent": "She said it was quite a useful thing to do.",
                    "label": 0
                },
                {
                    "sent": "So I'll just wait a few more.",
                    "label": 0
                },
                {
                    "sent": "It's a few more people.",
                    "label": 0
                },
                {
                    "sent": "Have answered.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's maybe.",
                    "label": 0
                },
                {
                    "sent": "OK, so so that's why we did very well.",
                    "label": 0
                },
                {
                    "sent": "OK, so again the message doesn't disappear.",
                    "label": 0
                },
                {
                    "sent": "Well actually, actually build the message does in some sense disappear, but I actually maybe of mean better there.",
                    "label": 0
                },
                {
                    "sent": "The send doesn't fail, the send doesn't wait to receive it.",
                    "label": 0
                },
                {
                    "sent": "Posted that synchronous sense.",
                    "label": 0
                },
                {
                    "sent": "It's asynchronous.",
                    "label": 0
                },
                {
                    "sent": "Synchronous senders like phoning somebody asynchronous tenders like sending an email or posting a letter so you don't need to wait to receive is posted.",
                    "label": 0
                },
                {
                    "sent": "The message is stored and delivered later on if possible, so that's how that's how that's how asynchronous send works.",
                    "label": 0
                },
                {
                    "sent": "If you think about it.",
                    "label": 0
                },
                {
                    "sent": "I want MPI to an asynchronous sense.",
                    "label": 0
                },
                {
                    "sent": "I want to be able to, say, send this data to somebody.",
                    "label": 0
                },
                {
                    "sent": "I don't care, you know.",
                    "label": 0
                },
                {
                    "sent": "So this is an array.",
                    "label": 0
                },
                {
                    "sent": "This is where analogies break.",
                    "label": 0
                },
                {
                    "sent": "But this is an array X. OK, I tell the MPI send the array X off to somebody OK. And because it's a synchronous, you don't know if or when it's received OK. What's the next thing you want to do?",
                    "label": 0
                },
                {
                    "sent": "We've got an array.",
                    "label": 0
                },
                {
                    "sent": "Storage is memory is valuable.",
                    "label": 0
                },
                {
                    "sent": "Or or reuse it.",
                    "label": 0
                },
                {
                    "sent": "Yeah you can't, so I don't know if and when this message is delivered, but to write a useful program I have to be able to reuse that memory.",
                    "label": 0
                },
                {
                    "sent": "I can't reserve memory for every outstanding, so the only way that a message passing system could implement asynchronous ends is by copying and that's why MPI calls them be.",
                    "label": 0
                },
                {
                    "sent": "Send B stands for buffered.",
                    "label": 0
                },
                {
                    "sent": "This says MPI will take a copy of this message and buffer it and send it later.",
                    "label": 0
                },
                {
                    "sent": "So if you think about it is impossible to implement asynchronous sense without copying, so both these are true.",
                    "label": 0
                },
                {
                    "sent": "The messages stored and delivered later on, but the program continues execution regardless of whether messages received and what.",
                    "label": 0
                },
                {
                    "sent": "I haven't put on here is even more importantly.",
                    "label": 0
                },
                {
                    "sent": "You can reuse the storage.",
                    "label": 0
                },
                {
                    "sent": "You can reuse that buffer.",
                    "label": 0
                },
                {
                    "sent": "That you are a.",
                    "label": 0
                },
                {
                    "sent": "You can overwrite it 'cause it's gone.",
                    "label": 0
                },
                {
                    "sent": "OK so if you do if you do synchronous send when synchronous end completes, you can reuse the data because you know it's been transferred.",
                    "label": 0
                },
                {
                    "sent": "When you do asynchronous sandwich in MPI is B said you can reuse the data 'cause you know it's been copied.",
                    "label": 0
                },
                {
                    "sent": "So in MPI when when a message passing operation is complete you can overwrite the data.",
                    "label": 0
                },
                {
                    "sent": "They might be able to overwrite it for different reasons, but you can overwrite the data OK.",
                    "label": 0
                },
                {
                    "sent": "Non blocking communications which come back to bit more bit more subtle, but so I forgot anymore.",
                    "label": 0
                },
                {
                    "sent": "OK, this is another one.",
                    "label": 0
                },
                {
                    "sent": "The MPI Recv routine has a parameter count.",
                    "label": 0
                },
                {
                    "sent": "So I if you look at the maybe just.",
                    "label": 0
                },
                {
                    "sent": "Look at the.",
                    "label": 0
                },
                {
                    "sent": "I'll go by the sea.",
                    "label": 0
                },
                {
                    "sent": "I didn't receive.",
                    "label": 0
                },
                {
                    "sent": "So I said I want to receive.",
                    "label": 0
                },
                {
                    "sent": "From any source, I want to receive 1 double into this, so this this is one.",
                    "label": 0
                },
                {
                    "sent": "OK, let's say receive 1 double well.",
                    "label": 0
                },
                {
                    "sent": "OK so everyone sending 1 double and I'm receiving 1 double.",
                    "label": 0
                },
                {
                    "sent": "What does that one mean?",
                    "label": 0
                },
                {
                    "sent": "It's called account in in an MPI recv.",
                    "label": 0
                },
                {
                    "sent": "It's called the count, so the MPI recv routine has account.",
                    "label": 0
                },
                {
                    "sent": "What does this mean?",
                    "label": 0
                },
                {
                    "sent": "So even if you're getting these questions right, it's making you think about about about how MPI works.",
                    "label": 0
                },
                {
                    "sent": "So people are racing through this.",
                    "label": 0
                },
                {
                    "sent": "That's pretty good.",
                    "label": 0
                },
                {
                    "sent": "I'll go up when I get 40, then I'll.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "NPI whoops.",
                    "label": 0
                },
                {
                    "sent": "NPI tries not to talk about bites.",
                    "label": 0
                },
                {
                    "sent": "NPI track because MPI is in principle can be implemented on heterogeneous architectures, there could in principle there could be 2 machines in the same MPI program which were on one machine and integer is 4 bytes, another machine integers 8 bytes.",
                    "label": 0
                },
                {
                    "sent": "Now, in practice we don't do that, but in principle you can do it.",
                    "label": 0
                },
                {
                    "sent": "So MPI like Fortran is have tries to be very pure and doesn't talk about bites.",
                    "label": 0
                },
                {
                    "sent": "So on the.",
                    "label": 0
                },
                {
                    "sent": "So C programmers love bites, they automate bites, but MPI counts in integers, reals, doubles, objects, so NPI tries.",
                    "label": 0
                },
                {
                    "sent": "So it's not OK.",
                    "label": 0
                },
                {
                    "sent": "It is not.",
                    "label": 0
                },
                {
                    "sent": "The size of the incoming message in terms of integers.",
                    "label": 0
                },
                {
                    "sent": "This is the most common one.",
                    "label": 0
                },
                {
                    "sent": "The most common is the size available.",
                    "label": 0
                },
                {
                    "sent": "Storing the message.",
                    "label": 0
                },
                {
                    "sent": "What you're saying is I want to receive a message from you, and I have count.",
                    "label": 0
                },
                {
                    "sent": "Storage to put it in.",
                    "label": 0
                },
                {
                    "sent": "That's what you're saying.",
                    "label": 0
                },
                {
                    "sent": "You don't specify anything about the size of the incoming message.",
                    "label": 0
                },
                {
                    "sent": "OK, the receivers actually set it off, and in that case it was one I knew the message would be one.",
                    "label": 0
                },
                {
                    "sent": "So OK, but but but in the MPI receivers say I want to receive a message from somebody and this is how much space I have to store it in, which is not necessarily the same as the incoming message.",
                    "label": 0
                },
                {
                    "sent": "OK, Yep.",
                    "label": 0
                },
                {
                    "sent": "Becausw for example.",
                    "label": 0
                },
                {
                    "sent": "If you have.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not true in the car, but it in a molecular dynamics example which has particles basing around you might want to send all the particles which were within.",
                    "label": 0
                },
                {
                    "sent": "1 centimeter of the boundary to your neighbor.",
                    "label": 0
                },
                {
                    "sent": "You don't know how many that is, OK, I could send you a message saying about it, but you don't.",
                    "label": 0
                },
                {
                    "sent": "So for regular problems, yes, at setup time regular domain decomposition you know everything but a lot of programs you don't.",
                    "label": 0
                },
                {
                    "sent": "You save someone is going to send me some beta.",
                    "label": 0
                },
                {
                    "sent": "I don't know how big it's going to be so you don't, so you would potentially reserve a lot of speed.",
                    "label": 0
                },
                {
                    "sent": "Have some maximum size possibly and then and then.",
                    "label": 0
                },
                {
                    "sent": "Yes, the message off for example.",
                    "label": 0
                },
                {
                    "sent": "You might be sending data, but you might have a special message which says this is the last message I'm going to send.",
                    "label": 0
                },
                {
                    "sent": "You might not want to send the data, I mean that starts, but the main thing is that you there are cases where you don't know how much data you're getting, 'cause you don't know how many points somebody is sending sending you.",
                    "label": 0
                },
                {
                    "sent": "So the next question is, and this is all point point, send received.",
                    "label": 0
                },
                {
                    "sent": "What happens if the incoming messages larger than count?",
                    "label": 0
                },
                {
                    "sent": "So I said I want to receive a message from you.",
                    "label": 0
                },
                {
                    "sent": "I've got 10 integers storage space for it, and the incoming messages 12 OK. What happens then?",
                    "label": 0
                },
                {
                    "sent": "So you actually write most MPI programs, you'll see.",
                    "label": 0
                },
                {
                    "sent": "That is, the size of the incoming message, but that's just because you've got a simple case where you know globally that these match up.",
                    "label": 0
                },
                {
                    "sent": "Disrupt to quite a few.",
                    "label": 0
                },
                {
                    "sent": "I'll wait till I get to 40 then we're doing really well, OK?",
                    "label": 0
                },
                {
                    "sent": "So OK, so again it's not obvious.",
                    "label": 0
                },
                {
                    "sent": "So the message beyond available storage MPI.",
                    "label": 0
                },
                {
                    "sent": "Does try and be reasonably safe, so this is this is this is this is a mean?",
                    "label": 0
                },
                {
                    "sent": "If you're if you're a security person, this is a buffer overflow.",
                    "label": 0
                },
                {
                    "sent": "It's the kind of thing you should never allow, should never allow somebody to over write your own story.",
                    "label": 0
                },
                {
                    "sent": "You never write, you never have a virus in an MPI program, heaven forbid, but so no, it doesn't do that because it doesn't want you to corrupt the data received.",
                    "label": 0
                },
                {
                    "sent": "This is this is a perfectly reasonable assumption, but no, this is not true.",
                    "label": 0
                },
                {
                    "sent": "They receive fails with an error.",
                    "label": 0
                },
                {
                    "sent": "MPI is very rarely.",
                    "label": 0
                },
                {
                    "sent": "Gives you useful error messages, but here it does.",
                    "label": 0
                },
                {
                    "sent": "Here it says incoming it will say something like incoming message.",
                    "label": 0
                },
                {
                    "sent": "Too large and it will report an error.",
                    "label": 0
                },
                {
                    "sent": "What do you think MPI does when it kept an error if you say?",
                    "label": 0
                },
                {
                    "sent": "I want to send to process 6 and there isn't a process 6.",
                    "label": 0
                },
                {
                    "sent": "There's only five or you try and receive an incoming messages too large.",
                    "label": 0
                },
                {
                    "sent": "It could cause the default error routine.",
                    "label": 0
                },
                {
                    "sent": "What do you think that does?",
                    "label": 0
                },
                {
                    "sent": "So yes, so I've been naughty here, but if I go back every MPI routine returns an error message.",
                    "label": 0
                },
                {
                    "sent": "So in the seat in the Fortran.",
                    "label": 0
                },
                {
                    "sent": "You'll see that.",
                    "label": 0
                },
                {
                    "sent": "There's my receive in Fortran.",
                    "label": 0
                },
                {
                    "sent": "I have to give and I are a variable.",
                    "label": 0
                },
                {
                    "sent": "And this if this error is MPI success, then it was successful.",
                    "label": 0
                },
                {
                    "sent": "Is this variable to anything other than MPI success, which is probably 0?",
                    "label": 0
                },
                {
                    "sent": "But anyway there was an error in C. See being see Casillas hockey.",
                    "label": 0
                },
                {
                    "sent": "Horrible language.",
                    "label": 0
                },
                {
                    "sent": "You can define a routine.",
                    "label": 0
                },
                {
                    "sent": "In CI should have said this.",
                    "label": 0
                },
                {
                    "sent": "OK, I should have done that, but nobody does.",
                    "label": 0
                },
                {
                    "sent": "OK, everyone does that.",
                    "label": 0
                },
                {
                    "sent": "Why do they do that?",
                    "label": 0
                },
                {
                    "sent": "What do you think MPI does on error?",
                    "label": 0
                },
                {
                    "sent": "Well, no.",
                    "label": 0
                },
                {
                    "sent": "It crashes, dumps core, every explodes completely at the slightest mistake.",
                    "label": 0
                },
                {
                    "sent": "The slightest error.",
                    "label": 0
                },
                {
                    "sent": "MPI just nukes.",
                    "label": 0
                },
                {
                    "sent": "Everything falls over OK, so the default behavior for MPI is an error handler called MPI.",
                    "label": 0
                },
                {
                    "sent": "Errors are fatal, which means every dies and dump score.",
                    "label": 0
                },
                {
                    "sent": "So you can check the error variable but it never catch there 'cause it crashes.",
                    "label": 0
                },
                {
                    "sent": "Now you can you can override the error handler to install your own error handler which is.",
                    "label": 0
                },
                {
                    "sent": "Slightly more advanced, but by MPI program is a sloppy.",
                    "label": 0
                },
                {
                    "sent": "They never checked the error routine because in standard configuration, MPI crashes and dies before it ever gets there.",
                    "label": 0
                },
                {
                    "sent": "That's not true of 1 section of MPI, which is.",
                    "label": 0
                },
                {
                    "sent": "MPI file IO.",
                    "label": 0
                },
                {
                    "sent": "That is not true there.",
                    "label": 0
                },
                {
                    "sent": "It gracefully handled errors there, but.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "The correct answer is.",
                    "label": 0
                },
                {
                    "sent": "The receive fails with an error and what an error means.",
                    "label": 0
                },
                {
                    "sent": "This the whole program just dies and everything collapses and every dumps a huge core and you run out of disk space and it's just a nightmare.",
                    "label": 0
                },
                {
                    "sent": "OK so but NPI when it fails it just crashes and burns, which is what you want.",
                    "label": 0
                },
                {
                    "sent": "You've submitted a job to bridges.",
                    "label": 0
                },
                {
                    "sent": "You've requested 1000 processors for 1000 hours.",
                    "label": 0
                },
                {
                    "sent": "It's going to cost you 10s of thousands of dollars.",
                    "label": 0
                },
                {
                    "sent": "You don't want it just to sit there and you know there was a little error there.",
                    "label": 0
                },
                {
                    "sent": "I better wait, you know.",
                    "label": 0
                },
                {
                    "sent": "If you want it just to die, so that means debugging is really hard.",
                    "label": 0
                },
                {
                    "sent": "So what about this one?",
                    "label": 0
                },
                {
                    "sent": "What happens if the incoming message of size N is smaller than count?",
                    "label": 0
                },
                {
                    "sent": "So I said I want to, you know, I want to receive a message and I've got 10 integers, but I get four.",
                    "label": 0
                },
                {
                    "sent": "OK, what happens there again?",
                    "label": 0
                },
                {
                    "sent": "I said none of these answers are obvious.",
                    "label": 0
                },
                {
                    "sent": "Different message passing systems have done different things, but MPI has made its choice.",
                    "label": 0
                },
                {
                    "sent": "So I planned to take a coffee break at quarter past and then start again, maybe just about half past.",
                    "label": 0
                },
                {
                    "sent": "So we're just about on time if.",
                    "label": 0
                },
                {
                    "sent": "OK, so we'll see how we're doing.",
                    "label": 0
                },
                {
                    "sent": "OK so um.",
                    "label": 0
                },
                {
                    "sent": "The receive fails when there are no, it doesn't actually, so it doesn't receive.",
                    "label": 0
                },
                {
                    "sent": "This would be if MPI was had graceful error messages.",
                    "label": 0
                },
                {
                    "sent": "This is what it would do.",
                    "label": 0
                },
                {
                    "sent": "It would say, or the income stream is too big and I didn't receive any what it actually does is the first 10 items are received, but it doesn't zero.",
                    "label": 0
                },
                {
                    "sent": "The rest of the storage.",
                    "label": 0
                },
                {
                    "sent": "So in MPI you know you can allow you to only overwrites the stuff that came in, so this is this is the correct answer.",
                    "label": 0
                },
                {
                    "sent": "The first N items are received, which again is not completely obvious.",
                    "label": 0
                },
                {
                    "sent": "But the rest of the storage is not zeroed.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, you could have defined it like that possibly.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't, it doesn't, it just leaves it untouched yet.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Next question.",
                    "label": 0
                },
                {
                    "sent": "How is the actual size of the incoming messages reported?",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the.",
                    "label": 0
                },
                {
                    "sent": "So I should say I'm recording this.",
                    "label": 0
                },
                {
                    "sent": "So it's possible it's possible, but very unlikely.",
                    "label": 0
                },
                {
                    "sent": "Your audio appears on the on the OR just be my stupid voice on the audio, not your.",
                    "label": 0
                },
                {
                    "sent": "Your voice is.",
                    "label": 0
                },
                {
                    "sent": "So when it gets to 40 or maybe just.",
                    "label": 0
                },
                {
                    "sent": "This is the last question so.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Right, so the account receives updated.",
                    "label": 0
                },
                {
                    "sent": "It's not now, so in fact, so it doesn't do that because you might, you know, that's something you don't want it fiddling with your data.",
                    "label": 0
                },
                {
                    "sent": "MPI does know, and it's stored in the status parameter, so there's this thing.",
                    "label": 0
                },
                {
                    "sent": "So again, these are perfectly reasonable.",
                    "label": 0
                },
                {
                    "sent": "These are perfectly reasonable things to say.",
                    "label": 0
                },
                {
                    "sent": "That would be the obvious thing, but that's not true.",
                    "label": 0
                },
                {
                    "sent": "That you could do manually, but it's not by it's this one, and so if you look at the way I send receive works.",
                    "label": 0
                },
                {
                    "sent": "If I go to the South look, I've done this.",
                    "label": 0
                },
                {
                    "sent": "Receive OK.",
                    "label": 0
                },
                {
                    "sent": "When I do a send where send.",
                    "label": 0
                },
                {
                    "sent": "All I do when I give a send is.",
                    "label": 0
                },
                {
                    "sent": "I say this is my data.",
                    "label": 0
                },
                {
                    "sent": "But I'm sending it it's 1 double OK. And I specify a pointer just because you need it.",
                    "label": 0
                },
                {
                    "sent": "Need a pointer in C. So I just say I am this is the data I want to send.",
                    "label": 0
                },
                {
                    "sent": "So you just send the data.",
                    "label": 0
                },
                {
                    "sent": "But but when you do receive so this, this MPI calls us to send buffer.",
                    "label": 0
                },
                {
                    "sent": "So MPs and the send buffer count data type.",
                    "label": 0
                },
                {
                    "sent": "I want to send this data and it's 1 double.",
                    "label": 0
                },
                {
                    "sent": "OK that's the send buffer.",
                    "label": 0
                },
                {
                    "sent": "So it sends side you specify the send buffer but on receive side you have to specify two separate storage spaces.",
                    "label": 0
                },
                {
                    "sent": "You specify the receive buffer but you also specify status.",
                    "label": 0
                },
                {
                    "sent": "So an MPI creates a message.",
                    "label": 0
                },
                {
                    "sent": "It creates the data and some metadata like the envelope information OK. That's created for you when you receive the message, you receive the data into the receive buffer, but you have to receive the metadata separately.",
                    "label": 0
                },
                {
                    "sent": "It's always the same size, so every MPI recv has to have a metadata which is MPI called status.",
                    "label": 0
                },
                {
                    "sent": "In Fortran, it's a little array.",
                    "label": 0
                },
                {
                    "sent": "In C, it's a little structure, and that's where the metadata goes, and for example.",
                    "label": 0
                },
                {
                    "sent": "I didn't receive from any source OK.",
                    "label": 0
                },
                {
                    "sent": "But I need to know who sent me the message because maybe I want to get back to you and say, well, here's some more work to do if you finished.",
                    "label": 0
                },
                {
                    "sent": "Status dot MPI source is set to the to the source and in here is also the site of the incoming message.",
                    "label": 0
                },
                {
                    "sent": "Now it's not stored directly in the status for technical reasons.",
                    "label": 0
                },
                {
                    "sent": "You have to pass the status to a helper function called MPI get count.",
                    "label": 0
                },
                {
                    "sent": "It's a bit horrible but it is, but the size of the messages stored in the status and the somewhat encrypted way, but that's where it's stored, so this stores where it came from, how long it was, what is tag was, which is some other data, benefits, or something else.",
                    "label": 0
                },
                {
                    "sent": "Doesn't store the type.",
                    "label": 0
                },
                {
                    "sent": "That doesn't make sense in MPI, but so so, so that's why I asked that question so I could have.",
                    "label": 0
                },
                {
                    "sent": "I could have put the.",
                    "label": 0
                },
                {
                    "sent": "So here I showed an example where I wildcarded on the the source and I gave, but again I could have asked for the size here because it would have been one, but in the general program it might not have been so as I said the what I hope to do here will show you that even with simple point to point messaging.",
                    "label": 0
                },
                {
                    "sent": "Things aren't as you know, possibly as straightforward as you might have thought, so I hope you hope you find that useful.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do?",
                    "label": 0
                },
                {
                    "sent": "In the next session is.",
                    "label": 0
                },
                {
                    "sent": "Cover very briefly the different people.",
                    "label": 0
                },
                {
                    "sent": "What MPI send does this is the biggest problem in MPI.",
                    "label": 0
                },
                {
                    "sent": "The default send routine MPI send can be synchronous or asynchronous.",
                    "label": 1
                },
                {
                    "sent": "You do not know which which is a bit of a.",
                    "label": 0
                },
                {
                    "sent": "Problem, I'll try and explain why that is.",
                    "label": 0
                },
                {
                    "sent": "I'll then talk about non blocking communications and then I'll introduce the traffic model so hopefully we'll get onto a reason amount of programming time.",
                    "label": 0
                },
                {
                    "sent": "So if we could start again at.",
                    "label": 0
                },
                {
                    "sent": "25 to 4 give you 15 minute break.",
                    "label": 0
                },
                {
                    "sent": "Is that OK?",
                    "label": 0
                },
                {
                    "sent": "Is that fine and start again?",
                    "label": 0
                },
                {
                    "sent": "I hope you find that useful.",
                    "label": 0
                },
                {
                    "sent": "I enjoyed doing the writing the questions.",
                    "label": 0
                },
                {
                    "sent": "Anyway, the technology worked, which is amazing, so I'll see you back here in 15 minutes and so I'll do a brief talk and then we'll get onto the exercise.",
                    "label": 0
                }
            ]
        }
    }
}