{
    "id": "tkce73j77eppp33nx7bgmk5ezll4as73",
    "title": "On-line learning competitive with reproducing kernel Hilbert spaces",
    "info": {
        "author": [
            "Vladimir Vovk, University of London"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "October 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/mcslw04_vovk_llcrk/",
    "segmentation": [
        [
            "With reproducing kernel Hilbert spaces OK."
        ],
        [
            "Thank you did.",
            "OK, so."
        ],
        [
            "So in this talk will be somewhat unusual in that I will not make the usual idea assumption.",
            "But as we know from Nicholas, you can deduce some results in this standard setting from this kind of results.",
            "So I'm interested in what is called prediction with expert advice.",
            "We have some pool of decisions, thread just, and our goal is to perform almost as well as the best.",
            "Strategy of the best strategies in our pool.",
            "And there are no assumptions at all about the way observations are generated, so I will explain how you can.",
            "Use what I call defensive forecasting for proving results of this kind.",
            "So my previous talk one year ago at this is analogous workshop.",
            "Last year was about defensive forecasting, so there will be some intersection.",
            "I hope not too big between this talk and last years talk.",
            "I know that the intersection is 1/3 between 0 dances at least potential audiences.",
            "So I will summarize briefly, at some point my previous talk, so.",
            "In this talk I will try something.",
            "Usually people talk about prediction.",
            "I will avoid.",
            "I try to avoid this word.",
            "Instead, I will talk about forecasting or decision-making.",
            "I will have two protocols, and both are more or less about prediction.",
            "But it might be.",
            "I will explain the difference between them.",
            "And philosophically they are very different, but it might be confusing at first.",
            "So to avoid confusion as much as possible, I will talk about forecasts or decisions.",
            "OK, so this is my decision making."
        ],
        [
            "Protocol, so the loss of decision making is also in the protocol.",
            "The time is discrete and it's round reality announces.",
            "Some XNXN can be some instance, for example.",
            "Decision makers then announces his decision Gamaran.",
            "Reality announces some observation.",
            "YN and the loss of the decision making is updated.",
            "So I have some loss function Lambda which measures then loss that decision Maker is his decision gamma and Surface is actual outcome as Y and so here I have Excel will be called data and.",
            "In fact, it's more convenient to include in X and all information that is relevant for prediction for predicting YN.",
            "So it can be in time series analysis.",
            "For example, it can be some previous observations or some measurements inside information, so I would like to view it slightly more generally than just predicting instances.",
            "So this is the protocol I'm really interested in."
        ],
        [
            "In this talk, so I want to compete against what I call decision rules.",
            "So you saw them in some cheese talks, so a decision rule is something that Maps the set of all possible data set of all possible Axis Capital X to the interval 01 support for each XI have.",
            "Some decision, so I want to compete against decision rules that are not too complicated.",
            "Not too strange, so let me give you some simple example just to illustrate the kind of results I'm interested in.",
            "So let me assume that Capital X is the interval 01."
        ],
        [
            "So relative announces some number in 01 and I want to predict the corresponding Y."
        ],
        [
            "OK, so let me define the way some way of measuring how complicated my decision rule is, so I will do it using the Sobolev norm.",
            "So so there are actually.",
            "So when people talk about Sobolev spaces, typically they mean to logical vector spaces without, so inside can be many norms that generates the same topological vector space.",
            "This is one of the convenient norms for that.",
            "So if you have some function F on the interval 01, you can square its average.",
            "The integral from zero to 1 plus the integral of X, ^2, ^2.",
            "Derivative if F is not absolutely continuous.",
            "If you cannot differentiate, or if this integral, for example is is infinite.",
            "I define this norm to be infinite.",
            "OK, so there's no, so some normal.",
            "Have"
        ],
        [
            "Functions will have functions that are not to be.",
            "It will have the snow which is not too large.",
            "The next slide I will say more about this norm, but let me state some simple proposition.",
            "So suppose X axis are taken from zero one and your loss function is absolute loss.",
            "You just measure the absolute deviation of the decision from the actual outcome.",
            "So decision making has strategy.",
            "Vet green teas that.",
            "So it in each round N he is average loss.",
            "So far his cumulative loss divided by N doesn't exceed the average loss of any decision rule.",
            "This is true for any decision rule D plus something that depends on the Sobolev norm of two D -- 1.",
            "So D takes values between zero and one and two D -- 1 is just some you sent our decision rule so that it takes values between minus one.",
            "And one so that zero becomes the new true value for it.",
            "So we take the Sobolev norm of two D -- 1 + 1 in divided by the square root of N. So here you can see this standard convergence rate.",
            "One of the square root of N. But here it has to be multiplied by this subliminal.",
            "So I'm talking about decision making here, but it's important that intuitively this kind of results are about small decision makers, so it's just using kacian.",
            "It's formally, there are no restrictions here.",
            "You don't assume anything about environment, but if you look what it means, so, so here we have the loss of the decision-making.",
            "He had the loss of the decision rule, and we measure them on the same sequence of observations.",
            "Why end and here why?",
            "In here, why end it if he imagines that reality pay some attention to what we're doing.",
            "It's possible that if we take decisions gamma and sensationalized observations will be different from when we take decisions D of XL.",
            "So it's very artificial if we are not small, it's official to prove this kind of results.",
            "So this isn't make it has to be understood in this sense, yes.",
            "This is the.",
            "Trade like cover.",
            "I would like to take an adult, yes.",
            "And then run as usual, pondering.",
            "For a finite number of the decisions.",
            "The center of the colors we get there.",
            "Although it's you.",
            "You cannot cover it.",
            "This is not a compact space.",
            "30 What you do?",
            "Eat.",
            "The.",
            "Oscar.",
            "But it's.",
            "Love.",
            "Yes, but it's not locally compact, it's.",
            "In other words, you can cover it.",
            "Scale.",
            "But it's it's closed.",
            "So it's a closed bowl, so it's if it's pre compacted.",
            "It must be compact.",
            "OK, so.",
            "Philly.",
            "Yes, this is some system.",
            "If it's locally compact then it must be."
        ],
        [
            "OK, so let's see what this norm means.",
            "So so so if in this sample protocol, when we're predicting observations are in 01, the Sobolev norm can be bounded by the absolute value of this mean value plus the square root of the new value of the squared derivative.",
            "So it sounds like 1 plus the mean slope.",
            "Of this function F where the mean slope is taken in this quadratic sense.",
            "So if the mean slope is is much less than the square root of the number of trials, so you can see that the bound on the previous slide.",
            "It shows that you perform almost as well as this decision rule, for which this means slope is not big.",
            "So you can think about piecewise linear functions.",
            "For example.",
            "OK, so some labels talk about as.",
            "X spaces of date, norms, and loss functions, but this is my motivating example, so I will explain how to prove this kind of results, and I think the proof technique is very interesting.",
            "Probably it's even more interesting than the result itself.",
            "OK, so is there a similar result in the leadership?"
        ],
        [
            "For example, in the paper by Nicola Conconi in Gentilly.",
            "The result is obtained for specific loss functions for decision making and the decision rule, and as far as I could see, the loss functions are different for this.",
            "For the decision, make it, we're just counting his mistakes.",
            "One case, and for the decision rules we the user hinge loss, at least in this particular paper that I saw.",
            "In there is a paper by Phil Long and Kinburn where it is assumed there is a perfect decision rule.",
            "Maybe, which is.",
            "Of course this is throwing assumptions about reality.",
            "In this talk there are no assumptions at all, but as results they tend to be about finite dimensional pools of experts.",
            "So here's a pool of possible decision strategies.",
            "It's it's huge, it's a sublease space.",
            "So it's it's very different kind of results, and the approach described here is inspired by a paper by first and water and many people were developing this approach, but there are no formal connections.",
            "I think there's a ton of connections left.",
            "In this work and in there between this work and their work.",
            "OK, so the plan for the rest of this talk.",
            "So the."
        ],
        [
            "Three items they intersect is my talk year ago, so I will explain that there are two approaches to for probably to probability to the foundations of probability game, theoretical measures theoretic and give one example.",
            "Game theoretic and measure theoretic law of large numbers.",
            "Strong law of large numbers.",
            "Then I will explain how how it's possible to make any.",
            "Continues game theoretic low probability into some forecasting strategy.",
            "It should have to implement it.",
            "Have to obtain from the weak law of large numbers what I call the algorithm of large numbers and then the algorithm of large numbers can be stated in reproducing kernel Hilbert spaces.",
            "And so theoretically it has very appealing properties, calibration resolution, but I will state what I need formally, but these properties they can look somewhat abstract.",
            "It might not be immediately clear why these properties are good statisticians like them, and it's intuitively it's clear they good properties, but it turns out that they can also be used for decision making.",
            "So once you you can use some probabilities for making.",
            "Good decisions, it looks like some ultimate justification of of probabilities, so this is some I like this justification better, so I will start from these two approach."
        ],
        [
            "Just as the foundations of probability so, so these two approaches are very old, both of them, but one of them is standard.",
            "It's approach based on measure theory, so it's usually called kolmogorov's axioms of probability, and the approach based on gambling.",
            "It was started by furnaces, but it's much less popular now, and it seems the crucial step here was made by Jonville, whom you place for me.",
            "This is collectives with.",
            "Martin Gales, so I will trade the difference using the simplest martingales, strong lofts, large numbers.",
            "So they have some random variables Y Y1Y2 and so on.",
            "And if we know the probability is the probability measure generating these observations the best prediction?",
            "Probably if this is all our information is predict says that the probability of YN is its conditional expectation given.",
            "The key is observations and maybe given some previous data.",
            "If you have something, but here I assume that this is all information we have, so we should condition on the past.",
            "So the coding for the strong law of large numbers.",
            "The average deviation of the actual observations from the forecasts tends to zero with probability one, so you can see.",
            "In measure here, this happens with probability one.",
            "It's a magic theoretic statement, but we can do it game theoretically as well."
        ],
        [
            "Now you can see protocol which is very different from the protocol you saw on the second slide, I think.",
            "So it's also about forecasting, but the goal here is completely different.",
            "For that protocol we didn't care about truth at all.",
            "We have a goal was to minimize the losses.",
            "Here we kind of claims that our forecasts are the best possible.",
            "So the protocol is this forecasting protocol, so we start from some initial Capital One and our game is forecast, announces some forecast, so he announces some probability.",
            "So intuitively this is the probability that YN will be one.",
            "But now he's serious about it.",
            "He he is willing to sell tickets, which will pay YN and his prices.",
            "PN for for such tickets.",
            "Now skeptic somebody who maybe doesn't believe the forecast, announces some numbers and how many tickets he wants to buy.",
            "SN can be positive or negative or zero.",
            "Now reality announces the actual outcome and we update the capital.",
            "For normal functions, here is the interest in the capital is how much money each ticket pace, how much it pays.",
            "YN minus how much we were charged for this ticket times the number of tickets we bought.",
            "Someone who so so so, so, so here again is skeptics capital.",
            "And intuitively forecast believes that he he knows the probability of the situation better than skeptic does.",
            "Of course, if skeptic and evaluate the situation better, he can make money in this protocol.",
            "OK, so it's possible to add realities moves at the beginning of this protocol, but I want to keep this simple.",
            "Now what you can do about this protocol, so I mentioned."
        ],
        [
            "Is this difference so intuitive?",
            "These protocols are ready?",
            "Different although they might look somewhat similar.",
            "So is the game theoretic through both legs."
        ],
        [
            "Numbers can be stated this way and I almost proved it last time, so skeptic has strategies which guarantees that first KN is never negative.",
            "He never has to borrow, so he plays using his own money and I.",
            "The forecasts are aesthetically unbiased in the same sense, or skeptics capital tends to Infinity.",
            "So one of the two pleasant things happens.",
            "Either the strong law of large numbers holds or skeptic becomes infinitely rich.",
            "OK, so now the measure theoretic flow of large numbers follows."
        ],
        [
            "Very easily, it's a general phenomenon, so if you have if reality plays using some probability distribution, then the difference YN minus pny PN is conditional probability of one is is Martin Gale difference.",
            "So skeptics capitalism, Martin Gale, you know that martingales tend to Infinity with probability zero.",
            "This must happen with probability one.",
            "OK, so let's see one.",
            "The election is very simple game theoretic results implying measure theoretic words.",
            "OK, but what is important about game theoretic results is that first reality doesn't need to follow any strategy, and for forecast doesn't have to ignore skeptic and actually this item is very important.",
            "This difference between measure theoretic probability and game theoretic probability.",
            "This makes defensive forecasting possible.",
            "OK, so there is some remark about measurability, but it's not very, very important.",
            "So typically when we prove game theoretic results, we don't say that skeptic strategy is measurable.",
            "It's clear that every individual strategy we construct is measurable, it's it's continuous, it's pleasant, it's you can see it's an explicit strategy, but it's awkward to impose this requirement formally.",
            "Either you should have compute ability, or you don't have anything.",
            "Then at all, measurability doesn't play any special role here.",
            "But but but this strategy for the game theoretical from of large numbers is definitely measurable.",
            "It's even continuous.",
            "OK, so now."
        ],
        [
            "Yeah.",
            "Recent observation is that this approach can be used for designing forecasting algorithms and even more recent observation is that you can use those forecasting algorithms for decision making.",
            "So for any continuous strategy of skeptic it turns out they exist strategy for forecast.",
            "There doesn't allow skeptics capital to draw.",
            "OK, so so."
        ],
        [
            "Almost any measure theoretic low probability can be translated into game theoretic terms.",
            "I will give you a reference at the end to book by Glenn Schaffen myself where we can see them all kinds of measure theoretic, loss of probability and we stayed then in game theoretic terms.",
            "So it's a very flexible approach.",
            "You can do a lot using it, but let's see that once you have some continuous strategy for skeptic you can use it for forecasting.",
            "For now I modify my protocol, so now is a.",
            "Suppose Katie can do something, for example, skeptic, and enforce the strong law of large numbers.",
            "Ice is stronger holds, or skeptic becomes infinitely rich, so skeptic has some strategy.",
            "We know his strategy at the beginning, so this protocol is different because skeptically know his strategy and I actually I don't need to know the full strategy for skeptic in advance at the beginning of each round.",
            "Cryptic must tell me what his strategy for this particular round is.",
            "For my new protocol is that reality?",
            "Announces is usually the datum skeptic announces some continuous function.",
            "He tells us how he will react to every possible move by forecast.",
            "So SN Maps the set of all possible moves of the forecast 2R he tells us how many tickets he will buy in response to any price.",
            "So it's his strategy for this round.",
            "Forecast announces the actual price for the ticket.",
            "Realty announces the actual outcome.",
            "We update the capital in the same way, but instead of so now we can see how many tickets skeptic by Santa VPN, and we multiplied by the payoff from each ticket.",
            "So it's very similar to that protocol.",
            "But now I can reach the camera observation is that forecaster has a strategy that ensures once this is continuous that ensures that skeptics capital never grows.",
            "So for example, in the case of the strong law of large numbers.",
            "He knows that his his capital will not go to Infinity.",
            "It will never increase, so it means forecaster can enforce the strong law of large numbers.",
            "And it's true for any continuous game theoretic low probability.",
            "And the lot of them.",
            "So now it's almost trivial.",
            "So first let me tell you what you should do here.",
            "So if it's a continuous function.",
            "So you have this function on the interval 01, so if this function intersects the horizontal axis, you take this as you appear as European.",
            "So forecast the choosers is the root of this equation.",
            "Ascent of P is 0, so we can see a sign of Ken will be 0.",
            "The capital will not change.",
            "Now if it's completely above.",
            "If it's completely above, then Y N -- P N will be always positive.",
            "So do.",
            "Yeah, so this will always be positive so we can make it negative by choosing PN equal to 1.",
            "And below it's a similar situation, so it's trivial, but it can be greatly generalized."
        ],
        [
            "Instead of this travel intermediate value theorem, you will have to use defence fixed point theorem and you can generalize it to locally convex spaces.",
            "It's very general result in fact.",
            "So there is a paper about the generalization.",
            "OK, so there is this research production."
        ],
        [
            "Each problem mentioned it in the previous talks and you you can choose any low probability you like and you can prove the corresponding game theoretic result or find it in our book.",
            "If you did it and apply theater one so you have some forecasting strategy that enforces that it automatically satisfies this law of probability.",
            "So is there is a slight problem with this program, becauses it it works too well, so we try it, for example to.",
            "To design forecast strategies that enforces equal fudge numbers in the same strategy also enforces the strong law.",
            "Such numbers.",
            "The important is important for us.",
            "Part of the law of iterated logarithm and it's.",
            "So so it didn't.",
            "Go far, but probably still it's possible.",
            "It's interesting to look for some other loss of probability, so I will show how to do this for the."
        ],
        [
            "A weak law of large numbers.",
            "So suppose our goal is to forecast to to enforce the law of large numbers.",
            "He can do it extremely well.",
            "So actually what he can do several of large numbers tells us that the mean deviation of the actual outcomes from the probabilities is not big and focus.",
            "That can make sure that this never exceeds 1/2.",
            "So have forecast that can do it.",
            "So his first move is 1/2 and then he just he just repeating the previous observation.",
            "And of course, it's the same.",
            "Here we have 1/2 instead of the usual the square root of N, for example.",
            "It's extremely good, but it's clear it's completely useless.",
            "It's not prediction.",
            "Something like pose diction you see is actual outcome, and you repeated.",
            "Of course you will be extremely unbiased.",
            "Because of this repetition, but it's we want to have something smart since there are some useful words, there are some less useful laws."
        ],
        [
            "But we want to have the Wolf Lodge numbers not in our original space, but in some Hilbert space.",
            "OK, so this is the kind of this is a lot.",
            "I want to have so you can prove that you can call it the convoluted more fudge numbers you take these differences by N minus PN and sketches them in some Hilbert space by multiplying them by some function Phi of PNXN.",
            "So Fly is a function that Maps each pair forecast and Dayton to some Hilbert space.",
            "So Angle assumes that you Prem of this.",
            "Images of the norms of this images doesn't exceed one, so this probability at least one minus Delta.",
            "This the sum of these scattered differences will not exceed 1 / sqrt N Delta.",
            "This is much more similar to the standard three cloth large numbers to the standard inequality, and it's it's true both magic theoretically and again theoretically.",
            "So.",
            "Oh no, it's a something.",
            "It is such a large division inequality is.",
            "This is what you get from Chebyshev's inequality.",
            "Yes you can.",
            "You can have something more sophisticated, but for my purpose this is better than large deviation inequality.",
            "Becausw I can enforce this with Delta equal to 1 and you from large deviation inequality we will not get a strong."
        ],
        [
            "Results.",
            "OK, so you you can apply defensive forecasting.",
            "Approach to this.",
            "Or probability and what you get exist.",
            "They exist forecasting strategy so some very explicit forecasting strategy that guarantees that this the norm of the same sum doesn't exceed one of the square root of N. You can tell it take Delta equal to 1.",
            "Becausw skeptics capital will never increase, so it corresponds that you you can take.",
            "So something to do is the result you get is.",
            "Is very strong because of that phenomenon.",
            "And you can see that in this case, even if Phi is equal to 1, this is still slightly better than the true probabilities for the true probabilities, you would expect the sum to grow as something like the orders and log log N. If you want this to hold for all possible end, this holds for all possible N. You have this inequality for.",
            "The true probability will never get such a good result.",
            "So in this sense it's still better than the true probabilities.",
            "It very much suspect that there is some important aspects of the true probabilities that is not reflected in the loss of privileges that are used here.",
            "There is a there must be some reason that the true probabilities have this log log.",
            "But in principle this is.",
            "This is what I need in this.",
            "This can be achieved by using some other probabilities.",
            "Defensive probabilities with 1 / sqrt N. OK, so so let's.",
            "What was?",
            "OK so it slows OK this thing to logs.",
            "What is important here is that, as usual, you don't need to deal with this Hilbert space directly in your.",
            "The algorithm Zelda isn't depends only on the dot product in the product or in this Hilbert space, only on the kernel.",
            "So once you have the kernel you have this very explicit algorithm.",
            "OK, so this is this is."
        ],
        [
            "Well, this kind of optimal.",
            "You can make it slightly stronger if less than this sum is less than equal to something more specific.",
            "So here is not assumed that file doesn't exceed one in absolute values on the realized PNXN so forecast can insurances and reality 2 the part of reality who chooses observation reality one chose the Dayton Realty chose as observation and it can choose.",
            "Opposite inequality summits.",
            "Upper and lower bounds are very close here.",
            "OK, so let me tell you how you can do it.",
            "Sorry very closely with the same yes.",
            "The exactly the same, so one can ensure this, so if they're playing, this strategy will have this equality here.",
            "Yes, yes, so so I had it's up about an inch on the previous slide so you have to make it slightly more precise to prove the matching lower bound.",
            "And I was assuming that this was bounded by one from above.",
            "Here's another assumption, so so this is always true.",
            "And for proving decision theoretic results, this bound is important.",
            "Sometimes you have to use this.",
            "Please especially fun bound to loss functions."
        ],
        [
            "Becomes important.",
            "OK, so probably most of you knows know much better than I do what reproducing kernel Hilbert spaces are.",
            "But let me just.",
            "You might just have to give the definition for the sake of people who don't know it well, so it reproducing kernel Hilbert space is just some Hilbert space to feel valued functions.",
            "I will be interested only in functions on my data.",
            "SpaceX, such searches evaluation functional S continues, so we fix some XF of X is continuous as a function is bounded as a function of F, so the normal space as usual space L2 is not.",
            "Reproducing kernel Hilbert space, but so believe spaces are because of this derivatives in them.",
            "So by the standard Ricefish theorem for each.",
            "For each X you can express the evaluation function, which is continuous as the inner product KX.",
            "Some element in your space times F. And see if you'll be the supremum of the norms of this KX.",
            "In my space, I will assume that it's this supremum as finite and the kernel.",
            "I have it.",
            "Here is the Colonel in."
        ],
        [
            "In the reproducing kernel, Hilbert space can be expressed as in the product between the representatives of X&X prime.",
            "So the kernel between X&X prime is this in the product.",
            "OK, so from the team's theorem, you can almost immediately deduce that in any reproducing kernel Hilbert space.",
            "If you find the sum of Y, N -- P, N and you multiply each Y N -- P N by some function in your reproducing kernel Hilbert space.",
            "This will never exceed.",
            "This song will never exceed this proficiency.",
            "F. The maximum of the norms of KX times is the norm of F, so if the norm of F is large, this can be large as well, but this average doesn't exceed this product over the square root of N. It's true for all N&F.",
            "OK, so it's easily follows and intuitively this tells you that the forecasting strategy is well calibrated and has high resolution, so I don't want to explain this words, But here if you don't have this, it doesn't mean anything.",
            "You can achieve this very, very simply, but with this term it becomes much more meaningful and much more useful.",
            "You can really use it in.",
            "Decision."
        ],
        [
            "OK, again, various assemble optimality result you can you have to improve this slightly replacing end by this sum of PN 1 -- P N and you can prove the matching lower bound.",
            "So there is no slack at this point, so so let me give you some example or examples of reproducing kernel Hilbert spaces into examples."
        ],
        [
            "I like most someone example I mentioned already.",
            "This Sobolev norm squared mean plus the the mean squared derivative.",
            "So the kernel for this space is given by this strange expression.",
            "So this result was obtained by Craven and War but in 1979 so so we can actually do all computations with this for this norm using this kernel.",
            "And but here you can see this kernel is on the interval 01.",
            "Well, what you can do if your data are not bound?",
            "You don't know you don't have any a purity bound, so we can use instead."
        ],
        [
            "Is Norm you take the integral from minus Infinity to Infinity F squared plus the integral of the derivative squared?",
            "And here's the kernel.",
            "It looks completely different, so the norm looks similar, but the kennel is different and it's it's a relatively recent result 1996 by something young.",
            "OK, so it's a in 1D, but if you are interested in many attributes you can use, for example, tensor products of these spaces.",
            "Or you can use send plate splines which are very popular as well.",
            "There are hundreds of different reproducing kernel Hilbert spaces, so usually this relation between the norm in the kernel is far from travel, so it's kind of duel and usually.",
            "In your algorithms you need kernels to do computations, but in mathematics for analysis you need nodes.",
            "OK, so it's yes.",
            "OK yeah, thanks for this remark.",
            "So it might be trivial, but.",
            "It didn't look so.",
            "Actually this is something that I got completely independently from Trayvon and Robin.",
            "For me, it was far from trivial.",
            "It was several pages of computations, but I'm sure it was because of my ignorance even use something more about full year transform that would have been sampling.",
            "OK, so."
        ],
        [
            "I have another research program, so the previous problem kind of failed.",
            "Maybe not completely, but I had just one algorithm in the previous program, so this problem is for decision making.",
            "So first you choose a goal that could be achieved if you use a true probabilities.",
            "So imagine you have you know the true probabilities and you can achieve this goal.",
            "Now you and you should be able to prove that you can achieve this goal.",
            "So we construct some decisions strategy which probably achieve achieve your goal and the proof yes.",
            "Or if I don't need this formally, just some some probabilities that reality is using.",
            "Maybe there are no true probabilities, but let's let's imagine there are some true probabilities.",
            "So of course this theory doesn't assume any true probabilities.",
            "OK, so now of course you proof will depend on some rules of probability.",
            "It depends if it depends on several laws you have to merge this loss into one more so you you isolate some continuous low probability game theoretical probability on, which approve depends and then.",
            "You don't eat through the true probabilities at this point.",
            "You remember you don't know them, but you don't need them.",
            "You have this method of defensive forecasting that can produce probabilities that are as good as the true probabilities as far as this continuous flow of probability is concerned.",
            "OK, so this is the idea.",
            "So this idea might sound too ambitious and even."
        ],
        [
            "The impossible, so let me talk briefly about the possible goal.",
            "So suppose your goal is moderate, and so it's not easy to come up with interesting goals.",
            "Example goal cannot be.",
            "I want to become a millionaire in 10 years, where your goal should be achievable regardless of the true probabilities.",
            "First, your goal should be in terms of observables.",
            "Your goal should not involve the true probabilities themselves.",
            "It should be something that you depends.",
            "And what you can see in the real world.",
            "But your goal cannot be.",
            "I want to be coming with your name 10 years, because maybe you live in a bad world.",
            "Maybe it's absolutely impossible.",
            "So the goal should be relative.",
            "It seems so if it's if there is not very complicated strategy of becoming a millionaire, you should become one.",
            "So not very complicated as the norm should should not be too large in some in some space.",
            "So it's something vague."
        ],
        [
            "At this point.",
            "OK, so now how does this reduction work?",
            "So let's so I had this most function, Lambda one gamma and Lambda, zero gamma and now introduce also it's average.",
            "So Lambda P gamma is the expected loss if the probability is that why is one is equal to P?",
            "So your expectation when the when the probability is P. Of course you don't know P, but the function is useful and if you do.",
            "P then you could visit given value of P. You could minimize your loss function with respect to.",
            "So let me assume that it's convex and nice.",
            "At this point it's not necessary, but.",
            "It will be simpler, so G of P. It's some decision.",
            "Some optimal decision under this probability, AP geography is your choice function.",
            "So for the square and blood loss functions you can take care of people to PSA they called propes, causing rule.",
            "You don't need to change P at all, you just can use your peers.",
            "Is your decision is absolute loss function.",
            "For example it's different.",
            "There are all kinds of loss functions.",
            "So I will say that the exposure of some decisions rule of some decision it first.",
            "So is the loss of this decision rule.",
            "If one happens minus its rules.",
            "If zero happens, so it's it's exposure to risk and I will need to functions the exposure of my choice function gamma.",
            "It's a function of P just for each P. It gives us the exposure of this decision GOP.",
            "Anne."
        ],
        [
            "And I will also need the exposure of decision rule.",
            "So it's a function of X now, and for each X we get the exposure of the decision recommended by this decision rule.",
            "So to show you have to prove the kind of results I mentioned because I'm not going to give you the actual proof, I will prove something very informal, but it will be clear from from this how the proofs work.",
            "So I will show that the decision output using this empirical loss minimization principle.",
            "Probability P. You find the best decision gamma for this probability, where at the end are output by the algorithm of large numbers with some universal kernel, for example this or believe Colonel I mentioned before this satisfy the average loss.",
            "The average loss of this strategy doesn't exceed doesn't exceed much, so this is some informal notation doesn't exceed much.",
            "The average loss of any decision rule with a small exposure and exposure is measured by its subliminal.",
            "So it is important, of course, the decision rule itself.",
            "It takes values in some strange space in general, in some decision space, but the exposure takes values.",
            "NVIDIA line.",
            "OK so this is the informal quality and how."
        ],
        [
            "If you can prove it, so would kind so so this if you want to prove this.",
            "It's clear that if you have the true probabilities, the empirical loss minimization is the right thing to do if the another true probabilities you are going to perform as well as any other strategy.",
            "Since this is a true probabilities, but what you should do if these are not with the true probability, you have to use the law of large numbers somehow.",
            "So so formal you can.",
            "Just some very simple manipulation shows that the deviation of your actual loss from your expected loss is equal to the exposure of your decision of your decision.",
            "Gamma Times the deviation of your actual outcome from the expected outcome, so it's it's very simple and the law of large numbers numbers at least is that was stated earlier.",
            "Do you remember there was something that?",
            "Is this some of Y N -- P N times something XNPN this was was not large and this is exactly what we need here.",
            "This function F is this exposure so that more is exactly what is needed in this decision theoretic result.",
            "So let's see how it works.",
            "So in conjunction with that with forecasting."
        ],
        [
            "Result if you look at the actual loss of this decisiones computed from the.",
            "Forecasts PN.",
            "You can try it.",
            "This actually looks can be represented as expected loss.",
            "Having eaten by expected I mean of course conditional conditional one step ahead expectations.",
            "So you take the expected loss plus the difference between the actual loss and the expected loss here.",
            "And this difference.",
            "We know it's small by the law of large numbers we know that you can you have PN for which this difference is small.",
            "So this doesn't exceed the expected loss and the expected loss.",
            "Because of the choice, so G of PN is the best possible decision.",
            "The decisions that gives.",
            "The minimum to this expression you know this doesn't exceed."
        ],
        [
            "Lambda PN in here any other decision?",
            "So we take the decision taken by some decision rule.",
            "It doesn't exceed this and now for this decision tools we can do the same thing.",
            "So here we have the expected loss but we are not interested in the expected, mostly interested in the deal is actual loss, so we can try the actual loss minus the difference is the difference is small.",
            "This doesn't exceed the actual loss of the decision rule.",
            "So let's understand that scheme.",
            "So the summary of the proof."
        ],
        [
            "A technique is here, so we want to prove that the actual loss of our strategy doesn't exceed the actual loss of some strategy.",
            "We want to compete with.",
            "So now we know that the actual loss of our strategy is approximately equal to the expected loss.",
            "The sum of Lambda, PN of GP and this is less than equal to the expected loss of the alternative strategy, and this is again approximately equal to the actual loss.",
            "So if you want to put to get some really strong results, it's better to split these two steps to have some kind of loss of some kinds of weak laws of large numbers that tailed to what you really need, not the general result I stated earlier.",
            "But let me tell you what you can."
        ],
        [
            "So you know these are special cases of some very gentle theorem which is applicable to all convex loss functions, and they can be unbounded if they're unbounded.",
            "They shouldn't have very sick tales.",
            "Bridal stated in special cases not introduced some general terminology, so if your loss function is quadratic, I will do this for some specific loss functions.",
            "They will tell you what the precise constants I haven't proved that these constants are optimal, but there is no.",
            "I can't see much slack in what I did I would be very much surprised if they could be improved at least by much.",
            "OK, so here I'm using the 2nd.",
            "So believe Norm that I mentioned this, I believe norm on the full line minus Infinity Infinity, so the loss of the decision maker doesn't exceed the losses.",
            "The cumulative loss of the alternative strategy plus 2D minus one this centered.",
            "Sent a decision rule.",
            "This is measured sub non plus 1 * 3 / 8 some reasonable constant, reasonably small constant times the square root of N. So it's true for all NND.",
            "And if your losses absolute loss, this is something very similar.",
            "Only this concept will be very slightly worse.",
            "It will be sqrt 6 / 4 and you have still to D -- 1 actually to D -- 1 will be the exposure.",
            "And is this loss functions of the decision Rule D. Yeah.",
            "Yes, yes, very similar.",
            "Only the constants are slightly different.",
            "6.",
            "Probably not.",
            "I've never notice it.",
            "OK, thank you.",
            "Actually, I did convey yesterday made sure that this is largely this, but my analysis was not as deep as yours.",
            "Insight.",
            "To investigate it."
        ],
        [
            "'cause OK, So what you get from the log loss function?",
            "So this function is is unbounded in front bounded functions.",
            "The formal condition in this theorem is that the tails should should decay fast enough and for for the Lobos functions stays.",
            "The details decay exponentially fast and each of the minus T and what is required is that they should decay faster than 1 / T. So it's it's well within the.",
            "Domain of applicability of this resolved, so here is the exposure will be slightly different.",
            "You should take log odds ratio block of D / 1 -- D and again.",
            "Sorry it should be as prime.",
            "I use the same.",
            "So here you have the loss.",
            "This overhead is oh point 7 but of course it's not directly comparable to the previous results.",
            "You have a different exposure here.",
            "Look at the ratio.",
            "OK, so this result can in fact be extended to non convex loss functions.",
            "All those functions that depend on several future observations.",
            "I think it's very interesting, but it has hasn't been done yet, but it's it's working progress.",
            "It seems it gives you some.",
            "Opportunities this proof."
        ],
        [
            "Technique, so this is a reference to our book.",
            "There is nothing about defensive forecasting in this book.",
            "We for some reason we didn't realize when we were writing this book that you can use it for forecasting because a lot of of theorems of loads of probabilities in this book like the Central Limit Theorem, the logs, iterated logarithm, someone sided forms of the central Limit theorem we don't have.",
            "Audience inequality for some reason as well, maybe because you were interested in classical limit theorems.",
            "And if you want to read more about defensive forecasting, so there are some working papers on the website for this book, so it's probability and finance without any.",
            "Punctuation here and there.",
            "Working paper 10 is about the generalization of this existence result to any Hilbert spaces.",
            "15 is about defensive forecasting and 15 is about applications to decision making.",
            "Thank you.",
            "I believe they are, but I don't have any proofs.",
            "No, no it doesn't.",
            "General research program #2.",
            "You look at what is the law of probability, yes.",
            "General rule of probability that is involved implicitly, but do you ever?",
            "Get for any decision problem.",
            "Do you get?",
            "So do you get?",
            "The law of large numbers in some form or another.",
            "There must be some very important clue.",
            "I'm sure that.",
            "Is difference between the probabilities in the forecast times something?",
            "It looks at something very universal, yes?",
            "Number.",
            "But you said there should be cases where you get something.",
            "Different, you know.",
            "I have bone disease, indirect evidence that there must be such cases because so this is the difference is small.",
            "It's some reflection of calibration and resolution.",
            "There seems to be very, very fundamental properties of any algorithm, But my probabilities uncalibrated much better than the true probabilities.",
            "And so I think it can mean only one thing.",
            "That calibration in resolution is also better, at least in my from formalisation.",
            "And I think it can mean only one thing.",
            "So there are some other important properties that can also be used for decision making in this.",
            "So this is actually a minus.",
            "You can't achieve the square root of N, you can only achieve the square root of log log and the reason for it must be that the true probabilities have some other properties we don't know yet, or I don't know yet.",
            "But it's possible.",
            "Nobody knows those properties.",
            "Some useful properties that you can use for decision making, but I have no idea what those properties can be.",
            "So.",
            "She's making her strategy that gives these guarantees, yes.",
            "Yes, yes they are very easy.",
            "They are very simple and easy.",
            "I haven't done much empirical work.",
            "I need some empirical work, but his defensive forecasting not with this decision making algorithms so.",
            "In these cases, the strategies are extremely simple and efficient.",
            "There's there fast, but it's only because you're you're.",
            "You know that your observation will be between zero and one.",
            "It can be in between any other bounds, but already in the multiclass case.",
            "I don't know how to find a fixed point efficiently, and it's a big problem.",
            "I know there is some big industry in economics about it, but I haven't had time to study it carefully, but it's it's it's travel.",
            "In this case is.",
            "For.",
            "Yes, yeah, yes it does.",
            "So when I was talking about efficiency meant there is nothing like so, it's it's.",
            "It's a small degree polynomial.",
            "It's it's not exponential or something.",
            "You can do it for reasonable size of some datasets.",
            "What?",
            "The shape of your.",
            "Pacifier.",
            "Let me try to remember how it works.",
            "Of course, the decision making algorithms have to achieve to go smooth Tanis Lee.",
            "Do remember those two items so one is that for the decision makers, actual loss should be close to the expected losses.",
            "This should be true for the decision rule.",
            "These are two different rules and I have to merge these two loss so sometimes it's slightly more complicated.",
            "Talk about one of these so when you want it is just about defensive forecasting.",
            "So when you want to find the prediction PN 4 YN so you find the sum of your kernel you you compute your kernel.",
            "Any kernel between P&X and, PIXI.",
            "Your thumb is from one to N. And this is multiplied by.",
            "Let me see whi I minus why this should be equal.",
            "Why I minus Pi?",
            "OK, so.",
            "Is this what you do in practice?",
            "So he the proof there were two cases when applying intersect the horizontal axis and when it's not so this is the case when it does intersect.",
            "Is this in practice?",
            "You can still only such cases maybe, except for the first trial or something degenerate.",
            "So here you have some kernels that measures how you knew examples, so you are shown XN.",
            "So sorry, you teachers pienso that this is true, this is.",
            "Haha you choose APN so if you look at your current situation PNXN and you compare to what you had in the past and you look only at the trials in the past when you're.",
            "That's right, sorry, that's right, yes.",
            "OK, so here you look only.",
            "The trials that are similar to you last trial and you you require this unbiasedness for those trials.",
            "This is how you choose the PN.",
            "Thank you.",
            "Yes, yes.",
            "But you can do it efficiently.",
            "In this case it's quite fast.",
            "I can feel.",
            "This message is sent to the cheerleading Max, maybe?",
            "Yes, yes, of course.",
            "It's yes.",
            "Wait?",
            "Yes, I had exactly the same feeling and so if you call this game theoretic foundations of probability, but we thought it wasn't connected to the normal game theory, it was about some theory of perfect information games.",
            "Something in the foundations of the semantics, but it seems there must be some connections with which those people are doing some equilibra, some mini Max.",
            "Yes, yes.",
            "I'm sure there's a connection, but you can't say much about it now."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With reproducing kernel Hilbert spaces OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you did.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this talk will be somewhat unusual in that I will not make the usual idea assumption.",
                    "label": 0
                },
                {
                    "sent": "But as we know from Nicholas, you can deduce some results in this standard setting from this kind of results.",
                    "label": 0
                },
                {
                    "sent": "So I'm interested in what is called prediction with expert advice.",
                    "label": 1
                },
                {
                    "sent": "We have some pool of decisions, thread just, and our goal is to perform almost as well as the best.",
                    "label": 1
                },
                {
                    "sent": "Strategy of the best strategies in our pool.",
                    "label": 0
                },
                {
                    "sent": "And there are no assumptions at all about the way observations are generated, so I will explain how you can.",
                    "label": 0
                },
                {
                    "sent": "Use what I call defensive forecasting for proving results of this kind.",
                    "label": 0
                },
                {
                    "sent": "So my previous talk one year ago at this is analogous workshop.",
                    "label": 0
                },
                {
                    "sent": "Last year was about defensive forecasting, so there will be some intersection.",
                    "label": 0
                },
                {
                    "sent": "I hope not too big between this talk and last years talk.",
                    "label": 0
                },
                {
                    "sent": "I know that the intersection is 1/3 between 0 dances at least potential audiences.",
                    "label": 0
                },
                {
                    "sent": "So I will summarize briefly, at some point my previous talk, so.",
                    "label": 0
                },
                {
                    "sent": "In this talk I will try something.",
                    "label": 0
                },
                {
                    "sent": "Usually people talk about prediction.",
                    "label": 0
                },
                {
                    "sent": "I will avoid.",
                    "label": 0
                },
                {
                    "sent": "I try to avoid this word.",
                    "label": 0
                },
                {
                    "sent": "Instead, I will talk about forecasting or decision-making.",
                    "label": 0
                },
                {
                    "sent": "I will have two protocols, and both are more or less about prediction.",
                    "label": 0
                },
                {
                    "sent": "But it might be.",
                    "label": 0
                },
                {
                    "sent": "I will explain the difference between them.",
                    "label": 0
                },
                {
                    "sent": "And philosophically they are very different, but it might be confusing at first.",
                    "label": 0
                },
                {
                    "sent": "So to avoid confusion as much as possible, I will talk about forecasts or decisions.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is my decision making.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Protocol, so the loss of decision making is also in the protocol.",
                    "label": 0
                },
                {
                    "sent": "The time is discrete and it's round reality announces.",
                    "label": 0
                },
                {
                    "sent": "Some XNXN can be some instance, for example.",
                    "label": 0
                },
                {
                    "sent": "Decision makers then announces his decision Gamaran.",
                    "label": 0
                },
                {
                    "sent": "Reality announces some observation.",
                    "label": 0
                },
                {
                    "sent": "YN and the loss of the decision making is updated.",
                    "label": 0
                },
                {
                    "sent": "So I have some loss function Lambda which measures then loss that decision Maker is his decision gamma and Surface is actual outcome as Y and so here I have Excel will be called data and.",
                    "label": 1
                },
                {
                    "sent": "In fact, it's more convenient to include in X and all information that is relevant for prediction for predicting YN.",
                    "label": 0
                },
                {
                    "sent": "So it can be in time series analysis.",
                    "label": 0
                },
                {
                    "sent": "For example, it can be some previous observations or some measurements inside information, so I would like to view it slightly more generally than just predicting instances.",
                    "label": 0
                },
                {
                    "sent": "So this is the protocol I'm really interested in.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this talk, so I want to compete against what I call decision rules.",
                    "label": 0
                },
                {
                    "sent": "So you saw them in some cheese talks, so a decision rule is something that Maps the set of all possible data set of all possible Axis Capital X to the interval 01 support for each XI have.",
                    "label": 0
                },
                {
                    "sent": "Some decision, so I want to compete against decision rules that are not too complicated.",
                    "label": 1
                },
                {
                    "sent": "Not too strange, so let me give you some simple example just to illustrate the kind of results I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "So let me assume that Capital X is the interval 01.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So relative announces some number in 01 and I want to predict the corresponding Y.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me define the way some way of measuring how complicated my decision rule is, so I will do it using the Sobolev norm.",
                    "label": 0
                },
                {
                    "sent": "So so there are actually.",
                    "label": 0
                },
                {
                    "sent": "So when people talk about Sobolev spaces, typically they mean to logical vector spaces without, so inside can be many norms that generates the same topological vector space.",
                    "label": 0
                },
                {
                    "sent": "This is one of the convenient norms for that.",
                    "label": 0
                },
                {
                    "sent": "So if you have some function F on the interval 01, you can square its average.",
                    "label": 0
                },
                {
                    "sent": "The integral from zero to 1 plus the integral of X, ^2, ^2.",
                    "label": 0
                },
                {
                    "sent": "Derivative if F is not absolutely continuous.",
                    "label": 1
                },
                {
                    "sent": "If you cannot differentiate, or if this integral, for example is is infinite.",
                    "label": 0
                },
                {
                    "sent": "I define this norm to be infinite.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's no, so some normal.",
                    "label": 0
                },
                {
                    "sent": "Have",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Functions will have functions that are not to be.",
                    "label": 0
                },
                {
                    "sent": "It will have the snow which is not too large.",
                    "label": 0
                },
                {
                    "sent": "The next slide I will say more about this norm, but let me state some simple proposition.",
                    "label": 0
                },
                {
                    "sent": "So suppose X axis are taken from zero one and your loss function is absolute loss.",
                    "label": 1
                },
                {
                    "sent": "You just measure the absolute deviation of the decision from the actual outcome.",
                    "label": 0
                },
                {
                    "sent": "So decision making has strategy.",
                    "label": 0
                },
                {
                    "sent": "Vet green teas that.",
                    "label": 0
                },
                {
                    "sent": "So it in each round N he is average loss.",
                    "label": 0
                },
                {
                    "sent": "So far his cumulative loss divided by N doesn't exceed the average loss of any decision rule.",
                    "label": 0
                },
                {
                    "sent": "This is true for any decision rule D plus something that depends on the Sobolev norm of two D -- 1.",
                    "label": 0
                },
                {
                    "sent": "So D takes values between zero and one and two D -- 1 is just some you sent our decision rule so that it takes values between minus one.",
                    "label": 0
                },
                {
                    "sent": "And one so that zero becomes the new true value for it.",
                    "label": 0
                },
                {
                    "sent": "So we take the Sobolev norm of two D -- 1 + 1 in divided by the square root of N. So here you can see this standard convergence rate.",
                    "label": 0
                },
                {
                    "sent": "One of the square root of N. But here it has to be multiplied by this subliminal.",
                    "label": 0
                },
                {
                    "sent": "So I'm talking about decision making here, but it's important that intuitively this kind of results are about small decision makers, so it's just using kacian.",
                    "label": 1
                },
                {
                    "sent": "It's formally, there are no restrictions here.",
                    "label": 0
                },
                {
                    "sent": "You don't assume anything about environment, but if you look what it means, so, so here we have the loss of the decision-making.",
                    "label": 0
                },
                {
                    "sent": "He had the loss of the decision rule, and we measure them on the same sequence of observations.",
                    "label": 0
                },
                {
                    "sent": "Why end and here why?",
                    "label": 0
                },
                {
                    "sent": "In here, why end it if he imagines that reality pay some attention to what we're doing.",
                    "label": 0
                },
                {
                    "sent": "It's possible that if we take decisions gamma and sensationalized observations will be different from when we take decisions D of XL.",
                    "label": 0
                },
                {
                    "sent": "So it's very artificial if we are not small, it's official to prove this kind of results.",
                    "label": 0
                },
                {
                    "sent": "So this isn't make it has to be understood in this sense, yes.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "Trade like cover.",
                    "label": 0
                },
                {
                    "sent": "I would like to take an adult, yes.",
                    "label": 0
                },
                {
                    "sent": "And then run as usual, pondering.",
                    "label": 0
                },
                {
                    "sent": "For a finite number of the decisions.",
                    "label": 0
                },
                {
                    "sent": "The center of the colors we get there.",
                    "label": 0
                },
                {
                    "sent": "Although it's you.",
                    "label": 0
                },
                {
                    "sent": "You cannot cover it.",
                    "label": 0
                },
                {
                    "sent": "This is not a compact space.",
                    "label": 0
                },
                {
                    "sent": "30 What you do?",
                    "label": 0
                },
                {
                    "sent": "Eat.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Oscar.",
                    "label": 0
                },
                {
                    "sent": "But it's.",
                    "label": 0
                },
                {
                    "sent": "Love.",
                    "label": 0
                },
                {
                    "sent": "Yes, but it's not locally compact, it's.",
                    "label": 0
                },
                {
                    "sent": "In other words, you can cover it.",
                    "label": 0
                },
                {
                    "sent": "Scale.",
                    "label": 0
                },
                {
                    "sent": "But it's it's closed.",
                    "label": 0
                },
                {
                    "sent": "So it's a closed bowl, so it's if it's pre compacted.",
                    "label": 0
                },
                {
                    "sent": "It must be compact.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Philly.",
                    "label": 1
                },
                {
                    "sent": "Yes, this is some system.",
                    "label": 0
                },
                {
                    "sent": "If it's locally compact then it must be.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's see what this norm means.",
                    "label": 0
                },
                {
                    "sent": "So so so if in this sample protocol, when we're predicting observations are in 01, the Sobolev norm can be bounded by the absolute value of this mean value plus the square root of the new value of the squared derivative.",
                    "label": 0
                },
                {
                    "sent": "So it sounds like 1 plus the mean slope.",
                    "label": 1
                },
                {
                    "sent": "Of this function F where the mean slope is taken in this quadratic sense.",
                    "label": 0
                },
                {
                    "sent": "So if the mean slope is is much less than the square root of the number of trials, so you can see that the bound on the previous slide.",
                    "label": 1
                },
                {
                    "sent": "It shows that you perform almost as well as this decision rule, for which this means slope is not big.",
                    "label": 1
                },
                {
                    "sent": "So you can think about piecewise linear functions.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 1
                },
                {
                    "sent": "OK, so some labels talk about as.",
                    "label": 0
                },
                {
                    "sent": "X spaces of date, norms, and loss functions, but this is my motivating example, so I will explain how to prove this kind of results, and I think the proof technique is very interesting.",
                    "label": 0
                },
                {
                    "sent": "Probably it's even more interesting than the result itself.",
                    "label": 0
                },
                {
                    "sent": "OK, so is there a similar result in the leadership?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example, in the paper by Nicola Conconi in Gentilly.",
                    "label": 0
                },
                {
                    "sent": "The result is obtained for specific loss functions for decision making and the decision rule, and as far as I could see, the loss functions are different for this.",
                    "label": 1
                },
                {
                    "sent": "For the decision, make it, we're just counting his mistakes.",
                    "label": 0
                },
                {
                    "sent": "One case, and for the decision rules we the user hinge loss, at least in this particular paper that I saw.",
                    "label": 0
                },
                {
                    "sent": "In there is a paper by Phil Long and Kinburn where it is assumed there is a perfect decision rule.",
                    "label": 1
                },
                {
                    "sent": "Maybe, which is.",
                    "label": 0
                },
                {
                    "sent": "Of course this is throwing assumptions about reality.",
                    "label": 0
                },
                {
                    "sent": "In this talk there are no assumptions at all, but as results they tend to be about finite dimensional pools of experts.",
                    "label": 0
                },
                {
                    "sent": "So here's a pool of possible decision strategies.",
                    "label": 0
                },
                {
                    "sent": "It's it's huge, it's a sublease space.",
                    "label": 0
                },
                {
                    "sent": "So it's it's very different kind of results, and the approach described here is inspired by a paper by first and water and many people were developing this approach, but there are no formal connections.",
                    "label": 0
                },
                {
                    "sent": "I think there's a ton of connections left.",
                    "label": 0
                },
                {
                    "sent": "In this work and in there between this work and their work.",
                    "label": 0
                },
                {
                    "sent": "OK, so the plan for the rest of this talk.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Three items they intersect is my talk year ago, so I will explain that there are two approaches to for probably to probability to the foundations of probability game, theoretical measures theoretic and give one example.",
                    "label": 1
                },
                {
                    "sent": "Game theoretic and measure theoretic law of large numbers.",
                    "label": 0
                },
                {
                    "sent": "Strong law of large numbers.",
                    "label": 0
                },
                {
                    "sent": "Then I will explain how how it's possible to make any.",
                    "label": 0
                },
                {
                    "sent": "Continues game theoretic low probability into some forecasting strategy.",
                    "label": 0
                },
                {
                    "sent": "It should have to implement it.",
                    "label": 0
                },
                {
                    "sent": "Have to obtain from the weak law of large numbers what I call the algorithm of large numbers and then the algorithm of large numbers can be stated in reproducing kernel Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "And so theoretically it has very appealing properties, calibration resolution, but I will state what I need formally, but these properties they can look somewhat abstract.",
                    "label": 0
                },
                {
                    "sent": "It might not be immediately clear why these properties are good statisticians like them, and it's intuitively it's clear they good properties, but it turns out that they can also be used for decision making.",
                    "label": 1
                },
                {
                    "sent": "So once you you can use some probabilities for making.",
                    "label": 0
                },
                {
                    "sent": "Good decisions, it looks like some ultimate justification of of probabilities, so this is some I like this justification better, so I will start from these two approach.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just as the foundations of probability so, so these two approaches are very old, both of them, but one of them is standard.",
                    "label": 0
                },
                {
                    "sent": "It's approach based on measure theory, so it's usually called kolmogorov's axioms of probability, and the approach based on gambling.",
                    "label": 0
                },
                {
                    "sent": "It was started by furnaces, but it's much less popular now, and it seems the crucial step here was made by Jonville, whom you place for me.",
                    "label": 0
                },
                {
                    "sent": "This is collectives with.",
                    "label": 0
                },
                {
                    "sent": "Martin Gales, so I will trade the difference using the simplest martingales, strong lofts, large numbers.",
                    "label": 1
                },
                {
                    "sent": "So they have some random variables Y Y1Y2 and so on.",
                    "label": 0
                },
                {
                    "sent": "And if we know the probability is the probability measure generating these observations the best prediction?",
                    "label": 0
                },
                {
                    "sent": "Probably if this is all our information is predict says that the probability of YN is its conditional expectation given.",
                    "label": 0
                },
                {
                    "sent": "The key is observations and maybe given some previous data.",
                    "label": 0
                },
                {
                    "sent": "If you have something, but here I assume that this is all information we have, so we should condition on the past.",
                    "label": 0
                },
                {
                    "sent": "So the coding for the strong law of large numbers.",
                    "label": 0
                },
                {
                    "sent": "The average deviation of the actual observations from the forecasts tends to zero with probability one, so you can see.",
                    "label": 0
                },
                {
                    "sent": "In measure here, this happens with probability one.",
                    "label": 0
                },
                {
                    "sent": "It's a magic theoretic statement, but we can do it game theoretically as well.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now you can see protocol which is very different from the protocol you saw on the second slide, I think.",
                    "label": 0
                },
                {
                    "sent": "So it's also about forecasting, but the goal here is completely different.",
                    "label": 0
                },
                {
                    "sent": "For that protocol we didn't care about truth at all.",
                    "label": 0
                },
                {
                    "sent": "We have a goal was to minimize the losses.",
                    "label": 0
                },
                {
                    "sent": "Here we kind of claims that our forecasts are the best possible.",
                    "label": 0
                },
                {
                    "sent": "So the protocol is this forecasting protocol, so we start from some initial Capital One and our game is forecast, announces some forecast, so he announces some probability.",
                    "label": 0
                },
                {
                    "sent": "So intuitively this is the probability that YN will be one.",
                    "label": 1
                },
                {
                    "sent": "But now he's serious about it.",
                    "label": 0
                },
                {
                    "sent": "He he is willing to sell tickets, which will pay YN and his prices.",
                    "label": 0
                },
                {
                    "sent": "PN for for such tickets.",
                    "label": 0
                },
                {
                    "sent": "Now skeptic somebody who maybe doesn't believe the forecast, announces some numbers and how many tickets he wants to buy.",
                    "label": 0
                },
                {
                    "sent": "SN can be positive or negative or zero.",
                    "label": 1
                },
                {
                    "sent": "Now reality announces the actual outcome and we update the capital.",
                    "label": 0
                },
                {
                    "sent": "For normal functions, here is the interest in the capital is how much money each ticket pace, how much it pays.",
                    "label": 0
                },
                {
                    "sent": "YN minus how much we were charged for this ticket times the number of tickets we bought.",
                    "label": 0
                },
                {
                    "sent": "Someone who so so so, so, so here again is skeptics capital.",
                    "label": 0
                },
                {
                    "sent": "And intuitively forecast believes that he he knows the probability of the situation better than skeptic does.",
                    "label": 0
                },
                {
                    "sent": "Of course, if skeptic and evaluate the situation better, he can make money in this protocol.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's possible to add realities moves at the beginning of this protocol, but I want to keep this simple.",
                    "label": 1
                },
                {
                    "sent": "Now what you can do about this protocol, so I mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is this difference so intuitive?",
                    "label": 0
                },
                {
                    "sent": "These protocols are ready?",
                    "label": 0
                },
                {
                    "sent": "Different although they might look somewhat similar.",
                    "label": 0
                },
                {
                    "sent": "So is the game theoretic through both legs.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Numbers can be stated this way and I almost proved it last time, so skeptic has strategies which guarantees that first KN is never negative.",
                    "label": 1
                },
                {
                    "sent": "He never has to borrow, so he plays using his own money and I.",
                    "label": 0
                },
                {
                    "sent": "The forecasts are aesthetically unbiased in the same sense, or skeptics capital tends to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So one of the two pleasant things happens.",
                    "label": 0
                },
                {
                    "sent": "Either the strong law of large numbers holds or skeptic becomes infinitely rich.",
                    "label": 0
                },
                {
                    "sent": "OK, so now the measure theoretic flow of large numbers follows.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very easily, it's a general phenomenon, so if you have if reality plays using some probability distribution, then the difference YN minus pny PN is conditional probability of one is is Martin Gale difference.",
                    "label": 0
                },
                {
                    "sent": "So skeptics capitalism, Martin Gale, you know that martingales tend to Infinity with probability zero.",
                    "label": 0
                },
                {
                    "sent": "This must happen with probability one.",
                    "label": 1
                },
                {
                    "sent": "OK, so let's see one.",
                    "label": 0
                },
                {
                    "sent": "The election is very simple game theoretic results implying measure theoretic words.",
                    "label": 0
                },
                {
                    "sent": "OK, but what is important about game theoretic results is that first reality doesn't need to follow any strategy, and for forecast doesn't have to ignore skeptic and actually this item is very important.",
                    "label": 0
                },
                {
                    "sent": "This difference between measure theoretic probability and game theoretic probability.",
                    "label": 0
                },
                {
                    "sent": "This makes defensive forecasting possible.",
                    "label": 1
                },
                {
                    "sent": "OK, so there is some remark about measurability, but it's not very, very important.",
                    "label": 0
                },
                {
                    "sent": "So typically when we prove game theoretic results, we don't say that skeptic strategy is measurable.",
                    "label": 0
                },
                {
                    "sent": "It's clear that every individual strategy we construct is measurable, it's it's continuous, it's pleasant, it's you can see it's an explicit strategy, but it's awkward to impose this requirement formally.",
                    "label": 1
                },
                {
                    "sent": "Either you should have compute ability, or you don't have anything.",
                    "label": 0
                },
                {
                    "sent": "Then at all, measurability doesn't play any special role here.",
                    "label": 0
                },
                {
                    "sent": "But but but this strategy for the game theoretical from of large numbers is definitely measurable.",
                    "label": 0
                },
                {
                    "sent": "It's even continuous.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Recent observation is that this approach can be used for designing forecasting algorithms and even more recent observation is that you can use those forecasting algorithms for decision making.",
                    "label": 1
                },
                {
                    "sent": "So for any continuous strategy of skeptic it turns out they exist strategy for forecast.",
                    "label": 0
                },
                {
                    "sent": "There doesn't allow skeptics capital to draw.",
                    "label": 0
                },
                {
                    "sent": "OK, so so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Almost any measure theoretic low probability can be translated into game theoretic terms.",
                    "label": 0
                },
                {
                    "sent": "I will give you a reference at the end to book by Glenn Schaffen myself where we can see them all kinds of measure theoretic, loss of probability and we stayed then in game theoretic terms.",
                    "label": 0
                },
                {
                    "sent": "So it's a very flexible approach.",
                    "label": 0
                },
                {
                    "sent": "You can do a lot using it, but let's see that once you have some continuous strategy for skeptic you can use it for forecasting.",
                    "label": 0
                },
                {
                    "sent": "For now I modify my protocol, so now is a.",
                    "label": 0
                },
                {
                    "sent": "Suppose Katie can do something, for example, skeptic, and enforce the strong law of large numbers.",
                    "label": 0
                },
                {
                    "sent": "Ice is stronger holds, or skeptic becomes infinitely rich, so skeptic has some strategy.",
                    "label": 0
                },
                {
                    "sent": "We know his strategy at the beginning, so this protocol is different because skeptically know his strategy and I actually I don't need to know the full strategy for skeptic in advance at the beginning of each round.",
                    "label": 0
                },
                {
                    "sent": "Cryptic must tell me what his strategy for this particular round is.",
                    "label": 0
                },
                {
                    "sent": "For my new protocol is that reality?",
                    "label": 0
                },
                {
                    "sent": "Announces is usually the datum skeptic announces some continuous function.",
                    "label": 0
                },
                {
                    "sent": "He tells us how he will react to every possible move by forecast.",
                    "label": 0
                },
                {
                    "sent": "So SN Maps the set of all possible moves of the forecast 2R he tells us how many tickets he will buy in response to any price.",
                    "label": 0
                },
                {
                    "sent": "So it's his strategy for this round.",
                    "label": 0
                },
                {
                    "sent": "Forecast announces the actual price for the ticket.",
                    "label": 0
                },
                {
                    "sent": "Realty announces the actual outcome.",
                    "label": 0
                },
                {
                    "sent": "We update the capital in the same way, but instead of so now we can see how many tickets skeptic by Santa VPN, and we multiplied by the payoff from each ticket.",
                    "label": 0
                },
                {
                    "sent": "So it's very similar to that protocol.",
                    "label": 0
                },
                {
                    "sent": "But now I can reach the camera observation is that forecaster has a strategy that ensures once this is continuous that ensures that skeptics capital never grows.",
                    "label": 1
                },
                {
                    "sent": "So for example, in the case of the strong law of large numbers.",
                    "label": 0
                },
                {
                    "sent": "He knows that his his capital will not go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "It will never increase, so it means forecaster can enforce the strong law of large numbers.",
                    "label": 0
                },
                {
                    "sent": "And it's true for any continuous game theoretic low probability.",
                    "label": 0
                },
                {
                    "sent": "And the lot of them.",
                    "label": 0
                },
                {
                    "sent": "So now it's almost trivial.",
                    "label": 0
                },
                {
                    "sent": "So first let me tell you what you should do here.",
                    "label": 0
                },
                {
                    "sent": "So if it's a continuous function.",
                    "label": 0
                },
                {
                    "sent": "So you have this function on the interval 01, so if this function intersects the horizontal axis, you take this as you appear as European.",
                    "label": 0
                },
                {
                    "sent": "So forecast the choosers is the root of this equation.",
                    "label": 0
                },
                {
                    "sent": "Ascent of P is 0, so we can see a sign of Ken will be 0.",
                    "label": 0
                },
                {
                    "sent": "The capital will not change.",
                    "label": 0
                },
                {
                    "sent": "Now if it's completely above.",
                    "label": 0
                },
                {
                    "sent": "If it's completely above, then Y N -- P N will be always positive.",
                    "label": 0
                },
                {
                    "sent": "So do.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this will always be positive so we can make it negative by choosing PN equal to 1.",
                    "label": 0
                },
                {
                    "sent": "And below it's a similar situation, so it's trivial, but it can be greatly generalized.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Instead of this travel intermediate value theorem, you will have to use defence fixed point theorem and you can generalize it to locally convex spaces.",
                    "label": 1
                },
                {
                    "sent": "It's very general result in fact.",
                    "label": 0
                },
                {
                    "sent": "So there is a paper about the generalization.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is this research production.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each problem mentioned it in the previous talks and you you can choose any low probability you like and you can prove the corresponding game theoretic result or find it in our book.",
                    "label": 1
                },
                {
                    "sent": "If you did it and apply theater one so you have some forecasting strategy that enforces that it automatically satisfies this law of probability.",
                    "label": 0
                },
                {
                    "sent": "So is there is a slight problem with this program, becauses it it works too well, so we try it, for example to.",
                    "label": 1
                },
                {
                    "sent": "To design forecast strategies that enforces equal fudge numbers in the same strategy also enforces the strong law.",
                    "label": 0
                },
                {
                    "sent": "Such numbers.",
                    "label": 0
                },
                {
                    "sent": "The important is important for us.",
                    "label": 0
                },
                {
                    "sent": "Part of the law of iterated logarithm and it's.",
                    "label": 0
                },
                {
                    "sent": "So so it didn't.",
                    "label": 0
                },
                {
                    "sent": "Go far, but probably still it's possible.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to look for some other loss of probability, so I will show how to do this for the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A weak law of large numbers.",
                    "label": 0
                },
                {
                    "sent": "So suppose our goal is to forecast to to enforce the law of large numbers.",
                    "label": 0
                },
                {
                    "sent": "He can do it extremely well.",
                    "label": 0
                },
                {
                    "sent": "So actually what he can do several of large numbers tells us that the mean deviation of the actual outcomes from the probabilities is not big and focus.",
                    "label": 0
                },
                {
                    "sent": "That can make sure that this never exceeds 1/2.",
                    "label": 0
                },
                {
                    "sent": "So have forecast that can do it.",
                    "label": 0
                },
                {
                    "sent": "So his first move is 1/2 and then he just he just repeating the previous observation.",
                    "label": 0
                },
                {
                    "sent": "And of course, it's the same.",
                    "label": 0
                },
                {
                    "sent": "Here we have 1/2 instead of the usual the square root of N, for example.",
                    "label": 0
                },
                {
                    "sent": "It's extremely good, but it's clear it's completely useless.",
                    "label": 0
                },
                {
                    "sent": "It's not prediction.",
                    "label": 0
                },
                {
                    "sent": "Something like pose diction you see is actual outcome, and you repeated.",
                    "label": 0
                },
                {
                    "sent": "Of course you will be extremely unbiased.",
                    "label": 0
                },
                {
                    "sent": "Because of this repetition, but it's we want to have something smart since there are some useful words, there are some less useful laws.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we want to have the Wolf Lodge numbers not in our original space, but in some Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the kind of this is a lot.",
                    "label": 0
                },
                {
                    "sent": "I want to have so you can prove that you can call it the convoluted more fudge numbers you take these differences by N minus PN and sketches them in some Hilbert space by multiplying them by some function Phi of PNXN.",
                    "label": 0
                },
                {
                    "sent": "So Fly is a function that Maps each pair forecast and Dayton to some Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "So Angle assumes that you Prem of this.",
                    "label": 0
                },
                {
                    "sent": "Images of the norms of this images doesn't exceed one, so this probability at least one minus Delta.",
                    "label": 1
                },
                {
                    "sent": "This the sum of these scattered differences will not exceed 1 / sqrt N Delta.",
                    "label": 1
                },
                {
                    "sent": "This is much more similar to the standard three cloth large numbers to the standard inequality, and it's it's true both magic theoretically and again theoretically.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Oh no, it's a something.",
                    "label": 0
                },
                {
                    "sent": "It is such a large division inequality is.",
                    "label": 0
                },
                {
                    "sent": "This is what you get from Chebyshev's inequality.",
                    "label": 0
                },
                {
                    "sent": "Yes you can.",
                    "label": 0
                },
                {
                    "sent": "You can have something more sophisticated, but for my purpose this is better than large deviation inequality.",
                    "label": 0
                },
                {
                    "sent": "Becausw I can enforce this with Delta equal to 1 and you from large deviation inequality we will not get a strong.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Results.",
                    "label": 0
                },
                {
                    "sent": "OK, so you you can apply defensive forecasting.",
                    "label": 0
                },
                {
                    "sent": "Approach to this.",
                    "label": 0
                },
                {
                    "sent": "Or probability and what you get exist.",
                    "label": 0
                },
                {
                    "sent": "They exist forecasting strategy so some very explicit forecasting strategy that guarantees that this the norm of the same sum doesn't exceed one of the square root of N. You can tell it take Delta equal to 1.",
                    "label": 1
                },
                {
                    "sent": "Becausw skeptics capital will never increase, so it corresponds that you you can take.",
                    "label": 0
                },
                {
                    "sent": "So something to do is the result you get is.",
                    "label": 0
                },
                {
                    "sent": "Is very strong because of that phenomenon.",
                    "label": 0
                },
                {
                    "sent": "And you can see that in this case, even if Phi is equal to 1, this is still slightly better than the true probabilities for the true probabilities, you would expect the sum to grow as something like the orders and log log N. If you want this to hold for all possible end, this holds for all possible N. You have this inequality for.",
                    "label": 0
                },
                {
                    "sent": "The true probability will never get such a good result.",
                    "label": 0
                },
                {
                    "sent": "So in this sense it's still better than the true probabilities.",
                    "label": 1
                },
                {
                    "sent": "It very much suspect that there is some important aspects of the true probabilities that is not reflected in the loss of privileges that are used here.",
                    "label": 0
                },
                {
                    "sent": "There is a there must be some reason that the true probabilities have this log log.",
                    "label": 0
                },
                {
                    "sent": "But in principle this is.",
                    "label": 0
                },
                {
                    "sent": "This is what I need in this.",
                    "label": 0
                },
                {
                    "sent": "This can be achieved by using some other probabilities.",
                    "label": 0
                },
                {
                    "sent": "Defensive probabilities with 1 / sqrt N. OK, so so let's.",
                    "label": 0
                },
                {
                    "sent": "What was?",
                    "label": 0
                },
                {
                    "sent": "OK so it slows OK this thing to logs.",
                    "label": 0
                },
                {
                    "sent": "What is important here is that, as usual, you don't need to deal with this Hilbert space directly in your.",
                    "label": 1
                },
                {
                    "sent": "The algorithm Zelda isn't depends only on the dot product in the product or in this Hilbert space, only on the kernel.",
                    "label": 0
                },
                {
                    "sent": "So once you have the kernel you have this very explicit algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, this kind of optimal.",
                    "label": 0
                },
                {
                    "sent": "You can make it slightly stronger if less than this sum is less than equal to something more specific.",
                    "label": 0
                },
                {
                    "sent": "So here is not assumed that file doesn't exceed one in absolute values on the realized PNXN so forecast can insurances and reality 2 the part of reality who chooses observation reality one chose the Dayton Realty chose as observation and it can choose.",
                    "label": 0
                },
                {
                    "sent": "Opposite inequality summits.",
                    "label": 0
                },
                {
                    "sent": "Upper and lower bounds are very close here.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me tell you how you can do it.",
                    "label": 0
                },
                {
                    "sent": "Sorry very closely with the same yes.",
                    "label": 0
                },
                {
                    "sent": "The exactly the same, so one can ensure this, so if they're playing, this strategy will have this equality here.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, so so I had it's up about an inch on the previous slide so you have to make it slightly more precise to prove the matching lower bound.",
                    "label": 0
                },
                {
                    "sent": "And I was assuming that this was bounded by one from above.",
                    "label": 0
                },
                {
                    "sent": "Here's another assumption, so so this is always true.",
                    "label": 0
                },
                {
                    "sent": "And for proving decision theoretic results, this bound is important.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you have to use this.",
                    "label": 0
                },
                {
                    "sent": "Please especially fun bound to loss functions.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Becomes important.",
                    "label": 0
                },
                {
                    "sent": "OK, so probably most of you knows know much better than I do what reproducing kernel Hilbert spaces are.",
                    "label": 0
                },
                {
                    "sent": "But let me just.",
                    "label": 0
                },
                {
                    "sent": "You might just have to give the definition for the sake of people who don't know it well, so it reproducing kernel Hilbert space is just some Hilbert space to feel valued functions.",
                    "label": 0
                },
                {
                    "sent": "I will be interested only in functions on my data.",
                    "label": 0
                },
                {
                    "sent": "SpaceX, such searches evaluation functional S continues, so we fix some XF of X is continuous as a function is bounded as a function of F, so the normal space as usual space L2 is not.",
                    "label": 0
                },
                {
                    "sent": "Reproducing kernel Hilbert space, but so believe spaces are because of this derivatives in them.",
                    "label": 0
                },
                {
                    "sent": "So by the standard Ricefish theorem for each.",
                    "label": 1
                },
                {
                    "sent": "For each X you can express the evaluation function, which is continuous as the inner product KX.",
                    "label": 0
                },
                {
                    "sent": "Some element in your space times F. And see if you'll be the supremum of the norms of this KX.",
                    "label": 0
                },
                {
                    "sent": "In my space, I will assume that it's this supremum as finite and the kernel.",
                    "label": 0
                },
                {
                    "sent": "I have it.",
                    "label": 0
                },
                {
                    "sent": "Here is the Colonel in.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the reproducing kernel, Hilbert space can be expressed as in the product between the representatives of X&X prime.",
                    "label": 0
                },
                {
                    "sent": "So the kernel between X&X prime is this in the product.",
                    "label": 0
                },
                {
                    "sent": "OK, so from the team's theorem, you can almost immediately deduce that in any reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "If you find the sum of Y, N -- P, N and you multiply each Y N -- P N by some function in your reproducing kernel Hilbert space.",
                    "label": 1
                },
                {
                    "sent": "This will never exceed.",
                    "label": 0
                },
                {
                    "sent": "This song will never exceed this proficiency.",
                    "label": 0
                },
                {
                    "sent": "F. The maximum of the norms of KX times is the norm of F, so if the norm of F is large, this can be large as well, but this average doesn't exceed this product over the square root of N. It's true for all N&F.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's easily follows and intuitively this tells you that the forecasting strategy is well calibrated and has high resolution, so I don't want to explain this words, But here if you don't have this, it doesn't mean anything.",
                    "label": 0
                },
                {
                    "sent": "You can achieve this very, very simply, but with this term it becomes much more meaningful and much more useful.",
                    "label": 1
                },
                {
                    "sent": "You can really use it in.",
                    "label": 0
                },
                {
                    "sent": "Decision.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, again, various assemble optimality result you can you have to improve this slightly replacing end by this sum of PN 1 -- P N and you can prove the matching lower bound.",
                    "label": 0
                },
                {
                    "sent": "So there is no slack at this point, so so let me give you some example or examples of reproducing kernel Hilbert spaces into examples.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I like most someone example I mentioned already.",
                    "label": 0
                },
                {
                    "sent": "This Sobolev norm squared mean plus the the mean squared derivative.",
                    "label": 0
                },
                {
                    "sent": "So the kernel for this space is given by this strange expression.",
                    "label": 0
                },
                {
                    "sent": "So this result was obtained by Craven and War but in 1979 so so we can actually do all computations with this for this norm using this kernel.",
                    "label": 0
                },
                {
                    "sent": "And but here you can see this kernel is on the interval 01.",
                    "label": 0
                },
                {
                    "sent": "Well, what you can do if your data are not bound?",
                    "label": 0
                },
                {
                    "sent": "You don't know you don't have any a purity bound, so we can use instead.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is Norm you take the integral from minus Infinity to Infinity F squared plus the integral of the derivative squared?",
                    "label": 0
                },
                {
                    "sent": "And here's the kernel.",
                    "label": 0
                },
                {
                    "sent": "It looks completely different, so the norm looks similar, but the kennel is different and it's it's a relatively recent result 1996 by something young.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a in 1D, but if you are interested in many attributes you can use, for example, tensor products of these spaces.",
                    "label": 0
                },
                {
                    "sent": "Or you can use send plate splines which are very popular as well.",
                    "label": 0
                },
                {
                    "sent": "There are hundreds of different reproducing kernel Hilbert spaces, so usually this relation between the norm in the kernel is far from travel, so it's kind of duel and usually.",
                    "label": 0
                },
                {
                    "sent": "In your algorithms you need kernels to do computations, but in mathematics for analysis you need nodes.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's yes.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, thanks for this remark.",
                    "label": 0
                },
                {
                    "sent": "So it might be trivial, but.",
                    "label": 0
                },
                {
                    "sent": "It didn't look so.",
                    "label": 0
                },
                {
                    "sent": "Actually this is something that I got completely independently from Trayvon and Robin.",
                    "label": 0
                },
                {
                    "sent": "For me, it was far from trivial.",
                    "label": 0
                },
                {
                    "sent": "It was several pages of computations, but I'm sure it was because of my ignorance even use something more about full year transform that would have been sampling.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have another research program, so the previous problem kind of failed.",
                    "label": 0
                },
                {
                    "sent": "Maybe not completely, but I had just one algorithm in the previous program, so this problem is for decision making.",
                    "label": 0
                },
                {
                    "sent": "So first you choose a goal that could be achieved if you use a true probabilities.",
                    "label": 1
                },
                {
                    "sent": "So imagine you have you know the true probabilities and you can achieve this goal.",
                    "label": 0
                },
                {
                    "sent": "Now you and you should be able to prove that you can achieve this goal.",
                    "label": 0
                },
                {
                    "sent": "So we construct some decisions strategy which probably achieve achieve your goal and the proof yes.",
                    "label": 0
                },
                {
                    "sent": "Or if I don't need this formally, just some some probabilities that reality is using.",
                    "label": 0
                },
                {
                    "sent": "Maybe there are no true probabilities, but let's let's imagine there are some true probabilities.",
                    "label": 0
                },
                {
                    "sent": "So of course this theory doesn't assume any true probabilities.",
                    "label": 0
                },
                {
                    "sent": "OK, so now of course you proof will depend on some rules of probability.",
                    "label": 0
                },
                {
                    "sent": "It depends if it depends on several laws you have to merge this loss into one more so you you isolate some continuous low probability game theoretical probability on, which approve depends and then.",
                    "label": 1
                },
                {
                    "sent": "You don't eat through the true probabilities at this point.",
                    "label": 0
                },
                {
                    "sent": "You remember you don't know them, but you don't need them.",
                    "label": 0
                },
                {
                    "sent": "You have this method of defensive forecasting that can produce probabilities that are as good as the true probabilities as far as this continuous flow of probability is concerned.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the idea.",
                    "label": 0
                },
                {
                    "sent": "So this idea might sound too ambitious and even.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The impossible, so let me talk briefly about the possible goal.",
                    "label": 0
                },
                {
                    "sent": "So suppose your goal is moderate, and so it's not easy to come up with interesting goals.",
                    "label": 0
                },
                {
                    "sent": "Example goal cannot be.",
                    "label": 0
                },
                {
                    "sent": "I want to become a millionaire in 10 years, where your goal should be achievable regardless of the true probabilities.",
                    "label": 1
                },
                {
                    "sent": "First, your goal should be in terms of observables.",
                    "label": 0
                },
                {
                    "sent": "Your goal should not involve the true probabilities themselves.",
                    "label": 0
                },
                {
                    "sent": "It should be something that you depends.",
                    "label": 0
                },
                {
                    "sent": "And what you can see in the real world.",
                    "label": 0
                },
                {
                    "sent": "But your goal cannot be.",
                    "label": 0
                },
                {
                    "sent": "I want to be coming with your name 10 years, because maybe you live in a bad world.",
                    "label": 1
                },
                {
                    "sent": "Maybe it's absolutely impossible.",
                    "label": 0
                },
                {
                    "sent": "So the goal should be relative.",
                    "label": 0
                },
                {
                    "sent": "It seems so if it's if there is not very complicated strategy of becoming a millionaire, you should become one.",
                    "label": 0
                },
                {
                    "sent": "So not very complicated as the norm should should not be too large in some in some space.",
                    "label": 0
                },
                {
                    "sent": "So it's something vague.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At this point.",
                    "label": 0
                },
                {
                    "sent": "OK, so now how does this reduction work?",
                    "label": 0
                },
                {
                    "sent": "So let's so I had this most function, Lambda one gamma and Lambda, zero gamma and now introduce also it's average.",
                    "label": 0
                },
                {
                    "sent": "So Lambda P gamma is the expected loss if the probability is that why is one is equal to P?",
                    "label": 0
                },
                {
                    "sent": "So your expectation when the when the probability is P. Of course you don't know P, but the function is useful and if you do.",
                    "label": 0
                },
                {
                    "sent": "P then you could visit given value of P. You could minimize your loss function with respect to.",
                    "label": 0
                },
                {
                    "sent": "So let me assume that it's convex and nice.",
                    "label": 0
                },
                {
                    "sent": "At this point it's not necessary, but.",
                    "label": 0
                },
                {
                    "sent": "It will be simpler, so G of P. It's some decision.",
                    "label": 0
                },
                {
                    "sent": "Some optimal decision under this probability, AP geography is your choice function.",
                    "label": 0
                },
                {
                    "sent": "So for the square and blood loss functions you can take care of people to PSA they called propes, causing rule.",
                    "label": 1
                },
                {
                    "sent": "You don't need to change P at all, you just can use your peers.",
                    "label": 1
                },
                {
                    "sent": "Is your decision is absolute loss function.",
                    "label": 0
                },
                {
                    "sent": "For example it's different.",
                    "label": 0
                },
                {
                    "sent": "There are all kinds of loss functions.",
                    "label": 0
                },
                {
                    "sent": "So I will say that the exposure of some decisions rule of some decision it first.",
                    "label": 0
                },
                {
                    "sent": "So is the loss of this decision rule.",
                    "label": 0
                },
                {
                    "sent": "If one happens minus its rules.",
                    "label": 0
                },
                {
                    "sent": "If zero happens, so it's it's exposure to risk and I will need to functions the exposure of my choice function gamma.",
                    "label": 1
                },
                {
                    "sent": "It's a function of P just for each P. It gives us the exposure of this decision GOP.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I will also need the exposure of decision rule.",
                    "label": 1
                },
                {
                    "sent": "So it's a function of X now, and for each X we get the exposure of the decision recommended by this decision rule.",
                    "label": 0
                },
                {
                    "sent": "So to show you have to prove the kind of results I mentioned because I'm not going to give you the actual proof, I will prove something very informal, but it will be clear from from this how the proofs work.",
                    "label": 0
                },
                {
                    "sent": "So I will show that the decision output using this empirical loss minimization principle.",
                    "label": 0
                },
                {
                    "sent": "Probability P. You find the best decision gamma for this probability, where at the end are output by the algorithm of large numbers with some universal kernel, for example this or believe Colonel I mentioned before this satisfy the average loss.",
                    "label": 0
                },
                {
                    "sent": "The average loss of this strategy doesn't exceed doesn't exceed much, so this is some informal notation doesn't exceed much.",
                    "label": 1
                },
                {
                    "sent": "The average loss of any decision rule with a small exposure and exposure is measured by its subliminal.",
                    "label": 0
                },
                {
                    "sent": "So it is important, of course, the decision rule itself.",
                    "label": 0
                },
                {
                    "sent": "It takes values in some strange space in general, in some decision space, but the exposure takes values.",
                    "label": 0
                },
                {
                    "sent": "NVIDIA line.",
                    "label": 0
                },
                {
                    "sent": "OK so this is the informal quality and how.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you can prove it, so would kind so so this if you want to prove this.",
                    "label": 0
                },
                {
                    "sent": "It's clear that if you have the true probabilities, the empirical loss minimization is the right thing to do if the another true probabilities you are going to perform as well as any other strategy.",
                    "label": 0
                },
                {
                    "sent": "Since this is a true probabilities, but what you should do if these are not with the true probability, you have to use the law of large numbers somehow.",
                    "label": 0
                },
                {
                    "sent": "So so formal you can.",
                    "label": 0
                },
                {
                    "sent": "Just some very simple manipulation shows that the deviation of your actual loss from your expected loss is equal to the exposure of your decision of your decision.",
                    "label": 0
                },
                {
                    "sent": "Gamma Times the deviation of your actual outcome from the expected outcome, so it's it's very simple and the law of large numbers numbers at least is that was stated earlier.",
                    "label": 0
                },
                {
                    "sent": "Do you remember there was something that?",
                    "label": 0
                },
                {
                    "sent": "Is this some of Y N -- P N times something XNPN this was was not large and this is exactly what we need here.",
                    "label": 0
                },
                {
                    "sent": "This function F is this exposure so that more is exactly what is needed in this decision theoretic result.",
                    "label": 0
                },
                {
                    "sent": "So let's see how it works.",
                    "label": 0
                },
                {
                    "sent": "So in conjunction with that with forecasting.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Result if you look at the actual loss of this decisiones computed from the.",
                    "label": 0
                },
                {
                    "sent": "Forecasts PN.",
                    "label": 0
                },
                {
                    "sent": "You can try it.",
                    "label": 0
                },
                {
                    "sent": "This actually looks can be represented as expected loss.",
                    "label": 0
                },
                {
                    "sent": "Having eaten by expected I mean of course conditional conditional one step ahead expectations.",
                    "label": 0
                },
                {
                    "sent": "So you take the expected loss plus the difference between the actual loss and the expected loss here.",
                    "label": 0
                },
                {
                    "sent": "And this difference.",
                    "label": 0
                },
                {
                    "sent": "We know it's small by the law of large numbers we know that you can you have PN for which this difference is small.",
                    "label": 0
                },
                {
                    "sent": "So this doesn't exceed the expected loss and the expected loss.",
                    "label": 0
                },
                {
                    "sent": "Because of the choice, so G of PN is the best possible decision.",
                    "label": 0
                },
                {
                    "sent": "The decisions that gives.",
                    "label": 0
                },
                {
                    "sent": "The minimum to this expression you know this doesn't exceed.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lambda PN in here any other decision?",
                    "label": 0
                },
                {
                    "sent": "So we take the decision taken by some decision rule.",
                    "label": 0
                },
                {
                    "sent": "It doesn't exceed this and now for this decision tools we can do the same thing.",
                    "label": 0
                },
                {
                    "sent": "So here we have the expected loss but we are not interested in the expected, mostly interested in the deal is actual loss, so we can try the actual loss minus the difference is the difference is small.",
                    "label": 0
                },
                {
                    "sent": "This doesn't exceed the actual loss of the decision rule.",
                    "label": 0
                },
                {
                    "sent": "So let's understand that scheme.",
                    "label": 0
                },
                {
                    "sent": "So the summary of the proof.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A technique is here, so we want to prove that the actual loss of our strategy doesn't exceed the actual loss of some strategy.",
                    "label": 1
                },
                {
                    "sent": "We want to compete with.",
                    "label": 1
                },
                {
                    "sent": "So now we know that the actual loss of our strategy is approximately equal to the expected loss.",
                    "label": 1
                },
                {
                    "sent": "The sum of Lambda, PN of GP and this is less than equal to the expected loss of the alternative strategy, and this is again approximately equal to the actual loss.",
                    "label": 0
                },
                {
                    "sent": "So if you want to put to get some really strong results, it's better to split these two steps to have some kind of loss of some kinds of weak laws of large numbers that tailed to what you really need, not the general result I stated earlier.",
                    "label": 0
                },
                {
                    "sent": "But let me tell you what you can.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you know these are special cases of some very gentle theorem which is applicable to all convex loss functions, and they can be unbounded if they're unbounded.",
                    "label": 0
                },
                {
                    "sent": "They shouldn't have very sick tales.",
                    "label": 0
                },
                {
                    "sent": "Bridal stated in special cases not introduced some general terminology, so if your loss function is quadratic, I will do this for some specific loss functions.",
                    "label": 1
                },
                {
                    "sent": "They will tell you what the precise constants I haven't proved that these constants are optimal, but there is no.",
                    "label": 0
                },
                {
                    "sent": "I can't see much slack in what I did I would be very much surprised if they could be improved at least by much.",
                    "label": 0
                },
                {
                    "sent": "OK, so here I'm using the 2nd.",
                    "label": 1
                },
                {
                    "sent": "So believe Norm that I mentioned this, I believe norm on the full line minus Infinity Infinity, so the loss of the decision maker doesn't exceed the losses.",
                    "label": 0
                },
                {
                    "sent": "The cumulative loss of the alternative strategy plus 2D minus one this centered.",
                    "label": 0
                },
                {
                    "sent": "Sent a decision rule.",
                    "label": 0
                },
                {
                    "sent": "This is measured sub non plus 1 * 3 / 8 some reasonable constant, reasonably small constant times the square root of N. So it's true for all NND.",
                    "label": 1
                },
                {
                    "sent": "And if your losses absolute loss, this is something very similar.",
                    "label": 0
                },
                {
                    "sent": "Only this concept will be very slightly worse.",
                    "label": 1
                },
                {
                    "sent": "It will be sqrt 6 / 4 and you have still to D -- 1 actually to D -- 1 will be the exposure.",
                    "label": 0
                },
                {
                    "sent": "And is this loss functions of the decision Rule D. Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, very similar.",
                    "label": 0
                },
                {
                    "sent": "Only the constants are slightly different.",
                    "label": 0
                },
                {
                    "sent": "6.",
                    "label": 0
                },
                {
                    "sent": "Probably not.",
                    "label": 0
                },
                {
                    "sent": "I've never notice it.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Actually, I did convey yesterday made sure that this is largely this, but my analysis was not as deep as yours.",
                    "label": 0
                },
                {
                    "sent": "Insight.",
                    "label": 0
                },
                {
                    "sent": "To investigate it.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "'cause OK, So what you get from the log loss function?",
                    "label": 1
                },
                {
                    "sent": "So this function is is unbounded in front bounded functions.",
                    "label": 0
                },
                {
                    "sent": "The formal condition in this theorem is that the tails should should decay fast enough and for for the Lobos functions stays.",
                    "label": 0
                },
                {
                    "sent": "The details decay exponentially fast and each of the minus T and what is required is that they should decay faster than 1 / T. So it's it's well within the.",
                    "label": 1
                },
                {
                    "sent": "Domain of applicability of this resolved, so here is the exposure will be slightly different.",
                    "label": 0
                },
                {
                    "sent": "You should take log odds ratio block of D / 1 -- D and again.",
                    "label": 0
                },
                {
                    "sent": "Sorry it should be as prime.",
                    "label": 0
                },
                {
                    "sent": "I use the same.",
                    "label": 0
                },
                {
                    "sent": "So here you have the loss.",
                    "label": 0
                },
                {
                    "sent": "This overhead is oh point 7 but of course it's not directly comparable to the previous results.",
                    "label": 0
                },
                {
                    "sent": "You have a different exposure here.",
                    "label": 0
                },
                {
                    "sent": "Look at the ratio.",
                    "label": 0
                },
                {
                    "sent": "OK, so this result can in fact be extended to non convex loss functions.",
                    "label": 1
                },
                {
                    "sent": "All those functions that depend on several future observations.",
                    "label": 0
                },
                {
                    "sent": "I think it's very interesting, but it has hasn't been done yet, but it's it's working progress.",
                    "label": 0
                },
                {
                    "sent": "It seems it gives you some.",
                    "label": 0
                },
                {
                    "sent": "Opportunities this proof.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Technique, so this is a reference to our book.",
                    "label": 0
                },
                {
                    "sent": "There is nothing about defensive forecasting in this book.",
                    "label": 0
                },
                {
                    "sent": "We for some reason we didn't realize when we were writing this book that you can use it for forecasting because a lot of of theorems of loads of probabilities in this book like the Central Limit Theorem, the logs, iterated logarithm, someone sided forms of the central Limit theorem we don't have.",
                    "label": 0
                },
                {
                    "sent": "Audience inequality for some reason as well, maybe because you were interested in classical limit theorems.",
                    "label": 0
                },
                {
                    "sent": "And if you want to read more about defensive forecasting, so there are some working papers on the website for this book, so it's probability and finance without any.",
                    "label": 1
                },
                {
                    "sent": "Punctuation here and there.",
                    "label": 0
                },
                {
                    "sent": "Working paper 10 is about the generalization of this existence result to any Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "15 is about defensive forecasting and 15 is about applications to decision making.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "I believe they are, but I don't have any proofs.",
                    "label": 0
                },
                {
                    "sent": "No, no it doesn't.",
                    "label": 0
                },
                {
                    "sent": "General research program #2.",
                    "label": 0
                },
                {
                    "sent": "You look at what is the law of probability, yes.",
                    "label": 0
                },
                {
                    "sent": "General rule of probability that is involved implicitly, but do you ever?",
                    "label": 0
                },
                {
                    "sent": "Get for any decision problem.",
                    "label": 0
                },
                {
                    "sent": "Do you get?",
                    "label": 0
                },
                {
                    "sent": "So do you get?",
                    "label": 0
                },
                {
                    "sent": "The law of large numbers in some form or another.",
                    "label": 0
                },
                {
                    "sent": "There must be some very important clue.",
                    "label": 0
                },
                {
                    "sent": "I'm sure that.",
                    "label": 0
                },
                {
                    "sent": "Is difference between the probabilities in the forecast times something?",
                    "label": 0
                },
                {
                    "sent": "It looks at something very universal, yes?",
                    "label": 0
                },
                {
                    "sent": "Number.",
                    "label": 0
                },
                {
                    "sent": "But you said there should be cases where you get something.",
                    "label": 0
                },
                {
                    "sent": "Different, you know.",
                    "label": 0
                },
                {
                    "sent": "I have bone disease, indirect evidence that there must be such cases because so this is the difference is small.",
                    "label": 0
                },
                {
                    "sent": "It's some reflection of calibration and resolution.",
                    "label": 0
                },
                {
                    "sent": "There seems to be very, very fundamental properties of any algorithm, But my probabilities uncalibrated much better than the true probabilities.",
                    "label": 0
                },
                {
                    "sent": "And so I think it can mean only one thing.",
                    "label": 0
                },
                {
                    "sent": "That calibration in resolution is also better, at least in my from formalisation.",
                    "label": 0
                },
                {
                    "sent": "And I think it can mean only one thing.",
                    "label": 0
                },
                {
                    "sent": "So there are some other important properties that can also be used for decision making in this.",
                    "label": 0
                },
                {
                    "sent": "So this is actually a minus.",
                    "label": 0
                },
                {
                    "sent": "You can't achieve the square root of N, you can only achieve the square root of log log and the reason for it must be that the true probabilities have some other properties we don't know yet, or I don't know yet.",
                    "label": 0
                },
                {
                    "sent": "But it's possible.",
                    "label": 0
                },
                {
                    "sent": "Nobody knows those properties.",
                    "label": 0
                },
                {
                    "sent": "Some useful properties that you can use for decision making, but I have no idea what those properties can be.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "She's making her strategy that gives these guarantees, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes they are very easy.",
                    "label": 0
                },
                {
                    "sent": "They are very simple and easy.",
                    "label": 0
                },
                {
                    "sent": "I haven't done much empirical work.",
                    "label": 0
                },
                {
                    "sent": "I need some empirical work, but his defensive forecasting not with this decision making algorithms so.",
                    "label": 0
                },
                {
                    "sent": "In these cases, the strategies are extremely simple and efficient.",
                    "label": 1
                },
                {
                    "sent": "There's there fast, but it's only because you're you're.",
                    "label": 0
                },
                {
                    "sent": "You know that your observation will be between zero and one.",
                    "label": 0
                },
                {
                    "sent": "It can be in between any other bounds, but already in the multiclass case.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to find a fixed point efficiently, and it's a big problem.",
                    "label": 0
                },
                {
                    "sent": "I know there is some big industry in economics about it, but I haven't had time to study it carefully, but it's it's it's travel.",
                    "label": 0
                },
                {
                    "sent": "In this case is.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah, yes it does.",
                    "label": 0
                },
                {
                    "sent": "So when I was talking about efficiency meant there is nothing like so, it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's a small degree polynomial.",
                    "label": 0
                },
                {
                    "sent": "It's it's not exponential or something.",
                    "label": 0
                },
                {
                    "sent": "You can do it for reasonable size of some datasets.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "The shape of your.",
                    "label": 0
                },
                {
                    "sent": "Pacifier.",
                    "label": 0
                },
                {
                    "sent": "Let me try to remember how it works.",
                    "label": 0
                },
                {
                    "sent": "Of course, the decision making algorithms have to achieve to go smooth Tanis Lee.",
                    "label": 0
                },
                {
                    "sent": "Do remember those two items so one is that for the decision makers, actual loss should be close to the expected losses.",
                    "label": 0
                },
                {
                    "sent": "This should be true for the decision rule.",
                    "label": 0
                },
                {
                    "sent": "These are two different rules and I have to merge these two loss so sometimes it's slightly more complicated.",
                    "label": 0
                },
                {
                    "sent": "Talk about one of these so when you want it is just about defensive forecasting.",
                    "label": 0
                },
                {
                    "sent": "So when you want to find the prediction PN 4 YN so you find the sum of your kernel you you compute your kernel.",
                    "label": 0
                },
                {
                    "sent": "Any kernel between P&X and, PIXI.",
                    "label": 0
                },
                {
                    "sent": "Your thumb is from one to N. And this is multiplied by.",
                    "label": 0
                },
                {
                    "sent": "Let me see whi I minus why this should be equal.",
                    "label": 0
                },
                {
                    "sent": "Why I minus Pi?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Is this what you do in practice?",
                    "label": 0
                },
                {
                    "sent": "So he the proof there were two cases when applying intersect the horizontal axis and when it's not so this is the case when it does intersect.",
                    "label": 0
                },
                {
                    "sent": "Is this in practice?",
                    "label": 0
                },
                {
                    "sent": "You can still only such cases maybe, except for the first trial or something degenerate.",
                    "label": 0
                },
                {
                    "sent": "So here you have some kernels that measures how you knew examples, so you are shown XN.",
                    "label": 0
                },
                {
                    "sent": "So sorry, you teachers pienso that this is true, this is.",
                    "label": 0
                },
                {
                    "sent": "Haha you choose APN so if you look at your current situation PNXN and you compare to what you had in the past and you look only at the trials in the past when you're.",
                    "label": 0
                },
                {
                    "sent": "That's right, sorry, that's right, yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so here you look only.",
                    "label": 0
                },
                {
                    "sent": "The trials that are similar to you last trial and you you require this unbiasedness for those trials.",
                    "label": 0
                },
                {
                    "sent": "This is how you choose the PN.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "But you can do it efficiently.",
                    "label": 0
                },
                {
                    "sent": "In this case it's quite fast.",
                    "label": 0
                },
                {
                    "sent": "I can feel.",
                    "label": 0
                },
                {
                    "sent": "This message is sent to the cheerleading Max, maybe?",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, of course.",
                    "label": 0
                },
                {
                    "sent": "It's yes.",
                    "label": 0
                },
                {
                    "sent": "Wait?",
                    "label": 0
                },
                {
                    "sent": "Yes, I had exactly the same feeling and so if you call this game theoretic foundations of probability, but we thought it wasn't connected to the normal game theory, it was about some theory of perfect information games.",
                    "label": 0
                },
                {
                    "sent": "Something in the foundations of the semantics, but it seems there must be some connections with which those people are doing some equilibra, some mini Max.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "I'm sure there's a connection, but you can't say much about it now.",
                    "label": 0
                }
            ]
        }
    }
}