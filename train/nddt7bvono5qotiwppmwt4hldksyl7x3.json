{
    "id": "nddt7bvono5qotiwppmwt4hldksyl7x3",
    "title": "An Incremental Subgradient Algorithm for Approximate MAP Estimation in Graphical Models",
    "info": {
        "author": [
            "Jeremy Jancsary, Microsoft Research, Cambridge, Microsoft Research"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_jancsary_isa/",
    "segmentation": [
        [
            "OK, so thanks for the introduction.",
            "This is joint work with my 2 devices.",
            "GAIL Mattson, hydroseed."
        ],
        [
            "OK, so since time is limited today, figured it makes sense to state a few important things.",
            "First.",
            "First of all, why do you think you might be interested in our algorithm for maximum posteriori estimation?",
            "Well, first of all, it is efficient both computationally and in terms of memory consumption.",
            "Second, it finds better solutions than the methods we compared it to.",
            "And third, it converges to the global optimum of the 1st order LP relaxation.",
            "So we actually know what we're optimizing, which is a good thing here.",
            "And finally, you also get the certificate of Optimality.",
            "So if we actually find the global optimum of the discrete problem, we will know that.",
            "Good, and since it's easy to get lost in technical details, just a rough outline of what we're doing this talk.",
            "First, start out with the tree related upper bound by Wainwrights and others.",
            "We heard about that before.",
            "This upper bound is developed until it assumes a particular degenerate form that involves only a large number of easy problems, and the tightest such bound can then be found very efficiently using incremental methods which requires only to solve 1 easy problem at a single time.",
            "And as we will see, this is equivalent to maximizing the LP relaxation of the map problem."
        ],
        [
            "Good so."
        ],
        [
            "We heard this before, but I'm still going through it to introduce the notation that we'll be using.",
            "What is maximum posteriori estimation?",
            "Well, we consider an undirected graphical model.",
            "Like this one so."
        ],
        [
            "If you've never seen one before, this is it basically.",
            "So the circles here correspond to variables and the edges between them denotes interactions going on between those variables."
        ],
        [
            "And the graphical models we consider here are undirected, and I assume that the random variables are discrete and we have pairwise interactions between them.",
            "So this is mainly to facilitate presentation.",
            "We can generalize the algorithm to general factors.",
            "Good, in this case, the potential of a particular joint variable state factors into some of.",
            "Local potentials and sum of interaction potentials cheated by Theta SC here.",
            "Good and map problem is then basically to find.",
            "The maximizing joint assignment or state which maximizes the scoring function here.",
            "Now the problem is that in general, computation of the quantities is NP hard, with a few notable exceptions such as trees or graphs over binary variables and submodular energies.",
            "So that multi weights need for approximations.",
            "We need some tractable approximation we can solve."
        ],
        [
            "And one principled way of doing this is for instance the tree related upper bound by Wainwright, which we can minimize to find the tightest upper bound, and basically this tree related upper bound is just a convex combination over spanning trees, overall spanning trees of a graph.",
            "And as you can see here is just a weighted sum of the map scores of the trees.",
            "And in that framework we can minimize over these tractable reparameterization's here, which are constrained to belong to a certain convex set, which ensures that the weighted sum of this.",
            "Tractable parameters here sums to the target parameter.",
            "Yeah, the coefficients.",
            "Basically our constraint to belong to the simplex of probability distributions.",
            "And the interesting thing about this tree rated upper bound now is that it is connected through strong duality to the 1st order LP relaxation.",
            "We heard about that one before.",
            "So we can solve either formulation to obtain the same global optimum.",
            "Now there will be.",
            "Relaxation is very similar to the definition of the map problem.",
            "The only difference here is really that we relax the parameters mute to belong to the local polytope.",
            "Such fractional solutions are admitted.",
            "This turns the industrial linear programming problem into an LP problem.",
            "Yeah, and what we doing here is will be minimizing the tree related upper bound rather than the LP relaxation."
        ],
        [
            "And the first step will be doing here is.",
            "I'll try to find simpler formulation of this upper bound such that we can minimize it more efficiently.",
            "And there is one curious fact associated with that Re rated upper bound, and that is that the choice of RO is irrelevant as long as all edges are covered with nonzero probability.",
            "And.",
            "That all edges have to be covered can easily be seen because otherwise the convex set would be empty because there would be no way for the potential for the parameters to sum up to the target parameter.",
            "Good and now.",
            "Minimization of the created upper bound is equivalent to maximization of the LP relaxation, which does not depend on the road.",
            "So it is either the seen that in turn minimization of that related upper bounds also does not depend on row.",
            "Good, what this means is that we can actually pick a small set of trees that are needed to cover all edges and set.",
            "The corresponding coefficients to a common constant drew.",
            "And set the other coefficients to 0.",
            "This ensures that all edges are covered to be good.",
            "Further on that, we can exploit the linearity of our scoring function and move through into the parameters by defining a new set of parameters Lambda, which is just a scaled version of our tractable parameter, Theta.",
            "And by doing so we obtain this dual decomposition like formulation which is very similar to the formulation of Como dockus.",
            "So what have we gained here?",
            "Actually, the gain is that we sum over a small set of spanning trees rather than the set of all spanning trees of a graph, which makes minimization of this sum here feasible."
        ],
        [
            "But in fact there is another curious fact Associated 3 rated upper bounds and char knowledge.",
            "This was first explicitly noted by Kolmogorov in 2006, and that fact is that the trees in the tree related upper bound need not be spending.",
            "That is to say that.",
            "The tightest bound is not impacted by the choice of trees, so our one of our central ideas in this paper is now to choose each tree as a single edge.",
            "And one benefit of this is that it determines almost all parameters, because the edge parameters obviously must correspond to the target parameters of the given edge.",
            "And for the other nodes and edges which do not belong to that particularly generated tree, well, these parameters must be 0.",
            "So the remaining parameters of an edge are simply parameters for the two variables which are contained in this edge.",
            "And.",
            "If we take all these changes together, we end up at this new formulation, which I call the tightest degenerate upper bound problem.",
            "And this is basically the previous formulation, except that we have here a sum over edges and I expanded the maximum posteriori score here.",
            "To actually reflect its definition for the edge rather than for a tree and for an edge, we can very easily find the map score.",
            "We just need to sum over all possible edge states.",
            "So previously for formulation of, Dawkins and others, for instance, we would have required maximum product belief propagation to compute that term here.",
            "So whatever gains here, we have a problem which is defined in terms of a large number of rather easy problems and also the number of parameters has decreased significantly."
        ],
        [
            "So how can we tighten this upper bound?",
            "Now first of all, this dual objective which we want to minimize is settling on differentiable.",
            "But still we can obtain a subgradient and the components of that subgradient are precisely one for those components which correspond to.",
            "Two variable stage states which are consistent with the edge map state and serial otherwise.",
            "And another ingredient we can use for efficient minimization here is that an efficient projection operator is available to us.",
            "For projection we want.",
            "Let's say we have given infeasible point Lambda prime.",
            "Then we want to find the feasible point Lambda which is closest to Lambda prime in the Euclidean sense.",
            "And actually a closed form solution can be obtained to this problem.",
            "And that's a bit unwieldy term here, but what it essentially does is that it redistributes.",
            "The amount of change which is then to parameter over the change in adjacent edges of a particular variable whose parameters were changed.",
            "So the second center of the year of ours is then that since we have a separable problem with a large number of component functions and the problem is non differentiable.",
            "But we have cheap projection available, we can use the incremental subgradient method by nation particles which was introduced in 2001."
        ],
        [
            "And if you apply that algorithm to our problem, we come up with a rather simple algorithm which is catched here.",
            "So what we basically do is.",
            "At each outer iteration, we first pick a step size Alpha.",
            "And we shuffle the set of edges.",
            "And then for each edge, we first find the edge map state.",
            "We subtract the sparse subgradient.",
            "And we perform projection for the variable parameters containing that edge.",
            "All of this can be done very efficiently.",
            "So after those updates we built a candidate primal solution, which is a feasible solution to our discrete problem, and we do that by choosing the components for each variable at random from the edge map states.",
            "Of those edges which contain the variable here.",
            "So we maintain actually the best primal solution found so far.",
            "So if the solution is better, we just pick that one, and otherwise we discard it.",
            "And now one interesting thing can happen here.",
            "And that is if the dual objective and the primal objective coincide.",
            "This means that we've actually found an optimal primal solution of the discrete problem.",
            "But that may not happen because they appear relaxation need not be tight, and in that case we just run the algorithm until convergence and return the best approximate primal solution found so far."
        ],
        [
            "So."
        ],
        [
            "Just like to mention a couple of formal properties of our algorithm.",
            "The first one is that for appropriate chosen sequence of steps losses, convergence to the global optimum or optimization problem is guaranteed in the limit.",
            "So now the choice of Alpha case.",
            "A bit tricky.",
            "There are a lot of stepsize schedules which guarantee global conversions, but unfortunately they can be rather slow in practice.",
            "So what we did is we applied some heuristic which decreases alfaqui rather aggressively in the beginning and once once Alpha drop below a certain threshold, we.",
            "Switch over to conversion scheme, basically unknown summable diminishing sequence, and this turned out to work very well for a large variety of graphs in practice, but you can find the details in the paper so and the second proposition of our algorithm is regards the optimality of primal solutions.",
            "I already talked about that when I presented the algorithm.",
            "Basically, if the dual and primal coincide, it follows that we found the true optimum of the discrete problem, and this happens precisely for each node.",
            "The edge map States and agree on a common node map state, and this is really just an application of the strong tree agreement condition of rain, right?",
            "And others to our degenerate case here."
        ],
        [
            "Good, so there are lots of different approaches which are all aimed at solving the LP relaxation, so I figured it would make sense to give a little feature matrix here.",
            "Which compares some formal properties.",
            "So our method is here in Kampe.",
            "And the first column I indicate whether the algorithm is guaranteed to converge to some fixed point.",
            "The second column contains whether convergence of the global optimum is guaranteed.",
            "In the third column indicated, the asymptotics, conversion rate, convergence rate as far as is known to me, and the last one indicated there.",
            "Synthetic memory requirements in terms of the variable cardinality.",
            "So as you can see, there are quite a bit quite a few differences here, and our algorithm compares rather favorably, except for the asymptotics convergence rate, so there are some recent developments here by.",
            "Ravikumar and you each, for instance, which have linear, superlinear rates there based on approximate point methods, but in general each iteration here must be expected to be significantly slower than the iterations of our algorithm, so this hasn't daughter creates do not really translate into practical performance.",
            "That's what I'm trying to say here."
        ],
        [
            "Good."
        ],
        [
            "So little couple of experiments to really assess the empirical performance of our algorithm.",
            "And we compared three solvers our own the dual decomposition scheme of Como Dockus, Wichita.",
            "Nobody's up here and the tree related message passing algorithm of Wainwright and others, and we applied these three different graphs.",
            "First 50 by 50 grid with binary variables and the potential chosen as an easing grid with the non deterministic part taken uniformly between minus one and plus one.",
            "Second here.",
            "A 20 by 20 grid with variables severity 16 and potentials chosen as Theta is zero and interaction potentials drawn from a Gaussian distribution with zero mean and variance 15.",
            "And finally choose a complete graph here of consisting of 50 binaries and put Jennifer's potentials chosen as in the for the 1st grade."
        ],
        [
            "Good, so here you can see the results.",
            "What we actually did is we measured this core of the best primal solution found as a function of running time.",
            "So the plot here is not in terms of asymptotic Inter iterations, but in terms of running time, right?",
            "And for our algorithm, and for DDS up, we constructed the primal solution candidates randomly from the edge or three map states, respectively, and with its at each iteration and for tree related message passing.",
            "We used the maximizers of the node beliefs at each iteration and as you can see our algorithm which is given in blue here, compares very competitively.",
            "Many cases finds very good solutions early on and what is even more important is it finds very good solutions eventually.",
            "Much better than the other algorithms, and one comparison for that behavior from our point of view is that.",
            "Since each iteration is so cheap, the algorithm achieves many iterations, which allows for construction evaluation of many primal candidate solutions, and this really helps in this case here because they will be relaxation is not tight, so it helps to evaluate the candidates."
        ],
        [
            "So I'll come to conclusion.",
            "First of all, this is work in progress and there are quite a few aspects I guess, which I'll have to look into more detail.",
            "For instance, the choice of the step size.",
            "I'm not really happy with that so far, because as I said, if you really want to get good performance, it is associated with some heuristics basically.",
            "So the classic step choices step size like 1 / T don't really perform that well, and what I found out is that the step size can actually be picked.",
            "Analytically serious not to increase the dual objective so we can get an increase or.",
            "At least yeah, a decrease or at least non increase at each iteration and this would turn our algorithm into a dual dissent method.",
            "These have been started but seekers before and the question is then if global conversions could still be guaranteed.",
            "But unfortunately the answer is that most likely we could get stuck in a corner.",
            "Then we heard about that before in the talk of Mark Schmidt because steepest descent subgradient methods are not in general guaranteed to converge to the global album, but I'll have to look further details here.",
            "The second thing I would like to do is use the algorithm which is presented in a branch and bound scheme.",
            "I believe that is very well suitable for that task because of its low working memory requirements.",
            "So if you have several solvers running at the same time, then you need a separate copy of the parameters for each procedure which runs.",
            "And since our algorithm has low memory requirements, this is obviously a good thing there.",
            "The second thing is that the constraints which will be added by branching can easily be added to our algorithm and we can still find solution efficiently, and in fact we could even use warm starting to speed this process up very well with our algorithm.",
            "And finally, I'd like to release the source code of our solver is open source within the five package for approximate training of discriminative graphical models which are currently work on as part of my PhD.",
            "So that's it.",
            "Thank you."
        ],
        [
            "Albert rounding issues like you have to round the answer.",
            "Do you even check?",
            "Come back to the discrete case?",
            "Actually, I don't do rounding because I don't obtain fractional solutions, so.",
            "What I do is I have these map states of the edges.",
            "And they construct the Primal Solutions candidates from the map states of the edges.",
            "So if all edges agree, in that case, we found the optimal to descript problem, so it's not even a random process, but really deterministic choice.",
            "But in the case where the map states of the edges disagree, I choose uniformly at random from those states to construct good primal candidates.",
            "So this is, yeah, maybe this corresponds to rounding in some sense in the dual setting.",
            "So this is what is done in this algorithm.",
            "Yes.",
            "One, how can you guarantee step size which is not decreased objective?",
            "Because it usually doesn't happen, yeah?",
            "So I mean OK.",
            "This is a recent work and as I said, I figured out how to do it in closed form, but I don't have any.",
            "I mean I would need some notes here probably to detail how to do that, or we can talk about it afterwards.",
            "The point is that.",
            "Basically you just.",
            "Have a look at the algorithm.",
            "Here you subtract some term and because it subtract that here in projection, you need to re distribute it.",
            "So and now if you choose that subtraction sufficiently small, you can guarantee that in projection determine that will be added.",
            "Basically will have will lead to a smaller increase.",
            "Because.",
            "The increase that you incur here will affect the map state of some other edges.",
            "But only this is only realized if the map state of the other edges really changes.",
            "So there is some.",
            "Let's say some some potential which you can add.",
            "Without actually increasing.",
            "So if you decrease here and add such a small step that you won't actually don't actually to increase or that only part of the increase is realized.",
            "Then this decrease, or at least.",
            "A fixed point will be guaranteed.",
            "Pardon.",
            "Yeah, that's a good question actually, and as I said, I'm still looking into these issues.",
            "Thing is.",
            "In my opinion, it could get very close actually to the.",
            "MP LP algorithm.",
            "If I did it this way because what the MP LP algorithm does, is basically a coordinate descent.",
            "And my algorithm doesn't use coordinate descent but rather subgradient incremental subgradient updates.",
            "But I could still choose the stepsize.",
            "In an optimal way, and that would be related, and there's been analysis on that issue by bread seekers in the nonlinear programming book, where he also details how the issues of steepest descent, subgradient methods and coordinate descent are basically related.",
            "If double checked.",
            "If it's not strictly convex, which is the case here.",
            "So.",
            "I mean oscillation would not happen I guess, but rather with that got stuck in a local minimum or it's not really a local minimum procedures cause it gets getting stuck in the corner.",
            "So it's a fixed point, which is not a global optimum.",
            "But it might still be worthwhile, because for particular graphs with binary variables, for instance, and PLP is still guaranteed to converge to the optimum.",
            "But this only works for binary variables, so I suspect that if I chose the step size.",
            "As it just detailed that these properties would be carried over to our algorithm, but I'm not quite sure that they will have to look into that.",
            "But it's I guess an interesting line of further work.",
            "OK, well with Germany again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so thanks for the introduction.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with my 2 devices.",
                    "label": 0
                },
                {
                    "sent": "GAIL Mattson, hydroseed.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so since time is limited today, figured it makes sense to state a few important things.",
                    "label": 0
                },
                {
                    "sent": "First.",
                    "label": 0
                },
                {
                    "sent": "First of all, why do you think you might be interested in our algorithm for maximum posteriori estimation?",
                    "label": 1
                },
                {
                    "sent": "Well, first of all, it is efficient both computationally and in terms of memory consumption.",
                    "label": 0
                },
                {
                    "sent": "Second, it finds better solutions than the methods we compared it to.",
                    "label": 1
                },
                {
                    "sent": "And third, it converges to the global optimum of the 1st order LP relaxation.",
                    "label": 0
                },
                {
                    "sent": "So we actually know what we're optimizing, which is a good thing here.",
                    "label": 0
                },
                {
                    "sent": "And finally, you also get the certificate of Optimality.",
                    "label": 0
                },
                {
                    "sent": "So if we actually find the global optimum of the discrete problem, we will know that.",
                    "label": 0
                },
                {
                    "sent": "Good, and since it's easy to get lost in technical details, just a rough outline of what we're doing this talk.",
                    "label": 0
                },
                {
                    "sent": "First, start out with the tree related upper bound by Wainwrights and others.",
                    "label": 0
                },
                {
                    "sent": "We heard about that before.",
                    "label": 1
                },
                {
                    "sent": "This upper bound is developed until it assumes a particular degenerate form that involves only a large number of easy problems, and the tightest such bound can then be found very efficiently using incremental methods which requires only to solve 1 easy problem at a single time.",
                    "label": 1
                },
                {
                    "sent": "And as we will see, this is equivalent to maximizing the LP relaxation of the map problem.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We heard this before, but I'm still going through it to introduce the notation that we'll be using.",
                    "label": 0
                },
                {
                    "sent": "What is maximum posteriori estimation?",
                    "label": 0
                },
                {
                    "sent": "Well, we consider an undirected graphical model.",
                    "label": 0
                },
                {
                    "sent": "Like this one so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you've never seen one before, this is it basically.",
                    "label": 0
                },
                {
                    "sent": "So the circles here correspond to variables and the edges between them denotes interactions going on between those variables.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the graphical models we consider here are undirected, and I assume that the random variables are discrete and we have pairwise interactions between them.",
                    "label": 0
                },
                {
                    "sent": "So this is mainly to facilitate presentation.",
                    "label": 0
                },
                {
                    "sent": "We can generalize the algorithm to general factors.",
                    "label": 0
                },
                {
                    "sent": "Good, in this case, the potential of a particular joint variable state factors into some of.",
                    "label": 1
                },
                {
                    "sent": "Local potentials and sum of interaction potentials cheated by Theta SC here.",
                    "label": 0
                },
                {
                    "sent": "Good and map problem is then basically to find.",
                    "label": 0
                },
                {
                    "sent": "The maximizing joint assignment or state which maximizes the scoring function here.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is that in general, computation of the quantities is NP hard, with a few notable exceptions such as trees or graphs over binary variables and submodular energies.",
                    "label": 1
                },
                {
                    "sent": "So that multi weights need for approximations.",
                    "label": 0
                },
                {
                    "sent": "We need some tractable approximation we can solve.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And one principled way of doing this is for instance the tree related upper bound by Wainwright, which we can minimize to find the tightest upper bound, and basically this tree related upper bound is just a convex combination over spanning trees, overall spanning trees of a graph.",
                    "label": 0
                },
                {
                    "sent": "And as you can see here is just a weighted sum of the map scores of the trees.",
                    "label": 0
                },
                {
                    "sent": "And in that framework we can minimize over these tractable reparameterization's here, which are constrained to belong to a certain convex set, which ensures that the weighted sum of this.",
                    "label": 0
                },
                {
                    "sent": "Tractable parameters here sums to the target parameter.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the coefficients.",
                    "label": 0
                },
                {
                    "sent": "Basically our constraint to belong to the simplex of probability distributions.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing about this tree rated upper bound now is that it is connected through strong duality to the 1st order LP relaxation.",
                    "label": 0
                },
                {
                    "sent": "We heard about that one before.",
                    "label": 0
                },
                {
                    "sent": "So we can solve either formulation to obtain the same global optimum.",
                    "label": 0
                },
                {
                    "sent": "Now there will be.",
                    "label": 0
                },
                {
                    "sent": "Relaxation is very similar to the definition of the map problem.",
                    "label": 0
                },
                {
                    "sent": "The only difference here is really that we relax the parameters mute to belong to the local polytope.",
                    "label": 0
                },
                {
                    "sent": "Such fractional solutions are admitted.",
                    "label": 0
                },
                {
                    "sent": "This turns the industrial linear programming problem into an LP problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and what we doing here is will be minimizing the tree related upper bound rather than the LP relaxation.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the first step will be doing here is.",
                    "label": 0
                },
                {
                    "sent": "I'll try to find simpler formulation of this upper bound such that we can minimize it more efficiently.",
                    "label": 0
                },
                {
                    "sent": "And there is one curious fact associated with that Re rated upper bound, and that is that the choice of RO is irrelevant as long as all edges are covered with nonzero probability.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That all edges have to be covered can easily be seen because otherwise the convex set would be empty because there would be no way for the potential for the parameters to sum up to the target parameter.",
                    "label": 0
                },
                {
                    "sent": "Good and now.",
                    "label": 0
                },
                {
                    "sent": "Minimization of the created upper bound is equivalent to maximization of the LP relaxation, which does not depend on the road.",
                    "label": 0
                },
                {
                    "sent": "So it is either the seen that in turn minimization of that related upper bounds also does not depend on row.",
                    "label": 0
                },
                {
                    "sent": "Good, what this means is that we can actually pick a small set of trees that are needed to cover all edges and set.",
                    "label": 0
                },
                {
                    "sent": "The corresponding coefficients to a common constant drew.",
                    "label": 0
                },
                {
                    "sent": "And set the other coefficients to 0.",
                    "label": 0
                },
                {
                    "sent": "This ensures that all edges are covered to be good.",
                    "label": 0
                },
                {
                    "sent": "Further on that, we can exploit the linearity of our scoring function and move through into the parameters by defining a new set of parameters Lambda, which is just a scaled version of our tractable parameter, Theta.",
                    "label": 0
                },
                {
                    "sent": "And by doing so we obtain this dual decomposition like formulation which is very similar to the formulation of Como dockus.",
                    "label": 0
                },
                {
                    "sent": "So what have we gained here?",
                    "label": 0
                },
                {
                    "sent": "Actually, the gain is that we sum over a small set of spanning trees rather than the set of all spanning trees of a graph, which makes minimization of this sum here feasible.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in fact there is another curious fact Associated 3 rated upper bounds and char knowledge.",
                    "label": 0
                },
                {
                    "sent": "This was first explicitly noted by Kolmogorov in 2006, and that fact is that the trees in the tree related upper bound need not be spending.",
                    "label": 0
                },
                {
                    "sent": "That is to say that.",
                    "label": 0
                },
                {
                    "sent": "The tightest bound is not impacted by the choice of trees, so our one of our central ideas in this paper is now to choose each tree as a single edge.",
                    "label": 0
                },
                {
                    "sent": "And one benefit of this is that it determines almost all parameters, because the edge parameters obviously must correspond to the target parameters of the given edge.",
                    "label": 0
                },
                {
                    "sent": "And for the other nodes and edges which do not belong to that particularly generated tree, well, these parameters must be 0.",
                    "label": 0
                },
                {
                    "sent": "So the remaining parameters of an edge are simply parameters for the two variables which are contained in this edge.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If we take all these changes together, we end up at this new formulation, which I call the tightest degenerate upper bound problem.",
                    "label": 0
                },
                {
                    "sent": "And this is basically the previous formulation, except that we have here a sum over edges and I expanded the maximum posteriori score here.",
                    "label": 0
                },
                {
                    "sent": "To actually reflect its definition for the edge rather than for a tree and for an edge, we can very easily find the map score.",
                    "label": 0
                },
                {
                    "sent": "We just need to sum over all possible edge states.",
                    "label": 0
                },
                {
                    "sent": "So previously for formulation of, Dawkins and others, for instance, we would have required maximum product belief propagation to compute that term here.",
                    "label": 0
                },
                {
                    "sent": "So whatever gains here, we have a problem which is defined in terms of a large number of rather easy problems and also the number of parameters has decreased significantly.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how can we tighten this upper bound?",
                    "label": 0
                },
                {
                    "sent": "Now first of all, this dual objective which we want to minimize is settling on differentiable.",
                    "label": 0
                },
                {
                    "sent": "But still we can obtain a subgradient and the components of that subgradient are precisely one for those components which correspond to.",
                    "label": 0
                },
                {
                    "sent": "Two variable stage states which are consistent with the edge map state and serial otherwise.",
                    "label": 0
                },
                {
                    "sent": "And another ingredient we can use for efficient minimization here is that an efficient projection operator is available to us.",
                    "label": 0
                },
                {
                    "sent": "For projection we want.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have given infeasible point Lambda prime.",
                    "label": 0
                },
                {
                    "sent": "Then we want to find the feasible point Lambda which is closest to Lambda prime in the Euclidean sense.",
                    "label": 0
                },
                {
                    "sent": "And actually a closed form solution can be obtained to this problem.",
                    "label": 0
                },
                {
                    "sent": "And that's a bit unwieldy term here, but what it essentially does is that it redistributes.",
                    "label": 0
                },
                {
                    "sent": "The amount of change which is then to parameter over the change in adjacent edges of a particular variable whose parameters were changed.",
                    "label": 1
                },
                {
                    "sent": "So the second center of the year of ours is then that since we have a separable problem with a large number of component functions and the problem is non differentiable.",
                    "label": 0
                },
                {
                    "sent": "But we have cheap projection available, we can use the incremental subgradient method by nation particles which was introduced in 2001.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you apply that algorithm to our problem, we come up with a rather simple algorithm which is catched here.",
                    "label": 0
                },
                {
                    "sent": "So what we basically do is.",
                    "label": 0
                },
                {
                    "sent": "At each outer iteration, we first pick a step size Alpha.",
                    "label": 0
                },
                {
                    "sent": "And we shuffle the set of edges.",
                    "label": 0
                },
                {
                    "sent": "And then for each edge, we first find the edge map state.",
                    "label": 0
                },
                {
                    "sent": "We subtract the sparse subgradient.",
                    "label": 0
                },
                {
                    "sent": "And we perform projection for the variable parameters containing that edge.",
                    "label": 0
                },
                {
                    "sent": "All of this can be done very efficiently.",
                    "label": 0
                },
                {
                    "sent": "So after those updates we built a candidate primal solution, which is a feasible solution to our discrete problem, and we do that by choosing the components for each variable at random from the edge map states.",
                    "label": 0
                },
                {
                    "sent": "Of those edges which contain the variable here.",
                    "label": 0
                },
                {
                    "sent": "So we maintain actually the best primal solution found so far.",
                    "label": 0
                },
                {
                    "sent": "So if the solution is better, we just pick that one, and otherwise we discard it.",
                    "label": 0
                },
                {
                    "sent": "And now one interesting thing can happen here.",
                    "label": 0
                },
                {
                    "sent": "And that is if the dual objective and the primal objective coincide.",
                    "label": 0
                },
                {
                    "sent": "This means that we've actually found an optimal primal solution of the discrete problem.",
                    "label": 0
                },
                {
                    "sent": "But that may not happen because they appear relaxation need not be tight, and in that case we just run the algorithm until convergence and return the best approximate primal solution found so far.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just like to mention a couple of formal properties of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "The first one is that for appropriate chosen sequence of steps losses, convergence to the global optimum or optimization problem is guaranteed in the limit.",
                    "label": 0
                },
                {
                    "sent": "So now the choice of Alpha case.",
                    "label": 0
                },
                {
                    "sent": "A bit tricky.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of stepsize schedules which guarantee global conversions, but unfortunately they can be rather slow in practice.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we applied some heuristic which decreases alfaqui rather aggressively in the beginning and once once Alpha drop below a certain threshold, we.",
                    "label": 0
                },
                {
                    "sent": "Switch over to conversion scheme, basically unknown summable diminishing sequence, and this turned out to work very well for a large variety of graphs in practice, but you can find the details in the paper so and the second proposition of our algorithm is regards the optimality of primal solutions.",
                    "label": 0
                },
                {
                    "sent": "I already talked about that when I presented the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Basically, if the dual and primal coincide, it follows that we found the true optimum of the discrete problem, and this happens precisely for each node.",
                    "label": 0
                },
                {
                    "sent": "The edge map States and agree on a common node map state, and this is really just an application of the strong tree agreement condition of rain, right?",
                    "label": 0
                },
                {
                    "sent": "And others to our degenerate case here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good, so there are lots of different approaches which are all aimed at solving the LP relaxation, so I figured it would make sense to give a little feature matrix here.",
                    "label": 0
                },
                {
                    "sent": "Which compares some formal properties.",
                    "label": 0
                },
                {
                    "sent": "So our method is here in Kampe.",
                    "label": 0
                },
                {
                    "sent": "And the first column I indicate whether the algorithm is guaranteed to converge to some fixed point.",
                    "label": 0
                },
                {
                    "sent": "The second column contains whether convergence of the global optimum is guaranteed.",
                    "label": 0
                },
                {
                    "sent": "In the third column indicated, the asymptotics, conversion rate, convergence rate as far as is known to me, and the last one indicated there.",
                    "label": 0
                },
                {
                    "sent": "Synthetic memory requirements in terms of the variable cardinality.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, there are quite a bit quite a few differences here, and our algorithm compares rather favorably, except for the asymptotics convergence rate, so there are some recent developments here by.",
                    "label": 0
                },
                {
                    "sent": "Ravikumar and you each, for instance, which have linear, superlinear rates there based on approximate point methods, but in general each iteration here must be expected to be significantly slower than the iterations of our algorithm, so this hasn't daughter creates do not really translate into practical performance.",
                    "label": 0
                },
                {
                    "sent": "That's what I'm trying to say here.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So little couple of experiments to really assess the empirical performance of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "And we compared three solvers our own the dual decomposition scheme of Como Dockus, Wichita.",
                    "label": 0
                },
                {
                    "sent": "Nobody's up here and the tree related message passing algorithm of Wainwright and others, and we applied these three different graphs.",
                    "label": 0
                },
                {
                    "sent": "First 50 by 50 grid with binary variables and the potential chosen as an easing grid with the non deterministic part taken uniformly between minus one and plus one.",
                    "label": 0
                },
                {
                    "sent": "Second here.",
                    "label": 0
                },
                {
                    "sent": "A 20 by 20 grid with variables severity 16 and potentials chosen as Theta is zero and interaction potentials drawn from a Gaussian distribution with zero mean and variance 15.",
                    "label": 0
                },
                {
                    "sent": "And finally choose a complete graph here of consisting of 50 binaries and put Jennifer's potentials chosen as in the for the 1st grade.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good, so here you can see the results.",
                    "label": 0
                },
                {
                    "sent": "What we actually did is we measured this core of the best primal solution found as a function of running time.",
                    "label": 0
                },
                {
                    "sent": "So the plot here is not in terms of asymptotic Inter iterations, but in terms of running time, right?",
                    "label": 0
                },
                {
                    "sent": "And for our algorithm, and for DDS up, we constructed the primal solution candidates randomly from the edge or three map states, respectively, and with its at each iteration and for tree related message passing.",
                    "label": 0
                },
                {
                    "sent": "We used the maximizers of the node beliefs at each iteration and as you can see our algorithm which is given in blue here, compares very competitively.",
                    "label": 0
                },
                {
                    "sent": "Many cases finds very good solutions early on and what is even more important is it finds very good solutions eventually.",
                    "label": 0
                },
                {
                    "sent": "Much better than the other algorithms, and one comparison for that behavior from our point of view is that.",
                    "label": 0
                },
                {
                    "sent": "Since each iteration is so cheap, the algorithm achieves many iterations, which allows for construction evaluation of many primal candidate solutions, and this really helps in this case here because they will be relaxation is not tight, so it helps to evaluate the candidates.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll come to conclusion.",
                    "label": 0
                },
                {
                    "sent": "First of all, this is work in progress and there are quite a few aspects I guess, which I'll have to look into more detail.",
                    "label": 0
                },
                {
                    "sent": "For instance, the choice of the step size.",
                    "label": 0
                },
                {
                    "sent": "I'm not really happy with that so far, because as I said, if you really want to get good performance, it is associated with some heuristics basically.",
                    "label": 0
                },
                {
                    "sent": "So the classic step choices step size like 1 / T don't really perform that well, and what I found out is that the step size can actually be picked.",
                    "label": 0
                },
                {
                    "sent": "Analytically serious not to increase the dual objective so we can get an increase or.",
                    "label": 0
                },
                {
                    "sent": "At least yeah, a decrease or at least non increase at each iteration and this would turn our algorithm into a dual dissent method.",
                    "label": 0
                },
                {
                    "sent": "These have been started but seekers before and the question is then if global conversions could still be guaranteed.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately the answer is that most likely we could get stuck in a corner.",
                    "label": 0
                },
                {
                    "sent": "Then we heard about that before in the talk of Mark Schmidt because steepest descent subgradient methods are not in general guaranteed to converge to the global album, but I'll have to look further details here.",
                    "label": 0
                },
                {
                    "sent": "The second thing I would like to do is use the algorithm which is presented in a branch and bound scheme.",
                    "label": 0
                },
                {
                    "sent": "I believe that is very well suitable for that task because of its low working memory requirements.",
                    "label": 0
                },
                {
                    "sent": "So if you have several solvers running at the same time, then you need a separate copy of the parameters for each procedure which runs.",
                    "label": 0
                },
                {
                    "sent": "And since our algorithm has low memory requirements, this is obviously a good thing there.",
                    "label": 0
                },
                {
                    "sent": "The second thing is that the constraints which will be added by branching can easily be added to our algorithm and we can still find solution efficiently, and in fact we could even use warm starting to speed this process up very well with our algorithm.",
                    "label": 0
                },
                {
                    "sent": "And finally, I'd like to release the source code of our solver is open source within the five package for approximate training of discriminative graphical models which are currently work on as part of my PhD.",
                    "label": 0
                },
                {
                    "sent": "So that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Albert rounding issues like you have to round the answer.",
                    "label": 0
                },
                {
                    "sent": "Do you even check?",
                    "label": 0
                },
                {
                    "sent": "Come back to the discrete case?",
                    "label": 0
                },
                {
                    "sent": "Actually, I don't do rounding because I don't obtain fractional solutions, so.",
                    "label": 0
                },
                {
                    "sent": "What I do is I have these map states of the edges.",
                    "label": 0
                },
                {
                    "sent": "And they construct the Primal Solutions candidates from the map states of the edges.",
                    "label": 0
                },
                {
                    "sent": "So if all edges agree, in that case, we found the optimal to descript problem, so it's not even a random process, but really deterministic choice.",
                    "label": 0
                },
                {
                    "sent": "But in the case where the map states of the edges disagree, I choose uniformly at random from those states to construct good primal candidates.",
                    "label": 0
                },
                {
                    "sent": "So this is, yeah, maybe this corresponds to rounding in some sense in the dual setting.",
                    "label": 0
                },
                {
                    "sent": "So this is what is done in this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "One, how can you guarantee step size which is not decreased objective?",
                    "label": 0
                },
                {
                    "sent": "Because it usually doesn't happen, yeah?",
                    "label": 0
                },
                {
                    "sent": "So I mean OK.",
                    "label": 0
                },
                {
                    "sent": "This is a recent work and as I said, I figured out how to do it in closed form, but I don't have any.",
                    "label": 0
                },
                {
                    "sent": "I mean I would need some notes here probably to detail how to do that, or we can talk about it afterwards.",
                    "label": 0
                },
                {
                    "sent": "The point is that.",
                    "label": 0
                },
                {
                    "sent": "Basically you just.",
                    "label": 0
                },
                {
                    "sent": "Have a look at the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Here you subtract some term and because it subtract that here in projection, you need to re distribute it.",
                    "label": 0
                },
                {
                    "sent": "So and now if you choose that subtraction sufficiently small, you can guarantee that in projection determine that will be added.",
                    "label": 0
                },
                {
                    "sent": "Basically will have will lead to a smaller increase.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "The increase that you incur here will affect the map state of some other edges.",
                    "label": 0
                },
                {
                    "sent": "But only this is only realized if the map state of the other edges really changes.",
                    "label": 0
                },
                {
                    "sent": "So there is some.",
                    "label": 0
                },
                {
                    "sent": "Let's say some some potential which you can add.",
                    "label": 0
                },
                {
                    "sent": "Without actually increasing.",
                    "label": 0
                },
                {
                    "sent": "So if you decrease here and add such a small step that you won't actually don't actually to increase or that only part of the increase is realized.",
                    "label": 0
                },
                {
                    "sent": "Then this decrease, or at least.",
                    "label": 0
                },
                {
                    "sent": "A fixed point will be guaranteed.",
                    "label": 0
                },
                {
                    "sent": "Pardon.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good question actually, and as I said, I'm still looking into these issues.",
                    "label": 0
                },
                {
                    "sent": "Thing is.",
                    "label": 0
                },
                {
                    "sent": "In my opinion, it could get very close actually to the.",
                    "label": 0
                },
                {
                    "sent": "MP LP algorithm.",
                    "label": 0
                },
                {
                    "sent": "If I did it this way because what the MP LP algorithm does, is basically a coordinate descent.",
                    "label": 0
                },
                {
                    "sent": "And my algorithm doesn't use coordinate descent but rather subgradient incremental subgradient updates.",
                    "label": 0
                },
                {
                    "sent": "But I could still choose the stepsize.",
                    "label": 0
                },
                {
                    "sent": "In an optimal way, and that would be related, and there's been analysis on that issue by bread seekers in the nonlinear programming book, where he also details how the issues of steepest descent, subgradient methods and coordinate descent are basically related.",
                    "label": 0
                },
                {
                    "sent": "If double checked.",
                    "label": 0
                },
                {
                    "sent": "If it's not strictly convex, which is the case here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I mean oscillation would not happen I guess, but rather with that got stuck in a local minimum or it's not really a local minimum procedures cause it gets getting stuck in the corner.",
                    "label": 0
                },
                {
                    "sent": "So it's a fixed point, which is not a global optimum.",
                    "label": 0
                },
                {
                    "sent": "But it might still be worthwhile, because for particular graphs with binary variables, for instance, and PLP is still guaranteed to converge to the optimum.",
                    "label": 0
                },
                {
                    "sent": "But this only works for binary variables, so I suspect that if I chose the step size.",
                    "label": 0
                },
                {
                    "sent": "As it just detailed that these properties would be carried over to our algorithm, but I'm not quite sure that they will have to look into that.",
                    "label": 0
                },
                {
                    "sent": "But it's I guess an interesting line of further work.",
                    "label": 0
                },
                {
                    "sent": "OK, well with Germany again.",
                    "label": 0
                }
            ]
        }
    }
}