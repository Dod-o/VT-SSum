{
    "id": "2nns6qqiar5hw3g2ldbr4mgtk5nz65sh",
    "title": "On Community Outliers and their Efficient Detection in Information Networks",
    "info": {
        "author": [
            "Jing Gao, Department of Computer Science and Engineering, University at Buffalo"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/kdd2010_gao_ocoe/",
    "segmentation": [
        [
            "Good morning everyone.",
            "My name is Jingle, I'm a fifth year PhD student at University of Illinois.",
            "This is joint work with full young.",
            "We find she won Eagle soon and Java hand."
        ],
        [
            "So in this work we considered these are information networks, so each network actually has two components.",
            "The first one is node, so each node represents an entity.",
            "So for example, like users in social networks or web pages on Internet and also each node usually has some feature values.",
            "So for example, each user has a profile and each web page has some contents.",
            "On the other hand, we have links so link represents relationship between entities.",
            "So for example, 2 users are linked if they are friends in social networks and two web pages are linked through hyperlinks.",
            "So actually we can observe that information networks are ubiquitous, so examples include Internet protein interaction, network coauthorship network, and social network."
        ],
        [
            "It's.",
            "So in this work we consider the data mining task, which is an outlier detection.",
            "It is also called a anomaly detection or novelty detection.",
            "So the goal is to identify points that deviate significantly from the majority of the data.",
            "So these points actually they are either like far away from the other data in the feature space or Lily do not follow the normal trends."
        ],
        [
            "So in this paper we consider a specific type of outlier which we call Community outlier.",
            "So again the data we care about Alyssa Information Network.",
            "It has two information sources, links or node features.",
            "So, for example, suppose we have a friendship network.",
            "Each node denotes a person and they have two nodes have a link.",
            "If they are friends and also suppose we know the salary information for each person.",
            "So that's the node features.",
            "And we can see that there is this communities based on Link San node features.",
            "So for example in this example we can divide them into two communities, high income or low income.",
            "So if you really too if two persons are linked together, they are more likely to be in the same community and also if they share similar salary levels they are more likely to be in the same community.",
            "So in this paper we want to find those objects that have feature values deviating from those of other Members in the same community.",
            "So for example, a low income person with many rich friends over vice versa.",
            "So we think these these persons are interesting to find out.",
            "So we define them as a community outliers.",
            "But the difficulties we don't know the communities.",
            "Therefore, in this paper we want to find out both the communities and the Community outliers together."
        ],
        [
            "So this work is actually motivated by the concept of contextual outliers.",
            "So traditional outlier detection can be regarded as finding global outliers which identify outliers among all the data.",
            "But in contextual outlier detection we try to identify outliers within a subset of data defined by contextual features.",
            "So here is a simple example.",
            "So suppose we want to find out the outliers along this.",
            "This line, which is the number of fever cases reported to a hospital.",
            "So if we only look at this feature if we only look at this feature, we can see that this one represents an outlier because it has many fewer cases, but all the other have very few fewer cases.",
            "However, if we consider contextual feature, which is the daily temperature, we can see that most of them follow a certain pattern.",
            "But this point does not follow that pattern.",
            "Therefore this is a contextual outlier."
        ],
        [
            "So the count has actually can be defined in various ways in existing studies, so these types of contests have been studied.",
            "So for example, a subset of features or temporal contest special contests.",
            "So in temporal contests we want to find some unusual events in time series.",
            "So if the account has a special information, then we want to find something on euro within a special neighborhood.",
            "So in this paper we try to you only use communities in information networks as contests."
        ],
        [
            "So before we talk about the Community outlier detection, let's first look at what types of outliers existing solutions can fund.",
            "So, for example, if we only consider node features, then we can use traditional outlet detection algorithm to find them.",
            "But this one only consider no features does not consider any link information.",
            "On the other hand, we can only use link information.",
            "So for example, if we only consider links in this friend network, we may find someone with very few friends.",
            "So for example, these two people and we call this least outliers as structural outliers.",
            "On the other hand, if you can also find like local outlier which only considers the feature values of the right neighbors.",
            "So like like this this outlier so we can see that least outliers are different from the Community, outliers redefined.",
            "So how can you find the communities and Community outliers?"
        ],
        [
            "We argue that a unified model is needed first of all, when we find communities, we need to consider Lincoln Note features together.",
            "So for example, if we only consider links to find communities, we may cut this networking this way.",
            "So then in each community they have various salary levels, so this is not what we want.",
            "On the other hand, we also want to find communities and outliers to take together, because it's well known that outliers can affect the discovery of communities."
        ],
        [
            "So that's our model, which is a unified probabilistic model to find community and Community outliers together.",
            "So this is actually a two layer graphical model in the top layer we have a bunch of nodes for the Community labels, so each each node here represents each node here represents the Community label of of the node in the network.",
            "Uh.",
            "Then at the community level, can be assigned to either like in this example, can be assigned to like high income or low income, but if its value is 0 then is is actually an outlier.",
            "And then we can see that the link structure is preserved at least layer, which means that the the if two nodes are from normal communities and they are linked together, then it's more likely that their their Community label are same.",
            "On the other hand, we put the node features at the bottom layer, so which represent the feature values so we can see that given the Community in this model, given the Community label, then the node feature is independent from the other nodes.",
            "And for all layers is uniformly will assume that it is uniformly generated.",
            "But for the note features of the normal communities, we assume that they are generated by a certain type of distributions.",
            "So for example, in this example, we assume that the salary comes from a Gaussian distribution.",
            "Therefore we have a set of parameters to describe the model.",
            "So suppose K is the number of communities.",
            "Then we have case set of model parameters.",
            "So in this example the salary from the high income community and low income community are generated by Gaussian distributions with different parameters like mean and standard deviation.",
            "So our goal is to find out the Community label.",
            "But on the other hand, the model parameters are also unknown, so therefore you need to find out these two sets of random arrivals."
        ],
        [
            "Then to find out them, we need to maximize the probability of us, which can be decomposed into probability of X given Z and probability of the.",
            "So as we can see from the graphical model, the probability of X given C depends on the Community label and model parameters.",
            "So for example, the salaries in the high or low income communities follow a Gaussian distributions defined by mean and standard deviation.",
            "But the problem is C is higher if neighboring nodes from normal communities share the same Community label.",
            "So this is big cause we the two linked persons are more likely to be in the same community.",
            "However, outliers are isolated because we cannot say that the neighbors of outliers are more likely to be outliers.",
            "So therefore for outliers, the label does not depend on the labels of neighbors."
        ],
        [
            "So to model the note features, we can use different types of distributions to describe different types of data.",
            "So in this paper we mainly focus on continuous data or test data and we use some widely used distribution to model them.",
            "So for continuous data we use Gaussian distribution, so the model parameters are mean and standard deviation and for test data use multinomial distribution and the parameters are the probability of.",
            "Award appearing in the community but in the paper, we show that our framework can be extended to general cases.",
            "So."
        ],
        [
            "So then we we propose the Community outlier detection algorithm to find out the Community and Community outliers.",
            "So remember that we have two sets of random arrivals.",
            "To find out theater represent model parameters and Z represent community labels.",
            "So this is a M types of approach.",
            "So we first initialize Z.",
            "And then given the we fancied her that maximizes the probability of X given Z.",
            "And then think of the fix the model parameters and find out.",
            "See that maximizes the probability of the given X.",
            "And we repeat these two steps until convergence.",
            "So therefore the two key components of the algorithms are parameterized emotion and influence.",
            "So let's look at."
        ],
        [
            "The details.",
            "So are there in influence?",
            "We want to find out the assignment for this.",
            "So now we suppose model parameters are known, but the difficulties are.",
            "We assume that the commuted labels our neighbors are coupled, so therefore we iteratively update the Community labels of the nodes in the network.",
            "So at each node we suppose that the neighboring the community that labels are neighboring nodes are known and we try to select.",
            "Label that maximizes the probability of Z given X and the Community labels of its neighbors.",
            "So let's look at an example.",
            "So suppose Alice step, we want to find out the Community label of this red node, and we know the salary is 100K and the suppose we know the Community labels of its neighbors and we know the model parameters of high income and low income communities.",
            "And we can compute this conditional probability of Z given X and the Community labels of neighbors, and we had to find out these numbers.",
            "And therefore we select the one that maximizes this probability.",
            "Now the question is how to compute this can do."
        ],
        [
            "No probability.",
            "To calculate this probability, we consider both the node features and Community labels of neighbors.",
            "If the indicates a normal community.",
            "On the other hand, if the probability of a node belonging to any community is low enough, we label it as an outlier, because we assume outliers is uniformly generated.",
            "So it's the probability is constant, so that as as a threshold.",
            "So the same example.",
            "So we first compute this probability for high income and low income community.",
            "We first look at this probability of salary equal to 100K given high income low income community because we know the model parameters.",
            "So this can be computed and then we compute the probability of high income or low income given the computer labels of neighbors.",
            "And for all hours is a constant.",
            "So we compute.",
            "We compare these three values and find out the one that maximizes the probability."
        ],
        [
            "Then another key component, is this a parameterized timation?",
            "So for this one we only need to estimate the model parameters for normal communities.",
            "So therefore the problem, the problem can be cast into maximum likelihood estimation.",
            "So we know that for Gaussian distribution the mean is the sample mean of the Community for standard deviation.",
            "Is the square root of the sample variance of the community.",
            "And for test data we use the empirical distribution to calculate this word distribution as the model parameters."
        ],
        [
            "So now let's look at some experimental results.",
            "Because this community alert detection is a pretty normal concept, so it's a it's a hard to find, find benchmark data set.",
            "So therefore this experimental evaluation is still a problem.",
            "Need to explore.",
            "So in this paper we use both simulated experiments and some case studies on DLP data set.",
            "So for simulated experiments we generate continuous data based on Gaussian distribution.",
            "And the generate labels according to the model and we varied the percentage of outliers in the data and the number of communities.",
            "So the baseline method we compare our the follows.",
            "The first one is this global outlet detection which consider only note features and the second one is local outlier detection which check the feature values of direct neighbors to find out outliers and the last one is we First partition data into communities based on links only and then conduct outlier detection in each community."
        ],
        [
            "So here are the results.",
            "Which is the precision so we can see that our algorithm can be the."
        ],
        [
            "The baselines.",
            "And the further experiments, undeveloped datasets are developes.",
            "A computer science bibliography and we extract for areas, data mining, artificial intelligence, database and information analysis.",
            "And we actually test on two different cases.",
            "The first in the first one we use conference as nodes.",
            "And the links between conference are defined as the percentage of common authors among two conferences.",
            "So therefore, if 2 conference share lots of common users, then they are linked closely.",
            "For authors, the links are the Co authorship relationship, and in both cases we use publication titles as a node features."
        ],
        [
            "So this is the case studies on conferences so we can see that there are four communities and so the the table shows the key top keywords for the communities and also we partition these conferences into this full communities and the Community are liars.",
            "We found our CVPR and CKM so CPR.",
            "We can see that this should fall into artificial intelligence area.",
            "However, is a focus on a specific type of problem, which is commute, computer vision, so therefore is different from its different from general artificial intelligence.",
            "Conference for CHM is is across the area.",
            "So therefore we regarded as an outlier."
        ],
        [
            "So in conclusion, we have two main contributions in this paper.",
            "The first is we propose the normal concept called Community Outliers.",
            "So we try to find nodes that have different behaviors compared with others in the community.",
            "Secondly, we propose a community outlier detection algorithm, so it is based on a unified probabilistic model, so it can conduct community discovery and order outlier detection simultaneously, and also it considers both links and note features."
        ],
        [
            "So thanks for your attention or any questions.",
            "The question is in the example you only use one feature, which is like the salary.",
            "So the question is how about like multiple features?",
            "So it depends on the compatibility of the features.",
            "If the features are compatible then we can put all the features in the just in a single feature vector.",
            "Then our algorithm can be directly applied.",
            "But if it's not compatible, so we cannot put them in a single feature vector there, then we may need some extension to make this framework work.",
            "Question.",
            "Like 1000 features.",
            "So the question is about the scalability of this algorithm.",
            "So actually in this algorithm we analyzed the time complexity which is proportional to the number of links.",
            "So because we use both nodes and links, so therefore we think that this is the I mean.",
            "Because we need to use links, so we at least need to visit all the links once.",
            "So therefore this algorithm is pretty efficient because it's only proportional to the number of links.",
            "So in sparse network the number of links is the number of links is small, so therefore we can say that the time complexity is proportional to the number of nodes.",
            "So real world networks such that the ones that that we have it, LinkedIn are usually free scale scale free networks, right?",
            "Those that power distributions followed the power law, so how would would would you adapt your methods to the scale free networks given that the higher degree nodes basically kind of breakdown?",
            "All those communities and community detection problem becomes much much more difficult.",
            "Yeah, I think for those type of for those type of networks so.",
            "Because there may be some like hub or some some notes that have like high influences over the network, so it's more likely it's becomes like the center of the Community.",
            "So then in our framework, because we consider this community discovery based on both links and note features, so therefore we can find out those.",
            "Nose as the centers of the communities and find out the centers.",
            "Find out the communities.",
            "Well, unfortunately it's not like half notes are not the center, so of the clusters, but we have to take that.",
            "Yeah, sure.",
            "I had so you use Gaussian distribution for for example like the salaries.",
            "Sounds reasonable.",
            "I'm not sure I understand why you do the DP experiments.",
            "You use titles as the features.",
            "What does it mean?",
            "The Gaussian distribution for the time?",
            "No, we actually discuss 2 cases.",
            "One is Gaussian for continuous data, the other is actually multinomial distribution for test data.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good morning everyone.",
                    "label": 0
                },
                {
                    "sent": "My name is Jingle, I'm a fifth year PhD student at University of Illinois.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with full young.",
                    "label": 0
                },
                {
                    "sent": "We find she won Eagle soon and Java hand.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this work we considered these are information networks, so each network actually has two components.",
                    "label": 0
                },
                {
                    "sent": "The first one is node, so each node represents an entity.",
                    "label": 1
                },
                {
                    "sent": "So for example, like users in social networks or web pages on Internet and also each node usually has some feature values.",
                    "label": 0
                },
                {
                    "sent": "So for example, each user has a profile and each web page has some contents.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, we have links so link represents relationship between entities.",
                    "label": 0
                },
                {
                    "sent": "So for example, 2 users are linked if they are friends in social networks and two web pages are linked through hyperlinks.",
                    "label": 1
                },
                {
                    "sent": "So actually we can observe that information networks are ubiquitous, so examples include Internet protein interaction, network coauthorship network, and social network.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "So in this work we consider the data mining task, which is an outlier detection.",
                    "label": 0
                },
                {
                    "sent": "It is also called a anomaly detection or novelty detection.",
                    "label": 1
                },
                {
                    "sent": "So the goal is to identify points that deviate significantly from the majority of the data.",
                    "label": 1
                },
                {
                    "sent": "So these points actually they are either like far away from the other data in the feature space or Lily do not follow the normal trends.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this paper we consider a specific type of outlier which we call Community outlier.",
                    "label": 0
                },
                {
                    "sent": "So again the data we care about Alyssa Information Network.",
                    "label": 0
                },
                {
                    "sent": "It has two information sources, links or node features.",
                    "label": 1
                },
                {
                    "sent": "So, for example, suppose we have a friendship network.",
                    "label": 0
                },
                {
                    "sent": "Each node denotes a person and they have two nodes have a link.",
                    "label": 0
                },
                {
                    "sent": "If they are friends and also suppose we know the salary information for each person.",
                    "label": 0
                },
                {
                    "sent": "So that's the node features.",
                    "label": 0
                },
                {
                    "sent": "And we can see that there is this communities based on Link San node features.",
                    "label": 0
                },
                {
                    "sent": "So for example in this example we can divide them into two communities, high income or low income.",
                    "label": 0
                },
                {
                    "sent": "So if you really too if two persons are linked together, they are more likely to be in the same community and also if they share similar salary levels they are more likely to be in the same community.",
                    "label": 0
                },
                {
                    "sent": "So in this paper we want to find those objects that have feature values deviating from those of other Members in the same community.",
                    "label": 1
                },
                {
                    "sent": "So for example, a low income person with many rich friends over vice versa.",
                    "label": 0
                },
                {
                    "sent": "So we think these these persons are interesting to find out.",
                    "label": 0
                },
                {
                    "sent": "So we define them as a community outliers.",
                    "label": 0
                },
                {
                    "sent": "But the difficulties we don't know the communities.",
                    "label": 0
                },
                {
                    "sent": "Therefore, in this paper we want to find out both the communities and the Community outliers together.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this work is actually motivated by the concept of contextual outliers.",
                    "label": 0
                },
                {
                    "sent": "So traditional outlier detection can be regarded as finding global outliers which identify outliers among all the data.",
                    "label": 1
                },
                {
                    "sent": "But in contextual outlier detection we try to identify outliers within a subset of data defined by contextual features.",
                    "label": 1
                },
                {
                    "sent": "So here is a simple example.",
                    "label": 0
                },
                {
                    "sent": "So suppose we want to find out the outliers along this.",
                    "label": 0
                },
                {
                    "sent": "This line, which is the number of fever cases reported to a hospital.",
                    "label": 0
                },
                {
                    "sent": "So if we only look at this feature if we only look at this feature, we can see that this one represents an outlier because it has many fewer cases, but all the other have very few fewer cases.",
                    "label": 0
                },
                {
                    "sent": "However, if we consider contextual feature, which is the daily temperature, we can see that most of them follow a certain pattern.",
                    "label": 0
                },
                {
                    "sent": "But this point does not follow that pattern.",
                    "label": 0
                },
                {
                    "sent": "Therefore this is a contextual outlier.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the count has actually can be defined in various ways in existing studies, so these types of contests have been studied.",
                    "label": 0
                },
                {
                    "sent": "So for example, a subset of features or temporal contest special contests.",
                    "label": 1
                },
                {
                    "sent": "So in temporal contests we want to find some unusual events in time series.",
                    "label": 0
                },
                {
                    "sent": "So if the account has a special information, then we want to find something on euro within a special neighborhood.",
                    "label": 1
                },
                {
                    "sent": "So in this paper we try to you only use communities in information networks as contests.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before we talk about the Community outlier detection, let's first look at what types of outliers existing solutions can fund.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if we only consider node features, then we can use traditional outlet detection algorithm to find them.",
                    "label": 1
                },
                {
                    "sent": "But this one only consider no features does not consider any link information.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we can only use link information.",
                    "label": 1
                },
                {
                    "sent": "So for example, if we only consider links in this friend network, we may find someone with very few friends.",
                    "label": 0
                },
                {
                    "sent": "So for example, these two people and we call this least outliers as structural outliers.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you can also find like local outlier which only considers the feature values of the right neighbors.",
                    "label": 1
                },
                {
                    "sent": "So like like this this outlier so we can see that least outliers are different from the Community, outliers redefined.",
                    "label": 0
                },
                {
                    "sent": "So how can you find the communities and Community outliers?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We argue that a unified model is needed first of all, when we find communities, we need to consider Lincoln Note features together.",
                    "label": 1
                },
                {
                    "sent": "So for example, if we only consider links to find communities, we may cut this networking this way.",
                    "label": 0
                },
                {
                    "sent": "So then in each community they have various salary levels, so this is not what we want.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, we also want to find communities and outliers to take together, because it's well known that outliers can affect the discovery of communities.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's our model, which is a unified probabilistic model to find community and Community outliers together.",
                    "label": 1
                },
                {
                    "sent": "So this is actually a two layer graphical model in the top layer we have a bunch of nodes for the Community labels, so each each node here represents each node here represents the Community label of of the node in the network.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Then at the community level, can be assigned to either like in this example, can be assigned to like high income or low income, but if its value is 0 then is is actually an outlier.",
                    "label": 0
                },
                {
                    "sent": "And then we can see that the link structure is preserved at least layer, which means that the the if two nodes are from normal communities and they are linked together, then it's more likely that their their Community label are same.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we put the node features at the bottom layer, so which represent the feature values so we can see that given the Community in this model, given the Community label, then the node feature is independent from the other nodes.",
                    "label": 0
                },
                {
                    "sent": "And for all layers is uniformly will assume that it is uniformly generated.",
                    "label": 0
                },
                {
                    "sent": "But for the note features of the normal communities, we assume that they are generated by a certain type of distributions.",
                    "label": 0
                },
                {
                    "sent": "So for example, in this example, we assume that the salary comes from a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Therefore we have a set of parameters to describe the model.",
                    "label": 1
                },
                {
                    "sent": "So suppose K is the number of communities.",
                    "label": 1
                },
                {
                    "sent": "Then we have case set of model parameters.",
                    "label": 0
                },
                {
                    "sent": "So in this example the salary from the high income community and low income community are generated by Gaussian distributions with different parameters like mean and standard deviation.",
                    "label": 0
                },
                {
                    "sent": "So our goal is to find out the Community label.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, the model parameters are also unknown, so therefore you need to find out these two sets of random arrivals.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then to find out them, we need to maximize the probability of us, which can be decomposed into probability of X given Z and probability of the.",
                    "label": 0
                },
                {
                    "sent": "So as we can see from the graphical model, the probability of X given C depends on the Community label and model parameters.",
                    "label": 1
                },
                {
                    "sent": "So for example, the salaries in the high or low income communities follow a Gaussian distributions defined by mean and standard deviation.",
                    "label": 1
                },
                {
                    "sent": "But the problem is C is higher if neighboring nodes from normal communities share the same Community label.",
                    "label": 1
                },
                {
                    "sent": "So this is big cause we the two linked persons are more likely to be in the same community.",
                    "label": 0
                },
                {
                    "sent": "However, outliers are isolated because we cannot say that the neighbors of outliers are more likely to be outliers.",
                    "label": 0
                },
                {
                    "sent": "So therefore for outliers, the label does not depend on the labels of neighbors.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to model the note features, we can use different types of distributions to describe different types of data.",
                    "label": 0
                },
                {
                    "sent": "So in this paper we mainly focus on continuous data or test data and we use some widely used distribution to model them.",
                    "label": 0
                },
                {
                    "sent": "So for continuous data we use Gaussian distribution, so the model parameters are mean and standard deviation and for test data use multinomial distribution and the parameters are the probability of.",
                    "label": 1
                },
                {
                    "sent": "Award appearing in the community but in the paper, we show that our framework can be extended to general cases.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then we we propose the Community outlier detection algorithm to find out the Community and Community outliers.",
                    "label": 1
                },
                {
                    "sent": "So remember that we have two sets of random arrivals.",
                    "label": 1
                },
                {
                    "sent": "To find out theater represent model parameters and Z represent community labels.",
                    "label": 1
                },
                {
                    "sent": "So this is a M types of approach.",
                    "label": 0
                },
                {
                    "sent": "So we first initialize Z.",
                    "label": 0
                },
                {
                    "sent": "And then given the we fancied her that maximizes the probability of X given Z.",
                    "label": 0
                },
                {
                    "sent": "And then think of the fix the model parameters and find out.",
                    "label": 0
                },
                {
                    "sent": "See that maximizes the probability of the given X.",
                    "label": 0
                },
                {
                    "sent": "And we repeat these two steps until convergence.",
                    "label": 0
                },
                {
                    "sent": "So therefore the two key components of the algorithms are parameterized emotion and influence.",
                    "label": 0
                },
                {
                    "sent": "So let's look at.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The details.",
                    "label": 0
                },
                {
                    "sent": "So are there in influence?",
                    "label": 0
                },
                {
                    "sent": "We want to find out the assignment for this.",
                    "label": 0
                },
                {
                    "sent": "So now we suppose model parameters are known, but the difficulties are.",
                    "label": 0
                },
                {
                    "sent": "We assume that the commuted labels our neighbors are coupled, so therefore we iteratively update the Community labels of the nodes in the network.",
                    "label": 0
                },
                {
                    "sent": "So at each node we suppose that the neighboring the community that labels are neighboring nodes are known and we try to select.",
                    "label": 0
                },
                {
                    "sent": "Label that maximizes the probability of Z given X and the Community labels of its neighbors.",
                    "label": 1
                },
                {
                    "sent": "So let's look at an example.",
                    "label": 0
                },
                {
                    "sent": "So suppose Alice step, we want to find out the Community label of this red node, and we know the salary is 100K and the suppose we know the Community labels of its neighbors and we know the model parameters of high income and low income communities.",
                    "label": 0
                },
                {
                    "sent": "And we can compute this conditional probability of Z given X and the Community labels of neighbors, and we had to find out these numbers.",
                    "label": 0
                },
                {
                    "sent": "And therefore we select the one that maximizes this probability.",
                    "label": 0
                },
                {
                    "sent": "Now the question is how to compute this can do.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No probability.",
                    "label": 0
                },
                {
                    "sent": "To calculate this probability, we consider both the node features and Community labels of neighbors.",
                    "label": 1
                },
                {
                    "sent": "If the indicates a normal community.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if the probability of a node belonging to any community is low enough, we label it as an outlier, because we assume outliers is uniformly generated.",
                    "label": 1
                },
                {
                    "sent": "So it's the probability is constant, so that as as a threshold.",
                    "label": 0
                },
                {
                    "sent": "So the same example.",
                    "label": 0
                },
                {
                    "sent": "So we first compute this probability for high income and low income community.",
                    "label": 0
                },
                {
                    "sent": "We first look at this probability of salary equal to 100K given high income low income community because we know the model parameters.",
                    "label": 0
                },
                {
                    "sent": "So this can be computed and then we compute the probability of high income or low income given the computer labels of neighbors.",
                    "label": 0
                },
                {
                    "sent": "And for all hours is a constant.",
                    "label": 0
                },
                {
                    "sent": "So we compute.",
                    "label": 0
                },
                {
                    "sent": "We compare these three values and find out the one that maximizes the probability.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then another key component, is this a parameterized timation?",
                    "label": 0
                },
                {
                    "sent": "So for this one we only need to estimate the model parameters for normal communities.",
                    "label": 0
                },
                {
                    "sent": "So therefore the problem, the problem can be cast into maximum likelihood estimation.",
                    "label": 0
                },
                {
                    "sent": "So we know that for Gaussian distribution the mean is the sample mean of the Community for standard deviation.",
                    "label": 1
                },
                {
                    "sent": "Is the square root of the sample variance of the community.",
                    "label": 1
                },
                {
                    "sent": "And for test data we use the empirical distribution to calculate this word distribution as the model parameters.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's look at some experimental results.",
                    "label": 0
                },
                {
                    "sent": "Because this community alert detection is a pretty normal concept, so it's a it's a hard to find, find benchmark data set.",
                    "label": 0
                },
                {
                    "sent": "So therefore this experimental evaluation is still a problem.",
                    "label": 0
                },
                {
                    "sent": "Need to explore.",
                    "label": 0
                },
                {
                    "sent": "So in this paper we use both simulated experiments and some case studies on DLP data set.",
                    "label": 0
                },
                {
                    "sent": "So for simulated experiments we generate continuous data based on Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "And the generate labels according to the model and we varied the percentage of outliers in the data and the number of communities.",
                    "label": 1
                },
                {
                    "sent": "So the baseline method we compare our the follows.",
                    "label": 0
                },
                {
                    "sent": "The first one is this global outlet detection which consider only note features and the second one is local outlier detection which check the feature values of direct neighbors to find out outliers and the last one is we First partition data into communities based on links only and then conduct outlier detection in each community.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are the results.",
                    "label": 0
                },
                {
                    "sent": "Which is the precision so we can see that our algorithm can be the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The baselines.",
                    "label": 0
                },
                {
                    "sent": "And the further experiments, undeveloped datasets are developes.",
                    "label": 0
                },
                {
                    "sent": "A computer science bibliography and we extract for areas, data mining, artificial intelligence, database and information analysis.",
                    "label": 1
                },
                {
                    "sent": "And we actually test on two different cases.",
                    "label": 0
                },
                {
                    "sent": "The first in the first one we use conference as nodes.",
                    "label": 1
                },
                {
                    "sent": "And the links between conference are defined as the percentage of common authors among two conferences.",
                    "label": 0
                },
                {
                    "sent": "So therefore, if 2 conference share lots of common users, then they are linked closely.",
                    "label": 0
                },
                {
                    "sent": "For authors, the links are the Co authorship relationship, and in both cases we use publication titles as a node features.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the case studies on conferences so we can see that there are four communities and so the the table shows the key top keywords for the communities and also we partition these conferences into this full communities and the Community are liars.",
                    "label": 1
                },
                {
                    "sent": "We found our CVPR and CKM so CPR.",
                    "label": 0
                },
                {
                    "sent": "We can see that this should fall into artificial intelligence area.",
                    "label": 0
                },
                {
                    "sent": "However, is a focus on a specific type of problem, which is commute, computer vision, so therefore is different from its different from general artificial intelligence.",
                    "label": 0
                },
                {
                    "sent": "Conference for CHM is is across the area.",
                    "label": 0
                },
                {
                    "sent": "So therefore we regarded as an outlier.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, we have two main contributions in this paper.",
                    "label": 0
                },
                {
                    "sent": "The first is we propose the normal concept called Community Outliers.",
                    "label": 0
                },
                {
                    "sent": "So we try to find nodes that have different behaviors compared with others in the community.",
                    "label": 1
                },
                {
                    "sent": "Secondly, we propose a community outlier detection algorithm, so it is based on a unified probabilistic model, so it can conduct community discovery and order outlier detection simultaneously, and also it considers both links and note features.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thanks for your attention or any questions.",
                    "label": 0
                },
                {
                    "sent": "The question is in the example you only use one feature, which is like the salary.",
                    "label": 0
                },
                {
                    "sent": "So the question is how about like multiple features?",
                    "label": 0
                },
                {
                    "sent": "So it depends on the compatibility of the features.",
                    "label": 0
                },
                {
                    "sent": "If the features are compatible then we can put all the features in the just in a single feature vector.",
                    "label": 0
                },
                {
                    "sent": "Then our algorithm can be directly applied.",
                    "label": 0
                },
                {
                    "sent": "But if it's not compatible, so we cannot put them in a single feature vector there, then we may need some extension to make this framework work.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Like 1000 features.",
                    "label": 0
                },
                {
                    "sent": "So the question is about the scalability of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So actually in this algorithm we analyzed the time complexity which is proportional to the number of links.",
                    "label": 0
                },
                {
                    "sent": "So because we use both nodes and links, so therefore we think that this is the I mean.",
                    "label": 0
                },
                {
                    "sent": "Because we need to use links, so we at least need to visit all the links once.",
                    "label": 0
                },
                {
                    "sent": "So therefore this algorithm is pretty efficient because it's only proportional to the number of links.",
                    "label": 0
                },
                {
                    "sent": "So in sparse network the number of links is the number of links is small, so therefore we can say that the time complexity is proportional to the number of nodes.",
                    "label": 0
                },
                {
                    "sent": "So real world networks such that the ones that that we have it, LinkedIn are usually free scale scale free networks, right?",
                    "label": 0
                },
                {
                    "sent": "Those that power distributions followed the power law, so how would would would you adapt your methods to the scale free networks given that the higher degree nodes basically kind of breakdown?",
                    "label": 0
                },
                {
                    "sent": "All those communities and community detection problem becomes much much more difficult.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think for those type of for those type of networks so.",
                    "label": 0
                },
                {
                    "sent": "Because there may be some like hub or some some notes that have like high influences over the network, so it's more likely it's becomes like the center of the Community.",
                    "label": 0
                },
                {
                    "sent": "So then in our framework, because we consider this community discovery based on both links and note features, so therefore we can find out those.",
                    "label": 0
                },
                {
                    "sent": "Nose as the centers of the communities and find out the centers.",
                    "label": 0
                },
                {
                    "sent": "Find out the communities.",
                    "label": 0
                },
                {
                    "sent": "Well, unfortunately it's not like half notes are not the center, so of the clusters, but we have to take that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "I had so you use Gaussian distribution for for example like the salaries.",
                    "label": 0
                },
                {
                    "sent": "Sounds reasonable.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure I understand why you do the DP experiments.",
                    "label": 0
                },
                {
                    "sent": "You use titles as the features.",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "The Gaussian distribution for the time?",
                    "label": 0
                },
                {
                    "sent": "No, we actually discuss 2 cases.",
                    "label": 0
                },
                {
                    "sent": "One is Gaussian for continuous data, the other is actually multinomial distribution for test data.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}