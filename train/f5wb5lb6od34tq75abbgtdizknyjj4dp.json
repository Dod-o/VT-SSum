{
    "id": "f5wb5lb6od34tq75abbgtdizknyjj4dp",
    "title": "Blind Signal Separation in the Presence of Gaussian Noise",
    "info": {
        "author": [
            "James Voss, Department of Computer Science and Engineering, Ohio State University"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_voss_signal/",
    "segmentation": [
        [
            "So a prototypical example of the blind signal set."
        ],
        [
            "Racial problem is the cocktail party problem.",
            "In it we have a bunch of people scattered throughout a large room, and we placed microphones throughout that room and people are speaking.",
            "And.",
            "Since the microphones are not associated with a particular person, you're just capturing some linear superposition of voices.",
            "Then the goal then is to recover the speech of the individual speakers from the.",
            "Literally super super anyway.",
            "From the mixtures of the speech that's captured by the microphone.",
            "And for some based on the model that we'll end up using, we assume that there's at least as many microphones and speakers."
        ],
        [
            "And the model that we use is called independent component analysis or ICA.",
            "In this we have a latent random signal S which is vector valued random variable.",
            "It's assumed that each component random variable is independent of each other component random variable.",
            "And there's a linear mixing matrix A.",
            "Which then mixes together all the latent signals and we're observing.",
            "The random variable X and drawing samples from that.",
            "Um?",
            "The covariance of as assumed to be the identity and the only reason for this is to make it so that the scaling or signal strength information is contained in the columns of the Matrix A.",
            "Can we assume the A is full rank and square, though in principle?",
            "There could be observed signal X could have more components than S. We require it to have at least as many, and for simplicity we assume that it's the same number.",
            "We say that the Earth independent component acts and direction AI, where AI is the 5th column of the Matrix A.",
            "And then Ada is a noise term.",
            "Traditionally in ICA the noise is ignored.",
            "It's viewed as being 0.",
            "So more recent work has been modeling as a Gaussian noise.",
            "Arbitrary Gaussian, it need not be spherical, and we're continuing this line of research.",
            "And the goal in ICA is to recover the mixing matrix A.",
            "With since that basically gets you access to the latent signals and S."
        ],
        [
            "So in the example plots were using uniform distribution versus a uniform distribution mixed.",
            "And a typical ICA procedure is done in 2 steps.",
            "The first step is you whiten the data.",
            "This is the noise was setting.",
            "You wipe down the data in the first step, which is to make it have identity covariance.",
            "At a result of this, is that the latent signals then act in orthogonal directions and also have the same scaling.",
            "This in that sense you're recovering the matrix a up to a rotation.",
            "And then the second step is then to recover that rotation.",
            "This talk will be primarily focused on the first step in the noisy setting."
        ],
        [
            "It turns out that in the noisy setting, whitening becomes a little bit tricky because what you want to whiten is the signal a * S, not including the noise, and this becomes impossible to do.",
            "Do too.",
            "Some indeterminacy that arises with the Gaussian noise, particularly take C to be a white Gaussian noise, will contain independent components in each coordinate.",
            "So if you take X + C, That's still an independent random variable or has independent coordinates.",
            "But a XE is also a Gaussian random variable, so the scaling information.",
            "Of the latest signals becomes indeterminate because of this issue.",
            "So the best that can be done is something called quasi whitening.",
            "And then you find a quasi whitening matrix W. Such that when you multiply W * A, you get a rotation matrix R * A diagonal matrix D, so D contains the scaling information.",
            "So an R contains just the rotation information, the scaling information and D will ultimately be lost.",
            "Um, yeah."
        ],
        [
            "So the noisy ICA problem was.",
            "First I'd buy high Varenna 1999, though they looked at a version where the noise covariance of the additive Gaussian noise was known.",
            "And it was a practical work.",
            "So in your door did a.",
            "Work where they did a one step solution for the noisy ICA setting where.",
            "The noise was unknown, however it has no polynomial guarantees, so it's.",
            "It's just it's going to work in expectation, but no error analysis is done.",
            "More, it's more recently at work by Aurora Kiboy Transact Deva in 2012.",
            "Does a probably efficient noisy ICA algorithm for.",
            "Some special case, in particular, their techniques require that each latent coordinate signal has to have fourth cumulant of.",
            "If all of the same sign.",
            "And there's also discussed one step solution has sued Kakatta that's.",
            "Work in 2012 that's similar to the air doors solution and that's.",
            "One step based on directional Hessian technique and no air analysis is given in that work."
        ],
        [
            "So what our contribution is, is we're looking at the first step, which is this quasi whitening step introduced in Aurora at all paper.",
            "And what we wanted to do was extend it to handle more cases and still have it be efficiently provable, have efficient polynomial guarantees.",
            "And so we're able to handle the case where each of the latent signals has nonzero 4th cumulant.",
            "And possibly have mixed sign.",
            "Our technique relies on.",
            "Multivariate cumulant tensors.",
            "And our technique is compatible with mild variations on existing techniques for ICA from the noiseless setting.",
            "In particular, if you look at the 4th keynote functional, which is defined on this slide.",
            "And under the assumption that the data is already been quasi white and.",
            "Then taking.",
            "V to be.",
            "Unit vector.",
            "You can project the data onto V and take the 4th cumulative this object.",
            "If you take the absolute value of that.",
            "This function actually takes its all of its local Maxima.",
            "And.",
            "Directions to give the columns of the rotation matrix R that needs to be recovered during the second step.",
            "But so, because of this, we just focus on the first step."
        ],
        [
            "Since our method relies on cumulants, it's I guess good to talk a little bit about them an cumulants.",
            "There's similar two moments in a lot of ways.",
            "However, unlike moments, so moments respect independence by between random variables by multiplication.",
            "Cumulus respect independence by separating under addition instead.",
            "And this makes some nicer for the ICA setting.",
            "Like moments, they have natural sample versions, so we're able to state an algorithm in terms of exact cumulants and then rewrite the algorithm with sample estimates and create an algorithm.",
            "Well, the note the cross accumulate before between 4 random variables YIYJYKYL by this qy Y jyk while notation.",
            "And we call and we can denote 4th order cumulant tensor.",
            "So 4th order.",
            "Cumulant via this Q supplied notation.",
            "Where we have ijkl off entry being across cumulant between the JKLF coordinates coordinate random variables.",
            "And then, similarly to the variance, which is actually the second word cumulant, the univariant cumulates occur in the diagonal entries of the tensor."
        ],
        [
            "So just a couple of properties that are important for the construction of the algorithm.",
            "1st.",
            "Symmetry that you have quite random variables.",
            "It doesn't matter there ordering.",
            "So then Cumulus are also multilinear, and that if you take two coordinate random variables or two random variables of 1 coordinate and there's a summation between them, then.",
            "The cumulative splits over the summation.",
            "But also if you multiply accordant random variable by a scalar Alpha, that's the same as multiplying the entire cross cumulant by the scalar Alpha.",
            "And.",
            "There's the independence.",
            "Which is that if you take 2 coordinate random variables are independent, then the croskey mill is 0.",
            "When you combine this with the multi linearity properties.",
            "You actually get that for two random vectors that are independent.",
            "The cumulant tensor splits across addition.",
            "So Q, Y + Z is equal to Q, y + Q Z.",
            "And finally, what allows us to handle Gaussian noise nicely is that for higher order cumulants order at least three.",
            "The cumulative Gaussian random variable is 0, so in some sense, via the independence assumption, if we have an additive independent additive Gaussian noise.",
            "Then the Gaussian noise will split off and accumulate tensor and vanish."
        ],
        [
            "So we define or there's.",
            "I guess it's existed before us, but there's a matrix.",
            "There's accumulate 4th or kimota matrix operation.",
            "Defined.",
            "By that summation, we're basically.",
            "It's basically a flattening of the tensor to be a matrix.",
            "So if you take him to be an arbitrary matrix then you can have QX acting upon them and define a new matrix with entry being the sum of the cross cumulus accell times MLK.",
            "And it turns out that.",
            "Using this operation, we're able to create a quasi whitening algorithm which I'll be discussing in more detail in the next couple of slides.",
            "So we'll see that algorithm again."
        ],
        [
            "So this lemma gives the intuition as to why.",
            "The.",
            "Tensor matrix operation is useful.",
            "In particular, if you take QX and act upon any arbitrary matrix M, the resulting matrix is of the Form 88 transpose.",
            "Where a is from our ICA model, so X = X + 8.",
            "A&D is some diagonal matrix.",
            "With Q's diagonal entry.",
            "Being the 4th chemo of.",
            "The cute latent signal times this entrance.",
            "The cue column of a transpose, MSMQ.",
            "And the proof of this actually follows almost directly from the multivariate cumulative properties.",
            "In particular, if you look at QX acting upon M. You're able to split.",
            "You're able to use construction X equal to S + A to split off.",
            "The additive Gaussian noise from the cumula tensor and have it vanish.",
            "And then just using the definition.",
            "Of.",
            "The operation we get the first large summation.",
            "And then it's basically expanding everything by multilinear properties and having all the cross terms cancel out by the independence assumptions.",
            "In doing this, we're able to get into the claim form."
        ],
        [
            "So.",
            "We then arrive at.",
            "A firm that basically states our algorithm again.",
            "And if you take QX applied to the identity matrix, invert that result, and apply QX to that result, we get a matrix M of the form JDA transpose.",
            "Where D has strictly positive diagonal entries.",
            "And particularly, the queue entry is one over the norm of the QS column of a ^2.",
            "And this follows directly from the previous lemma.",
            "And then if we take a decomposition BB transpose of this resulting matrix M, then the inverse is going to be a quasi whitening matrix.",
            "So to see why this is, we just note that first duty to 1/2 is real, since D is positive as strictly positive entries on the diagonal matrix square root exists.",
            "And is real valued, so we're able to take the decomposition and left and right multiply B&B, transpose, inverse.",
            "To create this decomposition, B inverse AD to the 1/2 times its transpose.",
            "So in particular.",
            "This implies that the inverse AD to the 1/2 is a rotation matrix.",
            "And then just.",
            "Inverting dealer 1/2 we can put on the other side and we get the inverse.",
            "A is a rotation matrix times a diagonal matrix.",
            "Which gives precisely that the inverse is a quasi whitening matrix."
        ],
        [
            "And then the main result is that the sample version of this algorithm is actually.",
            "Requires only polynomial samples, so it's an efficient algorithm.",
            "So we're taking QX.",
            "Here is the case in cystic estimate.",
            "Of the.",
            "Observed random variable, its fourth or a tensor.",
            "The first bullet point is saying, roughly that if you take two of the latent signals that are distinct after quasi whitening, they will act in approximately orthogonal directions.",
            "Using cosine error.",
            "And then the second bullet point is saying that.",
            "The scale associated with each latent signal after the quasi whitening will be close to what we would expect from the construction.",
            "Um?",
            "And then.",
            "Yeah, so it's polynomial samples and things that arise fairly naturally, so it's like the condition number of a.",
            "The ratio between the.",
            "4th Cumulant the maximum.",
            "4th cumulative the latent signals.",
            "Some dependence on the standard deviation of the Gaussian noise and also one over D1 over epsilon where.",
            "Epsilon is allowable Aaron.",
            "Besides here.",
            "I don't state it explicitly here, but it's the sample size will be polynomially large.",
            "And these.",
            "Parameters."
        ],
        [
            "And so this is then the quasi whitening algorithm.",
            "Restated.",
            "And So what we're doing is then just using the sample version of the algorithm applied by the theorem.",
            "And.",
            "So QX, then being the case to stick estimate.",
            "And then we're applying this transform to each and every single data point to quasi white on the data.",
            "And the algorithm is probably efficient for the first step of ICA.",
            "And it's compatible with variations on existing methods for the second step of ICA."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a prototypical example of the blind signal set.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Racial problem is the cocktail party problem.",
                    "label": 1
                },
                {
                    "sent": "In it we have a bunch of people scattered throughout a large room, and we placed microphones throughout that room and people are speaking.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Since the microphones are not associated with a particular person, you're just capturing some linear superposition of voices.",
                    "label": 0
                },
                {
                    "sent": "Then the goal then is to recover the speech of the individual speakers from the.",
                    "label": 0
                },
                {
                    "sent": "Literally super super anyway.",
                    "label": 1
                },
                {
                    "sent": "From the mixtures of the speech that's captured by the microphone.",
                    "label": 0
                },
                {
                    "sent": "And for some based on the model that we'll end up using, we assume that there's at least as many microphones and speakers.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the model that we use is called independent component analysis or ICA.",
                    "label": 1
                },
                {
                    "sent": "In this we have a latent random signal S which is vector valued random variable.",
                    "label": 0
                },
                {
                    "sent": "It's assumed that each component random variable is independent of each other component random variable.",
                    "label": 0
                },
                {
                    "sent": "And there's a linear mixing matrix A.",
                    "label": 1
                },
                {
                    "sent": "Which then mixes together all the latent signals and we're observing.",
                    "label": 0
                },
                {
                    "sent": "The random variable X and drawing samples from that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The covariance of as assumed to be the identity and the only reason for this is to make it so that the scaling or signal strength information is contained in the columns of the Matrix A.",
                    "label": 1
                },
                {
                    "sent": "Can we assume the A is full rank and square, though in principle?",
                    "label": 1
                },
                {
                    "sent": "There could be observed signal X could have more components than S. We require it to have at least as many, and for simplicity we assume that it's the same number.",
                    "label": 1
                },
                {
                    "sent": "We say that the Earth independent component acts and direction AI, where AI is the 5th column of the Matrix A.",
                    "label": 0
                },
                {
                    "sent": "And then Ada is a noise term.",
                    "label": 0
                },
                {
                    "sent": "Traditionally in ICA the noise is ignored.",
                    "label": 0
                },
                {
                    "sent": "It's viewed as being 0.",
                    "label": 0
                },
                {
                    "sent": "So more recent work has been modeling as a Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "Arbitrary Gaussian, it need not be spherical, and we're continuing this line of research.",
                    "label": 0
                },
                {
                    "sent": "And the goal in ICA is to recover the mixing matrix A.",
                    "label": 0
                },
                {
                    "sent": "With since that basically gets you access to the latent signals and S.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the example plots were using uniform distribution versus a uniform distribution mixed.",
                    "label": 0
                },
                {
                    "sent": "And a typical ICA procedure is done in 2 steps.",
                    "label": 1
                },
                {
                    "sent": "The first step is you whiten the data.",
                    "label": 0
                },
                {
                    "sent": "This is the noise was setting.",
                    "label": 0
                },
                {
                    "sent": "You wipe down the data in the first step, which is to make it have identity covariance.",
                    "label": 0
                },
                {
                    "sent": "At a result of this, is that the latent signals then act in orthogonal directions and also have the same scaling.",
                    "label": 1
                },
                {
                    "sent": "This in that sense you're recovering the matrix a up to a rotation.",
                    "label": 1
                },
                {
                    "sent": "And then the second step is then to recover that rotation.",
                    "label": 0
                },
                {
                    "sent": "This talk will be primarily focused on the first step in the noisy setting.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It turns out that in the noisy setting, whitening becomes a little bit tricky because what you want to whiten is the signal a * S, not including the noise, and this becomes impossible to do.",
                    "label": 0
                },
                {
                    "sent": "Do too.",
                    "label": 0
                },
                {
                    "sent": "Some indeterminacy that arises with the Gaussian noise, particularly take C to be a white Gaussian noise, will contain independent components in each coordinate.",
                    "label": 1
                },
                {
                    "sent": "So if you take X + C, That's still an independent random variable or has independent coordinates.",
                    "label": 0
                },
                {
                    "sent": "But a XE is also a Gaussian random variable, so the scaling information.",
                    "label": 0
                },
                {
                    "sent": "Of the latest signals becomes indeterminate because of this issue.",
                    "label": 0
                },
                {
                    "sent": "So the best that can be done is something called quasi whitening.",
                    "label": 1
                },
                {
                    "sent": "And then you find a quasi whitening matrix W. Such that when you multiply W * A, you get a rotation matrix R * A diagonal matrix D, so D contains the scaling information.",
                    "label": 0
                },
                {
                    "sent": "So an R contains just the rotation information, the scaling information and D will ultimately be lost.",
                    "label": 0
                },
                {
                    "sent": "Um, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the noisy ICA problem was.",
                    "label": 0
                },
                {
                    "sent": "First I'd buy high Varenna 1999, though they looked at a version where the noise covariance of the additive Gaussian noise was known.",
                    "label": 1
                },
                {
                    "sent": "And it was a practical work.",
                    "label": 0
                },
                {
                    "sent": "So in your door did a.",
                    "label": 1
                },
                {
                    "sent": "Work where they did a one step solution for the noisy ICA setting where.",
                    "label": 0
                },
                {
                    "sent": "The noise was unknown, however it has no polynomial guarantees, so it's.",
                    "label": 0
                },
                {
                    "sent": "It's just it's going to work in expectation, but no error analysis is done.",
                    "label": 0
                },
                {
                    "sent": "More, it's more recently at work by Aurora Kiboy Transact Deva in 2012.",
                    "label": 0
                },
                {
                    "sent": "Does a probably efficient noisy ICA algorithm for.",
                    "label": 1
                },
                {
                    "sent": "Some special case, in particular, their techniques require that each latent coordinate signal has to have fourth cumulant of.",
                    "label": 0
                },
                {
                    "sent": "If all of the same sign.",
                    "label": 0
                },
                {
                    "sent": "And there's also discussed one step solution has sued Kakatta that's.",
                    "label": 0
                },
                {
                    "sent": "Work in 2012 that's similar to the air doors solution and that's.",
                    "label": 0
                },
                {
                    "sent": "One step based on directional Hessian technique and no air analysis is given in that work.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what our contribution is, is we're looking at the first step, which is this quasi whitening step introduced in Aurora at all paper.",
                    "label": 0
                },
                {
                    "sent": "And what we wanted to do was extend it to handle more cases and still have it be efficiently provable, have efficient polynomial guarantees.",
                    "label": 0
                },
                {
                    "sent": "And so we're able to handle the case where each of the latent signals has nonzero 4th cumulant.",
                    "label": 0
                },
                {
                    "sent": "And possibly have mixed sign.",
                    "label": 1
                },
                {
                    "sent": "Our technique relies on.",
                    "label": 0
                },
                {
                    "sent": "Multivariate cumulant tensors.",
                    "label": 0
                },
                {
                    "sent": "And our technique is compatible with mild variations on existing techniques for ICA from the noiseless setting.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you look at the 4th keynote functional, which is defined on this slide.",
                    "label": 0
                },
                {
                    "sent": "And under the assumption that the data is already been quasi white and.",
                    "label": 0
                },
                {
                    "sent": "Then taking.",
                    "label": 0
                },
                {
                    "sent": "V to be.",
                    "label": 0
                },
                {
                    "sent": "Unit vector.",
                    "label": 0
                },
                {
                    "sent": "You can project the data onto V and take the 4th cumulative this object.",
                    "label": 0
                },
                {
                    "sent": "If you take the absolute value of that.",
                    "label": 1
                },
                {
                    "sent": "This function actually takes its all of its local Maxima.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Directions to give the columns of the rotation matrix R that needs to be recovered during the second step.",
                    "label": 1
                },
                {
                    "sent": "But so, because of this, we just focus on the first step.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Since our method relies on cumulants, it's I guess good to talk a little bit about them an cumulants.",
                    "label": 0
                },
                {
                    "sent": "There's similar two moments in a lot of ways.",
                    "label": 0
                },
                {
                    "sent": "However, unlike moments, so moments respect independence by between random variables by multiplication.",
                    "label": 1
                },
                {
                    "sent": "Cumulus respect independence by separating under addition instead.",
                    "label": 0
                },
                {
                    "sent": "And this makes some nicer for the ICA setting.",
                    "label": 1
                },
                {
                    "sent": "Like moments, they have natural sample versions, so we're able to state an algorithm in terms of exact cumulants and then rewrite the algorithm with sample estimates and create an algorithm.",
                    "label": 1
                },
                {
                    "sent": "Well, the note the cross accumulate before between 4 random variables YIYJYKYL by this qy Y jyk while notation.",
                    "label": 0
                },
                {
                    "sent": "And we call and we can denote 4th order cumulant tensor.",
                    "label": 0
                },
                {
                    "sent": "So 4th order.",
                    "label": 0
                },
                {
                    "sent": "Cumulant via this Q supplied notation.",
                    "label": 0
                },
                {
                    "sent": "Where we have ijkl off entry being across cumulant between the JKLF coordinates coordinate random variables.",
                    "label": 0
                },
                {
                    "sent": "And then, similarly to the variance, which is actually the second word cumulant, the univariant cumulates occur in the diagonal entries of the tensor.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just a couple of properties that are important for the construction of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "1st.",
                    "label": 0
                },
                {
                    "sent": "Symmetry that you have quite random variables.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter there ordering.",
                    "label": 0
                },
                {
                    "sent": "So then Cumulus are also multilinear, and that if you take two coordinate random variables or two random variables of 1 coordinate and there's a summation between them, then.",
                    "label": 0
                },
                {
                    "sent": "The cumulative splits over the summation.",
                    "label": 0
                },
                {
                    "sent": "But also if you multiply accordant random variable by a scalar Alpha, that's the same as multiplying the entire cross cumulant by the scalar Alpha.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "There's the independence.",
                    "label": 0
                },
                {
                    "sent": "Which is that if you take 2 coordinate random variables are independent, then the croskey mill is 0.",
                    "label": 1
                },
                {
                    "sent": "When you combine this with the multi linearity properties.",
                    "label": 0
                },
                {
                    "sent": "You actually get that for two random vectors that are independent.",
                    "label": 0
                },
                {
                    "sent": "The cumulant tensor splits across addition.",
                    "label": 0
                },
                {
                    "sent": "So Q, Y + Z is equal to Q, y + Q Z.",
                    "label": 0
                },
                {
                    "sent": "And finally, what allows us to handle Gaussian noise nicely is that for higher order cumulants order at least three.",
                    "label": 1
                },
                {
                    "sent": "The cumulative Gaussian random variable is 0, so in some sense, via the independence assumption, if we have an additive independent additive Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "Then the Gaussian noise will split off and accumulate tensor and vanish.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we define or there's.",
                    "label": 0
                },
                {
                    "sent": "I guess it's existed before us, but there's a matrix.",
                    "label": 0
                },
                {
                    "sent": "There's accumulate 4th or kimota matrix operation.",
                    "label": 0
                },
                {
                    "sent": "Defined.",
                    "label": 0
                },
                {
                    "sent": "By that summation, we're basically.",
                    "label": 0
                },
                {
                    "sent": "It's basically a flattening of the tensor to be a matrix.",
                    "label": 1
                },
                {
                    "sent": "So if you take him to be an arbitrary matrix then you can have QX acting upon them and define a new matrix with entry being the sum of the cross cumulus accell times MLK.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that.",
                    "label": 0
                },
                {
                    "sent": "Using this operation, we're able to create a quasi whitening algorithm which I'll be discussing in more detail in the next couple of slides.",
                    "label": 0
                },
                {
                    "sent": "So we'll see that algorithm again.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this lemma gives the intuition as to why.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Tensor matrix operation is useful.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you take QX and act upon any arbitrary matrix M, the resulting matrix is of the Form 88 transpose.",
                    "label": 0
                },
                {
                    "sent": "Where a is from our ICA model, so X = X + 8.",
                    "label": 0
                },
                {
                    "sent": "A&D is some diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "With Q's diagonal entry.",
                    "label": 0
                },
                {
                    "sent": "Being the 4th chemo of.",
                    "label": 0
                },
                {
                    "sent": "The cute latent signal times this entrance.",
                    "label": 0
                },
                {
                    "sent": "The cue column of a transpose, MSMQ.",
                    "label": 0
                },
                {
                    "sent": "And the proof of this actually follows almost directly from the multivariate cumulative properties.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you look at QX acting upon M. You're able to split.",
                    "label": 0
                },
                {
                    "sent": "You're able to use construction X equal to S + A to split off.",
                    "label": 0
                },
                {
                    "sent": "The additive Gaussian noise from the cumula tensor and have it vanish.",
                    "label": 0
                },
                {
                    "sent": "And then just using the definition.",
                    "label": 0
                },
                {
                    "sent": "Of.",
                    "label": 0
                },
                {
                    "sent": "The operation we get the first large summation.",
                    "label": 0
                },
                {
                    "sent": "And then it's basically expanding everything by multilinear properties and having all the cross terms cancel out by the independence assumptions.",
                    "label": 0
                },
                {
                    "sent": "In doing this, we're able to get into the claim form.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We then arrive at.",
                    "label": 0
                },
                {
                    "sent": "A firm that basically states our algorithm again.",
                    "label": 0
                },
                {
                    "sent": "And if you take QX applied to the identity matrix, invert that result, and apply QX to that result, we get a matrix M of the form JDA transpose.",
                    "label": 0
                },
                {
                    "sent": "Where D has strictly positive diagonal entries.",
                    "label": 0
                },
                {
                    "sent": "And particularly, the queue entry is one over the norm of the QS column of a ^2.",
                    "label": 0
                },
                {
                    "sent": "And this follows directly from the previous lemma.",
                    "label": 0
                },
                {
                    "sent": "And then if we take a decomposition BB transpose of this resulting matrix M, then the inverse is going to be a quasi whitening matrix.",
                    "label": 1
                },
                {
                    "sent": "So to see why this is, we just note that first duty to 1/2 is real, since D is positive as strictly positive entries on the diagonal matrix square root exists.",
                    "label": 0
                },
                {
                    "sent": "And is real valued, so we're able to take the decomposition and left and right multiply B&B, transpose, inverse.",
                    "label": 1
                },
                {
                    "sent": "To create this decomposition, B inverse AD to the 1/2 times its transpose.",
                    "label": 0
                },
                {
                    "sent": "So in particular.",
                    "label": 0
                },
                {
                    "sent": "This implies that the inverse AD to the 1/2 is a rotation matrix.",
                    "label": 0
                },
                {
                    "sent": "And then just.",
                    "label": 1
                },
                {
                    "sent": "Inverting dealer 1/2 we can put on the other side and we get the inverse.",
                    "label": 0
                },
                {
                    "sent": "A is a rotation matrix times a diagonal matrix.",
                    "label": 1
                },
                {
                    "sent": "Which gives precisely that the inverse is a quasi whitening matrix.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then the main result is that the sample version of this algorithm is actually.",
                    "label": 1
                },
                {
                    "sent": "Requires only polynomial samples, so it's an efficient algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we're taking QX.",
                    "label": 0
                },
                {
                    "sent": "Here is the case in cystic estimate.",
                    "label": 0
                },
                {
                    "sent": "Of the.",
                    "label": 0
                },
                {
                    "sent": "Observed random variable, its fourth or a tensor.",
                    "label": 0
                },
                {
                    "sent": "The first bullet point is saying, roughly that if you take two of the latent signals that are distinct after quasi whitening, they will act in approximately orthogonal directions.",
                    "label": 0
                },
                {
                    "sent": "Using cosine error.",
                    "label": 0
                },
                {
                    "sent": "And then the second bullet point is saying that.",
                    "label": 0
                },
                {
                    "sent": "The scale associated with each latent signal after the quasi whitening will be close to what we would expect from the construction.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's polynomial samples and things that arise fairly naturally, so it's like the condition number of a.",
                    "label": 0
                },
                {
                    "sent": "The ratio between the.",
                    "label": 0
                },
                {
                    "sent": "4th Cumulant the maximum.",
                    "label": 1
                },
                {
                    "sent": "4th cumulative the latent signals.",
                    "label": 0
                },
                {
                    "sent": "Some dependence on the standard deviation of the Gaussian noise and also one over D1 over epsilon where.",
                    "label": 0
                },
                {
                    "sent": "Epsilon is allowable Aaron.",
                    "label": 0
                },
                {
                    "sent": "Besides here.",
                    "label": 0
                },
                {
                    "sent": "I don't state it explicitly here, but it's the sample size will be polynomially large.",
                    "label": 0
                },
                {
                    "sent": "And these.",
                    "label": 0
                },
                {
                    "sent": "Parameters.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so this is then the quasi whitening algorithm.",
                    "label": 0
                },
                {
                    "sent": "Restated.",
                    "label": 0
                },
                {
                    "sent": "And So what we're doing is then just using the sample version of the algorithm applied by the theorem.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So QX, then being the case to stick estimate.",
                    "label": 0
                },
                {
                    "sent": "And then we're applying this transform to each and every single data point to quasi white on the data.",
                    "label": 0
                },
                {
                    "sent": "And the algorithm is probably efficient for the first step of ICA.",
                    "label": 0
                },
                {
                    "sent": "And it's compatible with variations on existing methods for the second step of ICA.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}