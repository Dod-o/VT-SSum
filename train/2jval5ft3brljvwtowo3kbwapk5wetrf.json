{
    "id": "2jval5ft3brljvwtowo3kbwapk5wetrf",
    "title": "Anti-Learning",
    "info": {
        "author": [
            "Adam Kowalczyk, National ICT Australia"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "February 2006",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss06au_kowalczyk_al/",
    "segmentation": [
        [
            "So Adam is all about jeans.",
            "Proteins and all that kind of stuff, and it's also developing core painting.",
            "Adam started first time I I knew Adam.",
            "My guest was in a in a Telstra research like this context switches.",
            "Strategies may be strange quivalent of build apps or something like that.",
            "And then you would fill in McAllen Cancer Institute in Melbourne for you before I need to put in so that we can start off with big project in the calendar look really good.",
            "Thanks Adam.",
            "Thanks Zach.",
            "I think many things were accurate.",
            "Guru I don't think I don't agree with that one.",
            "I mean it's real.",
            "I never learn biology in my life up except for what I was doing in there.",
            "So you can't be good with so many things I don't know, but it's embarrassing.",
            "But biologics still like talking to me, which is good.",
            "That's all what I need.",
            "An I was supposed to say few things about myself.",
            "So actually my PhD was in mathematics.",
            "I was doing very sort of exotic differential geometry for years.",
            "After that I was doing a little bit of physics doing something.",
            "Varieties of functions and so on.",
            "When I came to Australia end up in Telstra and sort of mathematics was disappearing, kind up with working with artificial intelligence section and there were doing typical things.",
            "Customer management, speech recognition, sort of human voice interfaces and so on.",
            "That's what people were doing in 80s.",
            "And finally end up doing data mining and when I was.",
            "Persuaded to come for a for a year to Peter Maccallum, actually to work with micro race.",
            "And after that I didn't want to go back to Telstra anymore.",
            "So I end up in Nick 'cause these guys were kind enough to provide funding for doing something which I believe is worthwhile doing.",
            "I mean, working with cancer and micro race and biology, I think it's all exciting.",
            "So what I want to talk today is actually something which is a problem which I encountered in my biological investigations and this something which is very weird and the name is until learning is something which is really counter intuitive and.",
            "Many people find this very sort of repulsive and not acceptable, but I managed to convince some of them and one of them is Alex smaller, right?",
            "We've written together some papers that he was sort of strongly saying that he prefers basket he prefers, but this doesn't exist.",
            "But actually it does exist and that actually is interesting in its own, because it shows that in all this learning, which we're doing, there are still things which we really hardly understand.",
            "It means there's plenty of room for.",
            "Or innovative research.",
            "And obviously you want to do something interesting in your life.",
            "So there is a chance.",
            "I'm not saying that you have to work with anti learning, but probably there are other problems which in spite of everything would be claiming actually very poorly understand and more.",
            "We actually start working with sooner.",
            "Later we get to this interesting problems.",
            "So not everything is done.",
            "Plenty is left for everybody.",
            "OK, so what's anti learning?",
            "Let's go to.",
            "2A."
        ],
        [
            "Review, so I want to.",
            "I want to start with a very simple example in two dimensions just to convince you that weird things could happen.",
            "And actually you could see them.",
            "So it's a sort of intuitive model which is behind some theory which I would like to explain in the end and then a few words about natural data were actually something like that could be observed when some synthetic data which is very interesting because it's a nontrivial sampling model.",
            "Which shows these properties and actually and which could be solved also analytically.",
            "So you could actually develop some some rezoning which is quite nontrivial and final on its own.",
            "And some sort of initial Fiore, and again is full of gaps and so on is just something what we managed to do so far.",
            "Eh?"
        ],
        [
            "OK, so very very sort of heuristic high level terms.",
            "What's anti learning?",
            "So it is a situation when you train your supervised learning classifier and obviously on training data set it should perform better than random guessing right?",
            "Because that's what we really hope for, otherwise it doesn't make any sense.",
            "However, usually you like independent is also to perform better than random guessing.",
            "However there are situations when actually it systematically performing worse than random guessing.",
            "And when you see that you say, OK, how that could be?",
            "Can you actually did something wrong, right?",
            "So first you look for errors.",
            "You can't find errors when you start thinking and try to find examples.",
            "Try to explain, try to generalize, invent, try to justify what's happening in natural data, which you have observed working in the 1st place.",
            "So that's sort of the cycle.",
            "OK, so usually.",
            "OK, it's a good question.",
            "This is something related to testing, so usually have training set and test set.",
            "Now of training.",
            "So I mean what what?",
            "That means, that which we're not seen in training.",
            "So let's think about scenario of final data set.",
            "I'm using something for training and then I'm talking about data which was completely not seen in training 'cause they often when you do IID testing you do overlap.",
            "It makes sense.",
            "Think is that in this situation is classifiers behave distinctly differently on data which were non seen in training data which were seen in training.",
            "So I'm interested in this off training because this is more interesting because on training data we know whatever we've seen in training we.",
            "Predict perfectly close to Patrick at least.",
            "But this other bit is so this thing, so that's what I mean by of training.",
            "Thanks."
        ],
        [
            "OK, so let's go to our low dimensional example.",
            "So what's that?",
            "This is X or problem right?",
            "Four points on plane labeled plus minus one and so on.",
            "And what we want we want basically to learn how to classify this by linear classifiers.",
            "It means by lines which split our.",
            "Play into positive and negative examples.",
            "OK.",
            "So let's.",
            "OK, I hope it's soon, but let's do our first experiment.",
            "So let's sample from this data set.",
            "This is who our space for points only.",
            "We sample training samples.",
            "So we take these two points for training right?",
            "That and that one.",
            "Now we develope classify our hyperplane right?",
            "So that's President hyperplane right through passing through Van Meter maximum margin and so that's positive such that negative side right?",
            "So this is training data, everything is perfect on training data.",
            "When we go to all training data which were not significant and you see that visiting exactly on opposite side.",
            "When it was designed.",
            "So OK could happen, right?",
            "So let's go to this example.",
            "OK, so we sample two data points.",
            "What happens if you sample three of them?",
            "OK, I need to press.",
            "Plus OK, so similar situation you sample three points from the data set.",
            "This is off training point.",
            "You don't see you develop your classifier again.",
            "This is a hyperplane sensible solution.",
            "This is positive side negative side.",
            "You go to your test example and sitting on the wrong side again.",
            "So due to symmetry, you immediately see that whenever you sample from this data set, a proper training subset.",
            "Anything which was not used in training would be misclassified.",
            "If you have full labels in training and test right?",
            "So this is example of perfect until learning data set.",
            "Obviously say OK, big deal.",
            "I mean that is not linear separable, so it's something with everybody knows that X or is not linearly separable and so OK.",
            "So let's do small modification.",
            "Now let's introduce third axis and so that we lift this negative examples little bit down positive little bit up so everything lives in three dimensions.",
            "Of course, if this shift is little bit.",
            "Qualitatively, we dealing with the same situation, but in principle that ice perfectly separable, right?",
            "This horizontal XY hyperplane which is perpendicular to that axis perfectly could classify data.",
            "Problem is that whenever you subsample from this data set a subset proper subset you would never sensibly think about this hyperplane as a sensible solution.",
            "Because this data, if you say, take these three points as these three points, obviously will suggest to you that you should pick separator which is perpendicular to X axis, write something which looks like that which we know will be classified.",
            "The 3rd fourth point.",
            "So that's the situation.",
            "When data is linear separable.",
            "However, whenever we have proper incomplete subset, we looking in a completely wrong direction.",
            "We are picking the wrong solution in a sense.",
            "OK. My ask a question, yeah, isn't just means that means that your model doesn't estimate your data perfectly, that you have to change your model actually OK, so how would you suggest me to change?",
            "But I mean just intuition, yeah, true?",
            "I mean, that's typical statistical approach.",
            "That's how how we think.",
            "Except I don't know how because I mean of course in this year we see it's obvious right?",
            "But when we see multidimensional situation, it's not that obvious so.",
            "Yep, this problem would disappear if you have lots of doctors.",
            "Or you could at least hang up.",
            "OK, my space contains 4 data points.",
            "Obviously if I over sampled this and I've seen this of course so that immediately goes to the major point that these until learning occurs in situation where might sample is small.",
            "If I have large sample which is sufficient to give me information about distribution, everything will disappear.",
            "So that's fine.",
            "That's where ordinary learning theory works.",
            "When you when you have sample above science or times VC dimension of your class, then obviously you sample enough distribution for now.",
            "Question is what I do in situations when actually I don't have that sample and my problems which I mentioned among biological ones are.",
            "But I have 100 samples in 10,000 dimensions and question is what you do then.",
            "Yeah, so it's a good question.",
            "We are sort of going deeper and deeper, but.",
            "Hey well.",
            "OK, so."
        ],
        [
            "We would like to somehow discuss situations more, by the way.",
            "Unfortunately, as everybody else I, I changed my slides.",
            "I forgot to say that right?",
            "I thought it goes without saying sorry for that I will put them on web.",
            "Number of slides.",
            "Yeah, work in progress, you know.",
            "Difficult to resist, not not to add new stuff.",
            "OK, so we have to go in our investigation a little bit beyond say X orinfor points.",
            "So for that we need some sensible evaluation measure and sensibly evaluation.",
            "In this case is area under receiver operating characteristic which I called error.",
            "Some people call it differently.",
            "Area under the curve and so on.",
            "And so I mean, yeah, I think we will have two populations in red and blue and try to find estimator which will separate this data.",
            "So if you have a particular estimator, it will map this data into one dimensional axis, so that becomes a true distributions blue and red, and when you shift your decision point decision threshold, you end up with a curve which is a lot of false positive versus through positive.",
            "I hope you know that I don't need to go for this, and in particular what I'm just in this area under this rock curve.",
            "That's my measure of.",
            "A of the.",
            "My metric of performance and this area has very nice probabilistic characterization.",
            "This actually you statistiques which is looking on order over points.",
            "If you order using score of this estimator and virtually this is probability that your estimator is ordering data properly.",
            "So it means that data with negative labor getting smaller score when data with positive labor.",
            "So that's exactly this probability.",
            "Modulo some correction in the case when you have draws the same score allocated to different labels, so small correction you have to, but that's really very nice, probabilistic.",
            "A characteristic characterization of this.",
            "Let someone imaginal, Eric is just.",
            "Our measure of performance and what is important is the following.",
            "But if this error is .5, this is random guessing.",
            "So random classifier should roughly follow this line.",
            "So .5 is random guessing.",
            "One is perfect.",
            "Everything is must be properly, properly order and 0 means that everything has to be misordered so everything every one of these guys have to be in the opposite direction.",
            "That that's the only option for 440 so."
        ],
        [
            "OK, and that's my sort of informal definition, so learning scenario.",
            "No.",
            "OK, it's not the site I was expecting.",
            "This should be a little bit up there, But anyway I fix it later so this is learning scenario right?",
            "But in training we have positive outlooks and in tests we have something better than random guessing and until learning is that situation which we are interested in.",
            "Yeah.",
            "And well, number of questions right?",
            "What can you do with the data if you see this, can you actually do something fun?",
            "A method which in spite of this is producing something like that something which is consistent?",
            "Because I mean simple method is that actually you reverse classifier and that goes up.",
            "Problem is that on training that all of a sudden this curve goes down, so it's no good.",
            "What you need is a method which produces consistent classifier 'cause if you go to medical doctors and say OK look I have diagnostic.",
            "Test which performs like that, but they come on.",
            "You must be joking, and that's exactly the problem I have right now.",
            "It's sitting on the data for two years and they have no courage to publish that are behave like that this year.",
            "Finally, we agreed that we will probably find out."
        ],
        [
            "OK, so let's go to my problem.",
            "Until anneken cancer genomics.",
            "And let's go strike."
        ],
        [
            "The heart of it.",
            "OK, so this is one of the data sets where this.",
            "Until then it was we encountered this anti learning.",
            "So what was done and this is a PET scan of supportive cancer right?",
            "This is patient with the Council here and so.",
            "A in this country at least.",
            "Usually you give chemo and radiotherapy to patient and hope that cancer will go and it works in 30% of cases in 70% of cases it doesn't work.",
            "Patient is suffering for a month or six weeks and after that it goes for resection and that's it could be fertile because it was delayed.",
            "So we want actually to find.",
            "It would be very good to predict who is good responders who is bad to the treatment, So what they've done they just selected a cohort of patients with 60 something patient, 63 patients.",
            "From each patient, they pick little bit of tissue from from tumor and little bit of from next to two more.",
            "So we have two more risk issue and normal tissue.",
            "Then this tissue was micro rate.",
            "So what Michael Ray is doing this is a technology which is looking for gene expression.",
            "We can think like.",
            "Activity of a gene in petition and we can measure of our special techniques and so on, is just all magic, at least to me.",
            "It's just very mixing different things.",
            "Look like water to me, and then they claim that they know which Gene was more active in VR.",
            "But that's how I see.",
            "But we all believe that this is true.",
            "And actually this actually.",
            "This shows that it must be something in it.",
            "So what they've done, they have 126 samples, some from normal tissue, some from consoles tissue, and they pass this through a. Unsupervised learning through clustering program which clustered in this direction.",
            "It was 10,000 genes and in this direction 126 samples and all of a sudden you see pattern.",
            "You start seeing this mess 'cause enormous Matthew see patterns in sort of a red Patch Redpath Redpath so program automatically separated data into normal tissue.",
            "And two histological subtypes, two cancers in there where adenocarcinoma and squamous cell carcinoma, two types of cells in which this cancer rises and histologically you could distinguish them.",
            "So it's separated very nicely into these two groups.",
            "So that's beautiful.",
            "It means that there is some patterning it.",
            "However, what we want is to find who from this group was good responder to the treatment.",
            "The same to this group clustering didn't help us at all.",
            "So what you do?",
            "Supervised learning.",
            "So in this case, OK, we have results for liver now test.",
            "So what you're doing you take for example schema cohort.",
            "You remove 1 sample.",
            "You train your classifier when you test on the remaining sample and you go through all sorts of 30 odd samples and then you plot your Roc curve.",
            "Looks very nicely, so this is tested.",
            "Cousin training, everything was perfect.",
            "It's not interesting at all.",
            "Test set, that's the only interesting thing.",
            "So it works nicely.",
            "However, when we looked on this.",
            "Adenocarcinoma, that's our test picture.",
            "That's what we saw and so far, no matter what we tried picture, it looks somewhat like that or just follows random guessing.",
            "We can't get to the right side unless we cheat and bias test or something.",
            "That's fine, everything works, but.",
            "That's what we observe.",
            "And once you faced with such problem questions, OK, what can you do?",
            "So one option is forget about data and do something more interesting or change to different problems.",
            "Another option is actually try to understand and explain, because maybe that's what it is and that's the nature of the data.",
            "So obviously we're being curious.",
            "Researchers were actually trying to understand.",
            "Firstly, weapon this is possible and then what to do with this data?",
            "IDK."
        ],
        [
            "Alright, so we've seen this plot.",
            "And here is sort of another test in which actually we repeated previous experiment.",
            "It was Levon out this time we're sampling 2/3 of the data and testing only 1/3 and that was repeated 100 times.",
            "And so this is sorry too fast.",
            "1 third, 80% sampled, 24 Tests and 5050 so depends on the color where yellow is probably very unfortunate with some blue, some dark blue, light blue and red, one is leaving out.",
            "So this is for schema which everything is in my learnable side.",
            "Over 50% but interesting case for us.",
            "Is Abner, right?",
            "Everything, urgency is sitting below 50%.",
            "And you could see another very interesting, intriguing pattern.",
            "We're using more.",
            "OK, go from the from the bottom up.",
            "50% was used for training, 66 for training, 80 for training.",
            "It means we're increasing more and more data, giving for system to train.",
            "So what you expect machine should learn better pattern.",
            "Now look on the lines where exactly in the reverse order 50% is sitting on top.",
            "A yellow 66 is below and 80 is even even lower, right?",
            "So boy you give data for learning this machine seems to be strongly going into this until learning.",
            "Direction, and this is Levon out which is sitting even lower.",
            "So that's the pattern.",
            "That's what we observe, right?",
            "Everything was repeated 100 times.",
            "These are averages.",
            "OK, so you look.",
            "And what bars are sort of standard deviation for in this population.",
            "OK, you look on that and your first question is, is it actually significant or is it noise what we're observing or something, right?",
            "You could easily devise a simple test to test significance and.",
            "Perhaps I should add it here so So what we doing?",
            "Let's form our null hypothesis, but actually sample population distribution looks like that with mean say sitting on 50% or above 50%.",
            "So what I mean by population is.",
            "Let's say I'm repeating the same experiment.",
            "Through all possible subsets I'm sampling from my data, say 2/3 of the data.",
            "With particular number of genes each time, so it's my training sample, I select my genes, create my model, test on the remaining data set, and I repeat this going over all possible data set.",
            "So that would be my each time I'm observing certain value of area under Roc curve on the test, so I will get some distribution and let's assume that actually population distribution is sitting at 50 or below.",
            "So that's distribution now.",
            "However, in our particular series of say, 100 repeats, which we've done, we observe mean sitting actually somewhat below 50%, and you may ask yourself question how significant it is, how what's probability that I could observe value this or below by random chance, which is a P value of the test, and you go through.",
            "Straight forward.",
            "A exercise, So what you're doing this was your population.",
            "Now here what I draw is a curve which was for averages.",
            "Because I'm I have 100 repeated means I sampled 100 times from this distribution will look like that.",
            "So mean again will be the same as this population mean except standard deviation will be much smaller.",
            "We now it's 10 times smaller because it was hundred repeats.",
            "And distribution is approximately normal and then OK, let's go to the next one.",
            "And we can calculate this P value, right?",
            "So if if my mean is sitting here, so my P value will be area under this normal curve over here normal testing procedure and you estimate this from sample and so on and go to the table and read OK.",
            "So when we do it, what we get?",
            "This low over here corresponds to this Lloyds unfortunately different computer and this was excellent.",
            "Is something else, but what you observe on these values and you see how significant they are.",
            "This is 10 to minus 51 not taking 10 to minus 3 -- 50 -- 100 because I mean so really if you look on average what we observe you can't sit by random chance must be something special in pattern in the data which generates this whatever it is.",
            "But that's not random chance.",
            "What we observe, which OK."
        ],
        [
            "OK, so that was the case.",
            "Of this concert genomics, when we when we saw this data with microarrays with this strange behavior think is that this is not only data set on which something like that has been observed.",
            "So now I want to switch to completely different biological data set classification of genes in yeast.",
            "So here.",
            "I'm talking about that."
        ],
        [
            "Which were used for KDD Cup in 202 and there was a particular pathway?",
            "And used in competition.",
            "So how competition look like.",
            "You were given a list of 3000 genes like that.",
            "Some of them were marked so 38 were marked by change and remaining up to 85 whereby console.",
            "So there were two different pathways of interest.",
            "And then you were given 1500 other genes in yeast.",
            "And they said OK, please create model which will predict which of these genes should belong to this class and which of these should belong to two classes joined together.",
            "So we're two supervised learning tasks given.",
            "So, OK, so that's your data.",
            "Obviously you can't do much from web, because velocity of unintelligible symbols and so on.",
            "But what it is is that our target class is very small, which is interesting only below 83,000 so far.",
            "Below operating below 3% of the data.",
            "So strong bias in the data and what was given for building models.",
            "OK very given abstracts from Medline.",
            "So it was 16,000 of abstracts.",
            "And saying OK, abstract such and such is relevant to diggin this to this gene and so on and so forth.",
            "And this is text of the abstract.",
            "Don't bother to read.",
            "I don't understand what it is.",
            "It's full of biological jargon, which doesn't mean much to me.",
            "And then now and didn't mean anything at the time of the competition.",
            "We treated this formally.",
            "So we use all these hedging files and as we put into the bag of words.",
            "We've done this with a colleague of mine.",
            "She never learned biology, nevermind by logic.",
            "What else could you do?",
            "Cycle ignore and that's what we've done.",
            "I mean, it's sadly, but that's what it was.",
            "OK, so there was some other data.",
            "Writes a verse 18,000 of abstract you have gene to gene interactions.",
            "Again, you have two symbols and apparently they interact with each other and very some hierarchical information.",
            "Again, more tax, and this is some hierarchy of respiration energy and so on.",
            "Subcellular location and so on, so it's some additional information about function over jeans and so on and so forth.",
            "And.",
            "And of course.",
            "Gene localization where they are.",
            "And in the end we have a list of Gin Alley assist because obviously in in different abstracts people use different symbols.",
            "So just to make life a little bit more difficult for you.",
            "So anyway, so we took this all together.",
            "We pass somehow what I want to point here is that all information is textual.",
            "We never see here a single measurement number in so fragile cancer.",
            "We measure this sort of gene expression, which is like a physical measurement, continuous number.",
            "Here everything is texture.",
            "This is a taxonomy of a gene, which is fine and onward because they extracted this from web at that time in formation.",
            "OK, so completely different data set.",
            "OK, so we process this on weekend because I worked the time for tests and of course East jeans is not what you supposed to do.",
            "Junior working power.",
            "So we've done it on Saturday and then actually we came.",
            "We came to work on Monday and my colleague of mine saying guess what?",
            "I've done work but nothing seems to be working because that's what we saw right point 4145.",
            "So what would you do with this?",
            "Of course you go through debugging which spent half a day trying to find it, but it didn't exist in vehicle.",
            "And in desperation we start manipulating and what we've done.",
            "I try to explain, you see so here that I was severely unbalanced, right?",
            "So that corresponded to balancing data by relating to make it equal.",
            "So this minority class was boosted by factor of 100 / 3%.",
            "That means whatever 33 and so on.",
            "So that was a balance situation.",
            "Most sensible situation with the which didn't work, and each one of them is actually Roc curve.",
            "So this is for one task and these are two tasks of the competition.",
            "This was some functional task which was not interesting.",
            "But listen, this was in competition.",
            "So when we discuss when we start actually discounting majority class, this 3000 genes we start actually to make them less and less influential.",
            "And when we know them out completely, all of sudden we saw that things are working.",
            "So somehow we discovered that in this case what works is actually one class SVM.",
            "When you remove majority class, use only minority one point 3% of the data, 2.8% of the data and you get something like.",
            "It's very weird.",
            "Unusual, but we had nothing better to do, so that's what we submitted, and surprisingly, it was the best solution in the competition.",
            "OK, so these are official."
        ],
        [
            "Results let's go slide for the next one and so on.",
            "So this was this this weird solution which worked in 14,000 dimensions and use on the first date examples in 84 and that was the best guess you could have done with this.",
            "However, what's more interesting is this corner.",
            "Over here you see, these are official results in the competition and looks like another of our colleagues competitors submit solutions which actually we're sitting below .5.",
            "When I saw this, I thought, OK, that's really interesting because I could.",
            "I think that I was doing something really bad or something strange with the data, so I got my sort of anti learning result.",
            "But these people obviously done something completely different and they also.",
            "All the same thing, obviously they didn't cross check they submitted whatever they have and they end up here.",
            "They were embarrassed, but I don't think those reason to be embarrassed because this data is weird, that's why.",
            "What I always say anyway, many other people actually did some sensible job.",
            "This solution is actually was done by hand, so they submit in 1000 abstracts by Fenton.",
            "Features by hand and obviously very done.",
            "Very good job.",
            "However, we didn't read anything and also we've done good job.",
            "So good point for machine learning.",
            "Something could be done if you lucky habit.",
            "OK. And after after this competition we.",
            "We took the whole data and we repeated this ourselves.",
            "So how you repeat data?",
            "It was very least the hidden test set after competition.",
            "So we had four and half thousand of jeans and we split this multiple times into 2/3 one third and test it so this again ever reach of 100 trials.",
            "And what you see?",
            "This is SVM 2 plus SVM.",
            "So on this axis we have regularization constant, right?",
            "So it's small parameter C. And this is very low.",
            "See very high see right now it could be higher, but whatever it is in the model it was right.",
            "And now this corresponds to data when you actually balance classes with manipulating this learning constant.",
            "So boosting minority class and so on.",
            "So that corresponds to the balance data.",
            "This green curve and what you see is that if you have high regularization constant is actually sitting and what we see here is obviously test test data was training.",
            "Everything works fine for at least for SVN.",
            "Always 100% or closely, but impairs.",
            "That's what you see, right?",
            "So you see that if you have very low, very low regularization, constant actually works nice, but at the time we never thought about using that low regularization constant because it didn't make any sense to us.",
            "We always stop somewhere here.",
            "About one or 10 or whatever in our setup.",
            "When you have high regulation, consent actually learns, but test comes on a completely wrong side, OK?",
            "And here, here, that one is actually pink.",
            "One is one class SVM.",
            "So that was a winner of the competition.",
            "This is also one class SVM, but train on majority class right?",
            "So it was 3000 or something examples and that always sitting down here.",
            "And this is centroid.",
            "This is another very triggered classifier, so you are dealing here with linear situation, right?",
            "You have a two class problem.",
            "So you have positive examples here.",
            "Here you have negative examples.",
            "And what centroid is doing?",
            "You find the center of mass of negative Guys, center of mass of positive guys and you take this direction and you could take hyperplane in the middle trivial calculation or computation and on this data set as you see works quite quite well, which was surprised which is surprised as well.",
            "So many working in this data set.",
            "But that's life.",
            "That's how this data looks like.",
            "OK, so that was done for linear classifiers.",
            "Then Oliver Sharper done extra experiments completely independently using here a normalized data standardized to 00 mean and unit standard deviation, and then he applied a number of advanced with with kernels.",
            "With polynomial candles of different degree and his curves look like that very similar to that one.",
            "So again, you have this until learning Parton and actually hired the kernel goes down, and so on.",
            "So again, something.",
            "Works a little bit in wrong direction.",
            "With that OK."
        ],
        [
            "So.",
            "So so far we covered 2 natural data sets.",
            "What I wanted only to show there that very situation with something weird and unexplainable.",
            "Innocence is happening in data.",
            "At least two data sets which I know very few other data sets, but it's not so convincing.",
            "Now I wanted to to tend to synthetic data which is, which is somewhat somewhat different, but also it's weird enough.",
            "And it looks almost like natural.",
            "So what we're doing here we're trying to model a experiment with forgery of money.",
            "OK, So what?"
        ],
        [
            "This means we try to make a perfect forgery.",
            "It means we look on the nodes and we actually we try to exactly repeat every pixel on this 5050 dollar node.",
            "And so obviously this big number of pixels.",
            "I don't know 1,000,000 or something.",
            "And so this is really high dimensional problem which we encounter here.",
            "OK.",
            "So how how you will deal with this?",
            "So you will.",
            "First you will just get a sample of original nodes, preferably fresh nodes, right brand fresh nodes so you have a pile of such nodes.",
            "When you just inspect these nodes.",
            "And after inspection you find sort of characteristics of these nodes.",
            "Then you print your faults.",
            "False notes now question is OK, so we developed this question is how well we doing so.",
            "How can you test it and pattern is very simple right?",
            "You mix all these together into one pile.",
            "Of course you keep track of labels when you split into two parts you train your classifier on one of them and test on the other one and see whether you are able to tell the difference.",
            "That's natural way of dealing with this.",
            "If you do supervised learning.",
            "OK, so.",
            "But obviously we need to.",
            "We won't be dealing with notes as such because it's little bit illegal, so we have to have some sort of proxy abstract proxy.",
            "So what we've done in our experiments was the following.",
            "We created a this natural genuine population by sampling from a high dimensional distribution, so it's actually normal distributions for the dimensions.",
            "For each dimension separate.",
            "Distribution with different mean and different standard deviations.",
            "This K in the bracket is just a index of the coordinates.",
            "I follow a statistical paper and that's what they use or we have this little bit weird notation.",
            "So we are sampling this high dimensional vectors and each node correspond to watch 1 sample from this distribution, right?",
            "So we have full power of and then we do estimation of these moments because they are hidden from us.",
            "Just from the sample.",
            "So given certain sample we just do sample estimate.",
            "From this sample, right?",
            "So we just forget about this this bit over here we just take mean and standard deviation in a typical unbiased way from the sample for this distribution for each coordinate separately, right?",
            "Because we assume here for simplicity, but these are independent guys and then we draw our fake sample from this estimated distribution and again experiment.",
            "We just were sampling number of genuine and fake nodes was taken randomly from from.",
            "We use 10 and 10,000 dimensions and we are sampling this also from random distribution separately in each run and.",
            "These are results.",
            "OK, so let's try to look on one column.",
            "What we have here here is maximal margin classifier, right?",
            "So this is performance on the training data set.",
            "So in the end we split into training and test our mixture and on training everything was working perfect on off training data something is going below 50 so this is percent what I'm showing here is percentage of area under receiver operating characteristics or 100% is perfect.",
            "Zero is quite opposite and 50% is a random guessing.",
            "So this all guys are strict below random guessing.",
            "Now what we have here here is a size of a training set.",
            "So for training our classifier, we're sampling 510 thirty 5070% of the data, and you see the trend over here, right?",
            "More you sample bigger training sample.",
            "Performance is closer to 0, so this is for maximal margin.",
            "This is for support vector regressions, centroid, bizar, exponentiated gradient's algorithm and so on.",
            "Which again, if you look on that one, it's also until learning is weaker, but it's still under learning and some people were claiming that actually this is prone to until learning and should work.",
            "It turns up experimentally, but it's not the case at all.",
            "OK, so this is another sort of a sampling model statistical model in which we can we observe this something weird happening.",
            "With this and after break, I would like actually to give some explanation if I have enough time, what's going on in this model?",
            "Because we can solve it analytically, asymptotically, so it's possible to gain some insight was behind this.",
            "Um?"
        ],
        [
            "OK, so that was.",
            "You have to wait until.",
            "Type is gone.",
            "OK, so that was one synthetic data versus another recipe for doing until learning data based on other metrics.",
            "So we use this in our experiment, so let me explain what other metrics this is.",
            "Orthogonal matrix with plus minus one entries orthogonal means.",
            "But if you take dot product of two rows it gives you 0.",
            "And recipes were following you take a role in particular any particular except this first one, because it's not interesting all once any other will do.",
            "So you take one row and you use this for labeling and then you take the remaining data which are obtained after removing this row.",
            "And this is what that will look like, but that will be your data, right?",
            "One, dimension less.",
            "So everything but this role.",
            "And these are labels.",
            "That's all what you need to do.",
            "And that's the data which you create.",
            "That way.",
            "And kernel metrics will look like that very symmetric with some plus minus ones here and there.",
            "That's all.",
            "So this is now in our experiments.",
            "We're actually also adding some noise to this to this other metrics just.",
            "Just for fun.",
            "I mean it's otherwise it's too easy.",
            "So the terministic.",
            "And here."
        ],
        [
            "Results of the test which we've done.",
            "So this is this noisy adima metrics when added where we added noise right?",
            "So this again you have area or a rock on vertical axis.",
            "Here is Sigma over noise.",
            "So here we're going from almost no noise to very heavy noise of variance.",
            "Standard deviation 10 right and our entries are plus minus one.",
            "So obviously you expect not to see anything.",
            "And yes, all classifiers actually are doing random guessing, so more noise you add, they go closer to 50%.",
            "Which is random guessing.",
            "However, if noise is smaller.",
            "All of them are sitting or most of them are sitting down here.",
            "So here what we have is support vector machines with linear, quadratic and cubic kernel.",
            "This is green one which we hardly see because it's hidden under this red one is actually for radial basis kernel.",
            "And this is central.",
            "It again is hidden and so on.",
            "So anyway, what we see here, this is again test only because in training everything works fine.",
            "In test again we see values close to 0.",
            "And here are some other classifiers, like back probe Decision Tree which perform here so backpropagation didn't cope with this.",
            "But of course maybe I had the wrong code and I can't use only one code, but I believe that this is just a difficult problem with this with symmetry for that one naive Bayes actually works very nicely.",
            "I mean until learning so is good rich regression again and so.",
            "So anyway, looks like again number, but all classifiers we tested were sitting below 50% on average no matter what.",
            "Which seems that this sort of pattern is somehow detectable, but different degree.",
            "OK, this is 1 most practices we've seen this part previously.",
            "This is this uniquely with 10,000 dimensions.",
            "This is this other metrics, but also after rotation.",
            "Think is that this pattern plus minus ones is very specific.",
            "And if you pass the pattern unchanged, plus minus ones, the way I suggested two decision tree and you have good solver, it will find X or structure it will it will end this.",
            "But only Becausw features which are using very very specific representation and it will detect this.",
            "So it was sort of surprised, but now we understand.",
            "However, once you rotate the whole thing.",
            "Then obviously everything disappears and then.",
            "You start seeing until learning with decision trees.",
            "Exponentiated gradient on this rotated Adam are very nicely until ends.",
            "So again that sort of cross checks and confirms with various this sort of weird pattern which at the moment, at least in these classes of algorithms, we can't really create consistent classifier 'cause you have always this sort of dichotomy.",
            "It's very nice training and very sort of back testing, but systematically.",
            "So again, if you have classified which is always lying.",
            "I mean, you know what to do is just reverse decision and you are right.",
            "So actually you have something which you could.",
            "You could do something if you need practical, except you have to be prepared psychologically to do it and that's that's the threshold which we need to cross across fast.",
            "And it's not easy.",
            "OK, so.",
            "One more slide before I think the break.",
            "So after all this, let's maybe finish with some formal definitions, because I mean we're sort of intuitively talking about until any concern we roughly see what it is, but questions?",
            "Can you somehow quantify this?",
            "And here you have also additional definition, which we call perfect until learning.",
            "So something perfect.",
            "OK, maybe you don't expect this to exist in life.",
            "However, perfect is usually much is amenable to formal.",
            "Analysis So it's actually not not bad thing.",
            "So this is definition which we came up to.",
            "And.",
            "It's a slightly different language, but it says the following, but we say, but other classifiers perfectly until learning if the following happen.",
            "But if you create your estimator using sample training data, so this is.",
            "Slapping which we train.",
            "And then you test on data which was not used in training, so this is off training data, not using training.",
            "Then the labor which this thing will attach, and then must exist a threshold which is universal.",
            "So we have to adjust always threshold.",
            "And but exist such a threshold that every label, every sign is wrong.",
            "So very some such point when actually your normal classification rate is maximal, because everything will be wrong, so that's what we say perfect until adding in a sort of weak sense.",
            "We don't.",
            "We are not saying hear anything about what's happening on training data, right?",
            "It's only on test because it's convenient.",
            "So, but you could add this extra condition as well without much.",
            "Huh now OK, so that was a definition which was on the level of individual sample right?",
            "Look here, I'm saying any sample which is not used in training will be mislabeled, but this condition of course is this.",
            "B is a little bit troublesome because you have to adjust this properly if you don't do properly then everything will be classified on level of 50%.",
            "So to see something other than 50% you have to have some special values of B.",
            "So this formulation when you use area with respect to overall complement of a training set is innocence threshold free.",
            "So it's actually more convenient measure.",
            "This is equivalent to existence and so on, but it's far more convenient to use.",
            "Because you don't worry about this B and so on.",
            "It's easier to express and actually cleaner that way.",
            "OK, so that was this perfect until learning and then our former until learning reiterating was the situation.",
            "The following when we have to specify some test.",
            "So say sampling.",
            "Let's say we sample our data 50 times in specific way, create models and so on and then we see that in the test set.",
            "So on the independent test set our average is below 50% and on the training data is above 50.",
            "So this sort of working definition capturing what we have seen.",
            "With natural data previously, so that's our sort of working definition of anti learning, but in for remaining I will rather talk about that one because it's easier that will imply this one under some module.",
            "Extra assumption on this part, right?",
            "So that's in essence more interesting and that's why we're looking on that one because it's analytically more tractable.",
            "Actually we can prove theorems about when things are until learning perfectly, we can't.",
            "Do much about this bit yet, but maybe one day.",
            "I think for me is very good moment to make a break.",
            "OK, OK.",
            "I see.",
            "Yes, I forgot my charger.",
            "You could you could I just?",
            "Oh OK, that's doesn't have much OK.",
            "So let's have this two minutes break and."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Adam is all about jeans.",
                    "label": 0
                },
                {
                    "sent": "Proteins and all that kind of stuff, and it's also developing core painting.",
                    "label": 0
                },
                {
                    "sent": "Adam started first time I I knew Adam.",
                    "label": 0
                },
                {
                    "sent": "My guest was in a in a Telstra research like this context switches.",
                    "label": 0
                },
                {
                    "sent": "Strategies may be strange quivalent of build apps or something like that.",
                    "label": 0
                },
                {
                    "sent": "And then you would fill in McAllen Cancer Institute in Melbourne for you before I need to put in so that we can start off with big project in the calendar look really good.",
                    "label": 0
                },
                {
                    "sent": "Thanks Adam.",
                    "label": 0
                },
                {
                    "sent": "Thanks Zach.",
                    "label": 0
                },
                {
                    "sent": "I think many things were accurate.",
                    "label": 0
                },
                {
                    "sent": "Guru I don't think I don't agree with that one.",
                    "label": 0
                },
                {
                    "sent": "I mean it's real.",
                    "label": 0
                },
                {
                    "sent": "I never learn biology in my life up except for what I was doing in there.",
                    "label": 0
                },
                {
                    "sent": "So you can't be good with so many things I don't know, but it's embarrassing.",
                    "label": 0
                },
                {
                    "sent": "But biologics still like talking to me, which is good.",
                    "label": 0
                },
                {
                    "sent": "That's all what I need.",
                    "label": 0
                },
                {
                    "sent": "An I was supposed to say few things about myself.",
                    "label": 0
                },
                {
                    "sent": "So actually my PhD was in mathematics.",
                    "label": 0
                },
                {
                    "sent": "I was doing very sort of exotic differential geometry for years.",
                    "label": 0
                },
                {
                    "sent": "After that I was doing a little bit of physics doing something.",
                    "label": 0
                },
                {
                    "sent": "Varieties of functions and so on.",
                    "label": 0
                },
                {
                    "sent": "When I came to Australia end up in Telstra and sort of mathematics was disappearing, kind up with working with artificial intelligence section and there were doing typical things.",
                    "label": 0
                },
                {
                    "sent": "Customer management, speech recognition, sort of human voice interfaces and so on.",
                    "label": 0
                },
                {
                    "sent": "That's what people were doing in 80s.",
                    "label": 0
                },
                {
                    "sent": "And finally end up doing data mining and when I was.",
                    "label": 0
                },
                {
                    "sent": "Persuaded to come for a for a year to Peter Maccallum, actually to work with micro race.",
                    "label": 0
                },
                {
                    "sent": "And after that I didn't want to go back to Telstra anymore.",
                    "label": 0
                },
                {
                    "sent": "So I end up in Nick 'cause these guys were kind enough to provide funding for doing something which I believe is worthwhile doing.",
                    "label": 0
                },
                {
                    "sent": "I mean, working with cancer and micro race and biology, I think it's all exciting.",
                    "label": 0
                },
                {
                    "sent": "So what I want to talk today is actually something which is a problem which I encountered in my biological investigations and this something which is very weird and the name is until learning is something which is really counter intuitive and.",
                    "label": 0
                },
                {
                    "sent": "Many people find this very sort of repulsive and not acceptable, but I managed to convince some of them and one of them is Alex smaller, right?",
                    "label": 0
                },
                {
                    "sent": "We've written together some papers that he was sort of strongly saying that he prefers basket he prefers, but this doesn't exist.",
                    "label": 0
                },
                {
                    "sent": "But actually it does exist and that actually is interesting in its own, because it shows that in all this learning, which we're doing, there are still things which we really hardly understand.",
                    "label": 0
                },
                {
                    "sent": "It means there's plenty of room for.",
                    "label": 0
                },
                {
                    "sent": "Or innovative research.",
                    "label": 0
                },
                {
                    "sent": "And obviously you want to do something interesting in your life.",
                    "label": 0
                },
                {
                    "sent": "So there is a chance.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying that you have to work with anti learning, but probably there are other problems which in spite of everything would be claiming actually very poorly understand and more.",
                    "label": 0
                },
                {
                    "sent": "We actually start working with sooner.",
                    "label": 0
                },
                {
                    "sent": "Later we get to this interesting problems.",
                    "label": 0
                },
                {
                    "sent": "So not everything is done.",
                    "label": 0
                },
                {
                    "sent": "Plenty is left for everybody.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's anti learning?",
                    "label": 0
                },
                {
                    "sent": "Let's go to.",
                    "label": 0
                },
                {
                    "sent": "2A.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Review, so I want to.",
                    "label": 0
                },
                {
                    "sent": "I want to start with a very simple example in two dimensions just to convince you that weird things could happen.",
                    "label": 0
                },
                {
                    "sent": "And actually you could see them.",
                    "label": 0
                },
                {
                    "sent": "So it's a sort of intuitive model which is behind some theory which I would like to explain in the end and then a few words about natural data were actually something like that could be observed when some synthetic data which is very interesting because it's a nontrivial sampling model.",
                    "label": 0
                },
                {
                    "sent": "Which shows these properties and actually and which could be solved also analytically.",
                    "label": 0
                },
                {
                    "sent": "So you could actually develop some some rezoning which is quite nontrivial and final on its own.",
                    "label": 0
                },
                {
                    "sent": "And some sort of initial Fiore, and again is full of gaps and so on is just something what we managed to do so far.",
                    "label": 0
                },
                {
                    "sent": "Eh?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so very very sort of heuristic high level terms.",
                    "label": 0
                },
                {
                    "sent": "What's anti learning?",
                    "label": 0
                },
                {
                    "sent": "So it is a situation when you train your supervised learning classifier and obviously on training data set it should perform better than random guessing right?",
                    "label": 0
                },
                {
                    "sent": "Because that's what we really hope for, otherwise it doesn't make any sense.",
                    "label": 0
                },
                {
                    "sent": "However, usually you like independent is also to perform better than random guessing.",
                    "label": 1
                },
                {
                    "sent": "However there are situations when actually it systematically performing worse than random guessing.",
                    "label": 0
                },
                {
                    "sent": "And when you see that you say, OK, how that could be?",
                    "label": 0
                },
                {
                    "sent": "Can you actually did something wrong, right?",
                    "label": 0
                },
                {
                    "sent": "So first you look for errors.",
                    "label": 0
                },
                {
                    "sent": "You can't find errors when you start thinking and try to find examples.",
                    "label": 0
                },
                {
                    "sent": "Try to explain, try to generalize, invent, try to justify what's happening in natural data, which you have observed working in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the cycle.",
                    "label": 0
                },
                {
                    "sent": "OK, so usually.",
                    "label": 0
                },
                {
                    "sent": "OK, it's a good question.",
                    "label": 0
                },
                {
                    "sent": "This is something related to testing, so usually have training set and test set.",
                    "label": 0
                },
                {
                    "sent": "Now of training.",
                    "label": 0
                },
                {
                    "sent": "So I mean what what?",
                    "label": 0
                },
                {
                    "sent": "That means, that which we're not seen in training.",
                    "label": 0
                },
                {
                    "sent": "So let's think about scenario of final data set.",
                    "label": 0
                },
                {
                    "sent": "I'm using something for training and then I'm talking about data which was completely not seen in training 'cause they often when you do IID testing you do overlap.",
                    "label": 0
                },
                {
                    "sent": "It makes sense.",
                    "label": 0
                },
                {
                    "sent": "Think is that in this situation is classifiers behave distinctly differently on data which were non seen in training data which were seen in training.",
                    "label": 0
                },
                {
                    "sent": "So I'm interested in this off training because this is more interesting because on training data we know whatever we've seen in training we.",
                    "label": 0
                },
                {
                    "sent": "Predict perfectly close to Patrick at least.",
                    "label": 0
                },
                {
                    "sent": "But this other bit is so this thing, so that's what I mean by of training.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's go to our low dimensional example.",
                    "label": 0
                },
                {
                    "sent": "So what's that?",
                    "label": 0
                },
                {
                    "sent": "This is X or problem right?",
                    "label": 0
                },
                {
                    "sent": "Four points on plane labeled plus minus one and so on.",
                    "label": 0
                },
                {
                    "sent": "And what we want we want basically to learn how to classify this by linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "It means by lines which split our.",
                    "label": 0
                },
                {
                    "sent": "Play into positive and negative examples.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let's.",
                    "label": 0
                },
                {
                    "sent": "OK, I hope it's soon, but let's do our first experiment.",
                    "label": 0
                },
                {
                    "sent": "So let's sample from this data set.",
                    "label": 0
                },
                {
                    "sent": "This is who our space for points only.",
                    "label": 0
                },
                {
                    "sent": "We sample training samples.",
                    "label": 0
                },
                {
                    "sent": "So we take these two points for training right?",
                    "label": 0
                },
                {
                    "sent": "That and that one.",
                    "label": 0
                },
                {
                    "sent": "Now we develope classify our hyperplane right?",
                    "label": 0
                },
                {
                    "sent": "So that's President hyperplane right through passing through Van Meter maximum margin and so that's positive such that negative side right?",
                    "label": 0
                },
                {
                    "sent": "So this is training data, everything is perfect on training data.",
                    "label": 0
                },
                {
                    "sent": "When we go to all training data which were not significant and you see that visiting exactly on opposite side.",
                    "label": 0
                },
                {
                    "sent": "When it was designed.",
                    "label": 0
                },
                {
                    "sent": "So OK could happen, right?",
                    "label": 0
                },
                {
                    "sent": "So let's go to this example.",
                    "label": 0
                },
                {
                    "sent": "OK, so we sample two data points.",
                    "label": 0
                },
                {
                    "sent": "What happens if you sample three of them?",
                    "label": 0
                },
                {
                    "sent": "OK, I need to press.",
                    "label": 0
                },
                {
                    "sent": "Plus OK, so similar situation you sample three points from the data set.",
                    "label": 0
                },
                {
                    "sent": "This is off training point.",
                    "label": 0
                },
                {
                    "sent": "You don't see you develop your classifier again.",
                    "label": 0
                },
                {
                    "sent": "This is a hyperplane sensible solution.",
                    "label": 0
                },
                {
                    "sent": "This is positive side negative side.",
                    "label": 0
                },
                {
                    "sent": "You go to your test example and sitting on the wrong side again.",
                    "label": 0
                },
                {
                    "sent": "So due to symmetry, you immediately see that whenever you sample from this data set, a proper training subset.",
                    "label": 0
                },
                {
                    "sent": "Anything which was not used in training would be misclassified.",
                    "label": 0
                },
                {
                    "sent": "If you have full labels in training and test right?",
                    "label": 0
                },
                {
                    "sent": "So this is example of perfect until learning data set.",
                    "label": 0
                },
                {
                    "sent": "Obviously say OK, big deal.",
                    "label": 0
                },
                {
                    "sent": "I mean that is not linear separable, so it's something with everybody knows that X or is not linearly separable and so OK.",
                    "label": 0
                },
                {
                    "sent": "So let's do small modification.",
                    "label": 0
                },
                {
                    "sent": "Now let's introduce third axis and so that we lift this negative examples little bit down positive little bit up so everything lives in three dimensions.",
                    "label": 0
                },
                {
                    "sent": "Of course, if this shift is little bit.",
                    "label": 0
                },
                {
                    "sent": "Qualitatively, we dealing with the same situation, but in principle that ice perfectly separable, right?",
                    "label": 0
                },
                {
                    "sent": "This horizontal XY hyperplane which is perpendicular to that axis perfectly could classify data.",
                    "label": 0
                },
                {
                    "sent": "Problem is that whenever you subsample from this data set a subset proper subset you would never sensibly think about this hyperplane as a sensible solution.",
                    "label": 0
                },
                {
                    "sent": "Because this data, if you say, take these three points as these three points, obviously will suggest to you that you should pick separator which is perpendicular to X axis, write something which looks like that which we know will be classified.",
                    "label": 0
                },
                {
                    "sent": "The 3rd fourth point.",
                    "label": 0
                },
                {
                    "sent": "So that's the situation.",
                    "label": 0
                },
                {
                    "sent": "When data is linear separable.",
                    "label": 0
                },
                {
                    "sent": "However, whenever we have proper incomplete subset, we looking in a completely wrong direction.",
                    "label": 0
                },
                {
                    "sent": "We are picking the wrong solution in a sense.",
                    "label": 0
                },
                {
                    "sent": "OK. My ask a question, yeah, isn't just means that means that your model doesn't estimate your data perfectly, that you have to change your model actually OK, so how would you suggest me to change?",
                    "label": 0
                },
                {
                    "sent": "But I mean just intuition, yeah, true?",
                    "label": 0
                },
                {
                    "sent": "I mean, that's typical statistical approach.",
                    "label": 0
                },
                {
                    "sent": "That's how how we think.",
                    "label": 0
                },
                {
                    "sent": "Except I don't know how because I mean of course in this year we see it's obvious right?",
                    "label": 0
                },
                {
                    "sent": "But when we see multidimensional situation, it's not that obvious so.",
                    "label": 0
                },
                {
                    "sent": "Yep, this problem would disappear if you have lots of doctors.",
                    "label": 0
                },
                {
                    "sent": "Or you could at least hang up.",
                    "label": 0
                },
                {
                    "sent": "OK, my space contains 4 data points.",
                    "label": 0
                },
                {
                    "sent": "Obviously if I over sampled this and I've seen this of course so that immediately goes to the major point that these until learning occurs in situation where might sample is small.",
                    "label": 0
                },
                {
                    "sent": "If I have large sample which is sufficient to give me information about distribution, everything will disappear.",
                    "label": 0
                },
                {
                    "sent": "So that's fine.",
                    "label": 0
                },
                {
                    "sent": "That's where ordinary learning theory works.",
                    "label": 0
                },
                {
                    "sent": "When you when you have sample above science or times VC dimension of your class, then obviously you sample enough distribution for now.",
                    "label": 0
                },
                {
                    "sent": "Question is what I do in situations when actually I don't have that sample and my problems which I mentioned among biological ones are.",
                    "label": 0
                },
                {
                    "sent": "But I have 100 samples in 10,000 dimensions and question is what you do then.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's a good question.",
                    "label": 0
                },
                {
                    "sent": "We are sort of going deeper and deeper, but.",
                    "label": 0
                },
                {
                    "sent": "Hey well.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We would like to somehow discuss situations more, by the way.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, as everybody else I, I changed my slides.",
                    "label": 0
                },
                {
                    "sent": "I forgot to say that right?",
                    "label": 0
                },
                {
                    "sent": "I thought it goes without saying sorry for that I will put them on web.",
                    "label": 0
                },
                {
                    "sent": "Number of slides.",
                    "label": 0
                },
                {
                    "sent": "Yeah, work in progress, you know.",
                    "label": 0
                },
                {
                    "sent": "Difficult to resist, not not to add new stuff.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have to go in our investigation a little bit beyond say X orinfor points.",
                    "label": 0
                },
                {
                    "sent": "So for that we need some sensible evaluation measure and sensibly evaluation.",
                    "label": 0
                },
                {
                    "sent": "In this case is area under receiver operating characteristic which I called error.",
                    "label": 1
                },
                {
                    "sent": "Some people call it differently.",
                    "label": 0
                },
                {
                    "sent": "Area under the curve and so on.",
                    "label": 0
                },
                {
                    "sent": "And so I mean, yeah, I think we will have two populations in red and blue and try to find estimator which will separate this data.",
                    "label": 0
                },
                {
                    "sent": "So if you have a particular estimator, it will map this data into one dimensional axis, so that becomes a true distributions blue and red, and when you shift your decision point decision threshold, you end up with a curve which is a lot of false positive versus through positive.",
                    "label": 0
                },
                {
                    "sent": "I hope you know that I don't need to go for this, and in particular what I'm just in this area under this rock curve.",
                    "label": 0
                },
                {
                    "sent": "That's my measure of.",
                    "label": 0
                },
                {
                    "sent": "A of the.",
                    "label": 0
                },
                {
                    "sent": "My metric of performance and this area has very nice probabilistic characterization.",
                    "label": 0
                },
                {
                    "sent": "This actually you statistiques which is looking on order over points.",
                    "label": 0
                },
                {
                    "sent": "If you order using score of this estimator and virtually this is probability that your estimator is ordering data properly.",
                    "label": 0
                },
                {
                    "sent": "So it means that data with negative labor getting smaller score when data with positive labor.",
                    "label": 0
                },
                {
                    "sent": "So that's exactly this probability.",
                    "label": 0
                },
                {
                    "sent": "Modulo some correction in the case when you have draws the same score allocated to different labels, so small correction you have to, but that's really very nice, probabilistic.",
                    "label": 0
                },
                {
                    "sent": "A characteristic characterization of this.",
                    "label": 0
                },
                {
                    "sent": "Let someone imaginal, Eric is just.",
                    "label": 0
                },
                {
                    "sent": "Our measure of performance and what is important is the following.",
                    "label": 0
                },
                {
                    "sent": "But if this error is .5, this is random guessing.",
                    "label": 0
                },
                {
                    "sent": "So random classifier should roughly follow this line.",
                    "label": 0
                },
                {
                    "sent": "So .5 is random guessing.",
                    "label": 0
                },
                {
                    "sent": "One is perfect.",
                    "label": 0
                },
                {
                    "sent": "Everything is must be properly, properly order and 0 means that everything has to be misordered so everything every one of these guys have to be in the opposite direction.",
                    "label": 0
                },
                {
                    "sent": "That that's the only option for 440 so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and that's my sort of informal definition, so learning scenario.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not the site I was expecting.",
                    "label": 0
                },
                {
                    "sent": "This should be a little bit up there, But anyway I fix it later so this is learning scenario right?",
                    "label": 0
                },
                {
                    "sent": "But in training we have positive outlooks and in tests we have something better than random guessing and until learning is that situation which we are interested in.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And well, number of questions right?",
                    "label": 0
                },
                {
                    "sent": "What can you do with the data if you see this, can you actually do something fun?",
                    "label": 0
                },
                {
                    "sent": "A method which in spite of this is producing something like that something which is consistent?",
                    "label": 0
                },
                {
                    "sent": "Because I mean simple method is that actually you reverse classifier and that goes up.",
                    "label": 0
                },
                {
                    "sent": "Problem is that on training that all of a sudden this curve goes down, so it's no good.",
                    "label": 0
                },
                {
                    "sent": "What you need is a method which produces consistent classifier 'cause if you go to medical doctors and say OK look I have diagnostic.",
                    "label": 0
                },
                {
                    "sent": "Test which performs like that, but they come on.",
                    "label": 0
                },
                {
                    "sent": "You must be joking, and that's exactly the problem I have right now.",
                    "label": 0
                },
                {
                    "sent": "It's sitting on the data for two years and they have no courage to publish that are behave like that this year.",
                    "label": 0
                },
                {
                    "sent": "Finally, we agreed that we will probably find out.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's go to my problem.",
                    "label": 0
                },
                {
                    "sent": "Until anneken cancer genomics.",
                    "label": 0
                },
                {
                    "sent": "And let's go strike.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The heart of it.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is one of the data sets where this.",
                    "label": 0
                },
                {
                    "sent": "Until then it was we encountered this anti learning.",
                    "label": 0
                },
                {
                    "sent": "So what was done and this is a PET scan of supportive cancer right?",
                    "label": 0
                },
                {
                    "sent": "This is patient with the Council here and so.",
                    "label": 0
                },
                {
                    "sent": "A in this country at least.",
                    "label": 0
                },
                {
                    "sent": "Usually you give chemo and radiotherapy to patient and hope that cancer will go and it works in 30% of cases in 70% of cases it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Patient is suffering for a month or six weeks and after that it goes for resection and that's it could be fertile because it was delayed.",
                    "label": 0
                },
                {
                    "sent": "So we want actually to find.",
                    "label": 0
                },
                {
                    "sent": "It would be very good to predict who is good responders who is bad to the treatment, So what they've done they just selected a cohort of patients with 60 something patient, 63 patients.",
                    "label": 0
                },
                {
                    "sent": "From each patient, they pick little bit of tissue from from tumor and little bit of from next to two more.",
                    "label": 0
                },
                {
                    "sent": "So we have two more risk issue and normal tissue.",
                    "label": 0
                },
                {
                    "sent": "Then this tissue was micro rate.",
                    "label": 0
                },
                {
                    "sent": "So what Michael Ray is doing this is a technology which is looking for gene expression.",
                    "label": 0
                },
                {
                    "sent": "We can think like.",
                    "label": 0
                },
                {
                    "sent": "Activity of a gene in petition and we can measure of our special techniques and so on, is just all magic, at least to me.",
                    "label": 0
                },
                {
                    "sent": "It's just very mixing different things.",
                    "label": 0
                },
                {
                    "sent": "Look like water to me, and then they claim that they know which Gene was more active in VR.",
                    "label": 0
                },
                {
                    "sent": "But that's how I see.",
                    "label": 0
                },
                {
                    "sent": "But we all believe that this is true.",
                    "label": 0
                },
                {
                    "sent": "And actually this actually.",
                    "label": 0
                },
                {
                    "sent": "This shows that it must be something in it.",
                    "label": 0
                },
                {
                    "sent": "So what they've done, they have 126 samples, some from normal tissue, some from consoles tissue, and they pass this through a. Unsupervised learning through clustering program which clustered in this direction.",
                    "label": 0
                },
                {
                    "sent": "It was 10,000 genes and in this direction 126 samples and all of a sudden you see pattern.",
                    "label": 0
                },
                {
                    "sent": "You start seeing this mess 'cause enormous Matthew see patterns in sort of a red Patch Redpath Redpath so program automatically separated data into normal tissue.",
                    "label": 0
                },
                {
                    "sent": "And two histological subtypes, two cancers in there where adenocarcinoma and squamous cell carcinoma, two types of cells in which this cancer rises and histologically you could distinguish them.",
                    "label": 0
                },
                {
                    "sent": "So it's separated very nicely into these two groups.",
                    "label": 0
                },
                {
                    "sent": "So that's beautiful.",
                    "label": 0
                },
                {
                    "sent": "It means that there is some patterning it.",
                    "label": 0
                },
                {
                    "sent": "However, what we want is to find who from this group was good responder to the treatment.",
                    "label": 0
                },
                {
                    "sent": "The same to this group clustering didn't help us at all.",
                    "label": 0
                },
                {
                    "sent": "So what you do?",
                    "label": 0
                },
                {
                    "sent": "Supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So in this case, OK, we have results for liver now test.",
                    "label": 0
                },
                {
                    "sent": "So what you're doing you take for example schema cohort.",
                    "label": 0
                },
                {
                    "sent": "You remove 1 sample.",
                    "label": 0
                },
                {
                    "sent": "You train your classifier when you test on the remaining sample and you go through all sorts of 30 odd samples and then you plot your Roc curve.",
                    "label": 0
                },
                {
                    "sent": "Looks very nicely, so this is tested.",
                    "label": 0
                },
                {
                    "sent": "Cousin training, everything was perfect.",
                    "label": 0
                },
                {
                    "sent": "It's not interesting at all.",
                    "label": 0
                },
                {
                    "sent": "Test set, that's the only interesting thing.",
                    "label": 0
                },
                {
                    "sent": "So it works nicely.",
                    "label": 0
                },
                {
                    "sent": "However, when we looked on this.",
                    "label": 0
                },
                {
                    "sent": "Adenocarcinoma, that's our test picture.",
                    "label": 0
                },
                {
                    "sent": "That's what we saw and so far, no matter what we tried picture, it looks somewhat like that or just follows random guessing.",
                    "label": 0
                },
                {
                    "sent": "We can't get to the right side unless we cheat and bias test or something.",
                    "label": 0
                },
                {
                    "sent": "That's fine, everything works, but.",
                    "label": 0
                },
                {
                    "sent": "That's what we observe.",
                    "label": 0
                },
                {
                    "sent": "And once you faced with such problem questions, OK, what can you do?",
                    "label": 0
                },
                {
                    "sent": "So one option is forget about data and do something more interesting or change to different problems.",
                    "label": 0
                },
                {
                    "sent": "Another option is actually try to understand and explain, because maybe that's what it is and that's the nature of the data.",
                    "label": 0
                },
                {
                    "sent": "So obviously we're being curious.",
                    "label": 0
                },
                {
                    "sent": "Researchers were actually trying to understand.",
                    "label": 0
                },
                {
                    "sent": "Firstly, weapon this is possible and then what to do with this data?",
                    "label": 0
                },
                {
                    "sent": "IDK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so we've seen this plot.",
                    "label": 0
                },
                {
                    "sent": "And here is sort of another test in which actually we repeated previous experiment.",
                    "label": 0
                },
                {
                    "sent": "It was Levon out this time we're sampling 2/3 of the data and testing only 1/3 and that was repeated 100 times.",
                    "label": 0
                },
                {
                    "sent": "And so this is sorry too fast.",
                    "label": 0
                },
                {
                    "sent": "1 third, 80% sampled, 24 Tests and 5050 so depends on the color where yellow is probably very unfortunate with some blue, some dark blue, light blue and red, one is leaving out.",
                    "label": 0
                },
                {
                    "sent": "So this is for schema which everything is in my learnable side.",
                    "label": 0
                },
                {
                    "sent": "Over 50% but interesting case for us.",
                    "label": 0
                },
                {
                    "sent": "Is Abner, right?",
                    "label": 0
                },
                {
                    "sent": "Everything, urgency is sitting below 50%.",
                    "label": 0
                },
                {
                    "sent": "And you could see another very interesting, intriguing pattern.",
                    "label": 0
                },
                {
                    "sent": "We're using more.",
                    "label": 0
                },
                {
                    "sent": "OK, go from the from the bottom up.",
                    "label": 0
                },
                {
                    "sent": "50% was used for training, 66 for training, 80 for training.",
                    "label": 0
                },
                {
                    "sent": "It means we're increasing more and more data, giving for system to train.",
                    "label": 0
                },
                {
                    "sent": "So what you expect machine should learn better pattern.",
                    "label": 0
                },
                {
                    "sent": "Now look on the lines where exactly in the reverse order 50% is sitting on top.",
                    "label": 0
                },
                {
                    "sent": "A yellow 66 is below and 80 is even even lower, right?",
                    "label": 0
                },
                {
                    "sent": "So boy you give data for learning this machine seems to be strongly going into this until learning.",
                    "label": 0
                },
                {
                    "sent": "Direction, and this is Levon out which is sitting even lower.",
                    "label": 0
                },
                {
                    "sent": "So that's the pattern.",
                    "label": 0
                },
                {
                    "sent": "That's what we observe, right?",
                    "label": 0
                },
                {
                    "sent": "Everything was repeated 100 times.",
                    "label": 0
                },
                {
                    "sent": "These are averages.",
                    "label": 0
                },
                {
                    "sent": "OK, so you look.",
                    "label": 0
                },
                {
                    "sent": "And what bars are sort of standard deviation for in this population.",
                    "label": 0
                },
                {
                    "sent": "OK, you look on that and your first question is, is it actually significant or is it noise what we're observing or something, right?",
                    "label": 0
                },
                {
                    "sent": "You could easily devise a simple test to test significance and.",
                    "label": 0
                },
                {
                    "sent": "Perhaps I should add it here so So what we doing?",
                    "label": 0
                },
                {
                    "sent": "Let's form our null hypothesis, but actually sample population distribution looks like that with mean say sitting on 50% or above 50%.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by population is.",
                    "label": 0
                },
                {
                    "sent": "Let's say I'm repeating the same experiment.",
                    "label": 0
                },
                {
                    "sent": "Through all possible subsets I'm sampling from my data, say 2/3 of the data.",
                    "label": 0
                },
                {
                    "sent": "With particular number of genes each time, so it's my training sample, I select my genes, create my model, test on the remaining data set, and I repeat this going over all possible data set.",
                    "label": 0
                },
                {
                    "sent": "So that would be my each time I'm observing certain value of area under Roc curve on the test, so I will get some distribution and let's assume that actually population distribution is sitting at 50 or below.",
                    "label": 0
                },
                {
                    "sent": "So that's distribution now.",
                    "label": 0
                },
                {
                    "sent": "However, in our particular series of say, 100 repeats, which we've done, we observe mean sitting actually somewhat below 50%, and you may ask yourself question how significant it is, how what's probability that I could observe value this or below by random chance, which is a P value of the test, and you go through.",
                    "label": 0
                },
                {
                    "sent": "Straight forward.",
                    "label": 0
                },
                {
                    "sent": "A exercise, So what you're doing this was your population.",
                    "label": 0
                },
                {
                    "sent": "Now here what I draw is a curve which was for averages.",
                    "label": 0
                },
                {
                    "sent": "Because I'm I have 100 repeated means I sampled 100 times from this distribution will look like that.",
                    "label": 0
                },
                {
                    "sent": "So mean again will be the same as this population mean except standard deviation will be much smaller.",
                    "label": 0
                },
                {
                    "sent": "We now it's 10 times smaller because it was hundred repeats.",
                    "label": 0
                },
                {
                    "sent": "And distribution is approximately normal and then OK, let's go to the next one.",
                    "label": 0
                },
                {
                    "sent": "And we can calculate this P value, right?",
                    "label": 0
                },
                {
                    "sent": "So if if my mean is sitting here, so my P value will be area under this normal curve over here normal testing procedure and you estimate this from sample and so on and go to the table and read OK.",
                    "label": 0
                },
                {
                    "sent": "So when we do it, what we get?",
                    "label": 0
                },
                {
                    "sent": "This low over here corresponds to this Lloyds unfortunately different computer and this was excellent.",
                    "label": 0
                },
                {
                    "sent": "Is something else, but what you observe on these values and you see how significant they are.",
                    "label": 0
                },
                {
                    "sent": "This is 10 to minus 51 not taking 10 to minus 3 -- 50 -- 100 because I mean so really if you look on average what we observe you can't sit by random chance must be something special in pattern in the data which generates this whatever it is.",
                    "label": 0
                },
                {
                    "sent": "But that's not random chance.",
                    "label": 0
                },
                {
                    "sent": "What we observe, which OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that was the case.",
                    "label": 0
                },
                {
                    "sent": "Of this concert genomics, when we when we saw this data with microarrays with this strange behavior think is that this is not only data set on which something like that has been observed.",
                    "label": 0
                },
                {
                    "sent": "So now I want to switch to completely different biological data set classification of genes in yeast.",
                    "label": 1
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "I'm talking about that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which were used for KDD Cup in 202 and there was a particular pathway?",
                    "label": 0
                },
                {
                    "sent": "And used in competition.",
                    "label": 0
                },
                {
                    "sent": "So how competition look like.",
                    "label": 0
                },
                {
                    "sent": "You were given a list of 3000 genes like that.",
                    "label": 0
                },
                {
                    "sent": "Some of them were marked so 38 were marked by change and remaining up to 85 whereby console.",
                    "label": 0
                },
                {
                    "sent": "So there were two different pathways of interest.",
                    "label": 0
                },
                {
                    "sent": "And then you were given 1500 other genes in yeast.",
                    "label": 0
                },
                {
                    "sent": "And they said OK, please create model which will predict which of these genes should belong to this class and which of these should belong to two classes joined together.",
                    "label": 0
                },
                {
                    "sent": "So we're two supervised learning tasks given.",
                    "label": 0
                },
                {
                    "sent": "So, OK, so that's your data.",
                    "label": 0
                },
                {
                    "sent": "Obviously you can't do much from web, because velocity of unintelligible symbols and so on.",
                    "label": 0
                },
                {
                    "sent": "But what it is is that our target class is very small, which is interesting only below 83,000 so far.",
                    "label": 0
                },
                {
                    "sent": "Below operating below 3% of the data.",
                    "label": 0
                },
                {
                    "sent": "So strong bias in the data and what was given for building models.",
                    "label": 0
                },
                {
                    "sent": "OK very given abstracts from Medline.",
                    "label": 0
                },
                {
                    "sent": "So it was 16,000 of abstracts.",
                    "label": 0
                },
                {
                    "sent": "And saying OK, abstract such and such is relevant to diggin this to this gene and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "And this is text of the abstract.",
                    "label": 0
                },
                {
                    "sent": "Don't bother to read.",
                    "label": 0
                },
                {
                    "sent": "I don't understand what it is.",
                    "label": 0
                },
                {
                    "sent": "It's full of biological jargon, which doesn't mean much to me.",
                    "label": 0
                },
                {
                    "sent": "And then now and didn't mean anything at the time of the competition.",
                    "label": 0
                },
                {
                    "sent": "We treated this formally.",
                    "label": 0
                },
                {
                    "sent": "So we use all these hedging files and as we put into the bag of words.",
                    "label": 0
                },
                {
                    "sent": "We've done this with a colleague of mine.",
                    "label": 0
                },
                {
                    "sent": "She never learned biology, nevermind by logic.",
                    "label": 0
                },
                {
                    "sent": "What else could you do?",
                    "label": 0
                },
                {
                    "sent": "Cycle ignore and that's what we've done.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's sadly, but that's what it was.",
                    "label": 0
                },
                {
                    "sent": "OK, so there was some other data.",
                    "label": 0
                },
                {
                    "sent": "Writes a verse 18,000 of abstract you have gene to gene interactions.",
                    "label": 0
                },
                {
                    "sent": "Again, you have two symbols and apparently they interact with each other and very some hierarchical information.",
                    "label": 0
                },
                {
                    "sent": "Again, more tax, and this is some hierarchy of respiration energy and so on.",
                    "label": 0
                },
                {
                    "sent": "Subcellular location and so on, so it's some additional information about function over jeans and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And of course.",
                    "label": 0
                },
                {
                    "sent": "Gene localization where they are.",
                    "label": 0
                },
                {
                    "sent": "And in the end we have a list of Gin Alley assist because obviously in in different abstracts people use different symbols.",
                    "label": 0
                },
                {
                    "sent": "So just to make life a little bit more difficult for you.",
                    "label": 0
                },
                {
                    "sent": "So anyway, so we took this all together.",
                    "label": 0
                },
                {
                    "sent": "We pass somehow what I want to point here is that all information is textual.",
                    "label": 0
                },
                {
                    "sent": "We never see here a single measurement number in so fragile cancer.",
                    "label": 0
                },
                {
                    "sent": "We measure this sort of gene expression, which is like a physical measurement, continuous number.",
                    "label": 0
                },
                {
                    "sent": "Here everything is texture.",
                    "label": 0
                },
                {
                    "sent": "This is a taxonomy of a gene, which is fine and onward because they extracted this from web at that time in formation.",
                    "label": 0
                },
                {
                    "sent": "OK, so completely different data set.",
                    "label": 0
                },
                {
                    "sent": "OK, so we process this on weekend because I worked the time for tests and of course East jeans is not what you supposed to do.",
                    "label": 0
                },
                {
                    "sent": "Junior working power.",
                    "label": 0
                },
                {
                    "sent": "So we've done it on Saturday and then actually we came.",
                    "label": 0
                },
                {
                    "sent": "We came to work on Monday and my colleague of mine saying guess what?",
                    "label": 0
                },
                {
                    "sent": "I've done work but nothing seems to be working because that's what we saw right point 4145.",
                    "label": 0
                },
                {
                    "sent": "So what would you do with this?",
                    "label": 0
                },
                {
                    "sent": "Of course you go through debugging which spent half a day trying to find it, but it didn't exist in vehicle.",
                    "label": 0
                },
                {
                    "sent": "And in desperation we start manipulating and what we've done.",
                    "label": 0
                },
                {
                    "sent": "I try to explain, you see so here that I was severely unbalanced, right?",
                    "label": 0
                },
                {
                    "sent": "So that corresponded to balancing data by relating to make it equal.",
                    "label": 0
                },
                {
                    "sent": "So this minority class was boosted by factor of 100 / 3%.",
                    "label": 0
                },
                {
                    "sent": "That means whatever 33 and so on.",
                    "label": 0
                },
                {
                    "sent": "So that was a balance situation.",
                    "label": 0
                },
                {
                    "sent": "Most sensible situation with the which didn't work, and each one of them is actually Roc curve.",
                    "label": 0
                },
                {
                    "sent": "So this is for one task and these are two tasks of the competition.",
                    "label": 0
                },
                {
                    "sent": "This was some functional task which was not interesting.",
                    "label": 0
                },
                {
                    "sent": "But listen, this was in competition.",
                    "label": 0
                },
                {
                    "sent": "So when we discuss when we start actually discounting majority class, this 3000 genes we start actually to make them less and less influential.",
                    "label": 0
                },
                {
                    "sent": "And when we know them out completely, all of sudden we saw that things are working.",
                    "label": 0
                },
                {
                    "sent": "So somehow we discovered that in this case what works is actually one class SVM.",
                    "label": 0
                },
                {
                    "sent": "When you remove majority class, use only minority one point 3% of the data, 2.8% of the data and you get something like.",
                    "label": 0
                },
                {
                    "sent": "It's very weird.",
                    "label": 0
                },
                {
                    "sent": "Unusual, but we had nothing better to do, so that's what we submitted, and surprisingly, it was the best solution in the competition.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are official.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Results let's go slide for the next one and so on.",
                    "label": 0
                },
                {
                    "sent": "So this was this this weird solution which worked in 14,000 dimensions and use on the first date examples in 84 and that was the best guess you could have done with this.",
                    "label": 1
                },
                {
                    "sent": "However, what's more interesting is this corner.",
                    "label": 0
                },
                {
                    "sent": "Over here you see, these are official results in the competition and looks like another of our colleagues competitors submit solutions which actually we're sitting below .5.",
                    "label": 0
                },
                {
                    "sent": "When I saw this, I thought, OK, that's really interesting because I could.",
                    "label": 0
                },
                {
                    "sent": "I think that I was doing something really bad or something strange with the data, so I got my sort of anti learning result.",
                    "label": 0
                },
                {
                    "sent": "But these people obviously done something completely different and they also.",
                    "label": 0
                },
                {
                    "sent": "All the same thing, obviously they didn't cross check they submitted whatever they have and they end up here.",
                    "label": 0
                },
                {
                    "sent": "They were embarrassed, but I don't think those reason to be embarrassed because this data is weird, that's why.",
                    "label": 0
                },
                {
                    "sent": "What I always say anyway, many other people actually did some sensible job.",
                    "label": 0
                },
                {
                    "sent": "This solution is actually was done by hand, so they submit in 1000 abstracts by Fenton.",
                    "label": 0
                },
                {
                    "sent": "Features by hand and obviously very done.",
                    "label": 0
                },
                {
                    "sent": "Very good job.",
                    "label": 0
                },
                {
                    "sent": "However, we didn't read anything and also we've done good job.",
                    "label": 0
                },
                {
                    "sent": "So good point for machine learning.",
                    "label": 0
                },
                {
                    "sent": "Something could be done if you lucky habit.",
                    "label": 0
                },
                {
                    "sent": "OK. And after after this competition we.",
                    "label": 0
                },
                {
                    "sent": "We took the whole data and we repeated this ourselves.",
                    "label": 0
                },
                {
                    "sent": "So how you repeat data?",
                    "label": 0
                },
                {
                    "sent": "It was very least the hidden test set after competition.",
                    "label": 0
                },
                {
                    "sent": "So we had four and half thousand of jeans and we split this multiple times into 2/3 one third and test it so this again ever reach of 100 trials.",
                    "label": 0
                },
                {
                    "sent": "And what you see?",
                    "label": 0
                },
                {
                    "sent": "This is SVM 2 plus SVM.",
                    "label": 0
                },
                {
                    "sent": "So on this axis we have regularization constant, right?",
                    "label": 0
                },
                {
                    "sent": "So it's small parameter C. And this is very low.",
                    "label": 0
                },
                {
                    "sent": "See very high see right now it could be higher, but whatever it is in the model it was right.",
                    "label": 0
                },
                {
                    "sent": "And now this corresponds to data when you actually balance classes with manipulating this learning constant.",
                    "label": 0
                },
                {
                    "sent": "So boosting minority class and so on.",
                    "label": 0
                },
                {
                    "sent": "So that corresponds to the balance data.",
                    "label": 0
                },
                {
                    "sent": "This green curve and what you see is that if you have high regularization constant is actually sitting and what we see here is obviously test test data was training.",
                    "label": 0
                },
                {
                    "sent": "Everything works fine for at least for SVN.",
                    "label": 0
                },
                {
                    "sent": "Always 100% or closely, but impairs.",
                    "label": 0
                },
                {
                    "sent": "That's what you see, right?",
                    "label": 0
                },
                {
                    "sent": "So you see that if you have very low, very low regularization, constant actually works nice, but at the time we never thought about using that low regularization constant because it didn't make any sense to us.",
                    "label": 0
                },
                {
                    "sent": "We always stop somewhere here.",
                    "label": 0
                },
                {
                    "sent": "About one or 10 or whatever in our setup.",
                    "label": 0
                },
                {
                    "sent": "When you have high regulation, consent actually learns, but test comes on a completely wrong side, OK?",
                    "label": 0
                },
                {
                    "sent": "And here, here, that one is actually pink.",
                    "label": 1
                },
                {
                    "sent": "One is one class SVM.",
                    "label": 0
                },
                {
                    "sent": "So that was a winner of the competition.",
                    "label": 0
                },
                {
                    "sent": "This is also one class SVM, but train on majority class right?",
                    "label": 0
                },
                {
                    "sent": "So it was 3000 or something examples and that always sitting down here.",
                    "label": 0
                },
                {
                    "sent": "And this is centroid.",
                    "label": 0
                },
                {
                    "sent": "This is another very triggered classifier, so you are dealing here with linear situation, right?",
                    "label": 0
                },
                {
                    "sent": "You have a two class problem.",
                    "label": 0
                },
                {
                    "sent": "So you have positive examples here.",
                    "label": 0
                },
                {
                    "sent": "Here you have negative examples.",
                    "label": 0
                },
                {
                    "sent": "And what centroid is doing?",
                    "label": 0
                },
                {
                    "sent": "You find the center of mass of negative Guys, center of mass of positive guys and you take this direction and you could take hyperplane in the middle trivial calculation or computation and on this data set as you see works quite quite well, which was surprised which is surprised as well.",
                    "label": 0
                },
                {
                    "sent": "So many working in this data set.",
                    "label": 0
                },
                {
                    "sent": "But that's life.",
                    "label": 0
                },
                {
                    "sent": "That's how this data looks like.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was done for linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "Then Oliver Sharper done extra experiments completely independently using here a normalized data standardized to 00 mean and unit standard deviation, and then he applied a number of advanced with with kernels.",
                    "label": 0
                },
                {
                    "sent": "With polynomial candles of different degree and his curves look like that very similar to that one.",
                    "label": 0
                },
                {
                    "sent": "So again, you have this until learning Parton and actually hired the kernel goes down, and so on.",
                    "label": 0
                },
                {
                    "sent": "So again, something.",
                    "label": 0
                },
                {
                    "sent": "Works a little bit in wrong direction.",
                    "label": 0
                },
                {
                    "sent": "With that OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So so far we covered 2 natural data sets.",
                    "label": 0
                },
                {
                    "sent": "What I wanted only to show there that very situation with something weird and unexplainable.",
                    "label": 0
                },
                {
                    "sent": "Innocence is happening in data.",
                    "label": 0
                },
                {
                    "sent": "At least two data sets which I know very few other data sets, but it's not so convincing.",
                    "label": 0
                },
                {
                    "sent": "Now I wanted to to tend to synthetic data which is, which is somewhat somewhat different, but also it's weird enough.",
                    "label": 0
                },
                {
                    "sent": "And it looks almost like natural.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing here we're trying to model a experiment with forgery of money.",
                    "label": 0
                },
                {
                    "sent": "OK, So what?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This means we try to make a perfect forgery.",
                    "label": 0
                },
                {
                    "sent": "It means we look on the nodes and we actually we try to exactly repeat every pixel on this 5050 dollar node.",
                    "label": 0
                },
                {
                    "sent": "And so obviously this big number of pixels.",
                    "label": 1
                },
                {
                    "sent": "I don't know 1,000,000 or something.",
                    "label": 0
                },
                {
                    "sent": "And so this is really high dimensional problem which we encounter here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So how how you will deal with this?",
                    "label": 0
                },
                {
                    "sent": "So you will.",
                    "label": 0
                },
                {
                    "sent": "First you will just get a sample of original nodes, preferably fresh nodes, right brand fresh nodes so you have a pile of such nodes.",
                    "label": 0
                },
                {
                    "sent": "When you just inspect these nodes.",
                    "label": 0
                },
                {
                    "sent": "And after inspection you find sort of characteristics of these nodes.",
                    "label": 0
                },
                {
                    "sent": "Then you print your faults.",
                    "label": 0
                },
                {
                    "sent": "False notes now question is OK, so we developed this question is how well we doing so.",
                    "label": 0
                },
                {
                    "sent": "How can you test it and pattern is very simple right?",
                    "label": 0
                },
                {
                    "sent": "You mix all these together into one pile.",
                    "label": 0
                },
                {
                    "sent": "Of course you keep track of labels when you split into two parts you train your classifier on one of them and test on the other one and see whether you are able to tell the difference.",
                    "label": 0
                },
                {
                    "sent": "That's natural way of dealing with this.",
                    "label": 0
                },
                {
                    "sent": "If you do supervised learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "But obviously we need to.",
                    "label": 0
                },
                {
                    "sent": "We won't be dealing with notes as such because it's little bit illegal, so we have to have some sort of proxy abstract proxy.",
                    "label": 0
                },
                {
                    "sent": "So what we've done in our experiments was the following.",
                    "label": 0
                },
                {
                    "sent": "We created a this natural genuine population by sampling from a high dimensional distribution, so it's actually normal distributions for the dimensions.",
                    "label": 1
                },
                {
                    "sent": "For each dimension separate.",
                    "label": 0
                },
                {
                    "sent": "Distribution with different mean and different standard deviations.",
                    "label": 0
                },
                {
                    "sent": "This K in the bracket is just a index of the coordinates.",
                    "label": 0
                },
                {
                    "sent": "I follow a statistical paper and that's what they use or we have this little bit weird notation.",
                    "label": 0
                },
                {
                    "sent": "So we are sampling this high dimensional vectors and each node correspond to watch 1 sample from this distribution, right?",
                    "label": 0
                },
                {
                    "sent": "So we have full power of and then we do estimation of these moments because they are hidden from us.",
                    "label": 0
                },
                {
                    "sent": "Just from the sample.",
                    "label": 0
                },
                {
                    "sent": "So given certain sample we just do sample estimate.",
                    "label": 0
                },
                {
                    "sent": "From this sample, right?",
                    "label": 0
                },
                {
                    "sent": "So we just forget about this this bit over here we just take mean and standard deviation in a typical unbiased way from the sample for this distribution for each coordinate separately, right?",
                    "label": 0
                },
                {
                    "sent": "Because we assume here for simplicity, but these are independent guys and then we draw our fake sample from this estimated distribution and again experiment.",
                    "label": 0
                },
                {
                    "sent": "We just were sampling number of genuine and fake nodes was taken randomly from from.",
                    "label": 0
                },
                {
                    "sent": "We use 10 and 10,000 dimensions and we are sampling this also from random distribution separately in each run and.",
                    "label": 0
                },
                {
                    "sent": "These are results.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's try to look on one column.",
                    "label": 0
                },
                {
                    "sent": "What we have here here is maximal margin classifier, right?",
                    "label": 0
                },
                {
                    "sent": "So this is performance on the training data set.",
                    "label": 0
                },
                {
                    "sent": "So in the end we split into training and test our mixture and on training everything was working perfect on off training data something is going below 50 so this is percent what I'm showing here is percentage of area under receiver operating characteristics or 100% is perfect.",
                    "label": 0
                },
                {
                    "sent": "Zero is quite opposite and 50% is a random guessing.",
                    "label": 0
                },
                {
                    "sent": "So this all guys are strict below random guessing.",
                    "label": 0
                },
                {
                    "sent": "Now what we have here here is a size of a training set.",
                    "label": 0
                },
                {
                    "sent": "So for training our classifier, we're sampling 510 thirty 5070% of the data, and you see the trend over here, right?",
                    "label": 0
                },
                {
                    "sent": "More you sample bigger training sample.",
                    "label": 0
                },
                {
                    "sent": "Performance is closer to 0, so this is for maximal margin.",
                    "label": 0
                },
                {
                    "sent": "This is for support vector regressions, centroid, bizar, exponentiated gradient's algorithm and so on.",
                    "label": 0
                },
                {
                    "sent": "Which again, if you look on that one, it's also until learning is weaker, but it's still under learning and some people were claiming that actually this is prone to until learning and should work.",
                    "label": 0
                },
                {
                    "sent": "It turns up experimentally, but it's not the case at all.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is another sort of a sampling model statistical model in which we can we observe this something weird happening.",
                    "label": 0
                },
                {
                    "sent": "With this and after break, I would like actually to give some explanation if I have enough time, what's going on in this model?",
                    "label": 0
                },
                {
                    "sent": "Because we can solve it analytically, asymptotically, so it's possible to gain some insight was behind this.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that was.",
                    "label": 0
                },
                {
                    "sent": "You have to wait until.",
                    "label": 0
                },
                {
                    "sent": "Type is gone.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was one synthetic data versus another recipe for doing until learning data based on other metrics.",
                    "label": 0
                },
                {
                    "sent": "So we use this in our experiment, so let me explain what other metrics this is.",
                    "label": 0
                },
                {
                    "sent": "Orthogonal matrix with plus minus one entries orthogonal means.",
                    "label": 0
                },
                {
                    "sent": "But if you take dot product of two rows it gives you 0.",
                    "label": 0
                },
                {
                    "sent": "And recipes were following you take a role in particular any particular except this first one, because it's not interesting all once any other will do.",
                    "label": 0
                },
                {
                    "sent": "So you take one row and you use this for labeling and then you take the remaining data which are obtained after removing this row.",
                    "label": 0
                },
                {
                    "sent": "And this is what that will look like, but that will be your data, right?",
                    "label": 0
                },
                {
                    "sent": "One, dimension less.",
                    "label": 0
                },
                {
                    "sent": "So everything but this role.",
                    "label": 0
                },
                {
                    "sent": "And these are labels.",
                    "label": 0
                },
                {
                    "sent": "That's all what you need to do.",
                    "label": 0
                },
                {
                    "sent": "And that's the data which you create.",
                    "label": 0
                },
                {
                    "sent": "That way.",
                    "label": 0
                },
                {
                    "sent": "And kernel metrics will look like that very symmetric with some plus minus ones here and there.",
                    "label": 0
                },
                {
                    "sent": "That's all.",
                    "label": 0
                },
                {
                    "sent": "So this is now in our experiments.",
                    "label": 0
                },
                {
                    "sent": "We're actually also adding some noise to this to this other metrics just.",
                    "label": 0
                },
                {
                    "sent": "Just for fun.",
                    "label": 0
                },
                {
                    "sent": "I mean it's otherwise it's too easy.",
                    "label": 0
                },
                {
                    "sent": "So the terministic.",
                    "label": 0
                },
                {
                    "sent": "And here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results of the test which we've done.",
                    "label": 0
                },
                {
                    "sent": "So this is this noisy adima metrics when added where we added noise right?",
                    "label": 0
                },
                {
                    "sent": "So this again you have area or a rock on vertical axis.",
                    "label": 0
                },
                {
                    "sent": "Here is Sigma over noise.",
                    "label": 0
                },
                {
                    "sent": "So here we're going from almost no noise to very heavy noise of variance.",
                    "label": 0
                },
                {
                    "sent": "Standard deviation 10 right and our entries are plus minus one.",
                    "label": 0
                },
                {
                    "sent": "So obviously you expect not to see anything.",
                    "label": 0
                },
                {
                    "sent": "And yes, all classifiers actually are doing random guessing, so more noise you add, they go closer to 50%.",
                    "label": 0
                },
                {
                    "sent": "Which is random guessing.",
                    "label": 0
                },
                {
                    "sent": "However, if noise is smaller.",
                    "label": 0
                },
                {
                    "sent": "All of them are sitting or most of them are sitting down here.",
                    "label": 0
                },
                {
                    "sent": "So here what we have is support vector machines with linear, quadratic and cubic kernel.",
                    "label": 0
                },
                {
                    "sent": "This is green one which we hardly see because it's hidden under this red one is actually for radial basis kernel.",
                    "label": 0
                },
                {
                    "sent": "And this is central.",
                    "label": 0
                },
                {
                    "sent": "It again is hidden and so on.",
                    "label": 0
                },
                {
                    "sent": "So anyway, what we see here, this is again test only because in training everything works fine.",
                    "label": 0
                },
                {
                    "sent": "In test again we see values close to 0.",
                    "label": 0
                },
                {
                    "sent": "And here are some other classifiers, like back probe Decision Tree which perform here so backpropagation didn't cope with this.",
                    "label": 0
                },
                {
                    "sent": "But of course maybe I had the wrong code and I can't use only one code, but I believe that this is just a difficult problem with this with symmetry for that one naive Bayes actually works very nicely.",
                    "label": 0
                },
                {
                    "sent": "I mean until learning so is good rich regression again and so.",
                    "label": 0
                },
                {
                    "sent": "So anyway, looks like again number, but all classifiers we tested were sitting below 50% on average no matter what.",
                    "label": 0
                },
                {
                    "sent": "Which seems that this sort of pattern is somehow detectable, but different degree.",
                    "label": 0
                },
                {
                    "sent": "OK, this is 1 most practices we've seen this part previously.",
                    "label": 0
                },
                {
                    "sent": "This is this uniquely with 10,000 dimensions.",
                    "label": 0
                },
                {
                    "sent": "This is this other metrics, but also after rotation.",
                    "label": 0
                },
                {
                    "sent": "Think is that this pattern plus minus ones is very specific.",
                    "label": 0
                },
                {
                    "sent": "And if you pass the pattern unchanged, plus minus ones, the way I suggested two decision tree and you have good solver, it will find X or structure it will it will end this.",
                    "label": 0
                },
                {
                    "sent": "But only Becausw features which are using very very specific representation and it will detect this.",
                    "label": 0
                },
                {
                    "sent": "So it was sort of surprised, but now we understand.",
                    "label": 0
                },
                {
                    "sent": "However, once you rotate the whole thing.",
                    "label": 0
                },
                {
                    "sent": "Then obviously everything disappears and then.",
                    "label": 0
                },
                {
                    "sent": "You start seeing until learning with decision trees.",
                    "label": 0
                },
                {
                    "sent": "Exponentiated gradient on this rotated Adam are very nicely until ends.",
                    "label": 0
                },
                {
                    "sent": "So again that sort of cross checks and confirms with various this sort of weird pattern which at the moment, at least in these classes of algorithms, we can't really create consistent classifier 'cause you have always this sort of dichotomy.",
                    "label": 0
                },
                {
                    "sent": "It's very nice training and very sort of back testing, but systematically.",
                    "label": 0
                },
                {
                    "sent": "So again, if you have classified which is always lying.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know what to do is just reverse decision and you are right.",
                    "label": 0
                },
                {
                    "sent": "So actually you have something which you could.",
                    "label": 0
                },
                {
                    "sent": "You could do something if you need practical, except you have to be prepared psychologically to do it and that's that's the threshold which we need to cross across fast.",
                    "label": 0
                },
                {
                    "sent": "And it's not easy.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "One more slide before I think the break.",
                    "label": 0
                },
                {
                    "sent": "So after all this, let's maybe finish with some formal definitions, because I mean we're sort of intuitively talking about until any concern we roughly see what it is, but questions?",
                    "label": 0
                },
                {
                    "sent": "Can you somehow quantify this?",
                    "label": 0
                },
                {
                    "sent": "And here you have also additional definition, which we call perfect until learning.",
                    "label": 0
                },
                {
                    "sent": "So something perfect.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe you don't expect this to exist in life.",
                    "label": 0
                },
                {
                    "sent": "However, perfect is usually much is amenable to formal.",
                    "label": 0
                },
                {
                    "sent": "Analysis So it's actually not not bad thing.",
                    "label": 0
                },
                {
                    "sent": "So this is definition which we came up to.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It's a slightly different language, but it says the following, but we say, but other classifiers perfectly until learning if the following happen.",
                    "label": 0
                },
                {
                    "sent": "But if you create your estimator using sample training data, so this is.",
                    "label": 0
                },
                {
                    "sent": "Slapping which we train.",
                    "label": 0
                },
                {
                    "sent": "And then you test on data which was not used in training, so this is off training data, not using training.",
                    "label": 0
                },
                {
                    "sent": "Then the labor which this thing will attach, and then must exist a threshold which is universal.",
                    "label": 0
                },
                {
                    "sent": "So we have to adjust always threshold.",
                    "label": 0
                },
                {
                    "sent": "And but exist such a threshold that every label, every sign is wrong.",
                    "label": 0
                },
                {
                    "sent": "So very some such point when actually your normal classification rate is maximal, because everything will be wrong, so that's what we say perfect until adding in a sort of weak sense.",
                    "label": 0
                },
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "We are not saying hear anything about what's happening on training data, right?",
                    "label": 0
                },
                {
                    "sent": "It's only on test because it's convenient.",
                    "label": 0
                },
                {
                    "sent": "So, but you could add this extra condition as well without much.",
                    "label": 0
                },
                {
                    "sent": "Huh now OK, so that was a definition which was on the level of individual sample right?",
                    "label": 0
                },
                {
                    "sent": "Look here, I'm saying any sample which is not used in training will be mislabeled, but this condition of course is this.",
                    "label": 0
                },
                {
                    "sent": "B is a little bit troublesome because you have to adjust this properly if you don't do properly then everything will be classified on level of 50%.",
                    "label": 0
                },
                {
                    "sent": "So to see something other than 50% you have to have some special values of B.",
                    "label": 0
                },
                {
                    "sent": "So this formulation when you use area with respect to overall complement of a training set is innocence threshold free.",
                    "label": 0
                },
                {
                    "sent": "So it's actually more convenient measure.",
                    "label": 0
                },
                {
                    "sent": "This is equivalent to existence and so on, but it's far more convenient to use.",
                    "label": 0
                },
                {
                    "sent": "Because you don't worry about this B and so on.",
                    "label": 0
                },
                {
                    "sent": "It's easier to express and actually cleaner that way.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was this perfect until learning and then our former until learning reiterating was the situation.",
                    "label": 0
                },
                {
                    "sent": "The following when we have to specify some test.",
                    "label": 0
                },
                {
                    "sent": "So say sampling.",
                    "label": 0
                },
                {
                    "sent": "Let's say we sample our data 50 times in specific way, create models and so on and then we see that in the test set.",
                    "label": 0
                },
                {
                    "sent": "So on the independent test set our average is below 50% and on the training data is above 50.",
                    "label": 0
                },
                {
                    "sent": "So this sort of working definition capturing what we have seen.",
                    "label": 0
                },
                {
                    "sent": "With natural data previously, so that's our sort of working definition of anti learning, but in for remaining I will rather talk about that one because it's easier that will imply this one under some module.",
                    "label": 0
                },
                {
                    "sent": "Extra assumption on this part, right?",
                    "label": 0
                },
                {
                    "sent": "So that's in essence more interesting and that's why we're looking on that one because it's analytically more tractable.",
                    "label": 0
                },
                {
                    "sent": "Actually we can prove theorems about when things are until learning perfectly, we can't.",
                    "label": 0
                },
                {
                    "sent": "Do much about this bit yet, but maybe one day.",
                    "label": 0
                },
                {
                    "sent": "I think for me is very good moment to make a break.",
                    "label": 0
                },
                {
                    "sent": "OK, OK.",
                    "label": 0
                },
                {
                    "sent": "I see.",
                    "label": 0
                },
                {
                    "sent": "Yes, I forgot my charger.",
                    "label": 0
                },
                {
                    "sent": "You could you could I just?",
                    "label": 0
                },
                {
                    "sent": "Oh OK, that's doesn't have much OK.",
                    "label": 0
                },
                {
                    "sent": "So let's have this two minutes break and.",
                    "label": 0
                }
            ]
        }
    }
}