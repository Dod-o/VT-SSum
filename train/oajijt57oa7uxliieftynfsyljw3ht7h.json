{
    "id": "oajijt57oa7uxliieftynfsyljw3ht7h",
    "title": "Linear Complementarity for Regularized Policy Evaluation and Improvement",
    "info": {
        "author": [
            "Jeff Johns, Department of Computer Science, Duke University"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Linear Models",
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_johns_lcr/",
    "segmentation": [
        [
            "I'm going to give you the high level idea of what this talk is about."
        ],
        [
            "So I'm interested in solving multiple related L1 regularization problems.",
            "An my particular application is to solving Markov decision processes via policy iteration in an efficient manner.",
            "And in order to do that kind of the high level idea is that I'm going to formulate these L1 regularization problems as a linear complementarity problem and then use this nice warm start capability of LCP solvers.",
            "OK, so that's the high level idea."
        ],
        [
            "I think most people here who are familiar with the Lasso know that are familiar with this formulation of the L1 regularization problem, where you're minimizing a sum of two functions.",
            "L is the loss function and the second term is the penalty.",
            "The L1 penalty with the betas, the regularization parameter there an the specific problems that I'm going to be talking about are a little bit more general than that.",
            "It's this fixed point formulation and the thing to notice is that the coefficient W is appearing on both sides of this equation on the left hand side on the right hand side inside the loss function.",
            "OK, so the typical benefits that we'll get using L1 regularization is sparse.",
            "Solutions will be able to learn with a large number of irrelevant features, etc."
        ],
        [
            "OK, so there's a lot of different applications here, and as I mentioned, my particular application in bold and on the bottom right is to Markov decision processes and specifically the value function approximation problem."
        ],
        [
            "OK, so here's our typical policy iteration framework for MDP's.",
            "Well, the first phase will solve a value function approximation problem for a particular policy.",
            "And the second phase will include improving that policy.",
            "So what's the role of linear complementarity.",
            "First thing is that it's going to provide a new value function approximation method.",
            "And the second is that warm starts are going to help us speed up this process, so we'll be able to reuse information across these rounds of policy iteration."
        ],
        [
            "OK, so that's the high level picture and let me jump into the background."
        ],
        [
            "Kaya finite micro Markov decision process is a tuple here defined as, where S is a set of states as a set of actions.",
            "P is a transition function, an R is the immediate reward.",
            "And the goal is to learn a good policy pie, which is a mapping from states to actions.",
            "The value function associated with a particular policy is equal to the immediate reward plus gamma.",
            "Here is a discount factor between zero and one.",
            "Times the expected value of the next state as prime.",
            "OK, so I'll abbreviate that more succinctly as V = R Plus gamma PV or V&R.",
            "Here are vectors over the states.",
            "P is a matrix.",
            "And in fact, I'll take the right hand side of that equation, R Plus gamma, PV, and represent that as TAV.",
            "And that's the backup of the value function."
        ],
        [
            "OK, so for large problems we need to approximate the value function and use a linear approximation here where the features are represented as fee associated with the different States and the coefficients are W. So we're going to be setting the coefficients.",
            "Typically there's a lot of algorithms to set the coefficients, but they are usually done using the residual here, so I'll denote that is Delta.",
            "So that's the backup of our approximate function minus the value function.",
            "OK, so the thing to note is that if the value function, you can represent exactly, then the residual 0."
        ],
        [
            "OK, so the algorithm that I'll be building off of is LST.",
            "That's least squares temporal difference learning.",
            "And rather than give you the equation, I'll just give you the geometric.",
            "Notion here I'm not sure if you can see.",
            "The terms down below but.",
            "With this image, but let me I'll try to point to it.",
            "OK, so the blue plane here represents the space of value functions that we can represent with their features.",
            "The point there is a one point here on the left portion of this triangle that is a particular value function for some coefficient W. The point outside the space is the backup of that value function, and then we can project that point back down into the space spanned by the features.",
            "So.",
            "The vector here is the residual.",
            "The dotted line that you probably can't see there in the blue is the projection of the residual and the L STD algorithm works by minimizing this vector, so you end up getting a solution that goes where the residual ends up being orthogonal to the features.",
            "So that's how this."
        ],
        [
            "Works and this has been extended recently to include L1 regularization, so this is the L1 fixed point that I was mentioning at the beginning of the talk where the loss function includes these two coefficients UMW.",
            "And in particular, I'm showing here the the two norm.",
            "That is comparing the value function Phi U and the backup of the value function Phi W."
        ],
        [
            "OK, so there's no analytical solution to the problem.",
            "But you can state the conditions under which a set of coefficients solves this fixed point.",
            "So I'm not going to go through the exact equations here on the right, but this kind of parallels that the Lasso case.",
            "One thing I will point out is that defining a new set of variables here see.",
            "These are you can think of them as the correlation between the features and the residual.",
            "And depending on the value of the correlation, that dictates the sign of the coefficients."
        ],
        [
            "OK, so at last year's ICL conference call, turning presented the large TD algorithm that solves this problem.",
            "And as the name suggests, it's a homotopy method inspired by Lars.",
            "So it provides a set of solutions for all values of the regularization parameter.",
            "Turns out there's a couple of disadvantages to using this within policy iteration.",
            "The first is that every time you go around the policy iteration loop, each policy you're going to be learning a value function from scratch.",
            "So it's not reusing passing information.",
            "And the second point is that it's nice that entire path has been swept out, but in order to perform policy improvement, we need to pick one particular value of the regularization parameter.",
            "So a lot of that computation gets kind of washed away when we do that, so."
        ],
        [
            "The algorithms I'm going to present are going to address these two concerns.",
            "OK, so."
        ],
        [
            "Let's move to L CPS.",
            "LCPS are mathematical programs.",
            "So you can formulate a linear program as an LCP.",
            "An LCP can be formulated as a QP.",
            "The quadratic program, and there's an interesting relationship between LCP's and QPS.",
            "I'm not going to go into here, but there's a.",
            "There's a freely available book online on LCP.",
            "Is that details this, so that's a good thing to look at if you're interested.",
            "OK, so in LCP has two inputs of vector Q and a square matrix M. And the goal is to compute two vectors D An X, such that these three conditions hold.",
            "The first is that D equals MX plus Q, second is at D&X are non negative.",
            "And the third is the complementarity condition.",
            "That ensures that only one element of D, an X can be non zero at any point in time."
        ],
        [
            "OK, so we can take the L1TD problem and formulate it as an LCP.",
            "So recall I define this this variable C to be the correlation between the features and the residual.",
            "And we could expand that out in terms of the residual, but it gets a little messy.",
            "So what I'm going to do is.",
            "Write that as a vector B -- A matrix a times the coefficients W. And the interesting thing is that we can formulate an LCP using this matrix a vector B and the regularization parameter, and that solution to an LC to this LCP achieves the L1TD first order conditions.",
            "So I'm going to denote this algorithmically as the linear complementarity approach to solving this problem, so LCD."
        ],
        [
            "OK, there's some important points to make about LCP solvers that are similar in some regards to linear programming solvers, so one of the main algorithms is called the Lemke algorithm.",
            "It's a pivoting method you can think of as similar to the simplex method.",
            "It has a worst case exponential complexity.",
            "But you have to have pretty contrived problems in order to exhibit that behavior.",
            "The second point is really the one that I'm going to be exploiting.",
            "Is that some of the solvers can be initialized with a warm start, so warm start is a guess at which elements of the output are non 0.",
            "So in the Third Point, I want to make is just that whether a solution to an LCP exists, whether it's unique.",
            "An whether it's computable, we can guarantee the ability to compute it depend on various properties of the inputs.",
            "I'm not going to go through this, but."
        ],
        [
            "OK, so let me jump into the algorithms."
        ],
        [
            "OK, the first thing that we can do this use this LCD style approach and I apologize here about the color but.",
            "What will use that to solve the value function approximation problem and the one additional?",
            "Thing that I'm adding to this general policy iteration structure is this feedback loop here that involves the support.",
            "Of the coefficients from the previous problem.",
            "OK, so the.",
            "So what does that mean?",
            "What's that doing?",
            "You can imagine kind of going around this loop a couple times, comparing two policies and if.",
            "At the value functions associated with those policies reuse many of the same features.",
            "Then this idea of re of warm starting the LCP with the previous solution should lead to reduced computational workload."
        ],
        [
            "OK, so that's what we end up seeing here experimentally.",
            "Compared the LC TD algorithm and large TD in terms of the number of steps that each algorithm used.",
            "And So what?",
            "What is a step?",
            "So for the LCD problem?",
            "I used the pivoting method so a pivot was with step and for large TDI I counted a step as a solution to a system of linear equations.",
            "OK, and you can see for two of the three domains we saw improvements in terms of the average number of steps.",
            "On the order of 2 two to three X.",
            "So the plot that I'm showing on the right is for one of the domains, the X axis is the round of policy iteration and the Y axis is the number of steps.",
            "And you can see the blue curve.",
            "Here is the large DD one and the red one is the LC.",
            "The linear complementarity algorithm.",
            "And you can see a drop off.",
            "And that is when the policies are similar enough that many of the features are getting reused across the value functions that you learn."
        ],
        [
            "OK, next step.",
            "How do we select beta the regularization parameter?",
            "OK, so the simplest thing that we could do is just run M separate trials for M different values of beta will generate M different policies and we could test them.",
            "See which one works best.",
            "So that's pretty simple."
        ],
        [
            "It's the.",
            "We can do something a little more clever that we called the LC MPI algorithm.",
            "So I'll bring up the same picture from before, but imagine this time the regularization parameter is going to be decreasing here from beta one down to beta M, so from left to right, beta is decreasing.",
            "And what we're going to do is essentially jump from one loop to the next.",
            "Alright, so once we've solved for the policy for beta, one will use the coefficients and the policy to start the process for the next loop, an ideally as the regularization parameter gets smaller, more features get added to value functions that you're computing, so this should help.",
            "Kind of see the process and speed up.",
            "These loops"
        ],
        [
            "So it turns out that's what happens here, so the experiments that I'm showing at this point are using large median LCD with eleven different values of the regularization parameter versus running this else MPI algorithm once for those same 11 values.",
            "And you can see that we end up getting.",
            "I think anywhere from 3X to 10X decrease in the workload in comparison to large DD."
        ],
        [
            "OK, so just to wrap up.",
            "Showing that the L1 fixed point here is equivalent to a linear complementarity problem an demonstrated that this formulation is useful when solving multiple related problems, so we can exploit warm starts, and that's a useful thing to do for policy iteration.",
            "And the last point is just that this else MPI algorithm kind of naturally lends itself to selecting that regularization parameter."
        ],
        [
            "Thank you.",
            "So we've got time for a few questions.",
            "OK, yeah.",
            "OK, so while the speaker is taking some questions, I'd like to ask the poster presenters to come and get ready, and one of you hasn't signed yet.",
            "Sort of form is right up front OK. Alright, go ahead.",
            "I've actually 2 questions I'd like you to make some brief remarks on complexity of your process, specially considering they have a bunch of nasty loops and everything.",
            "And nested loops so.",
            "They're not a nested loops.",
            "It's kind of just a sequence of.",
            "OK, but what I meant is that.",
            "I have you want to have a large number of features and then you're a matrix is going to be fairly large as well, and then running this process once.",
            "It might be expensive and then if you want to run the one policy iteration might be even more expensive and then running the whole process for different bidders.",
            "So is this scalable?",
            "As you make the number of features large?",
            "Well, I think the that's one one of the potential benefits right of having sparse solutions is that if you do have a large number of features but only a handful of them are active, the amount of time it takes to find those is relatively small, provided there's sparse solutions.",
            "About space complexity?",
            "Excuse me about space complexity, the space complexity.",
            "We just stored those huge matrices somewhere, right?",
            "So the space complexity here was was number of samples by number of features.",
            "Any other questions?",
            "OK, let me then ask a follow up question, so I guess you compared the number of steps but you didn't mention how long those steps take an are they essentially the same amount of time across algorithms?",
            "Yeah, that's a good question.",
            "I should have said that, but.",
            "You can think of the pivot process as kind of equivalent to the.",
            "The step in large city of computing the solution to a linear system of equations so they are equivalent timelines OK and then roughly speaking.",
            "So how long did it take to do those experiments?",
            "Those experiments probably took.",
            "And you know, at most.",
            "Half an hour OK?",
            "They're relatively small problems, so you know larger problems.",
            "It's going to start to get a little bit bigger, OK?",
            "Fruit question groupal one, have you formulated it as well?",
            "I haven't tried that yet.",
            "I think that's an interesting question.",
            "Is kind of going to more structured problems.",
            "An I'm not sure of what is going to happen when you do that.",
            "If the LCP formulation is still going to hold that I have reason to believe that will be the case.",
            "I'm not sure if there's going to be benefits.",
            "You might need a more tailored LCP solver to take advantage of some of the situations there, but it's a good question.",
            "Alright, let's thank our speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to give you the high level idea of what this talk is about.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm interested in solving multiple related L1 regularization problems.",
                    "label": 1
                },
                {
                    "sent": "An my particular application is to solving Markov decision processes via policy iteration in an efficient manner.",
                    "label": 0
                },
                {
                    "sent": "And in order to do that kind of the high level idea is that I'm going to formulate these L1 regularization problems as a linear complementarity problem and then use this nice warm start capability of LCP solvers.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the high level idea.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think most people here who are familiar with the Lasso know that are familiar with this formulation of the L1 regularization problem, where you're minimizing a sum of two functions.",
                    "label": 0
                },
                {
                    "sent": "L is the loss function and the second term is the penalty.",
                    "label": 0
                },
                {
                    "sent": "The L1 penalty with the betas, the regularization parameter there an the specific problems that I'm going to be talking about are a little bit more general than that.",
                    "label": 0
                },
                {
                    "sent": "It's this fixed point formulation and the thing to notice is that the coefficient W is appearing on both sides of this equation on the left hand side on the right hand side inside the loss function.",
                    "label": 0
                },
                {
                    "sent": "OK, so the typical benefits that we'll get using L1 regularization is sparse.",
                    "label": 0
                },
                {
                    "sent": "Solutions will be able to learn with a large number of irrelevant features, etc.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so there's a lot of different applications here, and as I mentioned, my particular application in bold and on the bottom right is to Markov decision processes and specifically the value function approximation problem.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's our typical policy iteration framework for MDP's.",
                    "label": 1
                },
                {
                    "sent": "Well, the first phase will solve a value function approximation problem for a particular policy.",
                    "label": 0
                },
                {
                    "sent": "And the second phase will include improving that policy.",
                    "label": 0
                },
                {
                    "sent": "So what's the role of linear complementarity.",
                    "label": 1
                },
                {
                    "sent": "First thing is that it's going to provide a new value function approximation method.",
                    "label": 0
                },
                {
                    "sent": "And the second is that warm starts are going to help us speed up this process, so we'll be able to reuse information across these rounds of policy iteration.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's the high level picture and let me jump into the background.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kaya finite micro Markov decision process is a tuple here defined as, where S is a set of states as a set of actions.",
                    "label": 0
                },
                {
                    "sent": "P is a transition function, an R is the immediate reward.",
                    "label": 0
                },
                {
                    "sent": "And the goal is to learn a good policy pie, which is a mapping from states to actions.",
                    "label": 1
                },
                {
                    "sent": "The value function associated with a particular policy is equal to the immediate reward plus gamma.",
                    "label": 0
                },
                {
                    "sent": "Here is a discount factor between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Times the expected value of the next state as prime.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll abbreviate that more succinctly as V = R Plus gamma PV or V&R.",
                    "label": 0
                },
                {
                    "sent": "Here are vectors over the states.",
                    "label": 0
                },
                {
                    "sent": "P is a matrix.",
                    "label": 0
                },
                {
                    "sent": "And in fact, I'll take the right hand side of that equation, R Plus gamma, PV, and represent that as TAV.",
                    "label": 1
                },
                {
                    "sent": "And that's the backup of the value function.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so for large problems we need to approximate the value function and use a linear approximation here where the features are represented as fee associated with the different States and the coefficients are W. So we're going to be setting the coefficients.",
                    "label": 0
                },
                {
                    "sent": "Typically there's a lot of algorithms to set the coefficients, but they are usually done using the residual here, so I'll denote that is Delta.",
                    "label": 0
                },
                {
                    "sent": "So that's the backup of our approximate function minus the value function.",
                    "label": 0
                },
                {
                    "sent": "OK, so the thing to note is that if the value function, you can represent exactly, then the residual 0.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the algorithm that I'll be building off of is LST.",
                    "label": 0
                },
                {
                    "sent": "That's least squares temporal difference learning.",
                    "label": 0
                },
                {
                    "sent": "And rather than give you the equation, I'll just give you the geometric.",
                    "label": 0
                },
                {
                    "sent": "Notion here I'm not sure if you can see.",
                    "label": 0
                },
                {
                    "sent": "The terms down below but.",
                    "label": 0
                },
                {
                    "sent": "With this image, but let me I'll try to point to it.",
                    "label": 0
                },
                {
                    "sent": "OK, so the blue plane here represents the space of value functions that we can represent with their features.",
                    "label": 0
                },
                {
                    "sent": "The point there is a one point here on the left portion of this triangle that is a particular value function for some coefficient W. The point outside the space is the backup of that value function, and then we can project that point back down into the space spanned by the features.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The vector here is the residual.",
                    "label": 1
                },
                {
                    "sent": "The dotted line that you probably can't see there in the blue is the projection of the residual and the L STD algorithm works by minimizing this vector, so you end up getting a solution that goes where the residual ends up being orthogonal to the features.",
                    "label": 0
                },
                {
                    "sent": "So that's how this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Works and this has been extended recently to include L1 regularization, so this is the L1 fixed point that I was mentioning at the beginning of the talk where the loss function includes these two coefficients UMW.",
                    "label": 1
                },
                {
                    "sent": "And in particular, I'm showing here the the two norm.",
                    "label": 0
                },
                {
                    "sent": "That is comparing the value function Phi U and the backup of the value function Phi W.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there's no analytical solution to the problem.",
                    "label": 1
                },
                {
                    "sent": "But you can state the conditions under which a set of coefficients solves this fixed point.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to go through the exact equations here on the right, but this kind of parallels that the Lasso case.",
                    "label": 0
                },
                {
                    "sent": "One thing I will point out is that defining a new set of variables here see.",
                    "label": 0
                },
                {
                    "sent": "These are you can think of them as the correlation between the features and the residual.",
                    "label": 0
                },
                {
                    "sent": "And depending on the value of the correlation, that dictates the sign of the coefficients.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so at last year's ICL conference call, turning presented the large TD algorithm that solves this problem.",
                    "label": 0
                },
                {
                    "sent": "And as the name suggests, it's a homotopy method inspired by Lars.",
                    "label": 1
                },
                {
                    "sent": "So it provides a set of solutions for all values of the regularization parameter.",
                    "label": 1
                },
                {
                    "sent": "Turns out there's a couple of disadvantages to using this within policy iteration.",
                    "label": 0
                },
                {
                    "sent": "The first is that every time you go around the policy iteration loop, each policy you're going to be learning a value function from scratch.",
                    "label": 0
                },
                {
                    "sent": "So it's not reusing passing information.",
                    "label": 0
                },
                {
                    "sent": "And the second point is that it's nice that entire path has been swept out, but in order to perform policy improvement, we need to pick one particular value of the regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "So a lot of that computation gets kind of washed away when we do that, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The algorithms I'm going to present are going to address these two concerns.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's move to L CPS.",
                    "label": 0
                },
                {
                    "sent": "LCPS are mathematical programs.",
                    "label": 0
                },
                {
                    "sent": "So you can formulate a linear program as an LCP.",
                    "label": 0
                },
                {
                    "sent": "An LCP can be formulated as a QP.",
                    "label": 0
                },
                {
                    "sent": "The quadratic program, and there's an interesting relationship between LCP's and QPS.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into here, but there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a freely available book online on LCP.",
                    "label": 0
                },
                {
                    "sent": "Is that details this, so that's a good thing to look at if you're interested.",
                    "label": 0
                },
                {
                    "sent": "OK, so in LCP has two inputs of vector Q and a square matrix M. And the goal is to compute two vectors D An X, such that these three conditions hold.",
                    "label": 0
                },
                {
                    "sent": "The first is that D equals MX plus Q, second is at D&X are non negative.",
                    "label": 0
                },
                {
                    "sent": "And the third is the complementarity condition.",
                    "label": 0
                },
                {
                    "sent": "That ensures that only one element of D, an X can be non zero at any point in time.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we can take the L1TD problem and formulate it as an LCP.",
                    "label": 1
                },
                {
                    "sent": "So recall I define this this variable C to be the correlation between the features and the residual.",
                    "label": 0
                },
                {
                    "sent": "And we could expand that out in terms of the residual, but it gets a little messy.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is.",
                    "label": 0
                },
                {
                    "sent": "Write that as a vector B -- A matrix a times the coefficients W. And the interesting thing is that we can formulate an LCP using this matrix a vector B and the regularization parameter, and that solution to an LC to this LCP achieves the L1TD first order conditions.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to denote this algorithmically as the linear complementarity approach to solving this problem, so LCD.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, there's some important points to make about LCP solvers that are similar in some regards to linear programming solvers, so one of the main algorithms is called the Lemke algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's a pivoting method you can think of as similar to the simplex method.",
                    "label": 0
                },
                {
                    "sent": "It has a worst case exponential complexity.",
                    "label": 1
                },
                {
                    "sent": "But you have to have pretty contrived problems in order to exhibit that behavior.",
                    "label": 0
                },
                {
                    "sent": "The second point is really the one that I'm going to be exploiting.",
                    "label": 0
                },
                {
                    "sent": "Is that some of the solvers can be initialized with a warm start, so warm start is a guess at which elements of the output are non 0.",
                    "label": 1
                },
                {
                    "sent": "So in the Third Point, I want to make is just that whether a solution to an LCP exists, whether it's unique.",
                    "label": 0
                },
                {
                    "sent": "An whether it's computable, we can guarantee the ability to compute it depend on various properties of the inputs.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go through this, but.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me jump into the algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the first thing that we can do this use this LCD style approach and I apologize here about the color but.",
                    "label": 0
                },
                {
                    "sent": "What will use that to solve the value function approximation problem and the one additional?",
                    "label": 0
                },
                {
                    "sent": "Thing that I'm adding to this general policy iteration structure is this feedback loop here that involves the support.",
                    "label": 0
                },
                {
                    "sent": "Of the coefficients from the previous problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so the.",
                    "label": 0
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "What's that doing?",
                    "label": 0
                },
                {
                    "sent": "You can imagine kind of going around this loop a couple times, comparing two policies and if.",
                    "label": 0
                },
                {
                    "sent": "At the value functions associated with those policies reuse many of the same features.",
                    "label": 0
                },
                {
                    "sent": "Then this idea of re of warm starting the LCP with the previous solution should lead to reduced computational workload.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's what we end up seeing here experimentally.",
                    "label": 0
                },
                {
                    "sent": "Compared the LC TD algorithm and large TD in terms of the number of steps that each algorithm used.",
                    "label": 0
                },
                {
                    "sent": "And So what?",
                    "label": 0
                },
                {
                    "sent": "What is a step?",
                    "label": 0
                },
                {
                    "sent": "So for the LCD problem?",
                    "label": 0
                },
                {
                    "sent": "I used the pivoting method so a pivot was with step and for large TDI I counted a step as a solution to a system of linear equations.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can see for two of the three domains we saw improvements in terms of the average number of steps.",
                    "label": 0
                },
                {
                    "sent": "On the order of 2 two to three X.",
                    "label": 0
                },
                {
                    "sent": "So the plot that I'm showing on the right is for one of the domains, the X axis is the round of policy iteration and the Y axis is the number of steps.",
                    "label": 0
                },
                {
                    "sent": "And you can see the blue curve.",
                    "label": 0
                },
                {
                    "sent": "Here is the large DD one and the red one is the LC.",
                    "label": 0
                },
                {
                    "sent": "The linear complementarity algorithm.",
                    "label": 0
                },
                {
                    "sent": "And you can see a drop off.",
                    "label": 0
                },
                {
                    "sent": "And that is when the policies are similar enough that many of the features are getting reused across the value functions that you learn.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, next step.",
                    "label": 0
                },
                {
                    "sent": "How do we select beta the regularization parameter?",
                    "label": 0
                },
                {
                    "sent": "OK, so the simplest thing that we could do is just run M separate trials for M different values of beta will generate M different policies and we could test them.",
                    "label": 1
                },
                {
                    "sent": "See which one works best.",
                    "label": 0
                },
                {
                    "sent": "So that's pretty simple.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's the.",
                    "label": 0
                },
                {
                    "sent": "We can do something a little more clever that we called the LC MPI algorithm.",
                    "label": 0
                },
                {
                    "sent": "So I'll bring up the same picture from before, but imagine this time the regularization parameter is going to be decreasing here from beta one down to beta M, so from left to right, beta is decreasing.",
                    "label": 0
                },
                {
                    "sent": "And what we're going to do is essentially jump from one loop to the next.",
                    "label": 0
                },
                {
                    "sent": "Alright, so once we've solved for the policy for beta, one will use the coefficients and the policy to start the process for the next loop, an ideally as the regularization parameter gets smaller, more features get added to value functions that you're computing, so this should help.",
                    "label": 0
                },
                {
                    "sent": "Kind of see the process and speed up.",
                    "label": 0
                },
                {
                    "sent": "These loops",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it turns out that's what happens here, so the experiments that I'm showing at this point are using large median LCD with eleven different values of the regularization parameter versus running this else MPI algorithm once for those same 11 values.",
                    "label": 0
                },
                {
                    "sent": "And you can see that we end up getting.",
                    "label": 0
                },
                {
                    "sent": "I think anywhere from 3X to 10X decrease in the workload in comparison to large DD.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to wrap up.",
                    "label": 0
                },
                {
                    "sent": "Showing that the L1 fixed point here is equivalent to a linear complementarity problem an demonstrated that this formulation is useful when solving multiple related problems, so we can exploit warm starts, and that's a useful thing to do for policy iteration.",
                    "label": 1
                },
                {
                    "sent": "And the last point is just that this else MPI algorithm kind of naturally lends itself to selecting that regularization parameter.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So we've got time for a few questions.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so while the speaker is taking some questions, I'd like to ask the poster presenters to come and get ready, and one of you hasn't signed yet.",
                    "label": 0
                },
                {
                    "sent": "Sort of form is right up front OK. Alright, go ahead.",
                    "label": 0
                },
                {
                    "sent": "I've actually 2 questions I'd like you to make some brief remarks on complexity of your process, specially considering they have a bunch of nasty loops and everything.",
                    "label": 0
                },
                {
                    "sent": "And nested loops so.",
                    "label": 0
                },
                {
                    "sent": "They're not a nested loops.",
                    "label": 0
                },
                {
                    "sent": "It's kind of just a sequence of.",
                    "label": 0
                },
                {
                    "sent": "OK, but what I meant is that.",
                    "label": 0
                },
                {
                    "sent": "I have you want to have a large number of features and then you're a matrix is going to be fairly large as well, and then running this process once.",
                    "label": 0
                },
                {
                    "sent": "It might be expensive and then if you want to run the one policy iteration might be even more expensive and then running the whole process for different bidders.",
                    "label": 0
                },
                {
                    "sent": "So is this scalable?",
                    "label": 0
                },
                {
                    "sent": "As you make the number of features large?",
                    "label": 0
                },
                {
                    "sent": "Well, I think the that's one one of the potential benefits right of having sparse solutions is that if you do have a large number of features but only a handful of them are active, the amount of time it takes to find those is relatively small, provided there's sparse solutions.",
                    "label": 0
                },
                {
                    "sent": "About space complexity?",
                    "label": 0
                },
                {
                    "sent": "Excuse me about space complexity, the space complexity.",
                    "label": 0
                },
                {
                    "sent": "We just stored those huge matrices somewhere, right?",
                    "label": 0
                },
                {
                    "sent": "So the space complexity here was was number of samples by number of features.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, let me then ask a follow up question, so I guess you compared the number of steps but you didn't mention how long those steps take an are they essentially the same amount of time across algorithms?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good question.",
                    "label": 0
                },
                {
                    "sent": "I should have said that, but.",
                    "label": 0
                },
                {
                    "sent": "You can think of the pivot process as kind of equivalent to the.",
                    "label": 0
                },
                {
                    "sent": "The step in large city of computing the solution to a linear system of equations so they are equivalent timelines OK and then roughly speaking.",
                    "label": 0
                },
                {
                    "sent": "So how long did it take to do those experiments?",
                    "label": 0
                },
                {
                    "sent": "Those experiments probably took.",
                    "label": 0
                },
                {
                    "sent": "And you know, at most.",
                    "label": 0
                },
                {
                    "sent": "Half an hour OK?",
                    "label": 0
                },
                {
                    "sent": "They're relatively small problems, so you know larger problems.",
                    "label": 0
                },
                {
                    "sent": "It's going to start to get a little bit bigger, OK?",
                    "label": 0
                },
                {
                    "sent": "Fruit question groupal one, have you formulated it as well?",
                    "label": 0
                },
                {
                    "sent": "I haven't tried that yet.",
                    "label": 0
                },
                {
                    "sent": "I think that's an interesting question.",
                    "label": 0
                },
                {
                    "sent": "Is kind of going to more structured problems.",
                    "label": 0
                },
                {
                    "sent": "An I'm not sure of what is going to happen when you do that.",
                    "label": 0
                },
                {
                    "sent": "If the LCP formulation is still going to hold that I have reason to believe that will be the case.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if there's going to be benefits.",
                    "label": 0
                },
                {
                    "sent": "You might need a more tailored LCP solver to take advantage of some of the situations there, but it's a good question.",
                    "label": 0
                },
                {
                    "sent": "Alright, let's thank our speaker.",
                    "label": 0
                }
            ]
        }
    }
}