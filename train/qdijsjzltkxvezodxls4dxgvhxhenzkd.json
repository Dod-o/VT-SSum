{
    "id": "qdijsjzltkxvezodxls4dxgvhxhenzkd",
    "title": "Online Learning and Game Theory",
    "info": {
        "author": [
            "Adam Kalai, Toyota Technological Institute at Chicago"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "May 2005",
        "category": [
            "Top->Mathematics->Game Theory",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/mlss05us_kalai_olgt/",
    "segmentation": [
        [
            "Alright, thank you very much.",
            "Today I'm going to talk about online and learning in game theory.",
            "I think it will be a nice compliment to richs talk.",
            "It's mostly theoretical work, so I don't have any experiments.",
            "So how many of you are familiar with the weighted majority analysis?",
            "OK, I'm just kidding.",
            "I'm not going to have a bunch of equations for the last talk of the week, so I figured I'll try to do a theoretical talk, but I'm just going to try to use mostly pictures and I'll see how well it works.",
            "But hopefully you'll get the big picture in the main ideas and some of the theorems as well.",
            "So this is a talk about."
        ],
        [
            "Learning, I'm going to talk both about online learning and how it relates to game game playing repeated game playing.",
            "One of the questions you know.",
            "How do people learn?",
            "How do what's the right model of learning from a theoretical point of view?"
        ],
        [
            "So one model of learning would be OK, you just got a bunch of data, data set, training data drawn from distribution, and a goal is to learn the function which is predicting Y from X. OK, that's the batch model, right?",
            "You get a bunch of data.",
            "You have time to run your training algorithm.",
            "You get samples from this distribution.",
            "And your goal is to output a function that will predict well in future examples from the same distribution.",
            "That's the setting that I guess Rich was working in and another model that would be the online model, which I think you've heard about already, but I'll just describe it again where it's more like a repeated game model where.",
            "You got this child.",
            "He's sitting there.",
            "He gets an example.",
            "Yes, to decide what to do and makes a decision.",
            "Did he get it right?",
            "Did he get it wrong?",
            "He finds out the answer.",
            "So each time he observes an example, makes a prediction and finds out whether or not he was right.",
            "OK, and this is this is much more difficult setting here because we don't have a probability distribution.",
            "There's no guarantee that future examples are coming from the same distribution as previous examples, and this is the way many real world situations are.",
            "We don't have these kind of guarantees and what we're going to see today is that there's a lot of similarities between these two settings, and that lot of the things we can prove the same kind of rates happen in both the online and offline settings.",
            "OK, and I mentioned I had on the slider this a lot of this talk is not things that I did myself, it's just a summary.",
            "Some of it is what I've done myself.",
            "Joint work with strong kakade.",
            "OK, and feel free to ask questions.",
            "I want to slowly make sure everyone understands and I don't mind if I don't get through everything.",
            "OK, so the outline we're going to talk about online at versus batch."
        ],
        [
            "Inability of functions and we're going to explain why.",
            "Not surprisingly, the online setting is harder than the batch setting.",
            "I'll give an example of a simple case where you have just a small number of functions and you want to learn to do almost as well as the best function.",
            "And then I'll show why these two settings are roughly the same.",
            "I mean then the second part I'm going to talk about online learning and repeated games, and I think that Rapture.",
            "Did some of that.",
            "Is that right?",
            "I wasn't here for his lecture, but I'll maybe go over that again and then I'll talk for 0 sum games.",
            "He showed how you could use boosting to play games, is that right?",
            "No, maybe not OK, well then it will be new and also general.",
            "Some games where it's not that one player wins and the other player loses.",
            "It's just a general game and there's very nice algorithms for learning in those settings that converge to something called correlated equilibrium.",
            "OK, so in the online setting again we have, you know and."
        ],
        [
            "Example, we have this adversary, let's say, so we make it really hard and he picks some point and we have to predict its label either plus or minus.",
            "We make a prediction.",
            "Minus, we find out if minus OK we got it right.",
            "We make another prediction.",
            "We find out we predict minus it's plus.",
            "We got it wrong and so on.",
            "This repeats in an online fashion, OK?",
            "And in the end, how are we going to judge our performance while we look at our area?",
            "Will simply count the number of mistakes we've made?",
            "On our data OK. And that this error is between zero and one and obviously wanted to be low.",
            "In the batch setting, it's easier setting.",
            "We have this probability."
        ],
        [
            "Tribu shun over this probability distribution mu.",
            "OK, and we get examples drawn from you.",
            "OK. Alright, and now we our goal is learning algorithm to predict the rule that will be accurate on future date.",
            "Future data drawn from the same distribution.",
            "OK, so we have future examples.",
            "I wanted to make the cat walk across the screen but I don't know PowerPoint well enough to do that.",
            "If anyone here knows is an expert on PowerPoint.",
            "You can use your help.",
            "OK, so we want to do well on future examples drawn from the same distribution and of course."
        ],
        [
            "We could look at our error on the training data.",
            "Which I just right is the error of the function that we output.",
            "Here we up with this function F which is predicting, let's say in this example is just predicting.",
            "Positive on 1/2 space and negative on the other half space.",
            "We could look at the error on the training data right?",
            "But what we would really like to get a guarantee on is error on future data drawn from the same distribution, OK?",
            "That makes sense.",
            "Any questions?",
            "So what?"
        ],
        [
            "What we can say about these things?",
            "So we have this family of functions F and we say that an algorithm learns this function family functions F. So think of F is like these linear separators halfspaces, thresholds we say that algorithm learns F online if there's some constants.",
            "Well, let's forget the technicalities.",
            "Basically you're going to get a bunch of data you're predicting as you go along.",
            "You made a certain number of mistakes.",
            "Now let's look at how many mistakes you've made you've made.",
            "And after the fact, let's look back and see how well did the best.",
            "Linear separator do OK and this difference.",
            "The difference between those two errors is what we call your regret after the fact.",
            "How much better could you have done had you change your mind and you decided to go with one of the a different linear predictor and use the same linear predictor the whole time OK?",
            "And we'll say that an algorithm is a good online learning algorithm if for any sequence of data it gives you a guarantee that, let's say it's expected regret of the algorithm on that on that data decreases.",
            "Polynomially with the number of examples and so you have an examples you want your regret to be small for large datasets, and theoretically you would say it's small if it decreases, quoting some inverse polynomial rate.",
            "OK, and similarly we would define the regret for a batch setting.",
            "So in the batch setting you have an input and they're drawn from this distribution mu and you would say that you can learn this in the batch setting if you can always output for any distribution mu, you can always output an algorithm that's doing almost as well as the best.",
            "The best single algorithm.",
            "In hindsight, the algorithm that had the minimum error from you, OK. Any questions?",
            "Yep.",
            "Is there any information in the values of excise tax and not repeating?",
            "And.",
            "Coming coming closer, you can always do the wire which is.",
            "Write the question was is there any?",
            "Are we assuming anything about the relationship between the excise?",
            "The early exercise, later excise and wise?",
            "No assumption at all.",
            "So a good algorithm in the online setting has to do well for any sequence of examples.",
            "Does it care with the exerciser?",
            "Yeah, it does.",
            "Actually, it needs two.",
            "What kind of saving us a little bit is that there's a family of functions we're comparing ourselves to.",
            "We're not comparing ourselves to the best predictions in hindsight.",
            "If this family functions weren't restricted in some way, you just compare yourself to the right decision every time someone in hindsight could of course gotten 0 error.",
            "So we're comparing ourselves to the best of a certain class of functions, for example these.",
            "Halfspaces yeah.",
            "Listen, live.",
            "Hugh Grant to see as well.",
            "Yeah, what do I mean here?",
            "I mean when I write the error of an algorithm on the data, I mean the number between zero and one, which is the percentage of the time that the algorithm was run, right?",
            "Respect what probability distribution to you will take expected value in the online setting.",
            "Good good, good good good question.",
            "OK so in the online setting, why is there an expectation here we want this to hold for all data sequences.",
            "It turns out that if you want to have a good algorithm in the online setting, your algorithm itself is going to need to randomize.",
            "Your algorithm is going to need to flip coins in order in order to make decisions.",
            "So its predictions will be randomized predictions if you allow it to predict instead of plus and minus, but you allow it to predict probabilities like Rich was doing, then you wouldn't need this expectation.",
            "But if you insist that your algorithm predict plus and minus, then you need actually randomized algorithm and the expectation is chosen over the random choices of the algorithm.",
            "Make sense.",
            "Great.",
            "More questions.",
            "OK, so this is the general model and we'll actually see that the online setting, which seems to."
        ],
        [
            "Harder is harder in one sense.",
            "In another sense, it's not.",
            "So here's the first observation is that on?",
            "If you can learn something in the online model, you can learn it in the batch model.",
            "OK, So what I'm saying is if you give me an online learning algorithm that has a good guarantee on its regret, or it's expected regret, then I will define a batch learning algorithm which does almost as well, OK?",
            "And this is not surprising because in the batch setting there's one fixed distribution.",
            "So what is the algorithm going to be?",
            "Well, the naive algorithm would be just take the last, take the last function that was chosen by the online algorithm.",
            "So what do I mean by that?",
            "You can talk.",
            "You can talk about the function of the online algorithm.",
            "So the online algorithm you can think of it as a function.",
            "It takes input.",
            "What is the input to the online algorithm?",
            "It takes a sequence of labeled examples, X1Y one up until XI minus one Huaihai minus one, and then usually it takes.",
            "Zion has to predict Yi, but you could stick anything you want in here and ask the algorithm to make a prediction.",
            "So you can really think of the online algorithm is.",
            "As making as a sequence of functions, if you have N pieces of data and you might just choose the last function that was output by the online algorithm.",
            "OK, but that's not going to be good because we have no guarantee about the last function.",
            "What does it mean for an algorithm?",
            "Good online algorithms?",
            "It didn't make too many mistakes in the sequence going from one until N. OK, so at least theoretically, the way to get around this problem to show that online learning implies Bachelorette.",
            "Show that online learning is harder than batch learning is to say Oh well.",
            "Just pick a random one of these functions.",
            "So how will we predict what will we predict in the batch setting?",
            "What function will we choose on a new input?",
            "X will essentially just take a random subsequence.",
            "Of the examples between one and N. And will predict will stick in this new X.",
            "And predict the value that the online algorithm predicts with that random subsequence.",
            "And so that also in what we want to show is that that algorithm does well in the batch setting, OK?",
            "So the regret of.",
            "The regret of the online algorithm is just how well it's error rate minus the minimum error rate on the data and the regret of the online algorithm is it's error rate minus the error rate on minus the best error rate on the distribution mu.",
            "So the first point is that actually these two things are equal, so the."
        ],
        [
            "Error of the batch algorithm which we've constructed on the distribution mu is actually equal to the expected error of a drawn on the data.",
            "And why is that?",
            "Because we've taken our data in order to construct a batch learning algorithm, we've actually taken our data from the distribution mu, so we just have.",
            "An estimates of N samples from this distribution and each one is just measuring the error of one of the individual FIS.",
            "So these two terms are equal.",
            "And that's a little subtle point.",
            "And the second point, which is also subtle, is that over here the minimum this term is less than this term in the sense that.",
            "It's only harder online if we look back and see how well could we have done in the online sequence.",
            "What's the best halfspace or whatever?",
            "What's the best single function on the data sequence we had?",
            "That that that error rate is actually going to be less than the error rate of the best function in the distribution.",
            "OK. And what was the reason for that?",
            "Well, if you think about whatever the best function.",
            "On the on the distribution you have some distribution and the best function on that distribution you could have just used that function itself on your data.",
            "And it's error, and at least expected error would be the same would be equal here.",
            "Instead, we're not just taking the best function, we're not comparing ourselves after the fact to the best function for the distribution we're comparing ourselves with best function for the data, which can only be, which can only do better on the data then that other best function for the distribution.",
            "So the conclusion in the end is that in the batch setting, our regret is no more than the regret of the online algorithm.",
            "OK. And this is a theoretical result.",
            "I don't want to say that this is exactly how you would convert an online algorithm to a batch algorithm.",
            "Of course, you might just take the last last function output, but as enriches talk you solve that.",
            "Sometimes randomizing over different outputs of different predictors, different training sets actually helps you.",
            "So you might want to randomize over the last half is less 10%.",
            "You might actually do better, but this shows that the online setting, which is a much more difficult setting because you don't have this distribution assumption.",
            "Example, they're just coming arbitrarily, is not harder than the batch setting.",
            "OK, so that was the 1st first part.",
            "Are there any questions about that?"
        ],
        [
            "OK, so now I want to give some simple examples of finite setting.",
            "Let's say that we had, you know, just a small number of functions that we're comparing ourselves to OK?",
            "And that's even be in a very simple setting where there's some function we know is perfect.",
            "There's no mistakes, so there's some function F star, and it has 0 error on our data.",
            "Will just say that the nice world.",
            "There's some perfect linear."
        ],
        [
            "Pacifier.",
            "And let's say that the number of functions in our set is even finite.",
            "We have a small number of functions and it's F. OK. Then what is the natural algorithm to use here in the online setting?",
            "Well.",
            "The first time we have to make a prediction, what do we see?",
            "We see the oops.",
            "We don't see this sorry.",
            "We see the predictions of.",
            "See the example X one and we can therefore compute the predictions of the all the functions in our family is the most obvious thing to do would be just to take the majority of these things, which in this case is plus and then afterwards we find out the truth.",
            "And now we know it can't be F2.",
            "For example, we cross out all the inconsistent functions and we continue.",
            "The next example is.",
            "We can now look at the computer functions that are still alive.",
            "The ones we've passed out take the majority of their predictions.",
            "It's plus and then we find out the truth.",
            "Minus we can cross out now another function and so and this repeats.",
            "OK, are there any questions about this intuitive algorithm?",
            "So what can you say about this is very easy to analyze.",
            "This algorithm you say look every time, this algorithm majority algorithm makes a mistake.",
            "We know that we've crossed out half of the functions in our family.",
            "OK.",
            "Right?",
            "Because because we predicted according to majority, if we made a mistake, if the majority made a mistake, that means we're crossing out the majority of the functions there can't be too many left.",
            "So if we're crossing out the majority every time, then we can only make log base two of the number of functions before mistakes before we've crossed out everyone and we can't cross out everyone.",
            "We're assuming that there's someone who doesn't make any mistakes.",
            "So already you have this nice guarantee that the error of the.",
            "His majority function on the data is not more than only a log in the size of the data set divided by N. How many people here are familiar with VC dimension and things like that?",
            "Great, so some of these things might start looking familiar to you.",
            "Hopefully as we go along these bounds that you get here and the corresponding ones you've seen from VC dimension.",
            "So as we said, the online learning setting is at least as hard as the batch setting.",
            "So that means that in a batch setting we can also get a guarantee.",
            "Like this OK?",
            "So what do we do in the batch setting?",
            "So suppose there's some again in the batch setting.",
            "Let's do the same, the same warmup setting where there's some perfect function.",
            "If there's some perfect function.",
            "So we now get to a lot of data in advance.",
            "We get end piece of data.",
            "The nice thing would be to select a consistent function.",
            "OK. And here's an example where that won't work.",
            "OK, so in this example I want to show that basically what I want to show here is that in the batch setting you're not going to.",
            "I mean, we know you can do as well as you did in the online setting, but you're actually going to be much better.",
            "So in the batch setting, let's say you take some consistent function here.",
            "Now it might be that there was that one perfect function, and everyone else wasn't so great.",
            "Maybe everyone else had this error rate here, which is kind of what we're trying to.",
            "To guarantee so, if everyone else had that error rate, if you work it out.",
            "There may actually be one of those bad functions that didn't make any mistakes on the data set.",
            "In fact, what's the chance that someone who has an error rate?",
            "Sorry, this error should be."
        ],
        [
            "On the distribution mu, if it has error rate, G if it has this error rate on the distribution mu, the chance it didn't make any mistakes is exactly this thing here.",
            "OK, so.",
            "Let me just write down one inequality.",
            "I guess I have to write really big.",
            "OK, so one just this is something that will come up a couple of times.",
            "So if you take 1 -- X / N to the end, you get about each of the minus X.",
            "That's something you've probably seen from your basic calculus class, so.",
            "Is this term here the chance that a single function doesn't make a mistake is actually not so small?",
            "It's one over the number of functions E to the log F is 1 / F, So what we get is that actually it's quite likely that one of these bad functions will be there and will spoil us.",
            "Why?",
            "Because each one has like a 1 / F chance of spoiling us their F of them.",
            "So it's actually quite reasonable that one of them will fail us and will pick one of the bad functions instead of this great function and will get error about log F / N. And that's what we saw in the online setting, so we're starting already to get.",
            "We're starting already to get a sense that the online setting is a bit like the batch setting, and this is something that there's been a lot of research in the online setting.",
            "A lot of research in the batch setting, and there's been many times everyone in this area is kind of observing that they're very similar.",
            "But but today I'm going to show you a way in which they are actually the same.",
            "Um, OK so?"
        ],
        [
            "Let's now just go over what happens to that naive algorithm.",
            "The batch algorithm, which is very simple.",
            "You take an examples and now let's look at it in the noisy setting or not, the noise.",
            "But there's no perfect function.",
            "Why should we assume there's a perfect function?",
            "So we just choose the function that minimizes the error on the data.",
            "OK, I don't know.",
            "I don't think it's quite worth worth it for me to go through the analysis, but let me just tell you the punchline.",
            "The point is that the regret in expectation of this naive batch learning algorithm on the distribution meal is going to be something like 1 / sqrt N. And what's the intuition behind that?",
            "It's if you flip a coin end times you're going to expect to get about 10 / 2 heads.",
            "But you'll probably get like an over 2 plus some something which depends on the standard deviation, which is actually one over root N. You probably get like an over 2 plus or minus one over root N heads.",
            "So now we have this.",
            "We have a bunch of functions.",
            "We have F functions where measuring the error of each one on the data set.",
            "Each one's estimate of the errors is pretty good, but actually there will be some that are off some that are ranked too high and some that are ranked too low.",
            "And it turns out that when you have half of them, you can expect some of them to be off by as much as.",
            "This term.",
            "OK, so this is so.",
            "This is kind of what you expect in the batch setting when you just use this very simple algorithm and these things are similar to what you see in the analysis of VC dimension.",
            "Here's the analysis of why that's the case, but I think I'll skip.",
            "OK, so now let me show you an on line algorithm that's kind of nice.",
            "It's a very famous algorithm.",
            "I'm going to start with a slightly modified version that's simpler, so the weighted majority."
        ],
        [
            "I call it Williams ready prime.",
            "How is this algorithm work in the online setting?",
            "Again, we're not assuming now that the data is perfect.",
            "You just have this data in these functions and you're trying to do as well as the best function.",
            "So the algorithm just assigns a weight to each one of the functions for each function we start with its weight being one.",
            "And on each.",
            "We're going to just predict according to the majority of the functions.",
            "But now we're going to take a weighted majority, 'cause each function has a weight associated to it.",
            "And then we're going to update our weights.",
            "We're going to divide the weight of every function that errors by two, so we're going to have the weights of all the guys who got it wrong.",
            "If they get it wrong, cut their weight in half.",
            "Actually, at this point you might be wondering why don't we just take the function?",
            "That's done the best so far on the data.",
            "That's much more intuitive, right?",
            "If you're living in an online world, your baby, you get a bunch of examples when it just you know, if you're considering a bunch of candidate functions.",
            "When I take the function that did the best.",
            "At least theoretically, and in many cases, especially in game play, some game playing cases that's not going to work, so I'll give you an example.",
            "A real world example is kind of like when you're driving on the road, and you have, let's say, two lanes and you have to decide which Lane to go in.",
            "Don't you always feel like you turn into the Lane that stops so you're on the highway in one of the lanes is advancing the other one isn't, so imagine you're in a situation where you have two choices and you have to choose between them and you just take the one that did the best so far.",
            "So in that case.",
            "Let's say one of them does well and then the other one does well, and then the first one does well in the second.",
            "The alternate how well they're doing and you take the one that's done the best so far.",
            "Well, if you're just so unlucky that they're really alternating which one is doing better, you're always going to go to the one that's about to get stuck.",
            "You're going to go to the Lane that's not going to advance, just going to keep moving back and forth, and you'll never actually get make any progress.",
            "OK, and that's what happens right when you try this strategy of just just keep changing lanes to get into the better late than the same thing can happen in case of learning where you have a bunch of functions and you're trying to just naively work with the best one.",
            "And that's actually if you think about it for awhile, that's actually a special case of overfitting.",
            "With this algorithm is doing is, it's trying to trying to keep that intuition of going with the functions that have done better in the past, but it's not going to just go with the best function.",
            "It's going to kind of smoothly move towards the best function without putting all of its weight on the best function so far.",
            "And so, how do we analyze the number of errors of this simple algorithm?",
            "Well, every time this algorithm makes a mistake, every time this weighted majority prime algorithm makes a mistake.",
            "OK, so we had majority of the functions were wrong.",
            "The majority of our weight was wrong.",
            "More than half of our weight was wrong and it will be half will be cut in half so we will decrease our total weights by at least 25%.",
            "OK, so the final total weight at the end of the day is if we've made certain number of mistakes.",
            "Then our weights which started at F. If there were functions, went down by 3/4 each time, our weight will be no more than F * 3/4 to this number.",
            "It might be much less depending on how many people in the state made a mistake each time.",
            "On the other hand, on the positive side, our total weight can't be too small.",
            "In particular, our total weight is just the sum of the weights of each of the functions, and they started at one and they got cut in half every time they made a mistake.",
            "And we know there's one function or we hope there's one function that did well.",
            "At least we're comparing ourselves to the one function that made the fewest number of mistakes.",
            "So if there's one function up there that whatever it is that made the fewest number mistakes, its weight is at most this quantity two to the minus, the number of mistakes it's made.",
            "And when you take these two terms and put them together, so this is bigger than this.",
            "What you get out when you take logarithms, you just take the log.",
            "Here you have log of F. That comes down and you have lot of 3/4 times of mistakes.",
            "Would you get you can work it out yourself or you can see here is that the number of mistakes will make is not too much more than the number of mistakes of the best function.",
            "This is not a great guarantee.",
            "We're saying here, you're making at most twice the number of mistakes of the function.",
            "This is the best guy had an error rate of 10% will have an error rate of 24% which is not very satisfying, but it's still interesting and it's a nice simple algorithm.",
            "So what is the right algorithm?"
        ],
        [
            "Use here, what's a nice a better guarantee?",
            "You can modify this algorithm.",
            "It turns out that that that cutting their weights in half was a little bit too steep, and we should have just knocked their weights down by a little bit.",
            "So I think this is upside down.",
            "We should have knocked down their weights by 1 minus, and this should be the square root of sort of log F over some term there, which is less than one, and we want to decrease their weights by that term, and then you will get a guarantee that the expected.",
            "Performance, there's no you no longer have the factor of two.",
            "Now you have a factor of 1 here plus a second term.",
            "Our number of mistakes is no more than the mistakes of the best function plus a second term.",
            "And So what you get is that actually the regret which is that difference of how, well how well we did on our data compared to how well we could have done is this term here, which is going to zero quite quickly.",
            "And in fact, that's exactly the same term that we had for the case when when the data wasn't coming online, right?",
            "This is when we had.",
            "If I go back.",
            "So the that remember, this is what we got over here.",
            "So sorry in in this case, this is what we got over here.",
            "We have this same same term here up to a constant factor here.",
            "And this constant is not a constant in terms in front of our regret.",
            "It's not if the best made 10% were made making 20%.",
            "This is saying if the best made 10 + 10% we're making 10 plus epsilon or two epsilon mistakes.",
            "So really it seems like it's not clear that you really need to have this distribution assumption that you really need to assume your data is coming from a distribution.",
            "It might just be coming at you from some.",
            "Arbitrary order, arbitrary data, and still somehow you're able to do the same, and this is kind of very surprising thing that people in online algorithms and online learning have been noticing is that you get pretty much the same kind of guarantees when you're in the online setting in the batch setting.",
            "Are there any questions?",
            "OK, let me tell you some other cool things you can do with this weighted majority."
        ],
        [
            "Algorithm, so you might say, well, what if the world is changing overtime?",
            "I mean you're making this nice assumption that you're not assuming that the data is coming from some distribution.",
            "The data could be changing overtime, but still you're only comparing yourself to the best single function in hindsight.",
            "Isn't that a little unfair?",
            "Wouldn't you want to do as well as the best changing function?",
            "OK, so here you might compare yourself to the best change function.",
            "One nice thing you can do is you can compare it.",
            "You can take a window of a certain width.",
            "OK, so take a window W and let's say it has width.",
            "The length of W there the width of W. Then what you look at you can modify the algorithm so that you get a very similar guarantee, but you get this guarantee on every window on every window of with W. You're doing almost as well as the best function in that class did on that window.",
            "So now you're really adapting to changing things.",
            "Of course, if you make that window very small, if you make that window one, then you're not.",
            "You're just comparing yourself to the best function on every single example, and you can't get very good guarantees.",
            "Have any questions?",
            "Let me just explain a little more so when you compare yourself to the best.",
            "When you when you set W equal 1, you're comparing yourself to the guy who got everything right and I also do work on stock markets algorithm for Skype.",
            "People want to know why can't I compare myself to the best stock everyday?",
            "Why do I have to look in hindsight and compare myself to the best single stock or whatever?",
            "And the problem is I guess the problem can be best described by by call my uncle made to me.",
            "Last last year when he called me and said I lost $2,000,000 in the stock market and I said really what happened said well if I had invested my money here and then I'd move my money here and then I move my money here and so on.",
            "I would have made $2,000,000.",
            "Ann, if that's how you're going to measure your performance, you'll have a lot of regret so.",
            "So that's kind of why we measure our performance after the fact that compared to the best single function, and not compared to the best sort of changing function.",
            "Although you can get these nice changing function bounds.",
            "Other cool things you can do is going on here.",
            "Other cool things you can do."
        ],
        [
            "In this setting it's called the so-called Multi Armed Bandit problem, where there's a nice paper about this called how to gamble in a rigged casino OK, and this is a classic problem going back to statistics.",
            "From a long time ago, and the idea is multi armed bandit is oh sorry the yeah the multi arm bandit.",
            "You have several slot machines, each one is a one armed bandit and.",
            "If they decide which slot machines have pull.",
            "And each time you post some slot machine you get some money, but you don't know how much you would have gotten from all the other slot machines, so it's kind of one of these classic problems in exploration.",
            "Exploitation tradeoff.",
            "How do you decide which slot machine to pull so that you make as much money as possible and what they give their actually algorithms for deciding which slot machines to pull using randomness that will guarantee that they're making almost as much money as if they had just sat and played the best single slot machine in hindsight.",
            "Um, trying to take this back to the learning setting in the learning setting.",
            "We have these functions which we're comparing ourselves to.",
            "I want to point out that these functions can also be.",
            "They can either be, you know linear separators, but they could also be just some other weird class of functions that that also are learning algorithms themselves that look at a sequence of data.",
            "You can think of them, like experts.",
            "They look at a sequence of data and they make a prediction in the future.",
            "So these functions could be a decision tree learner and neural net, and in SVM.",
            "And you want to get the best performance the best of all three worlds you want to get an algorithm.",
            "That when you use it online will make a few mistakes is the best of the three.",
            "So in the in this setting in the multi armed bandit setting, though, it's even trickier.",
            "You only you have to.",
            "Essentially you have to commit to one algorithm before you even know what's going on, so you have to actually choose one of the functions before you see any of the predictions you choose.",
            "The function you run, your algorithm, whatever you find out its prediction, then you go with that.",
            "You have no choice, you go with that.",
            "It's not weighted majority notes the bandit algorithm you go with that prediction and find out I got it wrong.",
            "So maybe then you move to another function, you go with its prediction.",
            "You see that you had to go with this prediction sites you minus you see that you got it right?",
            "You say OK, I'm going to stay there.",
            "OK, and you keep going and so on.",
            "So.",
            "This repeats OK, so actually here the nice thing.",
            "The nice thing is that you don't have to run all of your algorithms each time, you only run each each time you only want run one of your predictors to see what to do.",
            "And still you gotta guarantee.",
            "Unfortunately the guarantee you got is now linear in the number of functions you have, but still you can actually do better than that in this very difficult setting.",
            "OK. Any questions yet?",
            "Do you have this Windows site was used equipment class of functions are comparing two or so in the previous slide.",
            "Again, we're comparing ourselves.",
            "Sorry this should be regret.",
            "I'm sorry it's a bunch of mistakes that my slides I had a talk with many more formulas and then I was kind of informed that there were too many formulas in the toxic.",
            "So I kind of last night I went through and tried to take out many formulas.",
            "Anyway, this would be the regret.",
            "Sorry the expected regret and that's compared to again compared to some class of functions F and here this capital F represents the size of the number of functions in that family.",
            "Karen yeah yeah, it's easier to compare yourself in a large window to the best function because you don't have to adapt as quickly.",
            "Now I'm comparing myself to have a large window that's 100 or 10,000.",
            "I'm comparing myself to the best algorithm in the last 10,000.",
            "Examples I only have to adapt myself sort of once every 10,000 steps to the best algorithm.",
            "To the best function, sorry in the family only if you shoot me.",
            "If the window is 1, then you're comparing yourself to the best function on that single example, there will always be some function that gets it right and you're comparing yourself to somebody who gets no mistakes and you're making a lot of mistakes.",
            "Anymore questions.",
            "So there's a lot of papers on different extensions of this weighted majority algorithm and related things.",
            "Very interesting, but I don't have time to talk about too much more of them.",
            "OK, so we talked about the only."
        ],
        [
            "Learnability, we've talked about the finite learning we're going to talk about, why it is that actually, even though these two settings seem very similar, they actually very different, and this is what's been keeping us from from relating the two settings as we wanted to.",
            "So I was actually talking with David McAllister for awhile about how can we relate these two settings?",
            "How can we show that they're really in some sense the same that you can do as well online as you can in the batch setting?",
            "And I'll show you you can't.",
            "And then I'll show you can't.",
            "So."
        ],
        [
            "So why aren't they the same?",
            "Let me give you a very simple example where it's easier to learn in the distribution setting, and this is going to highlight the intuition behind what's going on in this in these kind of problems.",
            "OK, so let's take a very simple example where you have the interval between zero and one, and your function is plus from some point on an minus from some point below some points positive above some point minus below some points or your.",
            "For example, trying to learn whether your device that you built will withstand a certain amount of heat and how much you know what temperature will it start breaking at an.",
            "You get examples at different temperatures and something like that, right?",
            "OK, so so it might be here, so here's a very difficult online setting where we don't have a distribution.",
            "Our adversary gives us the first example and it's .5.",
            "OK, what are we going to predict?",
            "We have no idea.",
            "We make some prediction and then we find out it's plus we whatever I mean, the adversary could have chosen that one at random 5050 and we can force us to make a mistake with probability half.",
            "If you just flip the coin, there's no way we could do better.",
            "Then the adversary OK Pick plus there.",
            "What's he going to pick next to hurt us?",
            "Does anyone know what he's going to pick next to try to?",
            "Make it difficult.",
            "What would you pick next?",
            "So remember that you know that the function is positive from some point on and negative below that point.",
            "Fear the adversary.",
            "What would you pick?",
            "Where do you say?",
            "On the here.",
            "OK, I think so.",
            "Maybe I explain it backwards, but you kind of think that I mean at least given this information, you might think that things on the right are positive because the function you know is positive from some.",
            "There's some point.",
            "Who knows where could be here, where it's positive and after that it's negative.",
            "So it's going to be, yeah.",
            "O OK, the adversary wants to make it hard for us.",
            "Most likely zero is negative.",
            "If zero is positive, then probably everything else is positive.",
            ".25 Point 25 good I think part of the problem is I'm not explaining it will be hopefully compared.",
            "I'm not explaining exactly what the function is.",
            "There's some point here and then to the right of that point the function is all positive to the left of the point.",
            "The function is all negative, so we might really think that the function is positive here.",
            "Let me OK, there could be noise, but let's forget about that for a second so.",
            "We might think that we have some guess about what the function is over here, but really over here we have absolutely no idea.",
            "We have no prior.",
            "There's no distribution.",
            "We can make a prior over to guests what the value of the function is.",
            "I have absolutely no idea what the function is here.",
            "And the adversary again, he could flip a coin and decide what to label that and we would just guess what I mean.",
            "Whatever we predict, whatever advanced algorithm we use if he flips a coin, there's nothing we can do.",
            "We have a chance, have to get it right.",
            "Then it's minus.",
            "So now where are we going to go?",
            ".75 OK, we could go to .75, but we might guess the .75 is plus.",
            "I'm just asking you to do complicated math.",
            "What is the average of?",
            "I don't know what did I put here.",
            "Oh, I didn't even write the number, OK?",
            "375 Yeah, yes, exactly binary search exactly.",
            "So the adversary is doing a binary search, but in a weird way.",
            "He's doing a random binary search.",
            "He's flipping coins to decide which way to go.",
            "To us, after the fact, it may seem like he this is all premeditated and he picked the threshold in advance.",
            "But he's really just doing a binary search here X3 positive.",
            "Now he's going to flip a coin X for whatever he'll decide at random negative.",
            "OK, he's going to go in between and so on.",
            "So at the end of the day, he's essentially given something which looks to us after the fact.",
            "Like it's completely consistent with.",
            "Data that's positive on one side and negative on the other side, and yet we have only 1/2 chance to get it right.",
            "So as you pointed out, the adversary is doing sort of a binary search.",
            "A random binary search, each label he's flipping a coin equally likely to be positive or negative.",
            "The end of the day he's predicted according to this function here, which is I wrote it as the sign of X minus some number C here, so it's plus if X is bigger than C and minus effects is less than C and.",
            "That's what it looks like to us.",
            "And actually, if you think about it for awhile, you can also think of him as he's essentially picking the bits of C1 by one each each time he tells us a label.",
            "We're finding out a bit of this.",
            "So what can we say about this?",
            "Well, any online algorithm can't get more than 1/2 on average.",
            "Of these, right?",
            "But when we look in hindsight, we're going to have a lot of regret, because in hindsight, they'll always be somebody who made no mistakes.",
            "OK.",
            "So when we look at this example, we say darn, you know this is like the simplest learning problem there is.",
            "This is the same as the the linear threshold functions.",
            "The halfspaces problem that we started with, but it's only in one dimension and even this we're not going to be able to learn in this online setting where we think of ourselves as having an adversary in the batch setting where there's a distribution over examples.",
            "We can learn this.",
            "How is that?",
            "Well, in the batch setting we get many examples and even if many of the examples are near to the threshold, that's OK.",
            "I mean, even if much of the distribution I should say we have a distribution you could sort of imagine drawing the density of the distribution even there's the threshold, even if much of the distribution is near the threshold, which might sound like a tired.",
            "That's OK, because in our training data will have many examples near the threshold as well.",
            "So in the distribution setting, this is actually easy to deal with.",
            "So there seems to be a major difference between the batch and the online setting.",
            "Another way for those of you who remember the VC dimension, the VC dimension of this class is very small.",
            "I think it's like one or something.",
            "So the fact that you can't learn it is kind of depressing in the online setting.",
            "OK, so here's Anita."
        ],
        [
            "Idea?",
            "That will hopefully solve the problem OK, and the idea is the following.",
            "Let's assume that what we're actually getting we actually get to see the examples in advance, so we get to see the X is in advance.",
            "We get to see all the points in advance, but not their labels.",
            "So the child kind of can see the problems that are coming up, but doesn't have any idea about their labels.",
            "And he gets one.",
            "This is called the transductive setting, I guess.",
            "Well, it's an online transductive setting so.",
            "He sees all these examples, but he has no idea what their labels are.",
            "And then he asked to predict the label for the first example, he makes a prediction and then he finds out the label to see whether it is correct or not.",
            "And now we have to make a prediction for the second example makes a prediction and so on.",
            "So what you'll see overtime is that.",
            "He will see that he'll be able to get to do well in this problem, and the reason is well, if the adversary was doing this binary search like before, he's got a pretty good idea of where things are heading and where that function, where, where to where.",
            "The threshold might be.",
            "OK.",
            "In fact, the adversary of course doesn't have to label things like this.",
            "You could do something completely different, can put arbitrary noise in there, which will make the problem much bigger, but still now that he sees the data that's coming up, there's really if you have any data points, there's really only end different places and different interesting functions you might think about that you have to compare yourself to.",
            "Even though there are infinitely many functions on the line, we've now reduced it to problem with only N functions.",
            "By considering this transductive setting.",
            "K. And this is this part is joint work with Sean Packeting.",
            "So again, the transductive setting in the big picture here you would have a bunch of points.",
            "And you have to make predictions one by one and you get the first point."
        ],
        [
            "In fact, they don't even know need to know the order of the points, but that's another issue that you're going to get, so you get the first point you predict, minus.",
            "Let's say it's minus, you get the second point you predict minus it, plus you find Mexican.",
            "So it's the same as the online setting, except that you see the points in advance.",
            "And again we're going to measure error based on the number of mistakes we've made.",
            "The percentage of mistakes we've made in hindsight compared to the percentage of mistakes we've made compared to the percentage of mistakes of the best expert.",
            "Equally of doing, assuming that you have online learning but that is getting the samples IID from there to distribution failure.",
            "Did you see these?",
            "Drawing the sample from an ID said instead of giving you the worst samples he can in every case.",
            "Can you view this as a situation where the adversary is drawing the samples from an IID set?",
            "I don't think so because the adversary can make these examples come to you in any order he feels like, and the examples don't have to be ID from some distribution there, just some sequence of problems that you're countering as you go through the world running your learning algorithm.",
            "You moron is worth points when I be.",
            "He's telling you it's true that he's telling you where the upcoming points are going to be, but that doesn't really tell you what their labels are going to be.",
            "It's not that I think it's it's somewhat different.",
            "What you're describing is an in between model would be a hybrid model that's easier, intuitively, easier than the full online model in this model also, but more difficult than the well.",
            "But it's different than the batch model where you have, like any samples at once at the beginning, describing maybe a model where.",
            "You have to make predictions online, but you're assuming they're coming from some distribution ID.",
            "That would be an inbetween model.",
            "Well, here's another series.",
            "I think it again, if you rephrase it.",
            "Tell me another story.",
            "Red yeah, I mean, if the adversary is not trying to screw us, we can only do better.",
            "If you play, if you're playing a game against a nasty opponent, you'll do who's trying to hurt you.",
            "You'll do worse.",
            "You can only do better if you're playing against a benevolent opponent.",
            "Yeah.",
            "Exactly right.",
            "Yeah, so in this case you're saying maybe he.",
            "Well, he won't be as nasty.",
            "Maybe maybe this once he has to reveal which points he's going to choose the adversary.",
            "Maybe he won't.",
            "Maybe he just will give you ID points.",
            "Maybe that's what he'll do.",
            "But conceptually, what this answers for me at least, is the question that I had before about what's keeping these two settings from being the same, because they seemed awfully similar.",
            "And yet, in the one setting we could learn very simple functions in the batch setting.",
            "We could learn these functions in online setting, we can't.",
            "So this helps.",
            "Understand that I think it's still a very difficult setting where you have some idea of what the data is that's coming up.",
            "But you don't know what the labels are.",
            "If you don't like the advert adverse aerial way of thinking about it.",
            "Just think about this practically as this explains why you might think that learning in an online setting where datas are coming for data is coming at you from some from no distribution.",
            "You can still do as well as if the data were drawn independently.",
            "From from the same distribution.",
            "OK. Any questions?",
            "OK.",
            "So what is the algorithm we're going to use this online, transductive setting?",
            "Well, we've seen are examples in advance, but labels are revealed on line.",
            "So basically we have our functions.",
            "There might even be infinitely many functions.",
            "OK, what are we going to do?",
            "OK, well.",
            "We're going to look at these functions each function."
        ],
        [
            "Make some predictions on the labels of these unknown things.",
            "We compute these predictions in advance.",
            "OK, so given these predictions, what we can do?",
            "What we will do is we will just find that there may be many of these functions even in functions.",
            "Many of them may be exactly the same.",
            "And we can now eliminate the ones that are exactly the same.",
            "That's something that we couldn't have done before in the weighted majority algorithm, we couldn't.",
            "We just had to get a guarantee that depend on the number of functions that we can reduce it to the number of different ways there are of labeling the sequence.",
            "So this might start to remind some of you of the VC dimension.",
            "So now we just can pick a small number of candidate functions for the number of different ways there are to label the sequence.",
            "In the worst case, there might be 2 to the N different ways to label the sequence of.",
            "An example is, but for many classes of functions it's going to be much less.",
            "OK, so we just take our functions.",
            "We do that ahead of time and then now we've reduced ourselves to a problem.",
            "If there were L different ways of labeling these examples, we now have effective problem size of L and now we can run the weighted majority on these just L functions.",
            "And if you remember the regret guarantee it was something like this term here square root log all over North.",
            "Oops.",
            "This should not be in the exponent should be down there.",
            "OK, so it's two times the regret we have is at most two times that term which will be small.",
            "OK. That's a full proof that if you have all different ways to label your data, you can get an online guarantee in this transductive setting of that much.",
            "If you believe the weighted majority algorithm's original analysis.",
            "And any questions?",
            "So one question is, how many different ways are there to label?",
            "Points who does anyone remember the that they remember the name of the theorem that tells you how many different ways there are to label a set of points?",
            "I think I heard it from someone."
        ],
        [
            "Yeah.",
            "Sing OK good, so I'll say it then you'll see so OK it's our showers lemma.",
            "So if you define the how do you define the VC dimension VC dimension?",
            "As you all know, but to remind you is this concept which clearly tells you roughly how complicated the class of functions are and also tells you exactly which functions are possibly learnable and which functions are not learnable.",
            "If it has infinite dimension, there's no way you can learn that class of functions.",
            "In the worst case.",
            "So how do you define the?",
            "VC dimension, well, you say that a set of points is shattered.",
            "That's a technical term.",
            "If if you can label the points in any different ways.",
            "If there is some function that labels this one negative, one positive, one negative for every combination.",
            "In this case of eight different labelings, there's some function that labels it.",
            "So essentially you'd have no idea what to label it.",
            "OK."
        ],
        [
            "In the VC dimension, is defined as the largest set that shattered by functions in this family.",
            "So if we're doing spaces and in two dimensions, the VC dimension here is 3, because these three points we could always find no matter how you label them, we can always find 1/2 space that labels to positive and one negative.",
            "And as you remember, the VC dimension is something that really captures the complexity of a family of functions.",
            "On the other hand, any four points you can't necessarily find, or in general you can't find a labeling that's consistent with this area halfspaces.",
            "That's consistent with every labeling, so the VC dimension is 3.",
            "OK. Now there's this very nice lemma that."
        ],
        [
            "Used in proving the VC dimension guarantees in the standard batch setting and it's called sours lemon.",
            "I'm just going to say them.",
            "I'm not going to prove it.",
            "What it says is that the number of ways to label a sequence of examples is.",
            "This is something that grows polynomially in the worst case polynomial in the VC dimension, so if the VC dimension is 3, like in the previous example, there's only going to be Ncube different ways to label the data.",
            "OK, and then we get the guarantee is going to be approximately here.",
            "Some should be to hear the regret will be the log of the size of the number of labelings over N an if N is the number of labelings is that term there it's going to be something like the VC dimension login overhead.",
            "So in that case it's three login over end, so the regret is going to zero very quickly for these classes of functions that have small VC dimension.",
            "OK. Any questions?",
            "OK, so.",
            "Just to you know, completely solidify the connection between these two.",
            "This is actually the same, really the same kind of guarantee you get in the batch setting, looks very."
        ],
        [
            "Familiar, but in fact you know that you know whatever we can do on line, we can do in the batch setting so.",
            "So you can translate these guarantees back to the batch setting.",
            "But we want to make sure we can't do better in the batch setting, and in general you can't.",
            "Do.",
            "You really can't do better than something like this in general.",
            "Let me just convince you roughly that you can't do better than that.",
            "So again, we have our distribution in the batch setting in the batch setting with this distribution.",
            "Now we have this set that has some VC dimension.",
            "Let's take this set of points that the set that shattered.",
            "And let's just make a distribution over these where we don't do well.",
            "So let's just if we have an examples will put weight 1 / N on each of these examples.",
            "And the rest of the weight on one of the other examples.",
            "I just want to convince you that you really can't learn this very well.",
            "Well, if each of these examples has weight 1 / N, then there's.",
            "This is the chance that will even see one at each of those examples in the set, so any example that his weight 1 / N and we're picking out examples.",
            "We have about 1/2 chance one every chance of seeing that example of not seeing that example in the training data.",
            "I'm therefore we're going to see about half these example, not even going to see the other half.",
            "And now this set of examples can be labeled either way.",
            "Any of them can be labeled in any way.",
            "There's not necessarily any assumption that the labeling of 1 relates to the labeling another, so we're not going to be able to learn these very well at all.",
            "I mean, in the in the batch setting there is in some sense an adversary is picking this distribution and he could pick this distribution to be who knows what over over the set of points.",
            "I want one point address your question before about the adversary.",
            "Is there an adversary in it?",
            "I do think that these concepts that come out of thinking about things theoretically like VC dimension so do correspond to natural things that you see in practice when you do design algorithms and they are helpful to understand even if the real world doesn't have these adversaries.",
            "OK, So what anyway what's the conclusion here is that we're not going to do better than you think.",
            "If here we had a set of size S which was the VC dimension and there's endpoints here, we're going to make we're going to have error.",
            "At least.",
            "Let's say this term here.",
            "Because about half of those we're not even going to see.",
            "We have no idea what their labels should be, so we'll get those wrong with probability half also.",
            "So what is the conclusion?",
            "Putting it all together in the online setting?",
            "So the online setting is very similar to the offline.",
            "The guarantees you get are always very comparable to the offline, but it's very annoying that that you can't just get."
        ],
        [
            "From the batch setting back to the online setting and the reason now, one reason you can understand is that, at least if you go to this trend, UC transductive setting that explains why you can do it in the transductive setting.",
            "In general you can't do it in the general online setting and in the transductive setting you get this very simple analysis saying that.",
            "You get this many mistakes, which is almost identical to the standard VC bound.",
            "For those of you who know the VC bounded generalization error.",
            "In the batch setting, we just also showed that you can't do much better.",
            "In fact, you can make this bound worsen.",
            "Look more like the above 1, but you certainly can't do better.",
            "Much better so if your class of VC.",
            "This says that if your class is finite visas mentioned, you're going to learn it.",
            "Your errors going to your regrets going to 0.",
            "This says in the batch setting if your class has finite, has infinite VC dimension, you're not going to do very well.",
            "So in general, basically whatever is learnable in those transductive online setting, and whatever is learnable in the back setting.",
            "Is exactly characterized by the finite VC dimension.",
            "OK. Any questions?",
            "So let me finish up with the online stuff, and then I'll move to the games."
        ],
        [
            "So the conclusions in the learnability setting are if you have a finite VC dimension that explains exactly what's batch and transductive learnable 11 interesting question is well, what's going on in the standard online setting?",
            "Is there some notion like VC dimension but different which explains exactly what's learnable and what's not?",
            "In the non transductive setting, I don't know the answer to that.",
            "That may be easy or hard.",
            "Another complaint you might have is that the algorithm I described was kind of inefficient.",
            "I mean the algorithm that works in the batch setting is also inefficient, but theoretical algorithm is just look at your data and find the best.",
            "Find the best function from your infinite class of functions that maybe may or may not be inefficient, but what we can say is if you can solve that problem efficiently in the batch setting, then you can design an algorithm that works in the online setting.",
            "The online setting.",
            "Remember the algorithm I described as go through your infinite class of functions.",
            "Look at all the labelings and then decrease that set and it's not very efficient, but the theorem that you can say so it can go from one to the other.",
            "If you can design an efficient algorithm for the batch setting then you can also define and design efficient setting for the online setting and I'll be happy to.",
            "Explain it to somebody if they have questions.",
            "Um?",
            "So for the next part I want to talk about online learning and repeat."
        ],
        [
            "Games do people want to break question?",
            "Can you put the same result?",
            "Using an active adaptive learning setting, very good question you giving day.",
            "So quickly, good question.",
            "So the question was what about active learning?",
            "So in active learning we have this data and we get to choose which point we want to live like the baby sitting there and he sees the data in front of me like I want to know the label of this one and he picks it out.",
            "So the active learning setting is easier.",
            "Because you can pick.",
            "So this is sort of saying like maybe it's not.",
            "Maybe active learning is not going to help you so much in that more than actually just getting the data.",
            "In a in a random order, in some sense I mean in terms of the number of mistakes you make in terms of the number of examples you have to label active learning much do might do much better.",
            "OK. Learning in online setting.",
            "I mean, we're not using active learning in the sense that we are not choosing which one to label.",
            "But I am defined I'm talking about an active learning model where online you would pick which one to label you would.",
            "You would make a prediction yourself.",
            "Then you would find out whether you're right or wrong and you would overtime count the number of mistakes.",
            "And that wouldn't be.",
            "The same example over and over.",
            "Let's say you have to pick each example and let's say the average Sherry tells you how many times you have to pick each example.",
            "OK question.",
            "So all of these results.",
            "Are for the noisy setting.",
            "I mean I described a couple of the algorithms in the non noisy setting and this example is misleading because it's a non noisy example.",
            "But all of these results are with respect to arbitrary data and you're just comparing yourself to the best function in that class of functions.",
            "So you're not comparing yourself to.",
            "The best you could have done the best arbitrary function 'cause there's always some really good arbitrary function, but we're comparing ourselves to the best of in a class of functions, and that's what we mean by saying that class of functions is learnable.",
            "But still, these results are all apply to the setting where there's arbitrary.",
            "We could be adversary could label the data however he wants, and we look just compared to the single one in hindsight.",
            "OK. More questions.",
            "So next I'm going to talk about the online where we online learning and repeated games.",
            "I just tell you this will be pretty light with much less technical details.",
            "I'll just tell you something.",
            "Some of you may know about what a correlated equilibrium is an algorithm for learning them.",
            "If not, you'll find out.",
            "There's some really neat neat results and what's going on there?",
            "Do people do people need a break before that?",
            "Do people want raise your hand if you want to break for a few minutes before the?",
            "OK yeah, those of you who want to break go ahead.",
            "OK, let's take a quick let's take a."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Today I'm going to talk about online and learning in game theory.",
                    "label": 1
                },
                {
                    "sent": "I think it will be a nice compliment to richs talk.",
                    "label": 0
                },
                {
                    "sent": "It's mostly theoretical work, so I don't have any experiments.",
                    "label": 0
                },
                {
                    "sent": "So how many of you are familiar with the weighted majority analysis?",
                    "label": 0
                },
                {
                    "sent": "OK, I'm just kidding.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to have a bunch of equations for the last talk of the week, so I figured I'll try to do a theoretical talk, but I'm just going to try to use mostly pictures and I'll see how well it works.",
                    "label": 0
                },
                {
                    "sent": "But hopefully you'll get the big picture in the main ideas and some of the theorems as well.",
                    "label": 0
                },
                {
                    "sent": "So this is a talk about.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning, I'm going to talk both about online learning and how it relates to game game playing repeated game playing.",
                    "label": 0
                },
                {
                    "sent": "One of the questions you know.",
                    "label": 0
                },
                {
                    "sent": "How do people learn?",
                    "label": 0
                },
                {
                    "sent": "How do what's the right model of learning from a theoretical point of view?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one model of learning would be OK, you just got a bunch of data, data set, training data drawn from distribution, and a goal is to learn the function which is predicting Y from X. OK, that's the batch model, right?",
                    "label": 0
                },
                {
                    "sent": "You get a bunch of data.",
                    "label": 0
                },
                {
                    "sent": "You have time to run your training algorithm.",
                    "label": 0
                },
                {
                    "sent": "You get samples from this distribution.",
                    "label": 0
                },
                {
                    "sent": "And your goal is to output a function that will predict well in future examples from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "That's the setting that I guess Rich was working in and another model that would be the online model, which I think you've heard about already, but I'll just describe it again where it's more like a repeated game model where.",
                    "label": 0
                },
                {
                    "sent": "You got this child.",
                    "label": 0
                },
                {
                    "sent": "He's sitting there.",
                    "label": 0
                },
                {
                    "sent": "He gets an example.",
                    "label": 0
                },
                {
                    "sent": "Yes, to decide what to do and makes a decision.",
                    "label": 0
                },
                {
                    "sent": "Did he get it right?",
                    "label": 0
                },
                {
                    "sent": "Did he get it wrong?",
                    "label": 0
                },
                {
                    "sent": "He finds out the answer.",
                    "label": 0
                },
                {
                    "sent": "So each time he observes an example, makes a prediction and finds out whether or not he was right.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is this is much more difficult setting here because we don't have a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "There's no guarantee that future examples are coming from the same distribution as previous examples, and this is the way many real world situations are.",
                    "label": 0
                },
                {
                    "sent": "We don't have these kind of guarantees and what we're going to see today is that there's a lot of similarities between these two settings, and that lot of the things we can prove the same kind of rates happen in both the online and offline settings.",
                    "label": 0
                },
                {
                    "sent": "OK, and I mentioned I had on the slider this a lot of this talk is not things that I did myself, it's just a summary.",
                    "label": 0
                },
                {
                    "sent": "Some of it is what I've done myself.",
                    "label": 0
                },
                {
                    "sent": "Joint work with strong kakade.",
                    "label": 0
                },
                {
                    "sent": "OK, and feel free to ask questions.",
                    "label": 0
                },
                {
                    "sent": "I want to slowly make sure everyone understands and I don't mind if I don't get through everything.",
                    "label": 0
                },
                {
                    "sent": "OK, so the outline we're going to talk about online at versus batch.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inability of functions and we're going to explain why.",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly, the online setting is harder than the batch setting.",
                    "label": 0
                },
                {
                    "sent": "I'll give an example of a simple case where you have just a small number of functions and you want to learn to do almost as well as the best function.",
                    "label": 0
                },
                {
                    "sent": "And then I'll show why these two settings are roughly the same.",
                    "label": 0
                },
                {
                    "sent": "I mean then the second part I'm going to talk about online learning and repeated games, and I think that Rapture.",
                    "label": 1
                },
                {
                    "sent": "Did some of that.",
                    "label": 0
                },
                {
                    "sent": "Is that right?",
                    "label": 0
                },
                {
                    "sent": "I wasn't here for his lecture, but I'll maybe go over that again and then I'll talk for 0 sum games.",
                    "label": 0
                },
                {
                    "sent": "He showed how you could use boosting to play games, is that right?",
                    "label": 0
                },
                {
                    "sent": "No, maybe not OK, well then it will be new and also general.",
                    "label": 0
                },
                {
                    "sent": "Some games where it's not that one player wins and the other player loses.",
                    "label": 1
                },
                {
                    "sent": "It's just a general game and there's very nice algorithms for learning in those settings that converge to something called correlated equilibrium.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the online setting again we have, you know and.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example, we have this adversary, let's say, so we make it really hard and he picks some point and we have to predict its label either plus or minus.",
                    "label": 0
                },
                {
                    "sent": "We make a prediction.",
                    "label": 0
                },
                {
                    "sent": "Minus, we find out if minus OK we got it right.",
                    "label": 0
                },
                {
                    "sent": "We make another prediction.",
                    "label": 0
                },
                {
                    "sent": "We find out we predict minus it's plus.",
                    "label": 0
                },
                {
                    "sent": "We got it wrong and so on.",
                    "label": 0
                },
                {
                    "sent": "This repeats in an online fashion, OK?",
                    "label": 0
                },
                {
                    "sent": "And in the end, how are we going to judge our performance while we look at our area?",
                    "label": 0
                },
                {
                    "sent": "Will simply count the number of mistakes we've made?",
                    "label": 0
                },
                {
                    "sent": "On our data OK. And that this error is between zero and one and obviously wanted to be low.",
                    "label": 0
                },
                {
                    "sent": "In the batch setting, it's easier setting.",
                    "label": 0
                },
                {
                    "sent": "We have this probability.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tribu shun over this probability distribution mu.",
                    "label": 0
                },
                {
                    "sent": "OK, and we get examples drawn from you.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright, and now we our goal is learning algorithm to predict the rule that will be accurate on future date.",
                    "label": 0
                },
                {
                    "sent": "Future data drawn from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have future examples.",
                    "label": 0
                },
                {
                    "sent": "I wanted to make the cat walk across the screen but I don't know PowerPoint well enough to do that.",
                    "label": 0
                },
                {
                    "sent": "If anyone here knows is an expert on PowerPoint.",
                    "label": 0
                },
                {
                    "sent": "You can use your help.",
                    "label": 0
                },
                {
                    "sent": "OK, so we want to do well on future examples drawn from the same distribution and of course.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We could look at our error on the training data.",
                    "label": 0
                },
                {
                    "sent": "Which I just right is the error of the function that we output.",
                    "label": 0
                },
                {
                    "sent": "Here we up with this function F which is predicting, let's say in this example is just predicting.",
                    "label": 0
                },
                {
                    "sent": "Positive on 1/2 space and negative on the other half space.",
                    "label": 0
                },
                {
                    "sent": "We could look at the error on the training data right?",
                    "label": 0
                },
                {
                    "sent": "But what we would really like to get a guarantee on is error on future data drawn from the same distribution, OK?",
                    "label": 0
                },
                {
                    "sent": "That makes sense.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we can say about these things?",
                    "label": 0
                },
                {
                    "sent": "So we have this family of functions F and we say that an algorithm learns this function family functions F. So think of F is like these linear separators halfspaces, thresholds we say that algorithm learns F online if there's some constants.",
                    "label": 1
                },
                {
                    "sent": "Well, let's forget the technicalities.",
                    "label": 0
                },
                {
                    "sent": "Basically you're going to get a bunch of data you're predicting as you go along.",
                    "label": 0
                },
                {
                    "sent": "You made a certain number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at how many mistakes you've made you've made.",
                    "label": 0
                },
                {
                    "sent": "And after the fact, let's look back and see how well did the best.",
                    "label": 0
                },
                {
                    "sent": "Linear separator do OK and this difference.",
                    "label": 0
                },
                {
                    "sent": "The difference between those two errors is what we call your regret after the fact.",
                    "label": 0
                },
                {
                    "sent": "How much better could you have done had you change your mind and you decided to go with one of the a different linear predictor and use the same linear predictor the whole time OK?",
                    "label": 0
                },
                {
                    "sent": "And we'll say that an algorithm is a good online learning algorithm if for any sequence of data it gives you a guarantee that, let's say it's expected regret of the algorithm on that on that data decreases.",
                    "label": 0
                },
                {
                    "sent": "Polynomially with the number of examples and so you have an examples you want your regret to be small for large datasets, and theoretically you would say it's small if it decreases, quoting some inverse polynomial rate.",
                    "label": 0
                },
                {
                    "sent": "OK, and similarly we would define the regret for a batch setting.",
                    "label": 0
                },
                {
                    "sent": "So in the batch setting you have an input and they're drawn from this distribution mu and you would say that you can learn this in the batch setting if you can always output for any distribution mu, you can always output an algorithm that's doing almost as well as the best.",
                    "label": 0
                },
                {
                    "sent": "The best single algorithm.",
                    "label": 0
                },
                {
                    "sent": "In hindsight, the algorithm that had the minimum error from you, OK. Any questions?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Is there any information in the values of excise tax and not repeating?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Coming coming closer, you can always do the wire which is.",
                    "label": 0
                },
                {
                    "sent": "Write the question was is there any?",
                    "label": 0
                },
                {
                    "sent": "Are we assuming anything about the relationship between the excise?",
                    "label": 0
                },
                {
                    "sent": "The early exercise, later excise and wise?",
                    "label": 0
                },
                {
                    "sent": "No assumption at all.",
                    "label": 0
                },
                {
                    "sent": "So a good algorithm in the online setting has to do well for any sequence of examples.",
                    "label": 0
                },
                {
                    "sent": "Does it care with the exerciser?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it does.",
                    "label": 0
                },
                {
                    "sent": "Actually, it needs two.",
                    "label": 0
                },
                {
                    "sent": "What kind of saving us a little bit is that there's a family of functions we're comparing ourselves to.",
                    "label": 0
                },
                {
                    "sent": "We're not comparing ourselves to the best predictions in hindsight.",
                    "label": 0
                },
                {
                    "sent": "If this family functions weren't restricted in some way, you just compare yourself to the right decision every time someone in hindsight could of course gotten 0 error.",
                    "label": 0
                },
                {
                    "sent": "So we're comparing ourselves to the best of a certain class of functions, for example these.",
                    "label": 0
                },
                {
                    "sent": "Halfspaces yeah.",
                    "label": 0
                },
                {
                    "sent": "Listen, live.",
                    "label": 0
                },
                {
                    "sent": "Hugh Grant to see as well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, what do I mean here?",
                    "label": 0
                },
                {
                    "sent": "I mean when I write the error of an algorithm on the data, I mean the number between zero and one, which is the percentage of the time that the algorithm was run, right?",
                    "label": 0
                },
                {
                    "sent": "Respect what probability distribution to you will take expected value in the online setting.",
                    "label": 0
                },
                {
                    "sent": "Good good, good good good question.",
                    "label": 0
                },
                {
                    "sent": "OK so in the online setting, why is there an expectation here we want this to hold for all data sequences.",
                    "label": 0
                },
                {
                    "sent": "It turns out that if you want to have a good algorithm in the online setting, your algorithm itself is going to need to randomize.",
                    "label": 0
                },
                {
                    "sent": "Your algorithm is going to need to flip coins in order in order to make decisions.",
                    "label": 0
                },
                {
                    "sent": "So its predictions will be randomized predictions if you allow it to predict instead of plus and minus, but you allow it to predict probabilities like Rich was doing, then you wouldn't need this expectation.",
                    "label": 0
                },
                {
                    "sent": "But if you insist that your algorithm predict plus and minus, then you need actually randomized algorithm and the expectation is chosen over the random choices of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Make sense.",
                    "label": 0
                },
                {
                    "sent": "Great.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the general model and we'll actually see that the online setting, which seems to.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Harder is harder in one sense.",
                    "label": 0
                },
                {
                    "sent": "In another sense, it's not.",
                    "label": 0
                },
                {
                    "sent": "So here's the first observation is that on?",
                    "label": 0
                },
                {
                    "sent": "If you can learn something in the online model, you can learn it in the batch model.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm saying is if you give me an online learning algorithm that has a good guarantee on its regret, or it's expected regret, then I will define a batch learning algorithm which does almost as well, OK?",
                    "label": 1
                },
                {
                    "sent": "And this is not surprising because in the batch setting there's one fixed distribution.",
                    "label": 0
                },
                {
                    "sent": "So what is the algorithm going to be?",
                    "label": 0
                },
                {
                    "sent": "Well, the naive algorithm would be just take the last, take the last function that was chosen by the online algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "You can talk.",
                    "label": 0
                },
                {
                    "sent": "You can talk about the function of the online algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the online algorithm you can think of it as a function.",
                    "label": 0
                },
                {
                    "sent": "It takes input.",
                    "label": 0
                },
                {
                    "sent": "What is the input to the online algorithm?",
                    "label": 0
                },
                {
                    "sent": "It takes a sequence of labeled examples, X1Y one up until XI minus one Huaihai minus one, and then usually it takes.",
                    "label": 0
                },
                {
                    "sent": "Zion has to predict Yi, but you could stick anything you want in here and ask the algorithm to make a prediction.",
                    "label": 0
                },
                {
                    "sent": "So you can really think of the online algorithm is.",
                    "label": 0
                },
                {
                    "sent": "As making as a sequence of functions, if you have N pieces of data and you might just choose the last function that was output by the online algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, but that's not going to be good because we have no guarantee about the last function.",
                    "label": 0
                },
                {
                    "sent": "What does it mean for an algorithm?",
                    "label": 0
                },
                {
                    "sent": "Good online algorithms?",
                    "label": 0
                },
                {
                    "sent": "It didn't make too many mistakes in the sequence going from one until N. OK, so at least theoretically, the way to get around this problem to show that online learning implies Bachelorette.",
                    "label": 0
                },
                {
                    "sent": "Show that online learning is harder than batch learning is to say Oh well.",
                    "label": 0
                },
                {
                    "sent": "Just pick a random one of these functions.",
                    "label": 0
                },
                {
                    "sent": "So how will we predict what will we predict in the batch setting?",
                    "label": 0
                },
                {
                    "sent": "What function will we choose on a new input?",
                    "label": 0
                },
                {
                    "sent": "X will essentially just take a random subsequence.",
                    "label": 0
                },
                {
                    "sent": "Of the examples between one and N. And will predict will stick in this new X.",
                    "label": 0
                },
                {
                    "sent": "And predict the value that the online algorithm predicts with that random subsequence.",
                    "label": 0
                },
                {
                    "sent": "And so that also in what we want to show is that that algorithm does well in the batch setting, OK?",
                    "label": 0
                },
                {
                    "sent": "So the regret of.",
                    "label": 0
                },
                {
                    "sent": "The regret of the online algorithm is just how well it's error rate minus the minimum error rate on the data and the regret of the online algorithm is it's error rate minus the error rate on minus the best error rate on the distribution mu.",
                    "label": 0
                },
                {
                    "sent": "So the first point is that actually these two things are equal, so the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Error of the batch algorithm which we've constructed on the distribution mu is actually equal to the expected error of a drawn on the data.",
                    "label": 0
                },
                {
                    "sent": "And why is that?",
                    "label": 0
                },
                {
                    "sent": "Because we've taken our data in order to construct a batch learning algorithm, we've actually taken our data from the distribution mu, so we just have.",
                    "label": 1
                },
                {
                    "sent": "An estimates of N samples from this distribution and each one is just measuring the error of one of the individual FIS.",
                    "label": 0
                },
                {
                    "sent": "So these two terms are equal.",
                    "label": 0
                },
                {
                    "sent": "And that's a little subtle point.",
                    "label": 0
                },
                {
                    "sent": "And the second point, which is also subtle, is that over here the minimum this term is less than this term in the sense that.",
                    "label": 0
                },
                {
                    "sent": "It's only harder online if we look back and see how well could we have done in the online sequence.",
                    "label": 0
                },
                {
                    "sent": "What's the best halfspace or whatever?",
                    "label": 0
                },
                {
                    "sent": "What's the best single function on the data sequence we had?",
                    "label": 0
                },
                {
                    "sent": "That that that error rate is actually going to be less than the error rate of the best function in the distribution.",
                    "label": 0
                },
                {
                    "sent": "OK. And what was the reason for that?",
                    "label": 0
                },
                {
                    "sent": "Well, if you think about whatever the best function.",
                    "label": 0
                },
                {
                    "sent": "On the on the distribution you have some distribution and the best function on that distribution you could have just used that function itself on your data.",
                    "label": 0
                },
                {
                    "sent": "And it's error, and at least expected error would be the same would be equal here.",
                    "label": 0
                },
                {
                    "sent": "Instead, we're not just taking the best function, we're not comparing ourselves after the fact to the best function for the distribution we're comparing ourselves with best function for the data, which can only be, which can only do better on the data then that other best function for the distribution.",
                    "label": 0
                },
                {
                    "sent": "So the conclusion in the end is that in the batch setting, our regret is no more than the regret of the online algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK. And this is a theoretical result.",
                    "label": 0
                },
                {
                    "sent": "I don't want to say that this is exactly how you would convert an online algorithm to a batch algorithm.",
                    "label": 0
                },
                {
                    "sent": "Of course, you might just take the last last function output, but as enriches talk you solve that.",
                    "label": 0
                },
                {
                    "sent": "Sometimes randomizing over different outputs of different predictors, different training sets actually helps you.",
                    "label": 0
                },
                {
                    "sent": "So you might want to randomize over the last half is less 10%.",
                    "label": 0
                },
                {
                    "sent": "You might actually do better, but this shows that the online setting, which is a much more difficult setting because you don't have this distribution assumption.",
                    "label": 0
                },
                {
                    "sent": "Example, they're just coming arbitrarily, is not harder than the batch setting.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was the 1st first part.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions about that?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now I want to give some simple examples of finite setting.",
                    "label": 0
                },
                {
                    "sent": "Let's say that we had, you know, just a small number of functions that we're comparing ourselves to OK?",
                    "label": 0
                },
                {
                    "sent": "And that's even be in a very simple setting where there's some function we know is perfect.",
                    "label": 0
                },
                {
                    "sent": "There's no mistakes, so there's some function F star, and it has 0 error on our data.",
                    "label": 0
                },
                {
                    "sent": "Will just say that the nice world.",
                    "label": 0
                },
                {
                    "sent": "There's some perfect linear.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pacifier.",
                    "label": 0
                },
                {
                    "sent": "And let's say that the number of functions in our set is even finite.",
                    "label": 0
                },
                {
                    "sent": "We have a small number of functions and it's F. OK. Then what is the natural algorithm to use here in the online setting?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "The first time we have to make a prediction, what do we see?",
                    "label": 0
                },
                {
                    "sent": "We see the oops.",
                    "label": 0
                },
                {
                    "sent": "We don't see this sorry.",
                    "label": 0
                },
                {
                    "sent": "We see the predictions of.",
                    "label": 0
                },
                {
                    "sent": "See the example X one and we can therefore compute the predictions of the all the functions in our family is the most obvious thing to do would be just to take the majority of these things, which in this case is plus and then afterwards we find out the truth.",
                    "label": 0
                },
                {
                    "sent": "And now we know it can't be F2.",
                    "label": 0
                },
                {
                    "sent": "For example, we cross out all the inconsistent functions and we continue.",
                    "label": 0
                },
                {
                    "sent": "The next example is.",
                    "label": 0
                },
                {
                    "sent": "We can now look at the computer functions that are still alive.",
                    "label": 0
                },
                {
                    "sent": "The ones we've passed out take the majority of their predictions.",
                    "label": 0
                },
                {
                    "sent": "It's plus and then we find out the truth.",
                    "label": 0
                },
                {
                    "sent": "Minus we can cross out now another function and so and this repeats.",
                    "label": 0
                },
                {
                    "sent": "OK, are there any questions about this intuitive algorithm?",
                    "label": 0
                },
                {
                    "sent": "So what can you say about this is very easy to analyze.",
                    "label": 0
                },
                {
                    "sent": "This algorithm you say look every time, this algorithm majority algorithm makes a mistake.",
                    "label": 1
                },
                {
                    "sent": "We know that we've crossed out half of the functions in our family.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Because because we predicted according to majority, if we made a mistake, if the majority made a mistake, that means we're crossing out the majority of the functions there can't be too many left.",
                    "label": 1
                },
                {
                    "sent": "So if we're crossing out the majority every time, then we can only make log base two of the number of functions before mistakes before we've crossed out everyone and we can't cross out everyone.",
                    "label": 0
                },
                {
                    "sent": "We're assuming that there's someone who doesn't make any mistakes.",
                    "label": 0
                },
                {
                    "sent": "So already you have this nice guarantee that the error of the.",
                    "label": 0
                },
                {
                    "sent": "His majority function on the data is not more than only a log in the size of the data set divided by N. How many people here are familiar with VC dimension and things like that?",
                    "label": 0
                },
                {
                    "sent": "Great, so some of these things might start looking familiar to you.",
                    "label": 0
                },
                {
                    "sent": "Hopefully as we go along these bounds that you get here and the corresponding ones you've seen from VC dimension.",
                    "label": 0
                },
                {
                    "sent": "So as we said, the online learning setting is at least as hard as the batch setting.",
                    "label": 0
                },
                {
                    "sent": "So that means that in a batch setting we can also get a guarantee.",
                    "label": 0
                },
                {
                    "sent": "Like this OK?",
                    "label": 0
                },
                {
                    "sent": "So what do we do in the batch setting?",
                    "label": 0
                },
                {
                    "sent": "So suppose there's some again in the batch setting.",
                    "label": 0
                },
                {
                    "sent": "Let's do the same, the same warmup setting where there's some perfect function.",
                    "label": 1
                },
                {
                    "sent": "If there's some perfect function.",
                    "label": 0
                },
                {
                    "sent": "So we now get to a lot of data in advance.",
                    "label": 0
                },
                {
                    "sent": "We get end piece of data.",
                    "label": 0
                },
                {
                    "sent": "The nice thing would be to select a consistent function.",
                    "label": 0
                },
                {
                    "sent": "OK. And here's an example where that won't work.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this example I want to show that basically what I want to show here is that in the batch setting you're not going to.",
                    "label": 0
                },
                {
                    "sent": "I mean, we know you can do as well as you did in the online setting, but you're actually going to be much better.",
                    "label": 0
                },
                {
                    "sent": "So in the batch setting, let's say you take some consistent function here.",
                    "label": 0
                },
                {
                    "sent": "Now it might be that there was that one perfect function, and everyone else wasn't so great.",
                    "label": 0
                },
                {
                    "sent": "Maybe everyone else had this error rate here, which is kind of what we're trying to.",
                    "label": 0
                },
                {
                    "sent": "To guarantee so, if everyone else had that error rate, if you work it out.",
                    "label": 0
                },
                {
                    "sent": "There may actually be one of those bad functions that didn't make any mistakes on the data set.",
                    "label": 0
                },
                {
                    "sent": "In fact, what's the chance that someone who has an error rate?",
                    "label": 0
                },
                {
                    "sent": "Sorry, this error should be.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the distribution mu, if it has error rate, G if it has this error rate on the distribution mu, the chance it didn't make any mistakes is exactly this thing here.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Let me just write down one inequality.",
                    "label": 0
                },
                {
                    "sent": "I guess I have to write really big.",
                    "label": 0
                },
                {
                    "sent": "OK, so one just this is something that will come up a couple of times.",
                    "label": 0
                },
                {
                    "sent": "So if you take 1 -- X / N to the end, you get about each of the minus X.",
                    "label": 0
                },
                {
                    "sent": "That's something you've probably seen from your basic calculus class, so.",
                    "label": 0
                },
                {
                    "sent": "Is this term here the chance that a single function doesn't make a mistake is actually not so small?",
                    "label": 0
                },
                {
                    "sent": "It's one over the number of functions E to the log F is 1 / F, So what we get is that actually it's quite likely that one of these bad functions will be there and will spoil us.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because each one has like a 1 / F chance of spoiling us their F of them.",
                    "label": 0
                },
                {
                    "sent": "So it's actually quite reasonable that one of them will fail us and will pick one of the bad functions instead of this great function and will get error about log F / N. And that's what we saw in the online setting, so we're starting already to get.",
                    "label": 0
                },
                {
                    "sent": "We're starting already to get a sense that the online setting is a bit like the batch setting, and this is something that there's been a lot of research in the online setting.",
                    "label": 0
                },
                {
                    "sent": "A lot of research in the batch setting, and there's been many times everyone in this area is kind of observing that they're very similar.",
                    "label": 0
                },
                {
                    "sent": "But but today I'm going to show you a way in which they are actually the same.",
                    "label": 0
                },
                {
                    "sent": "Um, OK so?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's now just go over what happens to that naive algorithm.",
                    "label": 0
                },
                {
                    "sent": "The batch algorithm, which is very simple.",
                    "label": 1
                },
                {
                    "sent": "You take an examples and now let's look at it in the noisy setting or not, the noise.",
                    "label": 0
                },
                {
                    "sent": "But there's no perfect function.",
                    "label": 0
                },
                {
                    "sent": "Why should we assume there's a perfect function?",
                    "label": 1
                },
                {
                    "sent": "So we just choose the function that minimizes the error on the data.",
                    "label": 0
                },
                {
                    "sent": "OK, I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't think it's quite worth worth it for me to go through the analysis, but let me just tell you the punchline.",
                    "label": 0
                },
                {
                    "sent": "The point is that the regret in expectation of this naive batch learning algorithm on the distribution meal is going to be something like 1 / sqrt N. And what's the intuition behind that?",
                    "label": 1
                },
                {
                    "sent": "It's if you flip a coin end times you're going to expect to get about 10 / 2 heads.",
                    "label": 0
                },
                {
                    "sent": "But you'll probably get like an over 2 plus some something which depends on the standard deviation, which is actually one over root N. You probably get like an over 2 plus or minus one over root N heads.",
                    "label": 0
                },
                {
                    "sent": "So now we have this.",
                    "label": 0
                },
                {
                    "sent": "We have a bunch of functions.",
                    "label": 0
                },
                {
                    "sent": "We have F functions where measuring the error of each one on the data set.",
                    "label": 0
                },
                {
                    "sent": "Each one's estimate of the errors is pretty good, but actually there will be some that are off some that are ranked too high and some that are ranked too low.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that when you have half of them, you can expect some of them to be off by as much as.",
                    "label": 0
                },
                {
                    "sent": "This term.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is so.",
                    "label": 0
                },
                {
                    "sent": "This is kind of what you expect in the batch setting when you just use this very simple algorithm and these things are similar to what you see in the analysis of VC dimension.",
                    "label": 0
                },
                {
                    "sent": "Here's the analysis of why that's the case, but I think I'll skip.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let me show you an on line algorithm that's kind of nice.",
                    "label": 0
                },
                {
                    "sent": "It's a very famous algorithm.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start with a slightly modified version that's simpler, so the weighted majority.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I call it Williams ready prime.",
                    "label": 0
                },
                {
                    "sent": "How is this algorithm work in the online setting?",
                    "label": 0
                },
                {
                    "sent": "Again, we're not assuming now that the data is perfect.",
                    "label": 0
                },
                {
                    "sent": "You just have this data in these functions and you're trying to do as well as the best function.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm just assigns a weight to each one of the functions for each function we start with its weight being one.",
                    "label": 1
                },
                {
                    "sent": "And on each.",
                    "label": 0
                },
                {
                    "sent": "We're going to just predict according to the majority of the functions.",
                    "label": 1
                },
                {
                    "sent": "But now we're going to take a weighted majority, 'cause each function has a weight associated to it.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to update our weights.",
                    "label": 0
                },
                {
                    "sent": "We're going to divide the weight of every function that errors by two, so we're going to have the weights of all the guys who got it wrong.",
                    "label": 0
                },
                {
                    "sent": "If they get it wrong, cut their weight in half.",
                    "label": 0
                },
                {
                    "sent": "Actually, at this point you might be wondering why don't we just take the function?",
                    "label": 0
                },
                {
                    "sent": "That's done the best so far on the data.",
                    "label": 0
                },
                {
                    "sent": "That's much more intuitive, right?",
                    "label": 0
                },
                {
                    "sent": "If you're living in an online world, your baby, you get a bunch of examples when it just you know, if you're considering a bunch of candidate functions.",
                    "label": 0
                },
                {
                    "sent": "When I take the function that did the best.",
                    "label": 0
                },
                {
                    "sent": "At least theoretically, and in many cases, especially in game play, some game playing cases that's not going to work, so I'll give you an example.",
                    "label": 0
                },
                {
                    "sent": "A real world example is kind of like when you're driving on the road, and you have, let's say, two lanes and you have to decide which Lane to go in.",
                    "label": 0
                },
                {
                    "sent": "Don't you always feel like you turn into the Lane that stops so you're on the highway in one of the lanes is advancing the other one isn't, so imagine you're in a situation where you have two choices and you have to choose between them and you just take the one that did the best so far.",
                    "label": 0
                },
                {
                    "sent": "So in that case.",
                    "label": 0
                },
                {
                    "sent": "Let's say one of them does well and then the other one does well, and then the first one does well in the second.",
                    "label": 0
                },
                {
                    "sent": "The alternate how well they're doing and you take the one that's done the best so far.",
                    "label": 0
                },
                {
                    "sent": "Well, if you're just so unlucky that they're really alternating which one is doing better, you're always going to go to the one that's about to get stuck.",
                    "label": 0
                },
                {
                    "sent": "You're going to go to the Lane that's not going to advance, just going to keep moving back and forth, and you'll never actually get make any progress.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's what happens right when you try this strategy of just just keep changing lanes to get into the better late than the same thing can happen in case of learning where you have a bunch of functions and you're trying to just naively work with the best one.",
                    "label": 0
                },
                {
                    "sent": "And that's actually if you think about it for awhile, that's actually a special case of overfitting.",
                    "label": 0
                },
                {
                    "sent": "With this algorithm is doing is, it's trying to trying to keep that intuition of going with the functions that have done better in the past, but it's not going to just go with the best function.",
                    "label": 0
                },
                {
                    "sent": "It's going to kind of smoothly move towards the best function without putting all of its weight on the best function so far.",
                    "label": 0
                },
                {
                    "sent": "And so, how do we analyze the number of errors of this simple algorithm?",
                    "label": 0
                },
                {
                    "sent": "Well, every time this algorithm makes a mistake, every time this weighted majority prime algorithm makes a mistake.",
                    "label": 0
                },
                {
                    "sent": "OK, so we had majority of the functions were wrong.",
                    "label": 0
                },
                {
                    "sent": "The majority of our weight was wrong.",
                    "label": 0
                },
                {
                    "sent": "More than half of our weight was wrong and it will be half will be cut in half so we will decrease our total weights by at least 25%.",
                    "label": 0
                },
                {
                    "sent": "OK, so the final total weight at the end of the day is if we've made certain number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "Then our weights which started at F. If there were functions, went down by 3/4 each time, our weight will be no more than F * 3/4 to this number.",
                    "label": 0
                },
                {
                    "sent": "It might be much less depending on how many people in the state made a mistake each time.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, on the positive side, our total weight can't be too small.",
                    "label": 0
                },
                {
                    "sent": "In particular, our total weight is just the sum of the weights of each of the functions, and they started at one and they got cut in half every time they made a mistake.",
                    "label": 0
                },
                {
                    "sent": "And we know there's one function or we hope there's one function that did well.",
                    "label": 0
                },
                {
                    "sent": "At least we're comparing ourselves to the one function that made the fewest number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "So if there's one function up there that whatever it is that made the fewest number mistakes, its weight is at most this quantity two to the minus, the number of mistakes it's made.",
                    "label": 0
                },
                {
                    "sent": "And when you take these two terms and put them together, so this is bigger than this.",
                    "label": 0
                },
                {
                    "sent": "What you get out when you take logarithms, you just take the log.",
                    "label": 0
                },
                {
                    "sent": "Here you have log of F. That comes down and you have lot of 3/4 times of mistakes.",
                    "label": 0
                },
                {
                    "sent": "Would you get you can work it out yourself or you can see here is that the number of mistakes will make is not too much more than the number of mistakes of the best function.",
                    "label": 0
                },
                {
                    "sent": "This is not a great guarantee.",
                    "label": 0
                },
                {
                    "sent": "We're saying here, you're making at most twice the number of mistakes of the function.",
                    "label": 0
                },
                {
                    "sent": "This is the best guy had an error rate of 10% will have an error rate of 24% which is not very satisfying, but it's still interesting and it's a nice simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what is the right algorithm?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use here, what's a nice a better guarantee?",
                    "label": 0
                },
                {
                    "sent": "You can modify this algorithm.",
                    "label": 0
                },
                {
                    "sent": "It turns out that that that cutting their weights in half was a little bit too steep, and we should have just knocked their weights down by a little bit.",
                    "label": 0
                },
                {
                    "sent": "So I think this is upside down.",
                    "label": 0
                },
                {
                    "sent": "We should have knocked down their weights by 1 minus, and this should be the square root of sort of log F over some term there, which is less than one, and we want to decrease their weights by that term, and then you will get a guarantee that the expected.",
                    "label": 0
                },
                {
                    "sent": "Performance, there's no you no longer have the factor of two.",
                    "label": 0
                },
                {
                    "sent": "Now you have a factor of 1 here plus a second term.",
                    "label": 0
                },
                {
                    "sent": "Our number of mistakes is no more than the mistakes of the best function plus a second term.",
                    "label": 0
                },
                {
                    "sent": "And So what you get is that actually the regret which is that difference of how, well how well we did on our data compared to how well we could have done is this term here, which is going to zero quite quickly.",
                    "label": 0
                },
                {
                    "sent": "And in fact, that's exactly the same term that we had for the case when when the data wasn't coming online, right?",
                    "label": 0
                },
                {
                    "sent": "This is when we had.",
                    "label": 0
                },
                {
                    "sent": "If I go back.",
                    "label": 0
                },
                {
                    "sent": "So the that remember, this is what we got over here.",
                    "label": 0
                },
                {
                    "sent": "So sorry in in this case, this is what we got over here.",
                    "label": 0
                },
                {
                    "sent": "We have this same same term here up to a constant factor here.",
                    "label": 0
                },
                {
                    "sent": "And this constant is not a constant in terms in front of our regret.",
                    "label": 0
                },
                {
                    "sent": "It's not if the best made 10% were made making 20%.",
                    "label": 0
                },
                {
                    "sent": "This is saying if the best made 10 + 10% we're making 10 plus epsilon or two epsilon mistakes.",
                    "label": 0
                },
                {
                    "sent": "So really it seems like it's not clear that you really need to have this distribution assumption that you really need to assume your data is coming from a distribution.",
                    "label": 0
                },
                {
                    "sent": "It might just be coming at you from some.",
                    "label": 0
                },
                {
                    "sent": "Arbitrary order, arbitrary data, and still somehow you're able to do the same, and this is kind of very surprising thing that people in online algorithms and online learning have been noticing is that you get pretty much the same kind of guarantees when you're in the online setting in the batch setting.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions?",
                    "label": 0
                },
                {
                    "sent": "OK, let me tell you some other cool things you can do with this weighted majority.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm, so you might say, well, what if the world is changing overtime?",
                    "label": 0
                },
                {
                    "sent": "I mean you're making this nice assumption that you're not assuming that the data is coming from some distribution.",
                    "label": 0
                },
                {
                    "sent": "The data could be changing overtime, but still you're only comparing yourself to the best single function in hindsight.",
                    "label": 0
                },
                {
                    "sent": "Isn't that a little unfair?",
                    "label": 0
                },
                {
                    "sent": "Wouldn't you want to do as well as the best changing function?",
                    "label": 0
                },
                {
                    "sent": "OK, so here you might compare yourself to the best change function.",
                    "label": 0
                },
                {
                    "sent": "One nice thing you can do is you can compare it.",
                    "label": 0
                },
                {
                    "sent": "You can take a window of a certain width.",
                    "label": 0
                },
                {
                    "sent": "OK, so take a window W and let's say it has width.",
                    "label": 0
                },
                {
                    "sent": "The length of W there the width of W. Then what you look at you can modify the algorithm so that you get a very similar guarantee, but you get this guarantee on every window on every window of with W. You're doing almost as well as the best function in that class did on that window.",
                    "label": 0
                },
                {
                    "sent": "So now you're really adapting to changing things.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you make that window very small, if you make that window one, then you're not.",
                    "label": 0
                },
                {
                    "sent": "You're just comparing yourself to the best function on every single example, and you can't get very good guarantees.",
                    "label": 0
                },
                {
                    "sent": "Have any questions?",
                    "label": 0
                },
                {
                    "sent": "Let me just explain a little more so when you compare yourself to the best.",
                    "label": 0
                },
                {
                    "sent": "When you when you set W equal 1, you're comparing yourself to the guy who got everything right and I also do work on stock markets algorithm for Skype.",
                    "label": 0
                },
                {
                    "sent": "People want to know why can't I compare myself to the best stock everyday?",
                    "label": 0
                },
                {
                    "sent": "Why do I have to look in hindsight and compare myself to the best single stock or whatever?",
                    "label": 0
                },
                {
                    "sent": "And the problem is I guess the problem can be best described by by call my uncle made to me.",
                    "label": 0
                },
                {
                    "sent": "Last last year when he called me and said I lost $2,000,000 in the stock market and I said really what happened said well if I had invested my money here and then I'd move my money here and then I move my money here and so on.",
                    "label": 0
                },
                {
                    "sent": "I would have made $2,000,000.",
                    "label": 0
                },
                {
                    "sent": "Ann, if that's how you're going to measure your performance, you'll have a lot of regret so.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of why we measure our performance after the fact that compared to the best single function, and not compared to the best sort of changing function.",
                    "label": 0
                },
                {
                    "sent": "Although you can get these nice changing function bounds.",
                    "label": 0
                },
                {
                    "sent": "Other cool things you can do is going on here.",
                    "label": 0
                },
                {
                    "sent": "Other cool things you can do.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this setting it's called the so-called Multi Armed Bandit problem, where there's a nice paper about this called how to gamble in a rigged casino OK, and this is a classic problem going back to statistics.",
                    "label": 0
                },
                {
                    "sent": "From a long time ago, and the idea is multi armed bandit is oh sorry the yeah the multi arm bandit.",
                    "label": 0
                },
                {
                    "sent": "You have several slot machines, each one is a one armed bandit and.",
                    "label": 0
                },
                {
                    "sent": "If they decide which slot machines have pull.",
                    "label": 0
                },
                {
                    "sent": "And each time you post some slot machine you get some money, but you don't know how much you would have gotten from all the other slot machines, so it's kind of one of these classic problems in exploration.",
                    "label": 0
                },
                {
                    "sent": "Exploitation tradeoff.",
                    "label": 0
                },
                {
                    "sent": "How do you decide which slot machine to pull so that you make as much money as possible and what they give their actually algorithms for deciding which slot machines to pull using randomness that will guarantee that they're making almost as much money as if they had just sat and played the best single slot machine in hindsight.",
                    "label": 0
                },
                {
                    "sent": "Um, trying to take this back to the learning setting in the learning setting.",
                    "label": 0
                },
                {
                    "sent": "We have these functions which we're comparing ourselves to.",
                    "label": 0
                },
                {
                    "sent": "I want to point out that these functions can also be.",
                    "label": 0
                },
                {
                    "sent": "They can either be, you know linear separators, but they could also be just some other weird class of functions that that also are learning algorithms themselves that look at a sequence of data.",
                    "label": 0
                },
                {
                    "sent": "You can think of them, like experts.",
                    "label": 0
                },
                {
                    "sent": "They look at a sequence of data and they make a prediction in the future.",
                    "label": 0
                },
                {
                    "sent": "So these functions could be a decision tree learner and neural net, and in SVM.",
                    "label": 0
                },
                {
                    "sent": "And you want to get the best performance the best of all three worlds you want to get an algorithm.",
                    "label": 0
                },
                {
                    "sent": "That when you use it online will make a few mistakes is the best of the three.",
                    "label": 0
                },
                {
                    "sent": "So in the in this setting in the multi armed bandit setting, though, it's even trickier.",
                    "label": 0
                },
                {
                    "sent": "You only you have to.",
                    "label": 0
                },
                {
                    "sent": "Essentially you have to commit to one algorithm before you even know what's going on, so you have to actually choose one of the functions before you see any of the predictions you choose.",
                    "label": 0
                },
                {
                    "sent": "The function you run, your algorithm, whatever you find out its prediction, then you go with that.",
                    "label": 0
                },
                {
                    "sent": "You have no choice, you go with that.",
                    "label": 0
                },
                {
                    "sent": "It's not weighted majority notes the bandit algorithm you go with that prediction and find out I got it wrong.",
                    "label": 1
                },
                {
                    "sent": "So maybe then you move to another function, you go with its prediction.",
                    "label": 0
                },
                {
                    "sent": "You see that you had to go with this prediction sites you minus you see that you got it right?",
                    "label": 0
                },
                {
                    "sent": "You say OK, I'm going to stay there.",
                    "label": 0
                },
                {
                    "sent": "OK, and you keep going and so on.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This repeats OK, so actually here the nice thing.",
                    "label": 0
                },
                {
                    "sent": "The nice thing is that you don't have to run all of your algorithms each time, you only run each each time you only want run one of your predictors to see what to do.",
                    "label": 0
                },
                {
                    "sent": "And still you gotta guarantee.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately the guarantee you got is now linear in the number of functions you have, but still you can actually do better than that in this very difficult setting.",
                    "label": 0
                },
                {
                    "sent": "OK. Any questions yet?",
                    "label": 0
                },
                {
                    "sent": "Do you have this Windows site was used equipment class of functions are comparing two or so in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "Again, we're comparing ourselves.",
                    "label": 0
                },
                {
                    "sent": "Sorry this should be regret.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry it's a bunch of mistakes that my slides I had a talk with many more formulas and then I was kind of informed that there were too many formulas in the toxic.",
                    "label": 0
                },
                {
                    "sent": "So I kind of last night I went through and tried to take out many formulas.",
                    "label": 0
                },
                {
                    "sent": "Anyway, this would be the regret.",
                    "label": 0
                },
                {
                    "sent": "Sorry the expected regret and that's compared to again compared to some class of functions F and here this capital F represents the size of the number of functions in that family.",
                    "label": 0
                },
                {
                    "sent": "Karen yeah yeah, it's easier to compare yourself in a large window to the best function because you don't have to adapt as quickly.",
                    "label": 0
                },
                {
                    "sent": "Now I'm comparing myself to have a large window that's 100 or 10,000.",
                    "label": 0
                },
                {
                    "sent": "I'm comparing myself to the best algorithm in the last 10,000.",
                    "label": 0
                },
                {
                    "sent": "Examples I only have to adapt myself sort of once every 10,000 steps to the best algorithm.",
                    "label": 1
                },
                {
                    "sent": "To the best function, sorry in the family only if you shoot me.",
                    "label": 0
                },
                {
                    "sent": "If the window is 1, then you're comparing yourself to the best function on that single example, there will always be some function that gets it right and you're comparing yourself to somebody who gets no mistakes and you're making a lot of mistakes.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of papers on different extensions of this weighted majority algorithm and related things.",
                    "label": 0
                },
                {
                    "sent": "Very interesting, but I don't have time to talk about too much more of them.",
                    "label": 0
                },
                {
                    "sent": "OK, so we talked about the only.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learnability, we've talked about the finite learning we're going to talk about, why it is that actually, even though these two settings seem very similar, they actually very different, and this is what's been keeping us from from relating the two settings as we wanted to.",
                    "label": 0
                },
                {
                    "sent": "So I was actually talking with David McAllister for awhile about how can we relate these two settings?",
                    "label": 0
                },
                {
                    "sent": "How can we show that they're really in some sense the same that you can do as well online as you can in the batch setting?",
                    "label": 0
                },
                {
                    "sent": "And I'll show you you can't.",
                    "label": 0
                },
                {
                    "sent": "And then I'll show you can't.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why aren't they the same?",
                    "label": 0
                },
                {
                    "sent": "Let me give you a very simple example where it's easier to learn in the distribution setting, and this is going to highlight the intuition behind what's going on in this in these kind of problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's take a very simple example where you have the interval between zero and one, and your function is plus from some point on an minus from some point below some points positive above some point minus below some points or your.",
                    "label": 0
                },
                {
                    "sent": "For example, trying to learn whether your device that you built will withstand a certain amount of heat and how much you know what temperature will it start breaking at an.",
                    "label": 0
                },
                {
                    "sent": "You get examples at different temperatures and something like that, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so so it might be here, so here's a very difficult online setting where we don't have a distribution.",
                    "label": 0
                },
                {
                    "sent": "Our adversary gives us the first example and it's .5.",
                    "label": 0
                },
                {
                    "sent": "OK, what are we going to predict?",
                    "label": 0
                },
                {
                    "sent": "We have no idea.",
                    "label": 0
                },
                {
                    "sent": "We make some prediction and then we find out it's plus we whatever I mean, the adversary could have chosen that one at random 5050 and we can force us to make a mistake with probability half.",
                    "label": 0
                },
                {
                    "sent": "If you just flip the coin, there's no way we could do better.",
                    "label": 0
                },
                {
                    "sent": "Then the adversary OK Pick plus there.",
                    "label": 0
                },
                {
                    "sent": "What's he going to pick next to hurt us?",
                    "label": 0
                },
                {
                    "sent": "Does anyone know what he's going to pick next to try to?",
                    "label": 0
                },
                {
                    "sent": "Make it difficult.",
                    "label": 0
                },
                {
                    "sent": "What would you pick next?",
                    "label": 0
                },
                {
                    "sent": "So remember that you know that the function is positive from some point on and negative below that point.",
                    "label": 0
                },
                {
                    "sent": "Fear the adversary.",
                    "label": 0
                },
                {
                    "sent": "What would you pick?",
                    "label": 0
                },
                {
                    "sent": "Where do you say?",
                    "label": 0
                },
                {
                    "sent": "On the here.",
                    "label": 0
                },
                {
                    "sent": "OK, I think so.",
                    "label": 0
                },
                {
                    "sent": "Maybe I explain it backwards, but you kind of think that I mean at least given this information, you might think that things on the right are positive because the function you know is positive from some.",
                    "label": 0
                },
                {
                    "sent": "There's some point.",
                    "label": 0
                },
                {
                    "sent": "Who knows where could be here, where it's positive and after that it's negative.",
                    "label": 0
                },
                {
                    "sent": "So it's going to be, yeah.",
                    "label": 0
                },
                {
                    "sent": "O OK, the adversary wants to make it hard for us.",
                    "label": 0
                },
                {
                    "sent": "Most likely zero is negative.",
                    "label": 0
                },
                {
                    "sent": "If zero is positive, then probably everything else is positive.",
                    "label": 0
                },
                {
                    "sent": ".25 Point 25 good I think part of the problem is I'm not explaining it will be hopefully compared.",
                    "label": 0
                },
                {
                    "sent": "I'm not explaining exactly what the function is.",
                    "label": 0
                },
                {
                    "sent": "There's some point here and then to the right of that point the function is all positive to the left of the point.",
                    "label": 0
                },
                {
                    "sent": "The function is all negative, so we might really think that the function is positive here.",
                    "label": 0
                },
                {
                    "sent": "Let me OK, there could be noise, but let's forget about that for a second so.",
                    "label": 0
                },
                {
                    "sent": "We might think that we have some guess about what the function is over here, but really over here we have absolutely no idea.",
                    "label": 0
                },
                {
                    "sent": "We have no prior.",
                    "label": 0
                },
                {
                    "sent": "There's no distribution.",
                    "label": 0
                },
                {
                    "sent": "We can make a prior over to guests what the value of the function is.",
                    "label": 0
                },
                {
                    "sent": "I have absolutely no idea what the function is here.",
                    "label": 0
                },
                {
                    "sent": "And the adversary again, he could flip a coin and decide what to label that and we would just guess what I mean.",
                    "label": 0
                },
                {
                    "sent": "Whatever we predict, whatever advanced algorithm we use if he flips a coin, there's nothing we can do.",
                    "label": 0
                },
                {
                    "sent": "We have a chance, have to get it right.",
                    "label": 0
                },
                {
                    "sent": "Then it's minus.",
                    "label": 0
                },
                {
                    "sent": "So now where are we going to go?",
                    "label": 0
                },
                {
                    "sent": ".75 OK, we could go to .75, but we might guess the .75 is plus.",
                    "label": 0
                },
                {
                    "sent": "I'm just asking you to do complicated math.",
                    "label": 0
                },
                {
                    "sent": "What is the average of?",
                    "label": 0
                },
                {
                    "sent": "I don't know what did I put here.",
                    "label": 0
                },
                {
                    "sent": "Oh, I didn't even write the number, OK?",
                    "label": 0
                },
                {
                    "sent": "375 Yeah, yes, exactly binary search exactly.",
                    "label": 0
                },
                {
                    "sent": "So the adversary is doing a binary search, but in a weird way.",
                    "label": 0
                },
                {
                    "sent": "He's doing a random binary search.",
                    "label": 0
                },
                {
                    "sent": "He's flipping coins to decide which way to go.",
                    "label": 0
                },
                {
                    "sent": "To us, after the fact, it may seem like he this is all premeditated and he picked the threshold in advance.",
                    "label": 0
                },
                {
                    "sent": "But he's really just doing a binary search here X3 positive.",
                    "label": 0
                },
                {
                    "sent": "Now he's going to flip a coin X for whatever he'll decide at random negative.",
                    "label": 0
                },
                {
                    "sent": "OK, he's going to go in between and so on.",
                    "label": 0
                },
                {
                    "sent": "So at the end of the day, he's essentially given something which looks to us after the fact.",
                    "label": 0
                },
                {
                    "sent": "Like it's completely consistent with.",
                    "label": 0
                },
                {
                    "sent": "Data that's positive on one side and negative on the other side, and yet we have only 1/2 chance to get it right.",
                    "label": 0
                },
                {
                    "sent": "So as you pointed out, the adversary is doing sort of a binary search.",
                    "label": 0
                },
                {
                    "sent": "A random binary search, each label he's flipping a coin equally likely to be positive or negative.",
                    "label": 1
                },
                {
                    "sent": "The end of the day he's predicted according to this function here, which is I wrote it as the sign of X minus some number C here, so it's plus if X is bigger than C and minus effects is less than C and.",
                    "label": 0
                },
                {
                    "sent": "That's what it looks like to us.",
                    "label": 0
                },
                {
                    "sent": "And actually, if you think about it for awhile, you can also think of him as he's essentially picking the bits of C1 by one each each time he tells us a label.",
                    "label": 0
                },
                {
                    "sent": "We're finding out a bit of this.",
                    "label": 0
                },
                {
                    "sent": "So what can we say about this?",
                    "label": 0
                },
                {
                    "sent": "Well, any online algorithm can't get more than 1/2 on average.",
                    "label": 0
                },
                {
                    "sent": "Of these, right?",
                    "label": 0
                },
                {
                    "sent": "But when we look in hindsight, we're going to have a lot of regret, because in hindsight, they'll always be somebody who made no mistakes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So when we look at this example, we say darn, you know this is like the simplest learning problem there is.",
                    "label": 0
                },
                {
                    "sent": "This is the same as the the linear threshold functions.",
                    "label": 0
                },
                {
                    "sent": "The halfspaces problem that we started with, but it's only in one dimension and even this we're not going to be able to learn in this online setting where we think of ourselves as having an adversary in the batch setting where there's a distribution over examples.",
                    "label": 0
                },
                {
                    "sent": "We can learn this.",
                    "label": 0
                },
                {
                    "sent": "How is that?",
                    "label": 0
                },
                {
                    "sent": "Well, in the batch setting we get many examples and even if many of the examples are near to the threshold, that's OK.",
                    "label": 0
                },
                {
                    "sent": "I mean, even if much of the distribution I should say we have a distribution you could sort of imagine drawing the density of the distribution even there's the threshold, even if much of the distribution is near the threshold, which might sound like a tired.",
                    "label": 0
                },
                {
                    "sent": "That's OK, because in our training data will have many examples near the threshold as well.",
                    "label": 0
                },
                {
                    "sent": "So in the distribution setting, this is actually easy to deal with.",
                    "label": 0
                },
                {
                    "sent": "So there seems to be a major difference between the batch and the online setting.",
                    "label": 0
                },
                {
                    "sent": "Another way for those of you who remember the VC dimension, the VC dimension of this class is very small.",
                    "label": 0
                },
                {
                    "sent": "I think it's like one or something.",
                    "label": 0
                },
                {
                    "sent": "So the fact that you can't learn it is kind of depressing in the online setting.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's Anita.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Idea?",
                    "label": 0
                },
                {
                    "sent": "That will hopefully solve the problem OK, and the idea is the following.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that what we're actually getting we actually get to see the examples in advance, so we get to see the X is in advance.",
                    "label": 0
                },
                {
                    "sent": "We get to see all the points in advance, but not their labels.",
                    "label": 0
                },
                {
                    "sent": "So the child kind of can see the problems that are coming up, but doesn't have any idea about their labels.",
                    "label": 0
                },
                {
                    "sent": "And he gets one.",
                    "label": 0
                },
                {
                    "sent": "This is called the transductive setting, I guess.",
                    "label": 0
                },
                {
                    "sent": "Well, it's an online transductive setting so.",
                    "label": 0
                },
                {
                    "sent": "He sees all these examples, but he has no idea what their labels are.",
                    "label": 0
                },
                {
                    "sent": "And then he asked to predict the label for the first example, he makes a prediction and then he finds out the label to see whether it is correct or not.",
                    "label": 0
                },
                {
                    "sent": "And now we have to make a prediction for the second example makes a prediction and so on.",
                    "label": 0
                },
                {
                    "sent": "So what you'll see overtime is that.",
                    "label": 0
                },
                {
                    "sent": "He will see that he'll be able to get to do well in this problem, and the reason is well, if the adversary was doing this binary search like before, he's got a pretty good idea of where things are heading and where that function, where, where to where.",
                    "label": 0
                },
                {
                    "sent": "The threshold might be.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In fact, the adversary of course doesn't have to label things like this.",
                    "label": 0
                },
                {
                    "sent": "You could do something completely different, can put arbitrary noise in there, which will make the problem much bigger, but still now that he sees the data that's coming up, there's really if you have any data points, there's really only end different places and different interesting functions you might think about that you have to compare yourself to.",
                    "label": 0
                },
                {
                    "sent": "Even though there are infinitely many functions on the line, we've now reduced it to problem with only N functions.",
                    "label": 0
                },
                {
                    "sent": "By considering this transductive setting.",
                    "label": 0
                },
                {
                    "sent": "K. And this is this part is joint work with Sean Packeting.",
                    "label": 0
                },
                {
                    "sent": "So again, the transductive setting in the big picture here you would have a bunch of points.",
                    "label": 0
                },
                {
                    "sent": "And you have to make predictions one by one and you get the first point.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, they don't even know need to know the order of the points, but that's another issue that you're going to get, so you get the first point you predict, minus.",
                    "label": 0
                },
                {
                    "sent": "Let's say it's minus, you get the second point you predict minus it, plus you find Mexican.",
                    "label": 0
                },
                {
                    "sent": "So it's the same as the online setting, except that you see the points in advance.",
                    "label": 0
                },
                {
                    "sent": "And again we're going to measure error based on the number of mistakes we've made.",
                    "label": 0
                },
                {
                    "sent": "The percentage of mistakes we've made in hindsight compared to the percentage of mistakes we've made compared to the percentage of mistakes of the best expert.",
                    "label": 0
                },
                {
                    "sent": "Equally of doing, assuming that you have online learning but that is getting the samples IID from there to distribution failure.",
                    "label": 0
                },
                {
                    "sent": "Did you see these?",
                    "label": 0
                },
                {
                    "sent": "Drawing the sample from an ID said instead of giving you the worst samples he can in every case.",
                    "label": 0
                },
                {
                    "sent": "Can you view this as a situation where the adversary is drawing the samples from an IID set?",
                    "label": 0
                },
                {
                    "sent": "I don't think so because the adversary can make these examples come to you in any order he feels like, and the examples don't have to be ID from some distribution there, just some sequence of problems that you're countering as you go through the world running your learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "You moron is worth points when I be.",
                    "label": 0
                },
                {
                    "sent": "He's telling you it's true that he's telling you where the upcoming points are going to be, but that doesn't really tell you what their labels are going to be.",
                    "label": 0
                },
                {
                    "sent": "It's not that I think it's it's somewhat different.",
                    "label": 0
                },
                {
                    "sent": "What you're describing is an in between model would be a hybrid model that's easier, intuitively, easier than the full online model in this model also, but more difficult than the well.",
                    "label": 0
                },
                {
                    "sent": "But it's different than the batch model where you have, like any samples at once at the beginning, describing maybe a model where.",
                    "label": 0
                },
                {
                    "sent": "You have to make predictions online, but you're assuming they're coming from some distribution ID.",
                    "label": 0
                },
                {
                    "sent": "That would be an inbetween model.",
                    "label": 0
                },
                {
                    "sent": "Well, here's another series.",
                    "label": 0
                },
                {
                    "sent": "I think it again, if you rephrase it.",
                    "label": 0
                },
                {
                    "sent": "Tell me another story.",
                    "label": 0
                },
                {
                    "sent": "Red yeah, I mean, if the adversary is not trying to screw us, we can only do better.",
                    "label": 0
                },
                {
                    "sent": "If you play, if you're playing a game against a nasty opponent, you'll do who's trying to hurt you.",
                    "label": 0
                },
                {
                    "sent": "You'll do worse.",
                    "label": 0
                },
                {
                    "sent": "You can only do better if you're playing against a benevolent opponent.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Exactly right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in this case you're saying maybe he.",
                    "label": 0
                },
                {
                    "sent": "Well, he won't be as nasty.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe this once he has to reveal which points he's going to choose the adversary.",
                    "label": 0
                },
                {
                    "sent": "Maybe he won't.",
                    "label": 0
                },
                {
                    "sent": "Maybe he just will give you ID points.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's what he'll do.",
                    "label": 0
                },
                {
                    "sent": "But conceptually, what this answers for me at least, is the question that I had before about what's keeping these two settings from being the same, because they seemed awfully similar.",
                    "label": 0
                },
                {
                    "sent": "And yet, in the one setting we could learn very simple functions in the batch setting.",
                    "label": 0
                },
                {
                    "sent": "We could learn these functions in online setting, we can't.",
                    "label": 0
                },
                {
                    "sent": "So this helps.",
                    "label": 0
                },
                {
                    "sent": "Understand that I think it's still a very difficult setting where you have some idea of what the data is that's coming up.",
                    "label": 0
                },
                {
                    "sent": "But you don't know what the labels are.",
                    "label": 0
                },
                {
                    "sent": "If you don't like the advert adverse aerial way of thinking about it.",
                    "label": 0
                },
                {
                    "sent": "Just think about this practically as this explains why you might think that learning in an online setting where datas are coming for data is coming at you from some from no distribution.",
                    "label": 0
                },
                {
                    "sent": "You can still do as well as if the data were drawn independently.",
                    "label": 0
                },
                {
                    "sent": "From from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "OK. Any questions?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what is the algorithm we're going to use this online, transductive setting?",
                    "label": 0
                },
                {
                    "sent": "Well, we've seen are examples in advance, but labels are revealed on line.",
                    "label": 0
                },
                {
                    "sent": "So basically we have our functions.",
                    "label": 0
                },
                {
                    "sent": "There might even be infinitely many functions.",
                    "label": 0
                },
                {
                    "sent": "OK, what are we going to do?",
                    "label": 0
                },
                {
                    "sent": "OK, well.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at these functions each function.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Make some predictions on the labels of these unknown things.",
                    "label": 0
                },
                {
                    "sent": "We compute these predictions in advance.",
                    "label": 0
                },
                {
                    "sent": "OK, so given these predictions, what we can do?",
                    "label": 0
                },
                {
                    "sent": "What we will do is we will just find that there may be many of these functions even in functions.",
                    "label": 0
                },
                {
                    "sent": "Many of them may be exactly the same.",
                    "label": 0
                },
                {
                    "sent": "And we can now eliminate the ones that are exactly the same.",
                    "label": 0
                },
                {
                    "sent": "That's something that we couldn't have done before in the weighted majority algorithm, we couldn't.",
                    "label": 0
                },
                {
                    "sent": "We just had to get a guarantee that depend on the number of functions that we can reduce it to the number of different ways there are of labeling the sequence.",
                    "label": 0
                },
                {
                    "sent": "So this might start to remind some of you of the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "So now we just can pick a small number of candidate functions for the number of different ways there are to label the sequence.",
                    "label": 0
                },
                {
                    "sent": "In the worst case, there might be 2 to the N different ways to label the sequence of.",
                    "label": 0
                },
                {
                    "sent": "An example is, but for many classes of functions it's going to be much less.",
                    "label": 0
                },
                {
                    "sent": "OK, so we just take our functions.",
                    "label": 0
                },
                {
                    "sent": "We do that ahead of time and then now we've reduced ourselves to a problem.",
                    "label": 0
                },
                {
                    "sent": "If there were L different ways of labeling these examples, we now have effective problem size of L and now we can run the weighted majority on these just L functions.",
                    "label": 0
                },
                {
                    "sent": "And if you remember the regret guarantee it was something like this term here square root log all over North.",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "This should not be in the exponent should be down there.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's two times the regret we have is at most two times that term which will be small.",
                    "label": 0
                },
                {
                    "sent": "OK. That's a full proof that if you have all different ways to label your data, you can get an online guarantee in this transductive setting of that much.",
                    "label": 0
                },
                {
                    "sent": "If you believe the weighted majority algorithm's original analysis.",
                    "label": 0
                },
                {
                    "sent": "And any questions?",
                    "label": 0
                },
                {
                    "sent": "So one question is, how many different ways are there to label?",
                    "label": 0
                },
                {
                    "sent": "Points who does anyone remember the that they remember the name of the theorem that tells you how many different ways there are to label a set of points?",
                    "label": 0
                },
                {
                    "sent": "I think I heard it from someone.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Sing OK good, so I'll say it then you'll see so OK it's our showers lemma.",
                    "label": 0
                },
                {
                    "sent": "So if you define the how do you define the VC dimension VC dimension?",
                    "label": 0
                },
                {
                    "sent": "As you all know, but to remind you is this concept which clearly tells you roughly how complicated the class of functions are and also tells you exactly which functions are possibly learnable and which functions are not learnable.",
                    "label": 0
                },
                {
                    "sent": "If it has infinite dimension, there's no way you can learn that class of functions.",
                    "label": 0
                },
                {
                    "sent": "In the worst case.",
                    "label": 0
                },
                {
                    "sent": "So how do you define the?",
                    "label": 0
                },
                {
                    "sent": "VC dimension, well, you say that a set of points is shattered.",
                    "label": 0
                },
                {
                    "sent": "That's a technical term.",
                    "label": 0
                },
                {
                    "sent": "If if you can label the points in any different ways.",
                    "label": 0
                },
                {
                    "sent": "If there is some function that labels this one negative, one positive, one negative for every combination.",
                    "label": 0
                },
                {
                    "sent": "In this case of eight different labelings, there's some function that labels it.",
                    "label": 0
                },
                {
                    "sent": "So essentially you'd have no idea what to label it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the VC dimension, is defined as the largest set that shattered by functions in this family.",
                    "label": 0
                },
                {
                    "sent": "So if we're doing spaces and in two dimensions, the VC dimension here is 3, because these three points we could always find no matter how you label them, we can always find 1/2 space that labels to positive and one negative.",
                    "label": 0
                },
                {
                    "sent": "And as you remember, the VC dimension is something that really captures the complexity of a family of functions.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, any four points you can't necessarily find, or in general you can't find a labeling that's consistent with this area halfspaces.",
                    "label": 0
                },
                {
                    "sent": "That's consistent with every labeling, so the VC dimension is 3.",
                    "label": 0
                },
                {
                    "sent": "OK. Now there's this very nice lemma that.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Used in proving the VC dimension guarantees in the standard batch setting and it's called sours lemon.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to say them.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to prove it.",
                    "label": 0
                },
                {
                    "sent": "What it says is that the number of ways to label a sequence of examples is.",
                    "label": 0
                },
                {
                    "sent": "This is something that grows polynomially in the worst case polynomial in the VC dimension, so if the VC dimension is 3, like in the previous example, there's only going to be Ncube different ways to label the data.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we get the guarantee is going to be approximately here.",
                    "label": 0
                },
                {
                    "sent": "Some should be to hear the regret will be the log of the size of the number of labelings over N an if N is the number of labelings is that term there it's going to be something like the VC dimension login overhead.",
                    "label": 0
                },
                {
                    "sent": "So in that case it's three login over end, so the regret is going to zero very quickly for these classes of functions that have small VC dimension.",
                    "label": 0
                },
                {
                    "sent": "OK. Any questions?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Just to you know, completely solidify the connection between these two.",
                    "label": 0
                },
                {
                    "sent": "This is actually the same, really the same kind of guarantee you get in the batch setting, looks very.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Familiar, but in fact you know that you know whatever we can do on line, we can do in the batch setting so.",
                    "label": 0
                },
                {
                    "sent": "So you can translate these guarantees back to the batch setting.",
                    "label": 0
                },
                {
                    "sent": "But we want to make sure we can't do better in the batch setting, and in general you can't.",
                    "label": 0
                },
                {
                    "sent": "Do.",
                    "label": 0
                },
                {
                    "sent": "You really can't do better than something like this in general.",
                    "label": 0
                },
                {
                    "sent": "Let me just convince you roughly that you can't do better than that.",
                    "label": 0
                },
                {
                    "sent": "So again, we have our distribution in the batch setting in the batch setting with this distribution.",
                    "label": 0
                },
                {
                    "sent": "Now we have this set that has some VC dimension.",
                    "label": 0
                },
                {
                    "sent": "Let's take this set of points that the set that shattered.",
                    "label": 0
                },
                {
                    "sent": "And let's just make a distribution over these where we don't do well.",
                    "label": 0
                },
                {
                    "sent": "So let's just if we have an examples will put weight 1 / N on each of these examples.",
                    "label": 0
                },
                {
                    "sent": "And the rest of the weight on one of the other examples.",
                    "label": 0
                },
                {
                    "sent": "I just want to convince you that you really can't learn this very well.",
                    "label": 0
                },
                {
                    "sent": "Well, if each of these examples has weight 1 / N, then there's.",
                    "label": 0
                },
                {
                    "sent": "This is the chance that will even see one at each of those examples in the set, so any example that his weight 1 / N and we're picking out examples.",
                    "label": 0
                },
                {
                    "sent": "We have about 1/2 chance one every chance of seeing that example of not seeing that example in the training data.",
                    "label": 0
                },
                {
                    "sent": "I'm therefore we're going to see about half these example, not even going to see the other half.",
                    "label": 0
                },
                {
                    "sent": "And now this set of examples can be labeled either way.",
                    "label": 0
                },
                {
                    "sent": "Any of them can be labeled in any way.",
                    "label": 0
                },
                {
                    "sent": "There's not necessarily any assumption that the labeling of 1 relates to the labeling another, so we're not going to be able to learn these very well at all.",
                    "label": 0
                },
                {
                    "sent": "I mean, in the in the batch setting there is in some sense an adversary is picking this distribution and he could pick this distribution to be who knows what over over the set of points.",
                    "label": 0
                },
                {
                    "sent": "I want one point address your question before about the adversary.",
                    "label": 0
                },
                {
                    "sent": "Is there an adversary in it?",
                    "label": 0
                },
                {
                    "sent": "I do think that these concepts that come out of thinking about things theoretically like VC dimension so do correspond to natural things that you see in practice when you do design algorithms and they are helpful to understand even if the real world doesn't have these adversaries.",
                    "label": 0
                },
                {
                    "sent": "OK, So what anyway what's the conclusion here is that we're not going to do better than you think.",
                    "label": 0
                },
                {
                    "sent": "If here we had a set of size S which was the VC dimension and there's endpoints here, we're going to make we're going to have error.",
                    "label": 1
                },
                {
                    "sent": "At least.",
                    "label": 0
                },
                {
                    "sent": "Let's say this term here.",
                    "label": 0
                },
                {
                    "sent": "Because about half of those we're not even going to see.",
                    "label": 1
                },
                {
                    "sent": "We have no idea what their labels should be, so we'll get those wrong with probability half also.",
                    "label": 0
                },
                {
                    "sent": "So what is the conclusion?",
                    "label": 0
                },
                {
                    "sent": "Putting it all together in the online setting?",
                    "label": 0
                },
                {
                    "sent": "So the online setting is very similar to the offline.",
                    "label": 0
                },
                {
                    "sent": "The guarantees you get are always very comparable to the offline, but it's very annoying that that you can't just get.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From the batch setting back to the online setting and the reason now, one reason you can understand is that, at least if you go to this trend, UC transductive setting that explains why you can do it in the transductive setting.",
                    "label": 0
                },
                {
                    "sent": "In general you can't do it in the general online setting and in the transductive setting you get this very simple analysis saying that.",
                    "label": 0
                },
                {
                    "sent": "You get this many mistakes, which is almost identical to the standard VC bound.",
                    "label": 1
                },
                {
                    "sent": "For those of you who know the VC bounded generalization error.",
                    "label": 0
                },
                {
                    "sent": "In the batch setting, we just also showed that you can't do much better.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can make this bound worsen.",
                    "label": 0
                },
                {
                    "sent": "Look more like the above 1, but you certainly can't do better.",
                    "label": 0
                },
                {
                    "sent": "Much better so if your class of VC.",
                    "label": 0
                },
                {
                    "sent": "This says that if your class is finite visas mentioned, you're going to learn it.",
                    "label": 0
                },
                {
                    "sent": "Your errors going to your regrets going to 0.",
                    "label": 0
                },
                {
                    "sent": "This says in the batch setting if your class has finite, has infinite VC dimension, you're not going to do very well.",
                    "label": 0
                },
                {
                    "sent": "So in general, basically whatever is learnable in those transductive online setting, and whatever is learnable in the back setting.",
                    "label": 0
                },
                {
                    "sent": "Is exactly characterized by the finite VC dimension.",
                    "label": 0
                },
                {
                    "sent": "OK. Any questions?",
                    "label": 0
                },
                {
                    "sent": "So let me finish up with the online stuff, and then I'll move to the games.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the conclusions in the learnability setting are if you have a finite VC dimension that explains exactly what's batch and transductive learnable 11 interesting question is well, what's going on in the standard online setting?",
                    "label": 1
                },
                {
                    "sent": "Is there some notion like VC dimension but different which explains exactly what's learnable and what's not?",
                    "label": 1
                },
                {
                    "sent": "In the non transductive setting, I don't know the answer to that.",
                    "label": 0
                },
                {
                    "sent": "That may be easy or hard.",
                    "label": 0
                },
                {
                    "sent": "Another complaint you might have is that the algorithm I described was kind of inefficient.",
                    "label": 0
                },
                {
                    "sent": "I mean the algorithm that works in the batch setting is also inefficient, but theoretical algorithm is just look at your data and find the best.",
                    "label": 0
                },
                {
                    "sent": "Find the best function from your infinite class of functions that maybe may or may not be inefficient, but what we can say is if you can solve that problem efficiently in the batch setting, then you can design an algorithm that works in the online setting.",
                    "label": 0
                },
                {
                    "sent": "The online setting.",
                    "label": 0
                },
                {
                    "sent": "Remember the algorithm I described as go through your infinite class of functions.",
                    "label": 0
                },
                {
                    "sent": "Look at all the labelings and then decrease that set and it's not very efficient, but the theorem that you can say so it can go from one to the other.",
                    "label": 0
                },
                {
                    "sent": "If you can design an efficient algorithm for the batch setting then you can also define and design efficient setting for the online setting and I'll be happy to.",
                    "label": 1
                },
                {
                    "sent": "Explain it to somebody if they have questions.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So for the next part I want to talk about online learning and repeat.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Games do people want to break question?",
                    "label": 0
                },
                {
                    "sent": "Can you put the same result?",
                    "label": 0
                },
                {
                    "sent": "Using an active adaptive learning setting, very good question you giving day.",
                    "label": 0
                },
                {
                    "sent": "So quickly, good question.",
                    "label": 0
                },
                {
                    "sent": "So the question was what about active learning?",
                    "label": 0
                },
                {
                    "sent": "So in active learning we have this data and we get to choose which point we want to live like the baby sitting there and he sees the data in front of me like I want to know the label of this one and he picks it out.",
                    "label": 0
                },
                {
                    "sent": "So the active learning setting is easier.",
                    "label": 0
                },
                {
                    "sent": "Because you can pick.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of saying like maybe it's not.",
                    "label": 0
                },
                {
                    "sent": "Maybe active learning is not going to help you so much in that more than actually just getting the data.",
                    "label": 0
                },
                {
                    "sent": "In a in a random order, in some sense I mean in terms of the number of mistakes you make in terms of the number of examples you have to label active learning much do might do much better.",
                    "label": 0
                },
                {
                    "sent": "OK. Learning in online setting.",
                    "label": 1
                },
                {
                    "sent": "I mean, we're not using active learning in the sense that we are not choosing which one to label.",
                    "label": 0
                },
                {
                    "sent": "But I am defined I'm talking about an active learning model where online you would pick which one to label you would.",
                    "label": 0
                },
                {
                    "sent": "You would make a prediction yourself.",
                    "label": 0
                },
                {
                    "sent": "Then you would find out whether you're right or wrong and you would overtime count the number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "And that wouldn't be.",
                    "label": 0
                },
                {
                    "sent": "The same example over and over.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have to pick each example and let's say the average Sherry tells you how many times you have to pick each example.",
                    "label": 0
                },
                {
                    "sent": "OK question.",
                    "label": 0
                },
                {
                    "sent": "So all of these results.",
                    "label": 0
                },
                {
                    "sent": "Are for the noisy setting.",
                    "label": 0
                },
                {
                    "sent": "I mean I described a couple of the algorithms in the non noisy setting and this example is misleading because it's a non noisy example.",
                    "label": 0
                },
                {
                    "sent": "But all of these results are with respect to arbitrary data and you're just comparing yourself to the best function in that class of functions.",
                    "label": 0
                },
                {
                    "sent": "So you're not comparing yourself to.",
                    "label": 0
                },
                {
                    "sent": "The best you could have done the best arbitrary function 'cause there's always some really good arbitrary function, but we're comparing ourselves to the best of in a class of functions, and that's what we mean by saying that class of functions is learnable.",
                    "label": 0
                },
                {
                    "sent": "But still, these results are all apply to the setting where there's arbitrary.",
                    "label": 0
                },
                {
                    "sent": "We could be adversary could label the data however he wants, and we look just compared to the single one in hindsight.",
                    "label": 0
                },
                {
                    "sent": "OK. More questions.",
                    "label": 0
                },
                {
                    "sent": "So next I'm going to talk about the online where we online learning and repeated games.",
                    "label": 0
                },
                {
                    "sent": "I just tell you this will be pretty light with much less technical details.",
                    "label": 0
                },
                {
                    "sent": "I'll just tell you something.",
                    "label": 0
                },
                {
                    "sent": "Some of you may know about what a correlated equilibrium is an algorithm for learning them.",
                    "label": 0
                },
                {
                    "sent": "If not, you'll find out.",
                    "label": 0
                },
                {
                    "sent": "There's some really neat neat results and what's going on there?",
                    "label": 0
                },
                {
                    "sent": "Do people do people need a break before that?",
                    "label": 0
                },
                {
                    "sent": "Do people want raise your hand if you want to break for a few minutes before the?",
                    "label": 0
                },
                {
                    "sent": "OK yeah, those of you who want to break go ahead.",
                    "label": 0
                },
                {
                    "sent": "OK, let's take a quick let's take a.",
                    "label": 0
                }
            ]
        }
    }
}