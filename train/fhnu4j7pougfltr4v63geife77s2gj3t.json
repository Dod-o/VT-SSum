{
    "id": "fhnu4j7pougfltr4v63geife77s2gj3t",
    "title": "Learning with Submodular Functions: A Convex Optimization Perspective",
    "info": {
        "author": [
            "Francis R. Bach, INRIA - SIERRA project-team"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Discrete Optimization",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_bach_optimization/",
    "segmentation": [
        [
            "First, I would like to thank the organizers for inviting me and also I have to warn you I might lose my voice at mid talk, so then I have to speak with my hands and I'll try to do my best.",
            "So the goal for me today is to try to show you that the similarity is not limited to discrete optimization and they can be used within regular convex optimization or regular convex supervised learning.",
            "So the."
        ],
        [
            "This is a traditional setup of supervised learning or you want to minimize the regularised and political risk given some data XII.",
            "And often you will minimize loss function, which is the sum of a data fitting term.",
            "Certain loss, which we knew what you want to predict and your prediction.",
            "So F will be a function of X and you add a regularizer for many reasons I won't detail today.",
            "Often F is a vector space, an we.",
            "Many people make sure that the formulation is convex.",
            "So question is that in many many situations in machine learning the problems are just not just continuous.",
            "You have often the underlying discrete structure which is really important to model and trees, graphs and many many other structure, maybe maybe useful and important to model, and are many ways to do that an you can see like half of the talks in this Community doing doing this.",
            "How do we incorporate some structure into your problem, in particular, stochastic processes are a good way to do it.",
            "I think that in this talk I will focus more on a similar ideas, a tool to incorporate these great structure within the convex optimization framework."
        ],
        [
            "So if you see the outline of my talk, so we first do a brief review of similar functions and push mid career and already a good job and it will be a lot more this afternoon.",
            "But I will also focus more on the convex optimization view of the thing in particular through the Lovasz extension an through optimization on similar polyhedra, which I would define.",
            "There we go to showing that the recent work that I've done in the structure sparsity showing how you can incorporate sparse structure sparsity into a convex optimization.",
            "Walk through some of the functions and finally, if time permits, we'll revisit the approximate similar function minimisation from the eyes of a convex optimization, and in particular for approximate similar function."
        ],
        [
            "And if you want to know more, so I've written a tutorial or big tech report which is available on my web page which essentially contains that talk and a bit more alot more."
        ],
        [
            "So these are similar functions, which is a function defined on sets.",
            "OK, so we will always be my base set and it will have a P element and your function capital F is similar if only if this is this is true.",
            "This is true.",
            "So the way I see that for for, for additive measures we have equality here.",
            "OK and file submitted already, you just need to be to be to have an inequality.",
            "An equivalent definition is that one which is the so called diminishing returns property where the gain of adding K OK will be diminishing as doing his a so because of that and then re phrasing a low pass arguments.",
            "The first definition that you could have about about submitted.",
            "Function is one of the concavity.",
            "OK, this diminishing returns property is really characterizing concavity, and as an example, if your similar function, if your function depends only on the cardinality of your set, then it could be a similar if G is concave.",
            "So the first intuition is concavity.",
            "But I think this condition is that it behaves like convex functions.",
            "OK, so the reason why is that you can minimize them in polynomial time, and there's a nice contingency theory.",
            "And the link with convexity, which essentially makes them look like I behave like context functions.",
            "So these submitted our functions have been used in several areas of snow processing, machine learning, computer vision.",
            "As we have seen the first the first talk, but a killer for optimal design by crossing and get stream."
        ],
        [
            "So the classical examples, now we go back to them a bit later in the talk, and I think those ones are quite relevant for machine learning.",
            "We have concave functions of the cardinality we have cuts, and this is a heavily used in computer vision.",
            "Entropy is also also Symmetra and flows and set covers our social media.",
            "And finally, and maybe historically, this is how it started.",
            "Rank functions of matroids also submodular."
        ],
        [
            "So for me the key tool and the way I always see similar function is through the last extension.",
            "OK, so it's so weird to go from the set function which is defined on subsets to real real function.",
            "Define RP First we have to identify the subsets with the vertices of the hypercube, which is quite classical.",
            "Give me a subset that indicator vector and you obtain a vertex of the hypercube.",
            "So now how do you define this robust extension which is often called the Shockey Shockey Integral at address OK?",
            "Which is if you take any set function which may not be submodular and you take any value in RP, you order this couple its components and then you define this subject which is a linear function of W. To me the expression is not so important.",
            "What is important is that if you take W being an indicator vector then F of W is equal.",
            "Is that closer to the value of capital F at a?",
            "So this is clearly an extension from.",
            "That's set to that set.",
            "OK, so if he's always piecewise affine and positively homogeneous, and the key the killing between similarity and connectivity is through this very simple statement that your function you're set function is submodular if and only if it's low fat extension is convex.",
            "So this is really the reason why you can always see similarity to the eyes of convexity.",
            "One other aspect, which I will not use so much.",
            "In this talk, maybe at the end is that if you want to minimize the similar function which is equivalent to minimize F on the vertices on the hypercube, then it is equivalent.",
            "To minimize this on the entire hypercube.",
            "So this is a.",
            "One reason why minimizing similar function is a polynomial time."
        ],
        [
            "So associated to any piecewise affine to any piecewise affine convex function, you have always as a Poly Hedren or polytope hiding around as which would be so that the convex function is a support function of the convex set and those convex sets are the so called submitted after the run as a base polytron.",
            "So P of F is a set of elements in RP which satisfies is set of linear ecology.",
            "OK, that S of a.",
            "So here we use this a lot in this talk as of X is a vector S of SIS can be used as a modular set function where SH uses some of the elements of X which are correspond to into elements of a OK, so some of K in a of ask.",
            "So go F is something like that and if you play around with similar functions you will end up doing a lot of those of the things, which is a similar polyhedron, and the baseball legend, which is simply appear of intersected with the hyperplane so PEO S over there and be be of FBF is here.",
            "And then three you get something like that.",
            "So here all these supporting hyperplanes are always with a fixed direction.",
            "OK, because they correspond to victorious ones and zero so most resorts.",
            "Instead of functions can be seen through those polyhedra.",
            "So what do?"
        ],
        [
            "Link the former link with the lowest extension and this dates back to Jack Edmonds in the 70s.",
            "Is that the lowest extension?",
            "Happens to be the support function of the of the polyhedron, so by the support function is a maximum of linear functions.",
            "And as you remember from the expression of F, if you want to maximize a linear function on the baseboard run, then this can be done in closed form for the so called greedy algorithm.",
            "So if you take."
        ],
        [
            "All of those complex Polly Polly Polly.",
            "This is a polytope is a polytope.",
            "Imagine this in high dimensions, so there's a lot of faces, a lot of history points, but you can minimize."
        ],
        [
            "You can maximize or minimize linear functions in the enclosed form.",
            "OK, so the so called Red Dagger rhythm, and essentially most of the operations that people do in similar.",
            "Similarly Edra are based on the fact that you can maximize easily linear functions if you want to do more operations like projection, then you have also algorithms and you can see my tutorial to know more about that."
        ],
        [
            "So how can you optimize some of the functions so you want to minimize the set function so it's been known that it's a polynomial time and the currently best known algorithm with complexity bound is appear.",
            "Exact algorithm is a pita.",
            "The fix and those are the names of the classical name associated with those types of algorithm.",
            "There's another algorithm, cause a minimum nonpoint algorithm, which is somewhat efficient in practice but with low complexity bound.",
            "I will come back to that later and of course at this weather, especially the topic of the first invited talk when you have a spatial subdural function like occur tour flow, you have fast dedicated algorithm.",
            "Early days, and this is quite a negative I of research with several papers at these nips and last names."
        ],
        [
            "So this is the last last point about similar function is the link the last point I want to make is a link between convex optimization an similar optimization.",
            "So let's assume that you have a set of functions like a where case from one 2P which are strictly convex.",
            "So if you want, you can think of those as being the quadratic function TF WKWK square.",
            "Then you can look at problems OK which are regularised by several function.",
            "OK so essentially.",
            "In my regularised in particular risk minimization framework, my normal will be linked to the summit to the lowest extension, so those will appear quite quite often.",
            "So if you want to minimize that function which is convex because it is convex and convex, this has a unique minimum.",
            "Because of symmetry, convexity, and it turns out that you can find just find the dual of that problem, and he's simply a maximization problem on the on.",
            "The similar on the base polyhedron, so this is equivalent to that.",
            "So this is always.",
            "This is a very common.",
            "For example if you want to do the minimum to when you put it like that, the elbow norm, it is equivalent to project on the L Infinity norm on the Boulevard infinite number.",
            "So this is exactly the same same concept.",
            "So this is a one line of code.",
            "I think the key the key relationship outline by shovel and Abu and maybe earlier.",
            "I don't know is that if you take the solution and I got the unique if you take W start being the unique solution of that problem, then it turns out that for all possible Alpha in R, then through the level sets of that W star you have access to the minimal and maximal minimizers of that similar function.",
            "So F is similar, and this is simply a modular function.",
            "So here if you know how to solve that then you know how to solve that OK by simply taking the level sets.",
            "In particular if you make sure that.",
            "Fiquet prime of 0 is zero.",
            "OK, so this this so far 5 zeros disappears.",
            "Then you can have access to the minimum of F. On the contrary, you can, if you know how to solve that for all possible Alpha, you get all level sets of the produce.",
            "You can go back to the solution of that problem.",
            "So essentially convex optimization problems are that for my equivalent to sequence of similar function minimization."
        ],
        [
            "This is summarized in the.",
            "In this slide.",
            "So if you know if you start to solve the convex optimization problem, OK to solve the similar function minimization problem, you just threshold your solution at zero OK. And of course this is a well known and by especially when you take this aquatic function and in that case if you if you look."
        ],
        [
            "As a problem, this becomes a mini.",
            "You want to minimize the squared to dominate S4 as belonging to the base polyhedron.",
            "So essentially you want to find the orthogonal projection on zero of zero on the base polyhedron.",
            "So this was known."
        ],
        [
            "This was known by for awhile and this leads to an approach for similar function minimization.",
            "OK, so you minimize that problem.",
            "OK an then you obtain the minimum of that one.",
            "But you can go in the in the other around.",
            "OK, so if you know how to solve to minimize your similar function, maybe plus certain modular function, then it's enough for minimal minimizing those problems.",
            "So there is a very nice.",
            "Divide and conquer strategy outside by convert whereby at most P at most AP.",
            "Similar function minimization you get the exact minimum of this guy, so of course this is only efficient when you know how to stop those guys efficiently.",
            "So in particular for cuts in flows."
        ],
        [
            "For now, I know that I presented a bit in detail some other functions I go to present the how they can be linked through sparsity."
        ],
        [
            "So just to fix notation so when you supervise machine learning when you have data XII, so I would assume XI is in RP, then you put them in a response circuit Y Ana design metrics Capital X an momeni.",
            "Mini Worth will try to minimize the secularized empirical risk.",
            "We have a data fitting term again plus regularizer, and I will not do this as follows and you are people use normal mega to promote sparsity for several reasons.",
            "So many examples of the manager is the L1 norm to create some zeros, and this is called basis pursuit.",
            "Sticks, and often they use this as a proxy for interpretability.",
            "OK, if you have only a few a few variables which are non zero, you can look at the associated features to see which feature is important for your problem."
        ],
        [
            "So he's going to be using unsupervised learning, so if you're given a set of signals, YKC notes in RN, then the dictionary learning will try to essentially find a set of terms X12 XP so that you can express Y as a linear combination of your of your items.",
            "OK, so this is if you if you put no constraints or the X or W this is PCA, but as soon as you put a space constraint on Omega on WRX you.",
            "Something different, so if you put a space constraints on W you want to find a term so that you can express each of your signals with only a few basis vectors.",
            "And if you put a sparsity constraint on your on your dictionary, you want to have a lot of zeros in your new items."
        ],
        [
            "So why, why would you want to do sparsity?",
            "So you have several reasons, and for me the key one is interpretability.",
            "OK, so you it's not because you have a lot of zeros that you can really interpret the result of algorithm, and I will briefly briefly present 22 instances which I've worked with.",
            "American limousine scheme.",
            "The first one is when you do additional learning of PCA.",
            "You might want to have dictionary elements which are so much structured so."
        ],
        [
            "Let's give an example.",
            "When you apply these two faces, OK, let's imagine you have 1000 face faces and you want to find basis vectors so that you can express each of your face as a sparse combination.",
            "Then, if you want to, if you impose sparsity.",
            "So this is just suspense PCA, I just put an L1 norm on my dictionary elements, then I'll 10 basis vectors.",
            "Which of that form where?",
            "Here Gray is zero and black or white are positive and negative values.",
            "So here to help."
        ],
        [
            "So I put the zero values in green so you can see here you have a lot of a lot of grain."
        ],
        [
            "Lot of zeros, but this does not mean you can really interpret the results because the patterns are often scattered.",
            "The goal of this of this talk is to show that by simple."
        ],
        [
            "Changing the norm so you replace the element."
        ],
        [
            "Come by something which is structured.",
            "Then you can have something like this where here we have imposed the constraint that dictionary."
        ],
        [
            "Once I've have nonzero patterns which are convex or diamond shaped patterns in the in the plane, so here."
        ],
        [
            "So goal is not to discover so much resource, but just to convey the idea that but just changing your Norma can get results which are a lot more interpretable."
        ],
        [
            "This can be done.",
            "This can be known then as well if you want to put structure not on the dictionary elements but on the Alpha."
        ],
        [
            "Use.",
            "OK, so he's a typical reader that we that we have that we learn a dictionary which means which can be embedded into a tree, so I won't give the details of that application.",
            "But essentially again we say OK, sorry bout to impose a tree structure, so carbonated structure and with this we can do by using sparsity.",
            "Using norms based on similar functions.",
            "OK, so."
        ],
        [
            "What will be the key tool to do this will be to look at the convex envelopes of potential functions.",
            "So what is the convex envelope?",
            "Let me just use a classical album non example.",
            "OK so if you tap for if you start from any W like this and you.",
            "And you define IT support as a set of non zero elements of the value that the so called LO penalty is simply the cardinality of the support.",
            "So this is a highly nonconvex function is not even continuous and you can see the L1 norm as a convex envelope, which is the tightest convex Lebanon that function on the set minus one one.",
            "So just to give this distraction in 1D.",
            "This is a in Java minus one one in blue.",
            "You have the LC open energy which is 080 and one everywhere everywhere else, and the goal is to find a convex function which is below a nice tight as possible.",
            "So if you think of it you it must be the one norm.",
            "OK, so this is a traditional link between album Norman, The LO penalty.",
            "So the goal will be to change cardinality by a set function to encode your prior about your problem.",
            "OK so this is what we're going to do."
        ],
        [
            "So you take any set function OK, which are considered nondecreasing.",
            "I want to penalize more bigger subsets.",
            "OK, take a non decreasing function and you should see this function as a prior knowledge on supports.",
            "OK so some some subsets.",
            "I like a lot so in the context of the trees the ones which look like subtrees.",
            "I put a small value there, the ones for which the ones who don't look like subtrees put put high value.",
            "OK so it's.",
            "Are you on your on your on your supports?",
            "An the goal will be to look at that function where you take your set function F applied to the support so it is still nonconvex problem.",
            "OK, and we will try to follow the same route As for the end.",
            "Will not try to find the Contacts on the loop.",
            "And as we see, this will be simple when F is, F is a sub nom."
        ],
        [
            "OK, so this is a first position relating like similarity to sparsity, that function on the supports OK. Then if you right click on the Infinity ball like for the L1 norm then the context of the lab.",
            "So the titles convex Robin is exactly the low back extension taking taken at the absolute values of W. OK, so this is just as a sanity check if you take the F being the cardinality then the low back extension is a sum and you get back the L1 norm.",
            "OK, so this is simply a way to extend the classical relationship between L0 and L1.",
            "So let's look at the shape.",
            "So here and also you always obtain a norm.",
            "OK, that is."
        ],
        [
            "Set up so usually when you have norm and it is often the case in most talks about sparsity, you show you show the balls you throw the balls of that Norman.",
            "This is what I show here where is in 2D OK so this is a generic generic unit bowl for the for the norm which which I have defined.",
            "And of course since F the robot extension is a piecewise piecewise now then my ball has to be either has to be a polytope and depending on the values of the subterra functions some extreme points will go come and go and somebody.",
            "Appearance of European OK, so here this is a generic case where all these three points are present, but you also have like the L1 norm.",
            "OK, the Infinity Norm is a subcase an another number here.",
            "So of course when you do a convex relaxation like that the goal what you want is to that you preserve the properties of the original function.",
            "OK, because you make your problem convex that you have solved anything you want to relate one with another.",
            "One example of a good good property is the.",
            "Sterile sets, so it's set isn't really stable.",
            "Sets are the ones which can be obtained when you when you those when you use those types of higher on support.",
            "So you'll stable that only if if you increase the set OK then you have to strictly augment or similar function.",
            "OK, so if you're not stable you can always add a bit more and not increase your cost, so you will always at element.",
            "So essentially if you want to use that function as a prior, you would always have stable set as optimal values.",
            "Anyway, you can show that for the convex convex approximation this is true as well.",
            "OK, that with probability one only stable sets possible when you are when you optimize with that number.",
            "So simple example where you can link the properties of the original function to the properties of the convex laxation."
        ],
        [
            "Then you can look in 3D to look at several at several other unit ball.",
            "OK, this is beautiful, but not so nice."
        ],
        [
            "So now let's look at some examples.",
            "OK, so here what is nice about having a general framework is that you can look at existing norms an given you view into those existing norms and vice versa.",
            "Take your list of subdural functions and try to see if it corresponds to the new norm.",
            "So let's do the first direction on the on the right, and now you can consider all the norms which are based on so cool like everyone else.",
            "Unity norms where you some.",
            "About a set of groups.",
            "OK so Kelly graphic Jesus set a subset, a set of subsets and you penalize the sermon.",
            "Infinity Norm of the predictor strictly to that subset.",
            "OK, so he sees that we use a lot in the last in the last few years and it is known to induce notion of group sitting, which is the following at the optimum.",
            "So when your eyes by that Dom, the W that you obtain will be so that such that many of the WGS.",
            "Will be equal to zero.",
            "OK, so some of the groups that you have started from WG be equal to 0.",
            "So this means that essentially the supports the zeros which is complementary.",
            "The complement of the support would be a union of groups.",
            "OK, so this is the things which we have worked on a lot and it turns out that this corresponds to a certain similar function, essentially the one they want.",
            "The world which we think of.",
            "If there is simply the cardinality of the group which.",
            "Intersect you or yourself a OK.",
            "So the context of a partition in Jesus partition, you just count the number of active element in the partitioner.",
            "But of course if G is not the partition anymore, it's a.",
            "It's a bit more complicated and I will show examples in a few slides.",
            "So what is important?",
            "What is nice about like going from here to here is a way to interpret the properties of that norm, not only through the allow sparsity patterns, which was what we will do in previous work but through full prior.",
            "Onset functions in the paper we have examples.",
            "On the way this might be important.",
            "OK, let's give a very simp."
        ],
        [
            "Example OK, if you take Viagra switch organizing the sequence then you on the left.",
            "If you take those groups OK or the left introvert and order right intervals then if you take a union of those of any any number of those groups you get something which is shaded here a bit on the right and a bit on the left.",
            "So this will be the zero of your of your predictor.",
            "So the non zero would be what is in the middle.",
            "OK so here it's actually you will use that to only to enforce the constraint that you want.",
            "Goose pattern, so you want W the nonzeros of W to be to be continuous, and this corresponds to the set function, which is the range of a.",
            "So essentially when you want to impose a prior that you want to be contiguous, it mean that what counts for your predictor is just the range and not not anything more.",
            "And if you want to check the stable sets of that of the range function, essentially are essentially the potentially the.",
            "Doubles"
        ],
        [
            "So we have other examples of groups, so you can do so, like rectangles an now you get the notion of perimeter and if you add more if you add more more angles and you can go to this like diamond shaped patterns which I've shown for the sparse PCA case.",
            "If you have your keys like that then you can define a simple similar function which is which will given any set count your total number of ancestors.",
            "So if you use that as a prior then you will try to enforce to select.",
            "Viables only after you have selected all of your ancestors.",
            "OK, so essentially you can go and look at all all set functions based on covers and you can derive the associated similar function in normal."
        ],
        [
            "So this was in One Direction.",
            "You go from Norm to know Norm to submit a function.",
            "But you can do the other way around and this don't want to kiss too much of that.",
            "I will just simply mention two types of two possibilities, functions of cardinality.",
            "Then you can show in that case at the last extension depends on the order statistics and also functions of the values of a submatrix.",
            "So typically if you take the entropy of the gas on doctor then the entropy is a log cabin and and if you take a symmetrix.",
            "And take the log data that so symmetrix index by some of the rows and columns of the matrix, you obtain a similar functions and these can be used as non factorial prios or supervised learning if you want to know more about that you can look at the.",
            "At my newspaper of last year."
        ],
        [
            "So this was a set of examples and the good thing is that for all of those examples, you can use a unique algorithm or some Unix algorithm, and this is what I will try to describe now.",
            "So first the first approach, which I could think of is that OK, so my norms.",
            "Piecewise defined functions.",
            "It's a big LP, so why not use an LP and LP 2 bugs in our programming toolbox?",
            "The problem that you have many, many faces and extra points, so this is clearly inefficient.",
            "Then you could say, oh, because my love at extension is a maximum of inner functions which I can obtain through the greedy algorithm.",
            "I have a subgradient, OK. As a surgeon, so you could take your get your loss +2 regularizer so you can take the subject of your actual regularizer.",
            "But as you will see, this is too slow because it's known to be to be slow.",
            "Here we can leverage recent work in optimization, which is so called proxamol gradient methods, which will be adapted to functions of that form.",
            "OK, so typically machine learning we have alerts between Y, which is your output and WX which is your input.",
            "So this is.",
            "If you assume that dollar is differentiable, typically squares or logistic, then this is differentiable.",
            "This is not differentiable.",
            "OK, it's really not differentiable, and if this is a hard one to solve because it is non smooth.",
            "But if you can solve those problem easily, OK, so it's really if you can replace the loss by isotropic quadratic function like that.",
            "If this is easy, then by a sequence of many of these we can find the optimum of that and essentially it will converge as fast as.",
            "If they will not be as fast as if it was totally totally Smith, OK so proximal methods are away to do nonsmooth optimization, at the cost of smooth optimization, and this is only possible if you can solve that efficiently.",
            "So if you want to know more about this, you can see we have a recent paper with Jeanette Rodolphe.",
            "Key in the financial trends in machine learning which will expand this.",
            "How this works.",
            "So the question is how can you solve that efficiently?",
            "OK so here usually you should recognize essentially the.",
            "Laconia, sensually, the supercable optimization problem about this happening before OK.",
            "So these proxamol problem which we need to start over and over is exactly equivalent to this separable optimization.",
            "So it can be solved through a sequence of SABMiller function minimization.",
            "So when you do that, of course we can use a nice divide and conquer strategy of a ground web, but this is only possible if you have curves and flows.",
            "Namely, if you have a similar functions for which you can minimize which you can minimize the very efficiently and we have done that in the context of the norms with overlapping groups or in the generate case you can use the minimum number point algorithm which I want describe number, but I will bit later."
        ],
        [
            "So the type of the type of behavior that we always obtain but uses those type of algorithms.",
            "So we have taken a simple supervised learning problem.",
            "OK, where are with the 1000 variables and with a very simple similar function.",
            "The goal here is to compare the gains that you have by using by really using the inner compatible structure of jugular eyes are OK, so if you don't use anything you just go put 1st century subgradient, then you're covered.",
            "Converge, but very slowly, but as soon as you use the proximal operator, which requires to look at the discrete structures, the problem, then you go a lot faster if you use a non accelerated version an go even faster if you use the X rated version.",
            "So just a simple example where you can use this kid optimization to speed up convex optimization."
        ],
        [
            "For there are many extensions which you can consider.",
            "I won't talk about it first, have not talked a lot about statistics, but we have some unique algorithms for the for optimization.",
            "We also have some unified analysis.",
            "OK, especially if you use a similar function that you do what you expect, like recovering the sparsity pattern of your problem.",
            "We can extend this to a symmetric similar functions where this was work at this conference where you can instead of trying to put pressure on the 0 nonzeros of your problem, you put a prior on all level sets.",
            "Then we have two extensions where I think to me are very important because the sad story about these this current framework is the following."
        ],
        [
            "If you take the L Infinity Norm, OK guys, sponse to that subclass function.",
            "Essentially you put no prior no prior on the zeros.",
            "OK, you allow all possible patterns.",
            "OK, you put up higher, but so the problem that you still have a lot of corners in your in your ball.",
            "OK so you have.",
            "Garner switch we don't really want so at the end, and those colors corresponds to places where your vector as equal equal magnitudes.",
            "OK, so when you use any norm, you always extract higher, which you might want or might not want.",
            "Don't want it and what we have done."
        ],
        [
            "It is a German.",
            "You have a poster somewhere over there is that you can replace this an Infinity norm by L2 norms or more generally LP norms, to remove the artifact associated with those norms.",
            "Also also at the same way this same posterior, you can generalize this setup to other set functions.",
            "OK, so here we have focused.",
            "I focused on similar function.",
            "You can extend that to other set functions."
        ],
        [
            "OK, it's not."
        ],
        [
            "The problem of approximate subduction minimization.",
            "Essentially, for most machine learning applications, you don't need to get the exact solution.",
            "OK, so this is a very common theme in optimization in machine learning, so the goal will be to find a solution which is approximate.",
            "So without loss of generality, we will assume that the signal in terms of positive values and this is true.",
            "OK, this is a well known in similarity and nobody noticed square the square would be the essentially this quantity, which is essentially an upper bound on the right on the edges of the baseball.",
            "Even so, since we have similar function OK, and I've told you that minimizing the similar function F on the hypercube, we lead to a minimum of the similar function.",
            "You can think of the simplex algorithm to minimize.",
            "Functions which is celebrating dissent.",
            "So if you do sublimation dissent then after that iteration you could show that you get close to the optimum.",
            "OK, in the sense that the distance to optimum is apondi by this quantity.",
            "So for those who know the topic, this is simply directed gap between the primal and dual and dual problem and this is upper bounded by by this which is D that will sqrt 3 / sqrt T. OK so you can show that the.",
            "When you apply this then you always get you convert the right wonders crowd FT.",
            "So this is pretty slow.",
            "OK, so let's see if we can improve if we can improve improve this."
        ],
        [
            "So we will try to improve that by looking at the link with quadratic functions, which is often used to minimize similar function.",
            "So another goal would be to look at approximate quadratic optimization on the base polyhedron.",
            "So the goal is to minimize this, and as I shown later, as I showed later, this is equivalent to maximizing this, so we want to find the minimum Elton on point in the best polyhedron.",
            "And remember, we can only maximize enough functions in BF.",
            "So here what we have, we have a convex optimization problem on the.",
            "On the convex set on which can only maximize in our functions.",
            "So you see the classical setup for so called quote algorithms and the subtle point is that you have two types of Frank Wolf algorithms, other ones which are dedicated to quartic functions.",
            "I will try to be active set algorithm, so this is this is the usual minimum partner in them, so the way they work you do a sequence of maximization of functions.",
            "Do summer fun projections, and then you iterate that over and over.",
            "So what you can show that was appointed by the face of the polytope, there's a face you as soon as you optimize the optimal.",
            "As soon as you identify the optimal phase of your polytope, then you converge and This is why it is.",
            "It has finite convergence but has all active stagger them.",
            "There is no complexity bound, but the second set of algorithms is so-called also called conditional greggery them while you also do a sequence of maximization of inner functions but.",
            "A you simply do a very simple, very simple algorithm so."
        ],
        [
            "Let's look at a simple example.",
            "OK, so let's say you want to, so you want to be as close as you want to Project 0 on the on that pull it open, you start from that point, OK?",
            "Then you look at the gradient.",
            "OK, so you follow it.",
            "Follow the gradient so you want to maximize linear function at the top and you get you get to there.",
            "OK so and then you do a line search from where you started from to the vertex of that of the polytope.",
            "OK, if your answers will get there, so you start again, you want to go in that direction.",
            "OK, that direction, and if you do that then the proper the farthest point is this one.",
            "OK, so you do align search between different, this one and you throw it over and over.",
            "So then you see that this will never be.",
            "This will not be phonetic convergence.",
            "This will converge slowly to the optimum OK, but the good ASP."
        ],
        [
            "Yeah, that it's good aspect.",
            "Is that OK?",
            "There's no overheads.",
            "OK, so there's no official projections to do, and it comes with an approximate optimality band similar to gradient descent.",
            "So let's look at this optimal."
        ],
        [
            "Bound.",
            "So if you do the steps of conditional gradients with line search OK, then you will output.",
            "So an element of the best polytron plus probably viable.",
            "So here in All in all those results you always have abound plus certificate authority, which you can really satisfy that your optimal and you can show that your distance to optimum is upper bounded by something which is which decays as one of our team.",
            "OK, so this is an improvement here that to minimize cratic functions.",
            "Then it is a bit faster.",
            "OK. Then I would imagine this quickly.",
            "Then this you can improve results by by a lot by not taking the primary candidate which is given by the algorithm.",
            "OK, so given and this is based on the following, the following thing if F if F is a.",
            "If you fix the order of the values of WF becomes linear.",
            "OK.",
            "So now if you want to minimize those that function subject to fix ordering, it becomes a isotonic regression which can be solved North.",
            "OK so this is a way to improve a lot the.",
            "The primal value.",
            "So now this does it lead to a better bound for similar function minimization and."
        ],
        [
            "We see that this is not the case, so if you take any W which is approximately optimal for the quadratic function, then there is at least one level set of that of W, which is optimal OK, But you lose something in the process, you lose a square root in the process, and if you take the band I have for conditional gradient and you apply it, then what you get is exactly the one for dissent.",
            "In a sense, this is a looks like a kind of useless.",
            "Which is not really the case because with that algorithm and I won't give details, we can have a bit better behavior, which I won't detail, and in fact I've tried like four months to improve that down.",
            "OK, and I catch the conclusion that it might be, it might be, it might be impossible.",
            "In fact it is impossible.",
            "So you have a lower complexity bound for some deflation minimizations if you're only based, so this is a new statement.",
            "And if you want we can talk about like much, much more precise statements if you best.",
            "Dora the sequence of Summit of Code is a greedy algorithm and you combine what you obtained in early.",
            "Then there is no way to improve to improve the this this down.",
            "OK, at least for the first 2 iteration, so this is a class.",
            "This mimics the classical lower complexity bounds for convex optimization.",
            "So essentially this band is not really a improbable."
        ],
        [
            "So now we just keep."
        ],
        [
            "At least keep the."
        ],
        [
            "The experiments and conclude so he represented.",
            "So some of the functions from the from a convex optimization perspective and with applications to status Bar City.",
            "And I have also shown that seeing similar functions as being as being simply convex function bio linked with convex function can derive new algorithms for optimization.",
            "They are nice set.",
            "I think there's still a lot to do in that in that context on the link between convexity and similarity, in particular in terms of primal dual optimization.",
            "So I have not shown the experiments, but when you minimize or users.",
            "Those interactive algorithms we end up having.",
            "I think you're very good primary candidate and a very bad dual candidate.",
            "So essentially you have converse very quickly in the Bible, but you spend a lot of time improving your certificate of optimality, so you might hope to do a much better.",
            "And finally, this is really.",
            "Not clear what you could do, but all these similarities based on linear programming.",
            "OK, so it would be nice to see if you can extend that to other type of context problems like semidefinite programming.",
            "Thank you for your attention.",
            "But or I could neglected.",
            "You should wait, or less assumptions.",
            "So you can.",
            "So if you want so, let's get technical if she might frustrate you, need assumptions.",
            "If you want to slow right, you don't need assumptions.",
            "So for all apps you can do it.",
            "Under some of these assumptions.",
            "So you can do comics minimization fossil, so you think that there is a counterpart for submodular.",
            "Good question, but I have no clue.",
            "No, it is not clear that the.",
            "So for example, if incapacitation, when you have strong convexity, things are easier.",
            "So I've been looking for the last few months at as equivalent notion for similar functions.",
            "So what makes the problem easier?",
            "And it's not totally clear.",
            "So in in your long right you have some comparisons of the North and some of this.",
            "Is there anything since then?",
            "I mean any updated results that you skipped over here?",
            "Or is that no no.",
            "I haven't tried your submitted a function in which I plan to do.",
            "I guess it's your question.",
            "Any other any other known to have tried to benchmark so critical benchmarks?",
            "But I think those benchmarks which is a bit sad or min cut problems, so I think if you can derive some benchmarks which are not making problems that would be that would be nice because we have been killed you should do something else anyway so.",
            "You discussed approximating submodular minimization.",
            "Question is.",
            "Your continuous.",
            "Message.",
            "Add anything to actual submodular minimization, but if your goal is just to be approximate, if you just want to proximate minimizer, it's a lot faster.",
            "Oh no, because then what that case you need the epsilon so the year the gap to be very small in the sense that has to be the minimal value.",
            "The minimal in the minimal nonzero increase of between two values that you submitted function.",
            "And this might be a bit hard small.",
            "OK, so if your goal is to be exact, this will not help.",
            "You can find at worst cases of which we never help.",
            "So you would agree that.",
            "Checking real value of extension of the.",
            "Search functions.",
            "It's not like they can contribute anything to exact actual submodular memorization for the exact algorithm would be, I would say so as well.",
            "Yeah.",
            "There only provides a way.",
            "Real extensions only provide a way to.",
            "Approximant.",
            "I think this is true."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, I would like to thank the organizers for inviting me and also I have to warn you I might lose my voice at mid talk, so then I have to speak with my hands and I'll try to do my best.",
                    "label": 0
                },
                {
                    "sent": "So the goal for me today is to try to show you that the similarity is not limited to discrete optimization and they can be used within regular convex optimization or regular convex supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a traditional setup of supervised learning or you want to minimize the regularised and political risk given some data XII.",
                    "label": 0
                },
                {
                    "sent": "And often you will minimize loss function, which is the sum of a data fitting term.",
                    "label": 0
                },
                {
                    "sent": "Certain loss, which we knew what you want to predict and your prediction.",
                    "label": 0
                },
                {
                    "sent": "So F will be a function of X and you add a regularizer for many reasons I won't detail today.",
                    "label": 0
                },
                {
                    "sent": "Often F is a vector space, an we.",
                    "label": 1
                },
                {
                    "sent": "Many people make sure that the formulation is convex.",
                    "label": 0
                },
                {
                    "sent": "So question is that in many many situations in machine learning the problems are just not just continuous.",
                    "label": 0
                },
                {
                    "sent": "You have often the underlying discrete structure which is really important to model and trees, graphs and many many other structure, maybe maybe useful and important to model, and are many ways to do that an you can see like half of the talks in this Community doing doing this.",
                    "label": 0
                },
                {
                    "sent": "How do we incorporate some structure into your problem, in particular, stochastic processes are a good way to do it.",
                    "label": 0
                },
                {
                    "sent": "I think that in this talk I will focus more on a similar ideas, a tool to incorporate these great structure within the convex optimization framework.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you see the outline of my talk, so we first do a brief review of similar functions and push mid career and already a good job and it will be a lot more this afternoon.",
                    "label": 0
                },
                {
                    "sent": "But I will also focus more on the convex optimization view of the thing in particular through the Lovasz extension an through optimization on similar polyhedra, which I would define.",
                    "label": 1
                },
                {
                    "sent": "There we go to showing that the recent work that I've done in the structure sparsity showing how you can incorporate sparse structure sparsity into a convex optimization.",
                    "label": 0
                },
                {
                    "sent": "Walk through some of the functions and finally, if time permits, we'll revisit the approximate similar function minimisation from the eyes of a convex optimization, and in particular for approximate similar function.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you want to know more, so I've written a tutorial or big tech report which is available on my web page which essentially contains that talk and a bit more alot more.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are similar functions, which is a function defined on sets.",
                    "label": 0
                },
                {
                    "sent": "OK, so we will always be my base set and it will have a P element and your function capital F is similar if only if this is this is true.",
                    "label": 0
                },
                {
                    "sent": "This is true.",
                    "label": 0
                },
                {
                    "sent": "So the way I see that for for, for additive measures we have equality here.",
                    "label": 0
                },
                {
                    "sent": "OK and file submitted already, you just need to be to be to have an inequality.",
                    "label": 0
                },
                {
                    "sent": "An equivalent definition is that one which is the so called diminishing returns property where the gain of adding K OK will be diminishing as doing his a so because of that and then re phrasing a low pass arguments.",
                    "label": 0
                },
                {
                    "sent": "The first definition that you could have about about submitted.",
                    "label": 0
                },
                {
                    "sent": "Function is one of the concavity.",
                    "label": 0
                },
                {
                    "sent": "OK, this diminishing returns property is really characterizing concavity, and as an example, if your similar function, if your function depends only on the cardinality of your set, then it could be a similar if G is concave.",
                    "label": 1
                },
                {
                    "sent": "So the first intuition is concavity.",
                    "label": 0
                },
                {
                    "sent": "But I think this condition is that it behaves like convex functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so the reason why is that you can minimize them in polynomial time, and there's a nice contingency theory.",
                    "label": 0
                },
                {
                    "sent": "And the link with convexity, which essentially makes them look like I behave like context functions.",
                    "label": 0
                },
                {
                    "sent": "So these submitted our functions have been used in several areas of snow processing, machine learning, computer vision.",
                    "label": 1
                },
                {
                    "sent": "As we have seen the first the first talk, but a killer for optimal design by crossing and get stream.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the classical examples, now we go back to them a bit later in the talk, and I think those ones are quite relevant for machine learning.",
                    "label": 0
                },
                {
                    "sent": "We have concave functions of the cardinality we have cuts, and this is a heavily used in computer vision.",
                    "label": 1
                },
                {
                    "sent": "Entropy is also also Symmetra and flows and set covers our social media.",
                    "label": 0
                },
                {
                    "sent": "And finally, and maybe historically, this is how it started.",
                    "label": 1
                },
                {
                    "sent": "Rank functions of matroids also submodular.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for me the key tool and the way I always see similar function is through the last extension.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's so weird to go from the set function which is defined on subsets to real real function.",
                    "label": 0
                },
                {
                    "sent": "Define RP First we have to identify the subsets with the vertices of the hypercube, which is quite classical.",
                    "label": 0
                },
                {
                    "sent": "Give me a subset that indicator vector and you obtain a vertex of the hypercube.",
                    "label": 0
                },
                {
                    "sent": "So now how do you define this robust extension which is often called the Shockey Shockey Integral at address OK?",
                    "label": 0
                },
                {
                    "sent": "Which is if you take any set function which may not be submodular and you take any value in RP, you order this couple its components and then you define this subject which is a linear function of W. To me the expression is not so important.",
                    "label": 0
                },
                {
                    "sent": "What is important is that if you take W being an indicator vector then F of W is equal.",
                    "label": 0
                },
                {
                    "sent": "Is that closer to the value of capital F at a?",
                    "label": 0
                },
                {
                    "sent": "So this is clearly an extension from.",
                    "label": 0
                },
                {
                    "sent": "That's set to that set.",
                    "label": 0
                },
                {
                    "sent": "OK, so if he's always piecewise affine and positively homogeneous, and the key the killing between similarity and connectivity is through this very simple statement that your function you're set function is submodular if and only if it's low fat extension is convex.",
                    "label": 0
                },
                {
                    "sent": "So this is really the reason why you can always see similarity to the eyes of convexity.",
                    "label": 0
                },
                {
                    "sent": "One other aspect, which I will not use so much.",
                    "label": 0
                },
                {
                    "sent": "In this talk, maybe at the end is that if you want to minimize the similar function which is equivalent to minimize F on the vertices on the hypercube, then it is equivalent.",
                    "label": 0
                },
                {
                    "sent": "To minimize this on the entire hypercube.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "One reason why minimizing similar function is a polynomial time.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So associated to any piecewise affine to any piecewise affine convex function, you have always as a Poly Hedren or polytope hiding around as which would be so that the convex function is a support function of the convex set and those convex sets are the so called submitted after the run as a base polytron.",
                    "label": 0
                },
                {
                    "sent": "So P of F is a set of elements in RP which satisfies is set of linear ecology.",
                    "label": 0
                },
                {
                    "sent": "OK, that S of a.",
                    "label": 0
                },
                {
                    "sent": "So here we use this a lot in this talk as of X is a vector S of SIS can be used as a modular set function where SH uses some of the elements of X which are correspond to into elements of a OK, so some of K in a of ask.",
                    "label": 0
                },
                {
                    "sent": "So go F is something like that and if you play around with similar functions you will end up doing a lot of those of the things, which is a similar polyhedron, and the baseball legend, which is simply appear of intersected with the hyperplane so PEO S over there and be be of FBF is here.",
                    "label": 0
                },
                {
                    "sent": "And then three you get something like that.",
                    "label": 0
                },
                {
                    "sent": "So here all these supporting hyperplanes are always with a fixed direction.",
                    "label": 0
                },
                {
                    "sent": "OK, because they correspond to victorious ones and zero so most resorts.",
                    "label": 0
                },
                {
                    "sent": "Instead of functions can be seen through those polyhedra.",
                    "label": 0
                },
                {
                    "sent": "So what do?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Link the former link with the lowest extension and this dates back to Jack Edmonds in the 70s.",
                    "label": 0
                },
                {
                    "sent": "Is that the lowest extension?",
                    "label": 0
                },
                {
                    "sent": "Happens to be the support function of the of the polyhedron, so by the support function is a maximum of linear functions.",
                    "label": 0
                },
                {
                    "sent": "And as you remember from the expression of F, if you want to maximize a linear function on the baseboard run, then this can be done in closed form for the so called greedy algorithm.",
                    "label": 0
                },
                {
                    "sent": "So if you take.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All of those complex Polly Polly Polly.",
                    "label": 0
                },
                {
                    "sent": "This is a polytope is a polytope.",
                    "label": 0
                },
                {
                    "sent": "Imagine this in high dimensions, so there's a lot of faces, a lot of history points, but you can minimize.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can maximize or minimize linear functions in the enclosed form.",
                    "label": 0
                },
                {
                    "sent": "OK, so the so called Red Dagger rhythm, and essentially most of the operations that people do in similar.",
                    "label": 0
                },
                {
                    "sent": "Similarly Edra are based on the fact that you can maximize easily linear functions if you want to do more operations like projection, then you have also algorithms and you can see my tutorial to know more about that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how can you optimize some of the functions so you want to minimize the set function so it's been known that it's a polynomial time and the currently best known algorithm with complexity bound is appear.",
                    "label": 1
                },
                {
                    "sent": "Exact algorithm is a pita.",
                    "label": 0
                },
                {
                    "sent": "The fix and those are the names of the classical name associated with those types of algorithm.",
                    "label": 0
                },
                {
                    "sent": "There's another algorithm, cause a minimum nonpoint algorithm, which is somewhat efficient in practice but with low complexity bound.",
                    "label": 0
                },
                {
                    "sent": "I will come back to that later and of course at this weather, especially the topic of the first invited talk when you have a spatial subdural function like occur tour flow, you have fast dedicated algorithm.",
                    "label": 1
                },
                {
                    "sent": "Early days, and this is quite a negative I of research with several papers at these nips and last names.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the last last point about similar function is the link the last point I want to make is a link between convex optimization an similar optimization.",
                    "label": 0
                },
                {
                    "sent": "So let's assume that you have a set of functions like a where case from one 2P which are strictly convex.",
                    "label": 1
                },
                {
                    "sent": "So if you want, you can think of those as being the quadratic function TF WKWK square.",
                    "label": 0
                },
                {
                    "sent": "Then you can look at problems OK which are regularised by several function.",
                    "label": 0
                },
                {
                    "sent": "OK so essentially.",
                    "label": 0
                },
                {
                    "sent": "In my regularised in particular risk minimization framework, my normal will be linked to the summit to the lowest extension, so those will appear quite quite often.",
                    "label": 0
                },
                {
                    "sent": "So if you want to minimize that function which is convex because it is convex and convex, this has a unique minimum.",
                    "label": 0
                },
                {
                    "sent": "Because of symmetry, convexity, and it turns out that you can find just find the dual of that problem, and he's simply a maximization problem on the on.",
                    "label": 0
                },
                {
                    "sent": "The similar on the base polyhedron, so this is equivalent to that.",
                    "label": 1
                },
                {
                    "sent": "So this is always.",
                    "label": 0
                },
                {
                    "sent": "This is a very common.",
                    "label": 1
                },
                {
                    "sent": "For example if you want to do the minimum to when you put it like that, the elbow norm, it is equivalent to project on the L Infinity norm on the Boulevard infinite number.",
                    "label": 0
                },
                {
                    "sent": "So this is exactly the same same concept.",
                    "label": 0
                },
                {
                    "sent": "So this is a one line of code.",
                    "label": 0
                },
                {
                    "sent": "I think the key the key relationship outline by shovel and Abu and maybe earlier.",
                    "label": 0
                },
                {
                    "sent": "I don't know is that if you take the solution and I got the unique if you take W start being the unique solution of that problem, then it turns out that for all possible Alpha in R, then through the level sets of that W star you have access to the minimal and maximal minimizers of that similar function.",
                    "label": 0
                },
                {
                    "sent": "So F is similar, and this is simply a modular function.",
                    "label": 0
                },
                {
                    "sent": "So here if you know how to solve that then you know how to solve that OK by simply taking the level sets.",
                    "label": 0
                },
                {
                    "sent": "In particular if you make sure that.",
                    "label": 0
                },
                {
                    "sent": "Fiquet prime of 0 is zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so this this so far 5 zeros disappears.",
                    "label": 1
                },
                {
                    "sent": "Then you can have access to the minimum of F. On the contrary, you can, if you know how to solve that for all possible Alpha, you get all level sets of the produce.",
                    "label": 0
                },
                {
                    "sent": "You can go back to the solution of that problem.",
                    "label": 1
                },
                {
                    "sent": "So essentially convex optimization problems are that for my equivalent to sequence of similar function minimization.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is summarized in the.",
                    "label": 0
                },
                {
                    "sent": "In this slide.",
                    "label": 0
                },
                {
                    "sent": "So if you know if you start to solve the convex optimization problem, OK to solve the similar function minimization problem, you just threshold your solution at zero OK. And of course this is a well known and by especially when you take this aquatic function and in that case if you if you look.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a problem, this becomes a mini.",
                    "label": 0
                },
                {
                    "sent": "You want to minimize the squared to dominate S4 as belonging to the base polyhedron.",
                    "label": 0
                },
                {
                    "sent": "So essentially you want to find the orthogonal projection on zero of zero on the base polyhedron.",
                    "label": 0
                },
                {
                    "sent": "So this was known.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This was known by for awhile and this leads to an approach for similar function minimization.",
                    "label": 0
                },
                {
                    "sent": "OK, so you minimize that problem.",
                    "label": 0
                },
                {
                    "sent": "OK an then you obtain the minimum of that one.",
                    "label": 0
                },
                {
                    "sent": "But you can go in the in the other around.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you know how to solve to minimize your similar function, maybe plus certain modular function, then it's enough for minimal minimizing those problems.",
                    "label": 0
                },
                {
                    "sent": "So there is a very nice.",
                    "label": 0
                },
                {
                    "sent": "Divide and conquer strategy outside by convert whereby at most P at most AP.",
                    "label": 0
                },
                {
                    "sent": "Similar function minimization you get the exact minimum of this guy, so of course this is only efficient when you know how to stop those guys efficiently.",
                    "label": 0
                },
                {
                    "sent": "So in particular for cuts in flows.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For now, I know that I presented a bit in detail some other functions I go to present the how they can be linked through sparsity.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to fix notation so when you supervise machine learning when you have data XII, so I would assume XI is in RP, then you put them in a response circuit Y Ana design metrics Capital X an momeni.",
                    "label": 0
                },
                {
                    "sent": "Mini Worth will try to minimize the secularized empirical risk.",
                    "label": 1
                },
                {
                    "sent": "We have a data fitting term again plus regularizer, and I will not do this as follows and you are people use normal mega to promote sparsity for several reasons.",
                    "label": 1
                },
                {
                    "sent": "So many examples of the manager is the L1 norm to create some zeros, and this is called basis pursuit.",
                    "label": 0
                },
                {
                    "sent": "Sticks, and often they use this as a proxy for interpretability.",
                    "label": 1
                },
                {
                    "sent": "OK, if you have only a few a few variables which are non zero, you can look at the associated features to see which feature is important for your problem.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So he's going to be using unsupervised learning, so if you're given a set of signals, YKC notes in RN, then the dictionary learning will try to essentially find a set of terms X12 XP so that you can express Y as a linear combination of your of your items.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is if you if you put no constraints or the X or W this is PCA, but as soon as you put a space constraint on Omega on WRX you.",
                    "label": 0
                },
                {
                    "sent": "Something different, so if you put a space constraints on W you want to find a term so that you can express each of your signals with only a few basis vectors.",
                    "label": 0
                },
                {
                    "sent": "And if you put a sparsity constraint on your on your dictionary, you want to have a lot of zeros in your new items.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why, why would you want to do sparsity?",
                    "label": 0
                },
                {
                    "sent": "So you have several reasons, and for me the key one is interpretability.",
                    "label": 0
                },
                {
                    "sent": "OK, so you it's not because you have a lot of zeros that you can really interpret the result of algorithm, and I will briefly briefly present 22 instances which I've worked with.",
                    "label": 0
                },
                {
                    "sent": "American limousine scheme.",
                    "label": 0
                },
                {
                    "sent": "The first one is when you do additional learning of PCA.",
                    "label": 0
                },
                {
                    "sent": "You might want to have dictionary elements which are so much structured so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's give an example.",
                    "label": 0
                },
                {
                    "sent": "When you apply these two faces, OK, let's imagine you have 1000 face faces and you want to find basis vectors so that you can express each of your face as a sparse combination.",
                    "label": 0
                },
                {
                    "sent": "Then, if you want to, if you impose sparsity.",
                    "label": 0
                },
                {
                    "sent": "So this is just suspense PCA, I just put an L1 norm on my dictionary elements, then I'll 10 basis vectors.",
                    "label": 0
                },
                {
                    "sent": "Which of that form where?",
                    "label": 0
                },
                {
                    "sent": "Here Gray is zero and black or white are positive and negative values.",
                    "label": 0
                },
                {
                    "sent": "So here to help.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I put the zero values in green so you can see here you have a lot of a lot of grain.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lot of zeros, but this does not mean you can really interpret the results because the patterns are often scattered.",
                    "label": 0
                },
                {
                    "sent": "The goal of this of this talk is to show that by simple.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Changing the norm so you replace the element.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Come by something which is structured.",
                    "label": 0
                },
                {
                    "sent": "Then you can have something like this where here we have imposed the constraint that dictionary.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once I've have nonzero patterns which are convex or diamond shaped patterns in the in the plane, so here.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So goal is not to discover so much resource, but just to convey the idea that but just changing your Norma can get results which are a lot more interpretable.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This can be done.",
                    "label": 0
                },
                {
                    "sent": "This can be known then as well if you want to put structure not on the dictionary elements but on the Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use.",
                    "label": 0
                },
                {
                    "sent": "OK, so he's a typical reader that we that we have that we learn a dictionary which means which can be embedded into a tree, so I won't give the details of that application.",
                    "label": 0
                },
                {
                    "sent": "But essentially again we say OK, sorry bout to impose a tree structure, so carbonated structure and with this we can do by using sparsity.",
                    "label": 0
                },
                {
                    "sent": "Using norms based on similar functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What will be the key tool to do this will be to look at the convex envelopes of potential functions.",
                    "label": 0
                },
                {
                    "sent": "So what is the convex envelope?",
                    "label": 0
                },
                {
                    "sent": "Let me just use a classical album non example.",
                    "label": 0
                },
                {
                    "sent": "OK so if you tap for if you start from any W like this and you.",
                    "label": 0
                },
                {
                    "sent": "And you define IT support as a set of non zero elements of the value that the so called LO penalty is simply the cardinality of the support.",
                    "label": 0
                },
                {
                    "sent": "So this is a highly nonconvex function is not even continuous and you can see the L1 norm as a convex envelope, which is the tightest convex Lebanon that function on the set minus one one.",
                    "label": 0
                },
                {
                    "sent": "So just to give this distraction in 1D.",
                    "label": 0
                },
                {
                    "sent": "This is a in Java minus one one in blue.",
                    "label": 0
                },
                {
                    "sent": "You have the LC open energy which is 080 and one everywhere everywhere else, and the goal is to find a convex function which is below a nice tight as possible.",
                    "label": 0
                },
                {
                    "sent": "So if you think of it you it must be the one norm.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a traditional link between album Norman, The LO penalty.",
                    "label": 0
                },
                {
                    "sent": "So the goal will be to change cardinality by a set function to encode your prior about your problem.",
                    "label": 0
                },
                {
                    "sent": "OK so this is what we're going to do.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you take any set function OK, which are considered nondecreasing.",
                    "label": 0
                },
                {
                    "sent": "I want to penalize more bigger subsets.",
                    "label": 0
                },
                {
                    "sent": "OK, take a non decreasing function and you should see this function as a prior knowledge on supports.",
                    "label": 1
                },
                {
                    "sent": "OK so some some subsets.",
                    "label": 0
                },
                {
                    "sent": "I like a lot so in the context of the trees the ones which look like subtrees.",
                    "label": 0
                },
                {
                    "sent": "I put a small value there, the ones for which the ones who don't look like subtrees put put high value.",
                    "label": 0
                },
                {
                    "sent": "OK so it's.",
                    "label": 0
                },
                {
                    "sent": "Are you on your on your on your supports?",
                    "label": 0
                },
                {
                    "sent": "An the goal will be to look at that function where you take your set function F applied to the support so it is still nonconvex problem.",
                    "label": 0
                },
                {
                    "sent": "OK, and we will try to follow the same route As for the end.",
                    "label": 0
                },
                {
                    "sent": "Will not try to find the Contacts on the loop.",
                    "label": 1
                },
                {
                    "sent": "And as we see, this will be simple when F is, F is a sub nom.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is a first position relating like similarity to sparsity, that function on the supports OK. Then if you right click on the Infinity ball like for the L1 norm then the context of the lab.",
                    "label": 0
                },
                {
                    "sent": "So the titles convex Robin is exactly the low back extension taking taken at the absolute values of W. OK, so this is just as a sanity check if you take the F being the cardinality then the low back extension is a sum and you get back the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is simply a way to extend the classical relationship between L0 and L1.",
                    "label": 0
                },
                {
                    "sent": "So let's look at the shape.",
                    "label": 0
                },
                {
                    "sent": "So here and also you always obtain a norm.",
                    "label": 0
                },
                {
                    "sent": "OK, that is.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Set up so usually when you have norm and it is often the case in most talks about sparsity, you show you show the balls you throw the balls of that Norman.",
                    "label": 0
                },
                {
                    "sent": "This is what I show here where is in 2D OK so this is a generic generic unit bowl for the for the norm which which I have defined.",
                    "label": 0
                },
                {
                    "sent": "And of course since F the robot extension is a piecewise piecewise now then my ball has to be either has to be a polytope and depending on the values of the subterra functions some extreme points will go come and go and somebody.",
                    "label": 0
                },
                {
                    "sent": "Appearance of European OK, so here this is a generic case where all these three points are present, but you also have like the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "OK, the Infinity Norm is a subcase an another number here.",
                    "label": 0
                },
                {
                    "sent": "So of course when you do a convex relaxation like that the goal what you want is to that you preserve the properties of the original function.",
                    "label": 0
                },
                {
                    "sent": "OK, because you make your problem convex that you have solved anything you want to relate one with another.",
                    "label": 0
                },
                {
                    "sent": "One example of a good good property is the.",
                    "label": 0
                },
                {
                    "sent": "Sterile sets, so it's set isn't really stable.",
                    "label": 0
                },
                {
                    "sent": "Sets are the ones which can be obtained when you when you those when you use those types of higher on support.",
                    "label": 1
                },
                {
                    "sent": "So you'll stable that only if if you increase the set OK then you have to strictly augment or similar function.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you're not stable you can always add a bit more and not increase your cost, so you will always at element.",
                    "label": 0
                },
                {
                    "sent": "So essentially if you want to use that function as a prior, you would always have stable set as optimal values.",
                    "label": 0
                },
                {
                    "sent": "Anyway, you can show that for the convex convex approximation this is true as well.",
                    "label": 0
                },
                {
                    "sent": "OK, that with probability one only stable sets possible when you are when you optimize with that number.",
                    "label": 1
                },
                {
                    "sent": "So simple example where you can link the properties of the original function to the properties of the convex laxation.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you can look in 3D to look at several at several other unit ball.",
                    "label": 0
                },
                {
                    "sent": "OK, this is beautiful, but not so nice.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's look at some examples.",
                    "label": 0
                },
                {
                    "sent": "OK, so here what is nice about having a general framework is that you can look at existing norms an given you view into those existing norms and vice versa.",
                    "label": 0
                },
                {
                    "sent": "Take your list of subdural functions and try to see if it corresponds to the new norm.",
                    "label": 1
                },
                {
                    "sent": "So let's do the first direction on the on the right, and now you can consider all the norms which are based on so cool like everyone else.",
                    "label": 0
                },
                {
                    "sent": "Unity norms where you some.",
                    "label": 0
                },
                {
                    "sent": "About a set of groups.",
                    "label": 0
                },
                {
                    "sent": "OK so Kelly graphic Jesus set a subset, a set of subsets and you penalize the sermon.",
                    "label": 0
                },
                {
                    "sent": "Infinity Norm of the predictor strictly to that subset.",
                    "label": 0
                },
                {
                    "sent": "OK, so he sees that we use a lot in the last in the last few years and it is known to induce notion of group sitting, which is the following at the optimum.",
                    "label": 0
                },
                {
                    "sent": "So when your eyes by that Dom, the W that you obtain will be so that such that many of the WGS.",
                    "label": 0
                },
                {
                    "sent": "Will be equal to zero.",
                    "label": 1
                },
                {
                    "sent": "OK, so some of the groups that you have started from WG be equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So this means that essentially the supports the zeros which is complementary.",
                    "label": 0
                },
                {
                    "sent": "The complement of the support would be a union of groups.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the things which we have worked on a lot and it turns out that this corresponds to a certain similar function, essentially the one they want.",
                    "label": 0
                },
                {
                    "sent": "The world which we think of.",
                    "label": 1
                },
                {
                    "sent": "If there is simply the cardinality of the group which.",
                    "label": 0
                },
                {
                    "sent": "Intersect you or yourself a OK.",
                    "label": 0
                },
                {
                    "sent": "So the context of a partition in Jesus partition, you just count the number of active element in the partitioner.",
                    "label": 0
                },
                {
                    "sent": "But of course if G is not the partition anymore, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a bit more complicated and I will show examples in a few slides.",
                    "label": 0
                },
                {
                    "sent": "So what is important?",
                    "label": 0
                },
                {
                    "sent": "What is nice about like going from here to here is a way to interpret the properties of that norm, not only through the allow sparsity patterns, which was what we will do in previous work but through full prior.",
                    "label": 1
                },
                {
                    "sent": "Onset functions in the paper we have examples.",
                    "label": 0
                },
                {
                    "sent": "On the way this might be important.",
                    "label": 0
                },
                {
                    "sent": "OK, let's give a very simp.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example OK, if you take Viagra switch organizing the sequence then you on the left.",
                    "label": 0
                },
                {
                    "sent": "If you take those groups OK or the left introvert and order right intervals then if you take a union of those of any any number of those groups you get something which is shaded here a bit on the right and a bit on the left.",
                    "label": 0
                },
                {
                    "sent": "So this will be the zero of your of your predictor.",
                    "label": 0
                },
                {
                    "sent": "So the non zero would be what is in the middle.",
                    "label": 0
                },
                {
                    "sent": "OK so here it's actually you will use that to only to enforce the constraint that you want.",
                    "label": 0
                },
                {
                    "sent": "Goose pattern, so you want W the nonzeros of W to be to be continuous, and this corresponds to the set function, which is the range of a.",
                    "label": 1
                },
                {
                    "sent": "So essentially when you want to impose a prior that you want to be contiguous, it mean that what counts for your predictor is just the range and not not anything more.",
                    "label": 0
                },
                {
                    "sent": "And if you want to check the stable sets of that of the range function, essentially are essentially the potentially the.",
                    "label": 0
                },
                {
                    "sent": "Doubles",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have other examples of groups, so you can do so, like rectangles an now you get the notion of perimeter and if you add more if you add more more angles and you can go to this like diamond shaped patterns which I've shown for the sparse PCA case.",
                    "label": 0
                },
                {
                    "sent": "If you have your keys like that then you can define a simple similar function which is which will given any set count your total number of ancestors.",
                    "label": 0
                },
                {
                    "sent": "So if you use that as a prior then you will try to enforce to select.",
                    "label": 0
                },
                {
                    "sent": "Viables only after you have selected all of your ancestors.",
                    "label": 0
                },
                {
                    "sent": "OK, so essentially you can go and look at all all set functions based on covers and you can derive the associated similar function in normal.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this was in One Direction.",
                    "label": 0
                },
                {
                    "sent": "You go from Norm to know Norm to submit a function.",
                    "label": 0
                },
                {
                    "sent": "But you can do the other way around and this don't want to kiss too much of that.",
                    "label": 0
                },
                {
                    "sent": "I will just simply mention two types of two possibilities, functions of cardinality.",
                    "label": 0
                },
                {
                    "sent": "Then you can show in that case at the last extension depends on the order statistics and also functions of the values of a submatrix.",
                    "label": 1
                },
                {
                    "sent": "So typically if you take the entropy of the gas on doctor then the entropy is a log cabin and and if you take a symmetrix.",
                    "label": 1
                },
                {
                    "sent": "And take the log data that so symmetrix index by some of the rows and columns of the matrix, you obtain a similar functions and these can be used as non factorial prios or supervised learning if you want to know more about that you can look at the.",
                    "label": 0
                },
                {
                    "sent": "At my newspaper of last year.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this was a set of examples and the good thing is that for all of those examples, you can use a unique algorithm or some Unix algorithm, and this is what I will try to describe now.",
                    "label": 0
                },
                {
                    "sent": "So first the first approach, which I could think of is that OK, so my norms.",
                    "label": 0
                },
                {
                    "sent": "Piecewise defined functions.",
                    "label": 0
                },
                {
                    "sent": "It's a big LP, so why not use an LP and LP 2 bugs in our programming toolbox?",
                    "label": 0
                },
                {
                    "sent": "The problem that you have many, many faces and extra points, so this is clearly inefficient.",
                    "label": 1
                },
                {
                    "sent": "Then you could say, oh, because my love at extension is a maximum of inner functions which I can obtain through the greedy algorithm.",
                    "label": 0
                },
                {
                    "sent": "I have a subgradient, OK. As a surgeon, so you could take your get your loss +2 regularizer so you can take the subject of your actual regularizer.",
                    "label": 1
                },
                {
                    "sent": "But as you will see, this is too slow because it's known to be to be slow.",
                    "label": 0
                },
                {
                    "sent": "Here we can leverage recent work in optimization, which is so called proxamol gradient methods, which will be adapted to functions of that form.",
                    "label": 0
                },
                {
                    "sent": "OK, so typically machine learning we have alerts between Y, which is your output and WX which is your input.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "If you assume that dollar is differentiable, typically squares or logistic, then this is differentiable.",
                    "label": 0
                },
                {
                    "sent": "This is not differentiable.",
                    "label": 0
                },
                {
                    "sent": "OK, it's really not differentiable, and if this is a hard one to solve because it is non smooth.",
                    "label": 0
                },
                {
                    "sent": "But if you can solve those problem easily, OK, so it's really if you can replace the loss by isotropic quadratic function like that.",
                    "label": 0
                },
                {
                    "sent": "If this is easy, then by a sequence of many of these we can find the optimum of that and essentially it will converge as fast as.",
                    "label": 1
                },
                {
                    "sent": "If they will not be as fast as if it was totally totally Smith, OK so proximal methods are away to do nonsmooth optimization, at the cost of smooth optimization, and this is only possible if you can solve that efficiently.",
                    "label": 0
                },
                {
                    "sent": "So if you want to know more about this, you can see we have a recent paper with Jeanette Rodolphe.",
                    "label": 0
                },
                {
                    "sent": "Key in the financial trends in machine learning which will expand this.",
                    "label": 0
                },
                {
                    "sent": "How this works.",
                    "label": 0
                },
                {
                    "sent": "So the question is how can you solve that efficiently?",
                    "label": 1
                },
                {
                    "sent": "OK so here usually you should recognize essentially the.",
                    "label": 0
                },
                {
                    "sent": "Laconia, sensually, the supercable optimization problem about this happening before OK.",
                    "label": 0
                },
                {
                    "sent": "So these proxamol problem which we need to start over and over is exactly equivalent to this separable optimization.",
                    "label": 0
                },
                {
                    "sent": "So it can be solved through a sequence of SABMiller function minimization.",
                    "label": 0
                },
                {
                    "sent": "So when you do that, of course we can use a nice divide and conquer strategy of a ground web, but this is only possible if you have curves and flows.",
                    "label": 0
                },
                {
                    "sent": "Namely, if you have a similar functions for which you can minimize which you can minimize the very efficiently and we have done that in the context of the norms with overlapping groups or in the generate case you can use the minimum number point algorithm which I want describe number, but I will bit later.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the type of the type of behavior that we always obtain but uses those type of algorithms.",
                    "label": 0
                },
                {
                    "sent": "So we have taken a simple supervised learning problem.",
                    "label": 0
                },
                {
                    "sent": "OK, where are with the 1000 variables and with a very simple similar function.",
                    "label": 0
                },
                {
                    "sent": "The goal here is to compare the gains that you have by using by really using the inner compatible structure of jugular eyes are OK, so if you don't use anything you just go put 1st century subgradient, then you're covered.",
                    "label": 0
                },
                {
                    "sent": "Converge, but very slowly, but as soon as you use the proximal operator, which requires to look at the discrete structures, the problem, then you go a lot faster if you use a non accelerated version an go even faster if you use the X rated version.",
                    "label": 0
                },
                {
                    "sent": "So just a simple example where you can use this kid optimization to speed up convex optimization.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For there are many extensions which you can consider.",
                    "label": 0
                },
                {
                    "sent": "I won't talk about it first, have not talked a lot about statistics, but we have some unique algorithms for the for optimization.",
                    "label": 0
                },
                {
                    "sent": "We also have some unified analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, especially if you use a similar function that you do what you expect, like recovering the sparsity pattern of your problem.",
                    "label": 0
                },
                {
                    "sent": "We can extend this to a symmetric similar functions where this was work at this conference where you can instead of trying to put pressure on the 0 nonzeros of your problem, you put a prior on all level sets.",
                    "label": 0
                },
                {
                    "sent": "Then we have two extensions where I think to me are very important because the sad story about these this current framework is the following.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you take the L Infinity Norm, OK guys, sponse to that subclass function.",
                    "label": 0
                },
                {
                    "sent": "Essentially you put no prior no prior on the zeros.",
                    "label": 0
                },
                {
                    "sent": "OK, you allow all possible patterns.",
                    "label": 1
                },
                {
                    "sent": "OK, you put up higher, but so the problem that you still have a lot of corners in your in your ball.",
                    "label": 0
                },
                {
                    "sent": "OK so you have.",
                    "label": 0
                },
                {
                    "sent": "Garner switch we don't really want so at the end, and those colors corresponds to places where your vector as equal equal magnitudes.",
                    "label": 0
                },
                {
                    "sent": "OK, so when you use any norm, you always extract higher, which you might want or might not want.",
                    "label": 0
                },
                {
                    "sent": "Don't want it and what we have done.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is a German.",
                    "label": 0
                },
                {
                    "sent": "You have a poster somewhere over there is that you can replace this an Infinity norm by L2 norms or more generally LP norms, to remove the artifact associated with those norms.",
                    "label": 0
                },
                {
                    "sent": "Also also at the same way this same posterior, you can generalize this setup to other set functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we have focused.",
                    "label": 0
                },
                {
                    "sent": "I focused on similar function.",
                    "label": 0
                },
                {
                    "sent": "You can extend that to other set functions.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, it's not.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem of approximate subduction minimization.",
                    "label": 0
                },
                {
                    "sent": "Essentially, for most machine learning applications, you don't need to get the exact solution.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is a very common theme in optimization in machine learning, so the goal will be to find a solution which is approximate.",
                    "label": 0
                },
                {
                    "sent": "So without loss of generality, we will assume that the signal in terms of positive values and this is true.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a well known in similarity and nobody noticed square the square would be the essentially this quantity, which is essentially an upper bound on the right on the edges of the baseball.",
                    "label": 0
                },
                {
                    "sent": "Even so, since we have similar function OK, and I've told you that minimizing the similar function F on the hypercube, we lead to a minimum of the similar function.",
                    "label": 0
                },
                {
                    "sent": "You can think of the simplex algorithm to minimize.",
                    "label": 0
                },
                {
                    "sent": "Functions which is celebrating dissent.",
                    "label": 0
                },
                {
                    "sent": "So if you do sublimation dissent then after that iteration you could show that you get close to the optimum.",
                    "label": 0
                },
                {
                    "sent": "OK, in the sense that the distance to optimum is apondi by this quantity.",
                    "label": 0
                },
                {
                    "sent": "So for those who know the topic, this is simply directed gap between the primal and dual and dual problem and this is upper bounded by by this which is D that will sqrt 3 / sqrt T. OK so you can show that the.",
                    "label": 0
                },
                {
                    "sent": "When you apply this then you always get you convert the right wonders crowd FT.",
                    "label": 0
                },
                {
                    "sent": "So this is pretty slow.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see if we can improve if we can improve improve this.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we will try to improve that by looking at the link with quadratic functions, which is often used to minimize similar function.",
                    "label": 0
                },
                {
                    "sent": "So another goal would be to look at approximate quadratic optimization on the base polyhedron.",
                    "label": 1
                },
                {
                    "sent": "So the goal is to minimize this, and as I shown later, as I showed later, this is equivalent to maximizing this, so we want to find the minimum Elton on point in the best polyhedron.",
                    "label": 1
                },
                {
                    "sent": "And remember, we can only maximize enough functions in BF.",
                    "label": 1
                },
                {
                    "sent": "So here what we have, we have a convex optimization problem on the.",
                    "label": 0
                },
                {
                    "sent": "On the convex set on which can only maximize in our functions.",
                    "label": 1
                },
                {
                    "sent": "So you see the classical setup for so called quote algorithms and the subtle point is that you have two types of Frank Wolf algorithms, other ones which are dedicated to quartic functions.",
                    "label": 0
                },
                {
                    "sent": "I will try to be active set algorithm, so this is this is the usual minimum partner in them, so the way they work you do a sequence of maximization of functions.",
                    "label": 0
                },
                {
                    "sent": "Do summer fun projections, and then you iterate that over and over.",
                    "label": 0
                },
                {
                    "sent": "So what you can show that was appointed by the face of the polytope, there's a face you as soon as you optimize the optimal.",
                    "label": 1
                },
                {
                    "sent": "As soon as you identify the optimal phase of your polytope, then you converge and This is why it is.",
                    "label": 0
                },
                {
                    "sent": "It has finite convergence but has all active stagger them.",
                    "label": 0
                },
                {
                    "sent": "There is no complexity bound, but the second set of algorithms is so-called also called conditional greggery them while you also do a sequence of maximization of inner functions but.",
                    "label": 0
                },
                {
                    "sent": "A you simply do a very simple, very simple algorithm so.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at a simple example.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say you want to, so you want to be as close as you want to Project 0 on the on that pull it open, you start from that point, OK?",
                    "label": 0
                },
                {
                    "sent": "Then you look at the gradient.",
                    "label": 0
                },
                {
                    "sent": "OK, so you follow it.",
                    "label": 0
                },
                {
                    "sent": "Follow the gradient so you want to maximize linear function at the top and you get you get to there.",
                    "label": 0
                },
                {
                    "sent": "OK so and then you do a line search from where you started from to the vertex of that of the polytope.",
                    "label": 0
                },
                {
                    "sent": "OK, if your answers will get there, so you start again, you want to go in that direction.",
                    "label": 0
                },
                {
                    "sent": "OK, that direction, and if you do that then the proper the farthest point is this one.",
                    "label": 0
                },
                {
                    "sent": "OK, so you do align search between different, this one and you throw it over and over.",
                    "label": 0
                },
                {
                    "sent": "So then you see that this will never be.",
                    "label": 0
                },
                {
                    "sent": "This will not be phonetic convergence.",
                    "label": 0
                },
                {
                    "sent": "This will converge slowly to the optimum OK, but the good ASP.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, that it's good aspect.",
                    "label": 0
                },
                {
                    "sent": "Is that OK?",
                    "label": 0
                },
                {
                    "sent": "There's no overheads.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's no official projections to do, and it comes with an approximate optimality band similar to gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So let's look at this optimal.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bound.",
                    "label": 0
                },
                {
                    "sent": "So if you do the steps of conditional gradients with line search OK, then you will output.",
                    "label": 1
                },
                {
                    "sent": "So an element of the best polytron plus probably viable.",
                    "label": 0
                },
                {
                    "sent": "So here in All in all those results you always have abound plus certificate authority, which you can really satisfy that your optimal and you can show that your distance to optimum is upper bounded by something which is which decays as one of our team.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an improvement here that to minimize cratic functions.",
                    "label": 0
                },
                {
                    "sent": "Then it is a bit faster.",
                    "label": 0
                },
                {
                    "sent": "OK. Then I would imagine this quickly.",
                    "label": 0
                },
                {
                    "sent": "Then this you can improve results by by a lot by not taking the primary candidate which is given by the algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so given and this is based on the following, the following thing if F if F is a.",
                    "label": 0
                },
                {
                    "sent": "If you fix the order of the values of WF becomes linear.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now if you want to minimize those that function subject to fix ordering, it becomes a isotonic regression which can be solved North.",
                    "label": 0
                },
                {
                    "sent": "OK so this is a way to improve a lot the.",
                    "label": 0
                },
                {
                    "sent": "The primal value.",
                    "label": 1
                },
                {
                    "sent": "So now this does it lead to a better bound for similar function minimization and.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We see that this is not the case, so if you take any W which is approximately optimal for the quadratic function, then there is at least one level set of that of W, which is optimal OK, But you lose something in the process, you lose a square root in the process, and if you take the band I have for conditional gradient and you apply it, then what you get is exactly the one for dissent.",
                    "label": 0
                },
                {
                    "sent": "In a sense, this is a looks like a kind of useless.",
                    "label": 0
                },
                {
                    "sent": "Which is not really the case because with that algorithm and I won't give details, we can have a bit better behavior, which I won't detail, and in fact I've tried like four months to improve that down.",
                    "label": 0
                },
                {
                    "sent": "OK, and I catch the conclusion that it might be, it might be, it might be impossible.",
                    "label": 0
                },
                {
                    "sent": "In fact it is impossible.",
                    "label": 0
                },
                {
                    "sent": "So you have a lower complexity bound for some deflation minimizations if you're only based, so this is a new statement.",
                    "label": 1
                },
                {
                    "sent": "And if you want we can talk about like much, much more precise statements if you best.",
                    "label": 1
                },
                {
                    "sent": "Dora the sequence of Summit of Code is a greedy algorithm and you combine what you obtained in early.",
                    "label": 0
                },
                {
                    "sent": "Then there is no way to improve to improve the this this down.",
                    "label": 1
                },
                {
                    "sent": "OK, at least for the first 2 iteration, so this is a class.",
                    "label": 0
                },
                {
                    "sent": "This mimics the classical lower complexity bounds for convex optimization.",
                    "label": 0
                },
                {
                    "sent": "So essentially this band is not really a improbable.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we just keep.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At least keep the.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The experiments and conclude so he represented.",
                    "label": 0
                },
                {
                    "sent": "So some of the functions from the from a convex optimization perspective and with applications to status Bar City.",
                    "label": 1
                },
                {
                    "sent": "And I have also shown that seeing similar functions as being as being simply convex function bio linked with convex function can derive new algorithms for optimization.",
                    "label": 0
                },
                {
                    "sent": "They are nice set.",
                    "label": 0
                },
                {
                    "sent": "I think there's still a lot to do in that in that context on the link between convexity and similarity, in particular in terms of primal dual optimization.",
                    "label": 0
                },
                {
                    "sent": "So I have not shown the experiments, but when you minimize or users.",
                    "label": 0
                },
                {
                    "sent": "Those interactive algorithms we end up having.",
                    "label": 0
                },
                {
                    "sent": "I think you're very good primary candidate and a very bad dual candidate.",
                    "label": 0
                },
                {
                    "sent": "So essentially you have converse very quickly in the Bible, but you spend a lot of time improving your certificate of optimality, so you might hope to do a much better.",
                    "label": 0
                },
                {
                    "sent": "And finally, this is really.",
                    "label": 0
                },
                {
                    "sent": "Not clear what you could do, but all these similarities based on linear programming.",
                    "label": 1
                },
                {
                    "sent": "OK, so it would be nice to see if you can extend that to other type of context problems like semidefinite programming.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "But or I could neglected.",
                    "label": 0
                },
                {
                    "sent": "You should wait, or less assumptions.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "So if you want so, let's get technical if she might frustrate you, need assumptions.",
                    "label": 0
                },
                {
                    "sent": "If you want to slow right, you don't need assumptions.",
                    "label": 0
                },
                {
                    "sent": "So for all apps you can do it.",
                    "label": 0
                },
                {
                    "sent": "Under some of these assumptions.",
                    "label": 1
                },
                {
                    "sent": "So you can do comics minimization fossil, so you think that there is a counterpart for submodular.",
                    "label": 0
                },
                {
                    "sent": "Good question, but I have no clue.",
                    "label": 0
                },
                {
                    "sent": "No, it is not clear that the.",
                    "label": 0
                },
                {
                    "sent": "So for example, if incapacitation, when you have strong convexity, things are easier.",
                    "label": 0
                },
                {
                    "sent": "So I've been looking for the last few months at as equivalent notion for similar functions.",
                    "label": 0
                },
                {
                    "sent": "So what makes the problem easier?",
                    "label": 0
                },
                {
                    "sent": "And it's not totally clear.",
                    "label": 0
                },
                {
                    "sent": "So in in your long right you have some comparisons of the North and some of this.",
                    "label": 0
                },
                {
                    "sent": "Is there anything since then?",
                    "label": 0
                },
                {
                    "sent": "I mean any updated results that you skipped over here?",
                    "label": 0
                },
                {
                    "sent": "Or is that no no.",
                    "label": 0
                },
                {
                    "sent": "I haven't tried your submitted a function in which I plan to do.",
                    "label": 0
                },
                {
                    "sent": "I guess it's your question.",
                    "label": 0
                },
                {
                    "sent": "Any other any other known to have tried to benchmark so critical benchmarks?",
                    "label": 0
                },
                {
                    "sent": "But I think those benchmarks which is a bit sad or min cut problems, so I think if you can derive some benchmarks which are not making problems that would be that would be nice because we have been killed you should do something else anyway so.",
                    "label": 0
                },
                {
                    "sent": "You discussed approximating submodular minimization.",
                    "label": 0
                },
                {
                    "sent": "Question is.",
                    "label": 0
                },
                {
                    "sent": "Your continuous.",
                    "label": 0
                },
                {
                    "sent": "Message.",
                    "label": 0
                },
                {
                    "sent": "Add anything to actual submodular minimization, but if your goal is just to be approximate, if you just want to proximate minimizer, it's a lot faster.",
                    "label": 0
                },
                {
                    "sent": "Oh no, because then what that case you need the epsilon so the year the gap to be very small in the sense that has to be the minimal value.",
                    "label": 0
                },
                {
                    "sent": "The minimal in the minimal nonzero increase of between two values that you submitted function.",
                    "label": 0
                },
                {
                    "sent": "And this might be a bit hard small.",
                    "label": 0
                },
                {
                    "sent": "OK, so if your goal is to be exact, this will not help.",
                    "label": 0
                },
                {
                    "sent": "You can find at worst cases of which we never help.",
                    "label": 0
                },
                {
                    "sent": "So you would agree that.",
                    "label": 0
                },
                {
                    "sent": "Checking real value of extension of the.",
                    "label": 0
                },
                {
                    "sent": "Search functions.",
                    "label": 0
                },
                {
                    "sent": "It's not like they can contribute anything to exact actual submodular memorization for the exact algorithm would be, I would say so as well.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "There only provides a way.",
                    "label": 0
                },
                {
                    "sent": "Real extensions only provide a way to.",
                    "label": 0
                },
                {
                    "sent": "Approximant.",
                    "label": 0
                },
                {
                    "sent": "I think this is true.",
                    "label": 0
                }
            ]
        }
    }
}