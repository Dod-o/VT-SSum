{
    "id": "gm2n3ads65z4mpeazonddti7jja5s3is",
    "title": "Efficient max-margin Markov learning via conditional gradient and probabilistic inference",
    "info": {
        "author": [
            "Juho Rousu, Department of Computer Science, University of Helsinki"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/oh06_rousu_emmml/",
    "segmentation": [
        [
            "Spell into multi task learning so I'm kind of continuing their story.",
            "Usman started the morning so structured output via smoke Mountain Markov sense.",
            "OK so this should work with Greg Sanders.",
            "Son Rd said Morgan and John show Taylor.",
            "OK, so this is privileged domain of interest, so we have.",
            "Structured"
        ],
        [
            "Multi Label classification problem where we have data from domain where vertis and.",
            "Texas any sort of object objects and.",
            "Voice come from some some.",
            "These are vectors where the components are finite sets, so typically we will have kind of binary vector.",
            "This multi label with a binary vector but but it can be something else as well.",
            "So under individual components are called micro labels, we make this assumption that.",
            "Small labels have this kind of dependency structure that takes the form of a hyper graph, so this is kind of putting the thing into kind of general framework so.",
            "To be assumed that there is there are kind of hyper hyper edges so.",
            "So subset of nodes that have potentially some statistical dependency.",
            "And we want to build models that predict the multi label labels given to given to object.",
            "So this is overall.",
            "Goal so OK. Why this hyper craft structure?",
            "I guess this is just the reason this is nicely generalizes for many of the applications we might be interested.",
            "So starting from sequences."
        ],
        [
            "Going through hierarchies to general crafts like decks, so the only thing that changes is the how you define your hyper edges.",
            "So that's the kind of.",
            "Motivation of describing things in this way.",
            "Um?",
            "Right, so OK, so example of."
        ],
        [
            "This kind of classification problem is this hierarchical multi label classification that we already heard about in a couple of talks.",
            "Modify cloudy argenti LAN sunder as well.",
            "So he had to hear to.",
            "Task is given a document, so some text documents like this is a news article from BBC.",
            "Mere sight.",
            "The goal is to classified document into according to the taxonomy hierarchy and so you can allow this kind of multiple classifications that document talks about different topics.",
            "So you have this kind of.",
            "Possilpark multiple processor part of classification.",
            "So everything that is on green is part of the label on everything.",
            "That is why it is not not part of classification.",
            "So this is 1 example and.",
            "If it's."
        ],
        [
            "So kind of more general problem.",
            "You can have your kind of.",
            "Classification structure maybe a general crop like a like a dog.",
            "So this is gene auto, part of Gene Ontology where you're classifying jeans into some kind of.",
            "According to some kind of concept concept.",
            "Dag and so so if you have a gene sequence, so you may have, let's say DNA ligation might be some kind of function for the for the gene, and so this means that this kind of.",
            "Subcraft above this.",
            "This note would be part of the part of the label for four.",
            "For that particular gene.",
            "OK, so OK of course there are some many other other kind of.",
            "Applications where you can have this kind of kind of structured.",
            "A classification formulation.",
            "OK, so.",
            "Classification model that we that we use is based on the conditional random field field idea where we have."
        ],
        [
            "Have potential assigned to the High British after after.",
            "Craft.",
            "And the potentials are there is there is this exponential exponential so.",
            "Potential function.",
            "And here in the exponent you have this linear.",
            "Linear inner product between weight vector and joint feature map.",
            "So there is a joint feature map for documentation and.",
            "Any particular labeling of the hyperedge so you have a.",
            "This feature map is different for each.",
            "Pair.",
            "OK, and what you want to learn.",
            "You want to learn the weight vector in some sense, so typically in conditional random field fancy would try to learn to maximum likelihood estimate.",
            "But like I indicated, that's not what we're going to going to do here.",
            "OK, so this is.",
            "This is the.",
            "Model.",
            "So instead of trying to learn maximum likelihood para meters."
        ],
        [
            "We want to find this much marching solution where the goal is to separate correct multilabel.",
            "For the kind of defeats are joint feature map or image in the feature space of the correct multi label.",
            "So let's say this part from any incorrect one.",
            "And then try to push this margin as big as possible.",
            "And like Yasmin already indicated, there are many, many ways you can kind of penalize this incorrect.",
            "Prediction, so in terms of how far you want to push and Ann.",
            "The we have been using this this formalized scenario.",
            "You say that the margin should scale proportional to the loss, so so obviously you can do something different.",
            "Make different sources as well, but.",
            "This is what we have have been using.",
            "On the obvious, you want to allow some slack, so you might not be.",
            "Able to realized kind of hard margins, in this case, like like in binary SVM case.",
            "So this is this setup.",
            "OK, so you will get get this optimization problems that are very familiar from the morning 'cause."
        ],
        [
            "So OK. With the minor details so so here is, here is the loss.",
            "That we are required the margin of the incorrect multi labels to scale.",
            "Proportion of their loss.",
            "So this was mentioned by asking, but she had also some other other.",
            "Scaling snow.",
            "To do all.",
            "It's.",
            "More after saying exactly right it in a different little bit different way.",
            "So so this differences between between these guys are kind of substance to Colonel.",
            "Yes, we use this sees, but actually that might be nicer than what we're using it.",
            "OK, let's try to live with this.",
            "OK, so so the problem with exponential number of primal constraints and dual variables is.",
            "Well known and you have many approaches how you can make this learning tractable.",
            "So then this talk will kind of describe 11 way to actually that.",
            "I guess the key is the kind of combo."
        ],
        [
            "Composability of your of your reps and Jason so we want to arrive at the smaller problem that we can more easily.",
            "Optimize and this requires some kind of decomposition.",
            "Or let's say, one way to do it is to decompose.",
            "The problem is on some way in particular this we are interested in this kind of marginal dual formulation introduced by Topps card so.",
            "Here we require both the loss and the kernel to decompose in some way.",
            "For the features you have.",
            "Couple of different ways you can arrive at.",
            "But composition and these are.",
            "Kind of what we call order cannot representation where you have your joint feature vector is as composed of blocks for each high bridge and then you have.",
            "An alternative where you.",
            "Instead of having blocks, you're computing a sum over your your.",
            "Hyper Edge features and this this will lead to subtle differences in your your optimization.",
            "Most exciting"
        ],
        [
            "Briefly, will will kind of.",
            "Point out to you.",
            "So if we take this kind of optional feature, so we have this kind of.",
            "Presentation or baby have a. X features of the after.",
            "Related to the some edge and we prefix with an indicator dependent on what is the labeling of of the.",
            "Hybrid actually this should be not too so.",
            "This is kind of the.",
            "Crap cases at the edges on a Piper edges, but kind of useful edges in general.",
            "It can be anything else kind of larger done too.",
            "OK.",
            "Right and here.",
            "So basically you can use reuse the same same features for your expertise can be let's say back of your verge of your documenting hierarchal classification or something.",
            "Something like that and you can basically use the same feature vector.",
            "Because this this kind of indicator medicatio this kind of constant context sensitivity and we can learn different feature weights for this.",
            "Representation in different contexts.",
            "So this obviously scales linearly in your in the number of your high bridge is because you have a block for each hybrid.",
            "Given that your hyper edges are bounded by a constant, so they don't scale bigger whenever you structure it.",
            "And if you think about.",
            "Colonel Wise, I think for me it's interesting that there are no hyper at interactions in the kernel, so your kernel is also directly it decomposes.",
            "By the edges.",
            "OK, so this is."
        ],
        [
            "This is an example of what this representation looks is.",
            "If this is, it wasn't clear, so so if you have example document, let's say document and multi label.",
            "And we have some edge with this kind of labeling so.",
            "And if you are using this kind of let's say back of Birch of the documents folder.",
            "Each edge.",
            "So our feature vector will look like this, so we have this block for edge Edge or high Bridge.",
            "Each of these blocks in turn or decomposed into.",
            "Books for each possible labeling of this.",
            "Edge.",
            "So and this indicator notation means that you just put this global X feature vector in that slot that correspond to the to the correct classification.",
            "So that's basically what.",
            "What this means?",
            "OK, so if you take the alternative."
        ],
        [
            "Text this kind of additive features.",
            "So the difference is that then your X features need to be.",
            "Context specific, so because you're computing this some if you don't have have different feature vectors for each each at your will.",
            "You will end up with Twitter, nothing reasonable.",
            "And you have global feature vectors folder for this structure.",
            "So you have only one feature vector.",
            "OK, so you don't, kind of.",
            "Learn individual weights for forage.",
            "Hyperedge.",
            "OK, so because of this form, of course you're.",
            "Pizza vector sizes is constant in the size of your structure, so it means that you don't pay.",
            "Paying this feature vector vector by the size of your.",
            "Structure, but on the other hand, if you look at the dual.",
            "Your kernel will then contain.",
            "Hyper its interaction so so you will have pairwise interactions of the hyper edges so.",
            "In a way, your kernel scales quadratically, so there is kind of.",
            "Compared to the.",
            "What you have with this article features so this makes a difference when you are doing this marginal dual, do all formulas of the problem.",
            "Right?",
            "OK, so the other."
        ],
        [
            "Components.",
            "Loss functions.",
            "So this was already mentioned by Yasmin the morning, so you have many, many choices, obviously for four.",
            "Loss functions the 01 loss.",
            "It's kind of obviously quite limited because you only look at whether you got your structure completely correct or not.",
            "I know creating by how severe or your mistakes where.",
            "And the other extreme is, it is that you completed the composer structure.",
            "You go and look at its its node of your craft and see whether it was.",
            "Correctly classified and not an you sum up your your.",
            "Mistakes.",
            "Neither this takes the dependence structure of.",
            "Of of this micro labels into account in any way.",
            "So.",
            "One kind of middle ground solution.",
            "The races that you're.",
            "You use plus functions that decompose spider hyper edges so you can have more kind of expressive.",
            "Things that take into account this local.",
            "Dependencies.",
            "Described by the high branches, but nothing not more.",
            "So this still decomposes by the edges, so it's kind of.",
            "Those those subsume the parish right right?",
            "And they don't take that.",
            "Oh yeah, yeah yeah yeah yes yes yes you can represent the Hamming loss with this completely.",
            "No problem.",
            "Yeah, it depends what your hyper edges are, I guess, so that's.",
            "Yeah so yeah.",
            "OK, but if it's if you take your thing your edges so you have.",
            "To address and location so I don't know if it's any kind of I don't have any useful kind of loss functions in mind, but you could in principle have kind of because you have four possible configurations in the binary case, so you could.",
            "Think of some I don't know.",
            "Just a hypothetical thing, but.",
            "So I but."
        ],
        [
            "Have some concrete for hierarchy so so.",
            "So cloudy already introduced this hierarchical loss plus functions, or actually one of them yesterday, and so in this case, depending on how you represent your hierarchy, you can use different.",
            "Losses, for example, if you represent your hierarchy as as a tree with edges, so your tree.",
            "Really think of this.",
            "Edges then you can use kind of loss that we call edge loss value.",
            "Penalized the mistake in child.",
            "If you're if the parent is correct.",
            "So if we correctly classified some documents to be entertainment, and we mistake classifying the child as film, we will pay because we are our parents, correct?",
            "But if your parent was already incorrectly classified so, then we don't pay anything.",
            "In the child, so we can.",
            "Right in terms of both of the edges of the hierarchy.",
            "The sparse loss where where you want to penalize the first mistake along apart so this is something that we cannot represent in terms of the edges, so we need to take more more context.",
            "So what you can do there you can insert that hyperedge for each.",
            "Root to leaf bath in the hierarchy.",
            "So so OK. Actually I hear how I have here the partial path of the here.",
            "OK, but basically in this kind of context you can.",
            "You can already pinpoint what was the first mistake and decide on the loss.",
            "So with this kind of representation you can learn in terms of this loss.",
            "But but in this representation, all of it this.",
            "So so you have this kind of kind of choices.",
            "Right?"
        ],
        [
            "OK, so.",
            "So if you go back to the optimization problem so we had.",
            "Had to primal and dual door formulas and.",
            "We want to fund to go now.",
            "Find a kind of marginalized problem."
        ],
        [
            "Paul it so now.",
            "So if you take a loss on feature representation that decomposes by the hyper edges so we can, we can use this marginalization quick point by Topps cards at all.",
            "So the compute marginals of the dual variable so this is a dual variable for for labeling multilabel U and document X and we compute.",
            "Marginal of it in terms of.",
            "For some some edge, so we look at all possible multi labels that will have this particular labeling for the edge and some this sum this up further to arrive at the marginal.",
            "So with this this technique you can you can turn this optimization.",
            "To this marginal dual polytope, so.",
            "So which is?",
            "The benefit is that you will have polynomial size of this.",
            "This dual variables instead of the exponential size of the.",
            "Orignal dual variables.",
            "Right?",
            "Gay."
        ],
        [
            "On so in our case, if you take the.",
            "This loss loss that decomposes by Edge is an hyper attrition and.",
            "On this order, Colonel Fitzer representation we will get.",
            "Problem like this, so that's quite nicely.",
            "Kind of.",
            "Kind of this is.",
            "So quite quite kind of simple formats actually.",
            "So what we get we have still the box constraint that we had in the original problem.",
            "This ones they still remain.",
            "And they are now defined for each edge an edge, each example.",
            "And in addition, we have this kind of marginal consistency constraint.",
            "That that basically say that OK.",
            "Cheese, or hyperedges, need to agree?",
            "On the labeling OK in this hierarchical case?",
            "You will have something like this that you have edges that are are adolescent.",
            "So and you say that this marginals need to be the same.",
            "So if you don't do this, you're gonna lose the equivalence between the marginal and original origin problem.",
            "OK, so we will actually will end up with those techniques where we don't need to actually write down this this constraints explicitly so we can solve it in different way, but.",
            "OK."
        ],
        [
            "Hey, let me first first.",
            "There's something about this size of the problem, so OK, as I said, it's a polynomial size problem.",
            "So we have.",
            "Kind of the size proportional to the product of number of examples times.",
            "Times the number of hyperedges.",
            "Just a number of her.",
            "Number of variables.",
            "Which kernels, like any kernel you have the squared dependency on the number of training examples.",
            "Only linear relationship to the choice of structure constraint matrices you have.",
            "Linear relationship with with.",
            "Training points, part quadratic relationship it size of such a high bridge.",
            "So anyway this is.",
            "This is way too much.",
            "We were experimenting with this wiper data.",
            "So this formulation written like this would take something like 10 gigabytes of memory, so.",
            "I don't have a computer to run that kind of kind of.",
            "Problems right?",
            "OK, so we still need to do something so it's not.",
            "It's not kind of a."
        ],
        [
            "Feasible yet?",
            "OK, so the question is how to how you can decompose this whole thing further and and kind of.",
            "I guess the nontrivial thing about this decomposition is that you have this.",
            "Property tax notice consistency constraints.",
            "Tie your edges together so you cannot decompose.",
            "This matrices in terms of the edges so they take on tie or structure together.",
            "On the other hand, your kernels die examples together, so you cannot such triviality compose it.",
            "What about these various kind of?",
            "You can notice that if you look at the gradient there you have.",
            "Are you kind of?",
            "Can't get away from this this example intra interaction, so you will have a.",
            "Creating interact so that you can work without thinking about this kind of examples so you can you can train indeed called.",
            "Both form one example at a time using gradient based I throated methods.",
            "So, so although this quadratic program doesn't decompose, you can beat with gradient based methods you can.",
            "Drain in a decomposed form.",
            "OK."
        ],
        [
            "So for training we have, we have been using the conditional gradient gradient descent algorithm, that's.",
            "I think.",
            "Classical optimization algorithm.",
            "So it's not our inventation invention at all.",
            "What it does, it does iterative.",
            "Create insert indefeasible set.",
            "And.",
            "The updates are kind of.",
            "Update Direct sums are towards highest feasible points.",
            "Assuming current, current, crazy, and so you're making a linear approximation of your quadratic surface and look at how far you can go in the feasible set.",
            "And this directions you can be solved from linear program.",
            "So you have a linear program after.",
            "Telling you what is the best direction?",
            "Like I said, updates within single examples.",
            "Such spaces can be done independently.",
            "After computing the additional gradient so."
        ],
        [
            "So this is a small kind of illustration how this condition will create and work.",
            "So.",
            "So if we start from here, we first look compute.",
            "Crazy end of the object too."
        ],
        [
            "And OK, so then, pretending that Radian stays constant, we look at what would be the best point we can reach within the feasible set.",
            "So this is, this is the direction.",
            "So this is the best corner assuming a linear.",
            "Linear surface."
        ],
        [
            "Because there are saddle points or using line search exact line search, we compute the saddle point.",
            "Then repeat so complete."
        ],
        [
            "New gradient new conditional gradient.",
            "Saddle point and so on.",
            "So this is very simple procedure.",
            "You need to do.",
            "And it works pretty well actually.",
            "So I'm sorry.",
            "QP with with large constraints right now.",
            "Instead use creating.",
            "The second is celebrating.",
            "The constraints, yeah, well, the constraints at the moment the constraints are are still the same, so we still have.",
            "We have to."
        ],
        [
            "So.",
            "So this feasible set."
        ],
        [
            "Is defined in terms of this this construct.",
            "So this is the.",
            "Yeah.",
            "So the game here you still need to compute the kernel and everything.",
            "Um?",
            "OK.",
            "So I need to OK if I make this steps I need to.",
            "Yes, I need to.",
            "Look at the kernel values of the actually.",
            "Yeah, this single example that would do so.",
            "Kind of, but this kind of be done efficiently, so you can.",
            "You only need to do vector vector products, so this is you can there are some special properties properties of this kind of polytope where you can say you don't need to do any matrix vector operations, so it's pretty efficient actually.",
            "Wanted to find a direct sum.",
            "This direction finding is the hard part, so actually solving this and I will actually come the next so fighting with this thing.",
            "So.",
            "Yeah, but this is the dominating.",
            "If you use an LP solver, almost all time will be you solving this.",
            "These directions."
        ],
        [
            "OK, so so in order to.",
            "By the way, home doing in terms of time.",
            "I think Feynman probably today.",
            "OK. OK yeah, OK.",
            "So to solve this problem better they need to look a little bit about.",
            "How are?",
            "This March in US problem and and originel do our problem relate to each other?",
            "So what is the relationship between this orignal dual polytope and our marginal polytope?",
            "OK, so so if you look at this margin."
        ],
        [
            "Dual variables of single example.",
            "So we can write.",
            "This relationship between the Alpha under under Muse in terms of this kind of matrix M That contest this indicator so it says right up all your marginal constraints and put this indicators into this matrix.",
            "And then your relationship between any arbitrary Alpha and mu is given by this this relation.",
            "Obviously this M has exponential size.",
            "You don't really want to.",
            "Really want to do this, but I just this is just kind of to understand what is going on OK and what is.",
            "Also you can find that if Alpha is a vertex of your dual feasible set, so one vertex here the image.",
            "You is also vertex order.",
            "Marginal dual Polito, so that's that's the key.",
            "Key finding that there is kind of a correspondence between the vertices of this dual polytope and much not work for the top.",
            "It's one to one for threes, but for general craft is not you can have vertices on the marginal dual polytope that's are not images of any vertex here, so.",
            "But for trees, it's one to one.",
            "OK, so if if sulfur is a vertex it has single non 0."
        ],
        [
            "0.",
            "Our component, so it's somewhere there is some multi label where you have a non 0A.",
            "So you can compute the marginal image of this vertex.",
            "And then.",
            "This this mu then is kind of associated with some label and so you can kind of say that this New York this certain labeling.",
            "So this conditional gradient actually kind of becoming, so this linear program actually can be solved.",
            "Like this look forward Y that maximizes this question and this is nothing else but an inference problem on the hyper craft.",
            "So you need to.",
            "Find.",
            "Configuration for for wise that maximizes your.",
            "This is object.",
            "So, so this is.",
            "This is now pretty.",
            "But yes, yes it is, but you can, you can.",
            "You can solve it by.",
            "Yeah 4 four."
        ],
        [
            "None.",
            "Kind of single connected kraftsow sequences trees.",
            "Dynamic programming algorithms.",
            "You can do this in linear time, so it's kind of very large speedup over overline linear programming.",
            "So of course, the catch is that this is gonna be hard for general crafts so.",
            "Want to be us as?",
            "As much weaker, probably for general crafts, but there are many approximate or you Ristic algorithms for center crafts that can give you reasonable.",
            "Answers in reasonable time and what is kind of to be noted in this context.",
            "We don't mean it really need exact inference, so exact inference would mean that.",
            "We actually find this corner exactly or exactly the correct corner, so.",
            "It probably is good enough if you find something that is."
        ],
        [
            "Kind of close to the direction that we want to go, so approximate inference is.",
            "Is more than enough?",
            "So.",
            "In that sense, it's it's not, not.",
            "Deadly problem, right?"
        ],
        [
            "But this is of course an important question.",
            "Is that which bits of this inference algorithms you should use?",
            "And this is something we actually are currently looking at.",
            "What might be the best technique?",
            "The point is you need to have a first initial conversion, so something that actually very fast coast towards the correct region but doesn't matter what it does a synthetically.",
            "So if it doesn't have the conversation is it's not.",
            "That's not the point.",
            "OK, so very quickly something about experiments."
        ],
        [
            "So we have this is these are hierarchical classification experiments.",
            "So we have right destroyed this RC one.",
            "And that's why for our patent database.",
            "Reasonable sized data datasets 2500 documents in Android service on.",
            "Little over 1004 wiper and structures are not very large but but.",
            "Still still OK.",
            "Right and we have compared with.",
            "Q algorithms this is our implementation of the hierarchical classification.",
            "OK, so and then we have compared to flat SVM.",
            "So we're predicting each node of the hierarchy independently.",
            "Hierarchical train SVM.",
            "And this another algorithm by by Nicole Casablanca at all hierarchy, regularised least squares, and these are what cloudy over already pointing out this these two algorithms.",
            "This everything all actually, not everything was lost in a Matlab.",
            "I think what our argument was in was in Matlab.",
            "Pretty usual Pentium PC."
        ],
        [
            "This is a kind of typical optimization curve for, so this is for Viper.",
            "So CPU time goes here and this is the objective of the of the quadratic objective of the problem man.",
            "And there are this is to illustrate this.",
            "How much is dynamic programming inference gives you?",
            "Uh.",
            "As opposed to linear programming.",
            "So something like 15 minutes it takes to almost kind of solve the problem so.",
            "After that, all your support vectors are kind of.",
            "Selected and.",
            "Linear programming and you'll get something like almost 3 hours to get that kind of look at that kind of rate reaching.",
            "Right, so it's quite significant significant speedup."
        ],
        [
            "So this is something about hierarchical.",
            "Justification.",
            "These are level wise F1 scores so so we have compute.",
            "The micro label predictions of on each depth level of the hierarchy for both datasets and looked.",
            "F1 scores of on each level separately.",
            "OK, so Reuters is a little bit dead center at hierarchy so.",
            "It's hard to interpret that results.",
            "White boy is quite nicely balanced, balanced hierarchy, so so I think this is just easier, easier to understand what happens.",
            "So in general you can see that any method.",
            "Including in platte.",
            "SVM is doing well.",
            "Pretty thing to root and early kind of 1st levels, but when you further down you go you will see this kind of phenomenon that plot SVM kind of starts to tail out, and that's big cause it's kind of recall deep node so it will start start to predict negative for everything.",
            "In this hierarchical methods, they do much better in recall.",
            "Right, but maybe we can say that HVM is a little bit weaker, weaker.",
            "It's.",
            "It's hard to say, but these differences between the hierarchal ones are significant, but.",
            "Right, so this is.",
            "Kind of example what you can kind of get.",
            "So 1 interesting thing is that that OK, we cannot.",
            "So because we couldn't in this representing these hierarchies by this edge usually only could use this edge loss to train this so.",
            "I think we want to try this.",
            "This representing the hierarchy as in terms of.",
            "Apostle parts so we can use the.",
            "This full pipe loss and see what happens there, so this is for for the future I hope.",
            "Write something about the scalability so.",
            "So something like 10,000 examples by 1000 micro labels we can fit to the PC memory and this is about the amount that you can raise it reasonably then."
        ],
        [
            "Handle.",
            "But if you think about scale up to very large datasets, like 100,000 examples, by by 10,000 marker labels, and Marco was pointing out to even larger datasets, so OK, since so even storing all the relevant data that would take you something like full hard disk.",
            "And this is mostly about the kernel use discord, quadratic dependency.",
            "Between the size of your training set and so, this is most most of the data will be stored by.",
            "If you didn't store the girls you then.",
            "Yeah, but then I need to recompute the kernels.",
            "The hash rate table you know as you need.",
            "Yeah, yeah, yeah yeah, that's that's of course you can.",
            "You can kind of be more clever about what you need to store.",
            "Actually, that's true.",
            "OK, so this is 1 possibility and also.",
            "This methods are pretty easy to parallelize because you can allocate groups of examples to different processors and they can work on work on them so.",
            "Is going over.",
            "Quite easy, quite easy to parallelize, so in principle you're going to do this kind of thing so.",
            "Right, OK, so the conclusions so."
        ],
        [
            "To his son.",
            "Mother much Martin learning approach for structured outputs.",
            "So, so that's kind of this hyper craft formal lesson.",
            "And we can make the optimism tractable by several techniques.",
            "So we have tested composition, single example subproblems.",
            "Conditional gradient and then coupling with prosthetic inference.",
            "To find this update directions an OK.",
            "So we have tested this method on medium size datasets but not very large large scale.",
            "Any hierarchical classification, at least we can.",
            "We can show that the prediction accuracy is improved when you have deep deep hierarchies.",
            "Right, OK?",
            "So something about future.",
            "Yeah, really, the scaling up is quite interesting.",
            "How far we can scale up this?",
            "Yeah, something else then then then hierarchies is interested.",
            "We haven't really done anything on this direction, but I'm really interested in going there.",
            "This big issue is approximate inference algorithms that.",
            "Connor was fast.",
            "Right, that's about it."
        ],
        [
            "I have a comment about the approximate difference.",
            "Some other people I can't remember exactly now they have them, they they looked at parsing and they looked at instead of generating the full price please.",
            "They looked at generating the parse tree till that they hit an error and actually that is exactly the approximate inference you're talking and they do converge to some solutions where they're doing better than than the.",
            "Exact inference, right?",
            "Yeah OK, yeah.",
            "Yeah, but I guess I guess in our context I wouldn't be surprised if that happened Becausw.",
            "If you think about what this conditional gradient it goes to the corner points, and that's obviously not the correct place, so that's.",
            "It's overshoot somehow every time, so yeah in that sense I wouldn't.",
            "Although I must say there doing.",
            "Right, right, right right, yeah.",
            "In between your updates in the perception of faces, with there being yeah, I think I sometimes worked out something.",
            "So what would have been the kind of perceptron equivalent of this?",
            "And I think the conclusion of something like that that perceptron.",
            "Put 2 something comes, direction would be the same I guess, but size of how much you update would be covered by or some parameter that you given to the kind of learning rate of the perceptron or something like that.",
            "And I think this was this is a little bit vague because it's long time ago I.",
            "Look down Pat.",
            "No place to stop.",
            "Yeah, yeah.",
            "Yeah, it would be interesting to see the connexon.",
            "Yeah yeah yeah yeah yeah.",
            "Surprising people step side.",
            "Sorry.",
            "In this context, adjust margins.",
            "OK, we don't.",
            "OK.",
            "Some people Mystic.",
            "Right, so if I receive an anonymous email.",
            "OK. Next so.",
            "Shifting your projector SPN?",
            "We have the hypergraph, and yeah.",
            "As the regular.",
            "Laplacian as the reference.",
            "Ha.",
            "Off from the top of my head I. I don't really.",
            "I cannot really do.",
            "Yeah it's yeah I we.",
            "I guess we should.",
            "Is it gonna explain this?",
            "Camisa yeah.",
            "Yeah, I'm sorry.",
            "I kind of truly say.",
            "Thank."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spell into multi task learning so I'm kind of continuing their story.",
                    "label": 0
                },
                {
                    "sent": "Usman started the morning so structured output via smoke Mountain Markov sense.",
                    "label": 0
                },
                {
                    "sent": "OK so this should work with Greg Sanders.",
                    "label": 0
                },
                {
                    "sent": "Son Rd said Morgan and John show Taylor.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is privileged domain of interest, so we have.",
                    "label": 0
                },
                {
                    "sent": "Structured",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Multi Label classification problem where we have data from domain where vertis and.",
                    "label": 1
                },
                {
                    "sent": "Texas any sort of object objects and.",
                    "label": 1
                },
                {
                    "sent": "Voice come from some some.",
                    "label": 1
                },
                {
                    "sent": "These are vectors where the components are finite sets, so typically we will have kind of binary vector.",
                    "label": 1
                },
                {
                    "sent": "This multi label with a binary vector but but it can be something else as well.",
                    "label": 0
                },
                {
                    "sent": "So under individual components are called micro labels, we make this assumption that.",
                    "label": 0
                },
                {
                    "sent": "Small labels have this kind of dependency structure that takes the form of a hyper graph, so this is kind of putting the thing into kind of general framework so.",
                    "label": 0
                },
                {
                    "sent": "To be assumed that there is there are kind of hyper hyper edges so.",
                    "label": 0
                },
                {
                    "sent": "So subset of nodes that have potentially some statistical dependency.",
                    "label": 1
                },
                {
                    "sent": "And we want to build models that predict the multi label labels given to given to object.",
                    "label": 0
                },
                {
                    "sent": "So this is overall.",
                    "label": 0
                },
                {
                    "sent": "Goal so OK. Why this hyper craft structure?",
                    "label": 0
                },
                {
                    "sent": "I guess this is just the reason this is nicely generalizes for many of the applications we might be interested.",
                    "label": 0
                },
                {
                    "sent": "So starting from sequences.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going through hierarchies to general crafts like decks, so the only thing that changes is the how you define your hyper edges.",
                    "label": 0
                },
                {
                    "sent": "So that's the kind of.",
                    "label": 0
                },
                {
                    "sent": "Motivation of describing things in this way.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Right, so OK, so example of.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kind of classification problem is this hierarchical multi label classification that we already heard about in a couple of talks.",
                    "label": 0
                },
                {
                    "sent": "Modify cloudy argenti LAN sunder as well.",
                    "label": 0
                },
                {
                    "sent": "So he had to hear to.",
                    "label": 0
                },
                {
                    "sent": "Task is given a document, so some text documents like this is a news article from BBC.",
                    "label": 0
                },
                {
                    "sent": "Mere sight.",
                    "label": 0
                },
                {
                    "sent": "The goal is to classified document into according to the taxonomy hierarchy and so you can allow this kind of multiple classifications that document talks about different topics.",
                    "label": 0
                },
                {
                    "sent": "So you have this kind of.",
                    "label": 0
                },
                {
                    "sent": "Possilpark multiple processor part of classification.",
                    "label": 0
                },
                {
                    "sent": "So everything that is on green is part of the label on everything.",
                    "label": 0
                },
                {
                    "sent": "That is why it is not not part of classification.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 example and.",
                    "label": 0
                },
                {
                    "sent": "If it's.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So kind of more general problem.",
                    "label": 0
                },
                {
                    "sent": "You can have your kind of.",
                    "label": 0
                },
                {
                    "sent": "Classification structure maybe a general crop like a like a dog.",
                    "label": 0
                },
                {
                    "sent": "So this is gene auto, part of Gene Ontology where you're classifying jeans into some kind of.",
                    "label": 1
                },
                {
                    "sent": "According to some kind of concept concept.",
                    "label": 0
                },
                {
                    "sent": "Dag and so so if you have a gene sequence, so you may have, let's say DNA ligation might be some kind of function for the for the gene, and so this means that this kind of.",
                    "label": 0
                },
                {
                    "sent": "Subcraft above this.",
                    "label": 0
                },
                {
                    "sent": "This note would be part of the part of the label for four.",
                    "label": 0
                },
                {
                    "sent": "For that particular gene.",
                    "label": 0
                },
                {
                    "sent": "OK, so OK of course there are some many other other kind of.",
                    "label": 0
                },
                {
                    "sent": "Applications where you can have this kind of kind of structured.",
                    "label": 0
                },
                {
                    "sent": "A classification formulation.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Classification model that we that we use is based on the conditional random field field idea where we have.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have potential assigned to the High British after after.",
                    "label": 0
                },
                {
                    "sent": "Craft.",
                    "label": 0
                },
                {
                    "sent": "And the potentials are there is there is this exponential exponential so.",
                    "label": 0
                },
                {
                    "sent": "Potential function.",
                    "label": 0
                },
                {
                    "sent": "And here in the exponent you have this linear.",
                    "label": 0
                },
                {
                    "sent": "Linear inner product between weight vector and joint feature map.",
                    "label": 0
                },
                {
                    "sent": "So there is a joint feature map for documentation and.",
                    "label": 1
                },
                {
                    "sent": "Any particular labeling of the hyperedge so you have a.",
                    "label": 0
                },
                {
                    "sent": "This feature map is different for each.",
                    "label": 0
                },
                {
                    "sent": "Pair.",
                    "label": 0
                },
                {
                    "sent": "OK, and what you want to learn.",
                    "label": 1
                },
                {
                    "sent": "You want to learn the weight vector in some sense, so typically in conditional random field fancy would try to learn to maximum likelihood estimate.",
                    "label": 0
                },
                {
                    "sent": "But like I indicated, that's not what we're going to going to do here.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "Model.",
                    "label": 0
                },
                {
                    "sent": "So instead of trying to learn maximum likelihood para meters.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We want to find this much marching solution where the goal is to separate correct multilabel.",
                    "label": 1
                },
                {
                    "sent": "For the kind of defeats are joint feature map or image in the feature space of the correct multi label.",
                    "label": 1
                },
                {
                    "sent": "So let's say this part from any incorrect one.",
                    "label": 0
                },
                {
                    "sent": "And then try to push this margin as big as possible.",
                    "label": 0
                },
                {
                    "sent": "And like Yasmin already indicated, there are many, many ways you can kind of penalize this incorrect.",
                    "label": 0
                },
                {
                    "sent": "Prediction, so in terms of how far you want to push and Ann.",
                    "label": 0
                },
                {
                    "sent": "The we have been using this this formalized scenario.",
                    "label": 0
                },
                {
                    "sent": "You say that the margin should scale proportional to the loss, so so obviously you can do something different.",
                    "label": 1
                },
                {
                    "sent": "Make different sources as well, but.",
                    "label": 0
                },
                {
                    "sent": "This is what we have have been using.",
                    "label": 0
                },
                {
                    "sent": "On the obvious, you want to allow some slack, so you might not be.",
                    "label": 1
                },
                {
                    "sent": "Able to realized kind of hard margins, in this case, like like in binary SVM case.",
                    "label": 0
                },
                {
                    "sent": "So this is this setup.",
                    "label": 0
                },
                {
                    "sent": "OK, so you will get get this optimization problems that are very familiar from the morning 'cause.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK. With the minor details so so here is, here is the loss.",
                    "label": 0
                },
                {
                    "sent": "That we are required the margin of the incorrect multi labels to scale.",
                    "label": 0
                },
                {
                    "sent": "Proportion of their loss.",
                    "label": 0
                },
                {
                    "sent": "So this was mentioned by asking, but she had also some other other.",
                    "label": 0
                },
                {
                    "sent": "Scaling snow.",
                    "label": 0
                },
                {
                    "sent": "To do all.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "More after saying exactly right it in a different little bit different way.",
                    "label": 0
                },
                {
                    "sent": "So so this differences between between these guys are kind of substance to Colonel.",
                    "label": 0
                },
                {
                    "sent": "Yes, we use this sees, but actually that might be nicer than what we're using it.",
                    "label": 0
                },
                {
                    "sent": "OK, let's try to live with this.",
                    "label": 0
                },
                {
                    "sent": "OK, so so the problem with exponential number of primal constraints and dual variables is.",
                    "label": 0
                },
                {
                    "sent": "Well known and you have many approaches how you can make this learning tractable.",
                    "label": 0
                },
                {
                    "sent": "So then this talk will kind of describe 11 way to actually that.",
                    "label": 0
                },
                {
                    "sent": "I guess the key is the kind of combo.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Composability of your of your reps and Jason so we want to arrive at the smaller problem that we can more easily.",
                    "label": 0
                },
                {
                    "sent": "Optimize and this requires some kind of decomposition.",
                    "label": 0
                },
                {
                    "sent": "Or let's say, one way to do it is to decompose.",
                    "label": 1
                },
                {
                    "sent": "The problem is on some way in particular this we are interested in this kind of marginal dual formulation introduced by Topps card so.",
                    "label": 1
                },
                {
                    "sent": "Here we require both the loss and the kernel to decompose in some way.",
                    "label": 1
                },
                {
                    "sent": "For the features you have.",
                    "label": 0
                },
                {
                    "sent": "Couple of different ways you can arrive at.",
                    "label": 0
                },
                {
                    "sent": "But composition and these are.",
                    "label": 0
                },
                {
                    "sent": "Kind of what we call order cannot representation where you have your joint feature vector is as composed of blocks for each high bridge and then you have.",
                    "label": 0
                },
                {
                    "sent": "An alternative where you.",
                    "label": 0
                },
                {
                    "sent": "Instead of having blocks, you're computing a sum over your your.",
                    "label": 0
                },
                {
                    "sent": "Hyper Edge features and this this will lead to subtle differences in your your optimization.",
                    "label": 0
                },
                {
                    "sent": "Most exciting",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Briefly, will will kind of.",
                    "label": 0
                },
                {
                    "sent": "Point out to you.",
                    "label": 0
                },
                {
                    "sent": "So if we take this kind of optional feature, so we have this kind of.",
                    "label": 0
                },
                {
                    "sent": "Presentation or baby have a. X features of the after.",
                    "label": 0
                },
                {
                    "sent": "Related to the some edge and we prefix with an indicator dependent on what is the labeling of of the.",
                    "label": 0
                },
                {
                    "sent": "Hybrid actually this should be not too so.",
                    "label": 0
                },
                {
                    "sent": "This is kind of the.",
                    "label": 0
                },
                {
                    "sent": "Crap cases at the edges on a Piper edges, but kind of useful edges in general.",
                    "label": 0
                },
                {
                    "sent": "It can be anything else kind of larger done too.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right and here.",
                    "label": 0
                },
                {
                    "sent": "So basically you can use reuse the same same features for your expertise can be let's say back of your verge of your documenting hierarchal classification or something.",
                    "label": 0
                },
                {
                    "sent": "Something like that and you can basically use the same feature vector.",
                    "label": 0
                },
                {
                    "sent": "Because this this kind of indicator medicatio this kind of constant context sensitivity and we can learn different feature weights for this.",
                    "label": 0
                },
                {
                    "sent": "Representation in different contexts.",
                    "label": 0
                },
                {
                    "sent": "So this obviously scales linearly in your in the number of your high bridge is because you have a block for each hybrid.",
                    "label": 0
                },
                {
                    "sent": "Given that your hyper edges are bounded by a constant, so they don't scale bigger whenever you structure it.",
                    "label": 0
                },
                {
                    "sent": "And if you think about.",
                    "label": 0
                },
                {
                    "sent": "Colonel Wise, I think for me it's interesting that there are no hyper at interactions in the kernel, so your kernel is also directly it decomposes.",
                    "label": 0
                },
                {
                    "sent": "By the edges.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is an example of what this representation looks is.",
                    "label": 0
                },
                {
                    "sent": "If this is, it wasn't clear, so so if you have example document, let's say document and multi label.",
                    "label": 0
                },
                {
                    "sent": "And we have some edge with this kind of labeling so.",
                    "label": 0
                },
                {
                    "sent": "And if you are using this kind of let's say back of Birch of the documents folder.",
                    "label": 0
                },
                {
                    "sent": "Each edge.",
                    "label": 0
                },
                {
                    "sent": "So our feature vector will look like this, so we have this block for edge Edge or high Bridge.",
                    "label": 0
                },
                {
                    "sent": "Each of these blocks in turn or decomposed into.",
                    "label": 0
                },
                {
                    "sent": "Books for each possible labeling of this.",
                    "label": 0
                },
                {
                    "sent": "Edge.",
                    "label": 0
                },
                {
                    "sent": "So and this indicator notation means that you just put this global X feature vector in that slot that correspond to the to the correct classification.",
                    "label": 0
                },
                {
                    "sent": "So that's basically what.",
                    "label": 0
                },
                {
                    "sent": "What this means?",
                    "label": 0
                },
                {
                    "sent": "OK, so if you take the alternative.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Text this kind of additive features.",
                    "label": 0
                },
                {
                    "sent": "So the difference is that then your X features need to be.",
                    "label": 0
                },
                {
                    "sent": "Context specific, so because you're computing this some if you don't have have different feature vectors for each each at your will.",
                    "label": 0
                },
                {
                    "sent": "You will end up with Twitter, nothing reasonable.",
                    "label": 0
                },
                {
                    "sent": "And you have global feature vectors folder for this structure.",
                    "label": 0
                },
                {
                    "sent": "So you have only one feature vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so you don't, kind of.",
                    "label": 0
                },
                {
                    "sent": "Learn individual weights for forage.",
                    "label": 0
                },
                {
                    "sent": "Hyperedge.",
                    "label": 0
                },
                {
                    "sent": "OK, so because of this form, of course you're.",
                    "label": 0
                },
                {
                    "sent": "Pizza vector sizes is constant in the size of your structure, so it means that you don't pay.",
                    "label": 0
                },
                {
                    "sent": "Paying this feature vector vector by the size of your.",
                    "label": 0
                },
                {
                    "sent": "Structure, but on the other hand, if you look at the dual.",
                    "label": 0
                },
                {
                    "sent": "Your kernel will then contain.",
                    "label": 0
                },
                {
                    "sent": "Hyper its interaction so so you will have pairwise interactions of the hyper edges so.",
                    "label": 0
                },
                {
                    "sent": "In a way, your kernel scales quadratically, so there is kind of.",
                    "label": 0
                },
                {
                    "sent": "Compared to the.",
                    "label": 0
                },
                {
                    "sent": "What you have with this article features so this makes a difference when you are doing this marginal dual, do all formulas of the problem.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK, so the other.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Components.",
                    "label": 0
                },
                {
                    "sent": "Loss functions.",
                    "label": 0
                },
                {
                    "sent": "So this was already mentioned by Yasmin the morning, so you have many, many choices, obviously for four.",
                    "label": 0
                },
                {
                    "sent": "Loss functions the 01 loss.",
                    "label": 1
                },
                {
                    "sent": "It's kind of obviously quite limited because you only look at whether you got your structure completely correct or not.",
                    "label": 0
                },
                {
                    "sent": "I know creating by how severe or your mistakes where.",
                    "label": 0
                },
                {
                    "sent": "And the other extreme is, it is that you completed the composer structure.",
                    "label": 0
                },
                {
                    "sent": "You go and look at its its node of your craft and see whether it was.",
                    "label": 0
                },
                {
                    "sent": "Correctly classified and not an you sum up your your.",
                    "label": 0
                },
                {
                    "sent": "Mistakes.",
                    "label": 0
                },
                {
                    "sent": "Neither this takes the dependence structure of.",
                    "label": 1
                },
                {
                    "sent": "Of of this micro labels into account in any way.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One kind of middle ground solution.",
                    "label": 0
                },
                {
                    "sent": "The races that you're.",
                    "label": 1
                },
                {
                    "sent": "You use plus functions that decompose spider hyper edges so you can have more kind of expressive.",
                    "label": 0
                },
                {
                    "sent": "Things that take into account this local.",
                    "label": 0
                },
                {
                    "sent": "Dependencies.",
                    "label": 0
                },
                {
                    "sent": "Described by the high branches, but nothing not more.",
                    "label": 0
                },
                {
                    "sent": "So this still decomposes by the edges, so it's kind of.",
                    "label": 0
                },
                {
                    "sent": "Those those subsume the parish right right?",
                    "label": 0
                },
                {
                    "sent": "And they don't take that.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, yeah yeah yeah yes yes yes you can represent the Hamming loss with this completely.",
                    "label": 0
                },
                {
                    "sent": "No problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it depends what your hyper edges are, I guess, so that's.",
                    "label": 0
                },
                {
                    "sent": "Yeah so yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, but if it's if you take your thing your edges so you have.",
                    "label": 0
                },
                {
                    "sent": "To address and location so I don't know if it's any kind of I don't have any useful kind of loss functions in mind, but you could in principle have kind of because you have four possible configurations in the binary case, so you could.",
                    "label": 0
                },
                {
                    "sent": "Think of some I don't know.",
                    "label": 0
                },
                {
                    "sent": "Just a hypothetical thing, but.",
                    "label": 0
                },
                {
                    "sent": "So I but.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have some concrete for hierarchy so so.",
                    "label": 0
                },
                {
                    "sent": "So cloudy already introduced this hierarchical loss plus functions, or actually one of them yesterday, and so in this case, depending on how you represent your hierarchy, you can use different.",
                    "label": 0
                },
                {
                    "sent": "Losses, for example, if you represent your hierarchy as as a tree with edges, so your tree.",
                    "label": 0
                },
                {
                    "sent": "Really think of this.",
                    "label": 0
                },
                {
                    "sent": "Edges then you can use kind of loss that we call edge loss value.",
                    "label": 0
                },
                {
                    "sent": "Penalized the mistake in child.",
                    "label": 1
                },
                {
                    "sent": "If you're if the parent is correct.",
                    "label": 1
                },
                {
                    "sent": "So if we correctly classified some documents to be entertainment, and we mistake classifying the child as film, we will pay because we are our parents, correct?",
                    "label": 0
                },
                {
                    "sent": "But if your parent was already incorrectly classified so, then we don't pay anything.",
                    "label": 0
                },
                {
                    "sent": "In the child, so we can.",
                    "label": 1
                },
                {
                    "sent": "Right in terms of both of the edges of the hierarchy.",
                    "label": 1
                },
                {
                    "sent": "The sparse loss where where you want to penalize the first mistake along apart so this is something that we cannot represent in terms of the edges, so we need to take more more context.",
                    "label": 0
                },
                {
                    "sent": "So what you can do there you can insert that hyperedge for each.",
                    "label": 0
                },
                {
                    "sent": "Root to leaf bath in the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So so OK. Actually I hear how I have here the partial path of the here.",
                    "label": 1
                },
                {
                    "sent": "OK, but basically in this kind of context you can.",
                    "label": 0
                },
                {
                    "sent": "You can already pinpoint what was the first mistake and decide on the loss.",
                    "label": 0
                },
                {
                    "sent": "So with this kind of representation you can learn in terms of this loss.",
                    "label": 0
                },
                {
                    "sent": "But but in this representation, all of it this.",
                    "label": 0
                },
                {
                    "sent": "So so you have this kind of kind of choices.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So if you go back to the optimization problem so we had.",
                    "label": 0
                },
                {
                    "sent": "Had to primal and dual door formulas and.",
                    "label": 0
                },
                {
                    "sent": "We want to fund to go now.",
                    "label": 0
                },
                {
                    "sent": "Find a kind of marginalized problem.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paul it so now.",
                    "label": 0
                },
                {
                    "sent": "So if you take a loss on feature representation that decomposes by the hyper edges so we can, we can use this marginalization quick point by Topps cards at all.",
                    "label": 0
                },
                {
                    "sent": "So the compute marginals of the dual variable so this is a dual variable for for labeling multilabel U and document X and we compute.",
                    "label": 0
                },
                {
                    "sent": "Marginal of it in terms of.",
                    "label": 0
                },
                {
                    "sent": "For some some edge, so we look at all possible multi labels that will have this particular labeling for the edge and some this sum this up further to arrive at the marginal.",
                    "label": 0
                },
                {
                    "sent": "So with this this technique you can you can turn this optimization.",
                    "label": 0
                },
                {
                    "sent": "To this marginal dual polytope, so.",
                    "label": 0
                },
                {
                    "sent": "So which is?",
                    "label": 0
                },
                {
                    "sent": "The benefit is that you will have polynomial size of this.",
                    "label": 0
                },
                {
                    "sent": "This dual variables instead of the exponential size of the.",
                    "label": 0
                },
                {
                    "sent": "Orignal dual variables.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Gay.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On so in our case, if you take the.",
                    "label": 0
                },
                {
                    "sent": "This loss loss that decomposes by Edge is an hyper attrition and.",
                    "label": 0
                },
                {
                    "sent": "On this order, Colonel Fitzer representation we will get.",
                    "label": 0
                },
                {
                    "sent": "Problem like this, so that's quite nicely.",
                    "label": 0
                },
                {
                    "sent": "Kind of.",
                    "label": 0
                },
                {
                    "sent": "Kind of this is.",
                    "label": 0
                },
                {
                    "sent": "So quite quite kind of simple formats actually.",
                    "label": 0
                },
                {
                    "sent": "So what we get we have still the box constraint that we had in the original problem.",
                    "label": 1
                },
                {
                    "sent": "This ones they still remain.",
                    "label": 0
                },
                {
                    "sent": "And they are now defined for each edge an edge, each example.",
                    "label": 0
                },
                {
                    "sent": "And in addition, we have this kind of marginal consistency constraint.",
                    "label": 1
                },
                {
                    "sent": "That that basically say that OK.",
                    "label": 0
                },
                {
                    "sent": "Cheese, or hyperedges, need to agree?",
                    "label": 0
                },
                {
                    "sent": "On the labeling OK in this hierarchical case?",
                    "label": 0
                },
                {
                    "sent": "You will have something like this that you have edges that are are adolescent.",
                    "label": 0
                },
                {
                    "sent": "So and you say that this marginals need to be the same.",
                    "label": 1
                },
                {
                    "sent": "So if you don't do this, you're gonna lose the equivalence between the marginal and original origin problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so we will actually will end up with those techniques where we don't need to actually write down this this constraints explicitly so we can solve it in different way, but.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hey, let me first first.",
                    "label": 0
                },
                {
                    "sent": "There's something about this size of the problem, so OK, as I said, it's a polynomial size problem.",
                    "label": 1
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "Kind of the size proportional to the product of number of examples times.",
                    "label": 1
                },
                {
                    "sent": "Times the number of hyperedges.",
                    "label": 0
                },
                {
                    "sent": "Just a number of her.",
                    "label": 0
                },
                {
                    "sent": "Number of variables.",
                    "label": 1
                },
                {
                    "sent": "Which kernels, like any kernel you have the squared dependency on the number of training examples.",
                    "label": 0
                },
                {
                    "sent": "Only linear relationship to the choice of structure constraint matrices you have.",
                    "label": 0
                },
                {
                    "sent": "Linear relationship with with.",
                    "label": 0
                },
                {
                    "sent": "Training points, part quadratic relationship it size of such a high bridge.",
                    "label": 1
                },
                {
                    "sent": "So anyway this is.",
                    "label": 0
                },
                {
                    "sent": "This is way too much.",
                    "label": 0
                },
                {
                    "sent": "We were experimenting with this wiper data.",
                    "label": 0
                },
                {
                    "sent": "So this formulation written like this would take something like 10 gigabytes of memory, so.",
                    "label": 0
                },
                {
                    "sent": "I don't have a computer to run that kind of kind of.",
                    "label": 0
                },
                {
                    "sent": "Problems right?",
                    "label": 0
                },
                {
                    "sent": "OK, so we still need to do something so it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not kind of a.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Feasible yet?",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is how to how you can decompose this whole thing further and and kind of.",
                    "label": 0
                },
                {
                    "sent": "I guess the nontrivial thing about this decomposition is that you have this.",
                    "label": 0
                },
                {
                    "sent": "Property tax notice consistency constraints.",
                    "label": 1
                },
                {
                    "sent": "Tie your edges together so you cannot decompose.",
                    "label": 0
                },
                {
                    "sent": "This matrices in terms of the edges so they take on tie or structure together.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, your kernels die examples together, so you cannot such triviality compose it.",
                    "label": 0
                },
                {
                    "sent": "What about these various kind of?",
                    "label": 0
                },
                {
                    "sent": "You can notice that if you look at the gradient there you have.",
                    "label": 0
                },
                {
                    "sent": "Are you kind of?",
                    "label": 0
                },
                {
                    "sent": "Can't get away from this this example intra interaction, so you will have a.",
                    "label": 0
                },
                {
                    "sent": "Creating interact so that you can work without thinking about this kind of examples so you can you can train indeed called.",
                    "label": 0
                },
                {
                    "sent": "Both form one example at a time using gradient based I throated methods.",
                    "label": 1
                },
                {
                    "sent": "So, so although this quadratic program doesn't decompose, you can beat with gradient based methods you can.",
                    "label": 0
                },
                {
                    "sent": "Drain in a decomposed form.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for training we have, we have been using the conditional gradient gradient descent algorithm, that's.",
                    "label": 1
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "Classical optimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it's not our inventation invention at all.",
                    "label": 0
                },
                {
                    "sent": "What it does, it does iterative.",
                    "label": 0
                },
                {
                    "sent": "Create insert indefeasible set.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The updates are kind of.",
                    "label": 1
                },
                {
                    "sent": "Update Direct sums are towards highest feasible points.",
                    "label": 0
                },
                {
                    "sent": "Assuming current, current, crazy, and so you're making a linear approximation of your quadratic surface and look at how far you can go in the feasible set.",
                    "label": 1
                },
                {
                    "sent": "And this directions you can be solved from linear program.",
                    "label": 0
                },
                {
                    "sent": "So you have a linear program after.",
                    "label": 0
                },
                {
                    "sent": "Telling you what is the best direction?",
                    "label": 0
                },
                {
                    "sent": "Like I said, updates within single examples.",
                    "label": 1
                },
                {
                    "sent": "Such spaces can be done independently.",
                    "label": 0
                },
                {
                    "sent": "After computing the additional gradient so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a small kind of illustration how this condition will create and work.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So if we start from here, we first look compute.",
                    "label": 0
                },
                {
                    "sent": "Crazy end of the object too.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And OK, so then, pretending that Radian stays constant, we look at what would be the best point we can reach within the feasible set.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is the direction.",
                    "label": 0
                },
                {
                    "sent": "So this is the best corner assuming a linear.",
                    "label": 0
                },
                {
                    "sent": "Linear surface.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because there are saddle points or using line search exact line search, we compute the saddle point.",
                    "label": 0
                },
                {
                    "sent": "Then repeat so complete.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "New gradient new conditional gradient.",
                    "label": 1
                },
                {
                    "sent": "Saddle point and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is very simple procedure.",
                    "label": 0
                },
                {
                    "sent": "You need to do.",
                    "label": 0
                },
                {
                    "sent": "And it works pretty well actually.",
                    "label": 0
                },
                {
                    "sent": "So I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "QP with with large constraints right now.",
                    "label": 0
                },
                {
                    "sent": "Instead use creating.",
                    "label": 0
                },
                {
                    "sent": "The second is celebrating.",
                    "label": 0
                },
                {
                    "sent": "The constraints, yeah, well, the constraints at the moment the constraints are are still the same, so we still have.",
                    "label": 0
                },
                {
                    "sent": "We have to.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this feasible set.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is defined in terms of this this construct.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So the game here you still need to compute the kernel and everything.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I need to OK if I make this steps I need to.",
                    "label": 0
                },
                {
                    "sent": "Yes, I need to.",
                    "label": 0
                },
                {
                    "sent": "Look at the kernel values of the actually.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this single example that would do so.",
                    "label": 0
                },
                {
                    "sent": "Kind of, but this kind of be done efficiently, so you can.",
                    "label": 0
                },
                {
                    "sent": "You only need to do vector vector products, so this is you can there are some special properties properties of this kind of polytope where you can say you don't need to do any matrix vector operations, so it's pretty efficient actually.",
                    "label": 0
                },
                {
                    "sent": "Wanted to find a direct sum.",
                    "label": 0
                },
                {
                    "sent": "This direction finding is the hard part, so actually solving this and I will actually come the next so fighting with this thing.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but this is the dominating.",
                    "label": 0
                },
                {
                    "sent": "If you use an LP solver, almost all time will be you solving this.",
                    "label": 0
                },
                {
                    "sent": "These directions.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so in order to.",
                    "label": 0
                },
                {
                    "sent": "By the way, home doing in terms of time.",
                    "label": 0
                },
                {
                    "sent": "I think Feynman probably today.",
                    "label": 0
                },
                {
                    "sent": "OK. OK yeah, OK.",
                    "label": 0
                },
                {
                    "sent": "So to solve this problem better they need to look a little bit about.",
                    "label": 0
                },
                {
                    "sent": "How are?",
                    "label": 0
                },
                {
                    "sent": "This March in US problem and and originel do our problem relate to each other?",
                    "label": 0
                },
                {
                    "sent": "So what is the relationship between this orignal dual polytope and our marginal polytope?",
                    "label": 0
                },
                {
                    "sent": "OK, so so if you look at this margin.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dual variables of single example.",
                    "label": 0
                },
                {
                    "sent": "So we can write.",
                    "label": 0
                },
                {
                    "sent": "This relationship between the Alpha under under Muse in terms of this kind of matrix M That contest this indicator so it says right up all your marginal constraints and put this indicators into this matrix.",
                    "label": 0
                },
                {
                    "sent": "And then your relationship between any arbitrary Alpha and mu is given by this this relation.",
                    "label": 0
                },
                {
                    "sent": "Obviously this M has exponential size.",
                    "label": 0
                },
                {
                    "sent": "You don't really want to.",
                    "label": 0
                },
                {
                    "sent": "Really want to do this, but I just this is just kind of to understand what is going on OK and what is.",
                    "label": 0
                },
                {
                    "sent": "Also you can find that if Alpha is a vertex of your dual feasible set, so one vertex here the image.",
                    "label": 1
                },
                {
                    "sent": "You is also vertex order.",
                    "label": 0
                },
                {
                    "sent": "Marginal dual Polito, so that's that's the key.",
                    "label": 1
                },
                {
                    "sent": "Key finding that there is kind of a correspondence between the vertices of this dual polytope and much not work for the top.",
                    "label": 0
                },
                {
                    "sent": "It's one to one for threes, but for general craft is not you can have vertices on the marginal dual polytope that's are not images of any vertex here, so.",
                    "label": 1
                },
                {
                    "sent": "But for trees, it's one to one.",
                    "label": 0
                },
                {
                    "sent": "OK, so if if sulfur is a vertex it has single non 0.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "0.",
                    "label": 0
                },
                {
                    "sent": "Our component, so it's somewhere there is some multi label where you have a non 0A.",
                    "label": 0
                },
                {
                    "sent": "So you can compute the marginal image of this vertex.",
                    "label": 1
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "This this mu then is kind of associated with some label and so you can kind of say that this New York this certain labeling.",
                    "label": 1
                },
                {
                    "sent": "So this conditional gradient actually kind of becoming, so this linear program actually can be solved.",
                    "label": 1
                },
                {
                    "sent": "Like this look forward Y that maximizes this question and this is nothing else but an inference problem on the hyper craft.",
                    "label": 0
                },
                {
                    "sent": "So you need to.",
                    "label": 0
                },
                {
                    "sent": "Find.",
                    "label": 0
                },
                {
                    "sent": "Configuration for for wise that maximizes your.",
                    "label": 0
                },
                {
                    "sent": "This is object.",
                    "label": 0
                },
                {
                    "sent": "So, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is now pretty.",
                    "label": 0
                },
                {
                    "sent": "But yes, yes it is, but you can, you can.",
                    "label": 0
                },
                {
                    "sent": "You can solve it by.",
                    "label": 0
                },
                {
                    "sent": "Yeah 4 four.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "None.",
                    "label": 0
                },
                {
                    "sent": "Kind of single connected kraftsow sequences trees.",
                    "label": 0
                },
                {
                    "sent": "Dynamic programming algorithms.",
                    "label": 0
                },
                {
                    "sent": "You can do this in linear time, so it's kind of very large speedup over overline linear programming.",
                    "label": 0
                },
                {
                    "sent": "So of course, the catch is that this is gonna be hard for general crafts so.",
                    "label": 0
                },
                {
                    "sent": "Want to be us as?",
                    "label": 0
                },
                {
                    "sent": "As much weaker, probably for general crafts, but there are many approximate or you Ristic algorithms for center crafts that can give you reasonable.",
                    "label": 0
                },
                {
                    "sent": "Answers in reasonable time and what is kind of to be noted in this context.",
                    "label": 0
                },
                {
                    "sent": "We don't mean it really need exact inference, so exact inference would mean that.",
                    "label": 0
                },
                {
                    "sent": "We actually find this corner exactly or exactly the correct corner, so.",
                    "label": 0
                },
                {
                    "sent": "It probably is good enough if you find something that is.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of close to the direction that we want to go, so approximate inference is.",
                    "label": 0
                },
                {
                    "sent": "Is more than enough?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In that sense, it's it's not, not.",
                    "label": 0
                },
                {
                    "sent": "Deadly problem, right?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this is of course an important question.",
                    "label": 0
                },
                {
                    "sent": "Is that which bits of this inference algorithms you should use?",
                    "label": 0
                },
                {
                    "sent": "And this is something we actually are currently looking at.",
                    "label": 0
                },
                {
                    "sent": "What might be the best technique?",
                    "label": 0
                },
                {
                    "sent": "The point is you need to have a first initial conversion, so something that actually very fast coast towards the correct region but doesn't matter what it does a synthetically.",
                    "label": 0
                },
                {
                    "sent": "So if it doesn't have the conversation is it's not.",
                    "label": 0
                },
                {
                    "sent": "That's not the point.",
                    "label": 0
                },
                {
                    "sent": "OK, so very quickly something about experiments.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have this is these are hierarchical classification experiments.",
                    "label": 0
                },
                {
                    "sent": "So we have right destroyed this RC one.",
                    "label": 0
                },
                {
                    "sent": "And that's why for our patent database.",
                    "label": 0
                },
                {
                    "sent": "Reasonable sized data datasets 2500 documents in Android service on.",
                    "label": 0
                },
                {
                    "sent": "Little over 1004 wiper and structures are not very large but but.",
                    "label": 0
                },
                {
                    "sent": "Still still OK.",
                    "label": 0
                },
                {
                    "sent": "Right and we have compared with.",
                    "label": 0
                },
                {
                    "sent": "Q algorithms this is our implementation of the hierarchical classification.",
                    "label": 0
                },
                {
                    "sent": "OK, so and then we have compared to flat SVM.",
                    "label": 0
                },
                {
                    "sent": "So we're predicting each node of the hierarchy independently.",
                    "label": 0
                },
                {
                    "sent": "Hierarchical train SVM.",
                    "label": 0
                },
                {
                    "sent": "And this another algorithm by by Nicole Casablanca at all hierarchy, regularised least squares, and these are what cloudy over already pointing out this these two algorithms.",
                    "label": 0
                },
                {
                    "sent": "This everything all actually, not everything was lost in a Matlab.",
                    "label": 0
                },
                {
                    "sent": "I think what our argument was in was in Matlab.",
                    "label": 0
                },
                {
                    "sent": "Pretty usual Pentium PC.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a kind of typical optimization curve for, so this is for Viper.",
                    "label": 0
                },
                {
                    "sent": "So CPU time goes here and this is the objective of the of the quadratic objective of the problem man.",
                    "label": 1
                },
                {
                    "sent": "And there are this is to illustrate this.",
                    "label": 0
                },
                {
                    "sent": "How much is dynamic programming inference gives you?",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "As opposed to linear programming.",
                    "label": 0
                },
                {
                    "sent": "So something like 15 minutes it takes to almost kind of solve the problem so.",
                    "label": 0
                },
                {
                    "sent": "After that, all your support vectors are kind of.",
                    "label": 0
                },
                {
                    "sent": "Selected and.",
                    "label": 0
                },
                {
                    "sent": "Linear programming and you'll get something like almost 3 hours to get that kind of look at that kind of rate reaching.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's quite significant significant speedup.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is something about hierarchical.",
                    "label": 0
                },
                {
                    "sent": "Justification.",
                    "label": 0
                },
                {
                    "sent": "These are level wise F1 scores so so we have compute.",
                    "label": 0
                },
                {
                    "sent": "The micro label predictions of on each depth level of the hierarchy for both datasets and looked.",
                    "label": 0
                },
                {
                    "sent": "F1 scores of on each level separately.",
                    "label": 0
                },
                {
                    "sent": "OK, so Reuters is a little bit dead center at hierarchy so.",
                    "label": 0
                },
                {
                    "sent": "It's hard to interpret that results.",
                    "label": 0
                },
                {
                    "sent": "White boy is quite nicely balanced, balanced hierarchy, so so I think this is just easier, easier to understand what happens.",
                    "label": 0
                },
                {
                    "sent": "So in general you can see that any method.",
                    "label": 0
                },
                {
                    "sent": "Including in platte.",
                    "label": 0
                },
                {
                    "sent": "SVM is doing well.",
                    "label": 0
                },
                {
                    "sent": "Pretty thing to root and early kind of 1st levels, but when you further down you go you will see this kind of phenomenon that plot SVM kind of starts to tail out, and that's big cause it's kind of recall deep node so it will start start to predict negative for everything.",
                    "label": 0
                },
                {
                    "sent": "In this hierarchical methods, they do much better in recall.",
                    "label": 0
                },
                {
                    "sent": "Right, but maybe we can say that HVM is a little bit weaker, weaker.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "It's hard to say, but these differences between the hierarchal ones are significant, but.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is.",
                    "label": 0
                },
                {
                    "sent": "Kind of example what you can kind of get.",
                    "label": 0
                },
                {
                    "sent": "So 1 interesting thing is that that OK, we cannot.",
                    "label": 0
                },
                {
                    "sent": "So because we couldn't in this representing these hierarchies by this edge usually only could use this edge loss to train this so.",
                    "label": 0
                },
                {
                    "sent": "I think we want to try this.",
                    "label": 0
                },
                {
                    "sent": "This representing the hierarchy as in terms of.",
                    "label": 0
                },
                {
                    "sent": "Apostle parts so we can use the.",
                    "label": 0
                },
                {
                    "sent": "This full pipe loss and see what happens there, so this is for for the future I hope.",
                    "label": 0
                },
                {
                    "sent": "Write something about the scalability so.",
                    "label": 0
                },
                {
                    "sent": "So something like 10,000 examples by 1000 micro labels we can fit to the PC memory and this is about the amount that you can raise it reasonably then.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Handle.",
                    "label": 0
                },
                {
                    "sent": "But if you think about scale up to very large datasets, like 100,000 examples, by by 10,000 marker labels, and Marco was pointing out to even larger datasets, so OK, since so even storing all the relevant data that would take you something like full hard disk.",
                    "label": 1
                },
                {
                    "sent": "And this is mostly about the kernel use discord, quadratic dependency.",
                    "label": 0
                },
                {
                    "sent": "Between the size of your training set and so, this is most most of the data will be stored by.",
                    "label": 0
                },
                {
                    "sent": "If you didn't store the girls you then.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but then I need to recompute the kernels.",
                    "label": 0
                },
                {
                    "sent": "The hash rate table you know as you need.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yeah yeah, that's that's of course you can.",
                    "label": 0
                },
                {
                    "sent": "You can kind of be more clever about what you need to store.",
                    "label": 0
                },
                {
                    "sent": "Actually, that's true.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is 1 possibility and also.",
                    "label": 0
                },
                {
                    "sent": "This methods are pretty easy to parallelize because you can allocate groups of examples to different processors and they can work on work on them so.",
                    "label": 0
                },
                {
                    "sent": "Is going over.",
                    "label": 0
                },
                {
                    "sent": "Quite easy, quite easy to parallelize, so in principle you're going to do this kind of thing so.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so the conclusions so.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To his son.",
                    "label": 0
                },
                {
                    "sent": "Mother much Martin learning approach for structured outputs.",
                    "label": 1
                },
                {
                    "sent": "So, so that's kind of this hyper craft formal lesson.",
                    "label": 0
                },
                {
                    "sent": "And we can make the optimism tractable by several techniques.",
                    "label": 0
                },
                {
                    "sent": "So we have tested composition, single example subproblems.",
                    "label": 1
                },
                {
                    "sent": "Conditional gradient and then coupling with prosthetic inference.",
                    "label": 0
                },
                {
                    "sent": "To find this update directions an OK.",
                    "label": 1
                },
                {
                    "sent": "So we have tested this method on medium size datasets but not very large large scale.",
                    "label": 1
                },
                {
                    "sent": "Any hierarchical classification, at least we can.",
                    "label": 0
                },
                {
                    "sent": "We can show that the prediction accuracy is improved when you have deep deep hierarchies.",
                    "label": 0
                },
                {
                    "sent": "Right, OK?",
                    "label": 0
                },
                {
                    "sent": "So something about future.",
                    "label": 0
                },
                {
                    "sent": "Yeah, really, the scaling up is quite interesting.",
                    "label": 0
                },
                {
                    "sent": "How far we can scale up this?",
                    "label": 0
                },
                {
                    "sent": "Yeah, something else then then then hierarchies is interested.",
                    "label": 0
                },
                {
                    "sent": "We haven't really done anything on this direction, but I'm really interested in going there.",
                    "label": 0
                },
                {
                    "sent": "This big issue is approximate inference algorithms that.",
                    "label": 0
                },
                {
                    "sent": "Connor was fast.",
                    "label": 0
                },
                {
                    "sent": "Right, that's about it.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have a comment about the approximate difference.",
                    "label": 0
                },
                {
                    "sent": "Some other people I can't remember exactly now they have them, they they looked at parsing and they looked at instead of generating the full price please.",
                    "label": 0
                },
                {
                    "sent": "They looked at generating the parse tree till that they hit an error and actually that is exactly the approximate inference you're talking and they do converge to some solutions where they're doing better than than the.",
                    "label": 0
                },
                {
                    "sent": "Exact inference, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I guess I guess in our context I wouldn't be surprised if that happened Becausw.",
                    "label": 0
                },
                {
                    "sent": "If you think about what this conditional gradient it goes to the corner points, and that's obviously not the correct place, so that's.",
                    "label": 0
                },
                {
                    "sent": "It's overshoot somehow every time, so yeah in that sense I wouldn't.",
                    "label": 0
                },
                {
                    "sent": "Although I must say there doing.",
                    "label": 0
                },
                {
                    "sent": "Right, right, right right, yeah.",
                    "label": 0
                },
                {
                    "sent": "In between your updates in the perception of faces, with there being yeah, I think I sometimes worked out something.",
                    "label": 0
                },
                {
                    "sent": "So what would have been the kind of perceptron equivalent of this?",
                    "label": 0
                },
                {
                    "sent": "And I think the conclusion of something like that that perceptron.",
                    "label": 0
                },
                {
                    "sent": "Put 2 something comes, direction would be the same I guess, but size of how much you update would be covered by or some parameter that you given to the kind of learning rate of the perceptron or something like that.",
                    "label": 0
                },
                {
                    "sent": "And I think this was this is a little bit vague because it's long time ago I.",
                    "label": 0
                },
                {
                    "sent": "Look down Pat.",
                    "label": 0
                },
                {
                    "sent": "No place to stop.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it would be interesting to see the connexon.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Surprising people step side.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "In this context, adjust margins.",
                    "label": 0
                },
                {
                    "sent": "OK, we don't.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Some people Mystic.",
                    "label": 0
                },
                {
                    "sent": "Right, so if I receive an anonymous email.",
                    "label": 0
                },
                {
                    "sent": "OK. Next so.",
                    "label": 0
                },
                {
                    "sent": "Shifting your projector SPN?",
                    "label": 0
                },
                {
                    "sent": "We have the hypergraph, and yeah.",
                    "label": 0
                },
                {
                    "sent": "As the regular.",
                    "label": 0
                },
                {
                    "sent": "Laplacian as the reference.",
                    "label": 0
                },
                {
                    "sent": "Ha.",
                    "label": 0
                },
                {
                    "sent": "Off from the top of my head I. I don't really.",
                    "label": 0
                },
                {
                    "sent": "I cannot really do.",
                    "label": 0
                },
                {
                    "sent": "Yeah it's yeah I we.",
                    "label": 0
                },
                {
                    "sent": "I guess we should.",
                    "label": 0
                },
                {
                    "sent": "Is it gonna explain this?",
                    "label": 0
                },
                {
                    "sent": "Camisa yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "I kind of truly say.",
                    "label": 0
                },
                {
                    "sent": "Thank.",
                    "label": 0
                }
            ]
        }
    }
}