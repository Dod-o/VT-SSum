{
    "id": "tqiab34c2azz3if7bqkgt2yfvzajm4bp",
    "title": "Cross-Lingual Entity Alignment via Joint Attribute-Preserving Embedding",
    "info": {
        "author": [
            "Zequn Sun, State Key Laboratory for Novel Software Technology, Nanjing University"
        ],
        "published": "Nov. 28, 2017",
        "recorded": "October 2017",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2017_sun_embedding/",
    "segmentation": [
        [
            "Knowledge base is so rich, structured funding by the real world and."
        ],
        [
            "Widely noted, knowledge bases often suffer from 2 problems.",
            "The first one is Luke Carriage and different databases were constructed by different parties using different data sources that contain complementary thanks and the second one is multilingual game cause different knowledge bases may use different languages.",
            "This makes it both necessary and beneficial to integrate cross lingual knowledge bases.",
            "Cross lingual instrument aims to find energies in different knowledge bases, then refer to the same real world object.",
            "Where each node best may be labeled in different natural language such as Vayne in English versus Vienna in Chinese.",
            "While we choose this research topic, reasons are as follows.",
            "First, it plays a vital role in automatically integrating multiple knowledge bases.",
            "Second, it hires construct a coherent knowledge base and finally it enables different expressions of knowledge across diverse natural languages."
        ],
        [
            "To address this problem, traditional methods rely on machine translation to eliminate the language barrier, which is usually costly and airpro becausw.",
            "Because the quality of translation is usually hard to guarantee, there are.",
            "Some there are some other embedding pastor methods can then they use they use the knowledge base embeddings for anti element which trailed promising results.",
            "However they learned embeddings based only on relationship troubles while ignoring attributes and.",
            "The existing alignment user and supervision your accounts for small proportion.",
            "There's a man changes in applying embedding based models for energy element."
        ],
        [
            "So to do is about challenges.",
            "We proposed an embedding based model for cross lingual entity Element which is independent of the diverse natural languages.",
            "I will make certain memes, true cross lingual databases into a unified vector space.",
            "It has two modules namely structure embedding and and attribute imbedding.",
            "The motivation of structure embedding is and the two learnable knowledge bases are likely to have many aligned relationship troubles.",
            "And the idea behind the attributing bedding isn't aligned.",
            "Entities usually have high similarity attributes.",
            "To make full use of the lemon, we let each pair in Salem and share the same embedding.",
            "Look at the next bout some basic preliminaries about not device embedding and word embedding.",
            "Knowledge base embedding includes entities and relations and visitors to preserve area semantics."
        ],
        [
            "Tracy is is the most famous translation based on knowledge Base is very model.",
            "It increases our relationship.",
            "Victor and the translation from its head and Twitter to its tailwind editor.",
            "For example, given the relationship Triple H, RT8 expects Victor Edge Plus Victor are approximate sweetie.",
            "Let's say that sorry.",
            "Let's see the following triple Washington Capital of America.",
            "If we include them into the bigger space.",
            "It expects.",
            "There there we have to meet the constraints as shown in the figure."
        ],
        [
            "The next part would embedding word embedding, aims to learn world aims to learn word, imagine that controls various semantic world relationships.",
            "Skip Gram model is one of the most famous world Twitter Twitter Matic models.",
            "Italian word lens wording meetings are not good at predicting is nearby words.",
            "Then the is given the input word skip.",
            "Gram Square model predicts is contextual words.",
            "OK, let's talk about the proposed model."
        ],
        [
            "Joint attribute presuming bedding JP.",
            "Job.",
            "This is a framework this framework.",
            "The input is 2 customer bases and and some pre aligned energy or property pairs.",
            "We call them seed lemon.",
            "Structure embedding models, reship structures or knowledge bases.",
            "The relationship attributes of.",
            "Knowledge bases can be considered as a relationship graph.",
            "We let each pair in the seed alignment.",
            "Zillow mentor to share the same embedding to overlap their structures.",
            "Attributing many models correlations of attributes, that is, whether these attributes are usually used together to describe a certain group of entities.",
            "Then, given attributing meetings, we can conclude that entity similarities based their tributes.",
            "Then we use similarity constraints to refund to refine the embeddings learned by structure embedding.",
            "Finally, the output.",
            "The output of output is vector representations for NTS, then the alignment of target alignment of the source entity can be conducted by searching the nearest cross link neighbors in the vector space.",
            "Fall."
        ],
        [
            "For structure embedding, we use the score function of transit to measure the possibility of the relationship table.",
            "Underway, minimize the following objective function.",
            "Where we prefer lower scores for positive triples.",
            "That means the existing triples and we prefer higher scores for negative triples.",
            "The following figure shows an example.",
            "Given the input, we first the first initialize the knowledge base embeddings randomly, and then we let each pair in the seed alignment overlap to build the overlap relationship graph.",
            "Then we minimize the score function.",
            "For example.",
            "The relationship the relationship comes out and its counterpart.",
            "Sorry I don't know how to not see it.",
            "It's kind of hard.",
            "Will will turn out to be.",
            "Will turn out to be close to each other because they have the same height, energy, Paris and the same tail energy France.",
            "And in the meantime.",
            "Energy America Energy America and its counterpart will.",
            "Turn out to be close because our becausw they hang because they have the same identity and similar relationships.",
            "And finally, we can see that and finally we can see that the aligned entities like together in the regular space."
        ],
        [
            "For attribute embedding, recall acetyl tributes collected if they are commonly used together to describe energy such as long to link to the place name, 'cause they are the user together to describe classes, and we expect we expect the correlated entities are correlated.",
            "Attributes are embedded closely in the bigger space.",
            "Attributing controls the corrections or tributes it boards idea from square model and then he is given tribute at building products is correlated attributes.",
            "The opposite function is showing as follows.",
            "Attributing many also signs hard collections to the attributes if if they have the same training type.",
            "Once we learn ditributor embeddings."
        ],
        [
            "We sum up.",
            "With some of the embedding so well, energies, attributes and normalize it to get the entity representations that we use the cosine similarity to measure the energy victors and finally we got the pair pairwise similarity metrics.",
            "We want a similar entities to be clustered to refine their rebellions.",
            "The optical function is showing as follows.",
            "Takes the first item for four.",
            "As example, by minimizing between mountains and TV actors in by the end of two vectors in key one and they are similar entities in key be true by minor differences between the by minimizing the difference between the view and the similar entities.",
            "In KP two, we want the similar entities across different keys to be clustered.",
            "And finally, to preserve the boot structure and attribute information.",
            "We jointly minimize the following combined.",
            "Objective function.",
            "OK, the next part is bugged, our evaluation."
        ],
        [
            "We selected DB Pedia to build 3 cross lingual datasets datasets.",
            "We extracted 50,000 Inter language links from English to Chinese, Japanese and French respectively.",
            "We consider them as our coding standards.",
            "Then we extracted their their relationship and attribute in for both triples for the selected trip entities.",
            "We can see them to the entities involved in each data set for each language is much larger than 15,000, and and the tribute triples, triples, contributed, contributed to a significant proportion.",
            "For evaluation metrics we use hidden key and the mid rank to assess the performance.",
            "His and Key calculates the proportion of create entities, ranking the top key and the mirror, and calculates the meaning of this ranks higher hidden key and a lower rank indicate better performance.",
            "Due to time limitation, we only report the results, so we only allow English to Chinese with 30%.",
            "Supervising data we can see that our method, job or method job.",
            "The last line."
        ],
        [
            "Largely out for largely outperformed the compared methods entrance E&JE and we can also see that sent from our three variants of our model.",
            "The negative negative triples and the attribute embeddings are also useful.",
            "We have the same conclusions on the data set of English to Japanese and English to French."
        ],
        [
            "This figure shows the changes of his and Kiwis.",
            "With different proportion of settlement.",
            "We tested the proportion from 10% to 50% with a step 10% and we can see that result will also become better with the increase of the proportion and even with and even with a very small proportion of settlement like 10%, our method still achieved the.",
            "Sushi with promising results.",
            "This figure provides the visualization of some simple results for energy element and attribute correlations.",
            "The left part indicates that aligned entities alignments are embedded closely.",
            "And the right part indicates that the correlated attributes are embedded closely.",
            "Such as such as these attributes are used together to describe mobile phones."
        ],
        [
            "We also compared our method with the traditional method.",
            "Uh, we design.",
            "We design A machine translation based method using Google Translator.",
            "We translate their labels in different languages and then compare their labels similarities.",
            "We can see that we can see that the machine translation method achieved signifying results.",
            "We think this is due to the high accuracy of Google Translator.",
            "Then to further investigate the.",
            "Possibility or combination.",
            "We consider the lower rank of the two results and the compiled rank, and we can see that.",
            "And the last one, and we can see down to the component without a significantly better.",
            "Which shows the mutual commentary ternatea between our method and the machine translation."
        ],
        [
            "OK, finally we will give our conclusion.",
            "We propose a joint.",
            "We propose a joint in best method for cross lingual alignment, which is independent.",
            "Our machine translation and, to the best of our knowledge, we among the first to the embeddings, our cross lingual knowledge bases, while preserving their attribute information for future work.",
            "We would like to introduce attribute values and then we also want to turn it for holistic.",
            "Our entities, relations and attributes for cross lingual knowledge.",
            "Base completition.",
            "And so I think your time."
        ],
        [
            "How much time did it take to the process and this entities?",
            "If you're talking about knowledge pages, we were talking about like 1 million triples, maybe more images basically, and then to combine and to match this all to knowledge bases.",
            "How much did it time?",
            "How much time did it take?",
            "How much time is it raining tag?",
            "Yeah, there's no you compose two knowledge base is right to one you compare with French and English one in French or English with Chinese, right?",
            "Yes, and how much time did it take the translation tag?",
            "And now the whole do the whole thing.",
            "The whole of several several hours.",
            "Several hours, yes.",
            "The 1515 thousand decides.",
            "And we also tried a larger scale.",
            "Datasets 100 and 2020 thousand.",
            "This time will take about one day.",
            "Yeah hello thanks yeah very interesting talk.",
            "I have a question about your results.",
            "So it looked like the machine translation achieves comperable results for hits one.",
            "But then it doesn't get better for, you know, hits 10 and 50.",
            "Can you provide some intuition of why this is happening?",
            "OK, I think they see that routine is questioning Google Translator.",
            "I think.",
            "I think that's good.",
            "I'm better because enter.",
            "the Google Translator has a high accuracy.",
            "High accuracy, so the machine translation better measures countries higher.",
            "Results but but we find that if the Google translator mistakes the meanings of the labels and their results, results will.",
            "Got a lawyer so so are you in.",
            "And our method will almost count.",
            "I'll message you can.",
            "Can achieve high achieve higher results when the translation the quality of translation is not guaranteed.",
            "Another question, your attribute embeddings.",
            "Can you evaluate this separately from the different languages?",
            "So how much do you gain by embedding attributes and combining those with the entity embeddings just inside one language?",
            "Is that possible to evaluate?",
            "You mean even Chinese, Chinese, English to English?",
            "I think this can be left as future worker in this work I only considered across language development.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Knowledge base is so rich, structured funding by the real world and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Widely noted, knowledge bases often suffer from 2 problems.",
                    "label": 0
                },
                {
                    "sent": "The first one is Luke Carriage and different databases were constructed by different parties using different data sources that contain complementary thanks and the second one is multilingual game cause different knowledge bases may use different languages.",
                    "label": 0
                },
                {
                    "sent": "This makes it both necessary and beneficial to integrate cross lingual knowledge bases.",
                    "label": 1
                },
                {
                    "sent": "Cross lingual instrument aims to find energies in different knowledge bases, then refer to the same real world object.",
                    "label": 1
                },
                {
                    "sent": "Where each node best may be labeled in different natural language such as Vayne in English versus Vienna in Chinese.",
                    "label": 0
                },
                {
                    "sent": "While we choose this research topic, reasons are as follows.",
                    "label": 0
                },
                {
                    "sent": "First, it plays a vital role in automatically integrating multiple knowledge bases.",
                    "label": 1
                },
                {
                    "sent": "Second, it hires construct a coherent knowledge base and finally it enables different expressions of knowledge across diverse natural languages.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To address this problem, traditional methods rely on machine translation to eliminate the language barrier, which is usually costly and airpro becausw.",
                    "label": 1
                },
                {
                    "sent": "Because the quality of translation is usually hard to guarantee, there are.",
                    "label": 0
                },
                {
                    "sent": "Some there are some other embedding pastor methods can then they use they use the knowledge base embeddings for anti element which trailed promising results.",
                    "label": 0
                },
                {
                    "sent": "However they learned embeddings based only on relationship troubles while ignoring attributes and.",
                    "label": 1
                },
                {
                    "sent": "The existing alignment user and supervision your accounts for small proportion.",
                    "label": 0
                },
                {
                    "sent": "There's a man changes in applying embedding based models for energy element.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to do is about challenges.",
                    "label": 0
                },
                {
                    "sent": "We proposed an embedding based model for cross lingual entity Element which is independent of the diverse natural languages.",
                    "label": 0
                },
                {
                    "sent": "I will make certain memes, true cross lingual databases into a unified vector space.",
                    "label": 1
                },
                {
                    "sent": "It has two modules namely structure embedding and and attribute imbedding.",
                    "label": 0
                },
                {
                    "sent": "The motivation of structure embedding is and the two learnable knowledge bases are likely to have many aligned relationship troubles.",
                    "label": 1
                },
                {
                    "sent": "And the idea behind the attributing bedding isn't aligned.",
                    "label": 0
                },
                {
                    "sent": "Entities usually have high similarity attributes.",
                    "label": 1
                },
                {
                    "sent": "To make full use of the lemon, we let each pair in Salem and share the same embedding.",
                    "label": 0
                },
                {
                    "sent": "Look at the next bout some basic preliminaries about not device embedding and word embedding.",
                    "label": 0
                },
                {
                    "sent": "Knowledge base embedding includes entities and relations and visitors to preserve area semantics.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tracy is is the most famous translation based on knowledge Base is very model.",
                    "label": 1
                },
                {
                    "sent": "It increases our relationship.",
                    "label": 0
                },
                {
                    "sent": "Victor and the translation from its head and Twitter to its tailwind editor.",
                    "label": 1
                },
                {
                    "sent": "For example, given the relationship Triple H, RT8 expects Victor Edge Plus Victor are approximate sweetie.",
                    "label": 0
                },
                {
                    "sent": "Let's say that sorry.",
                    "label": 0
                },
                {
                    "sent": "Let's see the following triple Washington Capital of America.",
                    "label": 0
                },
                {
                    "sent": "If we include them into the bigger space.",
                    "label": 0
                },
                {
                    "sent": "It expects.",
                    "label": 0
                },
                {
                    "sent": "There there we have to meet the constraints as shown in the figure.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next part would embedding word embedding, aims to learn world aims to learn word, imagine that controls various semantic world relationships.",
                    "label": 0
                },
                {
                    "sent": "Skip Gram model is one of the most famous world Twitter Twitter Matic models.",
                    "label": 0
                },
                {
                    "sent": "Italian word lens wording meetings are not good at predicting is nearby words.",
                    "label": 1
                },
                {
                    "sent": "Then the is given the input word skip.",
                    "label": 1
                },
                {
                    "sent": "Gram Square model predicts is contextual words.",
                    "label": 0
                },
                {
                    "sent": "OK, let's talk about the proposed model.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Joint attribute presuming bedding JP.",
                    "label": 0
                },
                {
                    "sent": "Job.",
                    "label": 0
                },
                {
                    "sent": "This is a framework this framework.",
                    "label": 0
                },
                {
                    "sent": "The input is 2 customer bases and and some pre aligned energy or property pairs.",
                    "label": 0
                },
                {
                    "sent": "We call them seed lemon.",
                    "label": 0
                },
                {
                    "sent": "Structure embedding models, reship structures or knowledge bases.",
                    "label": 1
                },
                {
                    "sent": "The relationship attributes of.",
                    "label": 0
                },
                {
                    "sent": "Knowledge bases can be considered as a relationship graph.",
                    "label": 0
                },
                {
                    "sent": "We let each pair in the seed alignment.",
                    "label": 1
                },
                {
                    "sent": "Zillow mentor to share the same embedding to overlap their structures.",
                    "label": 0
                },
                {
                    "sent": "Attributing many models correlations of attributes, that is, whether these attributes are usually used together to describe a certain group of entities.",
                    "label": 1
                },
                {
                    "sent": "Then, given attributing meetings, we can conclude that entity similarities based their tributes.",
                    "label": 0
                },
                {
                    "sent": "Then we use similarity constraints to refund to refine the embeddings learned by structure embedding.",
                    "label": 0
                },
                {
                    "sent": "Finally, the output.",
                    "label": 0
                },
                {
                    "sent": "The output of output is vector representations for NTS, then the alignment of target alignment of the source entity can be conducted by searching the nearest cross link neighbors in the vector space.",
                    "label": 0
                },
                {
                    "sent": "Fall.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For structure embedding, we use the score function of transit to measure the possibility of the relationship table.",
                    "label": 1
                },
                {
                    "sent": "Underway, minimize the following objective function.",
                    "label": 0
                },
                {
                    "sent": "Where we prefer lower scores for positive triples.",
                    "label": 0
                },
                {
                    "sent": "That means the existing triples and we prefer higher scores for negative triples.",
                    "label": 0
                },
                {
                    "sent": "The following figure shows an example.",
                    "label": 0
                },
                {
                    "sent": "Given the input, we first the first initialize the knowledge base embeddings randomly, and then we let each pair in the seed alignment overlap to build the overlap relationship graph.",
                    "label": 0
                },
                {
                    "sent": "Then we minimize the score function.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "The relationship the relationship comes out and its counterpart.",
                    "label": 0
                },
                {
                    "sent": "Sorry I don't know how to not see it.",
                    "label": 0
                },
                {
                    "sent": "It's kind of hard.",
                    "label": 0
                },
                {
                    "sent": "Will will turn out to be.",
                    "label": 0
                },
                {
                    "sent": "Will turn out to be close to each other because they have the same height, energy, Paris and the same tail energy France.",
                    "label": 0
                },
                {
                    "sent": "And in the meantime.",
                    "label": 0
                },
                {
                    "sent": "Energy America Energy America and its counterpart will.",
                    "label": 0
                },
                {
                    "sent": "Turn out to be close because our becausw they hang because they have the same identity and similar relationships.",
                    "label": 0
                },
                {
                    "sent": "And finally, we can see that and finally we can see that the aligned entities like together in the regular space.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For attribute embedding, recall acetyl tributes collected if they are commonly used together to describe energy such as long to link to the place name, 'cause they are the user together to describe classes, and we expect we expect the correlated entities are correlated.",
                    "label": 1
                },
                {
                    "sent": "Attributes are embedded closely in the bigger space.",
                    "label": 0
                },
                {
                    "sent": "Attributing controls the corrections or tributes it boards idea from square model and then he is given tribute at building products is correlated attributes.",
                    "label": 0
                },
                {
                    "sent": "The opposite function is showing as follows.",
                    "label": 1
                },
                {
                    "sent": "Attributing many also signs hard collections to the attributes if if they have the same training type.",
                    "label": 0
                },
                {
                    "sent": "Once we learn ditributor embeddings.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We sum up.",
                    "label": 0
                },
                {
                    "sent": "With some of the embedding so well, energies, attributes and normalize it to get the entity representations that we use the cosine similarity to measure the energy victors and finally we got the pair pairwise similarity metrics.",
                    "label": 1
                },
                {
                    "sent": "We want a similar entities to be clustered to refine their rebellions.",
                    "label": 1
                },
                {
                    "sent": "The optical function is showing as follows.",
                    "label": 0
                },
                {
                    "sent": "Takes the first item for four.",
                    "label": 0
                },
                {
                    "sent": "As example, by minimizing between mountains and TV actors in by the end of two vectors in key one and they are similar entities in key be true by minor differences between the by minimizing the difference between the view and the similar entities.",
                    "label": 0
                },
                {
                    "sent": "In KP two, we want the similar entities across different keys to be clustered.",
                    "label": 1
                },
                {
                    "sent": "And finally, to preserve the boot structure and attribute information.",
                    "label": 0
                },
                {
                    "sent": "We jointly minimize the following combined.",
                    "label": 0
                },
                {
                    "sent": "Objective function.",
                    "label": 0
                },
                {
                    "sent": "OK, the next part is bugged, our evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We selected DB Pedia to build 3 cross lingual datasets datasets.",
                    "label": 0
                },
                {
                    "sent": "We extracted 50,000 Inter language links from English to Chinese, Japanese and French respectively.",
                    "label": 0
                },
                {
                    "sent": "We consider them as our coding standards.",
                    "label": 0
                },
                {
                    "sent": "Then we extracted their their relationship and attribute in for both triples for the selected trip entities.",
                    "label": 0
                },
                {
                    "sent": "We can see them to the entities involved in each data set for each language is much larger than 15,000, and and the tribute triples, triples, contributed, contributed to a significant proportion.",
                    "label": 0
                },
                {
                    "sent": "For evaluation metrics we use hidden key and the mid rank to assess the performance.",
                    "label": 0
                },
                {
                    "sent": "His and Key calculates the proportion of create entities, ranking the top key and the mirror, and calculates the meaning of this ranks higher hidden key and a lower rank indicate better performance.",
                    "label": 1
                },
                {
                    "sent": "Due to time limitation, we only report the results, so we only allow English to Chinese with 30%.",
                    "label": 0
                },
                {
                    "sent": "Supervising data we can see that our method, job or method job.",
                    "label": 0
                },
                {
                    "sent": "The last line.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Largely out for largely outperformed the compared methods entrance E&JE and we can also see that sent from our three variants of our model.",
                    "label": 0
                },
                {
                    "sent": "The negative negative triples and the attribute embeddings are also useful.",
                    "label": 0
                },
                {
                    "sent": "We have the same conclusions on the data set of English to Japanese and English to French.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This figure shows the changes of his and Kiwis.",
                    "label": 0
                },
                {
                    "sent": "With different proportion of settlement.",
                    "label": 1
                },
                {
                    "sent": "We tested the proportion from 10% to 50% with a step 10% and we can see that result will also become better with the increase of the proportion and even with and even with a very small proportion of settlement like 10%, our method still achieved the.",
                    "label": 1
                },
                {
                    "sent": "Sushi with promising results.",
                    "label": 0
                },
                {
                    "sent": "This figure provides the visualization of some simple results for energy element and attribute correlations.",
                    "label": 0
                },
                {
                    "sent": "The left part indicates that aligned entities alignments are embedded closely.",
                    "label": 0
                },
                {
                    "sent": "And the right part indicates that the correlated attributes are embedded closely.",
                    "label": 0
                },
                {
                    "sent": "Such as such as these attributes are used together to describe mobile phones.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also compared our method with the traditional method.",
                    "label": 0
                },
                {
                    "sent": "Uh, we design.",
                    "label": 0
                },
                {
                    "sent": "We design A machine translation based method using Google Translator.",
                    "label": 0
                },
                {
                    "sent": "We translate their labels in different languages and then compare their labels similarities.",
                    "label": 0
                },
                {
                    "sent": "We can see that we can see that the machine translation method achieved signifying results.",
                    "label": 0
                },
                {
                    "sent": "We think this is due to the high accuracy of Google Translator.",
                    "label": 1
                },
                {
                    "sent": "Then to further investigate the.",
                    "label": 0
                },
                {
                    "sent": "Possibility or combination.",
                    "label": 0
                },
                {
                    "sent": "We consider the lower rank of the two results and the compiled rank, and we can see that.",
                    "label": 1
                },
                {
                    "sent": "And the last one, and we can see down to the component without a significantly better.",
                    "label": 0
                },
                {
                    "sent": "Which shows the mutual commentary ternatea between our method and the machine translation.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, finally we will give our conclusion.",
                    "label": 0
                },
                {
                    "sent": "We propose a joint.",
                    "label": 0
                },
                {
                    "sent": "We propose a joint in best method for cross lingual alignment, which is independent.",
                    "label": 0
                },
                {
                    "sent": "Our machine translation and, to the best of our knowledge, we among the first to the embeddings, our cross lingual knowledge bases, while preserving their attribute information for future work.",
                    "label": 1
                },
                {
                    "sent": "We would like to introduce attribute values and then we also want to turn it for holistic.",
                    "label": 1
                },
                {
                    "sent": "Our entities, relations and attributes for cross lingual knowledge.",
                    "label": 0
                },
                {
                    "sent": "Base completition.",
                    "label": 0
                },
                {
                    "sent": "And so I think your time.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How much time did it take to the process and this entities?",
                    "label": 0
                },
                {
                    "sent": "If you're talking about knowledge pages, we were talking about like 1 million triples, maybe more images basically, and then to combine and to match this all to knowledge bases.",
                    "label": 0
                },
                {
                    "sent": "How much did it time?",
                    "label": 0
                },
                {
                    "sent": "How much time did it take?",
                    "label": 0
                },
                {
                    "sent": "How much time is it raining tag?",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's no you compose two knowledge base is right to one you compare with French and English one in French or English with Chinese, right?",
                    "label": 0
                },
                {
                    "sent": "Yes, and how much time did it take the translation tag?",
                    "label": 0
                },
                {
                    "sent": "And now the whole do the whole thing.",
                    "label": 0
                },
                {
                    "sent": "The whole of several several hours.",
                    "label": 0
                },
                {
                    "sent": "Several hours, yes.",
                    "label": 0
                },
                {
                    "sent": "The 1515 thousand decides.",
                    "label": 0
                },
                {
                    "sent": "And we also tried a larger scale.",
                    "label": 0
                },
                {
                    "sent": "Datasets 100 and 2020 thousand.",
                    "label": 0
                },
                {
                    "sent": "This time will take about one day.",
                    "label": 0
                },
                {
                    "sent": "Yeah hello thanks yeah very interesting talk.",
                    "label": 0
                },
                {
                    "sent": "I have a question about your results.",
                    "label": 0
                },
                {
                    "sent": "So it looked like the machine translation achieves comperable results for hits one.",
                    "label": 0
                },
                {
                    "sent": "But then it doesn't get better for, you know, hits 10 and 50.",
                    "label": 0
                },
                {
                    "sent": "Can you provide some intuition of why this is happening?",
                    "label": 0
                },
                {
                    "sent": "OK, I think they see that routine is questioning Google Translator.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "I think that's good.",
                    "label": 0
                },
                {
                    "sent": "I'm better because enter.",
                    "label": 0
                },
                {
                    "sent": "the Google Translator has a high accuracy.",
                    "label": 0
                },
                {
                    "sent": "High accuracy, so the machine translation better measures countries higher.",
                    "label": 0
                },
                {
                    "sent": "Results but but we find that if the Google translator mistakes the meanings of the labels and their results, results will.",
                    "label": 0
                },
                {
                    "sent": "Got a lawyer so so are you in.",
                    "label": 0
                },
                {
                    "sent": "And our method will almost count.",
                    "label": 0
                },
                {
                    "sent": "I'll message you can.",
                    "label": 0
                },
                {
                    "sent": "Can achieve high achieve higher results when the translation the quality of translation is not guaranteed.",
                    "label": 0
                },
                {
                    "sent": "Another question, your attribute embeddings.",
                    "label": 0
                },
                {
                    "sent": "Can you evaluate this separately from the different languages?",
                    "label": 0
                },
                {
                    "sent": "So how much do you gain by embedding attributes and combining those with the entity embeddings just inside one language?",
                    "label": 0
                },
                {
                    "sent": "Is that possible to evaluate?",
                    "label": 0
                },
                {
                    "sent": "You mean even Chinese, Chinese, English to English?",
                    "label": 0
                },
                {
                    "sent": "I think this can be left as future worker in this work I only considered across language development.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}