{
    "id": "u34ydr2jogwr3fyy4uboaygroyff2sal",
    "title": "Personalizing Web Search using Long Term Browsing History",
    "info": {
        "author": [
            "Nicolaas Matthijs, Centre for Applied Research in Educational Technologies, University of Cambridge"
        ],
        "published": "Aug. 9, 2011",
        "recorded": "February 2011",
        "category": [
            "Top->Computer Science->Web Search"
        ]
    },
    "url": "http://videolectures.net/wsdm2011_matthijs_pws/",
    "segmentation": [
        [
            "So a 30 second introduction what exactly personalized?"
        ],
        [
            "Web searches, so we've got these three people.",
            "The first person is A is a physicist.",
            "The second person is someone attending this conference, and the third person is a is a stock broker and they all enter the same query, which is IR, which is a short, unambiguous query.",
            "But they actually want something else.",
            "The physicist wants pages on infrared.",
            "The middle guy wants pages on information retrieval and this guy wants pages on International Rectifier and their stock quotes.",
            "So what personalized service tries to do is to present each of these three users with a different ranking which is tailored to what their personal interests are and what their information needed."
        ],
        [
            "So."
        ],
        [
            "That's why we tried to do so quickly.",
            "Some related work.",
            "There's been many approaches suggested for personalized search.",
            "There is in general there are two sort of broad directions, 1 is for the clickthrough based approaches where it's mainly based on.",
            "People search behavior the queries they do and the results that click on for the search queries.",
            "They do.",
            "The approach we found most promising was was P click presented in 2007.",
            "And what they do is they promote URLs which have previously been clicked by the same user for the same query.",
            "So if a user enters a query for the first time, nothing happens.",
            "If he enters the same query for a second time, it will promote URLs that the user click the previous time and we will compare against this method later on.",
            "And there's a whole range of profile based approaches where you actually tried to represent the users interest through user profile.",
            "The most promising we found this is the one presented by Steven in 2005.",
            "No.",
            "It's just I need power.",
            "OK, so presented in 2005 they they represent their users through rich model of user interests which is built from search related information, previously visited websites, documents on their hard drive, emails and so on.",
            "So basically everything they can find about the users and the strategy that uses they re rank the top return search results for those users to make them more."
        ],
        [
            "Interesting to use.",
            "So there were three three main goals in this project.",
            "The first one is to try and improve on on existing personalized web search technique by by doing.",
            "First of all, the combination of profile based, an clickthrough based trying to come up with a selection of new features that actually improve the personalization an.",
            "So we would build an improved user representation from long-term browsing history using a user's browsing history, and the second goal was to try and improve on the valuation methodology.",
            "Most importantly, we want to make we want to find out whether this makes a real difference in people's lives in their day-to-day searches.",
            "And another important thing is we want to try and do that without changing the environment.",
            "They search it and the third one is an additional goal is to try actually develop something that people can use."
        ],
        [
            "For real.",
            "So the search personalization process is A is a two step process in which the first one is user interest extraction and the second one is actually ranking the results.",
            "So further for the user interest extraction we actually tried to represent the user by three things.",
            "First of all, user profile which represents his interests and that's a list of weighted terms, then a list of old URLs that the user has visited and how often he's visited.",
            "Those analyst of all the search queries the user has done an.",
            "On which results he clicked for those queries, the last two can be easily derived from their browsing history.",
            "The first one we actually have to try and learn."
        ],
        [
            "So in order to try and learn those user interests, it's a it's a three step process is outlined here in which is the first step is that is term list generation.",
            "One important decision we've made is to not treat web pages as just flat documents, but we actually use the HTML structure that's embedded within them.",
            "So we've gotta.",
            "We've got six different sources of input data.",
            "We can actually use.",
            "So first of all, this is the title unigrams.",
            "This is all of the words used.",
            "Or all of the terms used in the title tag inside of the HTML.",
            "Second one metadata, description unigrams is all of the unigrams used in the metadata description tag.",
            "The third one is metadata keyword, so all of the terms used in the metadata keywords tag.",
            "Full text unigrams is just take the entire content of the HTML document and use all of those terms.",
            "The fifth one is extracted terms, so we've actually we've reimplemented the term extraction algorithm from view, sorry.",
            "And and what it does, is it so so we feed it the entire document and then it and then the algorithm will come up with a set of keywords that represents the actual document.",
            "OK, and then the 6th one is extracting noun phrases, so we use a statistical language parser from Clark concurrent to actually we feed it the entire document and we derive the we extract a noun phrases from that then so using all of these, all of these we are able to specify how important each of those data sources.",
            "Data sources should be an.",
            "We specify that through weight factor and that can be actually any number, but we've we've simplified it through three, either 0, so not include the data source one just included or relative changed the weight of the data source to how many of them there are.",
            "So for example, metadata keywords would be, which would get a heavier weight than, for example, all of the unigrams from from the from the full text.",
            "And so from this you can actually make any sort of combination of these different data sources.",
            "So this first step gives us a list of terms that are that can be associated."
        ],
        [
            "With the user and the second step is term list filtering.",
            "We've got three options.",
            "We can do no filtering at all.",
            "We can do Wordnet based part of speech filtering so we only keep the terms that have a certain.",
            "For example only denounce or only the verbs.",
            "The third one is Google Ngram based filtering, so we we only keep the words that occur more than a certain number in the in the Google Ngram corpus.",
            "So this will give us a filtered list of terms in later on that it sort of turned out that this didn't really make a difference, so we continued with no filtering."
        ],
        [
            "And in step three, is actually giving each each of the terms await.",
            "We've got three weighting methods.",
            "The first ones term frequency, so we've got the frequency vector of how often all of all of the terms occur in the different input data sources.",
            "We calculate the weight.",
            "We do the dot product of the weight factor and the frequency vector.",
            "Then for the second one is term frequency, inverse document frequency.",
            "We take the term frequency weight and we divide by the.",
            "Like the log of the document frequency and for document frequency, we haven't actually used how often in how many documents.",
            "The term you're looking at shows up in in your browsing history, because if a term shows up a lot in your browsing history, it actually means that it is relevant, so it wouldn't be fair to wait it less so to complete it with an estimate of document frequency, we use the Google Ngram corpus to get an idea of how often the term is used, just on the web.",
            "And the third one is is personalized BM 25 which have been presented by Steven in 2005.",
            "And so capital N will use this one actually stands for the total number of documents on the web.",
            "We've used the Google Ngram corpus to estimate that NT is the number of documents on the web that have the term you're looking at.",
            "Use Google Ngram corpus to estimate that, then capital R is the total number of documents that are relevant to the user, so we assume that all of the documents in the browsing history are relevant to the user and then RT is the.",
            "Number of documents in the browsing history that actually have this term, and so so.",
            "In the end, this gives us a user profile with a list of terms and await for."
        ],
        [
            "Turn.",
            "Second step in the personalization process is actually is re ranking search results.",
            "So we want to change the order of the results to better reflect the user's interests.",
            "The approach we've taken is to get the first 50 results for a query on Google and then we re rank that based on the profile and we give we give a score to each each of the search net."
        ],
        [
            "Return.",
            "Got three methods of scoring a snippet.",
            "The first ones matching is basically a sum over all of the terms in the snippet of the number of times the term shows up in the snippet times the weight of the of the term and unique matching is actually the same, but ignoring how often the term occurs in the in the sniper and the third one is a language model, so we create and add one language model from the profile and then so we some of the logs, so we actually calculate the.",
            "Probability of the snippet given the user's profile.",
            "So those are three three scoring methods.",
            "The second step is to actually.",
            "Where do we want to keep the Google ranking to account or not?",
            "Because so far we haven't actually taken it into account.",
            "You don't have by dividing by the log of the rank Ann, and we can choose to give extra way to previously visited pages.",
            "So this is sort of an extension to pee click and it's been done by T van already.",
            "So instead of just looking at the documents that have been clicked for a certain query we look at.",
            "If you've ever visited this page, that's part of the results, then we give it extra weight.",
            "And so so every every snippet gets a score and then we can re rank the results based on that."
        ],
        [
            "Or so how do we evaluate this?",
            "It's actually quite a difficult problem.",
            "In most previous work, most of the time, one of these three approaches have been used where either you have a small number of users that evaluate the relevance of a small number of queries for the results returned for a small number of queries, you can simulate a personalized search setting using the track query and document collection, and you can do it after the fact.",
            "Log based analysis of a search lock.",
            "But what we really wanted to find out is whether the personalization yields a real difference or an improvement in real life usage.",
            "So we had to get.",
            "Usage data from lots of users over long time in their day to day activities.",
            "But to get that was actually unfeasible because of the high number of parameters we have.",
            "So we came up with a 2 step evaluation process where the first step is a is in the offline relevance judgments session where we actually do this one where we have a small number of users evaluating relevance of small number of queries that allows us to make a short list of promising approaches, and then the second step in the process is to do a large scale online evaluation.",
            "With more users over a longer period of time to actually try and see whether our parameter combinations generalize over unseen users and unseen browsing history and also to find out whether it makes a real difference."
        ],
        [
            "Day to day searches.",
            "So first problem, we need users and data to actually work with.",
            "We need their full browsing history and we can't find it anywhere on the Internet.",
            "So what we've done is we've written the Firefox add on called Alter Ego, which actually kept captures your browsing history.",
            "And we found 41 users to install it, and they've run that over three months total of about half a million page visits and 40,000 Google searches."
        ],
        [
            "So step one, the offline relevance judgments.",
            "So the goal for this part of the evaluation is to identify the most promising parameter combination configurations.",
            "So we've organized an offline evaluation session where we have six different users assess the relevance of the top 50 results for 12 queries and assessing the relevance can happen by saying this return document is not relevant.",
            "Get Surrell score of 0.",
            "The return document is relevant.",
            "Get Surrell score of 1.",
            "And the order return document is very relevant.",
            "Get Surrell score of two.",
            "And that allows us to to assess all possible combinations of all of our parameters.",
            "And for each of the different rankings we calculate and DCG score an.",
            "This this G is sort of like a rank quality measure."
        ],
        [
            "So the results of that session.",
            "So we were able to evaluate 15,878 different profile and re ranking combinations and we compare those to three baseline systems.",
            "Google Peak lickint even.",
            "Anne.",
            "Of those 15 thousand 4455 performed better than Google based on the average NDC G 3335.",
            "Perform better than to even approach and 1580 performed better than peak like an.",
            "From this we identified the four most promising approaches.",
            "The first one is Max and ECG is actually the approach that got the highest average and ECG score of .568 compared to Google .506.",
            "The second one is Max Query which has had the highest number of queries improved for the 72 evaluated queries.",
            "Actually 50 two of them were improved.",
            "Anne.",
            "Then Max best pyramids.",
            "The third one which is which is selected by by greedily selecting the different parameters in the sequential order and then Max no maximum rank is an interesting one as well because this one was the best approach.",
            "It didn't actually take the Google rank into account and still managed to perform better.",
            "Now later it turned out that this one was mainly because of overtraining and didn't perform as well in the wild."
        ],
        [
            "A couple of things we learned from this is that treating web pages as flat documents didn't seem to work.",
            "Every time we included the all of the words mentioned in the document, the score actually went down significantly.",
            "Some of the advanced NLP techniques we've used, like the.",
            "Term extraction and keyword focused approach is like using metadata keywords.",
            "They seem to seem to work really well.",
            "In terms of actually ranking re ranking the results, there was one method that outperformed all of the other ones.",
            "So using language model snippet scoring, giving extra weight to visited URLs an keeping the Google rank into account.",
            "So we only had one one sort of parameter."
        ],
        [
            "Set left there.",
            "So Step 2 is the is the online interleaved evaluation where we actually want to assess the selected techniques in the wild.",
            "For a long term, for lots of users to see where it makes a difference in their day-to-day lives, so we extended the fire Fox item to to actually do search personalization in their browser in users browsers as they go.",
            "We use interleaved evaluation using team draft interleaving algorithm presented by Philip Ann, which has been shown to accurately reflect differences in ranking relevance."
        ],
        [
            "So 30 second introduction on what interleaved evaluation does.",
            "So we've got the original ranking, and you've got the personalized ranking Ann, and we sort of try to interleave them or combine them into one ranking.",
            "So there's a.",
            "There's a coin toss, the personalized ranking ones, so gets to deliver the 1st result.",
            "We remove the one from both rankings, then the original ranking can give one, and we will remove those from from both rankings.",
            "Then another coin toss.",
            "Now the original ranking wins brings in the result it's removed and the personalized ranking does the same.",
            "And then another coin toss in the original ranking wins again.",
            "Rings in the 5th result, and now we actually have an interleaved ranking.",
            "And so and so we then count which ranking is clicked most often.",
            "So if if two and three were clicked in a given for a given query impression, then the original ranking would get a vote."
        ],
        [
            "So the results for those is we had 41 users for two weeks actually doing this in their browser and we had 8000 queries, so all three of these approaches Max and Max best power in Mexico where they significantly outperformed Google One more significant than the other.",
            "And we have actually run them on all all of our queries.",
            "We haven't done any investigation on which ones we should personalize or which ones we shouldn't.",
            "And it turns out that 70% of the queries that doesn't make a difference for 20% of them, the query ranking improves and for 10% of them it becomes worse.",
            "On average, when there is an improvement it the result is brought up by 4 ranks.",
            "When it gets worse on average is brought down by one rank an both in the offline evaluation and the online evaluation.",
            "The Max and DCG approach.",
            "Is consistently the best so that one uses.",
            "Title unigrams metadata keywords and extracting noun phrases as input data.",
            "Uses no filtering and TF IDF as the term waiting and then for the RE ranking language model snippet scoring.",
            "We keep the rank into account and we give extra weight to visit you."
        ],
        [
            "So some future work we can do.",
            "We can expand the set of parameters by learning the optimal weight factor or using other fields like the like.",
            "The H1 tag.",
            "For example, we can incorporate temporal information.",
            "How much browsing history should be used, we can decay the weight of older, older items.",
            "We can keep the page visit duration into account, and then we've got access to other behavioral information like mouse movement.",
            "Interestingly, we can use extracted profiles for other purposes.",
            "So we we gave all of the participants their own profile that was generated and they a lot of them came back to me saying, well, this is actually quite good.",
            "It represents my interests quite well, so this could be used for like.",
            "Suggesting interesting news articles or or."
        ],
        [
            "Even advertising.",
            "So the conclusion we managed to outperform Google and best previous personalization strategies, we've built an improved user profile for personalization by not retreating web pages as flat documents and using more advanced NLP techniques like term extraction.",
            "We've improved upon the evaluation methodology, done the sort of first large online comparative evaluation of personalization techniques, and we tried to investigate whether it makes a difference in people's real life.",
            "This was all done in an academic setting with no large datasets available.",
            "An IT produced a tool that can be downloaded and used by everyone.",
            "The code is open sourced an everybody should feel free to have a look at it."
        ],
        [
            "Any questions?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a 30 second introduction what exactly personalized?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Web searches, so we've got these three people.",
                    "label": 0
                },
                {
                    "sent": "The first person is A is a physicist.",
                    "label": 0
                },
                {
                    "sent": "The second person is someone attending this conference, and the third person is a is a stock broker and they all enter the same query, which is IR, which is a short, unambiguous query.",
                    "label": 0
                },
                {
                    "sent": "But they actually want something else.",
                    "label": 0
                },
                {
                    "sent": "The physicist wants pages on infrared.",
                    "label": 0
                },
                {
                    "sent": "The middle guy wants pages on information retrieval and this guy wants pages on International Rectifier and their stock quotes.",
                    "label": 0
                },
                {
                    "sent": "So what personalized service tries to do is to present each of these three users with a different ranking which is tailored to what their personal interests are and what their information needed.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's why we tried to do so quickly.",
                    "label": 0
                },
                {
                    "sent": "Some related work.",
                    "label": 0
                },
                {
                    "sent": "There's been many approaches suggested for personalized search.",
                    "label": 0
                },
                {
                    "sent": "There is in general there are two sort of broad directions, 1 is for the clickthrough based approaches where it's mainly based on.",
                    "label": 0
                },
                {
                    "sent": "People search behavior the queries they do and the results that click on for the search queries.",
                    "label": 0
                },
                {
                    "sent": "They do.",
                    "label": 0
                },
                {
                    "sent": "The approach we found most promising was was P click presented in 2007.",
                    "label": 0
                },
                {
                    "sent": "And what they do is they promote URLs which have previously been clicked by the same user for the same query.",
                    "label": 1
                },
                {
                    "sent": "So if a user enters a query for the first time, nothing happens.",
                    "label": 0
                },
                {
                    "sent": "If he enters the same query for a second time, it will promote URLs that the user click the previous time and we will compare against this method later on.",
                    "label": 0
                },
                {
                    "sent": "And there's a whole range of profile based approaches where you actually tried to represent the users interest through user profile.",
                    "label": 0
                },
                {
                    "sent": "The most promising we found this is the one presented by Steven in 2005.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "It's just I need power.",
                    "label": 1
                },
                {
                    "sent": "OK, so presented in 2005 they they represent their users through rich model of user interests which is built from search related information, previously visited websites, documents on their hard drive, emails and so on.",
                    "label": 0
                },
                {
                    "sent": "So basically everything they can find about the users and the strategy that uses they re rank the top return search results for those users to make them more.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interesting to use.",
                    "label": 0
                },
                {
                    "sent": "So there were three three main goals in this project.",
                    "label": 0
                },
                {
                    "sent": "The first one is to try and improve on on existing personalized web search technique by by doing.",
                    "label": 1
                },
                {
                    "sent": "First of all, the combination of profile based, an clickthrough based trying to come up with a selection of new features that actually improve the personalization an.",
                    "label": 1
                },
                {
                    "sent": "So we would build an improved user representation from long-term browsing history using a user's browsing history, and the second goal was to try and improve on the valuation methodology.",
                    "label": 1
                },
                {
                    "sent": "Most importantly, we want to make we want to find out whether this makes a real difference in people's lives in their day-to-day searches.",
                    "label": 0
                },
                {
                    "sent": "And another important thing is we want to try and do that without changing the environment.",
                    "label": 0
                },
                {
                    "sent": "They search it and the third one is an additional goal is to try actually develop something that people can use.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For real.",
                    "label": 0
                },
                {
                    "sent": "So the search personalization process is A is a two step process in which the first one is user interest extraction and the second one is actually ranking the results.",
                    "label": 1
                },
                {
                    "sent": "So further for the user interest extraction we actually tried to represent the user by three things.",
                    "label": 1
                },
                {
                    "sent": "First of all, user profile which represents his interests and that's a list of weighted terms, then a list of old URLs that the user has visited and how often he's visited.",
                    "label": 1
                },
                {
                    "sent": "Those analyst of all the search queries the user has done an.",
                    "label": 0
                },
                {
                    "sent": "On which results he clicked for those queries, the last two can be easily derived from their browsing history.",
                    "label": 0
                },
                {
                    "sent": "The first one we actually have to try and learn.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order to try and learn those user interests, it's a it's a three step process is outlined here in which is the first step is that is term list generation.",
                    "label": 0
                },
                {
                    "sent": "One important decision we've made is to not treat web pages as just flat documents, but we actually use the HTML structure that's embedded within them.",
                    "label": 1
                },
                {
                    "sent": "So we've gotta.",
                    "label": 1
                },
                {
                    "sent": "We've got six different sources of input data.",
                    "label": 0
                },
                {
                    "sent": "We can actually use.",
                    "label": 0
                },
                {
                    "sent": "So first of all, this is the title unigrams.",
                    "label": 0
                },
                {
                    "sent": "This is all of the words used.",
                    "label": 0
                },
                {
                    "sent": "Or all of the terms used in the title tag inside of the HTML.",
                    "label": 0
                },
                {
                    "sent": "Second one metadata, description unigrams is all of the unigrams used in the metadata description tag.",
                    "label": 0
                },
                {
                    "sent": "The third one is metadata keyword, so all of the terms used in the metadata keywords tag.",
                    "label": 0
                },
                {
                    "sent": "Full text unigrams is just take the entire content of the HTML document and use all of those terms.",
                    "label": 0
                },
                {
                    "sent": "The fifth one is extracted terms, so we've actually we've reimplemented the term extraction algorithm from view, sorry.",
                    "label": 0
                },
                {
                    "sent": "And and what it does, is it so so we feed it the entire document and then it and then the algorithm will come up with a set of keywords that represents the actual document.",
                    "label": 1
                },
                {
                    "sent": "OK, and then the 6th one is extracting noun phrases, so we use a statistical language parser from Clark concurrent to actually we feed it the entire document and we derive the we extract a noun phrases from that then so using all of these, all of these we are able to specify how important each of those data sources.",
                    "label": 0
                },
                {
                    "sent": "Data sources should be an.",
                    "label": 0
                },
                {
                    "sent": "We specify that through weight factor and that can be actually any number, but we've we've simplified it through three, either 0, so not include the data source one just included or relative changed the weight of the data source to how many of them there are.",
                    "label": 0
                },
                {
                    "sent": "So for example, metadata keywords would be, which would get a heavier weight than, for example, all of the unigrams from from the from the full text.",
                    "label": 1
                },
                {
                    "sent": "And so from this you can actually make any sort of combination of these different data sources.",
                    "label": 0
                },
                {
                    "sent": "So this first step gives us a list of terms that are that can be associated.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With the user and the second step is term list filtering.",
                    "label": 1
                },
                {
                    "sent": "We've got three options.",
                    "label": 0
                },
                {
                    "sent": "We can do no filtering at all.",
                    "label": 0
                },
                {
                    "sent": "We can do Wordnet based part of speech filtering so we only keep the terms that have a certain.",
                    "label": 0
                },
                {
                    "sent": "For example only denounce or only the verbs.",
                    "label": 1
                },
                {
                    "sent": "The third one is Google Ngram based filtering, so we we only keep the words that occur more than a certain number in the in the Google Ngram corpus.",
                    "label": 0
                },
                {
                    "sent": "So this will give us a filtered list of terms in later on that it sort of turned out that this didn't really make a difference, so we continued with no filtering.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in step three, is actually giving each each of the terms await.",
                    "label": 0
                },
                {
                    "sent": "We've got three weighting methods.",
                    "label": 0
                },
                {
                    "sent": "The first ones term frequency, so we've got the frequency vector of how often all of all of the terms occur in the different input data sources.",
                    "label": 0
                },
                {
                    "sent": "We calculate the weight.",
                    "label": 0
                },
                {
                    "sent": "We do the dot product of the weight factor and the frequency vector.",
                    "label": 0
                },
                {
                    "sent": "Then for the second one is term frequency, inverse document frequency.",
                    "label": 0
                },
                {
                    "sent": "We take the term frequency weight and we divide by the.",
                    "label": 0
                },
                {
                    "sent": "Like the log of the document frequency and for document frequency, we haven't actually used how often in how many documents.",
                    "label": 0
                },
                {
                    "sent": "The term you're looking at shows up in in your browsing history, because if a term shows up a lot in your browsing history, it actually means that it is relevant, so it wouldn't be fair to wait it less so to complete it with an estimate of document frequency, we use the Google Ngram corpus to get an idea of how often the term is used, just on the web.",
                    "label": 0
                },
                {
                    "sent": "And the third one is is personalized BM 25 which have been presented by Steven in 2005.",
                    "label": 0
                },
                {
                    "sent": "And so capital N will use this one actually stands for the total number of documents on the web.",
                    "label": 0
                },
                {
                    "sent": "We've used the Google Ngram corpus to estimate that NT is the number of documents on the web that have the term you're looking at.",
                    "label": 0
                },
                {
                    "sent": "Use Google Ngram corpus to estimate that, then capital R is the total number of documents that are relevant to the user, so we assume that all of the documents in the browsing history are relevant to the user and then RT is the.",
                    "label": 0
                },
                {
                    "sent": "Number of documents in the browsing history that actually have this term, and so so.",
                    "label": 0
                },
                {
                    "sent": "In the end, this gives us a user profile with a list of terms and await for.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Turn.",
                    "label": 0
                },
                {
                    "sent": "Second step in the personalization process is actually is re ranking search results.",
                    "label": 0
                },
                {
                    "sent": "So we want to change the order of the results to better reflect the user's interests.",
                    "label": 1
                },
                {
                    "sent": "The approach we've taken is to get the first 50 results for a query on Google and then we re rank that based on the profile and we give we give a score to each each of the search net.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Return.",
                    "label": 0
                },
                {
                    "sent": "Got three methods of scoring a snippet.",
                    "label": 0
                },
                {
                    "sent": "The first ones matching is basically a sum over all of the terms in the snippet of the number of times the term shows up in the snippet times the weight of the of the term and unique matching is actually the same, but ignoring how often the term occurs in the in the sniper and the third one is a language model, so we create and add one language model from the profile and then so we some of the logs, so we actually calculate the.",
                    "label": 0
                },
                {
                    "sent": "Probability of the snippet given the user's profile.",
                    "label": 0
                },
                {
                    "sent": "So those are three three scoring methods.",
                    "label": 0
                },
                {
                    "sent": "The second step is to actually.",
                    "label": 0
                },
                {
                    "sent": "Where do we want to keep the Google ranking to account or not?",
                    "label": 0
                },
                {
                    "sent": "Because so far we haven't actually taken it into account.",
                    "label": 1
                },
                {
                    "sent": "You don't have by dividing by the log of the rank Ann, and we can choose to give extra way to previously visited pages.",
                    "label": 1
                },
                {
                    "sent": "So this is sort of an extension to pee click and it's been done by T van already.",
                    "label": 0
                },
                {
                    "sent": "So instead of just looking at the documents that have been clicked for a certain query we look at.",
                    "label": 0
                },
                {
                    "sent": "If you've ever visited this page, that's part of the results, then we give it extra weight.",
                    "label": 0
                },
                {
                    "sent": "And so so every every snippet gets a score and then we can re rank the results based on that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or so how do we evaluate this?",
                    "label": 0
                },
                {
                    "sent": "It's actually quite a difficult problem.",
                    "label": 0
                },
                {
                    "sent": "In most previous work, most of the time, one of these three approaches have been used where either you have a small number of users that evaluate the relevance of a small number of queries for the results returned for a small number of queries, you can simulate a personalized search setting using the track query and document collection, and you can do it after the fact.",
                    "label": 1
                },
                {
                    "sent": "Log based analysis of a search lock.",
                    "label": 1
                },
                {
                    "sent": "But what we really wanted to find out is whether the personalization yields a real difference or an improvement in real life usage.",
                    "label": 0
                },
                {
                    "sent": "So we had to get.",
                    "label": 1
                },
                {
                    "sent": "Usage data from lots of users over long time in their day to day activities.",
                    "label": 0
                },
                {
                    "sent": "But to get that was actually unfeasible because of the high number of parameters we have.",
                    "label": 0
                },
                {
                    "sent": "So we came up with a 2 step evaluation process where the first step is a is in the offline relevance judgments session where we actually do this one where we have a small number of users evaluating relevance of small number of queries that allows us to make a short list of promising approaches, and then the second step in the process is to do a large scale online evaluation.",
                    "label": 0
                },
                {
                    "sent": "With more users over a longer period of time to actually try and see whether our parameter combinations generalize over unseen users and unseen browsing history and also to find out whether it makes a real difference.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Day to day searches.",
                    "label": 0
                },
                {
                    "sent": "So first problem, we need users and data to actually work with.",
                    "label": 1
                },
                {
                    "sent": "We need their full browsing history and we can't find it anywhere on the Internet.",
                    "label": 0
                },
                {
                    "sent": "So what we've done is we've written the Firefox add on called Alter Ego, which actually kept captures your browsing history.",
                    "label": 0
                },
                {
                    "sent": "And we found 41 users to install it, and they've run that over three months total of about half a million page visits and 40,000 Google searches.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So step one, the offline relevance judgments.",
                    "label": 1
                },
                {
                    "sent": "So the goal for this part of the evaluation is to identify the most promising parameter combination configurations.",
                    "label": 0
                },
                {
                    "sent": "So we've organized an offline evaluation session where we have six different users assess the relevance of the top 50 results for 12 queries and assessing the relevance can happen by saying this return document is not relevant.",
                    "label": 1
                },
                {
                    "sent": "Get Surrell score of 0.",
                    "label": 0
                },
                {
                    "sent": "The return document is relevant.",
                    "label": 0
                },
                {
                    "sent": "Get Surrell score of 1.",
                    "label": 0
                },
                {
                    "sent": "And the order return document is very relevant.",
                    "label": 0
                },
                {
                    "sent": "Get Surrell score of two.",
                    "label": 1
                },
                {
                    "sent": "And that allows us to to assess all possible combinations of all of our parameters.",
                    "label": 0
                },
                {
                    "sent": "And for each of the different rankings we calculate and DCG score an.",
                    "label": 0
                },
                {
                    "sent": "This this G is sort of like a rank quality measure.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the results of that session.",
                    "label": 0
                },
                {
                    "sent": "So we were able to evaluate 15,878 different profile and re ranking combinations and we compare those to three baseline systems.",
                    "label": 0
                },
                {
                    "sent": "Google Peak lickint even.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Of those 15 thousand 4455 performed better than Google based on the average NDC G 3335.",
                    "label": 1
                },
                {
                    "sent": "Perform better than to even approach and 1580 performed better than peak like an.",
                    "label": 1
                },
                {
                    "sent": "From this we identified the four most promising approaches.",
                    "label": 0
                },
                {
                    "sent": "The first one is Max and ECG is actually the approach that got the highest average and ECG score of .568 compared to Google .506.",
                    "label": 0
                },
                {
                    "sent": "The second one is Max Query which has had the highest number of queries improved for the 72 evaluated queries.",
                    "label": 0
                },
                {
                    "sent": "Actually 50 two of them were improved.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Then Max best pyramids.",
                    "label": 0
                },
                {
                    "sent": "The third one which is which is selected by by greedily selecting the different parameters in the sequential order and then Max no maximum rank is an interesting one as well because this one was the best approach.",
                    "label": 0
                },
                {
                    "sent": "It didn't actually take the Google rank into account and still managed to perform better.",
                    "label": 0
                },
                {
                    "sent": "Now later it turned out that this one was mainly because of overtraining and didn't perform as well in the wild.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A couple of things we learned from this is that treating web pages as flat documents didn't seem to work.",
                    "label": 0
                },
                {
                    "sent": "Every time we included the all of the words mentioned in the document, the score actually went down significantly.",
                    "label": 0
                },
                {
                    "sent": "Some of the advanced NLP techniques we've used, like the.",
                    "label": 0
                },
                {
                    "sent": "Term extraction and keyword focused approach is like using metadata keywords.",
                    "label": 0
                },
                {
                    "sent": "They seem to seem to work really well.",
                    "label": 0
                },
                {
                    "sent": "In terms of actually ranking re ranking the results, there was one method that outperformed all of the other ones.",
                    "label": 0
                },
                {
                    "sent": "So using language model snippet scoring, giving extra weight to visited URLs an keeping the Google rank into account.",
                    "label": 1
                },
                {
                    "sent": "So we only had one one sort of parameter.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Set left there.",
                    "label": 0
                },
                {
                    "sent": "So Step 2 is the is the online interleaved evaluation where we actually want to assess the selected techniques in the wild.",
                    "label": 1
                },
                {
                    "sent": "For a long term, for lots of users to see where it makes a difference in their day-to-day lives, so we extended the fire Fox item to to actually do search personalization in their browser in users browsers as they go.",
                    "label": 0
                },
                {
                    "sent": "We use interleaved evaluation using team draft interleaving algorithm presented by Philip Ann, which has been shown to accurately reflect differences in ranking relevance.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So 30 second introduction on what interleaved evaluation does.",
                    "label": 1
                },
                {
                    "sent": "So we've got the original ranking, and you've got the personalized ranking Ann, and we sort of try to interleave them or combine them into one ranking.",
                    "label": 0
                },
                {
                    "sent": "So there's a.",
                    "label": 1
                },
                {
                    "sent": "There's a coin toss, the personalized ranking ones, so gets to deliver the 1st result.",
                    "label": 0
                },
                {
                    "sent": "We remove the one from both rankings, then the original ranking can give one, and we will remove those from from both rankings.",
                    "label": 0
                },
                {
                    "sent": "Then another coin toss.",
                    "label": 0
                },
                {
                    "sent": "Now the original ranking wins brings in the result it's removed and the personalized ranking does the same.",
                    "label": 1
                },
                {
                    "sent": "And then another coin toss in the original ranking wins again.",
                    "label": 0
                },
                {
                    "sent": "Rings in the 5th result, and now we actually have an interleaved ranking.",
                    "label": 0
                },
                {
                    "sent": "And so and so we then count which ranking is clicked most often.",
                    "label": 1
                },
                {
                    "sent": "So if if two and three were clicked in a given for a given query impression, then the original ranking would get a vote.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the results for those is we had 41 users for two weeks actually doing this in their browser and we had 8000 queries, so all three of these approaches Max and Max best power in Mexico where they significantly outperformed Google One more significant than the other.",
                    "label": 0
                },
                {
                    "sent": "And we have actually run them on all all of our queries.",
                    "label": 0
                },
                {
                    "sent": "We haven't done any investigation on which ones we should personalize or which ones we shouldn't.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that 70% of the queries that doesn't make a difference for 20% of them, the query ranking improves and for 10% of them it becomes worse.",
                    "label": 0
                },
                {
                    "sent": "On average, when there is an improvement it the result is brought up by 4 ranks.",
                    "label": 0
                },
                {
                    "sent": "When it gets worse on average is brought down by one rank an both in the offline evaluation and the online evaluation.",
                    "label": 0
                },
                {
                    "sent": "The Max and DCG approach.",
                    "label": 0
                },
                {
                    "sent": "Is consistently the best so that one uses.",
                    "label": 1
                },
                {
                    "sent": "Title unigrams metadata keywords and extracting noun phrases as input data.",
                    "label": 0
                },
                {
                    "sent": "Uses no filtering and TF IDF as the term waiting and then for the RE ranking language model snippet scoring.",
                    "label": 0
                },
                {
                    "sent": "We keep the rank into account and we give extra weight to visit you.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some future work we can do.",
                    "label": 0
                },
                {
                    "sent": "We can expand the set of parameters by learning the optimal weight factor or using other fields like the like.",
                    "label": 1
                },
                {
                    "sent": "The H1 tag.",
                    "label": 0
                },
                {
                    "sent": "For example, we can incorporate temporal information.",
                    "label": 1
                },
                {
                    "sent": "How much browsing history should be used, we can decay the weight of older, older items.",
                    "label": 1
                },
                {
                    "sent": "We can keep the page visit duration into account, and then we've got access to other behavioral information like mouse movement.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, we can use extracted profiles for other purposes.",
                    "label": 0
                },
                {
                    "sent": "So we we gave all of the participants their own profile that was generated and they a lot of them came back to me saying, well, this is actually quite good.",
                    "label": 0
                },
                {
                    "sent": "It represents my interests quite well, so this could be used for like.",
                    "label": 0
                },
                {
                    "sent": "Suggesting interesting news articles or or.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Even advertising.",
                    "label": 0
                },
                {
                    "sent": "So the conclusion we managed to outperform Google and best previous personalization strategies, we've built an improved user profile for personalization by not retreating web pages as flat documents and using more advanced NLP techniques like term extraction.",
                    "label": 1
                },
                {
                    "sent": "We've improved upon the evaluation methodology, done the sort of first large online comparative evaluation of personalization techniques, and we tried to investigate whether it makes a difference in people's real life.",
                    "label": 1
                },
                {
                    "sent": "This was all done in an academic setting with no large datasets available.",
                    "label": 0
                },
                {
                    "sent": "An IT produced a tool that can be downloaded and used by everyone.",
                    "label": 0
                },
                {
                    "sent": "The code is open sourced an everybody should feel free to have a look at it.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any questions?",
                    "label": 0
                }
            ]
        }
    }
}