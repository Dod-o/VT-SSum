{
    "id": "v3kfowevywmnxistvqnrzffg7lqz3yt3",
    "title": "Introduction to Graphical Models",
    "info": {
        "author": [
            "Silvia Chiappa, Faculty of Mathematics, University of Cambridge"
        ],
        "published": "Aug. 13, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/mlss2010_chiappa_mlfcs3/",
    "segmentation": [
        [
            "Yes.",
            "OK, so we can start but before starting I would like to thank the European Community for supporting me through Mercury into European fellowship.",
            "So in this presentation will give you an introduction to graphical models.",
            "So let's start by defining graphical models and give a little."
        ],
        [
            "Date of motivation or why we want to do to know about them.",
            "So graphical models are graphs which contain links and nodes and nodes represent random variable while links represent statistical dependencies between the variables.",
            "So graphical model give us provide us with.",
            "Visual tool for reasoning under uncertainty.",
            "And as we know, machine learning and related disciplines which are concerned with modeling or the word or extracting information about data.",
            "Uncertainty arises mainly from two sources and our limited understanding of the forward and the limited amount of data that is available to us.",
            "Therefore, having methods which account for uncertainty is very important."
        ],
        [
            "But why is it the visual representation important?",
            "Do with that graphical model, allow us to.",
            "I answer question about independence of random variables by just looking at the structure of the graph and therefore without requiring complex algebraic manipulation and in machine learning and then related discipline when we have a probabilistic model of something we normally have to deal with a huge number of variables so.",
            "Algebraic manipulation of this variable is often very, very complicated.",
            "Graphical model.",
            "Also define are useful because they enable us to define general algorithms that perform efficiently inference problems.",
            "As a consequence of that, we can say that graphical models provided us with a framework, a common framework from representing and understanding the properties of probabilistic models and enable us to relate models which were developed independently in different communities, such As for example, statistics, physics, machine learning and control theory and engineering another.",
            "Many other communities and they have also accelerated, in my opinion, progress in modeling because they have enabled us to overcome some difficulties.",
            "There are many types of graphical models in the literature and today will focus on mainly on belief, network mark connectors and factor graphs.",
            "So Neil yesterday gave a very nice introduction about probability with our fashion, so I will go very quickly through through it."
        ],
        [
            "We have seen Bayes rule already, so I would just remind you that we are interested in A and.",
            "We had a kind of prior belief about A and then we we observe be.",
            "We can update our belief about hey another word, compute the conditional distribution of a given B by simply multiplying our prior distribution by Theo be given a divided by POV.",
            "And I also give you a little bit of refreshing about what we mean by conditional and marginal independence between random variables.",
            "So intuitively to random variable and B are independent.",
            "If the state knowing the state of the one of the variable does not tell us anything about the state of the other variable.",
            "Informally, we can write that by saying that the probability of a given B is equal to the probability of a or equivalently.",
            "We can write down the joint distribution of A&B as in a factorized form as a distribution of hey times.",
            "The distribution would be, and similarly for conditional independence between A&B."
        ],
        [
            "Now before starting the description about graphical model, I will we need to introduce you to beat to the notation of graphs.",
            "So first of all, graphs are consist of nodes that can be undirected or directed.",
            "So here for example, I show an undirected graph, while here I show a directed graph and the path from node to another node is a sequence of connected nodes.",
            "For example.",
            "X1X2 X Three former path.",
            "Of note, which is in this case and directed an X one X5X8 form apart from X1 to X8, this part is directed.",
            "Notice that in the in the second we will use also undirected path within directed graphs.",
            "A note here.",
            "X2 is set to be a parent of a node or the node X5 because there is a direct link from X2 to expire and similarly X5 is set to be a child of X2.",
            "X1 is an ancestor of X AX8.",
            "Since there is a direct path starting from X1, lending at X 8X1 on the other hand is not an ancestor of exists because the link here is using the wrong direction.",
            "And then similarly exceed this cause to be a descendant of X5 or X Quan because there is yeah path which and attack say an startup extra 5 or Explorer.",
            "The last definition we need is definition of a directed acyclic graph.",
            "A graph is said.",
            "I cyclic if by following the direction of the arrow and node will be never be visited twice, so this one is an example of a directed graph."
        ],
        [
            "This one I sorted directed acyclic graph and this is an example of a directed cyclic graph.",
            "Because we added this link here from X42X5.",
            "So for example the.",
            "The path X 5X7 X 4X5 will visit the node X5 twice."
        ],
        [
            "OK, so when we are ready to define our first type of graphical model, namely they believe Nectar, I believe nature is directed acyclic graph in which each node has associated the probability distribution of the conditional distribution of that node given its parents.",
            "So for example, if you look at the graph here.",
            "And we look a node EE as parent B&C and for this node will have associated the conditional distribution of Y given B&C.",
            "And the joint distribution of all the nodes in the graph is simply obtained by taking the product or all these conditional distribution."
        ],
        [
            "OK, now I give you a little bit of an example of how to use belief network and let's assume that Sally Burglar alarm is sounding.",
            "Was it the burglar or boost alarm triggered by an earthquake solid turn the radio on for news of next week.",
            "So in this problem we have 4 random variable and BEAR.",
            "Representing the alarm the burglar, the earthquake and the radio, and if we start writing down the joint distribution of this variable.",
            "We can certainly condition the distribution of the on the other variable here and multiply by the joint distribution on our EMV.",
            "This will learn how to do that yesterday, and similarly for this term.",
            "Here we can condition are on ENB and multiply by the distribution eunbi and similarly for the last time.",
            "So we obtain a factorization of our initial distribution.",
            "And the belief net to representation of this is given here, in which for example a as a spare and TB&R an this correspond to the term that we see here are as as parent E. And B and this correspond to this term here and so on.",
            "Now, though, if we think about this problem a bit more carefully, we realize that.",
            "The alarm certainly is not causing any any broadcasting on the radio and therefore we can say that.",
            "The radio is not causing.",
            "In the alarm and in other words, would this distribution here is independent.",
            "The A is independent of art and therefore we can remove."
        ],
        [
            "This link from our graph, the link from R2 to a similar with a similar and we can certainly understand that.",
            "The burglar is not causing the derange and therefore we can remove this.",
            "Variable."
        ],
        [
            "And the associated link in our monograph, and finally, also the burglar is not directly causing the earthquake and therefore we."
        ],
        [
            "Can remove the final link from B2K and we end up with a quite much simpler type of belief network.",
            "That I wrote down here.",
            "Now we can use this belief network to make inference."
        ],
        [
            "For example, we can ask before actually doing that.",
            "We need to specify what the table entries are for each conditional distribution that we have in our factorization.",
            "So for example, we have to define with the probability of a given B&D is and certainly if the the burglar was in the House and the most an earthquake, then the alarm, the probability that alarm would be sounding is very high.",
            "On the other hand, if these two events do not occur, then the probability that the alarm is sounding is very low and.",
            "And similarly for this order trip table entry we we can we have to specify all this distribution setting and then the remaining tables has the probability of our root here which are very very low.",
            "The probability of the bargarran.",
            "The earthquake are very low."
        ],
        [
            "A priority or no, if we now know that alarm is sounding, we can ask what is the probability that Viagra was in the House given that the alarm is sounding and we can find out the probabilities very high.",
            "If we now have some additional evidence to that radio broadcast, an earthquake warning, then we can do a similar calculation and find out that the probability now that there wasn't a bug and is very low.",
            "So we started from.",
            "Quite high probability and then this additional evidence enable us to.",
            "To understand that the probability that a bargain was in the House is very, very small."
        ],
        [
            "Now I have said that.",
            "Ethical models are useful because they enable us to understand dependence independence.",
            "This relationship by just looking at the graph in the next few slides I will explain to you how to do that.",
            "Let's consider the very simple case of which we have three nodes.",
            "And three random variable an BNC an if we add a link between all the variables then we cannot say anything about independence about this variable.",
            "So we have to drop some link in the graph.",
            "Let's drop the link for example from A to B.",
            "And consider all the possible configurations all the possible we leave nectar which result from a different orientation of the arrow.",
            "I have listed the for all four of them here.",
            "If we look at the 1st three, what we can infer is that the variable A&B are conditionally independent, conditionally independent, given the variable.",
            "See that I hear indicate with a yellow color.",
            "On the other hand, if I look at this graph here, A&B are not conditionally independent given C. Well, first of all, how do I show that?",
            "For the first 3, how do I show the MBR independent given C, while I can simply write down, for example, for the first bill if an actor?",
            "The distribution of AMB given see, which is written as a ratio of the joint distribution ABC given by divided by the distribution of C. Now I can use the authorization that I read out from from the mill if nectar C as not apparent from.",
            "So I have to put the term here.",
            "Pfc then be as apparency.",
            "That's why I put the term pob given C&A's apparent see.",
            "So I need a term a given see, now we can see that these two terms cancel out and therefore I obtain the independence an I can you can show for the order to belief net or in similar way why they are independent.",
            "And similar with a similar reasoning, we can use to show that A&B are not independent given C in the sense that in general.",
            "Did the associated distribution.",
            "Well, for most of the associated distribution A&B will be dependent, but there might be cases in which of particular distribution for which A&B will be indeed independent given so we have to.",
            "This is a kind of a sub point that we have to understand about graphical models when we can really infer independence about random variable, but we truly cannot.",
            "In fact, through through dependencies, we can just say that A&B are not independent given C."
        ],
        [
            "Now, if I consider the case in which I don't condition oversee, then.",
            "This in the first 3 cases A&B become independent in the sense they explained before, while in the fourth case A&B become independent."
        ],
        [
            "Basically what I have told you in this to slide is that if C as more than one incoming link, then A&B will be independent but they will be independent when I condition and see on the other end if C has at most one incoming link then A&B will be conditionally independent given see, but it will be marginally dependent, so the first case here is called.",
            "Either in the second case is called an on Collider, so they collide, and Collider seems to be behaving a kind of opposite.",
            "Way."
        ],
        [
            "Or less than look at the summit complicated example in which we have both a Collider anonang Collider in the path between our variable of interest.",
            "So here for example, we have for variable and we want to see if A and B&B are independent.",
            "Given BSL always indicate that I'm conditioning over this variable.",
            "Since there is a Collider in the path and I'm conditioning on it, that means that I I maintain dependencies and I expressed that by this undirected link.",
            "Here on the other end, since I have no Collider.",
            "In the path.",
            "That means that this.",
            "The link will be cutted here and so the path from A to D will be cut and means that A&D will be independent given B&C.",
            "So what we see is if there is.",
            "And uncle, either in the condition instead.",
            "Then the path is blocked.",
            "On the other end, let's look at the same graph in which though I don't condition on VNC.",
            "This we in this case.",
            "The non collided here will maintain dependencies while the Collider will cut dependencies and again the path between A&D will be blocked, so we can infer another rule.",
            "That is, we find the Collider outside the condition inside.",
            "Then the path will be blocked.",
            "Now let's look at the case in which we have more than one path.",
            "So it is a bit more complicated.",
            "Like the case here.",
            "While this is quite simple becausw here I have an uncle either which is in the condition instead because it's yellow and this dependencies therefore will be cutted.",
            "On the other end, here I have an Uncle Collider which is not in the condition insect and therefore will cut the dependencies.",
            "So there is no link within VNC.",
            "Therefore the variables are independent.",
            "Now in this case.",
            "We have two colliders.",
            "Here, as it is there before I have a Collider, but I don't condition on it, so the dependencies will be counted on the other end.",
            "I have a Collider in the condition in certain, so the dependencies will be maintained and in the case B&C will be.",
            "Dependent given a."
        ],
        [
            "So that's I just explain you with simple example.",
            "What is the general rule for understanding in dependencies between in belief net.",
            "Or you can read this generally sound quite complicated, but if you understand I understood the example before, you will understand it very easily."
        ],
        [
            "Just so to give you an example of how you can use this independence for modeling.",
            "Considers this belief network for for time series, so I have a graphical model of this type.",
            "Well with the set of visible variable from one 2T which represent some time series observation.",
            "It can be, for example speech waveform and a set of discrete hidden variable, which I assume that generated the observation.",
            "So the graphical model I consider is.",
            "Live nectar is pretty simple, so at each time step they even variable as only the even variable at the previous time step and the observation.",
            "As aspiring, only the hidden variable at that time set step in the in the visible variable at the previous time step.",
            "Now, one of the problems we want to to do in this type of model is for example, to ask about the probability of HT given all the observation up to time T, which is quite related to this joint distribution here.",
            "And they say this is an inference problem called filtering.",
            "We will see some example of this later.",
            "Well, I can write down the joint distribution.",
            "Certainly this way.",
            "Then I can introduce.",
            "HT minus one in this last term here and some over it, so this will be equivalent to this term.",
            "And now I look at this last term and condition HT on the order variable, and then I therefore have to multiply by the distribution of this."
        ],
        [
            "Viable now, I claim that I can remove some of the links.",
            "And if I look at this term here, they will.",
            "This will become our simple term.",
            "In the in the belief that or and if I look also this term here, it will become the simple term.",
            "So basically what I wrote down is recursive way of computing this joint distribution, because this is the same John distribution but at the previous time step and then I have to manipulate it with the basic distribution.",
            "So this is an efficient way to do different, so let's see why I can."
        ],
        [
            "I claim that I can remove this tooling, just look at the first case.",
            "Well, if what I'm saying is that V is given HT an booties minus one is independent all the previous all the past visible variable.",
            "Well, it's true.",
            "Becausw All path which go from any of these valuable here in the past to booty will have to pass through any of these two variable.",
            "And.",
            "Let's suppose for example, the part cost from here to here.",
            "Or we can go also from here to here.",
            "While the variable HT will never be Collider right, it is always an uncle, either in the conditioning set and we said if we find a Collider and colliding in the condition instead, that means that the parties block.",
            "So the path that goes from here is always proud.",
            "And similarly the party grows from T. With this one is one will never be.",
            "The Collider, and therefore also this path, will be blocked and you can source the same thing for the.",
            "For the other case, and the reason why I introduced this example is be cause this belief network, for example, could represent a switching auto regressive model which is a model at being used a lot in the speech community.",
            "And if you look at the literature on how they they.",
            "We came out with an inference algorithm to compute the filter distribution.",
            "It looks quite complicated.",
            "Then, while if you just look at the photograph and use the rules that I explain you, everything become quite simple and you can build even more complicated model than this and just reason on the graph to understand independence and to come out with some efficient way of computing things.",
            "OK, that was the end of an introduction to to believe NATO.",
            "Now we can start to describe another type of.",
            "Graphic."
        ],
        [
            "Model, which is the mark on nectar for which we need the definition of a click.",
            "A click is a fully connected subset of nodes.",
            "So for example BC an E form a click.",
            "Because the nodes are all connected to each other and a click is said to be maximal if is not a subset of another click.",
            "So for example is B and click BNC maximal?",
            "Any anybody can tell me?",
            "No did why because he's containing this larger click formed by the variable BC&E.",
            "OK, so my connector is an undirected graph in which there is a potential by potential and mean and negative function PSI define on each maximize click and the joint distribution of all nodes in the graph is proportional to.",
            "The product all click potentials divided by the normalization constant."
        ],
        [
            "Now I give you a simple application of a mark on actual.",
            "I suppose that we observe a corrupted version of an image and we want to recover a binary image and we want to recover it.",
            "We want to recover the clean image from observing only the corrupted version of these.",
            "We can model this by a mark, an actor whose graphical representation is given here.",
            "the Y node here represent the clean pixels, while the.",
            "The filler node represents the corrupted pixel, so each corrupted pixel as a link to the corresponding.",
            "Clean pixela means that we have a potential between the.",
            "Corrupted pixel and clean pixel, which I I'll explain later here.",
            "This is a potential and each claim pixel as connection with the neighbor pixels and therefore I will have.",
            "Click between that pixel and each of his neighbors, which I I indicate here, so I'll know this clicks this potential look like they.",
            "Reasonable to do to construct this potential such that to encourage The Dirty or the corrupted pixel to be similar to the to the clean pixel.",
            "And also we want to impose smoothness in the in the.",
            "In the in the image and that's why we use this kind of potential with some positive constant here which will enforce the variable to be close to each other.",
            "Now if we want to find the most likely clean image given the corrupting, we will find it.",
            "There is a quite difficult problem because the graph is not singly connected.",
            "I will explain actually later would is.",
            "I mean by that, but we can use some approximate method to to perform."
        ],
        [
            "On that nest omation an I give you here.",
            "One example, this is the picture or the image where I start from.",
            "And this is the image that I have recovered most likely.",
            "Clean image that I recover using this approximate inference methods and this is the original clean image where that generated my corrupted version and I can maybe show you.",
            "Some order.",
            "Example.",
            "So.",
            "There are few iterations required for the agreement to converge, so you can see that is still converging.",
            "OK, you can see in this case it didn't work as well as before.",
            "So it depends on the problem.",
            "It can be more and more or less difficult to find.",
            "A satisfactory solution."
        ],
        [
            "So now we can.",
            "Explain what how to to do to understand independence in Marco.",
            "Matter is much more simpler than is much simpler than in belief network because we have only one type of link.",
            "So let's suppose that we have this for variable.",
            "Here ABC and D and we want to understand if B is independent of C given A&D.",
            "Already know that word if the probability of B given a DC is equal to the probability of B given A&E.",
            "But we can write this conditional distribution as the ratio between our joint distribution divided by POACD.",
            "Now pervasive is simply the sum over B of our joint distribution.",
            "Now I can use the factorization of the mark on nectar.",
            "I have only clicks formed by the two variables we DDCCA&AB.",
            "And that's what I I wrote here and similar for the denominator.",
            "Now I see that this term here does not depend on B.",
            "Similarly for this term.",
            "Therefore we can cancel them and we will obtain a function which is not dependent on C. So we demonstrated that B&C are indeed independent.",
            "Given a an D, so is a pretty easy way for marconato to understand."
        ],
        [
            "And.",
            "Now this is just what I I have showed before by graphically I cutting the link every time there is.",
            "And nodes.",
            "In the path between the variable that I'm conditioning or so in this case.",
            "So there is variable here and I'm not conditioning on it, so the link will be maintained.",
            "So in order to have in dependencies, we have to again block all the path between our valuables.",
            "Is that clear?"
        ],
        [
            "So this method actually give us another way of understanding, or of inferring dependencies in belief network.",
            "I won't describe it here.",
            "You can read it later, but the main idea is to transform your belief net or into the direct."
        ],
        [
            "Graph."
        ],
        [
            "With several step and then use the independence method that I just described earlier for Markov network.",
            "And sometimes this method might be a bit easier than the method I explain.",
            "Yes, the earlier for believing that they give exactly the same result, yes, but you might sometimes prefer to use this.",
            "A bit less.",
            "Complicated to understand.",
            "OK so."
        ],
        [
            "Let's now think I have described two type of.",
            "Graphical model Now how do they relate?",
            "Can we always express?",
            "I believe nature with a marker network, the answer is no.",
            "Like let's look at this example here I have a Collider, which means that the joint distribution will factorize this way right?",
            "Because I have to root and the and the cost C which has two parents and now obviously we want to form mark connector.",
            "I will have a potential on all three variables.",
            "That means that I will have a link between.",
            "All of them.",
            "And that which means that now A&B will not be independent anymore, while in our original graph A&B were independent.",
            "Now we can also ask the opposite question, can we express any mark on network with the belief network?",
            "And the answer is again no.",
            "We look at the.",
            "Like a sample here.",
            "Well.",
            "Do I have to transform it into believing that we will have to maintain kind of the same structure so we cannot have the link here between B&C and A&E?",
            "And also to ensure that they that the model is the graphical mode is a cyclic, we have to.",
            "We would like to have a Collider.",
            "These are us to point here.",
            "If you point on the other direction we will get cyclic graph.",
            "So that means that we will end up with different set of independence independence relations there for example.",
            "The original graph B&C were independent, given Andy, because I condition on both.",
            "So I cut the link here and the cattle in here.",
            "While in this case.",
            "The variable BNC will be dependent given A&B because if you look at the path BDC here.",
            "We have a Collider right in the path and we don't condition.",
            "Is it cool?",
            "C An we do conditions.",
            "Sorry on the on the Collider did for we we have dependency.",
            "Please tell me if there is something that is not clear."
        ],
        [
            "Let's now so talk about the third type of graphical model that we want to see in this talk and namely the factor graph in a factor graph in addition to.",
            "Note we represent random variable with the square nodes.",
            "We should represent nonnegative function of his neighbor variable.",
            "So for example, a factor of 1 year represent a non negative function of A&B and then we will have here if two is a non negative function of BC&D and so on and then join function is obtained by simply the product of all these factors.",
            "So here.",
            "Photograph are mainly or use a lot to perform efficient inference as we will see later and here.",
            "Notice that I talk about function, not probability distribution because they're being used also for performing efficient computation or just in probability."
        ],
        [
            "So inference problem.",
            "Now I so far explain you out to understand.",
            "Yeah, I explain the different graphical models and how to understanding dependencies as I told you, graphical model are also use a full because they enable us to develop efficient inference algorithms.",
            "So now I'm going to explain a little bit about that.",
            "So inference correspond as we know by now, operations such as computing marginal or compute conditional distribution from the joint distribution in general, as a inference is a very difficult problem.",
            "Also because we had on machine learning many many variable in graphical model normally.",
            "However, for single connected type of graph.",
            "There exist sufficient argument based on the concept of message passing.",
            "With whom do I mean by singly connected graph, a single connected graph is a graph in which there is only a path from a node to another node.",
            "Can you tell me if this part is simply connected?",
            "Yes, because I wrote it there right so that.",
            "OK, so if I add the link from D2."
        ],
        [
            "We will obtain multiple sorry from E2D multiply connected graph instead.",
            "Because there is for example apart from me to the just simply going this direction or I can go so from here to the passing through G so there are two paths."
        ],
        [
            "Let's discuss about this.",
            "How to perform inference?",
            "In photographs and the concept of message passing, let's suppose that we have.",
            "For binary variable ABC and D and our distribution is factorized this way, that means that I will have a photograph representation here where F1 connecting B and therefore I have a factor for this and so on right?",
            "And here I have only.",
            "Connection to this so I will have only factor that is a function only on the variable D. Now let's suppose that I want to compute the marginal distribution of a well.",
            "This is the sum over all the remaining variable of the joint distribution.",
            "No, I can write down my factorization of the photograph.",
            "And perform the submission and doing that.",
            "We realize that I will need to perform 2 to the power three summations.",
            "That means a number of summation which is exponentially in the number of variable.",
            "I have to sum over.",
            "No, so we can think a bit more clever way of doing that.",
            "So if we look for example this first term, it does not depend on D. That means that also this second term does not depend on Monday.",
            "That means that we can push this summation here before the three factor and."
        ],
        [
            "Similarly for the other summation, so I will end up that I can distribute and push the summation further.",
            "Now, if I look I have to compute here to sum over the van and we said this binary, so I've had to some here plus 2 something here plus two sums here.",
            "That means that we have 2 * 3 submissions and that means I have a number of summation which is now linear in the in the number of variable add to some.",
            "So we we gain something if we gave only two in this case but.",
            "You can imagine that if you have a bigger graph then you can see."
        ],
        [
            "A lot of computations.",
            "Well, we can see this procedure at the kind of message passing.",
            "So what do we can define here is, uh, this term as a message from the variable D to the variable C. And analogously for from here then.",
            "So I can define this term as a message from the variable C to the variable B.",
            "And finally, this as a message from B2A.",
            "So basically what I."
        ],
        [
            "I can view this as passing messages from the variable D up to the variable A."
        ],
        [
            "Now, if you think about if you have to compute the joint, they'd imaginal scene.",
            "Instead we will have to pass messages in in in both directions.",
            "Coming coming from this.",
            "So from hold the factor that have a link to see in this case there are two factors that will link to see, so I have to pass messages from here and also messages to.",
            "Here I can I can see that.",
            "Similar way."
        ],
        [
            "Um?",
            "No, in the more complicated case in which I have a bunching three, that means, for example, you're a factory is the function of three nodes.",
            "Things get a bit more complicated than I have, is not sufficient to define variable to variable messages as before, I'm to define factor to variable message and the variable to factor message and."
        ],
        [
            "Let's see how you define these kind of messages while.",
            "Let's look at the variable to factor message.",
            "If I have a variable here, I want to compute the message to from the variable V to the factor F, while in order to do that.",
            "I have to multiply the factor.",
            "Coming the messages from the factor.",
            "Which are of a link with the variable be so F1 as a link to V F2 as a link to be I to compute the message from front to V, the message from a 22V, multiply them and then I can send.",
            "I obtain the message from B2F.",
            "Is that clear?",
            "Now.",
            "If I want to send.",
            "A message from a factor to a variable, so from F2 to V. I will have to again compute the messages from.",
            "In this case we want to F2 to F. So all the messages from the other variable to the factor, except obviously that the message from me to have.",
            "Multiply them.",
            "Multiply by then this.",
            "That's what I I right here.",
            "Then multiply these two by the factor.",
            "And then sum over the two variable here.",
            "And you can see that would."
        ],
        [
            "Basically we are soon.",
            "Here is exactly the same and you can take a look at that more closely."
        ],
        [
            "Later in the slide.",
            "And then if you want to compute the marginal as we said, we have to just take the product of all factor to variable message.",
            "For the factor that link to that."
        ],
        [
            "I was.",
            "Now let's give a little bit of an example of inference in hidden Markov model, which is a very popular model in many disciplines and is a little extension of the of the belief network.",
            "We have said yes that we have seen earlier and.",
            "I consist of a set of.",
            "Visible variable one would be that represent our observation.",
            "For example the speech waveform, and then we have a set of hidden variable.",
            "Which are discrete?",
            "And I already, yeah, explain this.",
            "Basically the hidden variable at time T as only one parent and also the.",
            "In this case I don't have a link here and the visible variable depend on HT only, so it's a special case overseen before an Indian market model.",
            "Normally we assume that the transition from hidden variable to another in variable is time homogeneous and we can represent that by a matrix by and in this.",
            "In this example I'm going to show later.",
            "I will also assume that the observation are discrete and we can therefore.",
            "Summarize this table entries with the emission mattress role.",
            "Now three of the very common problem inference problem hidden Markov model.",
            "Inferring HD given only the observation up to time T which is called filtering or inferring HT from the distribution of HT given all the information.",
            "Also, the future observations and the third problem is to infer the most likely even sequence from basically maximizing the conditional distribution of age.",
            "From one to T given all the observations."
        ],
        [
            "So if you want to do this inference, we can just one way of doing it is, well, we can either reason as I explained before.",
            "Or you can use the dim after the photograph matter that I explain.",
            "In the previous life.",
            "So if we look at the HMM representation as a factor graph, we looked like that where for example we have a factor F-11 here, which is a function of the observation at time one and Aiden variable at time T. And then we need another factor F-12, which is a function of H1 and two.",
            "And then we can just use the argument I explain you before to to compute.",
            "There to perform inference."
        ],
        [
            "Now let's look at an example.",
            "So let's suppose that you are asleep upstairs and.",
            "In your house, so you are walking by a burglar on the ground floor and we want to understand where the burglar is by.",
            "Just based on a sequence of noise information that you have so you are, you can do that using an HMM and I'm going to explain you are you mentaly partition the floor into?",
            "A 5 by 5 grid and for each position you know the probability that someone if someone is in that position, the floorboard will click and this is what I show here.",
            "So we like Olori indicated probability 0.9 that if the burglar is in that position, the floor will create.",
            "And if the border is here, on the other hand, there is probability 0.1 that the floorboard will Creek.",
            "Now you also know the probability that if someone is in a certain position in the grid, it will bump into something in the dark, and that's what I show here.",
            "Scenarios assume that the burglar can move only one in one of the great position.",
            "You can move only one of the neighbors with equal probability."
        ],
        [
            "OK, so as I said, we can represent this problem as.",
            "An item in problem.",
            "Where the hidden variable represent the position of the burglar in the grid at time T, so the variability take take 25 value values and the visible variable Vt represent the currencies of creaks and bumps.",
            "So for example, if the variable we take values zero, that means that there was no Creek, no bomb.",
            "If it takes value one, that means that there was a Creek and Obama and so on, and we will have four for value."
        ],
        [
            "As for this.",
            "OK, so let's see what happened here with an example.",
            "Here I give you.",
            "The sequence of creaks and bumps for 10 time step.",
            "The light color indicate the occurrence of a Creek.",
            "On the on the left side on the right side the occurrence of of the bond.",
            "So we have this sequence and this is the position of the burglar.",
            "Here the last.",
            "Throw in the at each time step.",
            "On the second line here with the show is the filtering.",
            "So now you could use.",
            "So the distribution of HT given the information up to time T you can.",
            "Use that one the burglar is in the house.",
            "You just.",
            "Get what is a set of of probable or I probable positions for the burglar, while is there and at the beginning maybe you don't have a lot of information so you will not sure where the burglar here is, by the way, but with why I indicated high probability that the burglar is in the position.",
            "So here for example there are many possible position where the burglar can be.",
            "I am in the beginning so I don't have much information.",
            "While going ahead with time I I acquire more and more information and become more certain about where the burglar can be.",
            "This one show is booting instead and you can think of it.",
            "I mean, maybe the police can use that when.",
            "The police arrive.",
            "You can give all the information about the noise and then it can figure out the means.",
            "The observation up to time T and then the police can.",
            "Figure out the probable position for each.",
            "The burglar at each time step.",
            "All the police can do is also to compute the most likely path.",
            "They'll most likely sequence of position for each time step, which is what I show in the in the fourth line here.",
            "And again, maybe I if I time I show you then would later, but in this case you see can work quite well there.",
            "It depends a lot on the problem.",
            "There are cases in which in my work less well."
        ],
        [
            "I give you now another example of modeling with an HMM.",
            "Let's suppose that we have established finger typist that has a tendency to.",
            "Either it the correct key on a or enabling key in the keyboard and based on a type of sequence you want to infer.",
            "What is the most likely war.",
            "This sequence correspond to, and again we can represent that with a hidden Markov model in which HT now is the intended letter at time T. Anne Moody represent a letter that was actually type at time T. As a transition for.",
            "The hidden variable we can use just a standard transition matrix of the of the English language.",
            "So probably you don't see there, but for example is very highly probable that the letter you will follow the letter Q on that whole time is indicated here with the light color.",
            "And you also assume that the typist will probably type the correct key with high probability.",
            "That would show in this diagonal here.",
            "But there is a probability that it will type some of the key enabling key."
        ],
        [
            "So again, if you can do kind of Viterbi decoding, and given that the typists type with this sequence here, what is the most likely wore this correspond to while you can list the site of most likely even sequences using us said kind of beat urban, then disregard those that don't correspond to the sentence in the English dictionary.",
            "And then take the most proper English word as as intended word and anybody can tell me what the answer can be.",
            "Many who stand learning.",
            "Exactly, yeah, that's."
        ],
        [
            "OK so I still have think 20 minutes or.",
            "So.",
            "So far I have described.",
            "Several typographical models and how to do inference with that.",
            "How to understand independence, but I have always assumed that we knew the table entries in our distribution so.",
            "For example, in the case of the HMM, I assume that I knew the.",
            "That we knew the transition or emission probability.",
            "Now I want to discuss a little bit how to to learn this table entries and therefore suppose that we have a model of random variable X, which is the which depends on unknown parameter Theta.",
            "And from a set of observation I want.",
            "Two, infer something about system.",
            "Well, I assume that this observation are independent in identically distributed according to this model, and therefore if you can see that now the joint distribution, all our observation given the parameter theater, I can write it as a product of simple terms here.",
            "And I can also.",
            "Introduce a prior distribution or prior preference of my parameter feta and then with Bayes rule as we have seen, I can infer with the posterior of Theta is given my observations.",
            "And I can view this as an extension of our belief net or by simply consider an additional node in my belief net through which as link to all the observations.",
            "And I can also use a more compact notation here, in which I have just one sketch.",
            "The link from theater to one of the side, and I have a plate.",
            "In pink, that indicated I repetition of the same structure and times."
        ],
        [
            "If we want to summarize the our posterior to very popular way of doing it is the maximum a posteriori method which simply compute the mode of the posterior.",
            "Or the maximum likely matter?",
            "The near talk a lot about yesterday, which is a special case in which the prior is omitted, or equivalently, a flat prior on our parameter is imposed."
        ],
        [
            "Let's see a simple example of.",
            "Learning in a belief network call, basic plastic naive Bayes classifier.",
            "This is a very very popular method and the reason why is very popular is becausw learning in this method is very very simple.",
            "Let's see what this model is about.",
            "So is a classifier as it said.",
            "So we have a random variable, see which indicate the class which.",
            "For example, if you consider the problem of spam filtering, it could indicate that animal is spam or ham.",
            "And then we have said to attribute XI, which again in the case for example of spam filtering could indicate the presence or absence of a certain keyword in the email.",
            "Team the belief net representation of this model is given here.",
            "So we assume that the.",
            "The class influence directly each attribute, but all the attributes given the class are independent, so we can write the joint distribution into this simple form and then given our attributes we can infer with the class is.",
            "And how do we?"
        ],
        [
            "Lando, the table of this.",
            "From a set of Satan said, let's suppose that we call theater high the parameter associated to this table entries table entries, or the attribute XI.",
            "Given the Class C. Similarly for the class.",
            "Basically, if we assume that all these parameters are dependent, I can write a will live nectar, in which there is a link for each parameter to the associated.",
            "Random variable.",
            "And now we can use the base.",
            "Approach in which a Bayesian approach in which we compute the posterior work.",
            "We can summarize the posterior with the mode."
        ],
        [
            "Let's look at the simple case of a Maxim.",
            "Like you know if.",
            "We while we can write down the log likelihood as.",
            "The log of the product over all the observation.",
            "Now we know that how to split the joint distribution and if we consider, for example that we want to learn the parimeter theater IO which indicate the probability that the attribute excised zero given the class is 0.",
            "Then if we look at the dependencies of the log likelihood on data IO, this simply given by this.",
            "This term here where and zero, for example, indicate the number of times that C = 0 and X I = 0.",
            "Of course in our data.",
            "So that's the other term.",
            "Do not depend on this parameter here, so we can disregard them.",
            "Then by differentiated in Sturm with respect to Ty Ty Jay and setting the this the derivative to zero, we can obtain a value for our tee time zero which if we see corresponds simply to calculating frequencies so.",
            "Anne.",
            "This might be a little bit of a problem if, for example, this N 0 is 0.",
            "So we don't have any data count and we can improve on this model by using a Bayesian approach in which."
        ],
        [
            "Now we put some prior distribution on the.",
            "On the parimeter and such that this pathological case will not create problems.",
            "I won't describe that.",
            "Maybe it is you can."
        ],
        [
            "Edit later.",
            "Now let's discuss a little bit.",
            "Learning in Markham network.",
            "Now if you look at the log likelihood here as again I can split it over the observation, then I use the definition of the joint distribution.",
            "Applying it here, I take the log.",
            "The log of the product is the sum log, blah blah blah and would I end up with is term here log theater theater that I as I see.",
            "Does not factorize.",
            "Is not.",
            "I cannot split it into.",
            "Isolated the terms which depend on the single parameter Theta.",
            "That means that we cannot have a simple update form.",
            "As for learning data, we have to use numerical."
        ],
        [
            "Methods.",
            "So far I have discussed yeah learning in the case in which all the viable are absurdly now see very briefly the case in which I also have some even variable in the model, and suppose that I have.",
            "A graphical model, this this form I have 1 hidden variable H and the visible variable V. And I want to learn again.",
            "I have a parameter Theta it's associated to the variable H and a parameter T to be associated to the variable V. And now I want to learn Theta agent into V from the observations.",
            "I said observations.",
            "We can.",
            "Look at the maximum likelihood.",
            "And again, split it into factorizes into simple forms.",
            "Now, if I look at the form of POVI given Theta is the sum over hi of this term here.",
            "Right, but this submission, though we couple the para meters and that means that again we had to also with with the the case before we need to use numerical methods to perform learning in this well.",
            "Similarly for the base in case the posterior of the para meters will not a couple anymore and we will need to use some approximate metalform."
        ],
        [
            "Solving this problem.",
            "No, I'm reaching the end.",
            "I think this is one of my my last slides.",
            "I want to describe little bit about expectation optimization in market for maximum likelihood.",
            "So as we said we cannot use.",
            "We had this problem of this coupling and so the trick.",
            "One of the trick that we can use is to replace the log likelihood who the bound that as as the couple form.",
            "So in order to do that we can introduce a distribution Q and take the kullback, Leiber divergent between.",
            "The QH given V and my original posterior distribution P of H. Given B&P to know if I write down the KL divergent swear, I used as near the angle notation to denote expectation with respect to this distribution.",
            "Here, from the property of the KL divergent is always greater equal to zero.",
            "I can obtain abound on the likelihood here.",
            "I've found that will have an improper turn and some other time here that depend on my parameter.",
            "Now if I said Q of H given V, Theta, yes, I think I forgot the theater here there, but.",
            "Hey Court, 2 hour.",
            "Original distribution posterior distribution.",
            "But where I have a whole parameter here then this entropy term here.",
            "Does not depend on Theta.",
            "And therefore we have only this term that the penalty to burnout weakened a couple this term very easily.",
            "Into and split it into terms such that, Tita, we appears only in this term.",
            "Here entity each appearing only this term here.",
            "So now we have a bounded task, the couple form and learning or maximizing this bound.",
            "Now with respect to either Theta V or Theta H is easy.",
            "We so doing that we obtain kind of iterative.",
            "Great call expectation organization in which we have any step in which we need to compute the posterior distribution of H given V an theater, all using some inference method and once we have compute this that I need, we need to maximize the bound.",
            "I can maximize the bound with respect to Theta and then update my para meter."
        ],
        [
            "And I think yes, that's the end of my talk a bit earlier than expected, I think.",
            "So this is a little bit of reading.",
            "You can live there all books.",
            "The first one will be published soon.",
            "An nearly all example and then most I showed today where taken from this book you can download it online and I also have some copies.",
            "If you order chapters containing the material, explain today here if you want to take a look and this is another book written by machine learning person.",
            "But you certainly know, which is also quite quite simple to understand, and then these are more.",
            "Yes, they also interesting book, but written by statisticians.",
            "So yes, other people in the field.",
            "OK, that's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can start but before starting I would like to thank the European Community for supporting me through Mercury into European fellowship.",
                    "label": 0
                },
                {
                    "sent": "So in this presentation will give you an introduction to graphical models.",
                    "label": 1
                },
                {
                    "sent": "So let's start by defining graphical models and give a little.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Date of motivation or why we want to do to know about them.",
                    "label": 0
                },
                {
                    "sent": "So graphical models are graphs which contain links and nodes and nodes represent random variable while links represent statistical dependencies between the variables.",
                    "label": 1
                },
                {
                    "sent": "So graphical model give us provide us with.",
                    "label": 1
                },
                {
                    "sent": "Visual tool for reasoning under uncertainty.",
                    "label": 1
                },
                {
                    "sent": "And as we know, machine learning and related disciplines which are concerned with modeling or the word or extracting information about data.",
                    "label": 0
                },
                {
                    "sent": "Uncertainty arises mainly from two sources and our limited understanding of the forward and the limited amount of data that is available to us.",
                    "label": 0
                },
                {
                    "sent": "Therefore, having methods which account for uncertainty is very important.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But why is it the visual representation important?",
                    "label": 0
                },
                {
                    "sent": "Do with that graphical model, allow us to.",
                    "label": 0
                },
                {
                    "sent": "I answer question about independence of random variables by just looking at the structure of the graph and therefore without requiring complex algebraic manipulation and in machine learning and then related discipline when we have a probabilistic model of something we normally have to deal with a huge number of variables so.",
                    "label": 1
                },
                {
                    "sent": "Algebraic manipulation of this variable is often very, very complicated.",
                    "label": 0
                },
                {
                    "sent": "Graphical model.",
                    "label": 1
                },
                {
                    "sent": "Also define are useful because they enable us to define general algorithms that perform efficiently inference problems.",
                    "label": 1
                },
                {
                    "sent": "As a consequence of that, we can say that graphical models provided us with a framework, a common framework from representing and understanding the properties of probabilistic models and enable us to relate models which were developed independently in different communities, such As for example, statistics, physics, machine learning and control theory and engineering another.",
                    "label": 1
                },
                {
                    "sent": "Many other communities and they have also accelerated, in my opinion, progress in modeling because they have enabled us to overcome some difficulties.",
                    "label": 0
                },
                {
                    "sent": "There are many types of graphical models in the literature and today will focus on mainly on belief, network mark connectors and factor graphs.",
                    "label": 0
                },
                {
                    "sent": "So Neil yesterday gave a very nice introduction about probability with our fashion, so I will go very quickly through through it.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have seen Bayes rule already, so I would just remind you that we are interested in A and.",
                    "label": 0
                },
                {
                    "sent": "We had a kind of prior belief about A and then we we observe be.",
                    "label": 0
                },
                {
                    "sent": "We can update our belief about hey another word, compute the conditional distribution of a given B by simply multiplying our prior distribution by Theo be given a divided by POV.",
                    "label": 0
                },
                {
                    "sent": "And I also give you a little bit of refreshing about what we mean by conditional and marginal independence between random variables.",
                    "label": 0
                },
                {
                    "sent": "So intuitively to random variable and B are independent.",
                    "label": 0
                },
                {
                    "sent": "If the state knowing the state of the one of the variable does not tell us anything about the state of the other variable.",
                    "label": 0
                },
                {
                    "sent": "Informally, we can write that by saying that the probability of a given B is equal to the probability of a or equivalently.",
                    "label": 0
                },
                {
                    "sent": "We can write down the joint distribution of A&B as in a factorized form as a distribution of hey times.",
                    "label": 0
                },
                {
                    "sent": "The distribution would be, and similarly for conditional independence between A&B.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now before starting the description about graphical model, I will we need to introduce you to beat to the notation of graphs.",
                    "label": 0
                },
                {
                    "sent": "So first of all, graphs are consist of nodes that can be undirected or directed.",
                    "label": 1
                },
                {
                    "sent": "So here for example, I show an undirected graph, while here I show a directed graph and the path from node to another node is a sequence of connected nodes.",
                    "label": 1
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "X1X2 X Three former path.",
                    "label": 0
                },
                {
                    "sent": "Of note, which is in this case and directed an X one X5X8 form apart from X1 to X8, this part is directed.",
                    "label": 0
                },
                {
                    "sent": "Notice that in the in the second we will use also undirected path within directed graphs.",
                    "label": 1
                },
                {
                    "sent": "A note here.",
                    "label": 0
                },
                {
                    "sent": "X2 is set to be a parent of a node or the node X5 because there is a direct link from X2 to expire and similarly X5 is set to be a child of X2.",
                    "label": 1
                },
                {
                    "sent": "X1 is an ancestor of X AX8.",
                    "label": 1
                },
                {
                    "sent": "Since there is a direct path starting from X1, lending at X 8X1 on the other hand is not an ancestor of exists because the link here is using the wrong direction.",
                    "label": 0
                },
                {
                    "sent": "And then similarly exceed this cause to be a descendant of X5 or X Quan because there is yeah path which and attack say an startup extra 5 or Explorer.",
                    "label": 0
                },
                {
                    "sent": "The last definition we need is definition of a directed acyclic graph.",
                    "label": 0
                },
                {
                    "sent": "A graph is said.",
                    "label": 0
                },
                {
                    "sent": "I cyclic if by following the direction of the arrow and node will be never be visited twice, so this one is an example of a directed graph.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This one I sorted directed acyclic graph and this is an example of a directed cyclic graph.",
                    "label": 1
                },
                {
                    "sent": "Because we added this link here from X42X5.",
                    "label": 0
                },
                {
                    "sent": "So for example the.",
                    "label": 0
                },
                {
                    "sent": "The path X 5X7 X 4X5 will visit the node X5 twice.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so when we are ready to define our first type of graphical model, namely they believe Nectar, I believe nature is directed acyclic graph in which each node has associated the probability distribution of the conditional distribution of that node given its parents.",
                    "label": 1
                },
                {
                    "sent": "So for example, if you look at the graph here.",
                    "label": 0
                },
                {
                    "sent": "And we look a node EE as parent B&C and for this node will have associated the conditional distribution of Y given B&C.",
                    "label": 1
                },
                {
                    "sent": "And the joint distribution of all the nodes in the graph is simply obtained by taking the product or all these conditional distribution.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now I give you a little bit of an example of how to use belief network and let's assume that Sally Burglar alarm is sounding.",
                    "label": 0
                },
                {
                    "sent": "Was it the burglar or boost alarm triggered by an earthquake solid turn the radio on for news of next week.",
                    "label": 1
                },
                {
                    "sent": "So in this problem we have 4 random variable and BEAR.",
                    "label": 0
                },
                {
                    "sent": "Representing the alarm the burglar, the earthquake and the radio, and if we start writing down the joint distribution of this variable.",
                    "label": 0
                },
                {
                    "sent": "We can certainly condition the distribution of the on the other variable here and multiply by the joint distribution on our EMV.",
                    "label": 0
                },
                {
                    "sent": "This will learn how to do that yesterday, and similarly for this term.",
                    "label": 0
                },
                {
                    "sent": "Here we can condition are on ENB and multiply by the distribution eunbi and similarly for the last time.",
                    "label": 0
                },
                {
                    "sent": "So we obtain a factorization of our initial distribution.",
                    "label": 0
                },
                {
                    "sent": "And the belief net to representation of this is given here, in which for example a as a spare and TB&R an this correspond to the term that we see here are as as parent E. And B and this correspond to this term here and so on.",
                    "label": 0
                },
                {
                    "sent": "Now, though, if we think about this problem a bit more carefully, we realize that.",
                    "label": 1
                },
                {
                    "sent": "The alarm certainly is not causing any any broadcasting on the radio and therefore we can say that.",
                    "label": 0
                },
                {
                    "sent": "The radio is not causing.",
                    "label": 0
                },
                {
                    "sent": "In the alarm and in other words, would this distribution here is independent.",
                    "label": 0
                },
                {
                    "sent": "The A is independent of art and therefore we can remove.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This link from our graph, the link from R2 to a similar with a similar and we can certainly understand that.",
                    "label": 0
                },
                {
                    "sent": "The burglar is not causing the derange and therefore we can remove this.",
                    "label": 0
                },
                {
                    "sent": "Variable.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the associated link in our monograph, and finally, also the burglar is not directly causing the earthquake and therefore we.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can remove the final link from B2K and we end up with a quite much simpler type of belief network.",
                    "label": 0
                },
                {
                    "sent": "That I wrote down here.",
                    "label": 0
                },
                {
                    "sent": "Now we can use this belief network to make inference.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, we can ask before actually doing that.",
                    "label": 0
                },
                {
                    "sent": "We need to specify what the table entries are for each conditional distribution that we have in our factorization.",
                    "label": 0
                },
                {
                    "sent": "So for example, we have to define with the probability of a given B&D is and certainly if the the burglar was in the House and the most an earthquake, then the alarm, the probability that alarm would be sounding is very high.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if these two events do not occur, then the probability that the alarm is sounding is very low and.",
                    "label": 0
                },
                {
                    "sent": "And similarly for this order trip table entry we we can we have to specify all this distribution setting and then the remaining tables has the probability of our root here which are very very low.",
                    "label": 0
                },
                {
                    "sent": "The probability of the bargarran.",
                    "label": 0
                },
                {
                    "sent": "The earthquake are very low.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A priority or no, if we now know that alarm is sounding, we can ask what is the probability that Viagra was in the House given that the alarm is sounding and we can find out the probabilities very high.",
                    "label": 1
                },
                {
                    "sent": "If we now have some additional evidence to that radio broadcast, an earthquake warning, then we can do a similar calculation and find out that the probability now that there wasn't a bug and is very low.",
                    "label": 1
                },
                {
                    "sent": "So we started from.",
                    "label": 0
                },
                {
                    "sent": "Quite high probability and then this additional evidence enable us to.",
                    "label": 0
                },
                {
                    "sent": "To understand that the probability that a bargain was in the House is very, very small.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I have said that.",
                    "label": 0
                },
                {
                    "sent": "Ethical models are useful because they enable us to understand dependence independence.",
                    "label": 0
                },
                {
                    "sent": "This relationship by just looking at the graph in the next few slides I will explain to you how to do that.",
                    "label": 0
                },
                {
                    "sent": "Let's consider the very simple case of which we have three nodes.",
                    "label": 1
                },
                {
                    "sent": "And three random variable an BNC an if we add a link between all the variables then we cannot say anything about independence about this variable.",
                    "label": 0
                },
                {
                    "sent": "So we have to drop some link in the graph.",
                    "label": 0
                },
                {
                    "sent": "Let's drop the link for example from A to B.",
                    "label": 0
                },
                {
                    "sent": "And consider all the possible configurations all the possible we leave nectar which result from a different orientation of the arrow.",
                    "label": 0
                },
                {
                    "sent": "I have listed the for all four of them here.",
                    "label": 0
                },
                {
                    "sent": "If we look at the 1st three, what we can infer is that the variable A&B are conditionally independent, conditionally independent, given the variable.",
                    "label": 1
                },
                {
                    "sent": "See that I hear indicate with a yellow color.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if I look at this graph here, A&B are not conditionally independent given C. Well, first of all, how do I show that?",
                    "label": 0
                },
                {
                    "sent": "For the first 3, how do I show the MBR independent given C, while I can simply write down, for example, for the first bill if an actor?",
                    "label": 0
                },
                {
                    "sent": "The distribution of AMB given see, which is written as a ratio of the joint distribution ABC given by divided by the distribution of C. Now I can use the authorization that I read out from from the mill if nectar C as not apparent from.",
                    "label": 0
                },
                {
                    "sent": "So I have to put the term here.",
                    "label": 0
                },
                {
                    "sent": "Pfc then be as apparency.",
                    "label": 0
                },
                {
                    "sent": "That's why I put the term pob given C&A's apparent see.",
                    "label": 1
                },
                {
                    "sent": "So I need a term a given see, now we can see that these two terms cancel out and therefore I obtain the independence an I can you can show for the order to belief net or in similar way why they are independent.",
                    "label": 0
                },
                {
                    "sent": "And similar with a similar reasoning, we can use to show that A&B are not independent given C in the sense that in general.",
                    "label": 0
                },
                {
                    "sent": "Did the associated distribution.",
                    "label": 0
                },
                {
                    "sent": "Well, for most of the associated distribution A&B will be dependent, but there might be cases in which of particular distribution for which A&B will be indeed independent given so we have to.",
                    "label": 0
                },
                {
                    "sent": "This is a kind of a sub point that we have to understand about graphical models when we can really infer independence about random variable, but we truly cannot.",
                    "label": 0
                },
                {
                    "sent": "In fact, through through dependencies, we can just say that A&B are not independent given C.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, if I consider the case in which I don't condition oversee, then.",
                    "label": 0
                },
                {
                    "sent": "This in the first 3 cases A&B become independent in the sense they explained before, while in the fourth case A&B become independent.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically what I have told you in this to slide is that if C as more than one incoming link, then A&B will be independent but they will be independent when I condition and see on the other end if C has at most one incoming link then A&B will be conditionally independent given see, but it will be marginally dependent, so the first case here is called.",
                    "label": 1
                },
                {
                    "sent": "Either in the second case is called an on Collider, so they collide, and Collider seems to be behaving a kind of opposite.",
                    "label": 0
                },
                {
                    "sent": "Way.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or less than look at the summit complicated example in which we have both a Collider anonang Collider in the path between our variable of interest.",
                    "label": 0
                },
                {
                    "sent": "So here for example, we have for variable and we want to see if A and B&B are independent.",
                    "label": 0
                },
                {
                    "sent": "Given BSL always indicate that I'm conditioning over this variable.",
                    "label": 0
                },
                {
                    "sent": "Since there is a Collider in the path and I'm conditioning on it, that means that I I maintain dependencies and I expressed that by this undirected link.",
                    "label": 0
                },
                {
                    "sent": "Here on the other end, since I have no Collider.",
                    "label": 0
                },
                {
                    "sent": "In the path.",
                    "label": 0
                },
                {
                    "sent": "That means that this.",
                    "label": 0
                },
                {
                    "sent": "The link will be cutted here and so the path from A to D will be cut and means that A&D will be independent given B&C.",
                    "label": 0
                },
                {
                    "sent": "So what we see is if there is.",
                    "label": 0
                },
                {
                    "sent": "And uncle, either in the condition instead.",
                    "label": 0
                },
                {
                    "sent": "Then the path is blocked.",
                    "label": 0
                },
                {
                    "sent": "On the other end, let's look at the same graph in which though I don't condition on VNC.",
                    "label": 0
                },
                {
                    "sent": "This we in this case.",
                    "label": 0
                },
                {
                    "sent": "The non collided here will maintain dependencies while the Collider will cut dependencies and again the path between A&D will be blocked, so we can infer another rule.",
                    "label": 0
                },
                {
                    "sent": "That is, we find the Collider outside the condition inside.",
                    "label": 0
                },
                {
                    "sent": "Then the path will be blocked.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at the case in which we have more than one path.",
                    "label": 0
                },
                {
                    "sent": "So it is a bit more complicated.",
                    "label": 0
                },
                {
                    "sent": "Like the case here.",
                    "label": 0
                },
                {
                    "sent": "While this is quite simple becausw here I have an uncle either which is in the condition instead because it's yellow and this dependencies therefore will be cutted.",
                    "label": 0
                },
                {
                    "sent": "On the other end, here I have an Uncle Collider which is not in the condition insect and therefore will cut the dependencies.",
                    "label": 0
                },
                {
                    "sent": "So there is no link within VNC.",
                    "label": 0
                },
                {
                    "sent": "Therefore the variables are independent.",
                    "label": 0
                },
                {
                    "sent": "Now in this case.",
                    "label": 0
                },
                {
                    "sent": "We have two colliders.",
                    "label": 0
                },
                {
                    "sent": "Here, as it is there before I have a Collider, but I don't condition on it, so the dependencies will be counted on the other end.",
                    "label": 0
                },
                {
                    "sent": "I have a Collider in the condition in certain, so the dependencies will be maintained and in the case B&C will be.",
                    "label": 0
                },
                {
                    "sent": "Dependent given a.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's I just explain you with simple example.",
                    "label": 0
                },
                {
                    "sent": "What is the general rule for understanding in dependencies between in belief net.",
                    "label": 1
                },
                {
                    "sent": "Or you can read this generally sound quite complicated, but if you understand I understood the example before, you will understand it very easily.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just so to give you an example of how you can use this independence for modeling.",
                    "label": 0
                },
                {
                    "sent": "Considers this belief network for for time series, so I have a graphical model of this type.",
                    "label": 0
                },
                {
                    "sent": "Well with the set of visible variable from one 2T which represent some time series observation.",
                    "label": 0
                },
                {
                    "sent": "It can be, for example speech waveform and a set of discrete hidden variable, which I assume that generated the observation.",
                    "label": 1
                },
                {
                    "sent": "So the graphical model I consider is.",
                    "label": 0
                },
                {
                    "sent": "Live nectar is pretty simple, so at each time step they even variable as only the even variable at the previous time step and the observation.",
                    "label": 0
                },
                {
                    "sent": "As aspiring, only the hidden variable at that time set step in the in the visible variable at the previous time step.",
                    "label": 0
                },
                {
                    "sent": "Now, one of the problems we want to to do in this type of model is for example, to ask about the probability of HT given all the observation up to time T, which is quite related to this joint distribution here.",
                    "label": 0
                },
                {
                    "sent": "And they say this is an inference problem called filtering.",
                    "label": 0
                },
                {
                    "sent": "We will see some example of this later.",
                    "label": 1
                },
                {
                    "sent": "Well, I can write down the joint distribution.",
                    "label": 0
                },
                {
                    "sent": "Certainly this way.",
                    "label": 0
                },
                {
                    "sent": "Then I can introduce.",
                    "label": 0
                },
                {
                    "sent": "HT minus one in this last term here and some over it, so this will be equivalent to this term.",
                    "label": 0
                },
                {
                    "sent": "And now I look at this last term and condition HT on the order variable, and then I therefore have to multiply by the distribution of this.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Viable now, I claim that I can remove some of the links.",
                    "label": 0
                },
                {
                    "sent": "And if I look at this term here, they will.",
                    "label": 0
                },
                {
                    "sent": "This will become our simple term.",
                    "label": 0
                },
                {
                    "sent": "In the in the belief that or and if I look also this term here, it will become the simple term.",
                    "label": 0
                },
                {
                    "sent": "So basically what I wrote down is recursive way of computing this joint distribution, because this is the same John distribution but at the previous time step and then I have to manipulate it with the basic distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is an efficient way to do different, so let's see why I can.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I claim that I can remove this tooling, just look at the first case.",
                    "label": 0
                },
                {
                    "sent": "Well, if what I'm saying is that V is given HT an booties minus one is independent all the previous all the past visible variable.",
                    "label": 0
                },
                {
                    "sent": "Well, it's true.",
                    "label": 0
                },
                {
                    "sent": "Becausw All path which go from any of these valuable here in the past to booty will have to pass through any of these two variable.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose for example, the part cost from here to here.",
                    "label": 0
                },
                {
                    "sent": "Or we can go also from here to here.",
                    "label": 0
                },
                {
                    "sent": "While the variable HT will never be Collider right, it is always an uncle, either in the conditioning set and we said if we find a Collider and colliding in the condition instead, that means that the parties block.",
                    "label": 0
                },
                {
                    "sent": "So the path that goes from here is always proud.",
                    "label": 0
                },
                {
                    "sent": "And similarly the party grows from T. With this one is one will never be.",
                    "label": 0
                },
                {
                    "sent": "The Collider, and therefore also this path, will be blocked and you can source the same thing for the.",
                    "label": 0
                },
                {
                    "sent": "For the other case, and the reason why I introduced this example is be cause this belief network, for example, could represent a switching auto regressive model which is a model at being used a lot in the speech community.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the literature on how they they.",
                    "label": 0
                },
                {
                    "sent": "We came out with an inference algorithm to compute the filter distribution.",
                    "label": 0
                },
                {
                    "sent": "It looks quite complicated.",
                    "label": 0
                },
                {
                    "sent": "Then, while if you just look at the photograph and use the rules that I explain you, everything become quite simple and you can build even more complicated model than this and just reason on the graph to understand independence and to come out with some efficient way of computing things.",
                    "label": 0
                },
                {
                    "sent": "OK, that was the end of an introduction to to believe NATO.",
                    "label": 0
                },
                {
                    "sent": "Now we can start to describe another type of.",
                    "label": 0
                },
                {
                    "sent": "Graphic.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model, which is the mark on nectar for which we need the definition of a click.",
                    "label": 0
                },
                {
                    "sent": "A click is a fully connected subset of nodes.",
                    "label": 1
                },
                {
                    "sent": "So for example BC an E form a click.",
                    "label": 1
                },
                {
                    "sent": "Because the nodes are all connected to each other and a click is said to be maximal if is not a subset of another click.",
                    "label": 0
                },
                {
                    "sent": "So for example is B and click BNC maximal?",
                    "label": 0
                },
                {
                    "sent": "Any anybody can tell me?",
                    "label": 0
                },
                {
                    "sent": "No did why because he's containing this larger click formed by the variable BC&E.",
                    "label": 0
                },
                {
                    "sent": "OK, so my connector is an undirected graph in which there is a potential by potential and mean and negative function PSI define on each maximize click and the joint distribution of all nodes in the graph is proportional to.",
                    "label": 1
                },
                {
                    "sent": "The product all click potentials divided by the normalization constant.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I give you a simple application of a mark on actual.",
                    "label": 1
                },
                {
                    "sent": "I suppose that we observe a corrupted version of an image and we want to recover a binary image and we want to recover it.",
                    "label": 1
                },
                {
                    "sent": "We want to recover the clean image from observing only the corrupted version of these.",
                    "label": 0
                },
                {
                    "sent": "We can model this by a mark, an actor whose graphical representation is given here.",
                    "label": 0
                },
                {
                    "sent": "the Y node here represent the clean pixels, while the.",
                    "label": 0
                },
                {
                    "sent": "The filler node represents the corrupted pixel, so each corrupted pixel as a link to the corresponding.",
                    "label": 0
                },
                {
                    "sent": "Clean pixela means that we have a potential between the.",
                    "label": 0
                },
                {
                    "sent": "Corrupted pixel and clean pixel, which I I'll explain later here.",
                    "label": 0
                },
                {
                    "sent": "This is a potential and each claim pixel as connection with the neighbor pixels and therefore I will have.",
                    "label": 0
                },
                {
                    "sent": "Click between that pixel and each of his neighbors, which I I indicate here, so I'll know this clicks this potential look like they.",
                    "label": 1
                },
                {
                    "sent": "Reasonable to do to construct this potential such that to encourage The Dirty or the corrupted pixel to be similar to the to the clean pixel.",
                    "label": 1
                },
                {
                    "sent": "And also we want to impose smoothness in the in the.",
                    "label": 0
                },
                {
                    "sent": "In the in the image and that's why we use this kind of potential with some positive constant here which will enforce the variable to be close to each other.",
                    "label": 1
                },
                {
                    "sent": "Now if we want to find the most likely clean image given the corrupting, we will find it.",
                    "label": 0
                },
                {
                    "sent": "There is a quite difficult problem because the graph is not singly connected.",
                    "label": 0
                },
                {
                    "sent": "I will explain actually later would is.",
                    "label": 0
                },
                {
                    "sent": "I mean by that, but we can use some approximate method to to perform.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On that nest omation an I give you here.",
                    "label": 0
                },
                {
                    "sent": "One example, this is the picture or the image where I start from.",
                    "label": 0
                },
                {
                    "sent": "And this is the image that I have recovered most likely.",
                    "label": 0
                },
                {
                    "sent": "Clean image that I recover using this approximate inference methods and this is the original clean image where that generated my corrupted version and I can maybe show you.",
                    "label": 1
                },
                {
                    "sent": "Some order.",
                    "label": 0
                },
                {
                    "sent": "Example.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There are few iterations required for the agreement to converge, so you can see that is still converging.",
                    "label": 0
                },
                {
                    "sent": "OK, you can see in this case it didn't work as well as before.",
                    "label": 0
                },
                {
                    "sent": "So it depends on the problem.",
                    "label": 0
                },
                {
                    "sent": "It can be more and more or less difficult to find.",
                    "label": 0
                },
                {
                    "sent": "A satisfactory solution.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we can.",
                    "label": 0
                },
                {
                    "sent": "Explain what how to to do to understand independence in Marco.",
                    "label": 0
                },
                {
                    "sent": "Matter is much more simpler than is much simpler than in belief network because we have only one type of link.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose that we have this for variable.",
                    "label": 0
                },
                {
                    "sent": "Here ABC and D and we want to understand if B is independent of C given A&D.",
                    "label": 0
                },
                {
                    "sent": "Already know that word if the probability of B given a DC is equal to the probability of B given A&E.",
                    "label": 0
                },
                {
                    "sent": "But we can write this conditional distribution as the ratio between our joint distribution divided by POACD.",
                    "label": 0
                },
                {
                    "sent": "Now pervasive is simply the sum over B of our joint distribution.",
                    "label": 0
                },
                {
                    "sent": "Now I can use the factorization of the mark on nectar.",
                    "label": 0
                },
                {
                    "sent": "I have only clicks formed by the two variables we DDCCA&AB.",
                    "label": 0
                },
                {
                    "sent": "And that's what I I wrote here and similar for the denominator.",
                    "label": 0
                },
                {
                    "sent": "Now I see that this term here does not depend on B.",
                    "label": 0
                },
                {
                    "sent": "Similarly for this term.",
                    "label": 0
                },
                {
                    "sent": "Therefore we can cancel them and we will obtain a function which is not dependent on C. So we demonstrated that B&C are indeed independent.",
                    "label": 0
                },
                {
                    "sent": "Given a an D, so is a pretty easy way for marconato to understand.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Now this is just what I I have showed before by graphically I cutting the link every time there is.",
                    "label": 0
                },
                {
                    "sent": "And nodes.",
                    "label": 0
                },
                {
                    "sent": "In the path between the variable that I'm conditioning or so in this case.",
                    "label": 0
                },
                {
                    "sent": "So there is variable here and I'm not conditioning on it, so the link will be maintained.",
                    "label": 0
                },
                {
                    "sent": "So in order to have in dependencies, we have to again block all the path between our valuables.",
                    "label": 0
                },
                {
                    "sent": "Is that clear?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this method actually give us another way of understanding, or of inferring dependencies in belief network.",
                    "label": 0
                },
                {
                    "sent": "I won't describe it here.",
                    "label": 0
                },
                {
                    "sent": "You can read it later, but the main idea is to transform your belief net or into the direct.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graph.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With several step and then use the independence method that I just described earlier for Markov network.",
                    "label": 0
                },
                {
                    "sent": "And sometimes this method might be a bit easier than the method I explain.",
                    "label": 0
                },
                {
                    "sent": "Yes, the earlier for believing that they give exactly the same result, yes, but you might sometimes prefer to use this.",
                    "label": 0
                },
                {
                    "sent": "A bit less.",
                    "label": 0
                },
                {
                    "sent": "Complicated to understand.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's now think I have described two type of.",
                    "label": 0
                },
                {
                    "sent": "Graphical model Now how do they relate?",
                    "label": 0
                },
                {
                    "sent": "Can we always express?",
                    "label": 0
                },
                {
                    "sent": "I believe nature with a marker network, the answer is no.",
                    "label": 0
                },
                {
                    "sent": "Like let's look at this example here I have a Collider, which means that the joint distribution will factorize this way right?",
                    "label": 0
                },
                {
                    "sent": "Because I have to root and the and the cost C which has two parents and now obviously we want to form mark connector.",
                    "label": 0
                },
                {
                    "sent": "I will have a potential on all three variables.",
                    "label": 0
                },
                {
                    "sent": "That means that I will have a link between.",
                    "label": 0
                },
                {
                    "sent": "All of them.",
                    "label": 0
                },
                {
                    "sent": "And that which means that now A&B will not be independent anymore, while in our original graph A&B were independent.",
                    "label": 0
                },
                {
                    "sent": "Now we can also ask the opposite question, can we express any mark on network with the belief network?",
                    "label": 0
                },
                {
                    "sent": "And the answer is again no.",
                    "label": 0
                },
                {
                    "sent": "We look at the.",
                    "label": 0
                },
                {
                    "sent": "Like a sample here.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Do I have to transform it into believing that we will have to maintain kind of the same structure so we cannot have the link here between B&C and A&E?",
                    "label": 0
                },
                {
                    "sent": "And also to ensure that they that the model is the graphical mode is a cyclic, we have to.",
                    "label": 0
                },
                {
                    "sent": "We would like to have a Collider.",
                    "label": 0
                },
                {
                    "sent": "These are us to point here.",
                    "label": 0
                },
                {
                    "sent": "If you point on the other direction we will get cyclic graph.",
                    "label": 0
                },
                {
                    "sent": "So that means that we will end up with different set of independence independence relations there for example.",
                    "label": 0
                },
                {
                    "sent": "The original graph B&C were independent, given Andy, because I condition on both.",
                    "label": 0
                },
                {
                    "sent": "So I cut the link here and the cattle in here.",
                    "label": 0
                },
                {
                    "sent": "While in this case.",
                    "label": 0
                },
                {
                    "sent": "The variable BNC will be dependent given A&B because if you look at the path BDC here.",
                    "label": 0
                },
                {
                    "sent": "We have a Collider right in the path and we don't condition.",
                    "label": 0
                },
                {
                    "sent": "Is it cool?",
                    "label": 0
                },
                {
                    "sent": "C An we do conditions.",
                    "label": 0
                },
                {
                    "sent": "Sorry on the on the Collider did for we we have dependency.",
                    "label": 0
                },
                {
                    "sent": "Please tell me if there is something that is not clear.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's now so talk about the third type of graphical model that we want to see in this talk and namely the factor graph in a factor graph in addition to.",
                    "label": 0
                },
                {
                    "sent": "Note we represent random variable with the square nodes.",
                    "label": 0
                },
                {
                    "sent": "We should represent nonnegative function of his neighbor variable.",
                    "label": 0
                },
                {
                    "sent": "So for example, a factor of 1 year represent a non negative function of A&B and then we will have here if two is a non negative function of BC&D and so on and then join function is obtained by simply the product of all these factors.",
                    "label": 1
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "Photograph are mainly or use a lot to perform efficient inference as we will see later and here.",
                    "label": 1
                },
                {
                    "sent": "Notice that I talk about function, not probability distribution because they're being used also for performing efficient computation or just in probability.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So inference problem.",
                    "label": 0
                },
                {
                    "sent": "Now I so far explain you out to understand.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I explain the different graphical models and how to understanding dependencies as I told you, graphical model are also use a full because they enable us to develop efficient inference algorithms.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to explain a little bit about that.",
                    "label": 0
                },
                {
                    "sent": "So inference correspond as we know by now, operations such as computing marginal or compute conditional distribution from the joint distribution in general, as a inference is a very difficult problem.",
                    "label": 1
                },
                {
                    "sent": "Also because we had on machine learning many many variable in graphical model normally.",
                    "label": 0
                },
                {
                    "sent": "However, for single connected type of graph.",
                    "label": 1
                },
                {
                    "sent": "There exist sufficient argument based on the concept of message passing.",
                    "label": 0
                },
                {
                    "sent": "With whom do I mean by singly connected graph, a single connected graph is a graph in which there is only a path from a node to another node.",
                    "label": 1
                },
                {
                    "sent": "Can you tell me if this part is simply connected?",
                    "label": 0
                },
                {
                    "sent": "Yes, because I wrote it there right so that.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I add the link from D2.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will obtain multiple sorry from E2D multiply connected graph instead.",
                    "label": 0
                },
                {
                    "sent": "Because there is for example apart from me to the just simply going this direction or I can go so from here to the passing through G so there are two paths.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's discuss about this.",
                    "label": 0
                },
                {
                    "sent": "How to perform inference?",
                    "label": 0
                },
                {
                    "sent": "In photographs and the concept of message passing, let's suppose that we have.",
                    "label": 0
                },
                {
                    "sent": "For binary variable ABC and D and our distribution is factorized this way, that means that I will have a photograph representation here where F1 connecting B and therefore I have a factor for this and so on right?",
                    "label": 0
                },
                {
                    "sent": "And here I have only.",
                    "label": 0
                },
                {
                    "sent": "Connection to this so I will have only factor that is a function only on the variable D. Now let's suppose that I want to compute the marginal distribution of a well.",
                    "label": 0
                },
                {
                    "sent": "This is the sum over all the remaining variable of the joint distribution.",
                    "label": 0
                },
                {
                    "sent": "No, I can write down my factorization of the photograph.",
                    "label": 0
                },
                {
                    "sent": "And perform the submission and doing that.",
                    "label": 0
                },
                {
                    "sent": "We realize that I will need to perform 2 to the power three summations.",
                    "label": 0
                },
                {
                    "sent": "That means a number of summation which is exponentially in the number of variable.",
                    "label": 0
                },
                {
                    "sent": "I have to sum over.",
                    "label": 0
                },
                {
                    "sent": "No, so we can think a bit more clever way of doing that.",
                    "label": 0
                },
                {
                    "sent": "So if we look for example this first term, it does not depend on D. That means that also this second term does not depend on Monday.",
                    "label": 0
                },
                {
                    "sent": "That means that we can push this summation here before the three factor and.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarly for the other summation, so I will end up that I can distribute and push the summation further.",
                    "label": 0
                },
                {
                    "sent": "Now, if I look I have to compute here to sum over the van and we said this binary, so I've had to some here plus 2 something here plus two sums here.",
                    "label": 0
                },
                {
                    "sent": "That means that we have 2 * 3 submissions and that means I have a number of summation which is now linear in the in the number of variable add to some.",
                    "label": 0
                },
                {
                    "sent": "So we we gain something if we gave only two in this case but.",
                    "label": 0
                },
                {
                    "sent": "You can imagine that if you have a bigger graph then you can see.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A lot of computations.",
                    "label": 0
                },
                {
                    "sent": "Well, we can see this procedure at the kind of message passing.",
                    "label": 0
                },
                {
                    "sent": "So what do we can define here is, uh, this term as a message from the variable D to the variable C. And analogously for from here then.",
                    "label": 0
                },
                {
                    "sent": "So I can define this term as a message from the variable C to the variable B.",
                    "label": 0
                },
                {
                    "sent": "And finally, this as a message from B2A.",
                    "label": 0
                },
                {
                    "sent": "So basically what I.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can view this as passing messages from the variable D up to the variable A.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, if you think about if you have to compute the joint, they'd imaginal scene.",
                    "label": 0
                },
                {
                    "sent": "Instead we will have to pass messages in in in both directions.",
                    "label": 1
                },
                {
                    "sent": "Coming coming from this.",
                    "label": 0
                },
                {
                    "sent": "So from hold the factor that have a link to see in this case there are two factors that will link to see, so I have to pass messages from here and also messages to.",
                    "label": 0
                },
                {
                    "sent": "Here I can I can see that.",
                    "label": 0
                },
                {
                    "sent": "Similar way.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "No, in the more complicated case in which I have a bunching three, that means, for example, you're a factory is the function of three nodes.",
                    "label": 0
                },
                {
                    "sent": "Things get a bit more complicated than I have, is not sufficient to define variable to variable messages as before, I'm to define factor to variable message and the variable to factor message and.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see how you define these kind of messages while.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the variable to factor message.",
                    "label": 0
                },
                {
                    "sent": "If I have a variable here, I want to compute the message to from the variable V to the factor F, while in order to do that.",
                    "label": 0
                },
                {
                    "sent": "I have to multiply the factor.",
                    "label": 0
                },
                {
                    "sent": "Coming the messages from the factor.",
                    "label": 0
                },
                {
                    "sent": "Which are of a link with the variable be so F1 as a link to V F2 as a link to be I to compute the message from front to V, the message from a 22V, multiply them and then I can send.",
                    "label": 0
                },
                {
                    "sent": "I obtain the message from B2F.",
                    "label": 0
                },
                {
                    "sent": "Is that clear?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "If I want to send.",
                    "label": 0
                },
                {
                    "sent": "A message from a factor to a variable, so from F2 to V. I will have to again compute the messages from.",
                    "label": 0
                },
                {
                    "sent": "In this case we want to F2 to F. So all the messages from the other variable to the factor, except obviously that the message from me to have.",
                    "label": 0
                },
                {
                    "sent": "Multiply them.",
                    "label": 0
                },
                {
                    "sent": "Multiply by then this.",
                    "label": 0
                },
                {
                    "sent": "That's what I I right here.",
                    "label": 0
                },
                {
                    "sent": "Then multiply these two by the factor.",
                    "label": 0
                },
                {
                    "sent": "And then sum over the two variable here.",
                    "label": 0
                },
                {
                    "sent": "And you can see that would.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically we are soon.",
                    "label": 0
                },
                {
                    "sent": "Here is exactly the same and you can take a look at that more closely.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later in the slide.",
                    "label": 0
                },
                {
                    "sent": "And then if you want to compute the marginal as we said, we have to just take the product of all factor to variable message.",
                    "label": 0
                },
                {
                    "sent": "For the factor that link to that.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I was.",
                    "label": 0
                },
                {
                    "sent": "Now let's give a little bit of an example of inference in hidden Markov model, which is a very popular model in many disciplines and is a little extension of the of the belief network.",
                    "label": 1
                },
                {
                    "sent": "We have said yes that we have seen earlier and.",
                    "label": 1
                },
                {
                    "sent": "I consist of a set of.",
                    "label": 1
                },
                {
                    "sent": "Visible variable one would be that represent our observation.",
                    "label": 0
                },
                {
                    "sent": "For example the speech waveform, and then we have a set of hidden variable.",
                    "label": 0
                },
                {
                    "sent": "Which are discrete?",
                    "label": 0
                },
                {
                    "sent": "And I already, yeah, explain this.",
                    "label": 0
                },
                {
                    "sent": "Basically the hidden variable at time T as only one parent and also the.",
                    "label": 0
                },
                {
                    "sent": "In this case I don't have a link here and the visible variable depend on HT only, so it's a special case overseen before an Indian market model.",
                    "label": 0
                },
                {
                    "sent": "Normally we assume that the transition from hidden variable to another in variable is time homogeneous and we can represent that by a matrix by and in this.",
                    "label": 0
                },
                {
                    "sent": "In this example I'm going to show later.",
                    "label": 1
                },
                {
                    "sent": "I will also assume that the observation are discrete and we can therefore.",
                    "label": 1
                },
                {
                    "sent": "Summarize this table entries with the emission mattress role.",
                    "label": 0
                },
                {
                    "sent": "Now three of the very common problem inference problem hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "Inferring HD given only the observation up to time T which is called filtering or inferring HT from the distribution of HT given all the information.",
                    "label": 1
                },
                {
                    "sent": "Also, the future observations and the third problem is to infer the most likely even sequence from basically maximizing the conditional distribution of age.",
                    "label": 1
                },
                {
                    "sent": "From one to T given all the observations.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you want to do this inference, we can just one way of doing it is, well, we can either reason as I explained before.",
                    "label": 0
                },
                {
                    "sent": "Or you can use the dim after the photograph matter that I explain.",
                    "label": 0
                },
                {
                    "sent": "In the previous life.",
                    "label": 0
                },
                {
                    "sent": "So if we look at the HMM representation as a factor graph, we looked like that where for example we have a factor F-11 here, which is a function of the observation at time one and Aiden variable at time T. And then we need another factor F-12, which is a function of H1 and two.",
                    "label": 1
                },
                {
                    "sent": "And then we can just use the argument I explain you before to to compute.",
                    "label": 0
                },
                {
                    "sent": "There to perform inference.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's look at an example.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose that you are asleep upstairs and.",
                    "label": 0
                },
                {
                    "sent": "In your house, so you are walking by a burglar on the ground floor and we want to understand where the burglar is by.",
                    "label": 1
                },
                {
                    "sent": "Just based on a sequence of noise information that you have so you are, you can do that using an HMM and I'm going to explain you are you mentaly partition the floor into?",
                    "label": 0
                },
                {
                    "sent": "A 5 by 5 grid and for each position you know the probability that someone if someone is in that position, the floorboard will click and this is what I show here.",
                    "label": 1
                },
                {
                    "sent": "So we like Olori indicated probability 0.9 that if the burglar is in that position, the floor will create.",
                    "label": 1
                },
                {
                    "sent": "And if the border is here, on the other hand, there is probability 0.1 that the floorboard will Creek.",
                    "label": 1
                },
                {
                    "sent": "Now you also know the probability that if someone is in a certain position in the grid, it will bump into something in the dark, and that's what I show here.",
                    "label": 0
                },
                {
                    "sent": "Scenarios assume that the burglar can move only one in one of the great position.",
                    "label": 0
                },
                {
                    "sent": "You can move only one of the neighbors with equal probability.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so as I said, we can represent this problem as.",
                    "label": 0
                },
                {
                    "sent": "An item in problem.",
                    "label": 0
                },
                {
                    "sent": "Where the hidden variable represent the position of the burglar in the grid at time T, so the variability take take 25 value values and the visible variable Vt represent the currencies of creaks and bumps.",
                    "label": 0
                },
                {
                    "sent": "So for example, if the variable we take values zero, that means that there was no Creek, no bomb.",
                    "label": 0
                },
                {
                    "sent": "If it takes value one, that means that there was a Creek and Obama and so on, and we will have four for value.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As for this.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see what happened here with an example.",
                    "label": 0
                },
                {
                    "sent": "Here I give you.",
                    "label": 0
                },
                {
                    "sent": "The sequence of creaks and bumps for 10 time step.",
                    "label": 1
                },
                {
                    "sent": "The light color indicate the occurrence of a Creek.",
                    "label": 0
                },
                {
                    "sent": "On the on the left side on the right side the occurrence of of the bond.",
                    "label": 0
                },
                {
                    "sent": "So we have this sequence and this is the position of the burglar.",
                    "label": 0
                },
                {
                    "sent": "Here the last.",
                    "label": 0
                },
                {
                    "sent": "Throw in the at each time step.",
                    "label": 0
                },
                {
                    "sent": "On the second line here with the show is the filtering.",
                    "label": 0
                },
                {
                    "sent": "So now you could use.",
                    "label": 0
                },
                {
                    "sent": "So the distribution of HT given the information up to time T you can.",
                    "label": 0
                },
                {
                    "sent": "Use that one the burglar is in the house.",
                    "label": 0
                },
                {
                    "sent": "You just.",
                    "label": 0
                },
                {
                    "sent": "Get what is a set of of probable or I probable positions for the burglar, while is there and at the beginning maybe you don't have a lot of information so you will not sure where the burglar here is, by the way, but with why I indicated high probability that the burglar is in the position.",
                    "label": 0
                },
                {
                    "sent": "So here for example there are many possible position where the burglar can be.",
                    "label": 0
                },
                {
                    "sent": "I am in the beginning so I don't have much information.",
                    "label": 0
                },
                {
                    "sent": "While going ahead with time I I acquire more and more information and become more certain about where the burglar can be.",
                    "label": 0
                },
                {
                    "sent": "This one show is booting instead and you can think of it.",
                    "label": 0
                },
                {
                    "sent": "I mean, maybe the police can use that when.",
                    "label": 0
                },
                {
                    "sent": "The police arrive.",
                    "label": 0
                },
                {
                    "sent": "You can give all the information about the noise and then it can figure out the means.",
                    "label": 0
                },
                {
                    "sent": "The observation up to time T and then the police can.",
                    "label": 0
                },
                {
                    "sent": "Figure out the probable position for each.",
                    "label": 0
                },
                {
                    "sent": "The burglar at each time step.",
                    "label": 0
                },
                {
                    "sent": "All the police can do is also to compute the most likely path.",
                    "label": 0
                },
                {
                    "sent": "They'll most likely sequence of position for each time step, which is what I show in the in the fourth line here.",
                    "label": 0
                },
                {
                    "sent": "And again, maybe I if I time I show you then would later, but in this case you see can work quite well there.",
                    "label": 0
                },
                {
                    "sent": "It depends a lot on the problem.",
                    "label": 0
                },
                {
                    "sent": "There are cases in which in my work less well.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I give you now another example of modeling with an HMM.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose that we have established finger typist that has a tendency to.",
                    "label": 0
                },
                {
                    "sent": "Either it the correct key on a or enabling key in the keyboard and based on a type of sequence you want to infer.",
                    "label": 1
                },
                {
                    "sent": "What is the most likely war.",
                    "label": 0
                },
                {
                    "sent": "This sequence correspond to, and again we can represent that with a hidden Markov model in which HT now is the intended letter at time T. Anne Moody represent a letter that was actually type at time T. As a transition for.",
                    "label": 1
                },
                {
                    "sent": "The hidden variable we can use just a standard transition matrix of the of the English language.",
                    "label": 0
                },
                {
                    "sent": "So probably you don't see there, but for example is very highly probable that the letter you will follow the letter Q on that whole time is indicated here with the light color.",
                    "label": 0
                },
                {
                    "sent": "And you also assume that the typist will probably type the correct key with high probability.",
                    "label": 0
                },
                {
                    "sent": "That would show in this diagonal here.",
                    "label": 0
                },
                {
                    "sent": "But there is a probability that it will type some of the key enabling key.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, if you can do kind of Viterbi decoding, and given that the typists type with this sequence here, what is the most likely wore this correspond to while you can list the site of most likely even sequences using us said kind of beat urban, then disregard those that don't correspond to the sentence in the English dictionary.",
                    "label": 1
                },
                {
                    "sent": "And then take the most proper English word as as intended word and anybody can tell me what the answer can be.",
                    "label": 1
                },
                {
                    "sent": "Many who stand learning.",
                    "label": 0
                },
                {
                    "sent": "Exactly, yeah, that's.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I still have think 20 minutes or.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So far I have described.",
                    "label": 0
                },
                {
                    "sent": "Several typographical models and how to do inference with that.",
                    "label": 0
                },
                {
                    "sent": "How to understand independence, but I have always assumed that we knew the table entries in our distribution so.",
                    "label": 0
                },
                {
                    "sent": "For example, in the case of the HMM, I assume that I knew the.",
                    "label": 0
                },
                {
                    "sent": "That we knew the transition or emission probability.",
                    "label": 0
                },
                {
                    "sent": "Now I want to discuss a little bit how to to learn this table entries and therefore suppose that we have a model of random variable X, which is the which depends on unknown parameter Theta.",
                    "label": 0
                },
                {
                    "sent": "And from a set of observation I want.",
                    "label": 0
                },
                {
                    "sent": "Two, infer something about system.",
                    "label": 0
                },
                {
                    "sent": "Well, I assume that this observation are independent in identically distributed according to this model, and therefore if you can see that now the joint distribution, all our observation given the parameter theater, I can write it as a product of simple terms here.",
                    "label": 0
                },
                {
                    "sent": "And I can also.",
                    "label": 0
                },
                {
                    "sent": "Introduce a prior distribution or prior preference of my parameter feta and then with Bayes rule as we have seen, I can infer with the posterior of Theta is given my observations.",
                    "label": 0
                },
                {
                    "sent": "And I can view this as an extension of our belief net or by simply consider an additional node in my belief net through which as link to all the observations.",
                    "label": 0
                },
                {
                    "sent": "And I can also use a more compact notation here, in which I have just one sketch.",
                    "label": 0
                },
                {
                    "sent": "The link from theater to one of the side, and I have a plate.",
                    "label": 0
                },
                {
                    "sent": "In pink, that indicated I repetition of the same structure and times.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we want to summarize the our posterior to very popular way of doing it is the maximum a posteriori method which simply compute the mode of the posterior.",
                    "label": 1
                },
                {
                    "sent": "Or the maximum likely matter?",
                    "label": 0
                },
                {
                    "sent": "The near talk a lot about yesterday, which is a special case in which the prior is omitted, or equivalently, a flat prior on our parameter is imposed.",
                    "label": 1
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's see a simple example of.",
                    "label": 0
                },
                {
                    "sent": "Learning in a belief network call, basic plastic naive Bayes classifier.",
                    "label": 1
                },
                {
                    "sent": "This is a very very popular method and the reason why is very popular is becausw learning in this method is very very simple.",
                    "label": 0
                },
                {
                    "sent": "Let's see what this model is about.",
                    "label": 0
                },
                {
                    "sent": "So is a classifier as it said.",
                    "label": 0
                },
                {
                    "sent": "So we have a random variable, see which indicate the class which.",
                    "label": 0
                },
                {
                    "sent": "For example, if you consider the problem of spam filtering, it could indicate that animal is spam or ham.",
                    "label": 0
                },
                {
                    "sent": "And then we have said to attribute XI, which again in the case for example of spam filtering could indicate the presence or absence of a certain keyword in the email.",
                    "label": 0
                },
                {
                    "sent": "Team the belief net representation of this model is given here.",
                    "label": 0
                },
                {
                    "sent": "So we assume that the.",
                    "label": 0
                },
                {
                    "sent": "The class influence directly each attribute, but all the attributes given the class are independent, so we can write the joint distribution into this simple form and then given our attributes we can infer with the class is.",
                    "label": 0
                },
                {
                    "sent": "And how do we?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lando, the table of this.",
                    "label": 0
                },
                {
                    "sent": "From a set of Satan said, let's suppose that we call theater high the parameter associated to this table entries table entries, or the attribute XI.",
                    "label": 0
                },
                {
                    "sent": "Given the Class C. Similarly for the class.",
                    "label": 0
                },
                {
                    "sent": "Basically, if we assume that all these parameters are dependent, I can write a will live nectar, in which there is a link for each parameter to the associated.",
                    "label": 0
                },
                {
                    "sent": "Random variable.",
                    "label": 0
                },
                {
                    "sent": "And now we can use the base.",
                    "label": 0
                },
                {
                    "sent": "Approach in which a Bayesian approach in which we compute the posterior work.",
                    "label": 0
                },
                {
                    "sent": "We can summarize the posterior with the mode.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's look at the simple case of a Maxim.",
                    "label": 0
                },
                {
                    "sent": "Like you know if.",
                    "label": 0
                },
                {
                    "sent": "We while we can write down the log likelihood as.",
                    "label": 0
                },
                {
                    "sent": "The log of the product over all the observation.",
                    "label": 0
                },
                {
                    "sent": "Now we know that how to split the joint distribution and if we consider, for example that we want to learn the parimeter theater IO which indicate the probability that the attribute excised zero given the class is 0.",
                    "label": 1
                },
                {
                    "sent": "Then if we look at the dependencies of the log likelihood on data IO, this simply given by this.",
                    "label": 1
                },
                {
                    "sent": "This term here where and zero, for example, indicate the number of times that C = 0 and X I = 0.",
                    "label": 1
                },
                {
                    "sent": "Of course in our data.",
                    "label": 0
                },
                {
                    "sent": "So that's the other term.",
                    "label": 0
                },
                {
                    "sent": "Do not depend on this parameter here, so we can disregard them.",
                    "label": 1
                },
                {
                    "sent": "Then by differentiated in Sturm with respect to Ty Ty Jay and setting the this the derivative to zero, we can obtain a value for our tee time zero which if we see corresponds simply to calculating frequencies so.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "This might be a little bit of a problem if, for example, this N 0 is 0.",
                    "label": 0
                },
                {
                    "sent": "So we don't have any data count and we can improve on this model by using a Bayesian approach in which.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we put some prior distribution on the.",
                    "label": 0
                },
                {
                    "sent": "On the parimeter and such that this pathological case will not create problems.",
                    "label": 0
                },
                {
                    "sent": "I won't describe that.",
                    "label": 0
                },
                {
                    "sent": "Maybe it is you can.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Edit later.",
                    "label": 0
                },
                {
                    "sent": "Now let's discuss a little bit.",
                    "label": 0
                },
                {
                    "sent": "Learning in Markham network.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at the log likelihood here as again I can split it over the observation, then I use the definition of the joint distribution.",
                    "label": 0
                },
                {
                    "sent": "Applying it here, I take the log.",
                    "label": 0
                },
                {
                    "sent": "The log of the product is the sum log, blah blah blah and would I end up with is term here log theater theater that I as I see.",
                    "label": 0
                },
                {
                    "sent": "Does not factorize.",
                    "label": 0
                },
                {
                    "sent": "Is not.",
                    "label": 0
                },
                {
                    "sent": "I cannot split it into.",
                    "label": 0
                },
                {
                    "sent": "Isolated the terms which depend on the single parameter Theta.",
                    "label": 0
                },
                {
                    "sent": "That means that we cannot have a simple update form.",
                    "label": 0
                },
                {
                    "sent": "As for learning data, we have to use numerical.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Methods.",
                    "label": 0
                },
                {
                    "sent": "So far I have discussed yeah learning in the case in which all the viable are absurdly now see very briefly the case in which I also have some even variable in the model, and suppose that I have.",
                    "label": 0
                },
                {
                    "sent": "A graphical model, this this form I have 1 hidden variable H and the visible variable V. And I want to learn again.",
                    "label": 1
                },
                {
                    "sent": "I have a parameter Theta it's associated to the variable H and a parameter T to be associated to the variable V. And now I want to learn Theta agent into V from the observations.",
                    "label": 0
                },
                {
                    "sent": "I said observations.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 1
                },
                {
                    "sent": "Look at the maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "And again, split it into factorizes into simple forms.",
                    "label": 0
                },
                {
                    "sent": "Now, if I look at the form of POVI given Theta is the sum over hi of this term here.",
                    "label": 0
                },
                {
                    "sent": "Right, but this submission, though we couple the para meters and that means that again we had to also with with the the case before we need to use numerical methods to perform learning in this well.",
                    "label": 1
                },
                {
                    "sent": "Similarly for the base in case the posterior of the para meters will not a couple anymore and we will need to use some approximate metalform.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Solving this problem.",
                    "label": 0
                },
                {
                    "sent": "No, I'm reaching the end.",
                    "label": 0
                },
                {
                    "sent": "I think this is one of my my last slides.",
                    "label": 0
                },
                {
                    "sent": "I want to describe little bit about expectation optimization in market for maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "So as we said we cannot use.",
                    "label": 0
                },
                {
                    "sent": "We had this problem of this coupling and so the trick.",
                    "label": 1
                },
                {
                    "sent": "One of the trick that we can use is to replace the log likelihood who the bound that as as the couple form.",
                    "label": 0
                },
                {
                    "sent": "So in order to do that we can introduce a distribution Q and take the kullback, Leiber divergent between.",
                    "label": 1
                },
                {
                    "sent": "The QH given V and my original posterior distribution P of H. Given B&P to know if I write down the KL divergent swear, I used as near the angle notation to denote expectation with respect to this distribution.",
                    "label": 0
                },
                {
                    "sent": "Here, from the property of the KL divergent is always greater equal to zero.",
                    "label": 0
                },
                {
                    "sent": "I can obtain abound on the likelihood here.",
                    "label": 0
                },
                {
                    "sent": "I've found that will have an improper turn and some other time here that depend on my parameter.",
                    "label": 0
                },
                {
                    "sent": "Now if I said Q of H given V, Theta, yes, I think I forgot the theater here there, but.",
                    "label": 1
                },
                {
                    "sent": "Hey Court, 2 hour.",
                    "label": 0
                },
                {
                    "sent": "Original distribution posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "But where I have a whole parameter here then this entropy term here.",
                    "label": 1
                },
                {
                    "sent": "Does not depend on Theta.",
                    "label": 0
                },
                {
                    "sent": "And therefore we have only this term that the penalty to burnout weakened a couple this term very easily.",
                    "label": 0
                },
                {
                    "sent": "Into and split it into terms such that, Tita, we appears only in this term.",
                    "label": 1
                },
                {
                    "sent": "Here entity each appearing only this term here.",
                    "label": 0
                },
                {
                    "sent": "So now we have a bounded task, the couple form and learning or maximizing this bound.",
                    "label": 0
                },
                {
                    "sent": "Now with respect to either Theta V or Theta H is easy.",
                    "label": 0
                },
                {
                    "sent": "We so doing that we obtain kind of iterative.",
                    "label": 0
                },
                {
                    "sent": "Great call expectation organization in which we have any step in which we need to compute the posterior distribution of H given V an theater, all using some inference method and once we have compute this that I need, we need to maximize the bound.",
                    "label": 0
                },
                {
                    "sent": "I can maximize the bound with respect to Theta and then update my para meter.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I think yes, that's the end of my talk a bit earlier than expected, I think.",
                    "label": 0
                },
                {
                    "sent": "So this is a little bit of reading.",
                    "label": 0
                },
                {
                    "sent": "You can live there all books.",
                    "label": 0
                },
                {
                    "sent": "The first one will be published soon.",
                    "label": 0
                },
                {
                    "sent": "An nearly all example and then most I showed today where taken from this book you can download it online and I also have some copies.",
                    "label": 0
                },
                {
                    "sent": "If you order chapters containing the material, explain today here if you want to take a look and this is another book written by machine learning person.",
                    "label": 0
                },
                {
                    "sent": "But you certainly know, which is also quite quite simple to understand, and then these are more.",
                    "label": 0
                },
                {
                    "sent": "Yes, they also interesting book, but written by statisticians.",
                    "label": 0
                },
                {
                    "sent": "So yes, other people in the field.",
                    "label": 0
                },
                {
                    "sent": "OK, that's it.",
                    "label": 0
                }
            ]
        }
    }
}