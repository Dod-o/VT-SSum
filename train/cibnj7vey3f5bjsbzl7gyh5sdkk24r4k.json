{
    "id": "cibnj7vey3f5bjsbzl7gyh5sdkk24r4k",
    "title": "Hierarchical Classification via Orthogonal Transfer",
    "info": {
        "author": [
            "Lin Xiao, Microsoft Research"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_xiao_hco/",
    "segmentation": [
        [
            "OK, I think on the schedule it was.",
            "Turning job, but it turns out I will give the talk so posting on memory also with Microsoft."
        ],
        [
            "We consider hierarchical hierarchical classification problems.",
            "Basically multi class classification.",
            "You have one to our labels, but the labels are organized as a hierarchical structure.",
            "Here we.",
            "In particular, consider a category tree.",
            "There are other possible hierarchical structure, but here we only focus on trees.",
            "Given a set of training data, Zion YIM of them each XI in RN and Y is one of the labels we want to learn classification function which Maps, you know I know example to one of those labels.",
            "So here's an example.",
            "You might have some text categorization problem or document classification.",
            "Using technology in the category of business and then.",
            "Some of them are about auto industry and the other one about insurance.",
            "Under insurance you could have like auto insurance or home insurance.",
            "So this is nice and organized hierarchical structure an if you can wear that lives only in two dimension.",
            "For example, for simplicity just the two dimensions are the frequency of the word liability and the focus of the word airbag.",
            "You can imagine that in auto industry they may not really many.",
            "Liability mentioned instead.",
            "But on the other hand, the frequency airbags very because some text about auto industry will focus airbag.",
            "Another could completely has no air bag in it, but on the other hand for insurance you know most often will help the liability inside or a high frequency in the article.",
            "But of course between auto, industry and home insurance, the freezer airbag is the major distinguishing feature.",
            "So the question is giving such data, how do you train the classifier?"
        ],
        [
            "One way no immediate thing you can do is to ignore the hierarchical structure.",
            "Just treat the leaves the leaf categories at them.",
            "Treat them equally as multiple class on position problem.",
            "You want to train multiclass support vector machine.",
            "Essentially, at each node you will put a vector.",
            "WWV at node V, and then you just measure the inner product between the.",
            "Example with that vector and then pick the high score, then say that's my class and you want to hide.",
            "Classification margin with one and then of course cases when the case is not separable.",
            "You want to some select variable and you balance the average select variable with the norm that translate into margin.",
            "Of the support vector machine, the classical machine and see is just a relative constant an this method has been used.",
            "Format classification has been very quite effective, but as we can say it completely ignored the hierarchical structure."
        ],
        [
            "So the question is, can we improve accuracy using by using the hierarchical structure?",
            "So some obvious ways you do decomposition instead at each node.",
            "So for each node which has a.",
            "The top node, last node if they have no empty set or children, you just server independent.",
            "Sorry, independent multiclass SVM at each inner node.",
            "And they don't have to be anything related.",
            "Could be completely independent.",
            "Of course.",
            "You can introduce hierarchical induced regularization.",
            "Mostly you want forced the classifier adjusts node.",
            "Maybe if they have a parent child relationship you want their classifier to be close similar.",
            "An LMS errors in the literature is called tree induced loss, essentially you.",
            "Penalize the misclassification between two classes.",
            "But amount which is proportional to the.",
            "C graph distance in the tree.",
            "For example."
        ],
        [
            "In this one, you want to penalize classification between this one lightly, but penalized the classic error between this one.",
            "That one very heavily.",
            "This makes sense when using measure how, how useful your classification technique is, but it kind of doesn't make much sense if you want to train them.",
            "Because, but but doing this kind of waiting, you essentially downplays the difficulties, which exists mostly in classifying very similar classes.",
            "That's the most hard part, and if you would with small number you know you don't really learn enough about that."
        ],
        [
            "OK, so.",
            "Indeed, in the literature from the competition experiment, say now could really outperform the flat math class as we are in accuracy.",
            "And in this talk we in this work we give another try."
        ],
        [
            "Hopefully we can do something better.",
            "The key observation is that the classification at different levels of hierarchy rely on very different features.",
            "For example, if you classify between auto industry insurance, the word liabilities is very good indicator.",
            "But then if you go down the second level, you see you want to classify them the word lab, but it doesn't really.",
            "You know it's critical.",
            "It's hard to classify them based on the focus overlap, but instead the airbag.",
            "Easily distinguish these two, but.",
            "Bareback on the upper level doesn't really make sense because it could mix auto insurance auto industry easily.",
            "So the basic intuition is that classification advertisement levels may rely on very different features, or if they rely on the same features, they could be very different combinations of the same features."
        ],
        [
            "So the idea you know we somehow got extreme idea, say, makes the classifier at different levels different.",
            "And we will make them orthogonal instead.",
            "I will explain more detail about what is mean by orthogonal."
        ],
        [
            "So our indeed will introduce her as well with Saga Transfer, and I'll give a sufficient condition.",
            "See this SVM training is a convex optimization problem and then.",
            "Present the algorithm to solve it efficiently."
        ],
        [
            "Some experience first, some notations.",
            "Some standard notation is give a tree.",
            "Anne.",
            "C is set our children's as is set of siblings.",
            "A is the set of ancestors aplus mean you know sort of ancestors plus itself similar for the descendants.",
            "This is all standard."
        ],
        [
            "OK, so now we have a before we train the classifier, we have to say what classifier we want to train, right that we have a user recursive classifier.",
            "Essentially we never get unknown document X, you put in a rule, say.",
            "You know, just classified by on the first level she tried W1W2 transpose X picks the larger one.",
            "If W two wins, you go to the next level Southwest through W for you.",
            "Pick the one you know until you go to the bottom.",
            "There's no more children classes, so this is classifier we trained essentially at each node except for the route you have a vector WV to be trained."
        ],
        [
            "Indeed, here is the formulation directly, so the idea is that.",
            "To solve that, we want to.",
            "So for this linear classifiers we know that essentially is a hyperplane for class for separating two classes.",
            "Well, if you want you can use it for separating different classes with piecewise linear.",
            "Once we mix normal vectors of the hyperplanes orthogonal to those at its ancestors.",
            "Here, so the first term is summation of the quadratic norms.",
            "This classical regularization, the last term, is the.",
            "So the select variables.",
            "And this is constraint.",
            "Similar as before, you want to relax.",
            "By a select variable, see an notice that.",
            "Here we have this new term which is W transpose subway.",
            "Take absolute value, so if that number is small, you can say you know if this zero is these two vectors orthogonal, they won't minimize.",
            "Function including that you're trying to make the WU&V orthogonal, but only if.",
            "You is ancestor of E. If they are not on the path from the root, then you do not penalize the inner product.",
            "Similarly, when you do the constraint, you say I only compare.",
            "Two vectors at which which is a sibling to each other.",
            "Here K is just some given parameter.",
            "We're going to decide later.",
            "See if this problem is convex or not.",
            "And so, for example, giving XI.",
            "In this constant you will have a constraint of W1 and W2 here in the same constraint and you will also have W3W four in the same constraints.",
            "Something like that, because their siblings, but there's no constraint between W1W3W1W four Oh no constant between this to either.",
            "There's a penalty between these two, but there's no constraint in the in this large margin characterization.",
            "So in summary, this is the we added some structure in terms of constraints, how to constrain them.",
            "We do not penalize all pairs.",
            "But the main point is that we want to add a summation of the.",
            "We say awaiting some of the absolute values of the inner products.",
            "Obviously this is not really a convex optimization problem in general."
        ],
        [
            "So the question is when this problem is convex.",
            "Indeed, it is sufficient to establish convexity of.",
            "Same here, I just, you know, just the first 2 terms.",
            "You can write it in that way in general.",
            "So if you think you can let KUV equals to 0 if there's not an innocence in pass.",
            "And the theorem is that.",
            "So here we assume all the key coefficients are non negative.",
            "1st place and then you say if you construct symmetric matrix, we also assume this symmetric an mix off diagonal of the negative ones.",
            "Here I don't really need the absolute because I assume everything is not negative negative.",
            "Then you say if this matrix is positive semidefinite then.",
            "This function is convex.",
            "And for example, if.",
            "If KU is that dominant, you can show this.",
            "Indeed, it would be the case.",
            "And also if if K is positive then it's straight convex.",
            "The proof is."
        ],
        [
            "Very elementary, just directly use definition of convexity you see for two vectors and this is the classification of that context.",
            "If you if this one is nonnegative, you say this function is.",
            "Convex, you just go through some.",
            "And in the end you will reach this.",
            "Inequality there, and this is apparently this one is a positive definite.",
            "Then this is a convex function.",
            "In fact, we only need K bar to become positive because those those numbers multiplies is all nonnegative or norms.",
            "Composite means that is positive definite on.",
            "On active vectors."
        ],
        [
            "OK, we can also show a representative theorem that saying that if K bar is positive definite, then the solution admitted representation of this form.",
            "So this means that it's possible to extend a method to more general nonlinear classifiers, which we haven't done."
        ],
        [
            "OK, now the question is how do we solve this problem efficiently?",
            "It's convex, but that doesn't mean you have an efficient method to solve it.",
            "We can not transform these two quadratic program as a classical SVM.",
            "Obviously not a linear program aesopi.",
            "I've tried SDP know doesn't work.",
            "Of course you say."
        ],
        [
            "Just convert it to a equivalent unconstrained optimization problem, because this 'cause I you can you can, you know, put the hinge loss in this way.",
            "Is only active at the maximum.",
            "This active ones is the maximum one's maximum margin here.",
            "And this is unconstrained authorization problem.",
            "Apparently nonsmooth convex or non SMS.",
            "You can use classical subgradient methods.",
            "And.",
            "Also perform this rough gives some.",
            "Let's talk summary on some other new methods for solving nonsmooth convex problems.",
            "Here we we do is we will do."
        ],
        [
            "Something.",
            "No, I was first say what we do then explain why we do it.",
            "With defined object function like this this you know.",
            "I don't read everything here, but just write the first 2 terms is some function behind that.",
            "We assume keyboards policy is definitely and then we let.",
            "Smallest arguing value called Lambda mean.",
            "And then we split this function as two terms, why is?",
            "Five, which is this term?",
            "Stabenow you minus Lambda mean.",
            "So because I mean smallest eigenvalue of the keyboard, so this is still convex function.",
            "Can you use the function key?",
            "The term is still positive, some definite.",
            "This source is still convex, but here you get a strongly convex term.",
            "On a strongly convex term then."
        ],
        [
            "You know we go through this optimization of composite objectives, which is 5 is the convex, possibly non smooth Pussy is strongly convex, but it's simple in the sense that if you give a vector and is easy to solve this problem with inner product process, this beside of W which is you know if you can solve this problem easily or have an attic formal solution.",
            "We called Simple an apparently Jesus subgradient of I.",
            "You can also use this with your lower bound of the.",
            "Objective function."
        ],
        [
            "So here is algorithm.",
            "This is extension of natural still averaging algorithm.",
            "You start, so at each step you just compute subgradient and then you compute the running average of all your past subgradients and then you solve support.",
            "This is simple to solve, you just solve this minimisation problem and then you.",
            "You can also do update the upper bound easily.",
            "Just pick the best one you have ever got.",
            "The lower bound you can compute.",
            "I miss the details here, just.",
            "By using this kind of this point there.",
            "And then you can access."
        ],
        [
            "And this algorithm.",
            "Indeed you can show that it has complexity.",
            "Log T / T which is much better than if you just use subgradient method.",
            "You will get 1 / T ^2.",
            "Complexity for solving to the same precision.",
            "And indeed, I would say.",
            "Is very closely related to the Pegasus.",
            "They are almost.",
            "No equivalent if W is.",
            "Is the full space in the unconstrained case.",
            "But here, of course we have this nice way to give.",
            "Guaranteed automatic bonds which passes don't have."
        ],
        [
            "How many minutes do I have?",
            "Sorry, OK, enough time.",
            "Yeah, OK, we did some preliminary experiments on some text categorization datasets.",
            "Why is MCV one V2?",
            "And the other one is 20 newsgroups.",
            "We thought this is this is the massively proposed and then we solved.",
            "We did some data processing.",
            "How to say data processing to make the hierarchical structure more pronounced?",
            "There's some.",
            "We just remote some large categories which is flat.",
            "There's no hierarchical, you know.",
            "It's essentially if there's a large portion of the data, do not have a hierarchical structure, is hard to judge how effective this unheralded structure, so it is the remove some data set.",
            "Remove some data and we compare with the flat multi class one and some other methods will not go into detail here which is related to the one I.",
            "Reviewed in the introduction, including tree last.",
            "Penalty and also the you know you can including this ones are including the.",
            "Regularization to make the classifier at different level to be similar instead of more different so those but at end of day you know this is the MCV one data has 3 three different categories we.",
            "We experimented and we got.",
            "This is essentially we run 50 around the random stability or training citing data set and then got the.",
            "Standard deviation behind it and you can see our method on this limited data set shows it's very promising.",
            "And but also on the 20 news, we don't get much improvement.",
            "You don't really know what the reason we're still in.",
            "Maybe the hierarchical structure there is not really helpful in nature, but we don't know."
        ],
        [
            "OK to summarize.",
            "We proposed a hierarchical classification method based on orthogonal transfer between the classifier at different levels.",
            "So the key observation is based on the classification that different levels rely on rely on different features.",
            "Different combination of same features.",
            "An established the condition sufficient condition for this problem to be convex.",
            "An also present a variation of violence of the delivery method.",
            "We can solve this quite efficiently.",
            "The liminary experiment."
        ],
        [
            "Promising.",
            "Also would like to point out that just two results are independent interest.",
            "WHI is in general we can see.",
            "Just optimize this type of the function which you know general constraints that we showed that if kij if you negate the.",
            "Off the terms.",
            "We've got a keyboard key.",
            "Bars is positive, some definitely.",
            "Then this indeed is a convex function.",
            "You know, I think there might be other applications other than the one we consider here.",
            "Indeed, in compressed sensing, I don't know, I just make some random collection in compressed sensing.",
            "You see, when you design the sensing matrix you will see I want the columns to be as early as possible.",
            "Maybe there's some hint there.",
            "And also the new variant or regular dredging method is general for solving.",
            "Now the most problem with.",
            "Is strongly convex regularization I think.",
            "Which it has been proven very effective in solving this problem."
        ],
        [
            "And there's some future work.",
            "Of course, the first we need to do more extensive computational experiments in order to draw more concrete conclusions.",
            "The second is that we started this work from the intuitive like is this a good idea?",
            "Yeah, but.",
            "Do we have any theoretical support?",
            "See this is better in terms of learning theory perspective we don't have yet, but this is definitely a very interesting problem to workout.",
            "Thank you.",
            "Can you recommend where does the extra log factor come from in the convergence and lost track?",
            "Even log TYT?",
            "Oh, this one.",
            "I think it might be possible to get rid of the log.",
            "I use some.",
            "I use some proof for this for the online.",
            "In the morning or online setup, in stochastic online setup then I just let everything to be deterministic, then immediately get this bound without any effort.",
            "But maybe it's possible to get rid of that.",
            "I think should be able to determine.",
            "In the experiments.",
            "Do you remember what kind of her values do you get for lumbering like Islam?",
            "That number come up with music?",
            "Ola mean.",
            "Lambda mean.",
            "I think I used one.",
            "I have to see.",
            "If it will be, it's all relatively relative to say relative.",
            "In the say.",
            "I mean, if you modify the same here.",
            "Here it depends on this one, right?",
            "And then I mean it's all relative.",
            "Yeah, I think we picked them both once.",
            "And make sure that the case is strongly convex with minimal effort.",
            "Oh no, sorry, not not not.",
            "We picked the this is 1 and this is off.",
            "Diagonals are once and to this one big enough to make it make it positive some definite.",
            "Then I forgot what the exact value of the Lambda.",
            "Actually this is ready.",
            "Yeah.",
            "Search for exact motivation to subjektive random mean.",
            "To make your realizers problem convex, yeah, so here is that.",
            "The main motivation is essentially we want to use.",
            "Explicit a strongly convex regularization here and then we can get the algorithm with with.",
            "It still applies when they realize just convex customer comments.",
            "Right, yeah, yeah.",
            "Essentially that's the same.",
            "Same algorithm.",
            "Apply the trick.",
            "Yeah, yeah, I think I think it is necessary, yeah?",
            "It is an answer and the point is structured optimization.",
            "You have to suffer this to make that explicit and if you mix them just calculus, upgrading using subgradient method on that J function.",
            "Maybe you can show.",
            "A convergence result if you know the if you know the.",
            "Strong convex parameter by choosing the proper step size.",
            "Indeed, that's what Pegasus start date.",
            "Yeah, but here I won't see the the new not really new.",
            "This is compared to previous method we can actually get the upper lower bound.",
            "Indeed this already hinted by by nature of the original primal dual separating methods.",
            "You can easily compute lower upper bound.",
            "Saying the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I think on the schedule it was.",
                    "label": 0
                },
                {
                    "sent": "Turning job, but it turns out I will give the talk so posting on memory also with Microsoft.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We consider hierarchical hierarchical classification problems.",
                    "label": 1
                },
                {
                    "sent": "Basically multi class classification.",
                    "label": 0
                },
                {
                    "sent": "You have one to our labels, but the labels are organized as a hierarchical structure.",
                    "label": 1
                },
                {
                    "sent": "Here we.",
                    "label": 0
                },
                {
                    "sent": "In particular, consider a category tree.",
                    "label": 0
                },
                {
                    "sent": "There are other possible hierarchical structure, but here we only focus on trees.",
                    "label": 0
                },
                {
                    "sent": "Given a set of training data, Zion YIM of them each XI in RN and Y is one of the labels we want to learn classification function which Maps, you know I know example to one of those labels.",
                    "label": 1
                },
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "You might have some text categorization problem or document classification.",
                    "label": 0
                },
                {
                    "sent": "Using technology in the category of business and then.",
                    "label": 0
                },
                {
                    "sent": "Some of them are about auto industry and the other one about insurance.",
                    "label": 1
                },
                {
                    "sent": "Under insurance you could have like auto insurance or home insurance.",
                    "label": 0
                },
                {
                    "sent": "So this is nice and organized hierarchical structure an if you can wear that lives only in two dimension.",
                    "label": 0
                },
                {
                    "sent": "For example, for simplicity just the two dimensions are the frequency of the word liability and the focus of the word airbag.",
                    "label": 0
                },
                {
                    "sent": "You can imagine that in auto industry they may not really many.",
                    "label": 0
                },
                {
                    "sent": "Liability mentioned instead.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, the frequency airbags very because some text about auto industry will focus airbag.",
                    "label": 0
                },
                {
                    "sent": "Another could completely has no air bag in it, but on the other hand for insurance you know most often will help the liability inside or a high frequency in the article.",
                    "label": 0
                },
                {
                    "sent": "But of course between auto, industry and home insurance, the freezer airbag is the major distinguishing feature.",
                    "label": 0
                },
                {
                    "sent": "So the question is giving such data, how do you train the classifier?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One way no immediate thing you can do is to ignore the hierarchical structure.",
                    "label": 1
                },
                {
                    "sent": "Just treat the leaves the leaf categories at them.",
                    "label": 1
                },
                {
                    "sent": "Treat them equally as multiple class on position problem.",
                    "label": 0
                },
                {
                    "sent": "You want to train multiclass support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Essentially, at each node you will put a vector.",
                    "label": 0
                },
                {
                    "sent": "WWV at node V, and then you just measure the inner product between the.",
                    "label": 0
                },
                {
                    "sent": "Example with that vector and then pick the high score, then say that's my class and you want to hide.",
                    "label": 0
                },
                {
                    "sent": "Classification margin with one and then of course cases when the case is not separable.",
                    "label": 0
                },
                {
                    "sent": "You want to some select variable and you balance the average select variable with the norm that translate into margin.",
                    "label": 0
                },
                {
                    "sent": "Of the support vector machine, the classical machine and see is just a relative constant an this method has been used.",
                    "label": 0
                },
                {
                    "sent": "Format classification has been very quite effective, but as we can say it completely ignored the hierarchical structure.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question is, can we improve accuracy using by using the hierarchical structure?",
                    "label": 1
                },
                {
                    "sent": "So some obvious ways you do decomposition instead at each node.",
                    "label": 0
                },
                {
                    "sent": "So for each node which has a.",
                    "label": 0
                },
                {
                    "sent": "The top node, last node if they have no empty set or children, you just server independent.",
                    "label": 1
                },
                {
                    "sent": "Sorry, independent multiclass SVM at each inner node.",
                    "label": 0
                },
                {
                    "sent": "And they don't have to be anything related.",
                    "label": 0
                },
                {
                    "sent": "Could be completely independent.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "You can introduce hierarchical induced regularization.",
                    "label": 0
                },
                {
                    "sent": "Mostly you want forced the classifier adjusts node.",
                    "label": 0
                },
                {
                    "sent": "Maybe if they have a parent child relationship you want their classifier to be close similar.",
                    "label": 1
                },
                {
                    "sent": "An LMS errors in the literature is called tree induced loss, essentially you.",
                    "label": 1
                },
                {
                    "sent": "Penalize the misclassification between two classes.",
                    "label": 0
                },
                {
                    "sent": "But amount which is proportional to the.",
                    "label": 0
                },
                {
                    "sent": "C graph distance in the tree.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this one, you want to penalize classification between this one lightly, but penalized the classic error between this one.",
                    "label": 0
                },
                {
                    "sent": "That one very heavily.",
                    "label": 0
                },
                {
                    "sent": "This makes sense when using measure how, how useful your classification technique is, but it kind of doesn't make much sense if you want to train them.",
                    "label": 0
                },
                {
                    "sent": "Because, but but doing this kind of waiting, you essentially downplays the difficulties, which exists mostly in classifying very similar classes.",
                    "label": 0
                },
                {
                    "sent": "That's the most hard part, and if you would with small number you know you don't really learn enough about that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Indeed, in the literature from the competition experiment, say now could really outperform the flat math class as we are in accuracy.",
                    "label": 0
                },
                {
                    "sent": "And in this talk we in this work we give another try.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hopefully we can do something better.",
                    "label": 0
                },
                {
                    "sent": "The key observation is that the classification at different levels of hierarchy rely on very different features.",
                    "label": 1
                },
                {
                    "sent": "For example, if you classify between auto industry insurance, the word liabilities is very good indicator.",
                    "label": 0
                },
                {
                    "sent": "But then if you go down the second level, you see you want to classify them the word lab, but it doesn't really.",
                    "label": 0
                },
                {
                    "sent": "You know it's critical.",
                    "label": 0
                },
                {
                    "sent": "It's hard to classify them based on the focus overlap, but instead the airbag.",
                    "label": 0
                },
                {
                    "sent": "Easily distinguish these two, but.",
                    "label": 0
                },
                {
                    "sent": "Bareback on the upper level doesn't really make sense because it could mix auto insurance auto industry easily.",
                    "label": 0
                },
                {
                    "sent": "So the basic intuition is that classification advertisement levels may rely on very different features, or if they rely on the same features, they could be very different combinations of the same features.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea you know we somehow got extreme idea, say, makes the classifier at different levels different.",
                    "label": 1
                },
                {
                    "sent": "And we will make them orthogonal instead.",
                    "label": 0
                },
                {
                    "sent": "I will explain more detail about what is mean by orthogonal.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our indeed will introduce her as well with Saga Transfer, and I'll give a sufficient condition.",
                    "label": 1
                },
                {
                    "sent": "See this SVM training is a convex optimization problem and then.",
                    "label": 0
                },
                {
                    "sent": "Present the algorithm to solve it efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some experience first, some notations.",
                    "label": 0
                },
                {
                    "sent": "Some standard notation is give a tree.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "C is set our children's as is set of siblings.",
                    "label": 1
                },
                {
                    "sent": "A is the set of ancestors aplus mean you know sort of ancestors plus itself similar for the descendants.",
                    "label": 0
                },
                {
                    "sent": "This is all standard.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we have a before we train the classifier, we have to say what classifier we want to train, right that we have a user recursive classifier.",
                    "label": 0
                },
                {
                    "sent": "Essentially we never get unknown document X, you put in a rule, say.",
                    "label": 0
                },
                {
                    "sent": "You know, just classified by on the first level she tried W1W2 transpose X picks the larger one.",
                    "label": 0
                },
                {
                    "sent": "If W two wins, you go to the next level Southwest through W for you.",
                    "label": 0
                },
                {
                    "sent": "Pick the one you know until you go to the bottom.",
                    "label": 0
                },
                {
                    "sent": "There's no more children classes, so this is classifier we trained essentially at each node except for the route you have a vector WV to be trained.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Indeed, here is the formulation directly, so the idea is that.",
                    "label": 0
                },
                {
                    "sent": "To solve that, we want to.",
                    "label": 0
                },
                {
                    "sent": "So for this linear classifiers we know that essentially is a hyperplane for class for separating two classes.",
                    "label": 0
                },
                {
                    "sent": "Well, if you want you can use it for separating different classes with piecewise linear.",
                    "label": 0
                },
                {
                    "sent": "Once we mix normal vectors of the hyperplanes orthogonal to those at its ancestors.",
                    "label": 1
                },
                {
                    "sent": "Here, so the first term is summation of the quadratic norms.",
                    "label": 0
                },
                {
                    "sent": "This classical regularization, the last term, is the.",
                    "label": 0
                },
                {
                    "sent": "So the select variables.",
                    "label": 0
                },
                {
                    "sent": "And this is constraint.",
                    "label": 0
                },
                {
                    "sent": "Similar as before, you want to relax.",
                    "label": 0
                },
                {
                    "sent": "By a select variable, see an notice that.",
                    "label": 0
                },
                {
                    "sent": "Here we have this new term which is W transpose subway.",
                    "label": 0
                },
                {
                    "sent": "Take absolute value, so if that number is small, you can say you know if this zero is these two vectors orthogonal, they won't minimize.",
                    "label": 0
                },
                {
                    "sent": "Function including that you're trying to make the WU&V orthogonal, but only if.",
                    "label": 0
                },
                {
                    "sent": "You is ancestor of E. If they are not on the path from the root, then you do not penalize the inner product.",
                    "label": 0
                },
                {
                    "sent": "Similarly, when you do the constraint, you say I only compare.",
                    "label": 0
                },
                {
                    "sent": "Two vectors at which which is a sibling to each other.",
                    "label": 0
                },
                {
                    "sent": "Here K is just some given parameter.",
                    "label": 0
                },
                {
                    "sent": "We're going to decide later.",
                    "label": 0
                },
                {
                    "sent": "See if this problem is convex or not.",
                    "label": 0
                },
                {
                    "sent": "And so, for example, giving XI.",
                    "label": 0
                },
                {
                    "sent": "In this constant you will have a constraint of W1 and W2 here in the same constraint and you will also have W3W four in the same constraints.",
                    "label": 0
                },
                {
                    "sent": "Something like that, because their siblings, but there's no constraint between W1W3W1W four Oh no constant between this to either.",
                    "label": 0
                },
                {
                    "sent": "There's a penalty between these two, but there's no constraint in the in this large margin characterization.",
                    "label": 0
                },
                {
                    "sent": "So in summary, this is the we added some structure in terms of constraints, how to constrain them.",
                    "label": 0
                },
                {
                    "sent": "We do not penalize all pairs.",
                    "label": 0
                },
                {
                    "sent": "But the main point is that we want to add a summation of the.",
                    "label": 0
                },
                {
                    "sent": "We say awaiting some of the absolute values of the inner products.",
                    "label": 0
                },
                {
                    "sent": "Obviously this is not really a convex optimization problem in general.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question is when this problem is convex.",
                    "label": 1
                },
                {
                    "sent": "Indeed, it is sufficient to establish convexity of.",
                    "label": 1
                },
                {
                    "sent": "Same here, I just, you know, just the first 2 terms.",
                    "label": 0
                },
                {
                    "sent": "You can write it in that way in general.",
                    "label": 0
                },
                {
                    "sent": "So if you think you can let KUV equals to 0 if there's not an innocence in pass.",
                    "label": 0
                },
                {
                    "sent": "And the theorem is that.",
                    "label": 0
                },
                {
                    "sent": "So here we assume all the key coefficients are non negative.",
                    "label": 0
                },
                {
                    "sent": "1st place and then you say if you construct symmetric matrix, we also assume this symmetric an mix off diagonal of the negative ones.",
                    "label": 0
                },
                {
                    "sent": "Here I don't really need the absolute because I assume everything is not negative negative.",
                    "label": 1
                },
                {
                    "sent": "Then you say if this matrix is positive semidefinite then.",
                    "label": 1
                },
                {
                    "sent": "This function is convex.",
                    "label": 0
                },
                {
                    "sent": "And for example, if.",
                    "label": 0
                },
                {
                    "sent": "If KU is that dominant, you can show this.",
                    "label": 0
                },
                {
                    "sent": "Indeed, it would be the case.",
                    "label": 0
                },
                {
                    "sent": "And also if if K is positive then it's straight convex.",
                    "label": 0
                },
                {
                    "sent": "The proof is.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very elementary, just directly use definition of convexity you see for two vectors and this is the classification of that context.",
                    "label": 1
                },
                {
                    "sent": "If you if this one is nonnegative, you say this function is.",
                    "label": 0
                },
                {
                    "sent": "Convex, you just go through some.",
                    "label": 0
                },
                {
                    "sent": "And in the end you will reach this.",
                    "label": 0
                },
                {
                    "sent": "Inequality there, and this is apparently this one is a positive definite.",
                    "label": 0
                },
                {
                    "sent": "Then this is a convex function.",
                    "label": 0
                },
                {
                    "sent": "In fact, we only need K bar to become positive because those those numbers multiplies is all nonnegative or norms.",
                    "label": 1
                },
                {
                    "sent": "Composite means that is positive definite on.",
                    "label": 0
                },
                {
                    "sent": "On active vectors.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, we can also show a representative theorem that saying that if K bar is positive definite, then the solution admitted representation of this form.",
                    "label": 0
                },
                {
                    "sent": "So this means that it's possible to extend a method to more general nonlinear classifiers, which we haven't done.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now the question is how do we solve this problem efficiently?",
                    "label": 0
                },
                {
                    "sent": "It's convex, but that doesn't mean you have an efficient method to solve it.",
                    "label": 1
                },
                {
                    "sent": "We can not transform these two quadratic program as a classical SVM.",
                    "label": 0
                },
                {
                    "sent": "Obviously not a linear program aesopi.",
                    "label": 0
                },
                {
                    "sent": "I've tried SDP know doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Of course you say.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just convert it to a equivalent unconstrained optimization problem, because this 'cause I you can you can, you know, put the hinge loss in this way.",
                    "label": 1
                },
                {
                    "sent": "Is only active at the maximum.",
                    "label": 0
                },
                {
                    "sent": "This active ones is the maximum one's maximum margin here.",
                    "label": 0
                },
                {
                    "sent": "And this is unconstrained authorization problem.",
                    "label": 0
                },
                {
                    "sent": "Apparently nonsmooth convex or non SMS.",
                    "label": 0
                },
                {
                    "sent": "You can use classical subgradient methods.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Also perform this rough gives some.",
                    "label": 0
                },
                {
                    "sent": "Let's talk summary on some other new methods for solving nonsmooth convex problems.",
                    "label": 0
                },
                {
                    "sent": "Here we we do is we will do.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something.",
                    "label": 0
                },
                {
                    "sent": "No, I was first say what we do then explain why we do it.",
                    "label": 0
                },
                {
                    "sent": "With defined object function like this this you know.",
                    "label": 0
                },
                {
                    "sent": "I don't read everything here, but just write the first 2 terms is some function behind that.",
                    "label": 0
                },
                {
                    "sent": "We assume keyboards policy is definitely and then we let.",
                    "label": 0
                },
                {
                    "sent": "Smallest arguing value called Lambda mean.",
                    "label": 0
                },
                {
                    "sent": "And then we split this function as two terms, why is?",
                    "label": 0
                },
                {
                    "sent": "Five, which is this term?",
                    "label": 0
                },
                {
                    "sent": "Stabenow you minus Lambda mean.",
                    "label": 0
                },
                {
                    "sent": "So because I mean smallest eigenvalue of the keyboard, so this is still convex function.",
                    "label": 0
                },
                {
                    "sent": "Can you use the function key?",
                    "label": 0
                },
                {
                    "sent": "The term is still positive, some definite.",
                    "label": 0
                },
                {
                    "sent": "This source is still convex, but here you get a strongly convex term.",
                    "label": 0
                },
                {
                    "sent": "On a strongly convex term then.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know we go through this optimization of composite objectives, which is 5 is the convex, possibly non smooth Pussy is strongly convex, but it's simple in the sense that if you give a vector and is easy to solve this problem with inner product process, this beside of W which is you know if you can solve this problem easily or have an attic formal solution.",
                    "label": 1
                },
                {
                    "sent": "We called Simple an apparently Jesus subgradient of I.",
                    "label": 1
                },
                {
                    "sent": "You can also use this with your lower bound of the.",
                    "label": 0
                },
                {
                    "sent": "Objective function.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is extension of natural still averaging algorithm.",
                    "label": 0
                },
                {
                    "sent": "You start, so at each step you just compute subgradient and then you compute the running average of all your past subgradients and then you solve support.",
                    "label": 0
                },
                {
                    "sent": "This is simple to solve, you just solve this minimisation problem and then you.",
                    "label": 0
                },
                {
                    "sent": "You can also do update the upper bound easily.",
                    "label": 1
                },
                {
                    "sent": "Just pick the best one you have ever got.",
                    "label": 0
                },
                {
                    "sent": "The lower bound you can compute.",
                    "label": 1
                },
                {
                    "sent": "I miss the details here, just.",
                    "label": 0
                },
                {
                    "sent": "By using this kind of this point there.",
                    "label": 0
                },
                {
                    "sent": "And then you can access.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Indeed you can show that it has complexity.",
                    "label": 0
                },
                {
                    "sent": "Log T / T which is much better than if you just use subgradient method.",
                    "label": 0
                },
                {
                    "sent": "You will get 1 / T ^2.",
                    "label": 0
                },
                {
                    "sent": "Complexity for solving to the same precision.",
                    "label": 0
                },
                {
                    "sent": "And indeed, I would say.",
                    "label": 0
                },
                {
                    "sent": "Is very closely related to the Pegasus.",
                    "label": 0
                },
                {
                    "sent": "They are almost.",
                    "label": 0
                },
                {
                    "sent": "No equivalent if W is.",
                    "label": 0
                },
                {
                    "sent": "Is the full space in the unconstrained case.",
                    "label": 0
                },
                {
                    "sent": "But here, of course we have this nice way to give.",
                    "label": 0
                },
                {
                    "sent": "Guaranteed automatic bonds which passes don't have.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How many minutes do I have?",
                    "label": 0
                },
                {
                    "sent": "Sorry, OK, enough time.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, we did some preliminary experiments on some text categorization datasets.",
                    "label": 0
                },
                {
                    "sent": "Why is MCV one V2?",
                    "label": 0
                },
                {
                    "sent": "And the other one is 20 newsgroups.",
                    "label": 0
                },
                {
                    "sent": "We thought this is this is the massively proposed and then we solved.",
                    "label": 0
                },
                {
                    "sent": "We did some data processing.",
                    "label": 0
                },
                {
                    "sent": "How to say data processing to make the hierarchical structure more pronounced?",
                    "label": 0
                },
                {
                    "sent": "There's some.",
                    "label": 0
                },
                {
                    "sent": "We just remote some large categories which is flat.",
                    "label": 0
                },
                {
                    "sent": "There's no hierarchical, you know.",
                    "label": 0
                },
                {
                    "sent": "It's essentially if there's a large portion of the data, do not have a hierarchical structure, is hard to judge how effective this unheralded structure, so it is the remove some data set.",
                    "label": 0
                },
                {
                    "sent": "Remove some data and we compare with the flat multi class one and some other methods will not go into detail here which is related to the one I.",
                    "label": 0
                },
                {
                    "sent": "Reviewed in the introduction, including tree last.",
                    "label": 0
                },
                {
                    "sent": "Penalty and also the you know you can including this ones are including the.",
                    "label": 0
                },
                {
                    "sent": "Regularization to make the classifier at different level to be similar instead of more different so those but at end of day you know this is the MCV one data has 3 three different categories we.",
                    "label": 0
                },
                {
                    "sent": "We experimented and we got.",
                    "label": 0
                },
                {
                    "sent": "This is essentially we run 50 around the random stability or training citing data set and then got the.",
                    "label": 0
                },
                {
                    "sent": "Standard deviation behind it and you can see our method on this limited data set shows it's very promising.",
                    "label": 0
                },
                {
                    "sent": "And but also on the 20 news, we don't get much improvement.",
                    "label": 0
                },
                {
                    "sent": "You don't really know what the reason we're still in.",
                    "label": 0
                },
                {
                    "sent": "Maybe the hierarchical structure there is not really helpful in nature, but we don't know.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK to summarize.",
                    "label": 0
                },
                {
                    "sent": "We proposed a hierarchical classification method based on orthogonal transfer between the classifier at different levels.",
                    "label": 1
                },
                {
                    "sent": "So the key observation is based on the classification that different levels rely on rely on different features.",
                    "label": 1
                },
                {
                    "sent": "Different combination of same features.",
                    "label": 0
                },
                {
                    "sent": "An established the condition sufficient condition for this problem to be convex.",
                    "label": 0
                },
                {
                    "sent": "An also present a variation of violence of the delivery method.",
                    "label": 0
                },
                {
                    "sent": "We can solve this quite efficiently.",
                    "label": 0
                },
                {
                    "sent": "The liminary experiment.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Promising.",
                    "label": 0
                },
                {
                    "sent": "Also would like to point out that just two results are independent interest.",
                    "label": 0
                },
                {
                    "sent": "WHI is in general we can see.",
                    "label": 0
                },
                {
                    "sent": "Just optimize this type of the function which you know general constraints that we showed that if kij if you negate the.",
                    "label": 0
                },
                {
                    "sent": "Off the terms.",
                    "label": 0
                },
                {
                    "sent": "We've got a keyboard key.",
                    "label": 0
                },
                {
                    "sent": "Bars is positive, some definitely.",
                    "label": 0
                },
                {
                    "sent": "Then this indeed is a convex function.",
                    "label": 0
                },
                {
                    "sent": "You know, I think there might be other applications other than the one we consider here.",
                    "label": 0
                },
                {
                    "sent": "Indeed, in compressed sensing, I don't know, I just make some random collection in compressed sensing.",
                    "label": 0
                },
                {
                    "sent": "You see, when you design the sensing matrix you will see I want the columns to be as early as possible.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's some hint there.",
                    "label": 0
                },
                {
                    "sent": "And also the new variant or regular dredging method is general for solving.",
                    "label": 0
                },
                {
                    "sent": "Now the most problem with.",
                    "label": 0
                },
                {
                    "sent": "Is strongly convex regularization I think.",
                    "label": 0
                },
                {
                    "sent": "Which it has been proven very effective in solving this problem.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's some future work.",
                    "label": 1
                },
                {
                    "sent": "Of course, the first we need to do more extensive computational experiments in order to draw more concrete conclusions.",
                    "label": 1
                },
                {
                    "sent": "The second is that we started this work from the intuitive like is this a good idea?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but.",
                    "label": 0
                },
                {
                    "sent": "Do we have any theoretical support?",
                    "label": 1
                },
                {
                    "sent": "See this is better in terms of learning theory perspective we don't have yet, but this is definitely a very interesting problem to workout.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Can you recommend where does the extra log factor come from in the convergence and lost track?",
                    "label": 0
                },
                {
                    "sent": "Even log TYT?",
                    "label": 0
                },
                {
                    "sent": "Oh, this one.",
                    "label": 0
                },
                {
                    "sent": "I think it might be possible to get rid of the log.",
                    "label": 0
                },
                {
                    "sent": "I use some.",
                    "label": 0
                },
                {
                    "sent": "I use some proof for this for the online.",
                    "label": 0
                },
                {
                    "sent": "In the morning or online setup, in stochastic online setup then I just let everything to be deterministic, then immediately get this bound without any effort.",
                    "label": 0
                },
                {
                    "sent": "But maybe it's possible to get rid of that.",
                    "label": 0
                },
                {
                    "sent": "I think should be able to determine.",
                    "label": 0
                },
                {
                    "sent": "In the experiments.",
                    "label": 0
                },
                {
                    "sent": "Do you remember what kind of her values do you get for lumbering like Islam?",
                    "label": 0
                },
                {
                    "sent": "That number come up with music?",
                    "label": 0
                },
                {
                    "sent": "Ola mean.",
                    "label": 0
                },
                {
                    "sent": "Lambda mean.",
                    "label": 0
                },
                {
                    "sent": "I think I used one.",
                    "label": 0
                },
                {
                    "sent": "I have to see.",
                    "label": 0
                },
                {
                    "sent": "If it will be, it's all relatively relative to say relative.",
                    "label": 0
                },
                {
                    "sent": "In the say.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you modify the same here.",
                    "label": 0
                },
                {
                    "sent": "Here it depends on this one, right?",
                    "label": 0
                },
                {
                    "sent": "And then I mean it's all relative.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think we picked them both once.",
                    "label": 0
                },
                {
                    "sent": "And make sure that the case is strongly convex with minimal effort.",
                    "label": 0
                },
                {
                    "sent": "Oh no, sorry, not not not.",
                    "label": 0
                },
                {
                    "sent": "We picked the this is 1 and this is off.",
                    "label": 0
                },
                {
                    "sent": "Diagonals are once and to this one big enough to make it make it positive some definite.",
                    "label": 0
                },
                {
                    "sent": "Then I forgot what the exact value of the Lambda.",
                    "label": 0
                },
                {
                    "sent": "Actually this is ready.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Search for exact motivation to subjektive random mean.",
                    "label": 0
                },
                {
                    "sent": "To make your realizers problem convex, yeah, so here is that.",
                    "label": 0
                },
                {
                    "sent": "The main motivation is essentially we want to use.",
                    "label": 0
                },
                {
                    "sent": "Explicit a strongly convex regularization here and then we can get the algorithm with with.",
                    "label": 0
                },
                {
                    "sent": "It still applies when they realize just convex customer comments.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Essentially that's the same.",
                    "label": 0
                },
                {
                    "sent": "Same algorithm.",
                    "label": 0
                },
                {
                    "sent": "Apply the trick.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, I think I think it is necessary, yeah?",
                    "label": 0
                },
                {
                    "sent": "It is an answer and the point is structured optimization.",
                    "label": 0
                },
                {
                    "sent": "You have to suffer this to make that explicit and if you mix them just calculus, upgrading using subgradient method on that J function.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can show.",
                    "label": 0
                },
                {
                    "sent": "A convergence result if you know the if you know the.",
                    "label": 0
                },
                {
                    "sent": "Strong convex parameter by choosing the proper step size.",
                    "label": 0
                },
                {
                    "sent": "Indeed, that's what Pegasus start date.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but here I won't see the the new not really new.",
                    "label": 0
                },
                {
                    "sent": "This is compared to previous method we can actually get the upper lower bound.",
                    "label": 0
                },
                {
                    "sent": "Indeed this already hinted by by nature of the original primal dual separating methods.",
                    "label": 0
                },
                {
                    "sent": "You can easily compute lower upper bound.",
                    "label": 0
                },
                {
                    "sent": "Saying the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}