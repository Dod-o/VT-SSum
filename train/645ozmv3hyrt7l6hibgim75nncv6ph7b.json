{
    "id": "645ozmv3hyrt7l6hibgim75nncv6ph7b",
    "title": "Min-Sum Clustering of Protein Sequences with Limited Distance Information",
    "info": {
        "author": [
            "Konstantin Voevodski, Department of Computer Science, Boston University"
        ],
        "published": "Oct. 17, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering",
            "Top->Computer Science->Bioinformatics"
        ]
    },
    "url": "http://videolectures.net/simbad2011_voevodski_clustering/",
    "segmentation": [
        [
            "OK, the title of our talk is min some clustering approaching sequences with limited distance information."
        ],
        [
            "So I wanted to start off with some motivation."
        ],
        [
            "So, classifying protein protein sequences using similarity is a very well studied problem in computational biology, but the size of biological sequence databases is rapidly expanding."
        ],
        [
            "So for example Jane Bank, which is the repository approaching sequence data sizes, nearly doubled every year from 1982 to the president."
        ],
        [
            "So, however, sequence comparison is very computationally intensive.",
            "So usually computing all pairwise similarities may take orders of magnitude more time than performing the actual clustering."
        ],
        [
            "So in this type of setting, we consider clustering with limited distance information.",
            "So what I mean is that the algorithm queries some of the distances between the objects during its execution.",
            "So as I mentioned, these queries are expensive, so the task is to find an accurate clustering using few queries."
        ],
        [
            "So more specifically, we assume that we have access to one versus all distance queries that, given a single object, return the distances between that object and all the other ones in the data set.",
            "So it's like filling in a row or column in the pairwise distance matrix."
        ],
        [
            "So these one versus all queries is especially relevant for sequence similarity search in biology.",
            "So program such as BLAST is optimized to search a single sequence against all other sequences in the database.",
            "So in comparison, pairwise alignments would take a lot longer.",
            "And so even though the search performed by Blast is heuristic, it's been shown many times that its results are very meaningful."
        ],
        [
            "OK, now I want to go on and explain what we mean by clustering accuracy in our work."
        ],
        [
            "So we assume the approximation stability of them in some objective function for clustering.",
            "And more exactly, the approximation stability property we consider is from Balkan plumbing up them who assume the clusterings that approximate the objective function well.",
            "All must be structurally structurally close to some target clustering.",
            "So in this setting, our task is to find a clustering that's close to the target."
        ],
        [
            "So now I wanted to step back and outline what I mean by objective functions for clustering.",
            "So let's suppose we want to.",
            "We want to K clustering.",
            "See the partitions.",
            "The points at us into K says he wants to see K. So, one way to approach this problem is to define some objective function to measure the quality of the clustering and then optimize using that objective."
        ],
        [
            "So in particular them in some objective is the summation overall clusters for each cluster, the sum of the pairwise distances of the points in the clusters.",
            "So the lower the objective value, the better the clustering.",
            "So we will use up fee to denote the optimal objective value for objective function Phi.",
            "So this is the minimum objective.",
            "Are all possible clusterings."
        ],
        [
            "So now I can state the approximation stability property more exactly, so we assume that whenever we have a clustering that is within.",
            "Factor C of optimal.",
            "This clustering must be structurally close to some target clustering CT.",
            "So when every year within a factor C of the optimal value must be structurally close to the target.",
            "So to make the definition complete, I just have to tell you how we compare to clustering structurally, and that's on the bottom of this slide.",
            "So we consider.",
            "The number of mismatched points between.",
            "So we have two clusterings and we consider all possible matches between the two sets of clusters.",
            "And we consider the the optimum.",
            "Are the optimal number of mismatched points."
        ],
        [
            "Subaltern Brahmagupta show that, given the C Upsilon approximation stability.",
            "We can efficiently find clusterings that are structurally close to the target.",
            "Without approximating the objective function.",
            "And they show that this is true even when I see approximation of the objective is hard to compute."
        ],
        [
            "So now where we show that it's possible to find accurate, accurate clusterings in this model, even without knowing most of the distances between the points?",
            "And considering only some of the distances also gives us more efficient algorithms."
        ],
        [
            "So now I wanted to go on and give an outline of the algorithm and then.",
            "Say something about its analysis as well."
        ],
        [
            "So the way it works is that we select some small number of points that we call landmarks and then we use only the distances between the landmarks and other points to find the clustering."
        ],
        [
            "So this is another run of the algorithm.",
            "So the way it works is that we consider balls of radius are around all landmarks for increasing values of our.",
            "So for each value of our we consider our test which looks at the size of the ball times its radius and checks whether this quantity is greater than some threshold T. T is an input parameter.",
            "So if this is the case, then we consider this ball and all balls are overlapping and computer clustering the cluster containing all of those points.",
            "So we then remove these points.",
            "And we keep on going until we find exactly K clusters.",
            "So K is also parameter of the algorithm.",
            "And we only consider relevant values of our meaning.",
            "Only Landmark point distances."
        ],
        [
            "So here is a type of result that we can prove for this type of algorithm.",
            "So we can show that the approximation stability holds for them in some objective function.",
            "We're given the optimum objective value and each cluster in the target clustering is large.",
            "Then with high probability, our algorithm will find a clustering that structurally close to the target.",
            "In time, roughly K log KN log in using K log K1 versus all distance squares.",
            "So, and here is the size of the data set in case the number of clusters."
        ],
        [
            "So the way I proof works is that if the approximation stability holds for the for them in some objective, then the data must have a certain structure.",
            "So each target cluster must contain a core of well separated points.",
            "And we can find this clustering as long as we get a landmark in each cluster core."
        ],
        [
            "So here's a picture of what the data must look like.",
            "If the approximation stability holds for the for the Minnesota objective.",
            "So we have these well separated cluster chords.",
            "But there diameters are inversely proportional to the size of the clusters.",
            "So that means that means that the high cardinality clusters have low diameter.",
            "But these sets are Stillwell separated, so for any given cluster, if its diameter is D, then the distance between any point in that core and any point outside the core must be greater than the sum times some constant.",
            "So you can see that these are well separated by relative to their diameter.",
            "So that means that we must pull out the clusters in the right order.",
            "So for example, if we wait for a ball to contain all of C3 by that point, C1 and C2 may be merged.",
            "So we have to find C1 first, pull it out and then C2 and then C3."
        ],
        [
            "So previously I said that we have to know the optimal objective value, but in practice we usually do not know this.",
            "So we can prove that if this is the case, we can try increasing estimates of the T parameter.",
            "I'm until enough points are clustered.",
            "So in our proof, if we do know the optimal objective value, we said the T that T parameter using it.",
            "But if we don't know, we have to guess it and we can try increasing estimates.",
            "So however, probably we need N squared times the number of landmark iterations to find an accurate clustering.",
            "In such a case.",
            "But Luckily, in practice, the number of iterations that we need is much smaller.",
            "So usually what we do is we set some increment for T and we keep incrementing it, running the algorithm until we cluster enough of the points.",
            "So usually 80 to 90%.",
            "And that works well in practice."
        ],
        [
            "OK, now I wanted to go on and describe some of our computational experiments."
        ],
        [
            "So as I said, we cluster proteins by sequence similarity and blasted out is our one versus all distance squared.",
            "So we compare with gold standard manual classifications in the PFN database.",
            "Also be found.",
            "Classifieds are proteins by evolutionary relatedness and a part of it is manually built and a part of it is automated.",
            "So in our experiments we used from the theoretic part of our work to compare clusterings.",
            "So this is the fraction of mismatch of misclassified points under the optimum matching between the two sets of clusters."
        ],
        [
            "So these datasets are made by randomly selecting some families from the fam.",
            "And these datasets are too large to compute the full distance matrix.",
            "So we can only compare with a method that uses a similar amount of distance information."
        ],
        [
            "So one such method is the following.",
            "We can select a set of landmarks, then we can build a coordinate system using these landmarks and we can perform let's say K means clustering in this space.",
            "So more exactly, will choose a set of landmarks L. Let's say it's cardinality's deep.",
            "Now we're going to embed each point in a D dimensional space using the distances to L. Add to the point in L and then we can use K means clustering.",
            "In this space, the standard MATLAB implementation.",
            "And in our experiments we also compare with their other limited information clustering algorithm from a previous work."
        ],
        [
            "So this is what some of our results look like.",
            "So the way these datasets I created in one through 10 is each time I randomly choose a families from the family of size between 1000 and 10,000.",
            "So these datasets I believe are all between 15 and 20,000 points.",
            "And the three methods I'm comparing is the K means in the embedded space, which I just described.",
            "Then the landmark clustering means some, which is the algorithm I'm presenting now, and the landmark clustering is our algorithm from another work.",
            "So you can see.",
            "So here, for each data set.",
            "We compute.",
            "We use the algorithms to compute the clusterings and then each time we compute, we compare to the ground truth using our distance measure the fraction of misclassified points.",
            "So here lower is better.",
            "So you can see that for all the datasets are limited.",
            "Information algorithms do better than the K means in the embedded space.",
            "But actually the landmark clustering means some usually doesn't do as well as as the other landmark clustering algorithm.",
            "So I suppose other than the data set 7:00 and 9:00, where it does do better?",
            "But still overall we can see that the amount of misclassification is low, and then I suppose we can say here at least when the other algorithm doesn't work well.",
            "This one appears to work better.",
            "But there is some intuition for why this is happening."
        ],
        [
            "So the other algorithm, the landmark clustering, will find an accurate clustering if the product of the approximation stability holds for the K medium objective function.",
            "And when we look at the sequence data, when we actually look at the within cluster distances and the between cluster distances for the ground truth.",
            "This actually looks a lot like the K meeting structure."
        ],
        [
            "So so one challenge we have is actually finding data that has the nontrivial min some structure.",
            "So what I mean is that we must have clusters of varying size, but the diameters must be inversely proportional to the size of the clusters.",
            "So for this data we don't quite see that, so that's one indication why the the K median algorithm actually may outperform this one here.",
            "So this is one thing that we'd like to do in the future.",
            "So I also would like to consider smaller landmark selection.",
            "So the algorithm presented here, we just choose a bunch of landmarks at random and their analysis just ask how many do we need to get one in each cluster core with high probability.",
            "So the problem is that if some of the cluster cores are small, we have to use a lot of landmarks to get one in there.",
            "But there's a possibility possibility that a smarter adaptive strategy will work.",
            "So for example, with their other algorithm, we actually select one of the furthest landmarks from the ones chosen so far, and this works better than random in getting 1 landmark in each cluster core.",
            "So that doesn't quite work here, but maybe a similar approach will work.",
            "So also Lastly, it's interesting to consider the limited information setting given other properties about the structure of the ground truth.",
            "So for example, we can do a thought experiment where we just make a set of assumptions about what the ground truth data looks like, and then maybe we can probably find this structure without needing most of the distance information for the points."
        ],
        [
            "OK."
        ],
        [
            "So this is actually everything I went a little fast.",
            "So how do you test up for me in practice?",
            "It should get some property of the cluster, right?",
            "So let's go back to that.",
            "Right, so one thing we can do is it's hard to test the property explicitly, but one thing we can do is tested data to actually compute the within cluster in between cluster similarities in the distances for the ground truth and then see whether that looks like what must follow from our assumptions.",
            "So this is how we would do it.",
            "So I would actually get the ground truth.",
            "Let's say the P fan classification and I would actually compute these values and for this data actually I do see well separated cluster cores.",
            "But the diameter is done really follow the property that we assume in the theoretic part of our work, it looks more like the diameters are fixed, so yeah, they do not very sore inversely proportional to the size unfortunately, so that's one challenge is to actually find data that looks like this.",
            "Now looking for you don't have the ground truth.",
            "So then it's a bit hard.",
            "Yeah, so so we have to.",
            "We have to know something so yeah.",
            "Yeah, maybe my question goes the same direction you.",
            "You'll be quietly structural stability, so so if you have only explored your data set to a small fraction, is is the partial structural stability already sufficient for for the whole program into work?",
            "Or is it basically blind driving for petrol recognition, so you assume something and then in the end you might find out after you've done a lot of work, but it doesn't hold it.",
            "Could you?",
            "Interpretation is not lists.",
            "Well.",
            "I mean, one answer is a lot of data in real life is, well separable.",
            "So so in the way it has to have some structure, because if it doesn't then it's very hard to find it.",
            "So then two when we do some of these experiments where we make these datasets at random that come from a manual classification and we take a look at what it looks like, it is well separated.",
            "Especially if we only consider a fraction of the points.",
            "So in reality really, the property that does hold is that the approximation stability is true after you remove some points, so I think that's an assumption that's much closer to reality.",
            "Approval of all the entries in this distance movie matrix.",
            "Couldn't it be that I I hallucinate that they are well separated?",
            "And if I would have very whole distance information, I would actually realize it?",
            "It's no longer sunset, not separated in some sense.",
            "So what I would say is it's hard to.",
            "I mean, I strongly believe that actually if you remove some points, if you actually knew which ones to remove and you looked at all the distances between the remaining ones, you would see some very strong structure.",
            "Sort of how to do that experiment the right way to remove some points first.",
            "You know that's hard, so but when we do sample, some of the distances we see something that looks like a strong structure.",
            "So then when we experimentally run the algorithm then we do find low myslef misclassification.",
            "So I think there is some him that it's there.",
            "But yeah, I agree that you know we don't know for sure.",
            "Because with respect to your landmark last ring, it's really scary when on Sunday.",
            "This is an algorithm is doing so well and not another day less.",
            "Everything is such a poor performance.",
            "So so do you know what happened yesterday?",
            "Prefer for some of the datasets like 7 and 9 when I actually looked at the ground truth, some of those clusters there was a pair of clusters that have very high between similarity.",
            "So, so in a way, just looking at that data really, you would say there's nothing wrong from considering those two clusters.",
            "A single cluster, which is what the algorithm did.",
            "So so I would say when I look at them, sometimes they're not that well separated or perfectly separated.",
            "Let's say we may have a case where two clusters have really high up between cluster similarity.",
            "So in that case the algorithm will merge those, yes?",
            "OK, I was just wondering how would you select a number of marks?",
            "Yes so.",
            "So here are all the algorithms are using the same number of landmarks festival, so in there in the algorithm in the landmark clustering for the K median case, we probably need only OK landmarks for the landmark clustering Minnesota we probably need a lock it.",
            "So here in these experiments that K is actually 8, it's only 8 clusters, so I believe here I was using A30K landmarks for all available, so this would correspond to 241 versus all queries and the size of these datasets are between.",
            "15 and 20,000 points.",
            "So, so that's how I said I'm here.",
            "So I suppose yes.",
            "In practice you would just.",
            "You need to know roughly what K is, and then I would say K. To to some number, some constant times the number of clusters that you expect in this data.",
            "So usually if you use more landmarks, it doesn't hurt the algorithm, it will just take much longer to run.",
            "Also good for now.",
            "So this assumption about this inverse relationship in the diameter and the cluster size.",
            "So for me it seems to be in common, incompatible with any generative model.",
            "Because I mean if you add a generative process and you keep on selling data you expect of course the diameter increases, so it's not a bit alarming to have a model for which there is no generative process possible.",
            "Yes, it's a bit of a problem.",
            "So one thing I've been thinking about is you know what type of data would look like that.",
            "I mean, one possibility is that it could possibly work for outlier detection in the way.",
            "Let me go back to the picture.",
            "Right, so you get if you look at this picture here.",
            "Perhaps the three could be a set of outlier points.",
            "So if you're interested in grouping those together, I suppose.",
            "So one property about them in some clustering in general is that they are a Goodman.",
            "Some clustering does not have to be a Voronoi decomposition of the space.",
            "So in some cases, you actually.",
            "Presumably you are interested in finding a clustering that looks like that.",
            "But so far we haven't found too much real life data that looks like this.",
            "I do have to say, yeah.",
            "Questions.",
            "OK, so last night, speaking again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the title of our talk is min some clustering approaching sequences with limited distance information.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I wanted to start off with some motivation.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, classifying protein protein sequences using similarity is a very well studied problem in computational biology, but the size of biological sequence databases is rapidly expanding.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example Jane Bank, which is the repository approaching sequence data sizes, nearly doubled every year from 1982 to the president.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, however, sequence comparison is very computationally intensive.",
                    "label": 0
                },
                {
                    "sent": "So usually computing all pairwise similarities may take orders of magnitude more time than performing the actual clustering.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this type of setting, we consider clustering with limited distance information.",
                    "label": 1
                },
                {
                    "sent": "So what I mean is that the algorithm queries some of the distances between the objects during its execution.",
                    "label": 1
                },
                {
                    "sent": "So as I mentioned, these queries are expensive, so the task is to find an accurate clustering using few queries.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So more specifically, we assume that we have access to one versus all distance queries that, given a single object, return the distances between that object and all the other ones in the data set.",
                    "label": 0
                },
                {
                    "sent": "So it's like filling in a row or column in the pairwise distance matrix.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these one versus all queries is especially relevant for sequence similarity search in biology.",
                    "label": 1
                },
                {
                    "sent": "So program such as BLAST is optimized to search a single sequence against all other sequences in the database.",
                    "label": 0
                },
                {
                    "sent": "So in comparison, pairwise alignments would take a lot longer.",
                    "label": 0
                },
                {
                    "sent": "And so even though the search performed by Blast is heuristic, it's been shown many times that its results are very meaningful.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now I want to go on and explain what we mean by clustering accuracy in our work.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we assume the approximation stability of them in some objective function for clustering.",
                    "label": 1
                },
                {
                    "sent": "And more exactly, the approximation stability property we consider is from Balkan plumbing up them who assume the clusterings that approximate the objective function well.",
                    "label": 1
                },
                {
                    "sent": "All must be structurally structurally close to some target clustering.",
                    "label": 0
                },
                {
                    "sent": "So in this setting, our task is to find a clustering that's close to the target.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I wanted to step back and outline what I mean by objective functions for clustering.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose we want to.",
                    "label": 0
                },
                {
                    "sent": "We want to K clustering.",
                    "label": 0
                },
                {
                    "sent": "See the partitions.",
                    "label": 0
                },
                {
                    "sent": "The points at us into K says he wants to see K. So, one way to approach this problem is to define some objective function to measure the quality of the clustering and then optimize using that objective.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in particular them in some objective is the summation overall clusters for each cluster, the sum of the pairwise distances of the points in the clusters.",
                    "label": 0
                },
                {
                    "sent": "So the lower the objective value, the better the clustering.",
                    "label": 0
                },
                {
                    "sent": "So we will use up fee to denote the optimal objective value for objective function Phi.",
                    "label": 0
                },
                {
                    "sent": "So this is the minimum objective.",
                    "label": 0
                },
                {
                    "sent": "Are all possible clusterings.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I can state the approximation stability property more exactly, so we assume that whenever we have a clustering that is within.",
                    "label": 0
                },
                {
                    "sent": "Factor C of optimal.",
                    "label": 0
                },
                {
                    "sent": "This clustering must be structurally close to some target clustering CT.",
                    "label": 0
                },
                {
                    "sent": "So when every year within a factor C of the optimal value must be structurally close to the target.",
                    "label": 0
                },
                {
                    "sent": "So to make the definition complete, I just have to tell you how we compare to clustering structurally, and that's on the bottom of this slide.",
                    "label": 0
                },
                {
                    "sent": "So we consider.",
                    "label": 0
                },
                {
                    "sent": "The number of mismatched points between.",
                    "label": 0
                },
                {
                    "sent": "So we have two clusterings and we consider all possible matches between the two sets of clusters.",
                    "label": 0
                },
                {
                    "sent": "And we consider the the optimum.",
                    "label": 0
                },
                {
                    "sent": "Are the optimal number of mismatched points.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Subaltern Brahmagupta show that, given the C Upsilon approximation stability.",
                    "label": 0
                },
                {
                    "sent": "We can efficiently find clusterings that are structurally close to the target.",
                    "label": 1
                },
                {
                    "sent": "Without approximating the objective function.",
                    "label": 0
                },
                {
                    "sent": "And they show that this is true even when I see approximation of the objective is hard to compute.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now where we show that it's possible to find accurate, accurate clusterings in this model, even without knowing most of the distances between the points?",
                    "label": 0
                },
                {
                    "sent": "And considering only some of the distances also gives us more efficient algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I wanted to go on and give an outline of the algorithm and then.",
                    "label": 0
                },
                {
                    "sent": "Say something about its analysis as well.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way it works is that we select some small number of points that we call landmarks and then we use only the distances between the landmarks and other points to find the clustering.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is another run of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the way it works is that we consider balls of radius are around all landmarks for increasing values of our.",
                    "label": 0
                },
                {
                    "sent": "So for each value of our we consider our test which looks at the size of the ball times its radius and checks whether this quantity is greater than some threshold T. T is an input parameter.",
                    "label": 0
                },
                {
                    "sent": "So if this is the case, then we consider this ball and all balls are overlapping and computer clustering the cluster containing all of those points.",
                    "label": 0
                },
                {
                    "sent": "So we then remove these points.",
                    "label": 0
                },
                {
                    "sent": "And we keep on going until we find exactly K clusters.",
                    "label": 0
                },
                {
                    "sent": "So K is also parameter of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And we only consider relevant values of our meaning.",
                    "label": 0
                },
                {
                    "sent": "Only Landmark point distances.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a type of result that we can prove for this type of algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we can show that the approximation stability holds for them in some objective function.",
                    "label": 0
                },
                {
                    "sent": "We're given the optimum objective value and each cluster in the target clustering is large.",
                    "label": 1
                },
                {
                    "sent": "Then with high probability, our algorithm will find a clustering that structurally close to the target.",
                    "label": 0
                },
                {
                    "sent": "In time, roughly K log KN log in using K log K1 versus all distance squares.",
                    "label": 0
                },
                {
                    "sent": "So, and here is the size of the data set in case the number of clusters.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the way I proof works is that if the approximation stability holds for the for them in some objective, then the data must have a certain structure.",
                    "label": 1
                },
                {
                    "sent": "So each target cluster must contain a core of well separated points.",
                    "label": 1
                },
                {
                    "sent": "And we can find this clustering as long as we get a landmark in each cluster core.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a picture of what the data must look like.",
                    "label": 0
                },
                {
                    "sent": "If the approximation stability holds for the for the Minnesota objective.",
                    "label": 0
                },
                {
                    "sent": "So we have these well separated cluster chords.",
                    "label": 0
                },
                {
                    "sent": "But there diameters are inversely proportional to the size of the clusters.",
                    "label": 0
                },
                {
                    "sent": "So that means that means that the high cardinality clusters have low diameter.",
                    "label": 0
                },
                {
                    "sent": "But these sets are Stillwell separated, so for any given cluster, if its diameter is D, then the distance between any point in that core and any point outside the core must be greater than the sum times some constant.",
                    "label": 0
                },
                {
                    "sent": "So you can see that these are well separated by relative to their diameter.",
                    "label": 0
                },
                {
                    "sent": "So that means that we must pull out the clusters in the right order.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we wait for a ball to contain all of C3 by that point, C1 and C2 may be merged.",
                    "label": 0
                },
                {
                    "sent": "So we have to find C1 first, pull it out and then C2 and then C3.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So previously I said that we have to know the optimal objective value, but in practice we usually do not know this.",
                    "label": 0
                },
                {
                    "sent": "So we can prove that if this is the case, we can try increasing estimates of the T parameter.",
                    "label": 0
                },
                {
                    "sent": "I'm until enough points are clustered.",
                    "label": 1
                },
                {
                    "sent": "So in our proof, if we do know the optimal objective value, we said the T that T parameter using it.",
                    "label": 0
                },
                {
                    "sent": "But if we don't know, we have to guess it and we can try increasing estimates.",
                    "label": 1
                },
                {
                    "sent": "So however, probably we need N squared times the number of landmark iterations to find an accurate clustering.",
                    "label": 0
                },
                {
                    "sent": "In such a case.",
                    "label": 0
                },
                {
                    "sent": "But Luckily, in practice, the number of iterations that we need is much smaller.",
                    "label": 1
                },
                {
                    "sent": "So usually what we do is we set some increment for T and we keep incrementing it, running the algorithm until we cluster enough of the points.",
                    "label": 0
                },
                {
                    "sent": "So usually 80 to 90%.",
                    "label": 0
                },
                {
                    "sent": "And that works well in practice.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now I wanted to go on and describe some of our computational experiments.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I said, we cluster proteins by sequence similarity and blasted out is our one versus all distance squared.",
                    "label": 0
                },
                {
                    "sent": "So we compare with gold standard manual classifications in the PFN database.",
                    "label": 1
                },
                {
                    "sent": "Also be found.",
                    "label": 0
                },
                {
                    "sent": "Classifieds are proteins by evolutionary relatedness and a part of it is manually built and a part of it is automated.",
                    "label": 0
                },
                {
                    "sent": "So in our experiments we used from the theoretic part of our work to compare clusterings.",
                    "label": 1
                },
                {
                    "sent": "So this is the fraction of mismatch of misclassified points under the optimum matching between the two sets of clusters.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these datasets are made by randomly selecting some families from the fam.",
                    "label": 0
                },
                {
                    "sent": "And these datasets are too large to compute the full distance matrix.",
                    "label": 1
                },
                {
                    "sent": "So we can only compare with a method that uses a similar amount of distance information.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one such method is the following.",
                    "label": 0
                },
                {
                    "sent": "We can select a set of landmarks, then we can build a coordinate system using these landmarks and we can perform let's say K means clustering in this space.",
                    "label": 1
                },
                {
                    "sent": "So more exactly, will choose a set of landmarks L. Let's say it's cardinality's deep.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to embed each point in a D dimensional space using the distances to L. Add to the point in L and then we can use K means clustering.",
                    "label": 1
                },
                {
                    "sent": "In this space, the standard MATLAB implementation.",
                    "label": 0
                },
                {
                    "sent": "And in our experiments we also compare with their other limited information clustering algorithm from a previous work.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is what some of our results look like.",
                    "label": 0
                },
                {
                    "sent": "So the way these datasets I created in one through 10 is each time I randomly choose a families from the family of size between 1000 and 10,000.",
                    "label": 1
                },
                {
                    "sent": "So these datasets I believe are all between 15 and 20,000 points.",
                    "label": 0
                },
                {
                    "sent": "And the three methods I'm comparing is the K means in the embedded space, which I just described.",
                    "label": 0
                },
                {
                    "sent": "Then the landmark clustering means some, which is the algorithm I'm presenting now, and the landmark clustering is our algorithm from another work.",
                    "label": 0
                },
                {
                    "sent": "So you can see.",
                    "label": 0
                },
                {
                    "sent": "So here, for each data set.",
                    "label": 0
                },
                {
                    "sent": "We compute.",
                    "label": 0
                },
                {
                    "sent": "We use the algorithms to compute the clusterings and then each time we compute, we compare to the ground truth using our distance measure the fraction of misclassified points.",
                    "label": 0
                },
                {
                    "sent": "So here lower is better.",
                    "label": 0
                },
                {
                    "sent": "So you can see that for all the datasets are limited.",
                    "label": 0
                },
                {
                    "sent": "Information algorithms do better than the K means in the embedded space.",
                    "label": 1
                },
                {
                    "sent": "But actually the landmark clustering means some usually doesn't do as well as as the other landmark clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "So I suppose other than the data set 7:00 and 9:00, where it does do better?",
                    "label": 0
                },
                {
                    "sent": "But still overall we can see that the amount of misclassification is low, and then I suppose we can say here at least when the other algorithm doesn't work well.",
                    "label": 0
                },
                {
                    "sent": "This one appears to work better.",
                    "label": 0
                },
                {
                    "sent": "But there is some intuition for why this is happening.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the other algorithm, the landmark clustering, will find an accurate clustering if the product of the approximation stability holds for the K medium objective function.",
                    "label": 1
                },
                {
                    "sent": "And when we look at the sequence data, when we actually look at the within cluster distances and the between cluster distances for the ground truth.",
                    "label": 0
                },
                {
                    "sent": "This actually looks a lot like the K meeting structure.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so one challenge we have is actually finding data that has the nontrivial min some structure.",
                    "label": 1
                },
                {
                    "sent": "So what I mean is that we must have clusters of varying size, but the diameters must be inversely proportional to the size of the clusters.",
                    "label": 1
                },
                {
                    "sent": "So for this data we don't quite see that, so that's one indication why the the K median algorithm actually may outperform this one here.",
                    "label": 0
                },
                {
                    "sent": "So this is one thing that we'd like to do in the future.",
                    "label": 0
                },
                {
                    "sent": "So I also would like to consider smaller landmark selection.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm presented here, we just choose a bunch of landmarks at random and their analysis just ask how many do we need to get one in each cluster core with high probability.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that if some of the cluster cores are small, we have to use a lot of landmarks to get one in there.",
                    "label": 0
                },
                {
                    "sent": "But there's a possibility possibility that a smarter adaptive strategy will work.",
                    "label": 0
                },
                {
                    "sent": "So for example, with their other algorithm, we actually select one of the furthest landmarks from the ones chosen so far, and this works better than random in getting 1 landmark in each cluster core.",
                    "label": 0
                },
                {
                    "sent": "So that doesn't quite work here, but maybe a similar approach will work.",
                    "label": 1
                },
                {
                    "sent": "So also Lastly, it's interesting to consider the limited information setting given other properties about the structure of the ground truth.",
                    "label": 0
                },
                {
                    "sent": "So for example, we can do a thought experiment where we just make a set of assumptions about what the ground truth data looks like, and then maybe we can probably find this structure without needing most of the distance information for the points.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is actually everything I went a little fast.",
                    "label": 0
                },
                {
                    "sent": "So how do you test up for me in practice?",
                    "label": 0
                },
                {
                    "sent": "It should get some property of the cluster, right?",
                    "label": 0
                },
                {
                    "sent": "So let's go back to that.",
                    "label": 0
                },
                {
                    "sent": "Right, so one thing we can do is it's hard to test the property explicitly, but one thing we can do is tested data to actually compute the within cluster in between cluster similarities in the distances for the ground truth and then see whether that looks like what must follow from our assumptions.",
                    "label": 0
                },
                {
                    "sent": "So this is how we would do it.",
                    "label": 0
                },
                {
                    "sent": "So I would actually get the ground truth.",
                    "label": 0
                },
                {
                    "sent": "Let's say the P fan classification and I would actually compute these values and for this data actually I do see well separated cluster cores.",
                    "label": 0
                },
                {
                    "sent": "But the diameter is done really follow the property that we assume in the theoretic part of our work, it looks more like the diameters are fixed, so yeah, they do not very sore inversely proportional to the size unfortunately, so that's one challenge is to actually find data that looks like this.",
                    "label": 0
                },
                {
                    "sent": "Now looking for you don't have the ground truth.",
                    "label": 0
                },
                {
                    "sent": "So then it's a bit hard.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so we have to.",
                    "label": 0
                },
                {
                    "sent": "We have to know something so yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe my question goes the same direction you.",
                    "label": 0
                },
                {
                    "sent": "You'll be quietly structural stability, so so if you have only explored your data set to a small fraction, is is the partial structural stability already sufficient for for the whole program into work?",
                    "label": 0
                },
                {
                    "sent": "Or is it basically blind driving for petrol recognition, so you assume something and then in the end you might find out after you've done a lot of work, but it doesn't hold it.",
                    "label": 0
                },
                {
                    "sent": "Could you?",
                    "label": 0
                },
                {
                    "sent": "Interpretation is not lists.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "I mean, one answer is a lot of data in real life is, well separable.",
                    "label": 0
                },
                {
                    "sent": "So so in the way it has to have some structure, because if it doesn't then it's very hard to find it.",
                    "label": 0
                },
                {
                    "sent": "So then two when we do some of these experiments where we make these datasets at random that come from a manual classification and we take a look at what it looks like, it is well separated.",
                    "label": 0
                },
                {
                    "sent": "Especially if we only consider a fraction of the points.",
                    "label": 0
                },
                {
                    "sent": "So in reality really, the property that does hold is that the approximation stability is true after you remove some points, so I think that's an assumption that's much closer to reality.",
                    "label": 0
                },
                {
                    "sent": "Approval of all the entries in this distance movie matrix.",
                    "label": 0
                },
                {
                    "sent": "Couldn't it be that I I hallucinate that they are well separated?",
                    "label": 0
                },
                {
                    "sent": "And if I would have very whole distance information, I would actually realize it?",
                    "label": 0
                },
                {
                    "sent": "It's no longer sunset, not separated in some sense.",
                    "label": 0
                },
                {
                    "sent": "So what I would say is it's hard to.",
                    "label": 0
                },
                {
                    "sent": "I mean, I strongly believe that actually if you remove some points, if you actually knew which ones to remove and you looked at all the distances between the remaining ones, you would see some very strong structure.",
                    "label": 0
                },
                {
                    "sent": "Sort of how to do that experiment the right way to remove some points first.",
                    "label": 0
                },
                {
                    "sent": "You know that's hard, so but when we do sample, some of the distances we see something that looks like a strong structure.",
                    "label": 0
                },
                {
                    "sent": "So then when we experimentally run the algorithm then we do find low myslef misclassification.",
                    "label": 0
                },
                {
                    "sent": "So I think there is some him that it's there.",
                    "label": 0
                },
                {
                    "sent": "But yeah, I agree that you know we don't know for sure.",
                    "label": 0
                },
                {
                    "sent": "Because with respect to your landmark last ring, it's really scary when on Sunday.",
                    "label": 0
                },
                {
                    "sent": "This is an algorithm is doing so well and not another day less.",
                    "label": 0
                },
                {
                    "sent": "Everything is such a poor performance.",
                    "label": 0
                },
                {
                    "sent": "So so do you know what happened yesterday?",
                    "label": 0
                },
                {
                    "sent": "Prefer for some of the datasets like 7 and 9 when I actually looked at the ground truth, some of those clusters there was a pair of clusters that have very high between similarity.",
                    "label": 0
                },
                {
                    "sent": "So, so in a way, just looking at that data really, you would say there's nothing wrong from considering those two clusters.",
                    "label": 0
                },
                {
                    "sent": "A single cluster, which is what the algorithm did.",
                    "label": 0
                },
                {
                    "sent": "So so I would say when I look at them, sometimes they're not that well separated or perfectly separated.",
                    "label": 0
                },
                {
                    "sent": "Let's say we may have a case where two clusters have really high up between cluster similarity.",
                    "label": 0
                },
                {
                    "sent": "So in that case the algorithm will merge those, yes?",
                    "label": 0
                },
                {
                    "sent": "OK, I was just wondering how would you select a number of marks?",
                    "label": 0
                },
                {
                    "sent": "Yes so.",
                    "label": 0
                },
                {
                    "sent": "So here are all the algorithms are using the same number of landmarks festival, so in there in the algorithm in the landmark clustering for the K median case, we probably need only OK landmarks for the landmark clustering Minnesota we probably need a lock it.",
                    "label": 0
                },
                {
                    "sent": "So here in these experiments that K is actually 8, it's only 8 clusters, so I believe here I was using A30K landmarks for all available, so this would correspond to 241 versus all queries and the size of these datasets are between.",
                    "label": 0
                },
                {
                    "sent": "15 and 20,000 points.",
                    "label": 0
                },
                {
                    "sent": "So, so that's how I said I'm here.",
                    "label": 0
                },
                {
                    "sent": "So I suppose yes.",
                    "label": 0
                },
                {
                    "sent": "In practice you would just.",
                    "label": 0
                },
                {
                    "sent": "You need to know roughly what K is, and then I would say K. To to some number, some constant times the number of clusters that you expect in this data.",
                    "label": 0
                },
                {
                    "sent": "So usually if you use more landmarks, it doesn't hurt the algorithm, it will just take much longer to run.",
                    "label": 0
                },
                {
                    "sent": "Also good for now.",
                    "label": 0
                },
                {
                    "sent": "So this assumption about this inverse relationship in the diameter and the cluster size.",
                    "label": 0
                },
                {
                    "sent": "So for me it seems to be in common, incompatible with any generative model.",
                    "label": 0
                },
                {
                    "sent": "Because I mean if you add a generative process and you keep on selling data you expect of course the diameter increases, so it's not a bit alarming to have a model for which there is no generative process possible.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's a bit of a problem.",
                    "label": 0
                },
                {
                    "sent": "So one thing I've been thinking about is you know what type of data would look like that.",
                    "label": 0
                },
                {
                    "sent": "I mean, one possibility is that it could possibly work for outlier detection in the way.",
                    "label": 0
                },
                {
                    "sent": "Let me go back to the picture.",
                    "label": 0
                },
                {
                    "sent": "Right, so you get if you look at this picture here.",
                    "label": 0
                },
                {
                    "sent": "Perhaps the three could be a set of outlier points.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in grouping those together, I suppose.",
                    "label": 0
                },
                {
                    "sent": "So one property about them in some clustering in general is that they are a Goodman.",
                    "label": 0
                },
                {
                    "sent": "Some clustering does not have to be a Voronoi decomposition of the space.",
                    "label": 0
                },
                {
                    "sent": "So in some cases, you actually.",
                    "label": 0
                },
                {
                    "sent": "Presumably you are interested in finding a clustering that looks like that.",
                    "label": 0
                },
                {
                    "sent": "But so far we haven't found too much real life data that looks like this.",
                    "label": 0
                },
                {
                    "sent": "I do have to say, yeah.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so last night, speaking again.",
                    "label": 0
                }
            ]
        }
    }
}