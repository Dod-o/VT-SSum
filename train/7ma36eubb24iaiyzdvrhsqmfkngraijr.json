{
    "id": "7ma36eubb24iaiyzdvrhsqmfkngraijr",
    "title": "Distributed Dual Averaging In Networks",
    "info": {
        "author": [
            "John Duchi, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Network Analysis"
        ]
    },
    "url": "http://videolectures.net/nips2010_duchi_dda/",
    "segmentation": [
        [
            "Thanks so I'm John.",
            "So I want to just sort of to set the stage for the problem we're considering, so I imagine we have a graph around network and on the network each node has a different function and what we want to do is minimize the sum of the functions across the network.",
            "So there are lots of problems that probably fall into this setting.",
            "One example is on the slide where we have a huge data set and the data is so large that we can't fit it on any single computer, so we.",
            "Put different, we sort of shared the data set on to several different processors, and then the goal is you know, so each function on processor I is the loss evaluated on the data set assigned to that processor, and then the goal is to minimize the aggregate loss across the network.",
            "Another example might be in sensor networks where we have a bunch of distributed sensors.",
            "They're all collecting data.",
            "We want to minimize a loss on them, but so we want algorithms, though that are sort of.",
            "A fish and tan only require like local communication, so we only want to communicate on the edges in the network and not have any sort of."
        ],
        [
            "Realized control.",
            "And so to do that, we propose what we call a distributed dual averaging algorithm which builds on the dual averaging approach of Nesterov and so each node maintains two vectors.",
            "The vectors are primal vector X which is sort of the nodes estimate of the solution and this dual vector Z and the algorithm proceeds in two phases.",
            "The first phase is the dual update which you see on the top where we have kind of local communication between nodes that are connected by edges in the network.",
            "So these Z vectors are passed from one node.",
            "To its neighbors, then node receiving the vectors averages them and performs a small gradient update.",
            "Then in the second step we have an update which is totally local to each node.",
            "Or each processor in the network which is gives us the primal parameter at the next time step."
        ],
        [
            "And so the main point of our paper is to study the effects of the network on the convergence rates of procedure like this.",
            "So what we see is that on any given node I the optimization error decreases at a rate of one over the square root of number of iterations times penalty, which is sort of dependent on the structure or the connectivity properties of the graph and our analysis.",
            "We tie this to a sort of the convergence of a random walk on the graph there sort of intimately related.",
            "And with this we can get convergence for several different graph structures.",
            "You can see some examples on the bottom of this slide like cycles grids, random geometric graphs and bounded degree expanders.",
            "We are."
        ],
        [
            "We have several extensions to this sort of main result.",
            "One of the more interesting ones is probably the robustness of the algorithm to edge failures, so we can also analyze the algorithm when edges periodically fail or nodes fail.",
            "We can also give an analysis and study stochastic optimization settings where we have streaming data coming into each different node in the network.",
            "We also have several simulations and a lower bounds that suggests that you know our analysis is sharp and we have kind of the correct network scaling.",
            "Throughout our paper, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks so I'm John.",
                    "label": 0
                },
                {
                    "sent": "So I want to just sort of to set the stage for the problem we're considering, so I imagine we have a graph around network and on the network each node has a different function and what we want to do is minimize the sum of the functions across the network.",
                    "label": 0
                },
                {
                    "sent": "So there are lots of problems that probably fall into this setting.",
                    "label": 0
                },
                {
                    "sent": "One example is on the slide where we have a huge data set and the data is so large that we can't fit it on any single computer, so we.",
                    "label": 0
                },
                {
                    "sent": "Put different, we sort of shared the data set on to several different processors, and then the goal is you know, so each function on processor I is the loss evaluated on the data set assigned to that processor, and then the goal is to minimize the aggregate loss across the network.",
                    "label": 0
                },
                {
                    "sent": "Another example might be in sensor networks where we have a bunch of distributed sensors.",
                    "label": 0
                },
                {
                    "sent": "They're all collecting data.",
                    "label": 0
                },
                {
                    "sent": "We want to minimize a loss on them, but so we want algorithms, though that are sort of.",
                    "label": 0
                },
                {
                    "sent": "A fish and tan only require like local communication, so we only want to communicate on the edges in the network and not have any sort of.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Realized control.",
                    "label": 0
                },
                {
                    "sent": "And so to do that, we propose what we call a distributed dual averaging algorithm which builds on the dual averaging approach of Nesterov and so each node maintains two vectors.",
                    "label": 1
                },
                {
                    "sent": "The vectors are primal vector X which is sort of the nodes estimate of the solution and this dual vector Z and the algorithm proceeds in two phases.",
                    "label": 0
                },
                {
                    "sent": "The first phase is the dual update which you see on the top where we have kind of local communication between nodes that are connected by edges in the network.",
                    "label": 0
                },
                {
                    "sent": "So these Z vectors are passed from one node.",
                    "label": 0
                },
                {
                    "sent": "To its neighbors, then node receiving the vectors averages them and performs a small gradient update.",
                    "label": 0
                },
                {
                    "sent": "Then in the second step we have an update which is totally local to each node.",
                    "label": 0
                },
                {
                    "sent": "Or each processor in the network which is gives us the primal parameter at the next time step.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so the main point of our paper is to study the effects of the network on the convergence rates of procedure like this.",
                    "label": 0
                },
                {
                    "sent": "So what we see is that on any given node I the optimization error decreases at a rate of one over the square root of number of iterations times penalty, which is sort of dependent on the structure or the connectivity properties of the graph and our analysis.",
                    "label": 1
                },
                {
                    "sent": "We tie this to a sort of the convergence of a random walk on the graph there sort of intimately related.",
                    "label": 1
                },
                {
                    "sent": "And with this we can get convergence for several different graph structures.",
                    "label": 0
                },
                {
                    "sent": "You can see some examples on the bottom of this slide like cycles grids, random geometric graphs and bounded degree expanders.",
                    "label": 0
                },
                {
                    "sent": "We are.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have several extensions to this sort of main result.",
                    "label": 0
                },
                {
                    "sent": "One of the more interesting ones is probably the robustness of the algorithm to edge failures, so we can also analyze the algorithm when edges periodically fail or nodes fail.",
                    "label": 1
                },
                {
                    "sent": "We can also give an analysis and study stochastic optimization settings where we have streaming data coming into each different node in the network.",
                    "label": 0
                },
                {
                    "sent": "We also have several simulations and a lower bounds that suggests that you know our analysis is sharp and we have kind of the correct network scaling.",
                    "label": 1
                },
                {
                    "sent": "Throughout our paper, thanks.",
                    "label": 0
                }
            ]
        }
    }
}