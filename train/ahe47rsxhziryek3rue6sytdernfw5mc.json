{
    "id": "ahe47rsxhziryek3rue6sytdernfw5mc",
    "title": "Syntactic Approaches for Natural Language Processing",
    "info": {
        "author": [
            "Joan Andreu Sanchez, Technical University of Valencia (UPV)"
        ],
        "published": "March 31, 2011",
        "recorded": "February 2011",
        "category": [
            "Top->Computer Science->Natural Language Processing",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/aibootcamp2011_sanchez_mnlp/",
    "segmentation": [
        [
            "OK, thank you for being here too here in this talk.",
            "OK, so.",
            "Why are a talk about natural language processing in?",
            "In a machine learning course so.",
            "Natural language Processing is a research field that.",
            "Has taken a lot of ideas for several fields.",
            "Like machine learning.",
            "Like like pattern recognition like statistics like.",
            "Formal languages.",
            "So."
        ],
        [
            "Most of the ideas that I will describe in this talk.",
            "Has been taken mostly from or can be considered from a part of the machine learning theory.",
            "OK, so.",
            "This is.",
            "Yeah, this is the.",
            "The index of that of the talk.",
            "I first of all I will introduce.",
            "The problem of natural language processing.",
            "The problem applying that usually is also called that is also known as computational linguistics.",
            "And I will try to focus the talk in a specific in specific tools that are usually used.",
            "In this field.",
            "And I will describe concretely.",
            "Tools like.",
            "Hamm hidden Markov models.",
            "That is a useful tool in natural language processing.",
            "And probabilistic context free grammar that is also.",
            "A tool that is.",
            "Usually used in natural language processing in computational linguistics.",
            "This kind of models.",
            "Have been used for several specific tasks and I will describe two of them.",
            "Hidden Markov models applied to post tagging and probabilistic context free grammar for parsing and language modeling.",
            "OK.",
            "I will introduce some some notation about hidden Markov models and then we will see.",
            "The fundamental algorithms that are usually used.",
            "For.",
            "Hidden Markov models.",
            "Then we will see how to learn.",
            "The parameters of the model, not the structural part, just the probabilistic part of the model.",
            "And later we will see the use of.",
            "We will introduce the concept of probabilistic context, free grammar.",
            "Fundamental algorithms for this kind of models.",
            "And how the probabilities of this kind of models can be learned from data?",
            "And we will see how to use this sort of models to parsing and language modeling.",
            "OK.",
            "In the in the final part of the talk I will introduce some interesting topics about this kind of models like online learning, for syntactic models, altica learning, and.",
            "I will describe in this last point.",
            "A possible framework in order to.",
            "To join things like online learning and active learning, this is currently our my.",
            "Russia my English stop.",
            "I will see if we have time I will show a demo about.",
            "This this framework OK?"
        ],
        [
            "Go.",
            "Please think into account that most of the techniques that I will describe specifically for this kind of models can be used also for other models in mature lending.",
            "I will introduce the EM algorithm.",
            "I will describe the basic idea of the EM algorithm, but from another point point of view.",
            "A point of view that is known as growth transformations.",
            "And.",
            "This kind of techniques can be also applied.",
            "Two other to other fields.",
            "OK the Emily theme.",
            "Online learning active learning.",
            "OK, so tell me.",
            "Sorry.",
            "Stop this.",
            "Yeah yeah yeah in the in the lab sessions we will see.",
            "We will try to develop a practice about HHMMM.",
            "Hidden Markov models for POS tagging.",
            "OK that is for lap.",
            "And then I have I have to prepare it.",
            "Another exercise for the estimation of probabilistic context free grammar.",
            "Two to two different exercises.",
            "OK."
        ],
        [
            "OK, so let's continue.",
            "Well, the problem of natural language processing or or the problem of computational linguistics that is.",
            "Usually.",
            "That is another way of referring to the same concept.",
            "The minor problem deals with the problem of computational linguistics is dealing with one of the most difficult problems that is natural language.",
            "Natural language is a very difficult problem because there the apart from other problems like accent like.",
            "The specific problems related to this zoning, in which people live and things like that.",
            "There are a lot of ambiguity in language, so.",
            "This is a very difficult problem.",
            "And in computational linguistics, the mind approach that is used to tackle these problems.",
            "Currently relies on.",
            "Probably on probability.",
            "So the idea is to to solve the problem of of the ambiguity by.",
            "Using in this case probabilities in order to take decisions.",
            "OK."
        ],
        [
            "So the main goal in computational linguistics is to develop systems that are able to.",
            "To tackle the problem of natural language.",
            "Show they motivate the motivation of this or the need of computational linguistics is clear, but another reason another motivation could be this.",
            "That natural language is.",
            "A natural way of communicating of transferring knowledge to the people.",
            "So.",
            "Another reason for this field.",
            "For research in this field is that there there is a lot of information in natural language.",
            "And they and there are a lot of potential users.",
            "For using this kind, the kind of of tools that can be developed in computational linguistics."
        ],
        [
            "OK, so.",
            "For Focusin, for focusing and speaking about for speaking about most specific application.",
            "Some this is a sum of application that we can deal with.",
            "With the tools that are studied in computational linguistics.",
            "So.",
            "Systems for information extraction from text and speech like.",
            "For information retrieval, information extraction, text categorisation.",
            "In computational research, the tools and techniques that are exciting computational linguistics can be also used for developing systems that are able to.",
            "Translate speech to speech, speech to text, text to speech.",
            "Concretely, like machine translation, speech, translation, speech recognition.",
            "And our application of computational linguistics can be this systems for communication with humans like dialogue systems that are even more complex than other systems like a speech recognition system."
        ],
        [
            "So as far as I have commented previously, the idea is to use probabilistic techniques in order to solve the problem of ambiguity that usually arise in this kind.",
            "Of problems.",
            "So if we have to use.",
            "Probabilities in order to tackle these problems we have to deal at least with three issues.",
            "One of them is interpretation, that is to generate additional light.",
            "Interpretation from argument input sometimes in pattern recognition.",
            "This problem is not too much difficult because because the number of classes is not too large, but in computational linguistics.",
            "In the program, for example of post tagging in the problem of language modeling.",
            "The number of classes is infinite.",
            "And we have to develop adequate techniques for searching in that space.",
            "So this is a very important problem in computational linguistics.",
            "For example, in material in machine translation and in speech recognition.",
            "So another issue that that much we must take.",
            "To take into account is the problem of modeling.",
            "We need to define models.",
            "That are able to capture to represent the knowledge that we are trying.",
            "In that we have available.",
            "So most of the tools that are currently used in computational linguistics.",
            "Are based on.",
            "Statistical decision techniques and formal language theory.",
            "OK, and the idea is to define adequate models to represent the reality.",
            "And finally, given that our approach are based in inductive learning, learning from examples, we have to tackle with the problem of learning from from a given set of examples.",
            "True examples.",
            "We have three different problems and we have to tackle with.",
            "Everyone with everyone OK, OK."
        ],
        [
            "So the main goal of this talk?",
            "Is 2.",
            "Remind introduced just to introduce.",
            "Syntactic approaches to deal with difficult problems of natural language.",
            "Natural language processing OK. Another goal is to.",
            "To show some techniques, some tools of computer computational linguistics.",
            "Some vocabulary, some notions OK, and to learn some basic techniques that are necessary to develop and to work with this sort of.",
            "We with this with the model that we will see later."
        ],
        [
            "OK.",
            "So these are some of the applications.",
            "Of that need.",
            "Knowledge data in which computational linguistics has been working in the last years.",
            "Automatic speech recognition machine translation, dialogue systems, etc.",
            "And for this.",
            "Concrete applications.",
            "The idea is to use.",
            "Some abstract task like language modeling.",
            "Part of speech tagging, parsing an other after tax, and in this.",
            "In this talk we will just describe.",
            "3DS3DS3 problems we will just introduce these three problems, but there are a lot of problems named entity recognition.",
            "Close detection etc.",
            "Lexical disambiguation, semantic, analyze it, etc.",
            "OK.",
            "So you know there are two 2 to tackle with these problems.",
            "We must take into account that the natural language can be.",
            "Then the knowledge that are present in natural language can be divided in several levels according to the."
        ],
        [
            "Flexity, so these are the.",
            "A possible division of these levels, depending on the complexity of the knowledge that are in each level.",
            "So we could introduce a morphology level, syntax level, semantic level, pragmatic level, discourse level.",
            "So the tools that we will see in this in this talk will focus in this level and we will.",
            "We will be talking about.",
            "Part of speech tagging that is related to work category and sentence structure that is really an passing that disability would center the structure and language modeling.",
            "OK."
        ],
        [
            "So the the main problem in post tagging.",
            "Is the following.",
            "The idea is in natural language processing in computational linguistics, sometimes we have models that have a lot of parameters, so we need to.",
            "To group.",
            "For example, the words in classes in order to to decrease the number of parameters or to help the system by grouping words in a set of.",
            "In a set of of in a given set of.",
            "In this case, part of speech.",
            "Part OK.",
            "So the idea is to classify a given set to classify the words in a given set of part of speech tags.",
            "That in this case can be seen like this, which as you can note, this can be seen as a classification problem.",
            "So.",
            "There have been several approaches to tackle with this problem like.",
            "Hidden Markov models.",
            "Maximum entropy approaches or support vector machines for tackle with this problem.",
            "In this talk I will introduce.",
            "Some ideas about how to use hidden Markov models for part of speech tagging.",
            "Please note that one of the problems is to define a set of tasks.",
            "But we we leave this problem for another talk OK?",
            "And for example, this means that these are known a proper noun.",
            "This is a proper noun.",
            "This is a Cardinal number.",
            "OK, this is a.",
            "A singular noun.",
            "So this is for example, this tax has have been used.",
            "In a corpus that is very known, well known that is the Penn treebank corpus.",
            "OK, so Please note that.",
            "Sometimes we need we need to annotate data in order to have available this data and this is available complicated problem annotating data.",
            "OK, so.",
            "And this is related with the last part of the talk how to annotate?",
            "Data.",
            "In in, in the most cheapest way.",
            "OK."
        ],
        [
            "So the problem.",
            "Yes.",
            "I don't understand your question, sorry.",
            "Is complexity.",
            "There are so many definitions.",
            "But she's contacted.",
            "No no no no no.",
            "The sentence can be as long as you want and the problem is exactly the same.",
            "It's a problem of ambiguity in this case, mostly of ambiguity.",
            "The Post again is mostly a probability.",
            "OK, so how come we deal with this issue by using hidden Markov model?",
            "The idea is to define an automata like this in which.",
            "Are you?",
            "Which day is to go from an initial state to the end of the final state?",
            "Following a path through this through this.",
            "Automata and.",
            "Every time you see a word.",
            "You start in a given in a given state, and every time that you see another another word, you go to that state.",
            "Depending on the the probabilities.",
            "And you following this way until you arrive to this to the final state, and then the idea is 2, you must be able to recover the path that you have used and taking into account the state that you have used, you can attach a label label to every word that is the main idea.",
            "What is an automata?",
            "Hidden Markov models in this case.",
            "I will, I will explain later.",
            "I will explain later what is a hierarchal models.",
            "I will explain later.",
            "OK, this is that it will introduce the problem we will see later.",
            "What is a hidden Markov model?",
            "OK."
        ],
        [
            "So the other problem is related to probabilistic context free grammar.",
            "And the problem of parsing and the idea is similar.",
            "The idea is.",
            "Given a sentence to define relation between different parts of the sentence.",
            "And the problem here is the same.",
            "We have ambiguity problems, so we must.",
            "Tackle with this kind of ambiguity.",
            "And.",
            "The tools that we will use here will will be.",
            "Formal grammars, context free grammars.",
            "In this case, I will focus the talk in context free grammars.",
            "Quantity is just a formal tool that can be used to describe the relations between different parts of a sentence.",
            "His idea is we have an an initial symbol and actually this is the action of the grammar and all sentence are composed from this symbol and applying this rule and expanding its time are given given symbol like this that is usually called nonterminal symbol.",
            "You are applying recursively.",
            "This kind of expanding this kind nonterminal symbols until you arrive to a rule.",
            "That in the right part has award.",
            "So applying these rules we can compose.",
            "Apart, apart in three like this, a person through ladies.",
            "OK."
        ],
        [
            "So in order to tackle with ambiguity, we will use.",
            "In our case probability in order to decide the most probable.",
            "Syntactic structure structure that is associated to a given sentence.",
            "OK, we will see later in more detail things like this.",
            "This."
        ],
        [
            "Model OK so.",
            "Yeah, the idea is a. I will describe one application of probability cause our priority content, for parsing and also for language modeling.",
            "Although this is not the usual.",
            "Approach for language modeling cause the most usual approach for language modeling.",
            "It is ngram models.",
            "OK, so but it's a possible.",
            "It's a possibility.",
            "OK, to use probabilistic context free grammar for language model and I explored this kind of model for this problem."
        ],
        [
            "The idea is the following in apart from a pattern recognition point of view, the idea of developing our recognition can be seen in this way you have.",
            "On a signal that arrives to our recognizer and we must to provide a possible interpretation, the most usually the most possible interpretation.",
            "And the problem can be stated in this the other way.",
            "So the idea is.",
            "You if you see you we have reversed most in some kind in some way.",
            "The problem because the idea is to generate a possible output and then to measure how this output accounts for the input.",
            "So this is the the the main approach.",
            "That is usually used for speech recognition.",
            "And also in some.",
            "Applications like much in translation.",
            "This is called the language model.",
            "OK, and this is usually called the.",
            "Channel model choose the channel probability and this is the language model probability.",
            "The idea is to generate, for example, for speech recognition sentences, possible sentences and then to measure how each sentence accounts.",
            "For.",
            "For, for a given input.",
            "Logically, the this is a simplistic.",
            "This is a simplistic definition obviously cause as you must take into account the number of possible outputs can be infinite OK and here is where the problem of shared search appears.",
            "Search in a large space."
        ],
        [
            "OK, so this is the approach that is usually used for language for speech recognition.",
            "Here we have a signal.",
            "Are the color that usually.",
            "Is fed up with models hidden Markov models, usually hidden Markov models.",
            "And England models for for modeling the language there.",
            "The language model.",
            "And finally we provide.",
            "As the most probably words that are speaker.",
            "Has added OK so.",
            "In the speech recognition, the kind of hidden Markov models that are usually used.",
            "Are continuous hidden Markov models.",
            "But in our case, I will focus the talk just in discrete hidden Markov models, and this is I have prepared the talking this way because I think that the discrete version is easy to understand, most easy to explain OK.",
            "So the language model is.",
            "This part of the discussion and.",
            "It tries to model the open language.",
            "So the this is.",
            "This is a priority, can be discomposed discomposed in this way, but usually.",
            "Yeah, you see here the probability of a given word is conditioned in the in all the previous words, But the most common approaches to simplify this expression and just take into account not all the previous work but just a few few of them, one or two previous words.",
            "OK, but even in our case is given that we will use probabilistic interferon.",
            "We'll see how this can be computed.",
            "OK."
        ],
        [
            "So this is the usual approach for language modeling.",
            "The usual approach is to simplify the previous session this expression.",
            "And the idea is to compute the probability of award.",
            "Condition it.",
            "Not in all the previous words, but just in one 2.",
            "Or toward almost, and this is usually known as 3 gram triggering road 3.",
            "Three gram model for Gram model is not very useful, and the largest improvement are usually obtained when we used.",
            "When we go from bigram models to trigger modes, bigger models in a bigger model we have into account just one word, one previous word.",
            "OK, so this is the usual approach in.",
            "Which is the usual approach in.",
            "Immigrants, OK, the problem with bigrams models with trigger models with in general with Ingram models is that.",
            "They are not able to capture as equally long term dependencies within words.",
            "But the positive thing of this kind of models?",
            "The positive one of the positive.",
            "The positive things is that.",
            "Yeah, the.",
            "The probability can be officially computed in an online system.",
            "And there exist efficient methods to estimate the parameters of the model.",
            "OK, when you sing grammatical models as we will see in this in this talk.",
            "We could we can.",
            "We can compute the probability of a word not taking into account just a few words of previous three words that the complete sentence.",
            "The problem, one of the advantages of this approach is that is able to capture long-term dependencies.",
            "But is expensive to compute in an online system and.",
            "The efficient method that exist that exist for estimating the parameters of the model are very expensive.",
            "Very expensive.",
            "And.",
            "This is a very interesting research topic.",
            "Integrating generating syntactic models in language modeling.",
            "There has been some approaches, but I think that is also that I think that it continues being research.",
            "Problem.",
            "OK, because currently.",
            "For obtaining good results with this kind of models, we must interpolate.",
            "We must use both kind of models and interpolate.",
            "The interpolate model.",
            "So the idea of integrating a syntactic model in in system for speech recognition, for example, is a very challenging problem."
        ],
        [
            "OK.",
            "Questions.",
            "Some questions.",
            "I will go in depth.",
            "Now with the hidden Markov models.",
            "Where's the answer question?",
            "OK so.",
            "Let's continue."
        ],
        [
            "You so I will follow this.",
            "This reference in this slide.",
            "This is not the most well known reference for this kind of models, but I have used.",
            "The notation that is used in this in this.",
            "In this light, the most well the most popular reference is a reference from ball.",
            "I don't remember the name.",
            "Jellinek I think Lafferty.",
            "And.",
            "This kind of models were popular popular inside when they were used for speech recognition in the A in the first 80s.",
            "OK.",
            "So this kind of models are very interesting models becausw.",
            "A simple models for representing regular.",
            "Relation between words.",
            "Is a formal proof that is well understood, well known?",
            "Is although natural language is not completely regular, it can be considered almost regular.",
            "Which is equipped to represent short-term syntactic relation structures.",
            "And given the powerful of this kind of models that combines concepts from language theory on from statistics.",
            "Alec wait for modeling the big city.",
            "OK, and I is the core.",
            "Models for a large variety of systems, mainly speech recognition systems.",
            "There are free shower for tackling with this kind of models.",
            "For example, the most well known, So what is?",
            "H. Tiki that you can download and use for developing speech recognition systems.",
            "OK."
        ],
        [
            "OK, so our problem is to tackle with with some kind of objects.",
            "So our first in this.",
            "In this light I will define the kind of object that we that we will use.",
            "For for the plan that we interested in so.",
            "The first concept is the concept of primitives.",
            "In our case we will work with an alphabet that is.",
            "A set of symbols.",
            "A set of symbols, for example, words with these punctuation symbols.",
            "Date, Cardinal numbers, etc.",
            "This is our primitives.",
            "Our object will be composed by this kind of primitives.",
            "And.",
            "We will join this kind of primitives.",
            "The symbols of the alphabet which will join in order to compose.",
            "A more complex object that in our case will be a sentence.",
            "I want by joining symbols we we can form, in our case sentences.",
            "For example, a sentence like this.",
            "This is the first.",
            "Sentence I will be using this sentence because cause this is the.",
            "The sentence, the first sentence of a well known.",
            "Corpus that is the tree bank corpus.",
            "OK, so our projects.",
            "Will be sentences that will be grouped in order to form sets in the.",
            "In this case, sets of sentences and then this.",
            "This is.",
            "This will be our pattern set.",
            "OK, and the idea is to use and.",
            "Our problem is to obtain a possible interpretation for this object.",
            "That innovation will be post tax Association, that is, to associate a POS tag to every word.",
            "OK, so these are our objects.",
            "And.",
            "For this kind of objects, we will."
        ],
        [
            "Cheap.",
            "What we will I will introduce more formally this this this object.",
            "Through this concept, the concept of alphabet.",
            "OK, let's have.",
            "It is just a finite set of symbols.",
            "With these symbols, we will compose sentences strings in our case.",
            "So.",
            "A string is just the concatenation of a given number of symbols, and we can compose sentences larger sentences by joining.",
            "Yeah, shorter sentences.",
            "So.",
            "A special symbol is the empty string.",
            "That is, this is a string with length equal to 0.",
            "Yeah, this is this is it is important to remark this string becausw in some application.",
            "The use of this special string introduces complexity problems.",
            "So, but for some form or for most of the application that we that we will use that we will see in this talk, we can avoid this special string.",
            "So we defined the closure of a set of the closure of an alphabet, like in this way, that is, the number the infinite on controller set of strings.",
            "Of we finish the length and the positive closure is defined like the closure by avoiding the empty string.",
            "So the this is the formal definition of the object of the object that we will use in this for this kind of model.",
            "So a language can be defined it in this way just in this way OK?",
            "So this is our language.",
            "This is our objects.",
            "The object that we will use that in that we will use for describing our problems.",
            "So now we need.",
            "Some model that account for this kind of for this kind of objects, so in our case we will use.",
            "Hidden Markov models and completely discreet hidden Markov models.",
            "OK."
        ],
        [
            "So what gives him a commercial can be defined it in this way.",
            "This is a couple ladies where this is the number of states.",
            "Sorry this is the set of a state.",
            "This is a graphical representation representation or hidden Markov model.",
            "This is the set of stage, the alphabet and two and the three different probability distributions representing the probability of gooie.",
            "Passing from this from this state to this state.",
            "This is the transition probability.",
            "For what we from one state to another state.",
            "This is the emission probability, that is, the probability of serving a given symbol in a state.",
            "So this is the probability of being a state of state being initial state.",
            "That means that we can start to.",
            "To accept to recognize a given string in a in a in a state so.",
            "This is a graphical representation.",
            "This this restriction must be taken into account.",
            "So this probabilities must add add up to one.",
            "These probabilities must.",
            "But up to one and the same thing for each state and the same thing for these probabilities.",
            "This is a special kind of models that is very usually used in.",
            "In speech recognition, this is a model that usually called left to right.",
            "Left to right model becausw every time that you.",
            "At that you arrive to one state.",
            "You can not return to any state with a lower index.",
            "So it's a way of representing temporal information.",
            "OK.",
            "If you.",
            "If you see in another book you will you can find.",
            "Maybe different definitions, but all of them are very similar.",
            "OK.",
            "So this will be the kind of models that will be used for accounting.",
            "For this sort of object that we are interested in.",
            "So.",
            "How am I hidden Markov model?",
            "Works, so the idea is very simple.",
            "Maybe some of you are familiar familiar with this kind of models, but maybe the other ones are not familiar with this kind of model, so the idea is the following.",
            "Suppose that we have a string like this.",
            "So the idea is to go through a path in this.",
            "Model in order to account for this.",
            "For this for this string.",
            "So we can start in this state.",
            "To observe this symbol.",
            "Translate to another to go to another.",
            "State to observe another symbol.",
            "To observe the symbol.",
            "And.",
            "Finally, to arrive to the final state, something like this.",
            "OK, so immediately you can see that.",
            "We can define.",
            "Another path.",
            "Ladies.",
            "So this is a different path because we have changed this state here OK?",
            "So this is the way in which.",
            "Model account for a string.",
            "So the idea is.",
            "This is a possible path and the probability of this path for observing this string is computed just by multiplying the probabilities that we are using in this path for example.",
            "We have a start in this in this state, then we have arrived to the second state with probability.",
            "Zillow dot .1.",
            "We here we have shared this symbol with probability 0.9.",
            "And by.",
            "Multiplying all probabilities that are used.",
            "In in every path we are able to come to compute the probability of sharing this string through this path, OK?",
            "Questions is that clear?",
            "Yeah.",
            "This is this is this is this you are using this property distribution for transfer transiting from one state to another state.",
            "Q1.",
            "You don't understand.",
            "I don't understand this honey.",
            "Weather output.",
            "On the picture.",
            "No, no, this is the probability of this state being initial state.",
            "You must start always in this state.",
            "Although the definition is quite general, but this is a restricted model.",
            "This is the probability of starting in this.",
            "In this you cannot start in this in this state because the probability of key one is.",
            "Is 1 so there is no, you know have any chance to start in this state or in this state?",
            "On the way.",
            "One just from there.",
            "You have to read it correctly.",
            "I don't understand the.",
            "Where do you read characters?",
            "We are not reading character.",
            "We are OK. OK, OK, well my question so maybe.",
            "The problem is how Markov model accounts for for for a string.",
            "So this is the way of relating States and path with with with with a string.",
            "But we will see later how how OK. Have you have been explaining the hidden Markov model, like a generator or like an acceptor?",
            "OK so but and I think the problem is is.",
            "Being seen, the hidden Markov model like acceptor of like an generator, I think that it doesn't matter in this moment.",
            "If it is that the question.",
            "So we will see later how, given how, given a given a string, you can account for that string.",
            "I have explaining the hidden Markov model like a generator, but is a way of seeing how to relate a state with symbols OK.",
            "So the idea.",
            "So the idea of of hidden Markov models.",
            "The name of hidden Markov models becausw.",
            "In in usually you always see this.",
            "This is the part that you have seen in hidden Markov model.",
            "Does this part is usually unknown.",
            "This is usually unknown, so if you have for example, you can see that the number of possible path is infinite can be infinite.",
            "So our main problem now will be how to compute these efficiently.",
            "OK, but please take into account that the way of presenting the example is I have presented example like a generator, but it's a way of explaining how to relate symbols with states OK.",
            "So the problem now will be how to compute this efficiently.",
            "OK. We're still more questions.",
            "So.",
            "OK, this is the kind of model with that we will use so I have how have said the problem is how to compute these efficiently because the number of possible transitions could be.",
            "If it is not infinite, sorry exponential exponents.",
            "OK.",
            "So we have patterns we have.",
            "A model that accounts for the kind of model.",
            "Now we need to know to introduce some some.",
            "Some ideas in order how in order to define how to compute how to compute this efficiently?",
            "We need to describe how to compute the probability the probability of of a hidden Markov model accounting for a string.",
            "So what?"
        ],
        [
            "Now working so sorry.",
            "So this is the.",
            "This is an example of why have you been discussing here.",
            "This is the way in which.",
            "Hidden Markov model accounts for a string.",
            "Through a path.",
            "So we start with a new state.",
            "We are able to observe a symbol, then we translate translate to another state, observe another symbol, and at the end we arrived at the final state.",
            "So the probability of this string through this path can be computed in this way.",
            "This is the probability of a start of starting in this state or serving the symbol.",
            "Translate into this state of serving the symbol, and are the priorities that appear here.",
            "So.",
            "Given a set up path through this through this.",
            "Model that is able to accounting for this string.",
            "The probability of this.",
            "Sequence can be computed in this way.",
            "Is the probability of transiting from here to here from here to here, and this is the probability that appears here and the probability of observing the symbols can be expressed in this way.",
            "So finally given that the number of possible sequence could be exponential.",
            "Let this expression be the set of possible valid path.",
            "To account for this string so this probability can be can be defined in this way.",
            "OK. OK, so sorry.",
            "If you can see that this is a, you can see that this is a joint probability.",
            "This is a joint probability.",
            "The probability of transiting from one district to another state is independent.",
            "Yeah, yes, this is a kind of models.",
            "Hidden Markov models is just a subclass of other kinds of models, and depending on the on the depending on the conditioning you can define even more complex hidden Markov models.",
            "But in this case the probability of transiting from this from one state to another state.",
            "Only the pain depends on the state in which you are.",
            "Only this is a very hard restriction, but.",
            "Simplify the computation of the of four applications OK. What do I am saying that when you decide?",
            "When you are in a state and you go to another state.",
            "The analysis the state to which you arrive only depends in the state in which you are.",
            "OK. OK my question.",
            "Wait until until we will see later.",
            "So we have a patterns set of patterns.",
            "And we have this model that is able to account for this.",
            "For that is able to account for this.",
            "For these patterns, now we need we need to introduce some concepts in order to relate.",
            "These two concepts.",
            "So the idea is to use.",
            "An algorithm that is able to compute this.",
            "This probability in an efficient way.",
            "So now we'll see.",
            "Some of the fundamental algorithms that are usually used for.",
            "Computing for tackling with hidden Markov models.",
            "So the."
        ],
        [
            "The story theme is the.",
            "Where the forward algorithm.",
            "That there is a.",
            "Very important to notice him that is based in a dynamic programming scheme.",
            "So.",
            "So and this concept are our internal related with algorithmics.",
            "So the idea is how they days.",
            "How can we compute?"
        ],
        [
            "These efficiently you must take into account that this size of this set can be exponential.",
            "So.",
            "The probability of observing a given symbol only only depends on the state in which you are.",
            "I don't understand.",
            "Explain this.",
            "OK so the you must take into account that the number of possible path that accounts for a given a string can be exponential.",
            "You need to add up for all possible sequences.",
            "This is the set of all possible paths that counts for a given for a given string, you must shut up for all possible sequences.",
            "And that is the meaning of this of this expression.",
            "OK. OK."
        ],
        [
            "So.",
            "How can we compute this officially so we can use the forward algorithm that can be defined in this way?",
            "This is a tabular algorithm that fills in a table.",
            "Which is sometimes known as trelles.",
            "So the idea is to fill in a table by considering considering.",
            "Prefix.",
            "Of increasing size.",
            "So this is the initialization.",
            "So in the initialization we just.",
            "Compute this probability and then we.",
            "Use this loop did recursion, in which.",
            "We we we compute the probability of observing a given symbol, taking into account the computation for the previous prefix.",
            "So the idea is filling up at a table from left to right, considering at each time a symbol.",
            "So."
        ],
        [
            "Here we have an example.",
            "Idea is to complete a cell like this.",
            "We start here.",
            "And this is the model.",
            "And then we can.",
            "We can translate from from the first state to the first state by yourself in the symbol that we see.",
            "For example, this probability.",
            "Or is this probability, then transiting from Q1 to Q1 and then observing the symbol?",
            "This is the probability this probability transiting from Q 141 and then observing this symbol.",
            "Something similar for this.",
            "Unchallenging and in direct sample, for example, this.",
            "Expression is means that we have this is this probability transiting from Q1 to Q2 and observing the symbol in the.",
            "In this state, or we are in the in key two transiting to key 2.",
            "And then observing the symbol.",
            "In kitchen.",
            "OK. And we compute this.",
            "We complete this table.",
            "And can be."
        ],
        [
            "Can be computed with this algorithm that completes.",
            "That runs from left to right by considering.",
            "Prefix of increasing size.",
            "OK, so finally."
        ],
        [
            "We obtain the probability of this string in this cell.",
            "OK, the time complexity of this algorithm is linear.",
            "So you must take into."
        ],
        [
            "Know that.",
            "This.",
            "Especially that we are not able to compute in this way.",
            "Compute can be now computed."
        ],
        [
            "Not enough time.",
            "OK.",
            "So this is the forward algorithm that computes compute probabilities of increasing size.",
            "And."
        ],
        [
            "This is the definition.",
            "Is the probability of generating."
        ],
        [
            "A string and arrive into a given state."
        ],
        [
            "And we can use another origin.",
            "That is the bug.",
            "We're already theme that deviates very similar, but in this case we compute not the probability of prefix.",
            "Prefixes of increasing size, but the probability of suffixes of increasing size.",
            "Or fingers inside from right to left.",
            "So we can compute the probability of observing.",
            "Given suffix.",
            "From from from a given state.",
            "So the idea is very similar, OK?",
            "So.",
            "Sometimes.",
            "This is the forward terrorism.",
            "This is about what algorithm and sometimes we interested.",
            "In computing, not only the probability of a string.",
            "For a given hidden Markov model, but to compute the probability, the probability of the most probable path that is able to account for a given string.",
            "So in that case we can."
        ],
        [
            "Produce an algorithm that is there.",
            "It'll be algorithm that is known as the materialism.",
            "Suppose that we define the probability.",
            "Yeah.",
            "Here there here there is an error.",
            "Note here is the.",
            "Yeah.",
            "We need to hear an argument Arg Max.",
            "Puppy.",
            "So suppose that we are interested in computing the probability of the sequence that accounts for for the largest probability.",
            "That accounts for the for the string with the largest probability, so we need to introduce an algorithm in this case that is now like the material that is exactly the same, exactly the same as the forward algorithm.",
            "But we have 62 this addition.",
            "Buy a maximization, but it's exactly the same everything.",
            "The idea the interesting thing in the Viterbi algorithm?",
            "Is that given that we are maximizing, we can.",
            "We can leave back pointer to the cell with.",
            "We have used in the previous state and then when we are right to the final state we are able to recover in the path and obtaining the interpretation and that is the that is the fundamental idea for obtaining a possible interpretation forgiven, forgiven string.",
            "For example in speech recognition.",
            "This is the idea that is usually used in order to look for.",
            "A path with the largest probability."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, thank you for being here too here in this talk.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Why are a talk about natural language processing in?",
                    "label": 0
                },
                {
                    "sent": "In a machine learning course so.",
                    "label": 0
                },
                {
                    "sent": "Natural language Processing is a research field that.",
                    "label": 1
                },
                {
                    "sent": "Has taken a lot of ideas for several fields.",
                    "label": 0
                },
                {
                    "sent": "Like machine learning.",
                    "label": 0
                },
                {
                    "sent": "Like like pattern recognition like statistics like.",
                    "label": 0
                },
                {
                    "sent": "Formal languages.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most of the ideas that I will describe in this talk.",
                    "label": 0
                },
                {
                    "sent": "Has been taken mostly from or can be considered from a part of the machine learning theory.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is the.",
                    "label": 0
                },
                {
                    "sent": "The index of that of the talk.",
                    "label": 0
                },
                {
                    "sent": "I first of all I will introduce.",
                    "label": 0
                },
                {
                    "sent": "The problem of natural language processing.",
                    "label": 0
                },
                {
                    "sent": "The problem applying that usually is also called that is also known as computational linguistics.",
                    "label": 0
                },
                {
                    "sent": "And I will try to focus the talk in a specific in specific tools that are usually used.",
                    "label": 0
                },
                {
                    "sent": "In this field.",
                    "label": 0
                },
                {
                    "sent": "And I will describe concretely.",
                    "label": 0
                },
                {
                    "sent": "Tools like.",
                    "label": 0
                },
                {
                    "sent": "Hamm hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "That is a useful tool in natural language processing.",
                    "label": 0
                },
                {
                    "sent": "And probabilistic context free grammar that is also.",
                    "label": 0
                },
                {
                    "sent": "A tool that is.",
                    "label": 0
                },
                {
                    "sent": "Usually used in natural language processing in computational linguistics.",
                    "label": 0
                },
                {
                    "sent": "This kind of models.",
                    "label": 0
                },
                {
                    "sent": "Have been used for several specific tasks and I will describe two of them.",
                    "label": 0
                },
                {
                    "sent": "Hidden Markov models applied to post tagging and probabilistic context free grammar for parsing and language modeling.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I will introduce some some notation about hidden Markov models and then we will see.",
                    "label": 0
                },
                {
                    "sent": "The fundamental algorithms that are usually used.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "Hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "Then we will see how to learn.",
                    "label": 0
                },
                {
                    "sent": "The parameters of the model, not the structural part, just the probabilistic part of the model.",
                    "label": 0
                },
                {
                    "sent": "And later we will see the use of.",
                    "label": 0
                },
                {
                    "sent": "We will introduce the concept of probabilistic context, free grammar.",
                    "label": 0
                },
                {
                    "sent": "Fundamental algorithms for this kind of models.",
                    "label": 0
                },
                {
                    "sent": "And how the probabilities of this kind of models can be learned from data?",
                    "label": 0
                },
                {
                    "sent": "And we will see how to use this sort of models to parsing and language modeling.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In the in the final part of the talk I will introduce some interesting topics about this kind of models like online learning, for syntactic models, altica learning, and.",
                    "label": 0
                },
                {
                    "sent": "I will describe in this last point.",
                    "label": 0
                },
                {
                    "sent": "A possible framework in order to.",
                    "label": 0
                },
                {
                    "sent": "To join things like online learning and active learning, this is currently our my.",
                    "label": 0
                },
                {
                    "sent": "Russia my English stop.",
                    "label": 0
                },
                {
                    "sent": "I will see if we have time I will show a demo about.",
                    "label": 0
                },
                {
                    "sent": "This this framework OK?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go.",
                    "label": 0
                },
                {
                    "sent": "Please think into account that most of the techniques that I will describe specifically for this kind of models can be used also for other models in mature lending.",
                    "label": 0
                },
                {
                    "sent": "I will introduce the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "I will describe the basic idea of the EM algorithm, but from another point point of view.",
                    "label": 0
                },
                {
                    "sent": "A point of view that is known as growth transformations.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This kind of techniques can be also applied.",
                    "label": 0
                },
                {
                    "sent": "Two other to other fields.",
                    "label": 0
                },
                {
                    "sent": "OK the Emily theme.",
                    "label": 0
                },
                {
                    "sent": "Online learning active learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so tell me.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Stop this.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah in the in the lab sessions we will see.",
                    "label": 0
                },
                {
                    "sent": "We will try to develop a practice about HHMMM.",
                    "label": 0
                },
                {
                    "sent": "Hidden Markov models for POS tagging.",
                    "label": 0
                },
                {
                    "sent": "OK that is for lap.",
                    "label": 0
                },
                {
                    "sent": "And then I have I have to prepare it.",
                    "label": 0
                },
                {
                    "sent": "Another exercise for the estimation of probabilistic context free grammar.",
                    "label": 0
                },
                {
                    "sent": "Two to two different exercises.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's continue.",
                    "label": 0
                },
                {
                    "sent": "Well, the problem of natural language processing or or the problem of computational linguistics that is.",
                    "label": 0
                },
                {
                    "sent": "Usually.",
                    "label": 0
                },
                {
                    "sent": "That is another way of referring to the same concept.",
                    "label": 0
                },
                {
                    "sent": "The minor problem deals with the problem of computational linguistics is dealing with one of the most difficult problems that is natural language.",
                    "label": 1
                },
                {
                    "sent": "Natural language is a very difficult problem because there the apart from other problems like accent like.",
                    "label": 0
                },
                {
                    "sent": "The specific problems related to this zoning, in which people live and things like that.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of ambiguity in language, so.",
                    "label": 0
                },
                {
                    "sent": "This is a very difficult problem.",
                    "label": 0
                },
                {
                    "sent": "And in computational linguistics, the mind approach that is used to tackle these problems.",
                    "label": 0
                },
                {
                    "sent": "Currently relies on.",
                    "label": 0
                },
                {
                    "sent": "Probably on probability.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to to solve the problem of of the ambiguity by.",
                    "label": 0
                },
                {
                    "sent": "Using in this case probabilities in order to take decisions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main goal in computational linguistics is to develop systems that are able to.",
                    "label": 1
                },
                {
                    "sent": "To tackle the problem of natural language.",
                    "label": 0
                },
                {
                    "sent": "Show they motivate the motivation of this or the need of computational linguistics is clear, but another reason another motivation could be this.",
                    "label": 0
                },
                {
                    "sent": "That natural language is.",
                    "label": 0
                },
                {
                    "sent": "A natural way of communicating of transferring knowledge to the people.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "Another reason for this field.",
                    "label": 1
                },
                {
                    "sent": "For research in this field is that there there is a lot of information in natural language.",
                    "label": 0
                },
                {
                    "sent": "And they and there are a lot of potential users.",
                    "label": 0
                },
                {
                    "sent": "For using this kind, the kind of of tools that can be developed in computational linguistics.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "For Focusin, for focusing and speaking about for speaking about most specific application.",
                    "label": 0
                },
                {
                    "sent": "Some this is a sum of application that we can deal with.",
                    "label": 0
                },
                {
                    "sent": "With the tools that are studied in computational linguistics.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Systems for information extraction from text and speech like.",
                    "label": 1
                },
                {
                    "sent": "For information retrieval, information extraction, text categorisation.",
                    "label": 0
                },
                {
                    "sent": "In computational research, the tools and techniques that are exciting computational linguistics can be also used for developing systems that are able to.",
                    "label": 0
                },
                {
                    "sent": "Translate speech to speech, speech to text, text to speech.",
                    "label": 1
                },
                {
                    "sent": "Concretely, like machine translation, speech, translation, speech recognition.",
                    "label": 0
                },
                {
                    "sent": "And our application of computational linguistics can be this systems for communication with humans like dialogue systems that are even more complex than other systems like a speech recognition system.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as far as I have commented previously, the idea is to use probabilistic techniques in order to solve the problem of ambiguity that usually arise in this kind.",
                    "label": 0
                },
                {
                    "sent": "Of problems.",
                    "label": 0
                },
                {
                    "sent": "So if we have to use.",
                    "label": 0
                },
                {
                    "sent": "Probabilities in order to tackle these problems we have to deal at least with three issues.",
                    "label": 0
                },
                {
                    "sent": "One of them is interpretation, that is to generate additional light.",
                    "label": 0
                },
                {
                    "sent": "Interpretation from argument input sometimes in pattern recognition.",
                    "label": 0
                },
                {
                    "sent": "This problem is not too much difficult because because the number of classes is not too large, but in computational linguistics.",
                    "label": 0
                },
                {
                    "sent": "In the program, for example of post tagging in the problem of language modeling.",
                    "label": 0
                },
                {
                    "sent": "The number of classes is infinite.",
                    "label": 0
                },
                {
                    "sent": "And we have to develop adequate techniques for searching in that space.",
                    "label": 0
                },
                {
                    "sent": "So this is a very important problem in computational linguistics.",
                    "label": 0
                },
                {
                    "sent": "For example, in material in machine translation and in speech recognition.",
                    "label": 0
                },
                {
                    "sent": "So another issue that that much we must take.",
                    "label": 0
                },
                {
                    "sent": "To take into account is the problem of modeling.",
                    "label": 1
                },
                {
                    "sent": "We need to define models.",
                    "label": 0
                },
                {
                    "sent": "That are able to capture to represent the knowledge that we are trying.",
                    "label": 0
                },
                {
                    "sent": "In that we have available.",
                    "label": 0
                },
                {
                    "sent": "So most of the tools that are currently used in computational linguistics.",
                    "label": 0
                },
                {
                    "sent": "Are based on.",
                    "label": 0
                },
                {
                    "sent": "Statistical decision techniques and formal language theory.",
                    "label": 1
                },
                {
                    "sent": "OK, and the idea is to define adequate models to represent the reality.",
                    "label": 0
                },
                {
                    "sent": "And finally, given that our approach are based in inductive learning, learning from examples, we have to tackle with the problem of learning from from a given set of examples.",
                    "label": 0
                },
                {
                    "sent": "True examples.",
                    "label": 0
                },
                {
                    "sent": "We have three different problems and we have to tackle with.",
                    "label": 0
                },
                {
                    "sent": "Everyone with everyone OK, OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main goal of this talk?",
                    "label": 0
                },
                {
                    "sent": "Is 2.",
                    "label": 0
                },
                {
                    "sent": "Remind introduced just to introduce.",
                    "label": 1
                },
                {
                    "sent": "Syntactic approaches to deal with difficult problems of natural language.",
                    "label": 1
                },
                {
                    "sent": "Natural language processing OK. Another goal is to.",
                    "label": 0
                },
                {
                    "sent": "To show some techniques, some tools of computer computational linguistics.",
                    "label": 1
                },
                {
                    "sent": "Some vocabulary, some notions OK, and to learn some basic techniques that are necessary to develop and to work with this sort of.",
                    "label": 0
                },
                {
                    "sent": "We with this with the model that we will see later.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So these are some of the applications.",
                    "label": 0
                },
                {
                    "sent": "Of that need.",
                    "label": 0
                },
                {
                    "sent": "Knowledge data in which computational linguistics has been working in the last years.",
                    "label": 0
                },
                {
                    "sent": "Automatic speech recognition machine translation, dialogue systems, etc.",
                    "label": 1
                },
                {
                    "sent": "And for this.",
                    "label": 0
                },
                {
                    "sent": "Concrete applications.",
                    "label": 0
                },
                {
                    "sent": "The idea is to use.",
                    "label": 0
                },
                {
                    "sent": "Some abstract task like language modeling.",
                    "label": 1
                },
                {
                    "sent": "Part of speech tagging, parsing an other after tax, and in this.",
                    "label": 0
                },
                {
                    "sent": "In this talk we will just describe.",
                    "label": 0
                },
                {
                    "sent": "3DS3DS3 problems we will just introduce these three problems, but there are a lot of problems named entity recognition.",
                    "label": 1
                },
                {
                    "sent": "Close detection etc.",
                    "label": 0
                },
                {
                    "sent": "Lexical disambiguation, semantic, analyze it, etc.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you know there are two 2 to tackle with these problems.",
                    "label": 0
                },
                {
                    "sent": "We must take into account that the natural language can be.",
                    "label": 0
                },
                {
                    "sent": "Then the knowledge that are present in natural language can be divided in several levels according to the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Flexity, so these are the.",
                    "label": 0
                },
                {
                    "sent": "A possible division of these levels, depending on the complexity of the knowledge that are in each level.",
                    "label": 0
                },
                {
                    "sent": "So we could introduce a morphology level, syntax level, semantic level, pragmatic level, discourse level.",
                    "label": 0
                },
                {
                    "sent": "So the tools that we will see in this in this talk will focus in this level and we will.",
                    "label": 0
                },
                {
                    "sent": "We will be talking about.",
                    "label": 0
                },
                {
                    "sent": "Part of speech tagging that is related to work category and sentence structure that is really an passing that disability would center the structure and language modeling.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the the main problem in post tagging.",
                    "label": 0
                },
                {
                    "sent": "Is the following.",
                    "label": 0
                },
                {
                    "sent": "The idea is in natural language processing in computational linguistics, sometimes we have models that have a lot of parameters, so we need to.",
                    "label": 0
                },
                {
                    "sent": "To group.",
                    "label": 0
                },
                {
                    "sent": "For example, the words in classes in order to to decrease the number of parameters or to help the system by grouping words in a set of.",
                    "label": 0
                },
                {
                    "sent": "In a set of of in a given set of.",
                    "label": 1
                },
                {
                    "sent": "In this case, part of speech.",
                    "label": 0
                },
                {
                    "sent": "Part OK.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to classify a given set to classify the words in a given set of part of speech tags.",
                    "label": 0
                },
                {
                    "sent": "That in this case can be seen like this, which as you can note, this can be seen as a classification problem.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There have been several approaches to tackle with this problem like.",
                    "label": 0
                },
                {
                    "sent": "Hidden Markov models.",
                    "label": 1
                },
                {
                    "sent": "Maximum entropy approaches or support vector machines for tackle with this problem.",
                    "label": 0
                },
                {
                    "sent": "In this talk I will introduce.",
                    "label": 0
                },
                {
                    "sent": "Some ideas about how to use hidden Markov models for part of speech tagging.",
                    "label": 1
                },
                {
                    "sent": "Please note that one of the problems is to define a set of tasks.",
                    "label": 0
                },
                {
                    "sent": "But we we leave this problem for another talk OK?",
                    "label": 0
                },
                {
                    "sent": "And for example, this means that these are known a proper noun.",
                    "label": 0
                },
                {
                    "sent": "This is a proper noun.",
                    "label": 0
                },
                {
                    "sent": "This is a Cardinal number.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a.",
                    "label": 0
                },
                {
                    "sent": "A singular noun.",
                    "label": 0
                },
                {
                    "sent": "So this is for example, this tax has have been used.",
                    "label": 0
                },
                {
                    "sent": "In a corpus that is very known, well known that is the Penn treebank corpus.",
                    "label": 0
                },
                {
                    "sent": "OK, so Please note that.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we need we need to annotate data in order to have available this data and this is available complicated problem annotating data.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "And this is related with the last part of the talk how to annotate?",
                    "label": 0
                },
                {
                    "sent": "Data.",
                    "label": 0
                },
                {
                    "sent": "In in, in the most cheapest way.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the problem.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I don't understand your question, sorry.",
                    "label": 0
                },
                {
                    "sent": "Is complexity.",
                    "label": 0
                },
                {
                    "sent": "There are so many definitions.",
                    "label": 0
                },
                {
                    "sent": "But she's contacted.",
                    "label": 0
                },
                {
                    "sent": "No no no no no.",
                    "label": 0
                },
                {
                    "sent": "The sentence can be as long as you want and the problem is exactly the same.",
                    "label": 0
                },
                {
                    "sent": "It's a problem of ambiguity in this case, mostly of ambiguity.",
                    "label": 0
                },
                {
                    "sent": "The Post again is mostly a probability.",
                    "label": 0
                },
                {
                    "sent": "OK, so how come we deal with this issue by using hidden Markov model?",
                    "label": 0
                },
                {
                    "sent": "The idea is to define an automata like this in which.",
                    "label": 0
                },
                {
                    "sent": "Are you?",
                    "label": 0
                },
                {
                    "sent": "Which day is to go from an initial state to the end of the final state?",
                    "label": 0
                },
                {
                    "sent": "Following a path through this through this.",
                    "label": 0
                },
                {
                    "sent": "Automata and.",
                    "label": 0
                },
                {
                    "sent": "Every time you see a word.",
                    "label": 0
                },
                {
                    "sent": "You start in a given in a given state, and every time that you see another another word, you go to that state.",
                    "label": 0
                },
                {
                    "sent": "Depending on the the probabilities.",
                    "label": 0
                },
                {
                    "sent": "And you following this way until you arrive to this to the final state, and then the idea is 2, you must be able to recover the path that you have used and taking into account the state that you have used, you can attach a label label to every word that is the main idea.",
                    "label": 0
                },
                {
                    "sent": "What is an automata?",
                    "label": 0
                },
                {
                    "sent": "Hidden Markov models in this case.",
                    "label": 0
                },
                {
                    "sent": "I will, I will explain later.",
                    "label": 0
                },
                {
                    "sent": "I will explain later what is a hierarchal models.",
                    "label": 0
                },
                {
                    "sent": "I will explain later.",
                    "label": 0
                },
                {
                    "sent": "OK, this is that it will introduce the problem we will see later.",
                    "label": 0
                },
                {
                    "sent": "What is a hidden Markov model?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the other problem is related to probabilistic context free grammar.",
                    "label": 0
                },
                {
                    "sent": "And the problem of parsing and the idea is similar.",
                    "label": 0
                },
                {
                    "sent": "The idea is.",
                    "label": 0
                },
                {
                    "sent": "Given a sentence to define relation between different parts of the sentence.",
                    "label": 1
                },
                {
                    "sent": "And the problem here is the same.",
                    "label": 0
                },
                {
                    "sent": "We have ambiguity problems, so we must.",
                    "label": 0
                },
                {
                    "sent": "Tackle with this kind of ambiguity.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The tools that we will use here will will be.",
                    "label": 0
                },
                {
                    "sent": "Formal grammars, context free grammars.",
                    "label": 0
                },
                {
                    "sent": "In this case, I will focus the talk in context free grammars.",
                    "label": 0
                },
                {
                    "sent": "Quantity is just a formal tool that can be used to describe the relations between different parts of a sentence.",
                    "label": 0
                },
                {
                    "sent": "His idea is we have an an initial symbol and actually this is the action of the grammar and all sentence are composed from this symbol and applying this rule and expanding its time are given given symbol like this that is usually called nonterminal symbol.",
                    "label": 0
                },
                {
                    "sent": "You are applying recursively.",
                    "label": 0
                },
                {
                    "sent": "This kind of expanding this kind nonterminal symbols until you arrive to a rule.",
                    "label": 0
                },
                {
                    "sent": "That in the right part has award.",
                    "label": 0
                },
                {
                    "sent": "So applying these rules we can compose.",
                    "label": 0
                },
                {
                    "sent": "Apart, apart in three like this, a person through ladies.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in order to tackle with ambiguity, we will use.",
                    "label": 0
                },
                {
                    "sent": "In our case probability in order to decide the most probable.",
                    "label": 0
                },
                {
                    "sent": "Syntactic structure structure that is associated to a given sentence.",
                    "label": 0
                },
                {
                    "sent": "OK, we will see later in more detail things like this.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model OK so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the idea is a. I will describe one application of probability cause our priority content, for parsing and also for language modeling.",
                    "label": 0
                },
                {
                    "sent": "Although this is not the usual.",
                    "label": 0
                },
                {
                    "sent": "Approach for language modeling cause the most usual approach for language modeling.",
                    "label": 0
                },
                {
                    "sent": "It is ngram models.",
                    "label": 0
                },
                {
                    "sent": "OK, so but it's a possible.",
                    "label": 0
                },
                {
                    "sent": "It's a possibility.",
                    "label": 0
                },
                {
                    "sent": "OK, to use probabilistic context free grammar for language model and I explored this kind of model for this problem.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The idea is the following in apart from a pattern recognition point of view, the idea of developing our recognition can be seen in this way you have.",
                    "label": 0
                },
                {
                    "sent": "On a signal that arrives to our recognizer and we must to provide a possible interpretation, the most usually the most possible interpretation.",
                    "label": 0
                },
                {
                    "sent": "And the problem can be stated in this the other way.",
                    "label": 1
                },
                {
                    "sent": "So the idea is.",
                    "label": 0
                },
                {
                    "sent": "You if you see you we have reversed most in some kind in some way.",
                    "label": 0
                },
                {
                    "sent": "The problem because the idea is to generate a possible output and then to measure how this output accounts for the input.",
                    "label": 0
                },
                {
                    "sent": "So this is the the the main approach.",
                    "label": 0
                },
                {
                    "sent": "That is usually used for speech recognition.",
                    "label": 0
                },
                {
                    "sent": "And also in some.",
                    "label": 0
                },
                {
                    "sent": "Applications like much in translation.",
                    "label": 0
                },
                {
                    "sent": "This is called the language model.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is usually called the.",
                    "label": 0
                },
                {
                    "sent": "Channel model choose the channel probability and this is the language model probability.",
                    "label": 1
                },
                {
                    "sent": "The idea is to generate, for example, for speech recognition sentences, possible sentences and then to measure how each sentence accounts.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "For, for a given input.",
                    "label": 0
                },
                {
                    "sent": "Logically, the this is a simplistic.",
                    "label": 0
                },
                {
                    "sent": "This is a simplistic definition obviously cause as you must take into account the number of possible outputs can be infinite OK and here is where the problem of shared search appears.",
                    "label": 0
                },
                {
                    "sent": "Search in a large space.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the approach that is usually used for language for speech recognition.",
                    "label": 1
                },
                {
                    "sent": "Here we have a signal.",
                    "label": 0
                },
                {
                    "sent": "Are the color that usually.",
                    "label": 0
                },
                {
                    "sent": "Is fed up with models hidden Markov models, usually hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "And England models for for modeling the language there.",
                    "label": 0
                },
                {
                    "sent": "The language model.",
                    "label": 0
                },
                {
                    "sent": "And finally we provide.",
                    "label": 0
                },
                {
                    "sent": "As the most probably words that are speaker.",
                    "label": 0
                },
                {
                    "sent": "Has added OK so.",
                    "label": 0
                },
                {
                    "sent": "In the speech recognition, the kind of hidden Markov models that are usually used.",
                    "label": 0
                },
                {
                    "sent": "Are continuous hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "But in our case, I will focus the talk just in discrete hidden Markov models, and this is I have prepared the talking this way because I think that the discrete version is easy to understand, most easy to explain OK.",
                    "label": 1
                },
                {
                    "sent": "So the language model is.",
                    "label": 0
                },
                {
                    "sent": "This part of the discussion and.",
                    "label": 0
                },
                {
                    "sent": "It tries to model the open language.",
                    "label": 0
                },
                {
                    "sent": "So the this is.",
                    "label": 0
                },
                {
                    "sent": "This is a priority, can be discomposed discomposed in this way, but usually.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you see here the probability of a given word is conditioned in the in all the previous words, But the most common approaches to simplify this expression and just take into account not all the previous work but just a few few of them, one or two previous words.",
                    "label": 0
                },
                {
                    "sent": "OK, but even in our case is given that we will use probabilistic interferon.",
                    "label": 0
                },
                {
                    "sent": "We'll see how this can be computed.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the usual approach for language modeling.",
                    "label": 1
                },
                {
                    "sent": "The usual approach is to simplify the previous session this expression.",
                    "label": 1
                },
                {
                    "sent": "And the idea is to compute the probability of award.",
                    "label": 0
                },
                {
                    "sent": "Condition it.",
                    "label": 0
                },
                {
                    "sent": "Not in all the previous words, but just in one 2.",
                    "label": 0
                },
                {
                    "sent": "Or toward almost, and this is usually known as 3 gram triggering road 3.",
                    "label": 0
                },
                {
                    "sent": "Three gram model for Gram model is not very useful, and the largest improvement are usually obtained when we used.",
                    "label": 0
                },
                {
                    "sent": "When we go from bigram models to trigger modes, bigger models in a bigger model we have into account just one word, one previous word.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the usual approach in.",
                    "label": 0
                },
                {
                    "sent": "Which is the usual approach in.",
                    "label": 0
                },
                {
                    "sent": "Immigrants, OK, the problem with bigrams models with trigger models with in general with Ingram models is that.",
                    "label": 0
                },
                {
                    "sent": "They are not able to capture as equally long term dependencies within words.",
                    "label": 0
                },
                {
                    "sent": "But the positive thing of this kind of models?",
                    "label": 0
                },
                {
                    "sent": "The positive one of the positive.",
                    "label": 0
                },
                {
                    "sent": "The positive things is that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the.",
                    "label": 0
                },
                {
                    "sent": "The probability can be officially computed in an online system.",
                    "label": 0
                },
                {
                    "sent": "And there exist efficient methods to estimate the parameters of the model.",
                    "label": 1
                },
                {
                    "sent": "OK, when you sing grammatical models as we will see in this in this talk.",
                    "label": 0
                },
                {
                    "sent": "We could we can.",
                    "label": 1
                },
                {
                    "sent": "We can compute the probability of a word not taking into account just a few words of previous three words that the complete sentence.",
                    "label": 1
                },
                {
                    "sent": "The problem, one of the advantages of this approach is that is able to capture long-term dependencies.",
                    "label": 0
                },
                {
                    "sent": "But is expensive to compute in an online system and.",
                    "label": 0
                },
                {
                    "sent": "The efficient method that exist that exist for estimating the parameters of the model are very expensive.",
                    "label": 0
                },
                {
                    "sent": "Very expensive.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This is a very interesting research topic.",
                    "label": 0
                },
                {
                    "sent": "Integrating generating syntactic models in language modeling.",
                    "label": 0
                },
                {
                    "sent": "There has been some approaches, but I think that is also that I think that it continues being research.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "OK, because currently.",
                    "label": 0
                },
                {
                    "sent": "For obtaining good results with this kind of models, we must interpolate.",
                    "label": 0
                },
                {
                    "sent": "We must use both kind of models and interpolate.",
                    "label": 0
                },
                {
                    "sent": "The interpolate model.",
                    "label": 0
                },
                {
                    "sent": "So the idea of integrating a syntactic model in in system for speech recognition, for example, is a very challenging problem.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Some questions.",
                    "label": 0
                },
                {
                    "sent": "I will go in depth.",
                    "label": 0
                },
                {
                    "sent": "Now with the hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "Where's the answer question?",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Let's continue.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You so I will follow this.",
                    "label": 0
                },
                {
                    "sent": "This reference in this slide.",
                    "label": 0
                },
                {
                    "sent": "This is not the most well known reference for this kind of models, but I have used.",
                    "label": 0
                },
                {
                    "sent": "The notation that is used in this in this.",
                    "label": 0
                },
                {
                    "sent": "In this light, the most well the most popular reference is a reference from ball.",
                    "label": 0
                },
                {
                    "sent": "I don't remember the name.",
                    "label": 0
                },
                {
                    "sent": "Jellinek I think Lafferty.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This kind of models were popular popular inside when they were used for speech recognition in the A in the first 80s.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this kind of models are very interesting models becausw.",
                    "label": 0
                },
                {
                    "sent": "A simple models for representing regular.",
                    "label": 1
                },
                {
                    "sent": "Relation between words.",
                    "label": 1
                },
                {
                    "sent": "Is a formal proof that is well understood, well known?",
                    "label": 1
                },
                {
                    "sent": "Is although natural language is not completely regular, it can be considered almost regular.",
                    "label": 0
                },
                {
                    "sent": "Which is equipped to represent short-term syntactic relation structures.",
                    "label": 0
                },
                {
                    "sent": "And given the powerful of this kind of models that combines concepts from language theory on from statistics.",
                    "label": 0
                },
                {
                    "sent": "Alec wait for modeling the big city.",
                    "label": 0
                },
                {
                    "sent": "OK, and I is the core.",
                    "label": 0
                },
                {
                    "sent": "Models for a large variety of systems, mainly speech recognition systems.",
                    "label": 0
                },
                {
                    "sent": "There are free shower for tackling with this kind of models.",
                    "label": 0
                },
                {
                    "sent": "For example, the most well known, So what is?",
                    "label": 0
                },
                {
                    "sent": "H. Tiki that you can download and use for developing speech recognition systems.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so our problem is to tackle with with some kind of objects.",
                    "label": 0
                },
                {
                    "sent": "So our first in this.",
                    "label": 0
                },
                {
                    "sent": "In this light I will define the kind of object that we that we will use.",
                    "label": 0
                },
                {
                    "sent": "For for the plan that we interested in so.",
                    "label": 0
                },
                {
                    "sent": "The first concept is the concept of primitives.",
                    "label": 0
                },
                {
                    "sent": "In our case we will work with an alphabet that is.",
                    "label": 0
                },
                {
                    "sent": "A set of symbols.",
                    "label": 0
                },
                {
                    "sent": "A set of symbols, for example, words with these punctuation symbols.",
                    "label": 1
                },
                {
                    "sent": "Date, Cardinal numbers, etc.",
                    "label": 0
                },
                {
                    "sent": "This is our primitives.",
                    "label": 0
                },
                {
                    "sent": "Our object will be composed by this kind of primitives.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We will join this kind of primitives.",
                    "label": 1
                },
                {
                    "sent": "The symbols of the alphabet which will join in order to compose.",
                    "label": 0
                },
                {
                    "sent": "A more complex object that in our case will be a sentence.",
                    "label": 0
                },
                {
                    "sent": "I want by joining symbols we we can form, in our case sentences.",
                    "label": 0
                },
                {
                    "sent": "For example, a sentence like this.",
                    "label": 0
                },
                {
                    "sent": "This is the first.",
                    "label": 0
                },
                {
                    "sent": "Sentence I will be using this sentence because cause this is the.",
                    "label": 0
                },
                {
                    "sent": "The sentence, the first sentence of a well known.",
                    "label": 0
                },
                {
                    "sent": "Corpus that is the tree bank corpus.",
                    "label": 0
                },
                {
                    "sent": "OK, so our projects.",
                    "label": 0
                },
                {
                    "sent": "Will be sentences that will be grouped in order to form sets in the.",
                    "label": 0
                },
                {
                    "sent": "In this case, sets of sentences and then this.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "This will be our pattern set.",
                    "label": 1
                },
                {
                    "sent": "OK, and the idea is to use and.",
                    "label": 0
                },
                {
                    "sent": "Our problem is to obtain a possible interpretation for this object.",
                    "label": 0
                },
                {
                    "sent": "That innovation will be post tax Association, that is, to associate a POS tag to every word.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are our objects.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "For this kind of objects, we will.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cheap.",
                    "label": 0
                },
                {
                    "sent": "What we will I will introduce more formally this this this object.",
                    "label": 0
                },
                {
                    "sent": "Through this concept, the concept of alphabet.",
                    "label": 0
                },
                {
                    "sent": "OK, let's have.",
                    "label": 0
                },
                {
                    "sent": "It is just a finite set of symbols.",
                    "label": 1
                },
                {
                    "sent": "With these symbols, we will compose sentences strings in our case.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "A string is just the concatenation of a given number of symbols, and we can compose sentences larger sentences by joining.",
                    "label": 0
                },
                {
                    "sent": "Yeah, shorter sentences.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "A special symbol is the empty string.",
                    "label": 0
                },
                {
                    "sent": "That is, this is a string with length equal to 0.",
                    "label": 1
                },
                {
                    "sent": "Yeah, this is this is it is important to remark this string becausw in some application.",
                    "label": 0
                },
                {
                    "sent": "The use of this special string introduces complexity problems.",
                    "label": 0
                },
                {
                    "sent": "So, but for some form or for most of the application that we that we will use that we will see in this talk, we can avoid this special string.",
                    "label": 1
                },
                {
                    "sent": "So we defined the closure of a set of the closure of an alphabet, like in this way, that is, the number the infinite on controller set of strings.",
                    "label": 1
                },
                {
                    "sent": "Of we finish the length and the positive closure is defined like the closure by avoiding the empty string.",
                    "label": 0
                },
                {
                    "sent": "So the this is the formal definition of the object of the object that we will use in this for this kind of model.",
                    "label": 0
                },
                {
                    "sent": "So a language can be defined it in this way just in this way OK?",
                    "label": 0
                },
                {
                    "sent": "So this is our language.",
                    "label": 0
                },
                {
                    "sent": "This is our objects.",
                    "label": 0
                },
                {
                    "sent": "The object that we will use that in that we will use for describing our problems.",
                    "label": 0
                },
                {
                    "sent": "So now we need.",
                    "label": 0
                },
                {
                    "sent": "Some model that account for this kind of for this kind of objects, so in our case we will use.",
                    "label": 0
                },
                {
                    "sent": "Hidden Markov models and completely discreet hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what gives him a commercial can be defined it in this way.",
                    "label": 0
                },
                {
                    "sent": "This is a couple ladies where this is the number of states.",
                    "label": 0
                },
                {
                    "sent": "Sorry this is the set of a state.",
                    "label": 0
                },
                {
                    "sent": "This is a graphical representation representation or hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "This is the set of stage, the alphabet and two and the three different probability distributions representing the probability of gooie.",
                    "label": 0
                },
                {
                    "sent": "Passing from this from this state to this state.",
                    "label": 0
                },
                {
                    "sent": "This is the transition probability.",
                    "label": 0
                },
                {
                    "sent": "For what we from one state to another state.",
                    "label": 0
                },
                {
                    "sent": "This is the emission probability, that is, the probability of serving a given symbol in a state.",
                    "label": 0
                },
                {
                    "sent": "So this is the probability of being a state of state being initial state.",
                    "label": 0
                },
                {
                    "sent": "That means that we can start to.",
                    "label": 0
                },
                {
                    "sent": "To accept to recognize a given string in a in a in a state so.",
                    "label": 0
                },
                {
                    "sent": "This is a graphical representation.",
                    "label": 0
                },
                {
                    "sent": "This this restriction must be taken into account.",
                    "label": 0
                },
                {
                    "sent": "So this probabilities must add add up to one.",
                    "label": 0
                },
                {
                    "sent": "These probabilities must.",
                    "label": 0
                },
                {
                    "sent": "But up to one and the same thing for each state and the same thing for these probabilities.",
                    "label": 0
                },
                {
                    "sent": "This is a special kind of models that is very usually used in.",
                    "label": 0
                },
                {
                    "sent": "In speech recognition, this is a model that usually called left to right.",
                    "label": 0
                },
                {
                    "sent": "Left to right model becausw every time that you.",
                    "label": 0
                },
                {
                    "sent": "At that you arrive to one state.",
                    "label": 0
                },
                {
                    "sent": "You can not return to any state with a lower index.",
                    "label": 0
                },
                {
                    "sent": "So it's a way of representing temporal information.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                },
                {
                    "sent": "If you see in another book you will you can find.",
                    "label": 0
                },
                {
                    "sent": "Maybe different definitions, but all of them are very similar.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this will be the kind of models that will be used for accounting.",
                    "label": 0
                },
                {
                    "sent": "For this sort of object that we are interested in.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How am I hidden Markov model?",
                    "label": 0
                },
                {
                    "sent": "Works, so the idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "Maybe some of you are familiar familiar with this kind of models, but maybe the other ones are not familiar with this kind of model, so the idea is the following.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we have a string like this.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to go through a path in this.",
                    "label": 0
                },
                {
                    "sent": "Model in order to account for this.",
                    "label": 0
                },
                {
                    "sent": "For this for this string.",
                    "label": 0
                },
                {
                    "sent": "So we can start in this state.",
                    "label": 0
                },
                {
                    "sent": "To observe this symbol.",
                    "label": 0
                },
                {
                    "sent": "Translate to another to go to another.",
                    "label": 0
                },
                {
                    "sent": "State to observe another symbol.",
                    "label": 0
                },
                {
                    "sent": "To observe the symbol.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Finally, to arrive to the final state, something like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so immediately you can see that.",
                    "label": 0
                },
                {
                    "sent": "We can define.",
                    "label": 0
                },
                {
                    "sent": "Another path.",
                    "label": 0
                },
                {
                    "sent": "Ladies.",
                    "label": 0
                },
                {
                    "sent": "So this is a different path because we have changed this state here OK?",
                    "label": 0
                },
                {
                    "sent": "So this is the way in which.",
                    "label": 0
                },
                {
                    "sent": "Model account for a string.",
                    "label": 0
                },
                {
                    "sent": "So the idea is.",
                    "label": 0
                },
                {
                    "sent": "This is a possible path and the probability of this path for observing this string is computed just by multiplying the probabilities that we are using in this path for example.",
                    "label": 0
                },
                {
                    "sent": "We have a start in this in this state, then we have arrived to the second state with probability.",
                    "label": 0
                },
                {
                    "sent": "Zillow dot .1.",
                    "label": 0
                },
                {
                    "sent": "We here we have shared this symbol with probability 0.9.",
                    "label": 0
                },
                {
                    "sent": "And by.",
                    "label": 0
                },
                {
                    "sent": "Multiplying all probabilities that are used.",
                    "label": 0
                },
                {
                    "sent": "In in every path we are able to come to compute the probability of sharing this string through this path, OK?",
                    "label": 0
                },
                {
                    "sent": "Questions is that clear?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "This is this is this is this you are using this property distribution for transfer transiting from one state to another state.",
                    "label": 0
                },
                {
                    "sent": "Q1.",
                    "label": 0
                },
                {
                    "sent": "You don't understand.",
                    "label": 0
                },
                {
                    "sent": "I don't understand this honey.",
                    "label": 0
                },
                {
                    "sent": "Weather output.",
                    "label": 0
                },
                {
                    "sent": "On the picture.",
                    "label": 0
                },
                {
                    "sent": "No, no, this is the probability of this state being initial state.",
                    "label": 0
                },
                {
                    "sent": "You must start always in this state.",
                    "label": 0
                },
                {
                    "sent": "Although the definition is quite general, but this is a restricted model.",
                    "label": 0
                },
                {
                    "sent": "This is the probability of starting in this.",
                    "label": 0
                },
                {
                    "sent": "In this you cannot start in this in this state because the probability of key one is.",
                    "label": 0
                },
                {
                    "sent": "Is 1 so there is no, you know have any chance to start in this state or in this state?",
                    "label": 0
                },
                {
                    "sent": "On the way.",
                    "label": 0
                },
                {
                    "sent": "One just from there.",
                    "label": 0
                },
                {
                    "sent": "You have to read it correctly.",
                    "label": 0
                },
                {
                    "sent": "I don't understand the.",
                    "label": 0
                },
                {
                    "sent": "Where do you read characters?",
                    "label": 0
                },
                {
                    "sent": "We are not reading character.",
                    "label": 0
                },
                {
                    "sent": "We are OK. OK, OK, well my question so maybe.",
                    "label": 0
                },
                {
                    "sent": "The problem is how Markov model accounts for for for a string.",
                    "label": 0
                },
                {
                    "sent": "So this is the way of relating States and path with with with with a string.",
                    "label": 0
                },
                {
                    "sent": "But we will see later how how OK. Have you have been explaining the hidden Markov model, like a generator or like an acceptor?",
                    "label": 0
                },
                {
                    "sent": "OK so but and I think the problem is is.",
                    "label": 0
                },
                {
                    "sent": "Being seen, the hidden Markov model like acceptor of like an generator, I think that it doesn't matter in this moment.",
                    "label": 0
                },
                {
                    "sent": "If it is that the question.",
                    "label": 0
                },
                {
                    "sent": "So we will see later how, given how, given a given a string, you can account for that string.",
                    "label": 0
                },
                {
                    "sent": "I have explaining the hidden Markov model like a generator, but is a way of seeing how to relate a state with symbols OK.",
                    "label": 0
                },
                {
                    "sent": "So the idea.",
                    "label": 0
                },
                {
                    "sent": "So the idea of of hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "The name of hidden Markov models becausw.",
                    "label": 0
                },
                {
                    "sent": "In in usually you always see this.",
                    "label": 0
                },
                {
                    "sent": "This is the part that you have seen in hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "Does this part is usually unknown.",
                    "label": 0
                },
                {
                    "sent": "This is usually unknown, so if you have for example, you can see that the number of possible path is infinite can be infinite.",
                    "label": 0
                },
                {
                    "sent": "So our main problem now will be how to compute these efficiently.",
                    "label": 0
                },
                {
                    "sent": "OK, but please take into account that the way of presenting the example is I have presented example like a generator, but it's a way of explaining how to relate symbols with states OK.",
                    "label": 0
                },
                {
                    "sent": "So the problem now will be how to compute this efficiently.",
                    "label": 0
                },
                {
                    "sent": "OK. We're still more questions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the kind of model with that we will use so I have how have said the problem is how to compute these efficiently because the number of possible transitions could be.",
                    "label": 0
                },
                {
                    "sent": "If it is not infinite, sorry exponential exponents.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we have patterns we have.",
                    "label": 0
                },
                {
                    "sent": "A model that accounts for the kind of model.",
                    "label": 0
                },
                {
                    "sent": "Now we need to know to introduce some some.",
                    "label": 0
                },
                {
                    "sent": "Some ideas in order how in order to define how to compute how to compute this efficiently?",
                    "label": 0
                },
                {
                    "sent": "We need to describe how to compute the probability the probability of of a hidden Markov model accounting for a string.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now working so sorry.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "This is an example of why have you been discussing here.",
                    "label": 0
                },
                {
                    "sent": "This is the way in which.",
                    "label": 0
                },
                {
                    "sent": "Hidden Markov model accounts for a string.",
                    "label": 0
                },
                {
                    "sent": "Through a path.",
                    "label": 0
                },
                {
                    "sent": "So we start with a new state.",
                    "label": 0
                },
                {
                    "sent": "We are able to observe a symbol, then we translate translate to another state, observe another symbol, and at the end we arrived at the final state.",
                    "label": 0
                },
                {
                    "sent": "So the probability of this string through this path can be computed in this way.",
                    "label": 0
                },
                {
                    "sent": "This is the probability of a start of starting in this state or serving the symbol.",
                    "label": 0
                },
                {
                    "sent": "Translate into this state of serving the symbol, and are the priorities that appear here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Given a set up path through this through this.",
                    "label": 1
                },
                {
                    "sent": "Model that is able to accounting for this string.",
                    "label": 0
                },
                {
                    "sent": "The probability of this.",
                    "label": 0
                },
                {
                    "sent": "Sequence can be computed in this way.",
                    "label": 0
                },
                {
                    "sent": "Is the probability of transiting from here to here from here to here, and this is the probability that appears here and the probability of observing the symbols can be expressed in this way.",
                    "label": 0
                },
                {
                    "sent": "So finally given that the number of possible sequence could be exponential.",
                    "label": 0
                },
                {
                    "sent": "Let this expression be the set of possible valid path.",
                    "label": 1
                },
                {
                    "sent": "To account for this string so this probability can be can be defined in this way.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so sorry.",
                    "label": 0
                },
                {
                    "sent": "If you can see that this is a, you can see that this is a joint probability.",
                    "label": 0
                },
                {
                    "sent": "This is a joint probability.",
                    "label": 0
                },
                {
                    "sent": "The probability of transiting from one district to another state is independent.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yes, this is a kind of models.",
                    "label": 0
                },
                {
                    "sent": "Hidden Markov models is just a subclass of other kinds of models, and depending on the on the depending on the conditioning you can define even more complex hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "But in this case the probability of transiting from this from one state to another state.",
                    "label": 0
                },
                {
                    "sent": "Only the pain depends on the state in which you are.",
                    "label": 0
                },
                {
                    "sent": "Only this is a very hard restriction, but.",
                    "label": 0
                },
                {
                    "sent": "Simplify the computation of the of four applications OK. What do I am saying that when you decide?",
                    "label": 0
                },
                {
                    "sent": "When you are in a state and you go to another state.",
                    "label": 0
                },
                {
                    "sent": "The analysis the state to which you arrive only depends in the state in which you are.",
                    "label": 0
                },
                {
                    "sent": "OK. OK my question.",
                    "label": 0
                },
                {
                    "sent": "Wait until until we will see later.",
                    "label": 0
                },
                {
                    "sent": "So we have a patterns set of patterns.",
                    "label": 0
                },
                {
                    "sent": "And we have this model that is able to account for this.",
                    "label": 0
                },
                {
                    "sent": "For that is able to account for this.",
                    "label": 0
                },
                {
                    "sent": "For these patterns, now we need we need to introduce some concepts in order to relate.",
                    "label": 0
                },
                {
                    "sent": "These two concepts.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to use.",
                    "label": 0
                },
                {
                    "sent": "An algorithm that is able to compute this.",
                    "label": 0
                },
                {
                    "sent": "This probability in an efficient way.",
                    "label": 0
                },
                {
                    "sent": "So now we'll see.",
                    "label": 0
                },
                {
                    "sent": "Some of the fundamental algorithms that are usually used for.",
                    "label": 0
                },
                {
                    "sent": "Computing for tackling with hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The story theme is the.",
                    "label": 0
                },
                {
                    "sent": "Where the forward algorithm.",
                    "label": 0
                },
                {
                    "sent": "That there is a.",
                    "label": 0
                },
                {
                    "sent": "Very important to notice him that is based in a dynamic programming scheme.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So and this concept are our internal related with algorithmics.",
                    "label": 0
                },
                {
                    "sent": "So the idea is how they days.",
                    "label": 0
                },
                {
                    "sent": "How can we compute?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These efficiently you must take into account that this size of this set can be exponential.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The probability of observing a given symbol only only depends on the state in which you are.",
                    "label": 0
                },
                {
                    "sent": "I don't understand.",
                    "label": 0
                },
                {
                    "sent": "Explain this.",
                    "label": 0
                },
                {
                    "sent": "OK so the you must take into account that the number of possible path that accounts for a given a string can be exponential.",
                    "label": 0
                },
                {
                    "sent": "You need to add up for all possible sequences.",
                    "label": 0
                },
                {
                    "sent": "This is the set of all possible paths that counts for a given for a given string, you must shut up for all possible sequences.",
                    "label": 1
                },
                {
                    "sent": "And that is the meaning of this of this expression.",
                    "label": 0
                },
                {
                    "sent": "OK. OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How can we compute this officially so we can use the forward algorithm that can be defined in this way?",
                    "label": 0
                },
                {
                    "sent": "This is a tabular algorithm that fills in a table.",
                    "label": 0
                },
                {
                    "sent": "Which is sometimes known as trelles.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to fill in a table by considering considering.",
                    "label": 0
                },
                {
                    "sent": "Prefix.",
                    "label": 0
                },
                {
                    "sent": "Of increasing size.",
                    "label": 0
                },
                {
                    "sent": "So this is the initialization.",
                    "label": 0
                },
                {
                    "sent": "So in the initialization we just.",
                    "label": 0
                },
                {
                    "sent": "Compute this probability and then we.",
                    "label": 0
                },
                {
                    "sent": "Use this loop did recursion, in which.",
                    "label": 0
                },
                {
                    "sent": "We we we compute the probability of observing a given symbol, taking into account the computation for the previous prefix.",
                    "label": 0
                },
                {
                    "sent": "So the idea is filling up at a table from left to right, considering at each time a symbol.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we have an example.",
                    "label": 0
                },
                {
                    "sent": "Idea is to complete a cell like this.",
                    "label": 0
                },
                {
                    "sent": "We start here.",
                    "label": 0
                },
                {
                    "sent": "And this is the model.",
                    "label": 0
                },
                {
                    "sent": "And then we can.",
                    "label": 0
                },
                {
                    "sent": "We can translate from from the first state to the first state by yourself in the symbol that we see.",
                    "label": 0
                },
                {
                    "sent": "For example, this probability.",
                    "label": 0
                },
                {
                    "sent": "Or is this probability, then transiting from Q1 to Q1 and then observing the symbol?",
                    "label": 0
                },
                {
                    "sent": "This is the probability this probability transiting from Q 141 and then observing this symbol.",
                    "label": 0
                },
                {
                    "sent": "Something similar for this.",
                    "label": 0
                },
                {
                    "sent": "Unchallenging and in direct sample, for example, this.",
                    "label": 0
                },
                {
                    "sent": "Expression is means that we have this is this probability transiting from Q1 to Q2 and observing the symbol in the.",
                    "label": 0
                },
                {
                    "sent": "In this state, or we are in the in key two transiting to key 2.",
                    "label": 0
                },
                {
                    "sent": "And then observing the symbol.",
                    "label": 0
                },
                {
                    "sent": "In kitchen.",
                    "label": 0
                },
                {
                    "sent": "OK. And we compute this.",
                    "label": 0
                },
                {
                    "sent": "We complete this table.",
                    "label": 0
                },
                {
                    "sent": "And can be.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can be computed with this algorithm that completes.",
                    "label": 0
                },
                {
                    "sent": "That runs from left to right by considering.",
                    "label": 0
                },
                {
                    "sent": "Prefix of increasing size.",
                    "label": 0
                },
                {
                    "sent": "OK, so finally.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We obtain the probability of this string in this cell.",
                    "label": 0
                },
                {
                    "sent": "OK, the time complexity of this algorithm is linear.",
                    "label": 0
                },
                {
                    "sent": "So you must take into.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Know that.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Especially that we are not able to compute in this way.",
                    "label": 0
                },
                {
                    "sent": "Compute can be now computed.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not enough time.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the forward algorithm that computes compute probabilities of increasing size.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the definition.",
                    "label": 0
                },
                {
                    "sent": "Is the probability of generating.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A string and arrive into a given state.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can use another origin.",
                    "label": 0
                },
                {
                    "sent": "That is the bug.",
                    "label": 0
                },
                {
                    "sent": "We're already theme that deviates very similar, but in this case we compute not the probability of prefix.",
                    "label": 0
                },
                {
                    "sent": "Prefixes of increasing size, but the probability of suffixes of increasing size.",
                    "label": 0
                },
                {
                    "sent": "Or fingers inside from right to left.",
                    "label": 0
                },
                {
                    "sent": "So we can compute the probability of observing.",
                    "label": 0
                },
                {
                    "sent": "Given suffix.",
                    "label": 0
                },
                {
                    "sent": "From from from a given state.",
                    "label": 0
                },
                {
                    "sent": "So the idea is very similar, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Sometimes.",
                    "label": 0
                },
                {
                    "sent": "This is the forward terrorism.",
                    "label": 0
                },
                {
                    "sent": "This is about what algorithm and sometimes we interested.",
                    "label": 0
                },
                {
                    "sent": "In computing, not only the probability of a string.",
                    "label": 0
                },
                {
                    "sent": "For a given hidden Markov model, but to compute the probability, the probability of the most probable path that is able to account for a given string.",
                    "label": 0
                },
                {
                    "sent": "So in that case we can.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Produce an algorithm that is there.",
                    "label": 0
                },
                {
                    "sent": "It'll be algorithm that is known as the materialism.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we define the probability.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Here there here there is an error.",
                    "label": 0
                },
                {
                    "sent": "Note here is the.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "We need to hear an argument Arg Max.",
                    "label": 0
                },
                {
                    "sent": "Puppy.",
                    "label": 0
                },
                {
                    "sent": "So suppose that we are interested in computing the probability of the sequence that accounts for for the largest probability.",
                    "label": 0
                },
                {
                    "sent": "That accounts for the for the string with the largest probability, so we need to introduce an algorithm in this case that is now like the material that is exactly the same, exactly the same as the forward algorithm.",
                    "label": 0
                },
                {
                    "sent": "But we have 62 this addition.",
                    "label": 0
                },
                {
                    "sent": "Buy a maximization, but it's exactly the same everything.",
                    "label": 0
                },
                {
                    "sent": "The idea the interesting thing in the Viterbi algorithm?",
                    "label": 0
                },
                {
                    "sent": "Is that given that we are maximizing, we can.",
                    "label": 0
                },
                {
                    "sent": "We can leave back pointer to the cell with.",
                    "label": 0
                },
                {
                    "sent": "We have used in the previous state and then when we are right to the final state we are able to recover in the path and obtaining the interpretation and that is the that is the fundamental idea for obtaining a possible interpretation forgiven, forgiven string.",
                    "label": 0
                },
                {
                    "sent": "For example in speech recognition.",
                    "label": 0
                },
                {
                    "sent": "This is the idea that is usually used in order to look for.",
                    "label": 0
                },
                {
                    "sent": "A path with the largest probability.",
                    "label": 0
                }
            ]
        }
    }
}