{
    "id": "pc6vxfrnttzym433xbt2nxovddhgjmuv",
    "title": "Learning through Exploration",
    "info": {
        "author": [
            "Alina Beygelzimer, IBM Thomas J. Watson Research Center",
            "John Langford, Microsoft Research"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning"
        ]
    },
    "url": "http://videolectures.net/kdd2010_beygelzimer_langford_lte/",
    "segmentation": [
        [
            "Increasing the reward here is another setting, essentially this."
        ],
        [
            "Same problem, patient comes to a doctor."
        ],
        [
            "And there is a lot of associated information.",
            "Was the patient.",
            "There is hysteria, symptoms.",
            "Maybe some test results and the doctor chooses a treatment.",
            "There is some set of treatments from which the doctor chooses and then the doctor chooses one treatment and the patient responds to that treatment.",
            "But the doctor doesn't know what would have happened had he chosen a different treatment, right?",
            "So sort of a one shot.",
            "Partial information problem and of course the doctor wants a policy for choosing treatments that that is personalized to patients, right?",
            "He wants to improve the outcome."
        ],
        [
            "So let's try to formalize abstract and from these two problems.",
            "It's a repeated game where time goes from one to some time Horizon Capital T. In each timestep.",
            "The world produces some features.",
            "Acceptee in some space of features.",
            "Great, so a patient comes in or user comes.",
            "And both are encoded as some feature vector.",
            "And then the learner chooses an action in some space of actions.",
            "So let's call the actions one through K. And then the world reacts with the reward for the chosen action.",
            "And let's assume we will assume that all the rewards are between zero and one if there, yeah."
        ],
        [
            "So if it's bounded, then if the reward is bounded then you can just rescale.",
            "If the reward is unbounded, it's hard to talk about this type of problems, but.",
            "In any scenario you can think of.",
            "The rewards are can be assumed to be bounded, they have better.",
            "Cancer.",
            "Yeah.",
            "Yeah so.",
            "OK, so the goal is to learn a good policy, a good decision rule for mapping features to actions."
        ],
        [
            "What does it mean to learn?",
            "We so far we."
        ],
        [
            "And made any stochastic assumptions or any assumptions about the setting.",
            "It's in hindsight, it's better to think about learning as competing with some reasonably large class of alternative policies, right?",
            "So you want to do nearly as well as any pruned decision tree, for example, where as well as any linear classifier.",
            "So you choose some reference set of policies mapping from features to actions, and you want to do you want to compete?",
            "With this sad.",
            "Great, you want to do.",
            "Nearly as well as any policy in the set.",
            "So let's look at this formula which measures the performance of such an adaptive policy.",
            "It's called 3 grad, right?",
            "So at any time horizon T there is some policy in your reference set capital Pi that performed the best or no worse than any other policy in PIE.",
            "The best policy at this time carries and and you just some video boards of the actions predicted by that policy.",
            "So.",
            "So for any policy by.",
            "You can sum the rewards over the time steps of the reward of the action predicted by the policy on the tax.",
            "And then you just look at the reward accumulated by the best policy and pie so that this is the largest reword that any policy and by could have gotten.",
            "Over tea time steps.",
            "If something is unclear, please.",
            "Ask questions.",
            "Yeah, of course yes.",
            "So this is the game at every time step there is an axe.",
            "Where subscript little T where T ranges over the time steps.",
            "And then.",
            "Every policy in pie.",
            "Predict some action on axe and gets somebody word.",
            "What is so acts can be stochastic.",
            "It can be adversarially chosen.",
            "Oh, the word can be stochastic as well, but it's either drawn from some so it can be drawn from a distribution.",
            "It's OK.",
            "So then you look at the expectation of this quantity.",
            "Or maybe you want a high probability bound.",
            "So we will talk about these types of statements.",
            "But this is the essential quantity that.",
            "So we want to do.",
            "This is the best.",
            "But you were the largest reward than that.",
            "Any policy and pie could have gotten for that instantiation's instantiation of access access in ours.",
            "And this is what we got.",
            "Right, so this is our total reward.",
            "Where aseptis the action that we took an axe.",
            "And this is the word we gotten.",
            "Time step.",
            "Little T. And we want to minimize this gap.",
            "So we want to do.",
            "We want to compete with the best policy in PIE.",
            "Start.",
            "Is everyone comfortable?",
            "So it that's the same as competing with the sale of the set of all possible mapex right?",
            "So it's just a particular.",
            "Choice of capital plan.",
            "But yeah.",
            "Like so we're restricting the set of linear predictors and that.",
            "Particularizes your learning problem."
        ],
        [
            "OK, so this setting has a number of other names.",
            "It's called associative reinforcement learning or one step reinforcement learning, and we'll see why associative bandits for those who are familiar with multi arm bandit problems.",
            "Learning with partial feedback because the feedback that you get is partial.",
            "It's you only see the feedback for the action that you took.",
            "Or abandoned suicide information because you have this vector ax that's helping you to make predictions.",
            "Make decisions at every time step so you can think of access being your side information.",
            "OK, so a couple of observation."
        ],
        [
            "It's about this problem.",
            "It's not a supervised learning problem, right?",
            "We don't know the rewards of actions that we didn't take, so this information is missing even the training time.",
            "Exploration, which means taken possibly suboptimal actions, was the purpose of acquiring information about the problem.",
            "Rate is required.",
            "If if you don't know anything about an action you have.",
            "No.",
            "Way of ruling it out essentially, but it is simpler than general reinforcement learning, because here we assume that we know.",
            "Which action is responsible for each reward?",
            "So you take an action and you know the reward for that action and reinforcement learning.",
            "It's a sequence of actions that leads to a reward.",
            "So here it's one action that.",
            "Leeds dirty word.",
            "Right, and so you can think of it as one step.",
            "Reinforcement learning, so it's simpler than this very hard problem, but still challenging.",
            "Key."
        ],
        [
            "So the second observation is that it's not a multi arm bandit problem.",
            "Multi armed bandit problem is exactly like the problem we looked at, except that there is no axe.",
            "There is no side information.",
            "So essentially, you're computing was the set of constant actions.",
            "And this is just the week in practice because you're competing with decision policies that ignore.",
            "The available information when making decisions.",
            "This is just not good enough.",
            "If you actually want to solve a real problem.",
            "So not only exploration is required, but you also need to generalize over access in order to succeed."
        ],
        [
            "OK so I will talk about algorithms for solving these problems in different settings.",
            "1st, I'll talk about the stochastic setting where.",
            "Access and rewards are generated from some distribution and then talk about an adversarial setting where no assumptions are made about how access and rewards are generated.",
            "And then I'll talk about solutions for the using supervised learning algorithms to solve these problems in both online and batch settings where you have exploration data line around that you want to use, but you don't have control over which actions were chosen.",
            "And then John will talk about evaluation and the number of practical extensions.",
            "On the setting."
        ],
        [
            "And.",
            "Conclude.",
            "Was a bunch of interesting observations and discussions.",
            "Game.",
            "So here's the simplest way.",
            "The simplest algorithm that you can think about.",
            "Ann, this abstracts how any supervised learning algorithm.",
            "Can be used to approach these problems, but but we'll see that it's not a great solution.",
            "OK so.",
            "Let's say that you are in some time step T and you have accumulated data of this form.",
            "So you have an axe.",
            "An action taken and the reward for that action, and now you want to know what to do in the next time stop.",
            "So you can for each policy in \u03c0.",
            "You can look at the examples where the policy agrees with the action taken and you can sum the rewards over those rounds and use it as some estimate of the value of the policy and then go with the best policy.",
            "In the set.",
            "So far.",
            "Right, so you choose the policy maximizing the sum of rewards over the previous rounds where the policy and agreed with the action taken because that's the data you can use for evaluating that policy.",
            "It predicts the same action.",
            "Right, if the policy predicts the same action, then you can use the reward as some indicator of how good that policy is, because if it doesn't agree with the action, then you don't know what to do with that example.",
            "You don't know how to use that example.",
            "To evaluate your policy.",
            "Right, if your policy predicts take action one.",
            "And in that example, action Two was taken.",
            "You have no idea of how use that example.",
            "To tell how good your policy is.",
            "Because your feedback is only partial, you only know that word of action too.",
            "And your policy predicts action once.",
            "So how do use that?",
            "Right, so that's.",
            "And if we have.",
            "Solving this problem.",
            "And then you so you choose the best the leader so far based on the examples you have accumulated.",
            "And you observe your axe, and you predict the same way that either predicts on that axe.",
            "So you choose the action predicted by the leader and then you observe the reward and you add that example to your set so that you can.",
            "Do the same on next rounds.",
            "And this is.",
            "Basically, what any supervised learning algorithm tries to do is to find an empirical leader.",
            "On the data you have so far."
        ],
        [
            "But the problem is that even in the stochastic setting, even if access and the rewards are drawn from a distribution, you can do very well.",
            "I'm sorry you can very badly.",
            "Right, so your regret can scale linearly with T. So you can make a mistake almost every time.",
            "And here's."
        ],
        [
            "Simple example showing why this is so.",
            "So example cedrone independently from this distribution D, which has support only on 2 examples with equal probability.",
            "And you have you want to compete only with two policies that predict constant actions.",
            "So you have pie, one predicting some action A1 and Pi 2 predicting some action A2.",
            "We have just two examples.",
            "And this is the.",
            "These are the rewards, and the rewards are fixed.",
            "So if you draw X1, which you do with probability 1/2, then action one has both actions have reward, some small reward, say 0.1.",
            "And if you draw X2.",
            "Then an action two has a large reward of one and action one has no reward.",
            "So with probability 1/2.",
            "X1 is drawn.",
            "And then.",
            "The learner goes was either pie one or Pi 2 probably randomizes because there is no basis for choosing otherwise.",
            "So let's say it chooses by 1.",
            "And observe the word 0.1 and then from this point on, the learner will always choose by one, because the sum of the rewards 0.1 and that will never explore action too.",
            "Because this empirical, some of the words will always be higher for a one.",
            "Right, that will be comparing.",
            "A1 and A2.",
            "The term was sum of rewards will be 0.1 four A1.",
            "Because it has previously been chosen.",
            "And since 8 two was never chosen in the past and it will have.",
            "O as its sum of rewards.",
            "And I will never be chosen.",
            "So the learner will never discover this one and it will not compete.",
            "It will have a. Regrout that scales linearly with the number of steps."
        ],
        [
            "So this is bad.",
            "Um?",
            "Let's modify this simple algorithm.",
            "By choosing randomly by exploring in the first, all rounds will just choose random actions for the first style rounds and observe their rewards.",
            "And then find.",
            "A policy that use the leader on the 1st on the expiration date on the first hour rounds and then just go with that policy for the remaining rounds."
        ],
        [
            "So suppose that again example cedrone independently from a fixed distribution T over.",
            "Ask Ross, you want to the key the reward vectors, so access and rewards are generated from some distribution and then we can show that for this modified.",
            "Algorithm regards skills sublinearly.",
            "A stick to the 2/3.",
            "And here is the dependence on the number of actions and the.",
            "Number of policies that your computer was.",
            "And you can use standard.",
            "Arguments to substitute that by the VC dimension on the set.",
            "Your computer was.",
            "So you can do better than you can have a nontrivial statement saying that you regret that there is some learning going on because you regret this sub linear in T. And the proof is very simple."
        ],
        [
            "So we look by looking at this quantity.",
            "With someone over the examples.",
            "So Axe action taken the worth of that action.",
            "This is the indicator function that tells.",
            "Whether that this one, when the policy agrees with the action taken so it's similar to the quantity we looked at before, except that there is this key.",
            "This case here because you're waiting by the probability of taking that action for and the exploration was uniform.",
            "So your weight and instead of summing the rewards over the rounds where the policy agrees with the action you are someone important way that ask teammates of the reward.",
            "So this is your reward and you divide by the probability of taking that action.",
            "So you divide by one over key, which means multiplying by K. And bubbles see how this is important.",
            "So instead of salmon, the rewards over the rounds where this indicator function is true, you are summon.",
            "The your son in the way that rewards weighted by the probability of taking the action.",
            "And so if you divide that by Tao, this is just an empirical estimate of the reward weighted estimate of the reward and you can use standard deviation bounds like the hufton bound to bound the deviation from the true expectation with respect to this underlying distribution.",
            "So this is the this is.",
            "By how much it can deviate from the true expectations simultaneously for all policy spy?",
            "With high probability with probability 1 minus Delta.",
            "Right, and so that bounce the regret.",
            "Let's say you we can bound the regret on the exploration rounds by Tao by saying that regattas regret can be at most one in every round.",
            "So let's say it is.",
            "For the bound and then plus T minus style, which is certainly upper bounded by T. The remaining rounds, and this is by how much you can deviate from the true expectation.",
            "So this is by how much, how much you pay for going with an empirical leader versus the true leader.",
            "That you selected on the basis of the exploration rounds, and then he just optimize Tau.",
            "You find it out to minimize this upper bound.",
            "Right, so this is your upper bound on the regret.",
            "And you want to find Tau that minimizes this upper bound.",
            "And that finishes the proof.",
            "So you will get.",
            "The dependence and T will be tied to the 2/3.",
            "Sub linear in T. So and so that's the optimal value of Tau that minimizes the bound."
        ],
        [
            "Again, so that assume that you know T in advance, but this is not essential and you can.",
            "You can modify the algorithm to avoid this dependence and 90 by exploring this probability that at every round you explore with probability that equals the division bound and that round.",
            "So you can modify.",
            "You can explore a bit with some probability on every round instead of exploring in the beginning.",
            "And this way you don't have to know the time horizon.",
            "So key trick is to use importance weighted empirical astam it's of the reward.",
            "Right, so this is our estimate.",
            "Of the reward of the action taken by policy Pi on Acceptee and if the policy disagrees with the action, doesn't predict that action, it's 0.",
            "Anifa degrees was the action.",
            "Then it's the observed reward divided by the probability of taking that action and the importance of doing so is that the expectation of this quantity.",
            "This is an unbiased estimate of the actual reward.",
            "Because the probabilities will cancel out.",
            "And you will.",
            "This is an unbiased estimate of the reward.",
            "Right, as long as the probabilities are positive.",
            "You always there's some minimum probability for switch.",
            "Each action is chosen."
        ],
        [
            "This is how you validate any policy, so for any policy you can get an unbiased estimate of the value of that policy.",
            "So for a fix.",
            "And then you can go with the best.",
            "The probability of choosing action A.",
            "How do you?",
            "Because you are choosing actions, you are choosing actions and.",
            "You can form some distribution from which you draw actions.",
            "Connect that gets updated young.",
            "So this is when you have control but will see an algorithm.",
            "So for example in this algorithm.",
            "The probability.",
            "Here.",
            "So the probability was just one."
        ],
        [
            "OK, because you were choosing randomly, but we will see another algorithm that forms.",
            "Distribution over actions in a nontrivial way so that you can compete even in adversarial settings."
        ],
        [
            "If you have batch data where you don't have control over how actions were taken, you assume that you still know the distribution and there are various techniques for estimating this distribution from data.",
            "But we'll talk about this a bit later.",
            "OK.",
            "So this is a critical.",
            "Slide.",
            "It will be this important weighting technique for forming unbiased estimates will be used."
        ],
        [
            "In this more complicated algorithm but before."
        ],
        [
            "Let's ask ourselves the question, can we expect to do better, right?",
            "So what's the best regret that we can hope for?",
            "So linear in T is pretty bad.",
            "Because it means you're not learning, not competing.",
            "We saw that the simple algorithm where you explore uniformly at random gives you something nontrivial, gives you the graph T to the 2/3.",
            "But what's the best you can hope for?",
            "So what's the best dependence and T that you can hope for?",
            "So it turns out that it's not logged.",
            "You cannot helpful lock.",
            "So routine is the best you can hope for.",
            "And it's interesting because it's the same for stochastic settings and for adversarial settings.",
            "So the best you can do in the stochastic setting is the best you can do in the adversarial setting.",
            "So those stochastic assumptions are not essential.",
            "If you measure learn in this way.",
            "So let's see."
        ],
        [
            "The algorithm that gives you the best you can hope for.",
            "It may seem complicated, but it's actually not.",
            "This is the algorithm is called the XP 4.",
            "And it's in the paper by our at all, and so pretty good paper and pretty easy paper to read.",
            "So, but we'll keep a weight on every policy.",
            "Behalf our preference set of policy Spy Capital Pi that we want to compete with and we will keep a weight on every policy and we will start when will initialize all the weights to one.",
            "Now at every time step we observe our features.",
            "And then for every action bubble, determine the probability of choosing that action in this time step.",
            "So this is where we're forming our probability distribution over actions.",
            "That we will use so each action will have some minimum probability.",
            "With which it will get chosen.",
            "So we distribute some.",
            "Chemung probability mass.",
            "Um?",
            "But we'll put it on every action and this is the remaining probability mass after with itself, right?",
            "And this is how the remaining probability mass is distributed over the actions.",
            "You just sum the weights of the policies predicting that action, and you normalize by the total sum of weights.",
            "So to determine the probability of choosing action A, you just sum the weights.",
            "Of all the policies predicting that action on this axe.",
            "And you just normalize by, so it's pretty natural you choose action.",
            "Proportionally too.",
            "The the weight the total weight on policy is choosing that action.",
            "But there is some probability that mask that you put the site and distribute randomly over all the actions.",
            "So that every action there is a minimum probability with which each action is chosen.",
            "And this technicality is needed for the proof to go through.",
            "So you need some minimum probability that.",
            "Every action gets.",
            "OK, and it turns out that the optimal value of this minimum probability is skills with something like one over root Katie.",
            "And there's this lock of the number of policies again.",
            "OK, So what do you form your distribution of our actions and you draw an action from that distribution.",
            "You observe the reward.",
            "And now you update the weights.",
            "So that they can be used in the next round to update the probability over actions.",
            "And you do that by forming an unbiased estimate of the reward.",
            "So if you update the weight of a given policy \u03c0.",
            "If the policy doesn't agree with the action that was actually taken, then you do not update the weight.",
            "The weight remains the same.",
            "If it did agree.",
            "If Pie agrees with the action taken, then you multiply the weight by.",
            "Exponential function of the.",
            "This is the estimate of the reward, right?",
            "So we divide the reward by the probability of taking that action where the probability comes from here.",
            "Great and we multiply by the this.",
            "You can think of it as the learning rate payment.",
            "And the importance of.",
            "Multiplying by the exponential list that you can sum when you multiply the exponentials, you get some right in the exponent over the rounds when you.",
            "Look at the sum of.",
            "You can look at the.",
            "Some of these quantities over the rounds.",
            "When you multiply.",
            "So this algorithm.",
            "Well, about how access and ours are generated.",
            "So they can be adversarially chosen.",
            "And you can still compete in a very strong sense.",
            "See, a big rat will be will scale as square root of T where T is your time horizon and you don't need to note here.",
            "So this is a very good algorithm.",
            "Not sound."
        ],
        [
            "Right, so that."
        ],
        [
            "The theorem associated with it.",
            "And there is a matching lower bound."
        ],
        [
            "So this is a bound.",
            "On the expected regret.",
            "And it can be modified on to get the algorithm can be modified to get a high probability bound.",
            "Which means that the regret will be bounded by.",
            "Something like this with high probability.",
            "Which is important.",
            "In practice, because.",
            "You want something that's her bust.",
            "Right, you won't regret to be bounded with high probability versus in expectation.",
            "So this original algorithm does not give you a high probability bound, but you can modify it.",
            "To get one."
        ],
        [
            "And all the techniques were already present in that paper that I recommend you to read."
        ],
        [
            "So here is a summary so far.",
            "In the supervised and the full feedback vanilla unsupervised setting, you get full feedback.",
            "You know that word of every action.",
            "At training time and they regret skills as root, see Logan.",
            "Using standard sample complexity machinery.",
            "It's usually not face this way because you're talking about sort of the free grad per sample.",
            "But you can sum over, so the standard way of sample complexity bounds give you root login over T. But when you sum over T rounds, that's the to make it comperable to the other bonds.",
            "You get something that scales is routine.",
            "And yes, it's sufficient.",
            "Um?",
            "So this is the algorithm where you explore 1st and then you exploit the leader on the exploration rounds.",
            "It works in the stochastic setting.",
            "With partial feedback.",
            "And very worthy regret.",
            "Scale says T to the 2/3.",
            "Which is worse than routine.",
            "And yes, it is sufficient.",
            "In the number of policies that you're competing with, there is no.",
            "Um?",
            "Dependence.",
            "There is no intrinsic dependence on the number of policies.",
            "Because you can use for example any learning algorithm to choose the best on the exploration rounds.",
            "You can just feed it in into.",
            "An empirical risk minimized in Oracle to give you an approximate.",
            "In approximately best policy and Exp for works in the adversarial setting was partial feedback gives you this optimal regret, but it's not efficient because we had to keep a weight on every policy and we had to update those weights.",
            "So if you're competing was a large set, this is not efficient.",
            "So ideally we would like an algorithm that is like yes before but didn't have to explicitly maintain weights.",
            "On every policy, and this is one of the important problems in this area right now to get something.",
            "That for example, depends on an empirical risk minimizing Oracle."
        ],
        [
            "Came.",
            "So let's see."
        ],
        [
            "Switch gears and see so we so that this is our unbiased estimate of the reward, right?",
            "So you want the argmax over policies in \u03c0 of this estimate, which sums Sam's over the examples where the policy agrees with the action taken of the reward of that action divided by the probability of taking that action, and so how can we?",
            "Compute that quantity for reasonable policy clauses spy.",
            "This is."
        ],
        [
            "The hard problem in general, but we can try to reuse existence.",
            "You provides learning and algorithms to give you practical solutions to these problems.",
            "So if you encounter such a problem in practice, can you reuse existence supervised learning algorithms to give you a solution?"
        ],
        [
            "So here is very simple approach.",
            "You can just treat this partial label problem as a regression problem, right?",
            "So you have your feature vector AX.",
            "You have your action taken.",
            "You have the reward of that action and the probability.",
            "Of choosing the action, so you want to, you just want to regress on the reward given axe and a.",
            "And the importance weight by 1 / P. So it's an important important weighted regression problem.",
            "Right?",
            "Feature vector about states, yes ax yeah.",
            "Selections about actions, yes.",
            "Right so.",
            "Is is P gonna be a function of X?",
            "Yeah.",
            "Function of acts an A. Yeah.",
            "You some?"
        ],
        [
            "Over the policy's.",
            "Here.",
            "So here is the dependence in acts when he formed a puppy."
        ],
        [
            "But this is the simplest solution you can think.",
            "Think of right?",
            "So you just for X given Axon a, you just re grass on the reward.",
            "You try to predict the reward.",
            "And then when you get in, you ax.",
            "You validate your regressor on all the possible actions that you can take and take the action that gives you the best estimate predicted.",
            "Estimate of the reward.",
            "And so you just go with the argmax.",
            "Over the actions of the predicted value.",
            "So this is a very simple way of reducing this problem to unimportance.",
            "Weighted regression problem.",
            "And the."
        ],
        [
            "Can analyze it.",
            "You can have a statement that relates the performance of this of your regressor to your performance.",
            "So if you were regressor, does well on this regression problem and you are using this regressor, then you're guaranteed to do well with respect to you regret you policy, regret.",
            "So you can try to relate the two and the statement that you get depends, so the here's a statement and I'll show you a one slide proof on the next slide.",
            "So the policy regret.",
            "Is bounded by sqrt 2 K. The squared error of the regressor, and this is pretty bad because this quantity is between zero and one.",
            "The.",
            "Yeah.",
            "The graph of the regressor, this squared.",
            "The graph of the regressor.",
            "Right, so if this thing.",
            "If this thing is larger than one that, then it's a vacuous bound.",
            "It doesn't give you anything, and if it's less than one, then square root makes the dependence undesirable.",
            "It's really easy to find.",
            "So you assume that you don't have that many actions and you can just find the argmax.",
            "Complicated.",
            "So you're just some, you're just doing the Max over the actions.",
            "And your budget and your regressor on axe and that action.",
            "I don't understand, maybe the question?",
            "Particular pie.",
            "Policy.",
            "Yeah.",
            "Yeah, so for every regressor possible regressor there is a policy.",
            "That is defined by the argmax.",
            "So here is a simple approach.",
            "The analysis doesn't look very appealing and we will see that it actually shows up in practice, but it's an approach.",
            "It's very intuitive approach.",
            "OK, so."
        ],
        [
            "So why is it so?",
            "Here's the promised one.",
            "Slide was the proof.",
            "So let's say that we have six actions.",
            "Actions are also called arms, because that's how they are cold and multi arm bandit problems.",
            "So you have six actions.",
            "And this is the the blue bar shows the true payoff, the true reward of that action, and everything is conditioned on a given axe.",
            "So conditioned on a given axe.",
            "At this time there is 1 action that has a large.",
            "Pay off large reward action one and all the other actions.",
            "Have some small reward.",
            "And let's to analyze the worst case.",
            "Let's think of the regressors trying to fool us.",
            "So the regressor is trying to make it seem that action two is actually the better action.",
            "So the Regressor wants us to make a mistake to choose their own action.",
            "And let's see how efficiently it can do so.",
            "So the Regressor is a bad guy who doesn't want to pay a lot and squared loss City grad.",
            "So he wants to do well on the regression problem, but he wants us to do badly.",
            "On the frontier on the policy with respect to the policy regret.",
            "And that will define the worst case, because we want a bond that holds in the worst case as well.",
            "So to do that it needs to predict the way the red bars product.",
            "So it needs to make a mistake.",
            "Twice that wanted him, right?",
            "So it predicts perfectly on these guys.",
            "And then it makes it seem as if the reward of action two is actually better than it is.",
            "Slightly better even than action one.",
            "So to do that, it needs to pay twice this amount, and this amount is the difference in the expected rewards of the best action and 2nd action squared divided by two right?",
            "Because it's half that interval squared?",
            "Because that's the loss function that the regressor pays squared loss.",
            "Right and then this is a very did out of key regression estimates because there are key problems, 6 problems and we take the average over the six problems.",
            "So this is actually 1 / 2 K. Because so this is 2 / 4 to squared, so this is 1 / 2 and then we divide by key estimates of this quantity and we notice that this is the policy.",
            "Regret this quantity.",
            "Here is the policy graph.",
            "So we just solve for that.",
            "So this is sour.",
            "Regressors performance and we solve for the policy regret and we got that squared dependence on the previous slide."
        ],
        [
            "So here is another other.",
            "Any questions about the regression approach.",
            "So here is another simple way to reduce this partial feedback problem to standard supervised learning problem.",
            "So instead of treating it as a regression, we can treat it as an important weighted classification problem where given ax, the goal is to predict the action so the action becomes the label the multiclass label.",
            "Write an R / P is the importance of predicting correctly.",
            "So given this partial label sample, we converted into an important swated multiclass classification sample, where the goal is to.",
            "So you're saying that you should predict action A on X, and that's the importance of doing that, and you feed it.",
            "You feed those samples into your favorite importance weighted classification algorithm.",
            "And then he use the.",
            "Classifier that returns says your.",
            "Policy.",
            "Right, so the algorithm, your supervised classification algorithm will give you.",
            "A classifier map, an axe to actions, and that's what you're going to use to make predictions."
        ],
        [
            "And you can analyze it.",
            "And get a statement like this so your policy regret.",
            "Will be bounded by four key times.",
            "The binary regret the regret of the classifier.",
            "Only classification problem.",
            "So there is no square root dependence, but there is a dependence on the number of actions.",
            "So.",
            "Classifier.",
            "By rejecting rejection, sampling importance weights so you have some upper bound importance weight.",
            "Sample otherwise.",
            "I used to play.",
            "Just mentioned.",
            "Over.",
            "Numbers.",
            "And that actually works.",
            "And there is a link on the tutorial webpage.",
            "To the paper that shows how to do that.",
            "Regression approach.",
            "You can use any supervised regression approach.",
            "So you can use the same regret.",
            "You can use the same rejection sampling to get rid of the importances in the regression approach.",
            "Based on whether or not.",
            "If it's greater than the.",
            "So if the random number is greater than the importance weight, then you throw the data away.",
            "Right and then this is make sure that you you adjust the distribution so it's.",
            "It's as if you draw from.",
            "OK, so there's a paper that we worked on with Bianca and Gnocchi along time ago, which kind of describes the math.",
            "For this it it make sure that you don't.",
            "You're learning with fixed the right distribution so that you optimize overall.",
            "What's that?",
            "So here are the important ways.",
            "So this is the word divided by the probability of taking the action.",
            "So you use these as importance weights.",
            "So if it was a rare action, then you boost the reward.",
            "Because you divide by a small number, so you boost the reward the observed.",
            "The worth of rare actions.",
            "And that is as as we saw.",
            "This gives you this unbiased estimate of the reward.",
            "Bubble post a link to this paper that John mentioned.",
            "The algorithm is called.",
            "Goston same as saying.",
            "\u03a0 to the space of multiclass classifiers?",
            "Young.",
            "So if you started with the pie that was just multiclass classifiers, this is sort of the exact XP 4 hours, no, it's.",
            "So it's a different algorithm, it's just.",
            "It gives you a way of transform and a multiclass classifier into policy for your partial label problem.",
            "How to do?",
            "How?",
            "Talking about how to use.",
            "Asian dating right?",
            "So yeah, this is an important point so it doesn't tell you how to choose actions right?",
            "So this is what I talked about previously, right?",
            "You can either explore randomly and then exploit, or you can do X before.",
            "So this tells you how to explore how to choose actions to make optimal decisions and here.",
            "I'm giving your way of using exploration data, so if you have expiration date of this form.",
            "You can apply any supervised learning algorithm to learn a polishing.",
            "That will help you make decisions in the future.",
            "There are sort of orthogonal angles.",
            "Here.",
            "So the first was."
        ],
        [
            "To treat it as a regression problem so you already have data for Max, so feature features action taken, reward the probability of taking that action.",
            "And you can treat that as a regression problem.",
            "Or you can treat that as an important weighted class."
        ],
        [
            "Fication problem.",
            "Or you can do something better using the."
        ],
        [
            "Offset three that I will describe next.",
            "Ann, this is sort of a bug."
        ],
        [
            "Other way of using this exploration data to learn a slightly better policy?"
        ],
        [
            "So you have, let's just say that we have two actions.",
            "For simplicity, there are two actions we're choosing from.",
            "And this is a trick.",
            "That will give you better.",
            "Performance that will improve your performance and bubble Cy.",
            "So the three castu.",
            "So if the reward is larger than 1/2, so you have this example coming from, so this is your exploration example, ax action taken, reward probability of taking that action if the reward is higher than 1/2.",
            "Then you say, yeah, that was a good action to take.",
            "So you're forming an example.",
            "A classification example acts, A.",
            "So you want your classifier to predict a on acts and this is an important wait, right?",
            "So this is the observed reward minus 1/2.",
            "So this is the distance from 1/2 / P. So instead of important swaton by R. Over P important weight by thus offset from 1/2.",
            "By this distance from 1/2.",
            "Right, and if the observed reward was small.",
            "You pretend as if the other action was the better action.",
            "So you assume that the other action was a good action to take.",
            "So you tell your classifier to take that action instead.",
            "The other action, and this is the important weight.",
            "So this is the word that's less than 1/2 here.",
            "And the importance weight.",
            "Similarly as you do here by 1/2 -- R over the probability.",
            "So the important suite that approach was essentially this, except that you used 0 instead of 1/2.",
            "So there was no offset, and here you can offset by any amount.",
            "Between zero and one including zero, which would give you the importance weighted approach.",
            "And 1/2, which is sort of the Max solution.",
            "So this is what you do is to actions and let's see how and why it."
        ],
        [
            "And you learn, you feed that.",
            "You feed this transformed data set into your favorite importance weighted binary classifier, and you learn a classifier and then you choose to use it as your policy.",
            "I'm sorry I cannot hear.",
            "The first yeah sure sure you can.",
            "Yes, your actions are.",
            "You have two actions.",
            "Yeah, but a can be either positive or negative.",
            "Here you're just looking at a single example and is just a variable that denotes your action that can be either positive or negative.",
            "And you just invert the action.",
            "If the observed reward was small.",
            "So if it was a positive, you pretend that the true label is negative.",
            "If you observe an axe.",
            "Was a positive label.",
            "But smaller rewards.",
            "You pretend as if the true label is negative.",
            "You mean where there was a paper at KDD last year?",
            "Actually it's called right?",
            "Oh, how can the word be found?",
            "So this is.",
            "This is this comes from your exploration, so from the environment.",
            "Right, so you take an action and you assume that you got some reward for that action.",
            "So depends on your application in.",
            "In the advertisement example, it can be either click or not as simple as 01.",
            "So we yeah, we do not really address this problem here.",
            "Will you be talking about some experiments we can probe or?"
        ],
        [
            "Right, and then we draw an action from our action chosen distribution.",
            "And then with probability 1 / P, where P is the probability of choosing that action.",
            "And this is the absolute distance from the offset from 1/2.",
            "So if they were this larger than 1/2, then we generate.",
            "Ax, a so this is this rejection sampling that John talked about.",
            "Right, so with probability.",
            "So if the reward is larger than 1/2 we generate.",
            "We use a as our action, otherwise we invert the action.",
            "We use the other action as the true label.",
            "So the problem is used by importance weighted this importance weighted conversion is noisy and the streak the offset and trick can be used to decrease the noise in the induced problem and we will go through several examples to see why it does so.",
            "What time is it?"
        ],
        [
            "So should I go through the examples now OK?",
            "OK, so let's say that we have an example X.",
            "And we have two actions and these are the rewards of these actions.",
            "So the reward of the first action is 1/2, and the reward of the second action is 1.",
            "And let's say that there is some distribution.",
            "Choose and so we call this action left and we call this action right.",
            "Anne.",
            "So what's the probability of and?",
            "There's some distribution over the two actions that we're using, so what's the probability of generating this binary example?",
            "Versus this binary example.",
            "So what's the induced distribution?",
            "For the binary classification problem.",
            "In this case.",
            "So we have our.",
            "We just need to go through this procedure.",
            "To see how the distributions are transformed.",
            "Ray"
        ],
        [
            "So we draw action left with probability P of left.",
            "Right?",
            "Contribute in probability, so it's P of left times.",
            "1 /, P of left, which means that these cancel out.",
            "Times B.",
            "Difference from 1/2 the distance from 1/2, which is 0 because the reward.",
            "Of the left action is 1/2.",
            "Right, so both probability P of left.",
            "We or here.",
            "So it's P of laughed.",
            "Times 1 / P of left.",
            "Times the absolute difference.",
            "Distance from 1/2 which is 0.",
            "So if if we just look at this procedure.",
            "When can an example ax, left be generated?",
            "So we just try to analyze.",
            "The probability of that event happening.",
            "When it can be.",
            "Generated when you draw action left.",
            "Was probability pizza bluffed?",
            "And then you do this?",
            "Then you need to multiply by this probability cancel and P sub laughed.",
            "You still get probability 0.",
            "At the end, because the reward was right at the offset.",
            "So basically examples with label left are filtered out by this procedure.",
            "And you can do a similar access size to see.",
            "To compute the probability of generate an axe, right and you'll see that X, right will be generated with probability 1/2.",
            "And the randomness is over.",
            "The choice of actions.",
            "So you always in this example you always learn to predict, right?",
            "So your noise rate goes to 0 compared to not to use an offset zero as you didn't important with that classification approach.",
            "So if you compared to not using any offset, do you to use an offset 0?",
            "Then you had a noisy problem because you noise rate was 1/3 essentially.",
            "Right, you noise rate was 1/2 / 3 halves.",
            "1/3 and you transformed it into a problem that has no noise.",
            "Because you only generate examples with label, right?",
            "Please ask questions.",
            "So.",
            "Yeah.",
            "You can.",
            "It depends on what supervised algorithm you're using.",
            "You can compose.",
            "With an online algorithm.",
            "Update and online.",
            "This just tells you how to transform one problem into another and then you can compose with any supervised algorithm.",
            "Yeah, so you always learn the product in that in that particular case.",
            "If.",
            "Which is what you want because you want to learn to predict action, right?",
            "Right, so there are other examples that can be analyzed similarly, but we're just so this is just one example, we're just analyzing how this procedure transforms this example into an example for your binary supervised binary classification algorithm.",
            "Yeah.",
            "But this basically shows that you decreased.",
            "The noise compared to using sub Zero compared to?",
            "As.",
            "Wait that multiclass?",
            "Or binary classification problem?",
            "In this case you decreased the noise.",
            "You made it easier for the your supervised learning algorithm to predict well.",
            "So we will go through other examples to get more intuition.",
            "Two more examples, and then we're almost done for this part of the talk."
        ],
        [
            "So let's say that again you have two actions left and right.",
            "And the reward of the first action of left is 0 instead of 1/2 in the previous example, and the reward of right this one.",
            "You can analyze this similarly."
        ],
        [
            "Right, so in this case.",
            "The reward is 0, so you predict the other action you will actually generate the invite when you observe left.",
            "Because the reward was zero.",
            "And you do so with probability 1/2.",
            "Again, the probabilities cancel out.",
            "So it doesn't really matter what distribution was used to choose between left and right.",
            "Because these probabilities cancel out.",
            "And the distance from the offset is 1/2, so with probability 1/2 you generate right on left.",
            "And on the right you also generate right because the reward was high.",
            "And you do that with probability 1/2, so you learn to predict, right?",
            "As you would do.",
            "But you do it with twice the importance.",
            "Making it again easier for.",
            "The learner for the supervised learner and one more."
        ],
        [
            "Example, so let's say you have both rewards are larger than 1/2 larger than the threshold.",
            "So what happens in this case?"
        ],
        [
            "So on left you will predict left with probability 1/4.",
            "And then right, you will predict right?",
            "Because both rewards were high, so the action is the same.",
            "But the proportion of examples is different is modified by the offset.",
            "So here the probability is 1/4.",
            "And here's the probability is 1/2.",
            "So let's look at the noise rates.",
            "If you used offset zero, then the noise rate here would be.",
            "So it's 3/4 / 1 + 3/4.",
            "And you can compute that and it's larger than whatever you're getting here, which is 1/4.",
            "Over 1/4 + 1/2.",
            "So the proportion of examples that you generated as each label will be improved by the offset, making it the last noisier problem.",
            "So."
        ],
        [
            "You can analyze that for two actions to show that policy to grab is actually bounded by your binary graph.",
            "So it's the best.",
            "Statement you can hope for.",
            "So you'll do as well as the binary classifier does on the binary problem.",
            "OK."
        ],
        [
            "OK.",
            "So let's break here and I'll wrap up very quickly after we come back and hand it over to John.",
            "Yeah, I'll just wrap up the offset and tricks so we know what to do is 2 actions.",
            "How do we do?",
            "Noise was more than two actions and we will just build a tree.",
            "A binary tree on the set of actions.",
            "And Bob will have a tournament over the actions.",
            "Internal nodes of the tree will predict between the winner coming from the left and the winner coming from the right.",
            "Right, so this node decides one versus 2.",
            "Three versus 45 versus 6.",
            "This note will decide between the winner of 1 versus 2 versus the winner of three versus 4, so it's like running a single elimination tournament on the set of actions where we will use the same offset intrigue at every node of the tree.",
            "So we had this offset and Drake for two actions, and we will apply it at every node of this tree.",
            "So this is just a way of doing this demo ISM.",
            "For more than two actions in this hierarchical manner.",
            "And then we will just follow in at best time to get our policy.",
            "So we will learn the classifier at every node of the tree.",
            "And we will follow a chain of productions at this time.",
            "So both started from the root.",
            "And then will ask the classifier the root whether to go left or right, and then we will ask this classifier.",
            "If this classifier told us to go left and will just follow a chain of productions to some label, an bubble predict that label.",
            "Right, so we need to train the classifiers.",
            "And then at this time we just follow the predictions of those classifiers starting from the root to some label, and it's very efficient because.",
            "It's just the depth of the three, so there is a trick that another three besides offset and that we need to use during training.",
            "And this is.",
            "Cold filter and."
        ],
        [
            "So I will describe, so let's see what happens.",
            "How do we train on a partial label?",
            "Example acts action, three reward 0.75 and the probability of action of taking actions three is 0.5.",
            "So this is our partial label example.",
            "Partial feedback example.",
            "So let's see what happens.",
            "How do we train this complex?",
            "Classifier three classifier so bubble start that actions three and there is a path from 3 to the root.",
            "We need to train three classifiers on that example.",
            "So we start here.",
            "And we're training a classifier that predicts between 3:00 and 4:00, and we're telling this classifier to predict left on AX because three is to the left.",
            "Right, that's the importance of that prediction using the offset and think it's the offset from 1/2.",
            "So 0.75 -- 1/2 which is 1/4 divided by the probability of taking action 3.",
            "Which is 1/2.",
            "Now we propagate and train this classifier."
        ],
        [
            "Right, but Ray had an axe.",
            ", right?",
            "Because three is to the right.",
            "And.",
            "This is formed the same way.",
            "But we condition on this classifier predicting left correctly.",
            "Because if it doesn't, then this classifier should have no preference for the label.",
            "So classifiers closer to the leaves filter out examples.",
            "Propagate and so with train on a given example conditioned on all the classifiers closer to the leaves predicting correctly on that example, and that's critical.",
            "For having some theoretical guarantees on their robustness.",
            "Of this approach.",
            "So this is the right thing to do is to do this filtering.",
            "And then so it's it's."
        ],
        [
            "Not that complicated.",
            "There are two tricks you using the offset intricate every node to make this binary decision, and then we use the filter intrigue at training time to filter out examples.",
            "That he used for training deeper, closer to the road.",
            "So here you predict left because three is to the left.",
            "Right, that is formed the way this form the same way, and you conditioned on this classifier being correct, and this classifier being correct.",
            "So only conditions on these two classifiers predict them correctly.",
            "You use that example for training the root classifier.",
            "So when you train all the binary classifiers.",
            "Add the notes at the internal nodes, then test and is very easy.",
            "You just start at the root.",
            "So given ax you ask this classifier.",
            "Where do I go and acts?",
            "It tells you left and right and then you just follow the chain.",
            "The chain of productions to some label and you predict you output that label as your decision on acts.",
            "So testing is very easy.",
            "So training has to.",
            "Tricks that are being used."
        ],
        [
            "And then you get the statement.",
            "You can analyze this approach and you can bound the policy regret.",
            "Where the policy just follows the chain of productions as, which as described.",
            "It's bounded by K -- 1.",
            "Work is the number of actions times the binary free grab.",
            "So he."
        ],
        [
            "There's a summary of the second part of my part.",
            "Of the talk.",
            "And so this is.",
            "But talked about approaches for using exploration data.",
            "Of the form ax, action, reward for that action, The probability of taking that action an axe.",
            "We first casted the desert regression problem of just predicting the value of the reward given Axon 8.",
            "And that's the analysis of that approach.",
            "Right, that's it.",
            "Has the square root dependence, which is undesirable.",
            "Then we formally that doesn't important swated classification problem.",
            "Where we wanted to predict the action was important, which was the reward divided by the probability of taking the action.",
            "And that has.",
            "The dependence of four key times the.",
            "Performance on the binary problem.",
            "And then the offset 3.",
            "Got rid of this factor of 4.",
            "So it uses the same importance weighted tree, but decreases the noise in the result in binary problem.",
            "And you can get a better statement and the order of these approaches in practice is roughly captured by the analysis.",
            "So this analysis is indeed relevant.",
            "So offsetting gives you an edge over importance, weighted classification, and both approaches are much better than regression in practice.",
            "On standard benchmark datasets."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Increasing the reward here is another setting, essentially this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same problem, patient comes to a doctor.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there is a lot of associated information.",
                    "label": 0
                },
                {
                    "sent": "Was the patient.",
                    "label": 0
                },
                {
                    "sent": "There is hysteria, symptoms.",
                    "label": 0
                },
                {
                    "sent": "Maybe some test results and the doctor chooses a treatment.",
                    "label": 1
                },
                {
                    "sent": "There is some set of treatments from which the doctor chooses and then the doctor chooses one treatment and the patient responds to that treatment.",
                    "label": 0
                },
                {
                    "sent": "But the doctor doesn't know what would have happened had he chosen a different treatment, right?",
                    "label": 0
                },
                {
                    "sent": "So sort of a one shot.",
                    "label": 1
                },
                {
                    "sent": "Partial information problem and of course the doctor wants a policy for choosing treatments that that is personalized to patients, right?",
                    "label": 0
                },
                {
                    "sent": "He wants to improve the outcome.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's try to formalize abstract and from these two problems.",
                    "label": 0
                },
                {
                    "sent": "It's a repeated game where time goes from one to some time Horizon Capital T. In each timestep.",
                    "label": 0
                },
                {
                    "sent": "The world produces some features.",
                    "label": 1
                },
                {
                    "sent": "Acceptee in some space of features.",
                    "label": 0
                },
                {
                    "sent": "Great, so a patient comes in or user comes.",
                    "label": 0
                },
                {
                    "sent": "And both are encoded as some feature vector.",
                    "label": 0
                },
                {
                    "sent": "And then the learner chooses an action in some space of actions.",
                    "label": 1
                },
                {
                    "sent": "So let's call the actions one through K. And then the world reacts with the reward for the chosen action.",
                    "label": 0
                },
                {
                    "sent": "And let's assume we will assume that all the rewards are between zero and one if there, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if it's bounded, then if the reward is bounded then you can just rescale.",
                    "label": 0
                },
                {
                    "sent": "If the reward is unbounded, it's hard to talk about this type of problems, but.",
                    "label": 0
                },
                {
                    "sent": "In any scenario you can think of.",
                    "label": 0
                },
                {
                    "sent": "The rewards are can be assumed to be bounded, they have better.",
                    "label": 0
                },
                {
                    "sent": "Cancer.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "OK, so the goal is to learn a good policy, a good decision rule for mapping features to actions.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What does it mean to learn?",
                    "label": 0
                },
                {
                    "sent": "We so far we.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And made any stochastic assumptions or any assumptions about the setting.",
                    "label": 0
                },
                {
                    "sent": "It's in hindsight, it's better to think about learning as competing with some reasonably large class of alternative policies, right?",
                    "label": 0
                },
                {
                    "sent": "So you want to do nearly as well as any pruned decision tree, for example, where as well as any linear classifier.",
                    "label": 0
                },
                {
                    "sent": "So you choose some reference set of policies mapping from features to actions, and you want to do you want to compete?",
                    "label": 0
                },
                {
                    "sent": "With this sad.",
                    "label": 0
                },
                {
                    "sent": "Great, you want to do.",
                    "label": 0
                },
                {
                    "sent": "Nearly as well as any policy in the set.",
                    "label": 0
                },
                {
                    "sent": "So let's look at this formula which measures the performance of such an adaptive policy.",
                    "label": 0
                },
                {
                    "sent": "It's called 3 grad, right?",
                    "label": 0
                },
                {
                    "sent": "So at any time horizon T there is some policy in your reference set capital Pi that performed the best or no worse than any other policy in PIE.",
                    "label": 0
                },
                {
                    "sent": "The best policy at this time carries and and you just some video boards of the actions predicted by that policy.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So for any policy by.",
                    "label": 0
                },
                {
                    "sent": "You can sum the rewards over the time steps of the reward of the action predicted by the policy on the tax.",
                    "label": 0
                },
                {
                    "sent": "And then you just look at the reward accumulated by the best policy and pie so that this is the largest reword that any policy and by could have gotten.",
                    "label": 0
                },
                {
                    "sent": "Over tea time steps.",
                    "label": 0
                },
                {
                    "sent": "If something is unclear, please.",
                    "label": 0
                },
                {
                    "sent": "Ask questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course yes.",
                    "label": 0
                },
                {
                    "sent": "So this is the game at every time step there is an axe.",
                    "label": 0
                },
                {
                    "sent": "Where subscript little T where T ranges over the time steps.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Every policy in pie.",
                    "label": 0
                },
                {
                    "sent": "Predict some action on axe and gets somebody word.",
                    "label": 0
                },
                {
                    "sent": "What is so acts can be stochastic.",
                    "label": 0
                },
                {
                    "sent": "It can be adversarially chosen.",
                    "label": 0
                },
                {
                    "sent": "Oh, the word can be stochastic as well, but it's either drawn from some so it can be drawn from a distribution.",
                    "label": 0
                },
                {
                    "sent": "It's OK.",
                    "label": 0
                },
                {
                    "sent": "So then you look at the expectation of this quantity.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you want a high probability bound.",
                    "label": 0
                },
                {
                    "sent": "So we will talk about these types of statements.",
                    "label": 0
                },
                {
                    "sent": "But this is the essential quantity that.",
                    "label": 0
                },
                {
                    "sent": "So we want to do.",
                    "label": 0
                },
                {
                    "sent": "This is the best.",
                    "label": 0
                },
                {
                    "sent": "But you were the largest reward than that.",
                    "label": 0
                },
                {
                    "sent": "Any policy and pie could have gotten for that instantiation's instantiation of access access in ours.",
                    "label": 0
                },
                {
                    "sent": "And this is what we got.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is our total reward.",
                    "label": 0
                },
                {
                    "sent": "Where aseptis the action that we took an axe.",
                    "label": 0
                },
                {
                    "sent": "And this is the word we gotten.",
                    "label": 0
                },
                {
                    "sent": "Time step.",
                    "label": 0
                },
                {
                    "sent": "Little T. And we want to minimize this gap.",
                    "label": 0
                },
                {
                    "sent": "So we want to do.",
                    "label": 0
                },
                {
                    "sent": "We want to compete with the best policy in PIE.",
                    "label": 0
                },
                {
                    "sent": "Start.",
                    "label": 0
                },
                {
                    "sent": "Is everyone comfortable?",
                    "label": 0
                },
                {
                    "sent": "So it that's the same as competing with the sale of the set of all possible mapex right?",
                    "label": 0
                },
                {
                    "sent": "So it's just a particular.",
                    "label": 0
                },
                {
                    "sent": "Choice of capital plan.",
                    "label": 0
                },
                {
                    "sent": "But yeah.",
                    "label": 0
                },
                {
                    "sent": "Like so we're restricting the set of linear predictors and that.",
                    "label": 0
                },
                {
                    "sent": "Particularizes your learning problem.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this setting has a number of other names.",
                    "label": 1
                },
                {
                    "sent": "It's called associative reinforcement learning or one step reinforcement learning, and we'll see why associative bandits for those who are familiar with multi arm bandit problems.",
                    "label": 1
                },
                {
                    "sent": "Learning with partial feedback because the feedback that you get is partial.",
                    "label": 1
                },
                {
                    "sent": "It's you only see the feedback for the action that you took.",
                    "label": 0
                },
                {
                    "sent": "Or abandoned suicide information because you have this vector ax that's helping you to make predictions.",
                    "label": 0
                },
                {
                    "sent": "Make decisions at every time step so you can think of access being your side information.",
                    "label": 0
                },
                {
                    "sent": "OK, so a couple of observation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's about this problem.",
                    "label": 0
                },
                {
                    "sent": "It's not a supervised learning problem, right?",
                    "label": 1
                },
                {
                    "sent": "We don't know the rewards of actions that we didn't take, so this information is missing even the training time.",
                    "label": 0
                },
                {
                    "sent": "Exploration, which means taken possibly suboptimal actions, was the purpose of acquiring information about the problem.",
                    "label": 0
                },
                {
                    "sent": "Rate is required.",
                    "label": 0
                },
                {
                    "sent": "If if you don't know anything about an action you have.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Way of ruling it out essentially, but it is simpler than general reinforcement learning, because here we assume that we know.",
                    "label": 0
                },
                {
                    "sent": "Which action is responsible for each reward?",
                    "label": 1
                },
                {
                    "sent": "So you take an action and you know the reward for that action and reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "It's a sequence of actions that leads to a reward.",
                    "label": 0
                },
                {
                    "sent": "So here it's one action that.",
                    "label": 0
                },
                {
                    "sent": "Leeds dirty word.",
                    "label": 0
                },
                {
                    "sent": "Right, and so you can think of it as one step.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning, so it's simpler than this very hard problem, but still challenging.",
                    "label": 0
                },
                {
                    "sent": "Key.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the second observation is that it's not a multi arm bandit problem.",
                    "label": 0
                },
                {
                    "sent": "Multi armed bandit problem is exactly like the problem we looked at, except that there is no axe.",
                    "label": 0
                },
                {
                    "sent": "There is no side information.",
                    "label": 1
                },
                {
                    "sent": "So essentially, you're computing was the set of constant actions.",
                    "label": 1
                },
                {
                    "sent": "And this is just the week in practice because you're competing with decision policies that ignore.",
                    "label": 0
                },
                {
                    "sent": "The available information when making decisions.",
                    "label": 0
                },
                {
                    "sent": "This is just not good enough.",
                    "label": 0
                },
                {
                    "sent": "If you actually want to solve a real problem.",
                    "label": 0
                },
                {
                    "sent": "So not only exploration is required, but you also need to generalize over access in order to succeed.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I will talk about algorithms for solving these problems in different settings.",
                    "label": 0
                },
                {
                    "sent": "1st, I'll talk about the stochastic setting where.",
                    "label": 0
                },
                {
                    "sent": "Access and rewards are generated from some distribution and then talk about an adversarial setting where no assumptions are made about how access and rewards are generated.",
                    "label": 0
                },
                {
                    "sent": "And then I'll talk about solutions for the using supervised learning algorithms to solve these problems in both online and batch settings where you have exploration data line around that you want to use, but you don't have control over which actions were chosen.",
                    "label": 0
                },
                {
                    "sent": "And then John will talk about evaluation and the number of practical extensions.",
                    "label": 0
                },
                {
                    "sent": "On the setting.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Conclude.",
                    "label": 0
                },
                {
                    "sent": "Was a bunch of interesting observations and discussions.",
                    "label": 0
                },
                {
                    "sent": "Game.",
                    "label": 0
                },
                {
                    "sent": "So here's the simplest way.",
                    "label": 0
                },
                {
                    "sent": "The simplest algorithm that you can think about.",
                    "label": 0
                },
                {
                    "sent": "Ann, this abstracts how any supervised learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Can be used to approach these problems, but but we'll see that it's not a great solution.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Let's say that you are in some time step T and you have accumulated data of this form.",
                    "label": 0
                },
                {
                    "sent": "So you have an axe.",
                    "label": 0
                },
                {
                    "sent": "An action taken and the reward for that action, and now you want to know what to do in the next time stop.",
                    "label": 0
                },
                {
                    "sent": "So you can for each policy in \u03c0.",
                    "label": 0
                },
                {
                    "sent": "You can look at the examples where the policy agrees with the action taken and you can sum the rewards over those rounds and use it as some estimate of the value of the policy and then go with the best policy.",
                    "label": 0
                },
                {
                    "sent": "In the set.",
                    "label": 0
                },
                {
                    "sent": "So far.",
                    "label": 0
                },
                {
                    "sent": "Right, so you choose the policy maximizing the sum of rewards over the previous rounds where the policy and agreed with the action taken because that's the data you can use for evaluating that policy.",
                    "label": 1
                },
                {
                    "sent": "It predicts the same action.",
                    "label": 0
                },
                {
                    "sent": "Right, if the policy predicts the same action, then you can use the reward as some indicator of how good that policy is, because if it doesn't agree with the action, then you don't know what to do with that example.",
                    "label": 0
                },
                {
                    "sent": "You don't know how to use that example.",
                    "label": 0
                },
                {
                    "sent": "To evaluate your policy.",
                    "label": 0
                },
                {
                    "sent": "Right, if your policy predicts take action one.",
                    "label": 0
                },
                {
                    "sent": "And in that example, action Two was taken.",
                    "label": 0
                },
                {
                    "sent": "You have no idea of how use that example.",
                    "label": 0
                },
                {
                    "sent": "To tell how good your policy is.",
                    "label": 0
                },
                {
                    "sent": "Because your feedback is only partial, you only know that word of action too.",
                    "label": 0
                },
                {
                    "sent": "And your policy predicts action once.",
                    "label": 0
                },
                {
                    "sent": "So how do use that?",
                    "label": 0
                },
                {
                    "sent": "Right, so that's.",
                    "label": 0
                },
                {
                    "sent": "And if we have.",
                    "label": 0
                },
                {
                    "sent": "Solving this problem.",
                    "label": 0
                },
                {
                    "sent": "And then you so you choose the best the leader so far based on the examples you have accumulated.",
                    "label": 0
                },
                {
                    "sent": "And you observe your axe, and you predict the same way that either predicts on that axe.",
                    "label": 0
                },
                {
                    "sent": "So you choose the action predicted by the leader and then you observe the reward and you add that example to your set so that you can.",
                    "label": 0
                },
                {
                    "sent": "Do the same on next rounds.",
                    "label": 0
                },
                {
                    "sent": "And this is.",
                    "label": 0
                },
                {
                    "sent": "Basically, what any supervised learning algorithm tries to do is to find an empirical leader.",
                    "label": 0
                },
                {
                    "sent": "On the data you have so far.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the problem is that even in the stochastic setting, even if access and the rewards are drawn from a distribution, you can do very well.",
                    "label": 1
                },
                {
                    "sent": "I'm sorry you can very badly.",
                    "label": 0
                },
                {
                    "sent": "Right, so your regret can scale linearly with T. So you can make a mistake almost every time.",
                    "label": 0
                },
                {
                    "sent": "And here's.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simple example showing why this is so.",
                    "label": 0
                },
                {
                    "sent": "So example cedrone independently from this distribution D, which has support only on 2 examples with equal probability.",
                    "label": 1
                },
                {
                    "sent": "And you have you want to compete only with two policies that predict constant actions.",
                    "label": 0
                },
                {
                    "sent": "So you have pie, one predicting some action A1 and Pi 2 predicting some action A2.",
                    "label": 0
                },
                {
                    "sent": "We have just two examples.",
                    "label": 0
                },
                {
                    "sent": "And this is the.",
                    "label": 0
                },
                {
                    "sent": "These are the rewards, and the rewards are fixed.",
                    "label": 0
                },
                {
                    "sent": "So if you draw X1, which you do with probability 1/2, then action one has both actions have reward, some small reward, say 0.1.",
                    "label": 1
                },
                {
                    "sent": "And if you draw X2.",
                    "label": 0
                },
                {
                    "sent": "Then an action two has a large reward of one and action one has no reward.",
                    "label": 0
                },
                {
                    "sent": "So with probability 1/2.",
                    "label": 0
                },
                {
                    "sent": "X1 is drawn.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "The learner goes was either pie one or Pi 2 probably randomizes because there is no basis for choosing otherwise.",
                    "label": 0
                },
                {
                    "sent": "So let's say it chooses by 1.",
                    "label": 0
                },
                {
                    "sent": "And observe the word 0.1 and then from this point on, the learner will always choose by one, because the sum of the rewards 0.1 and that will never explore action too.",
                    "label": 1
                },
                {
                    "sent": "Because this empirical, some of the words will always be higher for a one.",
                    "label": 0
                },
                {
                    "sent": "Right, that will be comparing.",
                    "label": 0
                },
                {
                    "sent": "A1 and A2.",
                    "label": 0
                },
                {
                    "sent": "The term was sum of rewards will be 0.1 four A1.",
                    "label": 1
                },
                {
                    "sent": "Because it has previously been chosen.",
                    "label": 1
                },
                {
                    "sent": "And since 8 two was never chosen in the past and it will have.",
                    "label": 0
                },
                {
                    "sent": "O as its sum of rewards.",
                    "label": 0
                },
                {
                    "sent": "And I will never be chosen.",
                    "label": 0
                },
                {
                    "sent": "So the learner will never discover this one and it will not compete.",
                    "label": 0
                },
                {
                    "sent": "It will have a. Regrout that scales linearly with the number of steps.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is bad.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Let's modify this simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "By choosing randomly by exploring in the first, all rounds will just choose random actions for the first style rounds and observe their rewards.",
                    "label": 1
                },
                {
                    "sent": "And then find.",
                    "label": 0
                },
                {
                    "sent": "A policy that use the leader on the 1st on the expiration date on the first hour rounds and then just go with that policy for the remaining rounds.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So suppose that again example cedrone independently from a fixed distribution T over.",
                    "label": 1
                },
                {
                    "sent": "Ask Ross, you want to the key the reward vectors, so access and rewards are generated from some distribution and then we can show that for this modified.",
                    "label": 0
                },
                {
                    "sent": "Algorithm regards skills sublinearly.",
                    "label": 0
                },
                {
                    "sent": "A stick to the 2/3.",
                    "label": 0
                },
                {
                    "sent": "And here is the dependence on the number of actions and the.",
                    "label": 0
                },
                {
                    "sent": "Number of policies that your computer was.",
                    "label": 0
                },
                {
                    "sent": "And you can use standard.",
                    "label": 1
                },
                {
                    "sent": "Arguments to substitute that by the VC dimension on the set.",
                    "label": 0
                },
                {
                    "sent": "Your computer was.",
                    "label": 0
                },
                {
                    "sent": "So you can do better than you can have a nontrivial statement saying that you regret that there is some learning going on because you regret this sub linear in T. And the proof is very simple.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we look by looking at this quantity.",
                    "label": 0
                },
                {
                    "sent": "With someone over the examples.",
                    "label": 1
                },
                {
                    "sent": "So Axe action taken the worth of that action.",
                    "label": 0
                },
                {
                    "sent": "This is the indicator function that tells.",
                    "label": 0
                },
                {
                    "sent": "Whether that this one, when the policy agrees with the action taken so it's similar to the quantity we looked at before, except that there is this key.",
                    "label": 0
                },
                {
                    "sent": "This case here because you're waiting by the probability of taking that action for and the exploration was uniform.",
                    "label": 0
                },
                {
                    "sent": "So your weight and instead of summing the rewards over the rounds where the policy agrees with the action you are someone important way that ask teammates of the reward.",
                    "label": 0
                },
                {
                    "sent": "So this is your reward and you divide by the probability of taking that action.",
                    "label": 0
                },
                {
                    "sent": "So you divide by one over key, which means multiplying by K. And bubbles see how this is important.",
                    "label": 0
                },
                {
                    "sent": "So instead of salmon, the rewards over the rounds where this indicator function is true, you are summon.",
                    "label": 0
                },
                {
                    "sent": "The your son in the way that rewards weighted by the probability of taking the action.",
                    "label": 0
                },
                {
                    "sent": "And so if you divide that by Tao, this is just an empirical estimate of the reward weighted estimate of the reward and you can use standard deviation bounds like the hufton bound to bound the deviation from the true expectation with respect to this underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is the this is.",
                    "label": 0
                },
                {
                    "sent": "By how much it can deviate from the true expectations simultaneously for all policy spy?",
                    "label": 1
                },
                {
                    "sent": "With high probability with probability 1 minus Delta.",
                    "label": 1
                },
                {
                    "sent": "Right, and so that bounce the regret.",
                    "label": 0
                },
                {
                    "sent": "Let's say you we can bound the regret on the exploration rounds by Tao by saying that regattas regret can be at most one in every round.",
                    "label": 0
                },
                {
                    "sent": "So let's say it is.",
                    "label": 0
                },
                {
                    "sent": "For the bound and then plus T minus style, which is certainly upper bounded by T. The remaining rounds, and this is by how much you can deviate from the true expectation.",
                    "label": 0
                },
                {
                    "sent": "So this is by how much, how much you pay for going with an empirical leader versus the true leader.",
                    "label": 0
                },
                {
                    "sent": "That you selected on the basis of the exploration rounds, and then he just optimize Tau.",
                    "label": 0
                },
                {
                    "sent": "You find it out to minimize this upper bound.",
                    "label": 1
                },
                {
                    "sent": "Right, so this is your upper bound on the regret.",
                    "label": 0
                },
                {
                    "sent": "And you want to find Tau that minimizes this upper bound.",
                    "label": 0
                },
                {
                    "sent": "And that finishes the proof.",
                    "label": 0
                },
                {
                    "sent": "So you will get.",
                    "label": 0
                },
                {
                    "sent": "The dependence and T will be tied to the 2/3.",
                    "label": 0
                },
                {
                    "sent": "Sub linear in T. So and so that's the optimal value of Tau that minimizes the bound.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, so that assume that you know T in advance, but this is not essential and you can.",
                    "label": 0
                },
                {
                    "sent": "You can modify the algorithm to avoid this dependence and 90 by exploring this probability that at every round you explore with probability that equals the division bound and that round.",
                    "label": 0
                },
                {
                    "sent": "So you can modify.",
                    "label": 0
                },
                {
                    "sent": "You can explore a bit with some probability on every round instead of exploring in the beginning.",
                    "label": 0
                },
                {
                    "sent": "And this way you don't have to know the time horizon.",
                    "label": 0
                },
                {
                    "sent": "So key trick is to use importance weighted empirical astam it's of the reward.",
                    "label": 1
                },
                {
                    "sent": "Right, so this is our estimate.",
                    "label": 0
                },
                {
                    "sent": "Of the reward of the action taken by policy Pi on Acceptee and if the policy disagrees with the action, doesn't predict that action, it's 0.",
                    "label": 0
                },
                {
                    "sent": "Anifa degrees was the action.",
                    "label": 0
                },
                {
                    "sent": "Then it's the observed reward divided by the probability of taking that action and the importance of doing so is that the expectation of this quantity.",
                    "label": 0
                },
                {
                    "sent": "This is an unbiased estimate of the actual reward.",
                    "label": 0
                },
                {
                    "sent": "Because the probabilities will cancel out.",
                    "label": 0
                },
                {
                    "sent": "And you will.",
                    "label": 0
                },
                {
                    "sent": "This is an unbiased estimate of the reward.",
                    "label": 0
                },
                {
                    "sent": "Right, as long as the probabilities are positive.",
                    "label": 0
                },
                {
                    "sent": "You always there's some minimum probability for switch.",
                    "label": 0
                },
                {
                    "sent": "Each action is chosen.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is how you validate any policy, so for any policy you can get an unbiased estimate of the value of that policy.",
                    "label": 0
                },
                {
                    "sent": "So for a fix.",
                    "label": 0
                },
                {
                    "sent": "And then you can go with the best.",
                    "label": 0
                },
                {
                    "sent": "The probability of choosing action A.",
                    "label": 0
                },
                {
                    "sent": "How do you?",
                    "label": 0
                },
                {
                    "sent": "Because you are choosing actions, you are choosing actions and.",
                    "label": 0
                },
                {
                    "sent": "You can form some distribution from which you draw actions.",
                    "label": 0
                },
                {
                    "sent": "Connect that gets updated young.",
                    "label": 0
                },
                {
                    "sent": "So this is when you have control but will see an algorithm.",
                    "label": 0
                },
                {
                    "sent": "So for example in this algorithm.",
                    "label": 0
                },
                {
                    "sent": "The probability.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So the probability was just one.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, because you were choosing randomly, but we will see another algorithm that forms.",
                    "label": 0
                },
                {
                    "sent": "Distribution over actions in a nontrivial way so that you can compete even in adversarial settings.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you have batch data where you don't have control over how actions were taken, you assume that you still know the distribution and there are various techniques for estimating this distribution from data.",
                    "label": 0
                },
                {
                    "sent": "But we'll talk about this a bit later.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is a critical.",
                    "label": 0
                },
                {
                    "sent": "Slide.",
                    "label": 0
                },
                {
                    "sent": "It will be this important weighting technique for forming unbiased estimates will be used.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this more complicated algorithm but before.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's ask ourselves the question, can we expect to do better, right?",
                    "label": 0
                },
                {
                    "sent": "So what's the best regret that we can hope for?",
                    "label": 0
                },
                {
                    "sent": "So linear in T is pretty bad.",
                    "label": 0
                },
                {
                    "sent": "Because it means you're not learning, not competing.",
                    "label": 0
                },
                {
                    "sent": "We saw that the simple algorithm where you explore uniformly at random gives you something nontrivial, gives you the graph T to the 2/3.",
                    "label": 0
                },
                {
                    "sent": "But what's the best you can hope for?",
                    "label": 0
                },
                {
                    "sent": "So what's the best dependence and T that you can hope for?",
                    "label": 0
                },
                {
                    "sent": "So it turns out that it's not logged.",
                    "label": 0
                },
                {
                    "sent": "You cannot helpful lock.",
                    "label": 0
                },
                {
                    "sent": "So routine is the best you can hope for.",
                    "label": 0
                },
                {
                    "sent": "And it's interesting because it's the same for stochastic settings and for adversarial settings.",
                    "label": 0
                },
                {
                    "sent": "So the best you can do in the stochastic setting is the best you can do in the adversarial setting.",
                    "label": 0
                },
                {
                    "sent": "So those stochastic assumptions are not essential.",
                    "label": 0
                },
                {
                    "sent": "If you measure learn in this way.",
                    "label": 0
                },
                {
                    "sent": "So let's see.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The algorithm that gives you the best you can hope for.",
                    "label": 0
                },
                {
                    "sent": "It may seem complicated, but it's actually not.",
                    "label": 0
                },
                {
                    "sent": "This is the algorithm is called the XP 4.",
                    "label": 0
                },
                {
                    "sent": "And it's in the paper by our at all, and so pretty good paper and pretty easy paper to read.",
                    "label": 0
                },
                {
                    "sent": "So, but we'll keep a weight on every policy.",
                    "label": 0
                },
                {
                    "sent": "Behalf our preference set of policy Spy Capital Pi that we want to compete with and we will keep a weight on every policy and we will start when will initialize all the weights to one.",
                    "label": 0
                },
                {
                    "sent": "Now at every time step we observe our features.",
                    "label": 0
                },
                {
                    "sent": "And then for every action bubble, determine the probability of choosing that action in this time step.",
                    "label": 0
                },
                {
                    "sent": "So this is where we're forming our probability distribution over actions.",
                    "label": 0
                },
                {
                    "sent": "That we will use so each action will have some minimum probability.",
                    "label": 0
                },
                {
                    "sent": "With which it will get chosen.",
                    "label": 0
                },
                {
                    "sent": "So we distribute some.",
                    "label": 0
                },
                {
                    "sent": "Chemung probability mass.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But we'll put it on every action and this is the remaining probability mass after with itself, right?",
                    "label": 0
                },
                {
                    "sent": "And this is how the remaining probability mass is distributed over the actions.",
                    "label": 0
                },
                {
                    "sent": "You just sum the weights of the policies predicting that action, and you normalize by the total sum of weights.",
                    "label": 0
                },
                {
                    "sent": "So to determine the probability of choosing action A, you just sum the weights.",
                    "label": 0
                },
                {
                    "sent": "Of all the policies predicting that action on this axe.",
                    "label": 0
                },
                {
                    "sent": "And you just normalize by, so it's pretty natural you choose action.",
                    "label": 0
                },
                {
                    "sent": "Proportionally too.",
                    "label": 0
                },
                {
                    "sent": "The the weight the total weight on policy is choosing that action.",
                    "label": 0
                },
                {
                    "sent": "But there is some probability that mask that you put the site and distribute randomly over all the actions.",
                    "label": 0
                },
                {
                    "sent": "So that every action there is a minimum probability with which each action is chosen.",
                    "label": 0
                },
                {
                    "sent": "And this technicality is needed for the proof to go through.",
                    "label": 0
                },
                {
                    "sent": "So you need some minimum probability that.",
                    "label": 0
                },
                {
                    "sent": "Every action gets.",
                    "label": 0
                },
                {
                    "sent": "OK, and it turns out that the optimal value of this minimum probability is skills with something like one over root Katie.",
                    "label": 0
                },
                {
                    "sent": "And there's this lock of the number of policies again.",
                    "label": 0
                },
                {
                    "sent": "OK, So what do you form your distribution of our actions and you draw an action from that distribution.",
                    "label": 0
                },
                {
                    "sent": "You observe the reward.",
                    "label": 0
                },
                {
                    "sent": "And now you update the weights.",
                    "label": 0
                },
                {
                    "sent": "So that they can be used in the next round to update the probability over actions.",
                    "label": 0
                },
                {
                    "sent": "And you do that by forming an unbiased estimate of the reward.",
                    "label": 0
                },
                {
                    "sent": "So if you update the weight of a given policy \u03c0.",
                    "label": 0
                },
                {
                    "sent": "If the policy doesn't agree with the action that was actually taken, then you do not update the weight.",
                    "label": 0
                },
                {
                    "sent": "The weight remains the same.",
                    "label": 0
                },
                {
                    "sent": "If it did agree.",
                    "label": 0
                },
                {
                    "sent": "If Pie agrees with the action taken, then you multiply the weight by.",
                    "label": 0
                },
                {
                    "sent": "Exponential function of the.",
                    "label": 0
                },
                {
                    "sent": "This is the estimate of the reward, right?",
                    "label": 0
                },
                {
                    "sent": "So we divide the reward by the probability of taking that action where the probability comes from here.",
                    "label": 0
                },
                {
                    "sent": "Great and we multiply by the this.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as the learning rate payment.",
                    "label": 0
                },
                {
                    "sent": "And the importance of.",
                    "label": 0
                },
                {
                    "sent": "Multiplying by the exponential list that you can sum when you multiply the exponentials, you get some right in the exponent over the rounds when you.",
                    "label": 0
                },
                {
                    "sent": "Look at the sum of.",
                    "label": 0
                },
                {
                    "sent": "You can look at the.",
                    "label": 0
                },
                {
                    "sent": "Some of these quantities over the rounds.",
                    "label": 0
                },
                {
                    "sent": "When you multiply.",
                    "label": 0
                },
                {
                    "sent": "So this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Well, about how access and ours are generated.",
                    "label": 0
                },
                {
                    "sent": "So they can be adversarially chosen.",
                    "label": 0
                },
                {
                    "sent": "And you can still compete in a very strong sense.",
                    "label": 0
                },
                {
                    "sent": "See, a big rat will be will scale as square root of T where T is your time horizon and you don't need to note here.",
                    "label": 0
                },
                {
                    "sent": "So this is a very good algorithm.",
                    "label": 0
                },
                {
                    "sent": "Not sound.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so that.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The theorem associated with it.",
                    "label": 0
                },
                {
                    "sent": "And there is a matching lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a bound.",
                    "label": 0
                },
                {
                    "sent": "On the expected regret.",
                    "label": 0
                },
                {
                    "sent": "And it can be modified on to get the algorithm can be modified to get a high probability bound.",
                    "label": 1
                },
                {
                    "sent": "Which means that the regret will be bounded by.",
                    "label": 1
                },
                {
                    "sent": "Something like this with high probability.",
                    "label": 0
                },
                {
                    "sent": "Which is important.",
                    "label": 0
                },
                {
                    "sent": "In practice, because.",
                    "label": 0
                },
                {
                    "sent": "You want something that's her bust.",
                    "label": 0
                },
                {
                    "sent": "Right, you won't regret to be bounded with high probability versus in expectation.",
                    "label": 0
                },
                {
                    "sent": "So this original algorithm does not give you a high probability bound, but you can modify it.",
                    "label": 0
                },
                {
                    "sent": "To get one.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And all the techniques were already present in that paper that I recommend you to read.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a summary so far.",
                    "label": 1
                },
                {
                    "sent": "In the supervised and the full feedback vanilla unsupervised setting, you get full feedback.",
                    "label": 0
                },
                {
                    "sent": "You know that word of every action.",
                    "label": 0
                },
                {
                    "sent": "At training time and they regret skills as root, see Logan.",
                    "label": 0
                },
                {
                    "sent": "Using standard sample complexity machinery.",
                    "label": 0
                },
                {
                    "sent": "It's usually not face this way because you're talking about sort of the free grad per sample.",
                    "label": 0
                },
                {
                    "sent": "But you can sum over, so the standard way of sample complexity bounds give you root login over T. But when you sum over T rounds, that's the to make it comperable to the other bonds.",
                    "label": 0
                },
                {
                    "sent": "You get something that scales is routine.",
                    "label": 0
                },
                {
                    "sent": "And yes, it's sufficient.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this is the algorithm where you explore 1st and then you exploit the leader on the exploration rounds.",
                    "label": 0
                },
                {
                    "sent": "It works in the stochastic setting.",
                    "label": 0
                },
                {
                    "sent": "With partial feedback.",
                    "label": 0
                },
                {
                    "sent": "And very worthy regret.",
                    "label": 0
                },
                {
                    "sent": "Scale says T to the 2/3.",
                    "label": 0
                },
                {
                    "sent": "Which is worse than routine.",
                    "label": 0
                },
                {
                    "sent": "And yes, it is sufficient.",
                    "label": 0
                },
                {
                    "sent": "In the number of policies that you're competing with, there is no.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Dependence.",
                    "label": 0
                },
                {
                    "sent": "There is no intrinsic dependence on the number of policies.",
                    "label": 0
                },
                {
                    "sent": "Because you can use for example any learning algorithm to choose the best on the exploration rounds.",
                    "label": 0
                },
                {
                    "sent": "You can just feed it in into.",
                    "label": 0
                },
                {
                    "sent": "An empirical risk minimized in Oracle to give you an approximate.",
                    "label": 0
                },
                {
                    "sent": "In approximately best policy and Exp for works in the adversarial setting was partial feedback gives you this optimal regret, but it's not efficient because we had to keep a weight on every policy and we had to update those weights.",
                    "label": 0
                },
                {
                    "sent": "So if you're competing was a large set, this is not efficient.",
                    "label": 0
                },
                {
                    "sent": "So ideally we would like an algorithm that is like yes before but didn't have to explicitly maintain weights.",
                    "label": 0
                },
                {
                    "sent": "On every policy, and this is one of the important problems in this area right now to get something.",
                    "label": 0
                },
                {
                    "sent": "That for example, depends on an empirical risk minimizing Oracle.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Came.",
                    "label": 0
                },
                {
                    "sent": "So let's see.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Switch gears and see so we so that this is our unbiased estimate of the reward, right?",
                    "label": 0
                },
                {
                    "sent": "So you want the argmax over policies in \u03c0 of this estimate, which sums Sam's over the examples where the policy agrees with the action taken of the reward of that action divided by the probability of taking that action, and so how can we?",
                    "label": 0
                },
                {
                    "sent": "Compute that quantity for reasonable policy clauses spy.",
                    "label": 1
                },
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The hard problem in general, but we can try to reuse existence.",
                    "label": 1
                },
                {
                    "sent": "You provides learning and algorithms to give you practical solutions to these problems.",
                    "label": 0
                },
                {
                    "sent": "So if you encounter such a problem in practice, can you reuse existence supervised learning algorithms to give you a solution?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is very simple approach.",
                    "label": 0
                },
                {
                    "sent": "You can just treat this partial label problem as a regression problem, right?",
                    "label": 0
                },
                {
                    "sent": "So you have your feature vector AX.",
                    "label": 0
                },
                {
                    "sent": "You have your action taken.",
                    "label": 0
                },
                {
                    "sent": "You have the reward of that action and the probability.",
                    "label": 0
                },
                {
                    "sent": "Of choosing the action, so you want to, you just want to regress on the reward given axe and a.",
                    "label": 0
                },
                {
                    "sent": "And the importance weight by 1 / P. So it's an important important weighted regression problem.",
                    "label": 1
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Feature vector about states, yes ax yeah.",
                    "label": 0
                },
                {
                    "sent": "Selections about actions, yes.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Is is P gonna be a function of X?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Function of acts an A. Yeah.",
                    "label": 0
                },
                {
                    "sent": "You some?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over the policy's.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So here is the dependence in acts when he formed a puppy.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But this is the simplest solution you can think.",
                    "label": 1
                },
                {
                    "sent": "Think of right?",
                    "label": 0
                },
                {
                    "sent": "So you just for X given Axon a, you just re grass on the reward.",
                    "label": 0
                },
                {
                    "sent": "You try to predict the reward.",
                    "label": 1
                },
                {
                    "sent": "And then when you get in, you ax.",
                    "label": 0
                },
                {
                    "sent": "You validate your regressor on all the possible actions that you can take and take the action that gives you the best estimate predicted.",
                    "label": 0
                },
                {
                    "sent": "Estimate of the reward.",
                    "label": 0
                },
                {
                    "sent": "And so you just go with the argmax.",
                    "label": 0
                },
                {
                    "sent": "Over the actions of the predicted value.",
                    "label": 0
                },
                {
                    "sent": "So this is a very simple way of reducing this problem to unimportance.",
                    "label": 0
                },
                {
                    "sent": "Weighted regression problem.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can analyze it.",
                    "label": 0
                },
                {
                    "sent": "You can have a statement that relates the performance of this of your regressor to your performance.",
                    "label": 0
                },
                {
                    "sent": "So if you were regressor, does well on this regression problem and you are using this regressor, then you're guaranteed to do well with respect to you regret you policy, regret.",
                    "label": 0
                },
                {
                    "sent": "So you can try to relate the two and the statement that you get depends, so the here's a statement and I'll show you a one slide proof on the next slide.",
                    "label": 0
                },
                {
                    "sent": "So the policy regret.",
                    "label": 0
                },
                {
                    "sent": "Is bounded by sqrt 2 K. The squared error of the regressor, and this is pretty bad because this quantity is between zero and one.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The graph of the regressor, this squared.",
                    "label": 0
                },
                {
                    "sent": "The graph of the regressor.",
                    "label": 0
                },
                {
                    "sent": "Right, so if this thing.",
                    "label": 0
                },
                {
                    "sent": "If this thing is larger than one that, then it's a vacuous bound.",
                    "label": 0
                },
                {
                    "sent": "It doesn't give you anything, and if it's less than one, then square root makes the dependence undesirable.",
                    "label": 0
                },
                {
                    "sent": "It's really easy to find.",
                    "label": 0
                },
                {
                    "sent": "So you assume that you don't have that many actions and you can just find the argmax.",
                    "label": 0
                },
                {
                    "sent": "Complicated.",
                    "label": 0
                },
                {
                    "sent": "So you're just some, you're just doing the Max over the actions.",
                    "label": 0
                },
                {
                    "sent": "And your budget and your regressor on axe and that action.",
                    "label": 0
                },
                {
                    "sent": "I don't understand, maybe the question?",
                    "label": 0
                },
                {
                    "sent": "Particular pie.",
                    "label": 0
                },
                {
                    "sent": "Policy.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so for every regressor possible regressor there is a policy.",
                    "label": 0
                },
                {
                    "sent": "That is defined by the argmax.",
                    "label": 0
                },
                {
                    "sent": "So here is a simple approach.",
                    "label": 0
                },
                {
                    "sent": "The analysis doesn't look very appealing and we will see that it actually shows up in practice, but it's an approach.",
                    "label": 0
                },
                {
                    "sent": "It's very intuitive approach.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why is it so?",
                    "label": 0
                },
                {
                    "sent": "Here's the promised one.",
                    "label": 0
                },
                {
                    "sent": "Slide was the proof.",
                    "label": 0
                },
                {
                    "sent": "So let's say that we have six actions.",
                    "label": 0
                },
                {
                    "sent": "Actions are also called arms, because that's how they are cold and multi arm bandit problems.",
                    "label": 0
                },
                {
                    "sent": "So you have six actions.",
                    "label": 0
                },
                {
                    "sent": "And this is the the blue bar shows the true payoff, the true reward of that action, and everything is conditioned on a given axe.",
                    "label": 1
                },
                {
                    "sent": "So conditioned on a given axe.",
                    "label": 0
                },
                {
                    "sent": "At this time there is 1 action that has a large.",
                    "label": 0
                },
                {
                    "sent": "Pay off large reward action one and all the other actions.",
                    "label": 0
                },
                {
                    "sent": "Have some small reward.",
                    "label": 1
                },
                {
                    "sent": "And let's to analyze the worst case.",
                    "label": 0
                },
                {
                    "sent": "Let's think of the regressors trying to fool us.",
                    "label": 0
                },
                {
                    "sent": "So the regressor is trying to make it seem that action two is actually the better action.",
                    "label": 0
                },
                {
                    "sent": "So the Regressor wants us to make a mistake to choose their own action.",
                    "label": 0
                },
                {
                    "sent": "And let's see how efficiently it can do so.",
                    "label": 1
                },
                {
                    "sent": "So the Regressor is a bad guy who doesn't want to pay a lot and squared loss City grad.",
                    "label": 0
                },
                {
                    "sent": "So he wants to do well on the regression problem, but he wants us to do badly.",
                    "label": 0
                },
                {
                    "sent": "On the frontier on the policy with respect to the policy regret.",
                    "label": 0
                },
                {
                    "sent": "And that will define the worst case, because we want a bond that holds in the worst case as well.",
                    "label": 0
                },
                {
                    "sent": "So to do that it needs to predict the way the red bars product.",
                    "label": 0
                },
                {
                    "sent": "So it needs to make a mistake.",
                    "label": 0
                },
                {
                    "sent": "Twice that wanted him, right?",
                    "label": 0
                },
                {
                    "sent": "So it predicts perfectly on these guys.",
                    "label": 0
                },
                {
                    "sent": "And then it makes it seem as if the reward of action two is actually better than it is.",
                    "label": 0
                },
                {
                    "sent": "Slightly better even than action one.",
                    "label": 1
                },
                {
                    "sent": "So to do that, it needs to pay twice this amount, and this amount is the difference in the expected rewards of the best action and 2nd action squared divided by two right?",
                    "label": 0
                },
                {
                    "sent": "Because it's half that interval squared?",
                    "label": 0
                },
                {
                    "sent": "Because that's the loss function that the regressor pays squared loss.",
                    "label": 0
                },
                {
                    "sent": "Right and then this is a very did out of key regression estimates because there are key problems, 6 problems and we take the average over the six problems.",
                    "label": 1
                },
                {
                    "sent": "So this is actually 1 / 2 K. Because so this is 2 / 4 to squared, so this is 1 / 2 and then we divide by key estimates of this quantity and we notice that this is the policy.",
                    "label": 0
                },
                {
                    "sent": "Regret this quantity.",
                    "label": 0
                },
                {
                    "sent": "Here is the policy graph.",
                    "label": 0
                },
                {
                    "sent": "So we just solve for that.",
                    "label": 0
                },
                {
                    "sent": "So this is sour.",
                    "label": 0
                },
                {
                    "sent": "Regressors performance and we solve for the policy regret and we got that squared dependence on the previous slide.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is another other.",
                    "label": 0
                },
                {
                    "sent": "Any questions about the regression approach.",
                    "label": 0
                },
                {
                    "sent": "So here is another simple way to reduce this partial feedback problem to standard supervised learning problem.",
                    "label": 0
                },
                {
                    "sent": "So instead of treating it as a regression, we can treat it as an important weighted classification problem where given ax, the goal is to predict the action so the action becomes the label the multiclass label.",
                    "label": 0
                },
                {
                    "sent": "Write an R / P is the importance of predicting correctly.",
                    "label": 1
                },
                {
                    "sent": "So given this partial label sample, we converted into an important swated multiclass classification sample, where the goal is to.",
                    "label": 0
                },
                {
                    "sent": "So you're saying that you should predict action A on X, and that's the importance of doing that, and you feed it.",
                    "label": 0
                },
                {
                    "sent": "You feed those samples into your favorite importance weighted classification algorithm.",
                    "label": 1
                },
                {
                    "sent": "And then he use the.",
                    "label": 0
                },
                {
                    "sent": "Classifier that returns says your.",
                    "label": 0
                },
                {
                    "sent": "Policy.",
                    "label": 1
                },
                {
                    "sent": "Right, so the algorithm, your supervised classification algorithm will give you.",
                    "label": 0
                },
                {
                    "sent": "A classifier map, an axe to actions, and that's what you're going to use to make predictions.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you can analyze it.",
                    "label": 0
                },
                {
                    "sent": "And get a statement like this so your policy regret.",
                    "label": 0
                },
                {
                    "sent": "Will be bounded by four key times.",
                    "label": 0
                },
                {
                    "sent": "The binary regret the regret of the classifier.",
                    "label": 0
                },
                {
                    "sent": "Only classification problem.",
                    "label": 0
                },
                {
                    "sent": "So there is no square root dependence, but there is a dependence on the number of actions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Classifier.",
                    "label": 0
                },
                {
                    "sent": "By rejecting rejection, sampling importance weights so you have some upper bound importance weight.",
                    "label": 0
                },
                {
                    "sent": "Sample otherwise.",
                    "label": 0
                },
                {
                    "sent": "I used to play.",
                    "label": 0
                },
                {
                    "sent": "Just mentioned.",
                    "label": 0
                },
                {
                    "sent": "Over.",
                    "label": 0
                },
                {
                    "sent": "Numbers.",
                    "label": 0
                },
                {
                    "sent": "And that actually works.",
                    "label": 0
                },
                {
                    "sent": "And there is a link on the tutorial webpage.",
                    "label": 0
                },
                {
                    "sent": "To the paper that shows how to do that.",
                    "label": 0
                },
                {
                    "sent": "Regression approach.",
                    "label": 0
                },
                {
                    "sent": "You can use any supervised regression approach.",
                    "label": 0
                },
                {
                    "sent": "So you can use the same regret.",
                    "label": 1
                },
                {
                    "sent": "You can use the same rejection sampling to get rid of the importances in the regression approach.",
                    "label": 0
                },
                {
                    "sent": "Based on whether or not.",
                    "label": 0
                },
                {
                    "sent": "If it's greater than the.",
                    "label": 0
                },
                {
                    "sent": "So if the random number is greater than the importance weight, then you throw the data away.",
                    "label": 0
                },
                {
                    "sent": "Right and then this is make sure that you you adjust the distribution so it's.",
                    "label": 0
                },
                {
                    "sent": "It's as if you draw from.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a paper that we worked on with Bianca and Gnocchi along time ago, which kind of describes the math.",
                    "label": 0
                },
                {
                    "sent": "For this it it make sure that you don't.",
                    "label": 0
                },
                {
                    "sent": "You're learning with fixed the right distribution so that you optimize overall.",
                    "label": 0
                },
                {
                    "sent": "What's that?",
                    "label": 0
                },
                {
                    "sent": "So here are the important ways.",
                    "label": 0
                },
                {
                    "sent": "So this is the word divided by the probability of taking the action.",
                    "label": 0
                },
                {
                    "sent": "So you use these as importance weights.",
                    "label": 0
                },
                {
                    "sent": "So if it was a rare action, then you boost the reward.",
                    "label": 0
                },
                {
                    "sent": "Because you divide by a small number, so you boost the reward the observed.",
                    "label": 0
                },
                {
                    "sent": "The worth of rare actions.",
                    "label": 0
                },
                {
                    "sent": "And that is as as we saw.",
                    "label": 0
                },
                {
                    "sent": "This gives you this unbiased estimate of the reward.",
                    "label": 0
                },
                {
                    "sent": "Bubble post a link to this paper that John mentioned.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is called.",
                    "label": 0
                },
                {
                    "sent": "Goston same as saying.",
                    "label": 0
                },
                {
                    "sent": "\u03a0 to the space of multiclass classifiers?",
                    "label": 0
                },
                {
                    "sent": "Young.",
                    "label": 0
                },
                {
                    "sent": "So if you started with the pie that was just multiclass classifiers, this is sort of the exact XP 4 hours, no, it's.",
                    "label": 0
                },
                {
                    "sent": "So it's a different algorithm, it's just.",
                    "label": 1
                },
                {
                    "sent": "It gives you a way of transform and a multiclass classifier into policy for your partial label problem.",
                    "label": 0
                },
                {
                    "sent": "How to do?",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Talking about how to use.",
                    "label": 0
                },
                {
                    "sent": "Asian dating right?",
                    "label": 0
                },
                {
                    "sent": "So yeah, this is an important point so it doesn't tell you how to choose actions right?",
                    "label": 0
                },
                {
                    "sent": "So this is what I talked about previously, right?",
                    "label": 0
                },
                {
                    "sent": "You can either explore randomly and then exploit, or you can do X before.",
                    "label": 1
                },
                {
                    "sent": "So this tells you how to explore how to choose actions to make optimal decisions and here.",
                    "label": 0
                },
                {
                    "sent": "I'm giving your way of using exploration data, so if you have expiration date of this form.",
                    "label": 1
                },
                {
                    "sent": "You can apply any supervised learning algorithm to learn a polishing.",
                    "label": 0
                },
                {
                    "sent": "That will help you make decisions in the future.",
                    "label": 0
                },
                {
                    "sent": "There are sort of orthogonal angles.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So the first was.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To treat it as a regression problem so you already have data for Max, so feature features action taken, reward the probability of taking that action.",
                    "label": 0
                },
                {
                    "sent": "And you can treat that as a regression problem.",
                    "label": 0
                },
                {
                    "sent": "Or you can treat that as an important weighted class.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fication problem.",
                    "label": 0
                },
                {
                    "sent": "Or you can do something better using the.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Offset three that I will describe next.",
                    "label": 0
                },
                {
                    "sent": "Ann, this is sort of a bug.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other way of using this exploration data to learn a slightly better policy?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you have, let's just say that we have two actions.",
                    "label": 0
                },
                {
                    "sent": "For simplicity, there are two actions we're choosing from.",
                    "label": 0
                },
                {
                    "sent": "And this is a trick.",
                    "label": 0
                },
                {
                    "sent": "That will give you better.",
                    "label": 0
                },
                {
                    "sent": "Performance that will improve your performance and bubble Cy.",
                    "label": 0
                },
                {
                    "sent": "So the three castu.",
                    "label": 0
                },
                {
                    "sent": "So if the reward is larger than 1/2, so you have this example coming from, so this is your exploration example, ax action taken, reward probability of taking that action if the reward is higher than 1/2.",
                    "label": 0
                },
                {
                    "sent": "Then you say, yeah, that was a good action to take.",
                    "label": 0
                },
                {
                    "sent": "So you're forming an example.",
                    "label": 0
                },
                {
                    "sent": "A classification example acts, A.",
                    "label": 0
                },
                {
                    "sent": "So you want your classifier to predict a on acts and this is an important wait, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the observed reward minus 1/2.",
                    "label": 0
                },
                {
                    "sent": "So this is the distance from 1/2 / P. So instead of important swaton by R. Over P important weight by thus offset from 1/2.",
                    "label": 0
                },
                {
                    "sent": "By this distance from 1/2.",
                    "label": 0
                },
                {
                    "sent": "Right, and if the observed reward was small.",
                    "label": 0
                },
                {
                    "sent": "You pretend as if the other action was the better action.",
                    "label": 0
                },
                {
                    "sent": "So you assume that the other action was a good action to take.",
                    "label": 0
                },
                {
                    "sent": "So you tell your classifier to take that action instead.",
                    "label": 0
                },
                {
                    "sent": "The other action, and this is the important weight.",
                    "label": 0
                },
                {
                    "sent": "So this is the word that's less than 1/2 here.",
                    "label": 0
                },
                {
                    "sent": "And the importance weight.",
                    "label": 0
                },
                {
                    "sent": "Similarly as you do here by 1/2 -- R over the probability.",
                    "label": 0
                },
                {
                    "sent": "So the important suite that approach was essentially this, except that you used 0 instead of 1/2.",
                    "label": 0
                },
                {
                    "sent": "So there was no offset, and here you can offset by any amount.",
                    "label": 0
                },
                {
                    "sent": "Between zero and one including zero, which would give you the importance weighted approach.",
                    "label": 0
                },
                {
                    "sent": "And 1/2, which is sort of the Max solution.",
                    "label": 0
                },
                {
                    "sent": "So this is what you do is to actions and let's see how and why it.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you learn, you feed that.",
                    "label": 0
                },
                {
                    "sent": "You feed this transformed data set into your favorite importance weighted binary classifier, and you learn a classifier and then you choose to use it as your policy.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry I cannot hear.",
                    "label": 0
                },
                {
                    "sent": "The first yeah sure sure you can.",
                    "label": 0
                },
                {
                    "sent": "Yes, your actions are.",
                    "label": 0
                },
                {
                    "sent": "You have two actions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but a can be either positive or negative.",
                    "label": 0
                },
                {
                    "sent": "Here you're just looking at a single example and is just a variable that denotes your action that can be either positive or negative.",
                    "label": 0
                },
                {
                    "sent": "And you just invert the action.",
                    "label": 0
                },
                {
                    "sent": "If the observed reward was small.",
                    "label": 0
                },
                {
                    "sent": "So if it was a positive, you pretend that the true label is negative.",
                    "label": 0
                },
                {
                    "sent": "If you observe an axe.",
                    "label": 0
                },
                {
                    "sent": "Was a positive label.",
                    "label": 0
                },
                {
                    "sent": "But smaller rewards.",
                    "label": 0
                },
                {
                    "sent": "You pretend as if the true label is negative.",
                    "label": 0
                },
                {
                    "sent": "You mean where there was a paper at KDD last year?",
                    "label": 0
                },
                {
                    "sent": "Actually it's called right?",
                    "label": 0
                },
                {
                    "sent": "Oh, how can the word be found?",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "This is this comes from your exploration, so from the environment.",
                    "label": 0
                },
                {
                    "sent": "Right, so you take an action and you assume that you got some reward for that action.",
                    "label": 0
                },
                {
                    "sent": "So depends on your application in.",
                    "label": 0
                },
                {
                    "sent": "In the advertisement example, it can be either click or not as simple as 01.",
                    "label": 0
                },
                {
                    "sent": "So we yeah, we do not really address this problem here.",
                    "label": 0
                },
                {
                    "sent": "Will you be talking about some experiments we can probe or?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, and then we draw an action from our action chosen distribution.",
                    "label": 0
                },
                {
                    "sent": "And then with probability 1 / P, where P is the probability of choosing that action.",
                    "label": 1
                },
                {
                    "sent": "And this is the absolute distance from the offset from 1/2.",
                    "label": 0
                },
                {
                    "sent": "So if they were this larger than 1/2, then we generate.",
                    "label": 0
                },
                {
                    "sent": "Ax, a so this is this rejection sampling that John talked about.",
                    "label": 0
                },
                {
                    "sent": "Right, so with probability.",
                    "label": 0
                },
                {
                    "sent": "So if the reward is larger than 1/2 we generate.",
                    "label": 0
                },
                {
                    "sent": "We use a as our action, otherwise we invert the action.",
                    "label": 0
                },
                {
                    "sent": "We use the other action as the true label.",
                    "label": 0
                },
                {
                    "sent": "So the problem is used by importance weighted this importance weighted conversion is noisy and the streak the offset and trick can be used to decrease the noise in the induced problem and we will go through several examples to see why it does so.",
                    "label": 1
                },
                {
                    "sent": "What time is it?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So should I go through the examples now OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say that we have an example X.",
                    "label": 1
                },
                {
                    "sent": "And we have two actions and these are the rewards of these actions.",
                    "label": 0
                },
                {
                    "sent": "So the reward of the first action is 1/2, and the reward of the second action is 1.",
                    "label": 1
                },
                {
                    "sent": "And let's say that there is some distribution.",
                    "label": 1
                },
                {
                    "sent": "Choose and so we call this action left and we call this action right.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 1
                },
                {
                    "sent": "So what's the probability of and?",
                    "label": 1
                },
                {
                    "sent": "There's some distribution over the two actions that we're using, so what's the probability of generating this binary example?",
                    "label": 0
                },
                {
                    "sent": "Versus this binary example.",
                    "label": 0
                },
                {
                    "sent": "So what's the induced distribution?",
                    "label": 0
                },
                {
                    "sent": "For the binary classification problem.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "So we have our.",
                    "label": 0
                },
                {
                    "sent": "We just need to go through this procedure.",
                    "label": 0
                },
                {
                    "sent": "To see how the distributions are transformed.",
                    "label": 0
                },
                {
                    "sent": "Ray",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we draw action left with probability P of left.",
                    "label": 1
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Contribute in probability, so it's P of left times.",
                    "label": 0
                },
                {
                    "sent": "1 /, P of left, which means that these cancel out.",
                    "label": 0
                },
                {
                    "sent": "Times B.",
                    "label": 0
                },
                {
                    "sent": "Difference from 1/2 the distance from 1/2, which is 0 because the reward.",
                    "label": 0
                },
                {
                    "sent": "Of the left action is 1/2.",
                    "label": 0
                },
                {
                    "sent": "Right, so both probability P of left.",
                    "label": 0
                },
                {
                    "sent": "We or here.",
                    "label": 0
                },
                {
                    "sent": "So it's P of laughed.",
                    "label": 1
                },
                {
                    "sent": "Times 1 / P of left.",
                    "label": 0
                },
                {
                    "sent": "Times the absolute difference.",
                    "label": 0
                },
                {
                    "sent": "Distance from 1/2 which is 0.",
                    "label": 0
                },
                {
                    "sent": "So if if we just look at this procedure.",
                    "label": 0
                },
                {
                    "sent": "When can an example ax, left be generated?",
                    "label": 0
                },
                {
                    "sent": "So we just try to analyze.",
                    "label": 0
                },
                {
                    "sent": "The probability of that event happening.",
                    "label": 0
                },
                {
                    "sent": "When it can be.",
                    "label": 1
                },
                {
                    "sent": "Generated when you draw action left.",
                    "label": 0
                },
                {
                    "sent": "Was probability pizza bluffed?",
                    "label": 0
                },
                {
                    "sent": "And then you do this?",
                    "label": 0
                },
                {
                    "sent": "Then you need to multiply by this probability cancel and P sub laughed.",
                    "label": 0
                },
                {
                    "sent": "You still get probability 0.",
                    "label": 0
                },
                {
                    "sent": "At the end, because the reward was right at the offset.",
                    "label": 0
                },
                {
                    "sent": "So basically examples with label left are filtered out by this procedure.",
                    "label": 0
                },
                {
                    "sent": "And you can do a similar access size to see.",
                    "label": 1
                },
                {
                    "sent": "To compute the probability of generate an axe, right and you'll see that X, right will be generated with probability 1/2.",
                    "label": 0
                },
                {
                    "sent": "And the randomness is over.",
                    "label": 1
                },
                {
                    "sent": "The choice of actions.",
                    "label": 0
                },
                {
                    "sent": "So you always in this example you always learn to predict, right?",
                    "label": 0
                },
                {
                    "sent": "So your noise rate goes to 0 compared to not to use an offset zero as you didn't important with that classification approach.",
                    "label": 0
                },
                {
                    "sent": "So if you compared to not using any offset, do you to use an offset 0?",
                    "label": 0
                },
                {
                    "sent": "Then you had a noisy problem because you noise rate was 1/3 essentially.",
                    "label": 0
                },
                {
                    "sent": "Right, you noise rate was 1/2 / 3 halves.",
                    "label": 0
                },
                {
                    "sent": "1/3 and you transformed it into a problem that has no noise.",
                    "label": 0
                },
                {
                    "sent": "Because you only generate examples with label, right?",
                    "label": 0
                },
                {
                    "sent": "Please ask questions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "It depends on what supervised algorithm you're using.",
                    "label": 0
                },
                {
                    "sent": "You can compose.",
                    "label": 0
                },
                {
                    "sent": "With an online algorithm.",
                    "label": 0
                },
                {
                    "sent": "Update and online.",
                    "label": 0
                },
                {
                    "sent": "This just tells you how to transform one problem into another and then you can compose with any supervised algorithm.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you always learn the product in that in that particular case.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "Which is what you want because you want to learn to predict action, right?",
                    "label": 0
                },
                {
                    "sent": "Right, so there are other examples that can be analyzed similarly, but we're just so this is just one example, we're just analyzing how this procedure transforms this example into an example for your binary supervised binary classification algorithm.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "But this basically shows that you decreased.",
                    "label": 0
                },
                {
                    "sent": "The noise compared to using sub Zero compared to?",
                    "label": 0
                },
                {
                    "sent": "As.",
                    "label": 0
                },
                {
                    "sent": "Wait that multiclass?",
                    "label": 0
                },
                {
                    "sent": "Or binary classification problem?",
                    "label": 0
                },
                {
                    "sent": "In this case you decreased the noise.",
                    "label": 0
                },
                {
                    "sent": "You made it easier for the your supervised learning algorithm to predict well.",
                    "label": 0
                },
                {
                    "sent": "So we will go through other examples to get more intuition.",
                    "label": 0
                },
                {
                    "sent": "Two more examples, and then we're almost done for this part of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's say that again you have two actions left and right.",
                    "label": 1
                },
                {
                    "sent": "And the reward of the first action of left is 0 instead of 1/2 in the previous example, and the reward of right this one.",
                    "label": 1
                },
                {
                    "sent": "You can analyze this similarly.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so in this case.",
                    "label": 0
                },
                {
                    "sent": "The reward is 0, so you predict the other action you will actually generate the invite when you observe left.",
                    "label": 0
                },
                {
                    "sent": "Because the reward was zero.",
                    "label": 1
                },
                {
                    "sent": "And you do so with probability 1/2.",
                    "label": 1
                },
                {
                    "sent": "Again, the probabilities cancel out.",
                    "label": 1
                },
                {
                    "sent": "So it doesn't really matter what distribution was used to choose between left and right.",
                    "label": 0
                },
                {
                    "sent": "Because these probabilities cancel out.",
                    "label": 1
                },
                {
                    "sent": "And the distance from the offset is 1/2, so with probability 1/2 you generate right on left.",
                    "label": 0
                },
                {
                    "sent": "And on the right you also generate right because the reward was high.",
                    "label": 1
                },
                {
                    "sent": "And you do that with probability 1/2, so you learn to predict, right?",
                    "label": 1
                },
                {
                    "sent": "As you would do.",
                    "label": 0
                },
                {
                    "sent": "But you do it with twice the importance.",
                    "label": 0
                },
                {
                    "sent": "Making it again easier for.",
                    "label": 0
                },
                {
                    "sent": "The learner for the supervised learner and one more.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example, so let's say you have both rewards are larger than 1/2 larger than the threshold.",
                    "label": 0
                },
                {
                    "sent": "So what happens in this case?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So on left you will predict left with probability 1/4.",
                    "label": 1
                },
                {
                    "sent": "And then right, you will predict right?",
                    "label": 1
                },
                {
                    "sent": "Because both rewards were high, so the action is the same.",
                    "label": 0
                },
                {
                    "sent": "But the proportion of examples is different is modified by the offset.",
                    "label": 0
                },
                {
                    "sent": "So here the probability is 1/4.",
                    "label": 1
                },
                {
                    "sent": "And here's the probability is 1/2.",
                    "label": 0
                },
                {
                    "sent": "So let's look at the noise rates.",
                    "label": 0
                },
                {
                    "sent": "If you used offset zero, then the noise rate here would be.",
                    "label": 1
                },
                {
                    "sent": "So it's 3/4 / 1 + 3/4.",
                    "label": 0
                },
                {
                    "sent": "And you can compute that and it's larger than whatever you're getting here, which is 1/4.",
                    "label": 0
                },
                {
                    "sent": "Over 1/4 + 1/2.",
                    "label": 0
                },
                {
                    "sent": "So the proportion of examples that you generated as each label will be improved by the offset, making it the last noisier problem.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can analyze that for two actions to show that policy to grab is actually bounded by your binary graph.",
                    "label": 0
                },
                {
                    "sent": "So it's the best.",
                    "label": 0
                },
                {
                    "sent": "Statement you can hope for.",
                    "label": 0
                },
                {
                    "sent": "So you'll do as well as the binary classifier does on the binary problem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let's break here and I'll wrap up very quickly after we come back and hand it over to John.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll just wrap up the offset and tricks so we know what to do is 2 actions.",
                    "label": 0
                },
                {
                    "sent": "How do we do?",
                    "label": 0
                },
                {
                    "sent": "Noise was more than two actions and we will just build a tree.",
                    "label": 0
                },
                {
                    "sent": "A binary tree on the set of actions.",
                    "label": 0
                },
                {
                    "sent": "And Bob will have a tournament over the actions.",
                    "label": 0
                },
                {
                    "sent": "Internal nodes of the tree will predict between the winner coming from the left and the winner coming from the right.",
                    "label": 0
                },
                {
                    "sent": "Right, so this node decides one versus 2.",
                    "label": 0
                },
                {
                    "sent": "Three versus 45 versus 6.",
                    "label": 0
                },
                {
                    "sent": "This note will decide between the winner of 1 versus 2 versus the winner of three versus 4, so it's like running a single elimination tournament on the set of actions where we will use the same offset intrigue at every node of the tree.",
                    "label": 1
                },
                {
                    "sent": "So we had this offset and Drake for two actions, and we will apply it at every node of this tree.",
                    "label": 0
                },
                {
                    "sent": "So this is just a way of doing this demo ISM.",
                    "label": 0
                },
                {
                    "sent": "For more than two actions in this hierarchical manner.",
                    "label": 0
                },
                {
                    "sent": "And then we will just follow in at best time to get our policy.",
                    "label": 0
                },
                {
                    "sent": "So we will learn the classifier at every node of the tree.",
                    "label": 1
                },
                {
                    "sent": "And we will follow a chain of productions at this time.",
                    "label": 0
                },
                {
                    "sent": "So both started from the root.",
                    "label": 0
                },
                {
                    "sent": "And then will ask the classifier the root whether to go left or right, and then we will ask this classifier.",
                    "label": 0
                },
                {
                    "sent": "If this classifier told us to go left and will just follow a chain of productions to some label, an bubble predict that label.",
                    "label": 0
                },
                {
                    "sent": "Right, so we need to train the classifiers.",
                    "label": 1
                },
                {
                    "sent": "And then at this time we just follow the predictions of those classifiers starting from the root to some label, and it's very efficient because.",
                    "label": 0
                },
                {
                    "sent": "It's just the depth of the three, so there is a trick that another three besides offset and that we need to use during training.",
                    "label": 0
                },
                {
                    "sent": "And this is.",
                    "label": 0
                },
                {
                    "sent": "Cold filter and.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will describe, so let's see what happens.",
                    "label": 0
                },
                {
                    "sent": "How do we train on a partial label?",
                    "label": 0
                },
                {
                    "sent": "Example acts action, three reward 0.75 and the probability of action of taking actions three is 0.5.",
                    "label": 0
                },
                {
                    "sent": "So this is our partial label example.",
                    "label": 0
                },
                {
                    "sent": "Partial feedback example.",
                    "label": 0
                },
                {
                    "sent": "So let's see what happens.",
                    "label": 0
                },
                {
                    "sent": "How do we train this complex?",
                    "label": 0
                },
                {
                    "sent": "Classifier three classifier so bubble start that actions three and there is a path from 3 to the root.",
                    "label": 0
                },
                {
                    "sent": "We need to train three classifiers on that example.",
                    "label": 0
                },
                {
                    "sent": "So we start here.",
                    "label": 0
                },
                {
                    "sent": "And we're training a classifier that predicts between 3:00 and 4:00, and we're telling this classifier to predict left on AX because three is to the left.",
                    "label": 0
                },
                {
                    "sent": "Right, that's the importance of that prediction using the offset and think it's the offset from 1/2.",
                    "label": 0
                },
                {
                    "sent": "So 0.75 -- 1/2 which is 1/4 divided by the probability of taking action 3.",
                    "label": 0
                },
                {
                    "sent": "Which is 1/2.",
                    "label": 0
                },
                {
                    "sent": "Now we propagate and train this classifier.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, but Ray had an axe.",
                    "label": 0
                },
                {
                    "sent": ", right?",
                    "label": 0
                },
                {
                    "sent": "Because three is to the right.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This is formed the same way.",
                    "label": 0
                },
                {
                    "sent": "But we condition on this classifier predicting left correctly.",
                    "label": 0
                },
                {
                    "sent": "Because if it doesn't, then this classifier should have no preference for the label.",
                    "label": 0
                },
                {
                    "sent": "So classifiers closer to the leaves filter out examples.",
                    "label": 0
                },
                {
                    "sent": "Propagate and so with train on a given example conditioned on all the classifiers closer to the leaves predicting correctly on that example, and that's critical.",
                    "label": 0
                },
                {
                    "sent": "For having some theoretical guarantees on their robustness.",
                    "label": 0
                },
                {
                    "sent": "Of this approach.",
                    "label": 0
                },
                {
                    "sent": "So this is the right thing to do is to do this filtering.",
                    "label": 0
                },
                {
                    "sent": "And then so it's it's.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not that complicated.",
                    "label": 0
                },
                {
                    "sent": "There are two tricks you using the offset intricate every node to make this binary decision, and then we use the filter intrigue at training time to filter out examples.",
                    "label": 0
                },
                {
                    "sent": "That he used for training deeper, closer to the road.",
                    "label": 0
                },
                {
                    "sent": "So here you predict left because three is to the left.",
                    "label": 0
                },
                {
                    "sent": "Right, that is formed the way this form the same way, and you conditioned on this classifier being correct, and this classifier being correct.",
                    "label": 0
                },
                {
                    "sent": "So only conditions on these two classifiers predict them correctly.",
                    "label": 0
                },
                {
                    "sent": "You use that example for training the root classifier.",
                    "label": 0
                },
                {
                    "sent": "So when you train all the binary classifiers.",
                    "label": 0
                },
                {
                    "sent": "Add the notes at the internal nodes, then test and is very easy.",
                    "label": 0
                },
                {
                    "sent": "You just start at the root.",
                    "label": 0
                },
                {
                    "sent": "So given ax you ask this classifier.",
                    "label": 0
                },
                {
                    "sent": "Where do I go and acts?",
                    "label": 0
                },
                {
                    "sent": "It tells you left and right and then you just follow the chain.",
                    "label": 0
                },
                {
                    "sent": "The chain of productions to some label and you predict you output that label as your decision on acts.",
                    "label": 0
                },
                {
                    "sent": "So testing is very easy.",
                    "label": 0
                },
                {
                    "sent": "So training has to.",
                    "label": 0
                },
                {
                    "sent": "Tricks that are being used.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you get the statement.",
                    "label": 0
                },
                {
                    "sent": "You can analyze this approach and you can bound the policy regret.",
                    "label": 0
                },
                {
                    "sent": "Where the policy just follows the chain of productions as, which as described.",
                    "label": 0
                },
                {
                    "sent": "It's bounded by K -- 1.",
                    "label": 0
                },
                {
                    "sent": "Work is the number of actions times the binary free grab.",
                    "label": 0
                },
                {
                    "sent": "So he.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a summary of the second part of my part.",
                    "label": 0
                },
                {
                    "sent": "Of the talk.",
                    "label": 0
                },
                {
                    "sent": "And so this is.",
                    "label": 0
                },
                {
                    "sent": "But talked about approaches for using exploration data.",
                    "label": 0
                },
                {
                    "sent": "Of the form ax, action, reward for that action, The probability of taking that action an axe.",
                    "label": 0
                },
                {
                    "sent": "We first casted the desert regression problem of just predicting the value of the reward given Axon 8.",
                    "label": 0
                },
                {
                    "sent": "And that's the analysis of that approach.",
                    "label": 0
                },
                {
                    "sent": "Right, that's it.",
                    "label": 0
                },
                {
                    "sent": "Has the square root dependence, which is undesirable.",
                    "label": 0
                },
                {
                    "sent": "Then we formally that doesn't important swated classification problem.",
                    "label": 0
                },
                {
                    "sent": "Where we wanted to predict the action was important, which was the reward divided by the probability of taking the action.",
                    "label": 0
                },
                {
                    "sent": "And that has.",
                    "label": 0
                },
                {
                    "sent": "The dependence of four key times the.",
                    "label": 0
                },
                {
                    "sent": "Performance on the binary problem.",
                    "label": 0
                },
                {
                    "sent": "And then the offset 3.",
                    "label": 0
                },
                {
                    "sent": "Got rid of this factor of 4.",
                    "label": 0
                },
                {
                    "sent": "So it uses the same importance weighted tree, but decreases the noise in the result in binary problem.",
                    "label": 0
                },
                {
                    "sent": "And you can get a better statement and the order of these approaches in practice is roughly captured by the analysis.",
                    "label": 0
                },
                {
                    "sent": "So this analysis is indeed relevant.",
                    "label": 0
                },
                {
                    "sent": "So offsetting gives you an edge over importance, weighted classification, and both approaches are much better than regression in practice.",
                    "label": 0
                },
                {
                    "sent": "On standard benchmark datasets.",
                    "label": 0
                }
            ]
        }
    }
}