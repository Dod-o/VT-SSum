{
    "id": "lowh5f5feqaattbvkjn4mb7if3hqqqdn",
    "title": "Markov Logic: A Unifying Language for Information and Knowledge Management",
    "info": {
        "author": [
            "Pedro Domingos, Dept. of Computer Science & Engineering, University of Washington"
        ],
        "published": "Nov. 19, 2008",
        "recorded": "October 2008",
        "category": [
            "Top->Business->Management->Knowledge Management"
        ]
    },
    "url": "http://videolectures.net/cikm08_domingos_mlmaul/",
    "segmentation": [
        [
            "Can you hear me alright?",
            "Thank you all for being here and thanks to Alec for bringing me here and I'll try to make this interesting for you so I'm going to talk about Markov logic, a unifying language for information and knowledge management.",
            "This is work that I've done at the University of Washington with Stanley Kok Daniel out.",
            "Orfeum met Richardson processing with Mark Sumner and."
        ],
        [
            "And Julie Wink, here's a brief outline of my talk.",
            "I will begin with a little bit of motivation and then some necessary background, and then I'll get into the heart of things.",
            "Which is this new language called Markov logic?",
            "I will talk a little about inference and learning algorithms for Markov logic.",
            "In this software, open source software that they're implemented in, and then I will talk about some of the things that we can do with Markov logic and conclude with a little bit of discuss."
        ],
        [
            "So here's the motivation, and here's a cartoon you know, very, very oversimplified cartoon of the state of information and knowledge management about 20 years ago.",
            "You can think of information as lying on this axis that goes between structures on the one side and then structured on the other, and 20 years ago.",
            "What happened was that there were these things that were definitely very much on the structured side like databases and knowledge bases and the languages that people used to deal with them like SQL data log and 1st order logic and its many variants.",
            "This was the structured end of the spectrum and then there was the unstructured into the spectrum where you just had three texts.",
            "And of course, there was information retrieval that dealt with free text and there was an LP that dealt with free texts, and this is where things stood OK Now."
        ],
        [
            "Fast forward to today and look what's happened.",
            "It's really amazing, right?",
            "This spectrum between structured and unstructured has become completely populated.",
            "The combinations of structured and unstructured and the degrees of structure you know, have have exploded.",
            "There's of course hypertext, and you know HTML, but then there's also things somewhat more structure than hypertext, like semi structured information.",
            "For example XML.",
            "There's the deep web right, which is kind of like structured masquerading as unstructured.",
            "There's a semantic web you know with things like RDF and Owl web services, so the SDL.",
            "And then there's things like information extraction that go from structure to unstructured, and there's even things like ubiquitous computing and sensor data that produced all this data that is kind of much noisier than people were used to, and on the one hand this is very exciting.",
            "But there's all these things that we can do today that we couldn't do before, but on the other hand, you know this is a nightmare, right?",
            "If you're, you know, a poor software engineer, or, you know, applications programmer out there in the real world, you know there's ever more things that you need to learn, ever.",
            "More things that you need to understand and you know the question is, can we make this a little bit better?",
            "And what's happened?",
            "As you know, this spectrum got populated.",
            "Is that not surprisingly, people from both ends try to and are still trying, right?",
            "This is a lot of what's going on today, including at this conference.",
            "They tried to extend their technology towards the middle.",
            "And towards the other side, right?",
            "So people you know in databases, you know if you go to database conferences, a lot of the work that appears there is really trying to deal with data that is less structured than a standard database like XML for example, and so forth.",
            "Likewise, on the unstructured side, right?",
            "We have information retrieval that uses things like the links in the HTML, of course, and so forth.",
            "But the question is, can we do better than just this plethora of approaches?",
            "And instead of you know, can we somehow actually meet in the middle here and come up with a language and you know a way of doing things that actually accommodates the full spectrum and makes life easy for everybody again?",
            "And first of all, I would argue that we have to try to do that.",
            "And second of all what I'm going to talk about here is a proposal in that direction, Markov logic and we will see that you know, even though it's a fairly young idea, it really has some notable successes to its credit.",
            "But of course there's much more to do.",
            "And hopefully people here will get interested in, you know, developing some of these things and and using them and so forth.",
            "OK?",
            "And by the way, I was looking at the program conference and I was happy to notice that the topic of almost every tutorial is directly related to something that you're going to see here, right?",
            "Which makes me believe that indeed you know the time has come."
        ],
        [
            "To try to do something like this.",
            "So to summarize, what we need is languages can both handle structured and unstructured information and any variation or combination of them.",
            "But now of course as before with SQL, then you know TF IDF and whatnot, it's not enough to have a nice language, we need efficient algorithms for doing inference in that language, including all the tasks that people have in the past done with IR NLP or knowledge bases or databases.",
            "And you know for going between them also.",
            "And you know to maybe do something in one.",
            "Can you know?",
            "Go to the next one you know, do something there and come back.",
            "And also, of course we need machine learning algorithms that are efficient because you know, it's pretty clear these days that the way you can actually leverage a lot of this state and be able to handle it has to go through machine learning.",
            "OK, so in essence what I'm going to do here is propose an example of this."
        ],
        [
            "And the example is this language called Markov logic.",
            "Here's the one slide summary of what Markov logic is, right?",
            "So if you fall asleep after this, you still haven't.",
            "You know you still got the main idea to talk.",
            "Markov logic is a language that unifies 1st order logic and probabilistic graphical models, which I believe is a good way to try to achieve these goals because first order logic handles structured information right.",
            "Knowledge bases are first order logic and things like data logger.",
            "Really special cases of 1st order logic probability handles the unstructured information, the noise, the uncertainty inherent in text, and then you know semi structured sources.",
            "And it doesn't just glue them together, it really, truly I hope I will proceed with that.",
            "Markov logic really does unify the two.",
            "There is no separation between the logic and the probability, and this structure.",
            "Then the instructor.",
            "Then in Markov logic anymore.",
            "So if you get into this frame of mind that you don't have to worry about, you know well this structured here, and there's in structured over there anymore.",
            "You can just think think of it all in a in a uniform way.",
            "It does of course build a lot on previous work.",
            "I'm going to not really going to that a lot in the interest of time, but I would just wanted to mention that here.",
            "It builds on ideas that go as far back as the early night.",
            "Isn't something called knowledge based model construction where he was taken knowledge base and and you know extract Bayesian networks from it.",
            "Things like probabilistic relational models and so forth.",
            "But compared to the previous work, I think there's an important difference, which is that Markov logic released the first practical language as opposed to just a theoretical proposal that actually comes with real efficient learning and inference algorithms in the complete open source implementation of them that you can actually use today.",
            "OK, so as far as I know, at this point mark of logic is really the only language that does this, but you know, I expect that more will will appear in the."
        ],
        [
            "Future OK so.",
            "Here's the one slide summary of Markov logic and what I'm going to talk about today.",
            "The syntax of Markov logic, right?",
            "So what is this language?",
            "Markov logic syntax is very, very simple.",
            "The syntax is just formulas in first order logic.",
            "No difference from before, but one addition we're going to have weights for the formulas.",
            "That's the syntax.",
            "The semantics, as we'll see, is that we're going to interpret these formulas as templates for constructing graphical models, in particular for the features of Markov networks.",
            "So Markov logic sets up a big probabilistic graphical model.",
            "A Markov, you know, knowledge base in Markov logic, and then we're going to see some inference algorithms, which not surprisingly combine ideas from logic and probability, including things like satisfiability testing, Markov chain, Monte Carlo, Knowledge, Base model construction, and so forth.",
            "And similarly, learning is going to combine ideas from the logic and from the statistical side, things like the voted perceptrons pseudo likelihood, inductive logic program programming and so on.",
            "And as I said, this is all available in the alchemy software package.",
            "That's what it's called, and I'll put up the URL at the end and it's been used already for a wide array of information and knowledge management applications like information extraction, web mining, social networks, ontology refinement, personal assistance, etc.",
            "The best paper award that CHM last year was a paper that applied Markov logic to do ontology learning.",
            "So, and you know, we'll see more interesting examples of successes of."
        ],
        [
            "Logic, so let me in the interest of fairness.",
            "Now walk over to this side of the room.",
            "So let's start with a little bit of background, and you know most of you are probably familiar with at least some of this background.",
            "But just to get us all on the same page, I'm just going to very briefly cover."
        ],
        [
            "Things Markov networks and 1st order logic so many of you are probably at least passingly familiar with vision networks.",
            "Markov networks might be slightly less familiar to you, so let me just briefly introduce them here.",
            "A Markov network is an undirected graphical model, so it models the joint distribution of a set of variables.",
            "So, for example, here's four Boolean variable smoking, cancer, asthma and cough, and the Markov network has two parts.",
            "One is the graph and the other one is the parameters.",
            "Again, a lot like a vision network, except that it's undirected.",
            "An arc between two variables means that they are directly dependent on each other and Conversely, if I take out the arcs that are adjacent to an arc that makes it independent of the rest of the network.",
            "OK, so the structure of the network gives you the conditional independence between variables.",
            "Now the parameters are defined as follows.",
            "For every click in the graph, and here there's two clicks, smoking cancer and cancer asthma cough we're going to define a potential function and the potential function is just a real valued function of the state.",
            "So for example, real valued non negative function of the state.",
            "So for example, for the smoking cancer click, there's obviously 4 States and here's the value of the potential function for each one of them and the way you compute the probability of a state is just you go to each click you see what state it's in.",
            "You get the corresponding value of the potential function and you multiply them all OK, and then of course you have to normalize by the sum of this overall states to make sure that it's illegal probability.",
            "OK, now this is all very nice, but there's actually a problem here.",
            "Which is.",
            "This only works in terms of scalability.",
            "If your clicks are small, right?",
            "If I have a large click I have to do a larger number of States and things fall apart.",
            "Fortunately there's something else that we can do which."
        ],
        [
            "Something that's very popular with statisticians, which is to represent the Markov network in the form of what's called a log linear model and the log linear model is just a normalized exponentiated sum of weighted features.",
            "And you can convert from one to the other because the product of potentials if you take a law becomes a sum of things OK, and so in the simplest case I can just have one feature for every possible state of every click, and the weight is the log of the corresponding potential right?",
            "So I can convert from potential function form to log linear form very easily, but of course the the thing that we gain here is that I don't have to have one feature for every possible state.",
            "I could have a very very large click relating many things.",
            "As long as I know that there's a few important features of that click.",
            "I can keep only those OK and save me.",
            "You know, a lot of memory, a lot of learning sample that I need to learn this parameters and so forth.",
            "So for example.",
            "If I define a feature on the smoking cancers click that is 1.",
            "If you don't smoke or have cancer and 0 otherwise.",
            "And give that a weight of 1.5.",
            "I get the same model as before."
        ],
        [
            "So of course, in this case it's not very impressive because I just went from.",
            "You know, having 4 lines here to having one thing, but of course if the clique was large, this would be a huge game."
        ],
        [
            "OK. And then we're going to take we're going to very much take advantage of this in Markov logic, so that's the."
        ],
        [
            "Or with introduction to Markov networks, let me just mention a few points of terminology to do with first order logic.",
            "So in first order, logic will build formulas that say things about the world and they will top out of logical symbols like conjunction, disjunction, quantifiers and so forth and four types of symbols, constants representing objects in the domain like say Anna variables like X, that range over the objects functions like mother of X and predicates like friends, XY that represent relations between objects or properties of objects.",
            "And I'm going to call a grounding of a formula or a predicate.",
            "What you get by replacing all the variables by constants.",
            "So for example, if one of the predicates is friends and two of the constants are in an Bob, then friends Anna Bob would be one such grounding and friends Anna Bob is just a Boolean variable.",
            "It's true if Ann and Bob are friends, and it's false if they're not OK. And I'm going to call a world also known as a model or interpretation and assignment of truth values to all the ground predicates.",
            "Right, so I have a bunch of predicates.",
            "I have a bunch of constants.",
            "I replaced the constants into the predicates in all possible ways.",
            "I get a very large number of Boolean variables and this is the state of the world.",
            "It contains everything you might want to know, and the thing that we're going to be interested in here is probability distributions over these very large Boolean vectors.",
            "OK, so."
        ],
        [
            "Wrong direction, so that's the background.",
            "Let me now introduce Markov."
        ],
        [
            "Magic.",
            "So here's the idea in Markov logic.",
            "This is one way to introduce it.",
            "The problem with logic is that it's very brittle.",
            "Like people found this out back in the days of expert systems in the 80s.",
            "Is the world you know.",
            "Every formula has to be exactly correct and you have and you must have no contradictions between formulas because if you do everything falls apart and this is what makes logic essentially very hard to use in the real world of, you know, messy data and unstructured things and contradictions and contributions from multiple sources and whatnot.",
            "So the idea Markov logic is, well, the formulas don't have to be thought of as hard constraints on the world where if you violate a formula, you become impossible.",
            "We can just think of the formulas as soft constraints, meaning that if the world violates the formula, it becomes less probable but not impossible.",
            "OK, so this is the idea in Markov logic, and now what we're going to do is we're going to give you formula, await their represents how strong of a constraint it is.",
            "So if you really believe in a formula.",
            "Then you give it a high weight and the world was a big penalty for violating it.",
            "If you kind of believe in the formula, then you can give it a low weight, but you know worlds will can still violate that formula and be fairly likely.",
            "And now the probability of a world is just going to be a log linear model of the form that we just saw, so it's going to be proportional to the exponentiated some of the weights that the formula satisfies.",
            "So the more formulas that are world satisfies in the more high weight there are, the more the more like of the world will be.",
            "And as we're going to see, you can do all sorts of wonderful things with just this very simple concept."
        ],
        [
            "So here's a more formal definition.",
            "A Markov logic network or MLN for short is a set of pairs FW where if is a formula in first order logic and W is a real number together with this set of constants representing objects in the domain, it defines a Markov network as follows.",
            "The Markov network is going to have one note for each grounding of each predicate in the MLN, and it's going to have one feature in the log in your model for each grounding of each formula in the MLN.",
            "With the corresponding wait OK, Now that's a bit of a mouthful, so."
        ],
        [
            "Here's an example that will hopefully clarify things.",
            "Social networks are of course an extremely popular thing these days.",
            "And then there's a lot of both.",
            "You know real systems out there dealing with social networks and a lot of research on them.",
            "Here's an example of a real social network from the New York Times.",
            "This is a network of what you might call friends and smokers.",
            "It's people who smoke, whether how much people smoke and who their friends are, and this is from 1971 to 2000.",
            "How this network changed.",
            "An interesting thing about the network changes that first of all, many fewer people smoke now, so there's progress.",
            "But the people who stopped smoking were not chosen at random.",
            "People were much more likely to stop smoking if their friends also stopped smoking OK, and the unfortunately also happens the other way around.",
            "The single most important factor in making a teenager start smoking is whether the majority of her best friends smoke.",
            "This is actually been studied.",
            "This one thing predicts more than everything else you can imagine so."
        ],
        [
            "How about we try to model?",
            "But domain like this in first of all let's do it in natural language.",
            "OK, here are two true statements.",
            "Smoking causes cancer, which is one of the reasons why we want to avoid it.",
            "And France have similar smoking habits, so this is the English statement of the problem.",
            "Now you know CS1."
        ],
        [
            "I want to turn this into logic.",
            "You can write something like for every X smokes of X implies cancer of X, and for every X friends XY implies that smokes X and smokes.",
            "Why are equivalent easy street?",
            "But there's a."
        ],
        [
            "These two statements were."
        ],
        [
            "True.",
            "And these two are false.",
            "Because not everybody who smokes gets cancer and certainly not all pairs of friends have this.",
            "Have the same smoking habits.",
            "OK, So what we really need to do is treat these as statistical regularity's right?",
            "Which is what they are."
        ],
        [
            "And we can do that by turning this first order knowledge base into a Markov logic network by attaching weights to these formulas, and we will see in a little bit how the weights can be learned from data.",
            "You can also set them by hand.",
            "Of course, intuitively, this formula has a higher weight because it's a stronger regularity than this formula.",
            "So now I have my Markov logic network and the question is what is this actually saying about the world, right?",
            "What is the probability distribution that this represents?"
        ],
        [
            "So let's suppose I have a very simple world where there's only two objects, Ann and Bob, right?",
            "'cause that's what's going to fit on a slide.",
            "So the first rule that I said was that I'm going to have a node in the network for every grounding of every predicate.",
            "OK, so."
        ],
        [
            "I'm going to have smokes Anna and cancer.",
            "Anna right smokes and as a Boolean variable.",
            "True defender smokes false if she doesn't.",
            "Same thing for Bob right?",
            "And then same thing for."
        ],
        [
            "Trans XY, so I'm going to have friends.",
            "Anna Bob friends Bobana because a sociologist will tell you friendship is not symmetric, right?",
            "Bob could be much better friend of Anna then she's of Bob for example.",
            "So I need to represent these two and also have this degenerate case of friends.",
            "Anna Annan friends, Bob Bob which maybe has to do their self esteem or something.",
            "OK so these so notice what I have now what I have here now is a bunch of Boolean variables.",
            "And at some level at this point it doesn't matter where they came from.",
            "They just boolean variables and I'm going to build a probability distribution over these over these variables.",
            "And how do I do that?",
            "Well, there's going to be a feature for every grounding of every formula, right?",
            "And feature creates a dependency between the variables that it relates.",
            "So I'm going to draw an arc between all the predicates that share a formula that appear together in some formula.",
            "So, for example, this rule smokes xinput."
        ],
        [
            "Ask Answer X is gonna leave too if the feature canceran smokes and implies cancer in and therefore an arc in the graph here.",
            "OK, the same thing for Bob.",
            "Now this formula had two predicates in it, so it led to a 2A clique.",
            "This formula has three."
        ],
        [
            "So it's going to lead to three way cliques, like smokes, Anna Friends and above, and smokes Bob.",
            "OK, and here's the complete structure of the graph that I've created, so this is."
        ],
        [
            "Graph what's the actual distribution where the probability of a world is going to be?",
            "Again the normalized exponentiated sum over all the formulas of the weight of the Formula Times the number of true groundings of the formula index.",
            "OK, so now what's going to happen?",
            "Is that the more smokers have cancer, the more likely the world is going to be, which is exactly what we want.",
            "We don't want to fall off a Cliff as soon as one smoker doesn't get cancer.",
            "OK, notice also that the MLN doesn't really just represent one probability distribution.",
            "It actually represents a family of distributions because in different worlds with different objects I may get very different MLN.",
            "Some could be very large, some could be very small.",
            "But what they all have in common is that they're all going to be repetitions of the same templates that are set up by the 1st order formulas.",
            "OK, and now something that some of you may be thinking is yes, this is all very nice, but this is not going to be remotely practical because I get a combinatorial explosion of groundings when I you know go to any real domain and we're going to stop dead right there and in the.",
            "A good chunk of what this talk is going to be about is how you can make these things efficient enough to be usable in practice in real domains, and the first one that you can do is something very simple but incredibly useful, which is just have typed variables and constants.",
            "If I have a predicate like works for XY, it only makes sense to replace X by people.",
            "And why by organisations?",
            "Because anything else is a waste of time, so if you do that, that will already greatly cut down on the size of your network and Markov logic, and naturally handled the full range of 1st order logic, including functions, existential quantifiers, infinite and continuous domains.",
            "They're not the most relevant for us here, so I'm going to skip over them, but you know, you can ask me about them and look it up in the papers if you're interested."
        ],
        [
            "So how does?",
            "Markov logic relates to the kinds of statistical models that people use in handling unstructured information.",
            "Today, like the kinds of things that you know.",
            "If you pick a random paper from this conference, there's a good chance you find one of those models in there.",
            "Well, the nice thing is that they are all very direct special cases of Markov logic.",
            "Things like Markov networks, Markov, random fields, vision networks, log in your models, exponential models, Maxim entropy models, gives distributions, Boltzmann machines, logistic regression, hidden Markov models, conditional random fields.",
            "Each of these things are field unto its own.",
            "They're all very direct, special cases of Markov logic and will see a couple of examples of how they are special cases and therefore how you can set them up by writing things in Markov logic the same way that you would write you know statements in SQL or or Datalog or whatever.",
            "Now there's it's also interesting to know what it is that Markov logic adds to these.",
            "These models typically all make the assumption that your data is IID.",
            "Meaning your objects are all independent and identically distributed, meaning the properties of 1 object don't depend on the properties of another, which if you think about it, is a very strange thing, right?",
            "Again, your probability of smoking depends on whether your friend smoke your probability of having the flu is not just a function of your symptoms, it's also a function of you know whether there's a flu epidemic going around and you've been in contact with somebody who has the flu.",
            "This is very easy to this is very difficult to express in these kinds of models, but very easy to express in Markov logic as we just saw.",
            "OK, so this is sort of."
        ],
        [
            "The statistical side, what about the logical side?",
            "Well, it's probably fairly intuitive that infinite weights give you first order logic as a special case of Markov logic.",
            "So if I let all my weights go to Infinity, then you pay an infinite penalty for violating a formula and you retrieve back the first of the case and we have a theorem that says you can answer all entailment quiz in first order logic by computing conditional probabilities on the Markov logic network that you get by taking those formulas and giving them infinite weights.",
            "So this is nice that we have first order logic as a clean special case.",
            "But of course, we're really interested in the case where the weights are finite.",
            "So what can we say there?",
            "Well, if the knowledge base is satisfiable, meaning it's possible to make all the formulas true at the same time, and although it's are positive, which you can always ensure by flipping by negating formulas and their weights.",
            "The satisfying assignments are the most of the distribution.",
            "Which means that even in the finite noisy case, the worlds that first order logic likes, there's still in there.",
            "There the modes of the distribution, and we're going to make very good use of this when it comes time to do inference.",
            "But of course the most important thing about Markov logic is that, unlike in logic, where if you have a contradiction in the in the knowledge base now pigs fly and anything can happen, Markov logic has no problem with contradictions between formulas.",
            "If there's a contradiction, Markov logic just waste the evidence on each side.",
            "And produces a probability for the formula.",
            "So now it's actually easy to build large knowledge bases because you don't have to.",
            "At least in this respect.",
            "Of course, because you don't have to make sure that they're all consistent anymore.",
            "And more importantly, it's easy to do things like the semantic web where you have contributions from many different people, and there's no hope of ever making the more consistent.",
            "But you still want to be able to reason with those contributions, so Markov logic, at least at this level, this becomes pretty straightforward."
        ],
        [
            "OK, so let me talk a little bit about the inference and learning algorithms.",
            "Now of course this would all be only of theoretical interest if we didn't have such efficient inference and learning algorithms.",
            "And of course there's still a long way to go in making them more efficient.",
            "But we've also come a very long way already, and we can definitely handle a lot of rules.",
            "Real world problems with what we have today, so let's."
        ],
        [
            "Get that a little bit.",
            "Let's start by looking at inference.",
            "One of the most frequent and most useful types of inference that people want to do is what's called MAP maximum a posteriori or MPE.",
            "Most probable explanation, inference, and this consists of given some evidence.",
            "Let's call that X.",
            "Finding the most likely state of the world, I eat the most likely values of another set of variables.",
            "Why you can think of this as finding the most probable explanation for what you see here are the symptoms.",
            "What is the most probable disease that explains them?",
            "For those of you who know, for example, about HMM, this is what the Viterbi algorithm is doing.",
            "So what does this mean in Markov logic?",
            "Well, let's replace this peer, why given?"
        ],
        [
            "Text by the expression that we saw before, right?",
            "So we want to find the white that maximizes this.",
            "This is a constant.",
            "This is a monotonically increasing function.",
            "So to maximize this, although you have to Maxim."
        ],
        [
            "This is what's inside here.",
            "OK. What are you?",
            "What are we doing here?",
            "We were trying to find the assignment of values to the Boolean variables that maximizes the sum of the weights of the ground formulas that you have."
        ],
        [
            "Surprise, surprise, this is just the weighted Max sat problem, right?",
            "There's a satisfiability problem which is try to satisfy all formulas.",
            "You know, the quintessential NP complete problem.",
            "The Max set problem is try to satisfy as many as possible the Wave Max.",
            "That problem is when it formula has a weight and you want to maximize the weight of satisfied formulas.",
            "And the reason this is very nice for us is that we don't have to invent anything new to do inference in Markov logic, we can just use an off the shelf where it SAT solver like for example Max Walk set.",
            "And we could take advantage of the last 50 years of intensive research on SAT solvers, right?",
            "These days, SAT solvers can handle problems with millions of variables in minutes or seconds, and we're going to take advantage of that.",
            "And of all the future improvements of the continuing research on this subject and another sort of like very surprising and nice thing that falls out of this, is that we thought going in that if you think about it, first order logic inference is not renowned for being fast, right?",
            "And neither is probabilistic inference.",
            "If you combine the two.",
            "You probably have a really big headache on your hands, right?",
            "The surprising thing is that it turns out that using this scheme we can actually do inference in Markov logic faster than inference in first order logic.",
            "And the reason we can do it faster is that as we'll see shortly, Max works at it's pretty much the exact same algorithm with a very minor change as what you would do is, you know, for satisfiability .1, but .2.",
            "If I took some of the constraints that were previously hard, that should have been soft.",
            "And make them soft.",
            "Now I have an easier problem to solve and therefore I can solve it in fewer in fewer steps.",
            "So actually turns out they often when we make the first knowledge based soft and their influence on that, we actually save time."
        ],
        [
            "Interest I'm in the process, so for those of you not familiar with satisfiability solvers, here's a here's a.",
            "It actually is easy to explain something like walk sat in just one slide.",
            "It's incredible how powerful it is given how simple it is.",
            "So the goal of the walk set is to find an assignment of truth values to Boolean variables.",
            "That makes all the clauses in a knowledge based true at the same time right?",
            "And the clause is just a disjunction of literals, and that can convert ineligible to that form.",
            "So how does it work?",
            "Very simple.",
            "I start out by assigning truth values at random to the variables.",
            "And then I check if all the clauses are satisfied and if they are great, I'm done.",
            "If they're not satisfied, then I go into this loop where I either flip, I pick an unsatisfied clause.",
            "And I find that I picked.",
            "If I flip the variable in it that most increases the number of satisfied clauses, right?",
            "So this is just the greedy search step I'm trying to satisfy more clauses or to avoid local minima.",
            "I can instead take a random step where I just flip a random variable in the closet and I will do this after some maximum number of flips and repeat the whole process up to some maximum number of times.",
            "And once I find a solution, I return it.",
            "If I don't find a solution at the end of this process, I returned failure.",
            "So."
        ],
        [
            "This is works at Max Box at ignore this part here 'cause it's not important is really just the same algorithm, except that instead of flipping the variable that maximizes the number of satisfied formulas, it just maximizes the sum of the weights of the satisfied farmers.",
            "And that's it.",
            "OK, so we can use this for me in."
        ],
        [
            "It's in my car logic.",
            "There is a problem though, which I already alluded to, which is the memory explosion that you get when you try to ground out your network to then apply the SAT solver too, right?",
            "Because if you think about it, if there are N constants in your domain and the hyest clause darity meaning the highest number of distinct variables appearing in a clauses C, The ground networks need to require the order fantasy memory.",
            "OK, so suppose I have 1000 constants, which is not that much and the highest clarity is 3.",
            "Again, not that much.",
            "Already I have a billion ground clauses, so we need to do something about this and we have done something about it.",
            "We can handle things, for example, where every word in the English language is a constant, and the way we do that is by exploiting the sparseness of relational domains.",
            "If you think about it, there's 6 billion people in the world, so there are 36 billion billion possible groundings of that predicate, right?",
            "But the vast majority of them are false, 'cause you know, most people only have a few dozen friends, maybe a few 100.",
            "Maybe thousands at most if you're on Facebook, but that's it.",
            "OK, so very very sparse.",
            "Conversely, because of that, most ground clauses are trivially satisfied because their preconditions don't fire right?",
            "If two people aren't friends, we don't need to worry about whether the smoking habits of running full smoking habits or the other.",
            "So we've developed a lazy extension of walk set called lazy set that actually exploits this sparseness and increases the size of the means that we can scale to buy.",
            "Many, many orders of magnitude and I won't go into the details here, but this is certainly an important."
        ],
        [
            "Part of making this practical?",
            "OK, so the other main inference problem that we usually want to solve is to actually compute the probabilities of some query predicates given some evidence once.",
            "And the most general questions that you can ask of an MLN is what is the probability that some formula holds OK?",
            "What is the probability that some formula holds given it some other formula holds and in principle this question is easy to answer because the probability of a formula is just the sum of the probabilities of the world where the formula is true.",
            "So I can just run over each world, check whether the formula holds, add up the probabilities of the world's the probability of world can of course be computers in the formula and I'm done right?",
            "However, this is of course not remotely feasible, 'cause there's an exponential number of worlds.",
            "What we can do though is use a Monte Carlo scheme of 1 sort or another to sample worlds randomly, and then in each rule that I sample, I check whether the formula holds and the fraction of worlds where the formula holds.",
            "Is my estimate of the probability very well established way to do probabilistic inference?",
            "And if I'm conditioning on another formula, what I can do is, as I sample worlds, the first thing I do is check if the conditional formula conditioning formula holds.",
            "If it doesn't hold, I ignore that world and they only count among the remainder.",
            "And this gives me conditional inference.",
            "OK, now this may still not be very efficient, because again, you know the network could very easily blow up, right?",
            "However, there's something else that we can do if it's the case that the conditioning formula is a conjunction of ground atoms, like a set of facts, which is almost always the case, right?",
            "It's like you know this is what the symptoms are.",
            "This is through the friends of NR.",
            "And now you know is there's any smoke?",
            "Or does Anna have lung cancer?",
            "And so forth.",
            "In that case, what we can do is we can first construct the minimum subset of the network that is necessary to answer the query.",
            "And then we do inference on that network, right?",
            "This is actually what's called knowledge based model construction and what we've done is generalized some earlier ideas from from vision networks.",
            "Another thing that you can do that's very exciting that I won't talk about here is do lifted inference the same way that you do lifted inference by Resolution 1st order logic do inference for whole sets of objects at once.",
            "Potentially even do inference for infinite objects in finite time."
        ],
        [
            "So, but just going back to knowledge base model construction, how do you construct your minimum network?",
            "Very simple idea.",
            "I'm going to construct the network out from the query notes until I hit evidence.",
            "So I start out with an empty network and the queue containing the query nodes, and then I repeatedly go to the front of the queue, take a note.",
            "Add it to the network, check if it's in the evidence.",
            "If it's in the evidence, I can stop.",
            "If it's not in the evidence, what I do is like I get its neighbors and add those to the cube and keep going.",
            "And the hope is that I will bump into a boundary of evidence that separates me from the rest of the world.",
            "Of course, in the worst case, this is going to choose the whole world, but most of the time it's going to be something much, much smaller in the whole world.",
            "Again.",
            "For example, if I'm asking a question about whether Anna smokes or not, I really need to know only about some small number of people that you might have interacted with, not people across the world who she's never seen."
        ],
        [
            "OK.",
            "So then you can do inference by something like Markov chain Monte Carlo.",
            "The most popular algorithm very widely used is something called Gibbs sampling that many of you probably know bout again, joyfully.",
            "Simple algorithm given how popular it is.",
            "So here's what you do.",
            "You start with a random truth assignment and then you repeat the following some number of times or until some convergence criterion is met.",
            "I go to each variable in turn and I sample and you value for that variable condition on its neighbors.",
            "And then I just record which fraction of states the formula that I'm interested in was true, and that's my answer.",
            "OK there."
        ],
        [
            "Simple idea, very powerful, but again there's a problem which is.",
            "This is enough to handle the terministic dependency nondeterministic dependences, but if you add deterministic dependences, which is something you want to do in general, this breaks down.",
            "The reason it's Rick down is that the deterministic dependences break up the state space into regions that don't communicate.",
            "So if I start my Gibbs sampling over here, it never gets over there and I get the wrong answer.",
            "And it's actually a longstanding problem in AI.",
            "We've actually been able to make very good progress on it, using.",
            "Not surprisingly.",
            "A combination of MCMC with ideas from satisfiability testing.",
            "So we developed this algorithm called MC set, where the next sample.",
            "Instead of being proposed the way I just described is generated by a SAT solver, and so I'm able to very efficiently jump between these modes because this is what satisfiability solvers do well.",
            "And of course there's some care that has to go into making sure that you still wind up with the right distribution, and again, I won't go into the details here, but this is orders of magnitude faster than Gibbs sampling, and again is one of the things that actually makes this all feasible impact."
        ],
        [
            "OK, so now let's talk a little bit about learning.",
            "You know where?",
            "Where do these Markov logic networks come from?",
            "The formulas?"
        ],
        [
            "The weights of course, you could just write it all by hand, but most of the time that's probably not what you want to do.",
            "You might have formulas that you or somebody wrote down and you might want to learn weights for them, or in the most general case you might actually want to learn the formulas themselves, or just take formulas that people contributed to you.",
            "Again, think of something like the semantic web that may not be entirely wrong, but need to be refined and maybe you can refine them by looking at data.",
            "OK, so you can think of the data that you're going to learn from as a relational database.",
            "So if you have a relational database, you can apply Markov logic winning to that before you have isn't structured information, and we're going to see an example of that here.",
            "What you can do is you can represent it as a database in the most simple minded where you can think of like.",
            "For example, you can think of free text as just being a sequence of predicates.",
            "Thing that this token appears in this position in the document.",
            "OK, so this is the data.",
            "And here in this talk for simplicity I'm going to make the closed world assumption, which is that every Atom.",
            "Every ground predicate that is not in the database is assumed to be false.",
            "OK, this is a standard assumption in databases.",
            "Often it's not appropriate, and when it's not appropriate, what's going to happen is that we're going to need to have EM versions of the algorithms that I'm going to talk about, and those are available in the software, but here to simplify, I'm going to ignore that.",
            "So two main tasks, as always in machine learning, learning parameters.",
            "In our case, the weights learning structure.",
            "In our case, the formulas and the parameters can be learned generatively and discriminatively, and I'm going to briefly.",
            "Look at each of these in turn."
        ],
        [
            "So how do I learn weights generatively?",
            "Well learning which generatively means?",
            "Finding the weights that are most likely to have generated the database that you're looking at maximum likelihood.",
            "Unfortunately, there is no closed form solution right?",
            "I am going to need to do an optimization process, but Fortunately likelihood is a convex function of the weights, meaning there's a single global optimum.",
            "I don't have to worry about local optimum, I don't have to worry about initialization OK, and I can use, you know, any gradient descent algorithm and fast 2nd order quasi Newton methods like LBF GS to find the optimal weights and the key thing of course is what is the gradient?",
            "Here's the gradient.",
            "The derivative of the log likelihood with respect to await has actually very intuitive form, which is the falling.",
            "It's the difference between the number of true groundings of the corresponding clause in the data and the expected number of true groundings according to the model.",
            "So what happens is that if your model is predicting that a clause is true less often than it really is, the weight of that clause needs to go up if it's predicting that it's true more often than it really is, then the way it needs to go down.",
            "And when all the weights and when all the counts lineup, you've learned the maximum likelihood weights and you can stop.",
            "OK, so very simple.",
            "At some level.",
            "Very intuitive, but there's a big snag here.",
            "The big snack, usually when you have a powerful language there is a snack somewhere, right?",
            "The big snack here is that computing these expectations requires inference.",
            "And you know, inference is still costly no matter how many tricks we do right?",
            "And if you now have to do inference that every step of a gradient descent procedure, this is going to end up being very, very slow and you know we tried it and it was very, very slow, so we had to look for alternatives.",
            "Fortunately, they exist one of the alternatives is changing the subject if the thing that you're trying to optimize, it's too hard to optimize, optimize something else that's easier and that hopefully has some correspondence with the first thing.",
            "And you know, in Markov networks there is this standard approach of."
        ],
        [
            "What's called pseudo likelihood pseudo likelihood is just the product overall variables of the probability of the variable condition on the state of its neighbors in the data.",
            "And the great thing about pseudo likelihood is that you don't need the inference to compute this right.",
            "This is just Markov blanket, right?",
            "It's like Gibbs sampling, right?",
            "And if you combine this with something like LBF GS you actually get quite fast learning and it's a consistent estimated so it has some nice difficult properties.",
            "So for many problems to the likelihood is the way to go.",
            "It's widely used in areas like vision, special statistics, some natural language processing as well, and and so forth.",
            "However, sometimes to the likelihood will give you very bad results.",
            "And this is not surprising, because if you think about it, so the likelihood is only taking local interactions into account.",
            "It's not taking long range effects, so you for inference time I need to infer from some variable over there this time variable.",
            "Over here I'm actually doing something that the pseudo likelihood did not optimize the parameters for.",
            "OK, so for some problems this will not give good results."
        ],
        [
            "Fortunately for those problems, there's another thing that we can use which is very popular machine learning, which is discriminative learning.",
            "What is discriminative learning?",
            "Discriminative learning is something that you can do when you know in advance which variables are going to be evidence and which variables are going to be query.",
            "And that's usually the case and people do discriminative learning in the main reason for this is that it just tends to give better results.",
            "For us.",
            "It's going to have the additional nice effect that we can.",
            "You know there's other approximations that we can.",
            "Exploit here, so now what we're going to do is we're going to maximize the conditional likelihood of the query given the evidence instead of their joint likelihood, right?",
            "And already I saved something here, because now I don't need to model dependencies among the evidence variables because the query time they're going to be known and that would be a waste of time.",
            "OK, and I can just optimize what I really care about, so I have this formula that is pretty much the same as before, except that it depends on X&Y instead of, you know on all the variables in undifferentiated.",
            "But now there's another nice thing that we can do about this difficult turn here, which is the following.",
            "The thing that makes inference heart is that there's usually many modes, right?",
            "Then you need to find them an average over all of them.",
            "But when you condition on evidence, what usually happens is that many or most of those modes go away.",
            "In fact, as the condition on more and more evidence, eventually you probably wind up with just one very large mode.",
            "OK, So what we can do is instead of doing a sum over all possible states, I can just find my most likely state.",
            "And take the counts in that state as an approximation for the average of the count several States and if that mode is large enough, this actually will work very well, and often it is OK."
        ],
        [
            "So this idea was actually originally introduced in something called the voted Perceptron algorithm by Mike Collins, which was for doing, you know, HMM, things like part of speech tagging in natural language and so forth.",
            "So in the voter perception, of course it's an HMM.",
            "So what we assume is that the network is a linear chain like this.",
            "So here are my query variables and here's my evidence.",
            "And this is a sequential generalization of the good ol perceptron algorithm and the way it works is as follows are initialized, although it's to zero.",
            "And then I do the following some number of times I find the most likely value of Y given X using the Viterbi algorithm.",
            "And then I do the difference between the accounts in the data and accounts in that state multiplied by a learning rate update the weights.",
            "Do this some number of times.",
            "Average the results because that improves accuracy by fighting over fitting and I'm done.",
            "OK, now what we would like to have is a similar algorithm that works for them.",
            "Lens meaning when this structure is not only your chain but is arbitrary.",
            "And if you think about it, we already have everything we need to do that."
        ],
        [
            "Because we have the Max walk set to do MVP inference, all we have to do is go to the voted Perceptron algorithm, replacing interview with Max Walk set and now we have a much more powerful algorithm to learn weights for arbitrary ambulance OK. And this and this is what we do."
        ],
        [
            "OK, let me say a little bit about structure learning.",
            "Structure learning is the problem of discovering what formulas called from the data or revising correcting formulas that somebody gave you.",
            "So how might you do that?",
            "Well, this is if you think about it, the generalization of the well known problem of feature induction in Markov networks.",
            "It's also if you think about it, a form of inductive logic programming the field of inductive logic programming is concerned with discovering prolog programs from data, discovering horn clauses from a database.",
            "And this is a form of inductive logic programming, and so we can use all the good techniques that people have developed there with a couple of important differences.",
            "One is that most of IRP is about learning horn clauses because it's essentially for classification.",
            "But here we're trying to model the joint distribution of a bunch of variables, so we need to be able to learn any clauses, not just horn clauses.",
            "This is one thing the other one is that in IRP typically your search is guided by a criterion like accuracy information gain.",
            "Some combination of this with coverage.",
            "But here what we're trying to do is learn a probabilistic model.",
            "So what we should be evaluating our candidates by is likelihood, right?",
            "We're trying to maximize likelihood it into the deck, which sounds very straightforward until you realize that in order to evaluate the candidates structures so you know, I have my current MLN, and I'm going to try, for example, adding a little to a clause to evaluate how good this is, I have to recompute the weights for the MLN.",
            "And I'm going to have to recompute the weights every time I propose a new candidate.",
            "And, you know, I could easily consider millions of candidates, so this isn't going to work.",
            "OK, So what can we do?",
            "Well, surprisingly, learning which which can actually turns out to not be the bottom.",
            "Like if you do a couple of very simple things, one is, you can loosen your convergence thresholds, right?",
            "I'm just trying to decide which one is best.",
            "I don't even know exactly what the weights are, right, just enough to pick the right candidate most of the time, and then when I pick that kind of that can really run the process to convergence.",
            "To make sure I have the right weights.",
            "OK, so this is one thing.",
            "The other thing is when I do my knew it optimization, I should just start from the previous weights.",
            "If I have a new clause, I start with the weight of 04.",
            "If I'm modifying a clause, I just start with the same weight.",
            "Because the chances are when I entered Issaquah, this isn't going to affect the width of most other clauses, and so if I use something like LB RTS that converges very fast when you're near the optimum and I start with the current weights off and I will get the right weights in just two or three iterations, so the bottleneck actually turns out to be something else.",
            "The bottleneck is computing the number of true groundings of each clause, which surprisingly is itself an intractable problem, and in weight learning we only had to do this once at the beginning and then we were done, so it was not too bad, but now I have to.",
            "This is for every new clothes that I come up with, and again this could get very very expensive.",
            "But again, there's a there's a set of tricks that you can use to make this much math facture.",
            "Let me mention only the most important one in terms of how much you gain from it, which is subsampling.",
            "If I have a clause with the billing your trilling groundings, which can very easily happen, I don't actually need to look at every one of those groundings to estimate the number of programmers I can.",
            "Random Sample 1000 of the groundings, check those and then extrapolate to the total and this you know again gives you orders of magnitude speed improvement.",
            "So with these things you can actually do structure learning in Markov logic at about the same speed that you can do traditional inductive logic programming, which truth in advertising is still not all that fast.",
            "So if all the things that I'm covering here today, the one that.",
            "It really doesn't quite scale to large ruler problems.",
            "Is the structure learning, but you know we're working on it.",
            "Certainly for your doing is just revising a knowledge base and there's not too much revision to do.",
            "This will probably still be fast enough.",
            "OK, so now you."
        ],
        [
            "Have the usual choices.",
            "You know it's a search process, so your initial state, your operators, your evaluation function, your search method, your initial state can just be unit clauses, or it can be a hand coded knowledge base.",
            "Operators are things like available in the software, things like adding and removing literals, flipping signs of literals.",
            "The evaluation function is pseudo likelihood plus the structure prior, right?",
            "I need the structure prior.",
            "Just isn't loading.",
            "Visionaire was to make sure that I don't over fit, so I penalized for moving away from the initial knowledge base.",
            "And then various search methods have been proposed by our group and others.",
            "Things like beam search, shorts for search bottom up learning and so forth."
        ],
        [
            "So that's the learning.",
            "Let me briefly mention the software in which all of this is."
        ],
        [
            "Is available, it's called alchemy.",
            "It's open source.",
            "It allows you to write down your knowledge or capture it from somewhere in the form of 1st order formulas, and then all the algorithms that I just described for learning and inference are available.",
            "And in addition you have programming language features like you know file inclusion, typing syntactic sugar for the things that you do most often, the ability to link in external functions and predicates and so forth.",
            "Here's the URL I will put it up again at the end."
        ],
        [
            "It's interesting to compare alchemy with the kinds of things that were available before on both the logical side and the statistical site, so comparing alchemy with Prolog in Prolog, the representation is horn clauses and then alchemy.",
            "It's arbitrary 1st order formula, so alchemy is more flexible in that respect.",
            "In Prolog, inference is done by theorem proving, which you know back in the 70s was the fastest form of inference available, but you know these days you know the fastest inferences by.",
            "Model checking by satisfiability, and that's what we do, but of course most importantly in alchemy.",
            "You have learning and handling of uncertainty right out of the box, and in Prolog you don't.",
            "Correct now comparing alchemy with sort of like the software that exists out there for doing things like statistical inference and machine learning.",
            "There's of course many different packages, no single one that's used for everything, but probably the most popular one by a good margin is something called bugs, meaning you know vision inference using Gibbs sampling.",
            "So Bugs uses Bayes Nets as the representation.",
            "Malcolm uses Markov Nets in bugs influences, then by Gibbs sampling, which we already saw can get very, very slow if you have strong dependencies.",
            "And how come you have Gibbs sampling in a number of other things, including empty set, which is much, much faster in bugs?",
            "You can only learn parameters in alchemy.",
            "You can also in structure, but of course the most important difference is that alchemy is relational.",
            "In bugs there's no simple general way to handle non IID data in alchemy as we just saw.",
            "This is very easy.",
            "So alchemy even looking at it only as logic or only as probability, is already in many ways in advance over the state of the art.",
            "But the nicest thing of course is that it seamlessly combines these two things.",
            "You have a problem that has both structured and unstructured aspects.",
            "You don't have to glue together pieces, you don't have to start with Prolog and try to make it probabilistic or start with bugs and try to handle relational information.",
            "It's all done very uniformly and smoothly in the language of Markov logic."
        ],
        [
            "So last part of the talk, let me mention some examples of what you can do with Markov logic and in some sense this is the payoff right so far we've been investing in understanding this representation and developing the arguments for it.",
            "So what is it that you can do with?"
        ],
        [
            "An awful lot.",
            "It turns out, so here's the here's a sample of some of the things that Markov logic and alchemy, in some cases, other implementations of Markov logic that people have developed, things that have been done.",
            "Information extraction is 1.",
            "It's actually example that I'm going to go over here in some detail to illustrate things.",
            "It's of course a very important problem.",
            "There was a system by Riedel and client that actually won the Triple L 2005.",
            "Information extraction competition using Markov logic.",
            "This was the goal of this was to extract gene interactions from from Medline abstracts into resolution.",
            "Another very important problem link prediction collective classification.",
            "Things like classifying webpages, using their links as well as their content, various kinds of web mining, natural language processing.",
            "I just came from the MLP where we had the paper applying Markov logic to natural language processing ontology refinement.",
            "Another very important problem.",
            "As I mentioned the best paper award at CHM last year.",
            "As a paper by Fei Wu and and then, well that use Markov logic to learn and refine ontologists from from Wikipedia and then other things like com, bio, social network analysis, and so forth.",
            "Here's a couple of examples that I think worth mentioning, Psych, which some of you know is the world's largest knowledge base, right, but didn't exactly succeed because of the problems in first order logic.",
            "The folks at Sitecore have begun to transform Psych into into an MLN by adding weights to their formulas.",
            "Right?",
            "Again, very straightforward thing to do.",
            "And another one is at Caillou, the largest project in Darby History which is trying to build this automated personal assistant that manage is your email and your meetings for you and automatically files things in folders and what not.",
            "The main inference engine that Caillou uses is based on Markov logic, and there's more of these things which hopefully and again you can go to the alchemy website and there's you know, a bunch of pointers to papers and applications and millions and databases and what not there, but you know, hopefully this will persuade you that this really is something that you can.",
            "Do state of the art solutions for many important problems in information and knowledge Management Today.",
            "So for concreteness, let me."
        ],
        [
            "The last thing in this talk, before I conclude, just show you how we can do something like information extraction in Markov logic, right?",
            "And I think this is a nice illustration because information extraction is your quintessential bridge between the structure and the instruction, right?",
            "You start out with in structured it's just text and you populate a database and how you can do that.",
            "It based inference.",
            "Then you know logical inference over it, right?",
            "So I'm going to use as an example the problem of extracting a database of citations from the reference lists in papers, right?",
            "This is the problem that sites here.",
            "And Google Scholar has to solve very problem.",
            "Very popular problem among researchers today.",
            "So here's a very small example and now.",
            "You need to do 2 main."
        ],
        [
            "As an information extraction, the first one is what's called segmentation.",
            "I need to run over the text string and figure out where the fields of my database are.",
            "OK, so for example, perox, England and painted them Ingles are authors.",
            "Tripoli 06 is a venue and so forth, so this is the first part, it's segmentation."
        ],
        [
            "The other part is internal resolution.",
            "I need to figure out that triple your six and the proceedings of the 21st National Conference on Artificial Intelligence are the same thing, right?"
        ],
        [
            "And then I also need to figure out that these two papers are the same thing.",
            "These two papers are the same and these two papers are the same.",
            "Otherwise when I query my database, I'm going to get duplicate results right?",
            "Which should probably seen happen when you get, you know, duplicate junk mailings to you with different misspellings of your name and so forth.",
            "And if I try to do mining on top of this and the data is not properly resolved, the results will be garbage.",
            "OK, so we need to be able to do this."
        ],
        [
            "Two things and the state of the art is basically as follows.",
            "For segmentation, people use something like a hidden Markov model or a conditional random field to assign.",
            "Each took into a field like you could run over the the input string into go like OK.",
            "Author, author, author, author, title, title, title, title, venue.",
            "OK, so this is the segmentation part and then for the entity resolution people typically use the classifier like say logistic regression, to predict for each pair of fields, whether they're the same.",
            "Right are these two the same people?",
            "And for each pair of papers, whether they're the same right and then they do some kind of transitive closure?",
            "Right, because if A is the same as B&B is the same as C, then a is the same as see OK and of course I'm exemplifying this here for the problem of citation matching, but you can imagine doing this for any other field like you know, extracting city names and people names from the news and so forth.",
            "Now if you did this today, as you know, people at you know lots of companies like Yahoo and Google and Amazon and so forth.",
            "Do and roll your own information extraction system.",
            "This is going to take a long time, and it's probably going to run into the 10s of thousands of Java code and once it put in the bells and whistles even more in alchemy, you can implement the complete information extraction system in just seven formulas that actually fit on a single slide that I'm going to put up shortly.",
            "So you get several orders of magnitude improvement in how quick and simple it is to program a complex, you know, application dealing with structure than."
        ],
        [
            "And my structured information."
        ],
        [
            "So here's how we do that.",
            "Fields.",
            "In this case, I think, like Arthur, title, venue, and there's also citations in positions.",
            "OK, so I'm going to put this here for clarity, but you could actually infer them from."
        ],
        [
            "From the input database, now predicates as evidence.",
            "I'm going to have just the predicate token, which says that this token appears in this position in this citation.",
            "So I process the text into this form and that's why what's going to drive my."
        ],
        [
            "Inference, and now it's query predicates.",
            "I'm going to have three things in field which says that this position in this field is part of this citation.",
            "Sorry this position in this citation is part of this field.",
            "So for example, position one in citation one is part of the author field.",
            "OK, so this query predicate is going to do the segmentation for me.",
            "Once I've inferred the truth files of this predicate, I will know which tokens belong to which fields, and then these two predicates.",
            "These to query predicates do the entire resolution.",
            "Same field says that for example, the authors field in Citation 1.",
            "Citation two is the same or not, right?",
            "Did the same people piece England proxying are the same person and same site just means that the two citations are the same?",
            "OK, So what I'm going to do is I'm going to write an MLN that takes this and infers these."
        ],
        [
            "So here's the MLN.",
            "As advertised, just 7 short formulas and these three formulas here do the segmentation and these four do they do the entity resolution OK?",
            "And here's how this works.",
            "These three formulas are all it takes to implement any."
        ],
        [
            "Umm, and it works as follows.",
            "This first formula says that if I have a certain token in a certain position, then that position is in a certain field and as in prolog, all free variables are universally quantified.",
            "So this is going to ground out to every field and every token etc.",
            "And this little plus notation is a nice piece of syntactic sugar that says we want to learn a different weight for every combination of these variables that have pluses.",
            "So I'm going to learn a different, which for each combination of token and field.",
            "Which means that what this formula is going to represent is exactly.",
            "The observation matrix of a hidden Markov model where the observations are tokens and the states are fields.",
            "So what these words are going to capture is the correlations between tokens and fields.",
            "Conference is probably indicative of venue.",
            "Smith is probably indicative of author etc etc.",
            "There is problem not indicative of much of anything, so that will have a little weight.",
            "So this is the observation made."
        ],
        [
            "Tricks?",
            "This here is the transition matrix.",
            "It just says that if you're in the field.",
            "So soon you author Field, then in the next position you're likely to be in the author field as well.",
            "In Journal you could capture the complete correlation between fields, but for information extraction purposes the only thing you really need to capture is that you tend to stay in the same field.",
            "So here I have F&F, but if I wanted the general transition matrix I would just have.",
            "If you're in field F at position I, UN field F prime at position I plus one and then again I would have the complete matrix, OK."
        ],
        [
            "And this formula just says that I can't be in more than one field at once, and this is actually formula that's not needed.",
            "The reason we're putting it here is that we can actually handle things that are not in any field without having to introduce another field just by allowing things to not be in any field.",
            "But then we still have to say that you can't be in more than one field at the same time.",
            "OK, so this does the information extraction.",
            "There's the segmentation part of the information extraction using HMM."
        ],
        [
            "Now let's look at how we do the entity resolution.",
            "So here's a formula that predicts whether or not two fields are the same from whether or not they contain the same tokens.",
            "If the field content if a citation contains a certain token, and, say, the author field, and so does another citation, then this makes them more likely to be the same field.",
            "Right field share words.",
            "They are more likely to be the same, and again, I can learn weights for these because some words will be much more informative than others, so one way to think of this formula is as a similarity comparison between fields.",
            "Another one is to see this as a logistic regression.",
            "This is.",
            "This formula is ridiculously a logistic regression, where this is the class variable and these are the features, right?",
            "So you can set up logistic regression by defining the features using these formulas, and then you know an application to the class."
        ],
        [
            "So here's a very little formula that does a lot of work.",
            "It says that if the fields are the same, then the citations are the same and vice versa.",
            "So if the author field is the same and the venue field is the same and the title for this, and then probably this is the same citation and again certain authors might have a higher weight than for example, venues and titles would probably have an even higher weight, right?",
            "But notice that I've put an equivalence here, which means that you can also do the other way.",
            "You can say if the citations are the same, then the fields are the same and this is incredible useful for the following reason.",
            "Remember the example with Tripoli eye and the National Conference on Artificial Intelligence Traditional into resolution systems will choke on this because there's no similarity between those two strings at all.",
            "But if I have two papers that I conclude are the same because the authors in the titles are the same and one of them is in triple AI and the other one is in the national Conference, probably from that I can conclude that those two things are the same, and now this becomes available for a bunch of other inferences.",
            "And now maybe I can decide that certain papers are the same that I couldn't before.",
            "And you know this.",
            "Will you know this will propagate and potentially give me much better results than it would otherwise?",
            "And indeed, you know we've had papers or we will show exactly that.",
            "This works very well."
        ],
        [
            "So finally these two formulas just do the transitive closure for you.",
            "This formula says if field sees the field, F is the same in C as in C prime, and it's same as in C prime is in C prime, and this is the same as in C as in C plus prime right?",
            "And this is just the same thing for citations and the beautiful thing about this is that.",
            "There have been no in the papers on different ethnic waits to incorporate transitive closure into anti resolution.",
            "If years ago, for example, there was a widely cited paper by McCallum and Wellner which was how do we extend the CRF to do transitive closure for Inter resolution and they had to develop their representation of this inference.",
            "Learning algorithms, you know was quite an achievement at the time.",
            "In Markov logic.",
            "All that you have to do is write down the transitivity axiom from logic.",
            "That's it.",
            "This is the accent of transitivity.",
            "We know that you know the equality is a transitive relation.",
            "Put that in the MLN and everything goes through OK, so.",
            "This thing is actually, believe it or not, just these four formulas are actually pretty state of the art into the resolution system.",
            "The segmentation part, however, is not so great.",
            "The reason it's not so great is that it's not very good at telling where the boundaries of fields are.",
            "It can propagate author, author, author, but it doesn't exactly know where the author ends in the title begins, and people have known this information extraction and what they usually do is they have rules for deciding where field begins and where field ends, like for example the whole area of wrapper, induction for extracting things from web pages based on this idea.",
            "OK, now that's sort of like a rule based approach, and then there's sort of like the CRF type.",
            "Properly, you know probabilistic approaches.",
            "What we actually like to do is to combine both.",
            "Now in Markov logic you can combine both very very simply."
        ],
        [
            "For example, in the case of citation matching, we know that there's a really good predictor of where field ends under another one begins, which is a period right in citations.",
            "Usually, you know at the end of the authors appeared and then disappeared at the end of title.",
            "So what we can do is we can just modify this rule that says contiguous positions like to be in the same field, unless one of them is it.",
            "OK, so the period breaks the propagation of same field and we just this little change.",
            "Suddenly we get much, much better result."
        ],
        [
            "Here's an example on the corner database of computer science papers.",
            "Standard data set for these problems.",
            "These are precision recall curves, so I would like to be up here.",
            "Here's what happens when I only use the token information.",
            "Here's what happens when I use sequence information, and when I add that little modification with the period, suddenly I get to this yellow curve and you find out, do the same thing for, which of course is a noisy signal but still useful signal.",
            "I'm almost up there, OK?"
        ],
        [
            "So and you know, again, we also get very good results on matching venues with a few more formulas, we get the state of the art system that was developed by a student in three months as opposed to five people over two years throwing everything you know.",
            "But the kitchen sink."
        ],
        [
            "OK, so let me just briefly."
        ],
        [
            "Conclude.",
            "The structure then structured information spectrum has exploded in the last 20 years used to have either structured or unstructured information and languages for each of those and inference methods for them.",
            "Now we have the whole spectrum and we need the language.",
            "We need languages that can handle that in a uniform, general and simple way.",
            "Markov logic is one such possible language that I described here.",
            "We saw how it works, the different inference and learning algorithms for it, and what you can do with it.",
            "There is of course much research left to do.",
            "In terms of scaling up, difference in learning, making the algorithms more robust, right things are not yet at the push button stage where you push the button and things run.",
            "You have to play with parameters.",
            "Would like to make that easier, to the point where things can be used by non experts.",
            "And of course there's many new applications to try and more generally I think what this points to that's very exciting is really a new way of doing computer science, which is instead of having to program everything that you do, you just define the brush structure of what you want to do is in the information extraction example.",
            "And then you let the data and the probabilistic inference do the hard lifting.",
            "You know to do the rest of the work again.",
            "As I said, this is all available at the alchemy website datasets, MLN software, tutorials, etc.",
            "So I encourage you to try it out.",
            "It's at this URL and thank you very much.",
            "We talked about this once before.",
            "Ron Baker, my HP laptop.",
            "Distributed or parallel inference procedure randecker manageable as highly considering distributed parallel inference procedures in Marklogic network.",
            "Because you know, they love their theoretical properties of the network and can be used absolutely.",
            "So one of the ways to scale this up to you know web scale has to be going parallel and the publicly available Alchemist software is purely sequential at this point.",
            "But we already have internal parallel versions of certain things like inference and structure learning, and I am actually collaborating with Carlos Guestrin who's done a lot of work on inference.",
            "Probabilistic inference on multicore and cluster machines to paralyze alchemist.",
            "So we expect this to become available in the foreseeable future.",
            "And indeed it's a very important thing to do."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can you hear me alright?",
                    "label": 0
                },
                {
                    "sent": "Thank you all for being here and thanks to Alec for bringing me here and I'll try to make this interesting for you so I'm going to talk about Markov logic, a unifying language for information and knowledge management.",
                    "label": 1
                },
                {
                    "sent": "This is work that I've done at the University of Washington with Stanley Kok Daniel out.",
                    "label": 0
                },
                {
                    "sent": "Orfeum met Richardson processing with Mark Sumner and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And Julie Wink, here's a brief outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "I will begin with a little bit of motivation and then some necessary background, and then I'll get into the heart of things.",
                    "label": 0
                },
                {
                    "sent": "Which is this new language called Markov logic?",
                    "label": 1
                },
                {
                    "sent": "I will talk a little about inference and learning algorithms for Markov logic.",
                    "label": 0
                },
                {
                    "sent": "In this software, open source software that they're implemented in, and then I will talk about some of the things that we can do with Markov logic and conclude with a little bit of discuss.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the motivation, and here's a cartoon you know, very, very oversimplified cartoon of the state of information and knowledge management about 20 years ago.",
                    "label": 0
                },
                {
                    "sent": "You can think of information as lying on this axis that goes between structures on the one side and then structured on the other, and 20 years ago.",
                    "label": 0
                },
                {
                    "sent": "What happened was that there were these things that were definitely very much on the structured side like databases and knowledge bases and the languages that people used to deal with them like SQL data log and 1st order logic and its many variants.",
                    "label": 0
                },
                {
                    "sent": "This was the structured end of the spectrum and then there was the unstructured into the spectrum where you just had three texts.",
                    "label": 0
                },
                {
                    "sent": "And of course, there was information retrieval that dealt with free text and there was an LP that dealt with free texts, and this is where things stood OK Now.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fast forward to today and look what's happened.",
                    "label": 0
                },
                {
                    "sent": "It's really amazing, right?",
                    "label": 0
                },
                {
                    "sent": "This spectrum between structured and unstructured has become completely populated.",
                    "label": 0
                },
                {
                    "sent": "The combinations of structured and unstructured and the degrees of structure you know, have have exploded.",
                    "label": 0
                },
                {
                    "sent": "There's of course hypertext, and you know HTML, but then there's also things somewhat more structure than hypertext, like semi structured information.",
                    "label": 0
                },
                {
                    "sent": "For example XML.",
                    "label": 0
                },
                {
                    "sent": "There's the deep web right, which is kind of like structured masquerading as unstructured.",
                    "label": 0
                },
                {
                    "sent": "There's a semantic web you know with things like RDF and Owl web services, so the SDL.",
                    "label": 1
                },
                {
                    "sent": "And then there's things like information extraction that go from structure to unstructured, and there's even things like ubiquitous computing and sensor data that produced all this data that is kind of much noisier than people were used to, and on the one hand this is very exciting.",
                    "label": 0
                },
                {
                    "sent": "But there's all these things that we can do today that we couldn't do before, but on the other hand, you know this is a nightmare, right?",
                    "label": 0
                },
                {
                    "sent": "If you're, you know, a poor software engineer, or, you know, applications programmer out there in the real world, you know there's ever more things that you need to learn, ever.",
                    "label": 0
                },
                {
                    "sent": "More things that you need to understand and you know the question is, can we make this a little bit better?",
                    "label": 0
                },
                {
                    "sent": "And what's happened?",
                    "label": 0
                },
                {
                    "sent": "As you know, this spectrum got populated.",
                    "label": 0
                },
                {
                    "sent": "Is that not surprisingly, people from both ends try to and are still trying, right?",
                    "label": 0
                },
                {
                    "sent": "This is a lot of what's going on today, including at this conference.",
                    "label": 0
                },
                {
                    "sent": "They tried to extend their technology towards the middle.",
                    "label": 0
                },
                {
                    "sent": "And towards the other side, right?",
                    "label": 0
                },
                {
                    "sent": "So people you know in databases, you know if you go to database conferences, a lot of the work that appears there is really trying to deal with data that is less structured than a standard database like XML for example, and so forth.",
                    "label": 0
                },
                {
                    "sent": "Likewise, on the unstructured side, right?",
                    "label": 0
                },
                {
                    "sent": "We have information retrieval that uses things like the links in the HTML, of course, and so forth.",
                    "label": 0
                },
                {
                    "sent": "But the question is, can we do better than just this plethora of approaches?",
                    "label": 0
                },
                {
                    "sent": "And instead of you know, can we somehow actually meet in the middle here and come up with a language and you know a way of doing things that actually accommodates the full spectrum and makes life easy for everybody again?",
                    "label": 0
                },
                {
                    "sent": "And first of all, I would argue that we have to try to do that.",
                    "label": 0
                },
                {
                    "sent": "And second of all what I'm going to talk about here is a proposal in that direction, Markov logic and we will see that you know, even though it's a fairly young idea, it really has some notable successes to its credit.",
                    "label": 0
                },
                {
                    "sent": "But of course there's much more to do.",
                    "label": 0
                },
                {
                    "sent": "And hopefully people here will get interested in, you know, developing some of these things and and using them and so forth.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "And by the way, I was looking at the program conference and I was happy to notice that the topic of almost every tutorial is directly related to something that you're going to see here, right?",
                    "label": 0
                },
                {
                    "sent": "Which makes me believe that indeed you know the time has come.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To try to do something like this.",
                    "label": 0
                },
                {
                    "sent": "So to summarize, what we need is languages can both handle structured and unstructured information and any variation or combination of them.",
                    "label": 1
                },
                {
                    "sent": "But now of course as before with SQL, then you know TF IDF and whatnot, it's not enough to have a nice language, we need efficient algorithms for doing inference in that language, including all the tasks that people have in the past done with IR NLP or knowledge bases or databases.",
                    "label": 0
                },
                {
                    "sent": "And you know for going between them also.",
                    "label": 0
                },
                {
                    "sent": "And you know to maybe do something in one.",
                    "label": 0
                },
                {
                    "sent": "Can you know?",
                    "label": 0
                },
                {
                    "sent": "Go to the next one you know, do something there and come back.",
                    "label": 0
                },
                {
                    "sent": "And also, of course we need machine learning algorithms that are efficient because you know, it's pretty clear these days that the way you can actually leverage a lot of this state and be able to handle it has to go through machine learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so in essence what I'm going to do here is propose an example of this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the example is this language called Markov logic.",
                    "label": 0
                },
                {
                    "sent": "Here's the one slide summary of what Markov logic is, right?",
                    "label": 0
                },
                {
                    "sent": "So if you fall asleep after this, you still haven't.",
                    "label": 0
                },
                {
                    "sent": "You know you still got the main idea to talk.",
                    "label": 0
                },
                {
                    "sent": "Markov logic is a language that unifies 1st order logic and probabilistic graphical models, which I believe is a good way to try to achieve these goals because first order logic handles structured information right.",
                    "label": 1
                },
                {
                    "sent": "Knowledge bases are first order logic and things like data logger.",
                    "label": 0
                },
                {
                    "sent": "Really special cases of 1st order logic probability handles the unstructured information, the noise, the uncertainty inherent in text, and then you know semi structured sources.",
                    "label": 0
                },
                {
                    "sent": "And it doesn't just glue them together, it really, truly I hope I will proceed with that.",
                    "label": 1
                },
                {
                    "sent": "Markov logic really does unify the two.",
                    "label": 1
                },
                {
                    "sent": "There is no separation between the logic and the probability, and this structure.",
                    "label": 0
                },
                {
                    "sent": "Then the instructor.",
                    "label": 0
                },
                {
                    "sent": "Then in Markov logic anymore.",
                    "label": 0
                },
                {
                    "sent": "So if you get into this frame of mind that you don't have to worry about, you know well this structured here, and there's in structured over there anymore.",
                    "label": 0
                },
                {
                    "sent": "You can just think think of it all in a in a uniform way.",
                    "label": 0
                },
                {
                    "sent": "It does of course build a lot on previous work.",
                    "label": 0
                },
                {
                    "sent": "I'm going to not really going to that a lot in the interest of time, but I would just wanted to mention that here.",
                    "label": 0
                },
                {
                    "sent": "It builds on ideas that go as far back as the early night.",
                    "label": 0
                },
                {
                    "sent": "Isn't something called knowledge based model construction where he was taken knowledge base and and you know extract Bayesian networks from it.",
                    "label": 0
                },
                {
                    "sent": "Things like probabilistic relational models and so forth.",
                    "label": 0
                },
                {
                    "sent": "But compared to the previous work, I think there's an important difference, which is that Markov logic released the first practical language as opposed to just a theoretical proposal that actually comes with real efficient learning and inference algorithms in the complete open source implementation of them that you can actually use today.",
                    "label": 0
                },
                {
                    "sent": "OK, so as far as I know, at this point mark of logic is really the only language that does this, but you know, I expect that more will will appear in the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Future OK so.",
                    "label": 0
                },
                {
                    "sent": "Here's the one slide summary of Markov logic and what I'm going to talk about today.",
                    "label": 0
                },
                {
                    "sent": "The syntax of Markov logic, right?",
                    "label": 0
                },
                {
                    "sent": "So what is this language?",
                    "label": 0
                },
                {
                    "sent": "Markov logic syntax is very, very simple.",
                    "label": 0
                },
                {
                    "sent": "The syntax is just formulas in first order logic.",
                    "label": 0
                },
                {
                    "sent": "No difference from before, but one addition we're going to have weights for the formulas.",
                    "label": 0
                },
                {
                    "sent": "That's the syntax.",
                    "label": 0
                },
                {
                    "sent": "The semantics, as we'll see, is that we're going to interpret these formulas as templates for constructing graphical models, in particular for the features of Markov networks.",
                    "label": 0
                },
                {
                    "sent": "So Markov logic sets up a big probabilistic graphical model.",
                    "label": 0
                },
                {
                    "sent": "A Markov, you know, knowledge base in Markov logic, and then we're going to see some inference algorithms, which not surprisingly combine ideas from logic and probability, including things like satisfiability testing, Markov chain, Monte Carlo, Knowledge, Base model construction, and so forth.",
                    "label": 0
                },
                {
                    "sent": "And similarly, learning is going to combine ideas from the logic and from the statistical side, things like the voted perceptrons pseudo likelihood, inductive logic program programming and so on.",
                    "label": 0
                },
                {
                    "sent": "And as I said, this is all available in the alchemy software package.",
                    "label": 0
                },
                {
                    "sent": "That's what it's called, and I'll put up the URL at the end and it's been used already for a wide array of information and knowledge management applications like information extraction, web mining, social networks, ontology refinement, personal assistance, etc.",
                    "label": 0
                },
                {
                    "sent": "The best paper award that CHM last year was a paper that applied Markov logic to do ontology learning.",
                    "label": 0
                },
                {
                    "sent": "So, and you know, we'll see more interesting examples of successes of.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Logic, so let me in the interest of fairness.",
                    "label": 0
                },
                {
                    "sent": "Now walk over to this side of the room.",
                    "label": 0
                },
                {
                    "sent": "So let's start with a little bit of background, and you know most of you are probably familiar with at least some of this background.",
                    "label": 0
                },
                {
                    "sent": "But just to get us all on the same page, I'm just going to very briefly cover.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things Markov networks and 1st order logic so many of you are probably at least passingly familiar with vision networks.",
                    "label": 0
                },
                {
                    "sent": "Markov networks might be slightly less familiar to you, so let me just briefly introduce them here.",
                    "label": 0
                },
                {
                    "sent": "A Markov network is an undirected graphical model, so it models the joint distribution of a set of variables.",
                    "label": 0
                },
                {
                    "sent": "So, for example, here's four Boolean variable smoking, cancer, asthma and cough, and the Markov network has two parts.",
                    "label": 0
                },
                {
                    "sent": "One is the graph and the other one is the parameters.",
                    "label": 0
                },
                {
                    "sent": "Again, a lot like a vision network, except that it's undirected.",
                    "label": 0
                },
                {
                    "sent": "An arc between two variables means that they are directly dependent on each other and Conversely, if I take out the arcs that are adjacent to an arc that makes it independent of the rest of the network.",
                    "label": 0
                },
                {
                    "sent": "OK, so the structure of the network gives you the conditional independence between variables.",
                    "label": 0
                },
                {
                    "sent": "Now the parameters are defined as follows.",
                    "label": 0
                },
                {
                    "sent": "For every click in the graph, and here there's two clicks, smoking cancer and cancer asthma cough we're going to define a potential function and the potential function is just a real valued function of the state.",
                    "label": 0
                },
                {
                    "sent": "So for example, real valued non negative function of the state.",
                    "label": 0
                },
                {
                    "sent": "So for example, for the smoking cancer click, there's obviously 4 States and here's the value of the potential function for each one of them and the way you compute the probability of a state is just you go to each click you see what state it's in.",
                    "label": 0
                },
                {
                    "sent": "You get the corresponding value of the potential function and you multiply them all OK, and then of course you have to normalize by the sum of this overall states to make sure that it's illegal probability.",
                    "label": 0
                },
                {
                    "sent": "OK, now this is all very nice, but there's actually a problem here.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "This only works in terms of scalability.",
                    "label": 0
                },
                {
                    "sent": "If your clicks are small, right?",
                    "label": 0
                },
                {
                    "sent": "If I have a large click I have to do a larger number of States and things fall apart.",
                    "label": 0
                },
                {
                    "sent": "Fortunately there's something else that we can do which.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something that's very popular with statisticians, which is to represent the Markov network in the form of what's called a log linear model and the log linear model is just a normalized exponentiated sum of weighted features.",
                    "label": 0
                },
                {
                    "sent": "And you can convert from one to the other because the product of potentials if you take a law becomes a sum of things OK, and so in the simplest case I can just have one feature for every possible state of every click, and the weight is the log of the corresponding potential right?",
                    "label": 0
                },
                {
                    "sent": "So I can convert from potential function form to log linear form very easily, but of course the the thing that we gain here is that I don't have to have one feature for every possible state.",
                    "label": 0
                },
                {
                    "sent": "I could have a very very large click relating many things.",
                    "label": 0
                },
                {
                    "sent": "As long as I know that there's a few important features of that click.",
                    "label": 0
                },
                {
                    "sent": "I can keep only those OK and save me.",
                    "label": 0
                },
                {
                    "sent": "You know, a lot of memory, a lot of learning sample that I need to learn this parameters and so forth.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "If I define a feature on the smoking cancers click that is 1.",
                    "label": 0
                },
                {
                    "sent": "If you don't smoke or have cancer and 0 otherwise.",
                    "label": 0
                },
                {
                    "sent": "And give that a weight of 1.5.",
                    "label": 0
                },
                {
                    "sent": "I get the same model as before.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of course, in this case it's not very impressive because I just went from.",
                    "label": 0
                },
                {
                    "sent": "You know, having 4 lines here to having one thing, but of course if the clique was large, this would be a huge game.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. And then we're going to take we're going to very much take advantage of this in Markov logic, so that's the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or with introduction to Markov networks, let me just mention a few points of terminology to do with first order logic.",
                    "label": 0
                },
                {
                    "sent": "So in first order, logic will build formulas that say things about the world and they will top out of logical symbols like conjunction, disjunction, quantifiers and so forth and four types of symbols, constants representing objects in the domain like say Anna variables like X, that range over the objects functions like mother of X and predicates like friends, XY that represent relations between objects or properties of objects.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to call a grounding of a formula or a predicate.",
                    "label": 0
                },
                {
                    "sent": "What you get by replacing all the variables by constants.",
                    "label": 1
                },
                {
                    "sent": "So for example, if one of the predicates is friends and two of the constants are in an Bob, then friends Anna Bob would be one such grounding and friends Anna Bob is just a Boolean variable.",
                    "label": 0
                },
                {
                    "sent": "It's true if Ann and Bob are friends, and it's false if they're not OK. And I'm going to call a world also known as a model or interpretation and assignment of truth values to all the ground predicates.",
                    "label": 1
                },
                {
                    "sent": "Right, so I have a bunch of predicates.",
                    "label": 0
                },
                {
                    "sent": "I have a bunch of constants.",
                    "label": 0
                },
                {
                    "sent": "I replaced the constants into the predicates in all possible ways.",
                    "label": 0
                },
                {
                    "sent": "I get a very large number of Boolean variables and this is the state of the world.",
                    "label": 0
                },
                {
                    "sent": "It contains everything you might want to know, and the thing that we're going to be interested in here is probability distributions over these very large Boolean vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wrong direction, so that's the background.",
                    "label": 0
                },
                {
                    "sent": "Let me now introduce Markov.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Magic.",
                    "label": 0
                },
                {
                    "sent": "So here's the idea in Markov logic.",
                    "label": 0
                },
                {
                    "sent": "This is one way to introduce it.",
                    "label": 0
                },
                {
                    "sent": "The problem with logic is that it's very brittle.",
                    "label": 0
                },
                {
                    "sent": "Like people found this out back in the days of expert systems in the 80s.",
                    "label": 0
                },
                {
                    "sent": "Is the world you know.",
                    "label": 0
                },
                {
                    "sent": "Every formula has to be exactly correct and you have and you must have no contradictions between formulas because if you do everything falls apart and this is what makes logic essentially very hard to use in the real world of, you know, messy data and unstructured things and contradictions and contributions from multiple sources and whatnot.",
                    "label": 0
                },
                {
                    "sent": "So the idea Markov logic is, well, the formulas don't have to be thought of as hard constraints on the world where if you violate a formula, you become impossible.",
                    "label": 1
                },
                {
                    "sent": "We can just think of the formulas as soft constraints, meaning that if the world violates the formula, it becomes less probable but not impossible.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is the idea in Markov logic, and now what we're going to do is we're going to give you formula, await their represents how strong of a constraint it is.",
                    "label": 0
                },
                {
                    "sent": "So if you really believe in a formula.",
                    "label": 0
                },
                {
                    "sent": "Then you give it a high weight and the world was a big penalty for violating it.",
                    "label": 0
                },
                {
                    "sent": "If you kind of believe in the formula, then you can give it a low weight, but you know worlds will can still violate that formula and be fairly likely.",
                    "label": 0
                },
                {
                    "sent": "And now the probability of a world is just going to be a log linear model of the form that we just saw, so it's going to be proportional to the exponentiated some of the weights that the formula satisfies.",
                    "label": 0
                },
                {
                    "sent": "So the more formulas that are world satisfies in the more high weight there are, the more the more like of the world will be.",
                    "label": 0
                },
                {
                    "sent": "And as we're going to see, you can do all sorts of wonderful things with just this very simple concept.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a more formal definition.",
                    "label": 0
                },
                {
                    "sent": "A Markov logic network or MLN for short is a set of pairs FW where if is a formula in first order logic and W is a real number together with this set of constants representing objects in the domain, it defines a Markov network as follows.",
                    "label": 1
                },
                {
                    "sent": "The Markov network is going to have one note for each grounding of each predicate in the MLN, and it's going to have one feature in the log in your model for each grounding of each formula in the MLN.",
                    "label": 0
                },
                {
                    "sent": "With the corresponding wait OK, Now that's a bit of a mouthful, so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example that will hopefully clarify things.",
                    "label": 0
                },
                {
                    "sent": "Social networks are of course an extremely popular thing these days.",
                    "label": 0
                },
                {
                    "sent": "And then there's a lot of both.",
                    "label": 0
                },
                {
                    "sent": "You know real systems out there dealing with social networks and a lot of research on them.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of a real social network from the New York Times.",
                    "label": 0
                },
                {
                    "sent": "This is a network of what you might call friends and smokers.",
                    "label": 0
                },
                {
                    "sent": "It's people who smoke, whether how much people smoke and who their friends are, and this is from 1971 to 2000.",
                    "label": 0
                },
                {
                    "sent": "How this network changed.",
                    "label": 0
                },
                {
                    "sent": "An interesting thing about the network changes that first of all, many fewer people smoke now, so there's progress.",
                    "label": 0
                },
                {
                    "sent": "But the people who stopped smoking were not chosen at random.",
                    "label": 0
                },
                {
                    "sent": "People were much more likely to stop smoking if their friends also stopped smoking OK, and the unfortunately also happens the other way around.",
                    "label": 0
                },
                {
                    "sent": "The single most important factor in making a teenager start smoking is whether the majority of her best friends smoke.",
                    "label": 0
                },
                {
                    "sent": "This is actually been studied.",
                    "label": 0
                },
                {
                    "sent": "This one thing predicts more than everything else you can imagine so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How about we try to model?",
                    "label": 0
                },
                {
                    "sent": "But domain like this in first of all let's do it in natural language.",
                    "label": 0
                },
                {
                    "sent": "OK, here are two true statements.",
                    "label": 0
                },
                {
                    "sent": "Smoking causes cancer, which is one of the reasons why we want to avoid it.",
                    "label": 0
                },
                {
                    "sent": "And France have similar smoking habits, so this is the English statement of the problem.",
                    "label": 0
                },
                {
                    "sent": "Now you know CS1.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to turn this into logic.",
                    "label": 0
                },
                {
                    "sent": "You can write something like for every X smokes of X implies cancer of X, and for every X friends XY implies that smokes X and smokes.",
                    "label": 1
                },
                {
                    "sent": "Why are equivalent easy street?",
                    "label": 0
                },
                {
                    "sent": "But there's a.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These two statements were.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "True.",
                    "label": 0
                },
                {
                    "sent": "And these two are false.",
                    "label": 0
                },
                {
                    "sent": "Because not everybody who smokes gets cancer and certainly not all pairs of friends have this.",
                    "label": 0
                },
                {
                    "sent": "Have the same smoking habits.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we really need to do is treat these as statistical regularity's right?",
                    "label": 0
                },
                {
                    "sent": "Which is what they are.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can do that by turning this first order knowledge base into a Markov logic network by attaching weights to these formulas, and we will see in a little bit how the weights can be learned from data.",
                    "label": 0
                },
                {
                    "sent": "You can also set them by hand.",
                    "label": 0
                },
                {
                    "sent": "Of course, intuitively, this formula has a higher weight because it's a stronger regularity than this formula.",
                    "label": 0
                },
                {
                    "sent": "So now I have my Markov logic network and the question is what is this actually saying about the world, right?",
                    "label": 0
                },
                {
                    "sent": "What is the probability distribution that this represents?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's suppose I have a very simple world where there's only two objects, Ann and Bob, right?",
                    "label": 0
                },
                {
                    "sent": "'cause that's what's going to fit on a slide.",
                    "label": 0
                },
                {
                    "sent": "So the first rule that I said was that I'm going to have a node in the network for every grounding of every predicate.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to have smokes Anna and cancer.",
                    "label": 0
                },
                {
                    "sent": "Anna right smokes and as a Boolean variable.",
                    "label": 0
                },
                {
                    "sent": "True defender smokes false if she doesn't.",
                    "label": 0
                },
                {
                    "sent": "Same thing for Bob right?",
                    "label": 0
                },
                {
                    "sent": "And then same thing for.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trans XY, so I'm going to have friends.",
                    "label": 0
                },
                {
                    "sent": "Anna Bob friends Bobana because a sociologist will tell you friendship is not symmetric, right?",
                    "label": 0
                },
                {
                    "sent": "Bob could be much better friend of Anna then she's of Bob for example.",
                    "label": 0
                },
                {
                    "sent": "So I need to represent these two and also have this degenerate case of friends.",
                    "label": 0
                },
                {
                    "sent": "Anna Annan friends, Bob Bob which maybe has to do their self esteem or something.",
                    "label": 0
                },
                {
                    "sent": "OK so these so notice what I have now what I have here now is a bunch of Boolean variables.",
                    "label": 0
                },
                {
                    "sent": "And at some level at this point it doesn't matter where they came from.",
                    "label": 0
                },
                {
                    "sent": "They just boolean variables and I'm going to build a probability distribution over these over these variables.",
                    "label": 0
                },
                {
                    "sent": "And how do I do that?",
                    "label": 0
                },
                {
                    "sent": "Well, there's going to be a feature for every grounding of every formula, right?",
                    "label": 0
                },
                {
                    "sent": "And feature creates a dependency between the variables that it relates.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to draw an arc between all the predicates that share a formula that appear together in some formula.",
                    "label": 0
                },
                {
                    "sent": "So, for example, this rule smokes xinput.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ask Answer X is gonna leave too if the feature canceran smokes and implies cancer in and therefore an arc in the graph here.",
                    "label": 0
                },
                {
                    "sent": "OK, the same thing for Bob.",
                    "label": 0
                },
                {
                    "sent": "Now this formula had two predicates in it, so it led to a 2A clique.",
                    "label": 0
                },
                {
                    "sent": "This formula has three.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's going to lead to three way cliques, like smokes, Anna Friends and above, and smokes Bob.",
                    "label": 0
                },
                {
                    "sent": "OK, and here's the complete structure of the graph that I've created, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Graph what's the actual distribution where the probability of a world is going to be?",
                    "label": 1
                },
                {
                    "sent": "Again the normalized exponentiated sum over all the formulas of the weight of the Formula Times the number of true groundings of the formula index.",
                    "label": 1
                },
                {
                    "sent": "OK, so now what's going to happen?",
                    "label": 0
                },
                {
                    "sent": "Is that the more smokers have cancer, the more likely the world is going to be, which is exactly what we want.",
                    "label": 0
                },
                {
                    "sent": "We don't want to fall off a Cliff as soon as one smoker doesn't get cancer.",
                    "label": 0
                },
                {
                    "sent": "OK, notice also that the MLN doesn't really just represent one probability distribution.",
                    "label": 0
                },
                {
                    "sent": "It actually represents a family of distributions because in different worlds with different objects I may get very different MLN.",
                    "label": 0
                },
                {
                    "sent": "Some could be very large, some could be very small.",
                    "label": 0
                },
                {
                    "sent": "But what they all have in common is that they're all going to be repetitions of the same templates that are set up by the 1st order formulas.",
                    "label": 0
                },
                {
                    "sent": "OK, and now something that some of you may be thinking is yes, this is all very nice, but this is not going to be remotely practical because I get a combinatorial explosion of groundings when I you know go to any real domain and we're going to stop dead right there and in the.",
                    "label": 0
                },
                {
                    "sent": "A good chunk of what this talk is going to be about is how you can make these things efficient enough to be usable in practice in real domains, and the first one that you can do is something very simple but incredibly useful, which is just have typed variables and constants.",
                    "label": 0
                },
                {
                    "sent": "If I have a predicate like works for XY, it only makes sense to replace X by people.",
                    "label": 0
                },
                {
                    "sent": "And why by organisations?",
                    "label": 0
                },
                {
                    "sent": "Because anything else is a waste of time, so if you do that, that will already greatly cut down on the size of your network and Markov logic, and naturally handled the full range of 1st order logic, including functions, existential quantifiers, infinite and continuous domains.",
                    "label": 1
                },
                {
                    "sent": "They're not the most relevant for us here, so I'm going to skip over them, but you know, you can ask me about them and look it up in the papers if you're interested.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does?",
                    "label": 0
                },
                {
                    "sent": "Markov logic relates to the kinds of statistical models that people use in handling unstructured information.",
                    "label": 0
                },
                {
                    "sent": "Today, like the kinds of things that you know.",
                    "label": 0
                },
                {
                    "sent": "If you pick a random paper from this conference, there's a good chance you find one of those models in there.",
                    "label": 0
                },
                {
                    "sent": "Well, the nice thing is that they are all very direct special cases of Markov logic.",
                    "label": 0
                },
                {
                    "sent": "Things like Markov networks, Markov, random fields, vision networks, log in your models, exponential models, Maxim entropy models, gives distributions, Boltzmann machines, logistic regression, hidden Markov models, conditional random fields.",
                    "label": 1
                },
                {
                    "sent": "Each of these things are field unto its own.",
                    "label": 0
                },
                {
                    "sent": "They're all very direct, special cases of Markov logic and will see a couple of examples of how they are special cases and therefore how you can set them up by writing things in Markov logic the same way that you would write you know statements in SQL or or Datalog or whatever.",
                    "label": 0
                },
                {
                    "sent": "Now there's it's also interesting to know what it is that Markov logic adds to these.",
                    "label": 0
                },
                {
                    "sent": "These models typically all make the assumption that your data is IID.",
                    "label": 0
                },
                {
                    "sent": "Meaning your objects are all independent and identically distributed, meaning the properties of 1 object don't depend on the properties of another, which if you think about it, is a very strange thing, right?",
                    "label": 0
                },
                {
                    "sent": "Again, your probability of smoking depends on whether your friend smoke your probability of having the flu is not just a function of your symptoms, it's also a function of you know whether there's a flu epidemic going around and you've been in contact with somebody who has the flu.",
                    "label": 0
                },
                {
                    "sent": "This is very easy to this is very difficult to express in these kinds of models, but very easy to express in Markov logic as we just saw.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is sort of.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The statistical side, what about the logical side?",
                    "label": 0
                },
                {
                    "sent": "Well, it's probably fairly intuitive that infinite weights give you first order logic as a special case of Markov logic.",
                    "label": 1
                },
                {
                    "sent": "So if I let all my weights go to Infinity, then you pay an infinite penalty for violating a formula and you retrieve back the first of the case and we have a theorem that says you can answer all entailment quiz in first order logic by computing conditional probabilities on the Markov logic network that you get by taking those formulas and giving them infinite weights.",
                    "label": 0
                },
                {
                    "sent": "So this is nice that we have first order logic as a clean special case.",
                    "label": 0
                },
                {
                    "sent": "But of course, we're really interested in the case where the weights are finite.",
                    "label": 0
                },
                {
                    "sent": "So what can we say there?",
                    "label": 0
                },
                {
                    "sent": "Well, if the knowledge base is satisfiable, meaning it's possible to make all the formulas true at the same time, and although it's are positive, which you can always ensure by flipping by negating formulas and their weights.",
                    "label": 0
                },
                {
                    "sent": "The satisfying assignments are the most of the distribution.",
                    "label": 0
                },
                {
                    "sent": "Which means that even in the finite noisy case, the worlds that first order logic likes, there's still in there.",
                    "label": 0
                },
                {
                    "sent": "There the modes of the distribution, and we're going to make very good use of this when it comes time to do inference.",
                    "label": 0
                },
                {
                    "sent": "But of course the most important thing about Markov logic is that, unlike in logic, where if you have a contradiction in the in the knowledge base now pigs fly and anything can happen, Markov logic has no problem with contradictions between formulas.",
                    "label": 0
                },
                {
                    "sent": "If there's a contradiction, Markov logic just waste the evidence on each side.",
                    "label": 0
                },
                {
                    "sent": "And produces a probability for the formula.",
                    "label": 0
                },
                {
                    "sent": "So now it's actually easy to build large knowledge bases because you don't have to.",
                    "label": 0
                },
                {
                    "sent": "At least in this respect.",
                    "label": 0
                },
                {
                    "sent": "Of course, because you don't have to make sure that they're all consistent anymore.",
                    "label": 0
                },
                {
                    "sent": "And more importantly, it's easy to do things like the semantic web where you have contributions from many different people, and there's no hope of ever making the more consistent.",
                    "label": 0
                },
                {
                    "sent": "But you still want to be able to reason with those contributions, so Markov logic, at least at this level, this becomes pretty straightforward.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me talk a little bit about the inference and learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Now of course this would all be only of theoretical interest if we didn't have such efficient inference and learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "And of course there's still a long way to go in making them more efficient.",
                    "label": 0
                },
                {
                    "sent": "But we've also come a very long way already, and we can definitely handle a lot of rules.",
                    "label": 0
                },
                {
                    "sent": "Real world problems with what we have today, so let's.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get that a little bit.",
                    "label": 0
                },
                {
                    "sent": "Let's start by looking at inference.",
                    "label": 0
                },
                {
                    "sent": "One of the most frequent and most useful types of inference that people want to do is what's called MAP maximum a posteriori or MPE.",
                    "label": 0
                },
                {
                    "sent": "Most probable explanation, inference, and this consists of given some evidence.",
                    "label": 0
                },
                {
                    "sent": "Let's call that X.",
                    "label": 0
                },
                {
                    "sent": "Finding the most likely state of the world, I eat the most likely values of another set of variables.",
                    "label": 1
                },
                {
                    "sent": "Why you can think of this as finding the most probable explanation for what you see here are the symptoms.",
                    "label": 0
                },
                {
                    "sent": "What is the most probable disease that explains them?",
                    "label": 0
                },
                {
                    "sent": "For those of you who know, for example, about HMM, this is what the Viterbi algorithm is doing.",
                    "label": 0
                },
                {
                    "sent": "So what does this mean in Markov logic?",
                    "label": 0
                },
                {
                    "sent": "Well, let's replace this peer, why given?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Text by the expression that we saw before, right?",
                    "label": 0
                },
                {
                    "sent": "So we want to find the white that maximizes this.",
                    "label": 0
                },
                {
                    "sent": "This is a constant.",
                    "label": 0
                },
                {
                    "sent": "This is a monotonically increasing function.",
                    "label": 0
                },
                {
                    "sent": "So to maximize this, although you have to Maxim.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is what's inside here.",
                    "label": 0
                },
                {
                    "sent": "OK. What are you?",
                    "label": 0
                },
                {
                    "sent": "What are we doing here?",
                    "label": 0
                },
                {
                    "sent": "We were trying to find the assignment of values to the Boolean variables that maximizes the sum of the weights of the ground formulas that you have.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Surprise, surprise, this is just the weighted Max sat problem, right?",
                    "label": 1
                },
                {
                    "sent": "There's a satisfiability problem which is try to satisfy all formulas.",
                    "label": 0
                },
                {
                    "sent": "You know, the quintessential NP complete problem.",
                    "label": 0
                },
                {
                    "sent": "The Max set problem is try to satisfy as many as possible the Wave Max.",
                    "label": 0
                },
                {
                    "sent": "That problem is when it formula has a weight and you want to maximize the weight of satisfied formulas.",
                    "label": 0
                },
                {
                    "sent": "And the reason this is very nice for us is that we don't have to invent anything new to do inference in Markov logic, we can just use an off the shelf where it SAT solver like for example Max Walk set.",
                    "label": 0
                },
                {
                    "sent": "And we could take advantage of the last 50 years of intensive research on SAT solvers, right?",
                    "label": 0
                },
                {
                    "sent": "These days, SAT solvers can handle problems with millions of variables in minutes or seconds, and we're going to take advantage of that.",
                    "label": 0
                },
                {
                    "sent": "And of all the future improvements of the continuing research on this subject and another sort of like very surprising and nice thing that falls out of this, is that we thought going in that if you think about it, first order logic inference is not renowned for being fast, right?",
                    "label": 0
                },
                {
                    "sent": "And neither is probabilistic inference.",
                    "label": 0
                },
                {
                    "sent": "If you combine the two.",
                    "label": 0
                },
                {
                    "sent": "You probably have a really big headache on your hands, right?",
                    "label": 0
                },
                {
                    "sent": "The surprising thing is that it turns out that using this scheme we can actually do inference in Markov logic faster than inference in first order logic.",
                    "label": 0
                },
                {
                    "sent": "And the reason we can do it faster is that as we'll see shortly, Max works at it's pretty much the exact same algorithm with a very minor change as what you would do is, you know, for satisfiability .1, but .2.",
                    "label": 0
                },
                {
                    "sent": "If I took some of the constraints that were previously hard, that should have been soft.",
                    "label": 0
                },
                {
                    "sent": "And make them soft.",
                    "label": 0
                },
                {
                    "sent": "Now I have an easier problem to solve and therefore I can solve it in fewer in fewer steps.",
                    "label": 0
                },
                {
                    "sent": "So actually turns out they often when we make the first knowledge based soft and their influence on that, we actually save time.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interest I'm in the process, so for those of you not familiar with satisfiability solvers, here's a here's a.",
                    "label": 0
                },
                {
                    "sent": "It actually is easy to explain something like walk sat in just one slide.",
                    "label": 0
                },
                {
                    "sent": "It's incredible how powerful it is given how simple it is.",
                    "label": 0
                },
                {
                    "sent": "So the goal of the walk set is to find an assignment of truth values to Boolean variables.",
                    "label": 0
                },
                {
                    "sent": "That makes all the clauses in a knowledge based true at the same time right?",
                    "label": 0
                },
                {
                    "sent": "And the clause is just a disjunction of literals, and that can convert ineligible to that form.",
                    "label": 0
                },
                {
                    "sent": "So how does it work?",
                    "label": 0
                },
                {
                    "sent": "Very simple.",
                    "label": 0
                },
                {
                    "sent": "I start out by assigning truth values at random to the variables.",
                    "label": 0
                },
                {
                    "sent": "And then I check if all the clauses are satisfied and if they are great, I'm done.",
                    "label": 0
                },
                {
                    "sent": "If they're not satisfied, then I go into this loop where I either flip, I pick an unsatisfied clause.",
                    "label": 1
                },
                {
                    "sent": "And I find that I picked.",
                    "label": 0
                },
                {
                    "sent": "If I flip the variable in it that most increases the number of satisfied clauses, right?",
                    "label": 1
                },
                {
                    "sent": "So this is just the greedy search step I'm trying to satisfy more clauses or to avoid local minima.",
                    "label": 1
                },
                {
                    "sent": "I can instead take a random step where I just flip a random variable in the closet and I will do this after some maximum number of flips and repeat the whole process up to some maximum number of times.",
                    "label": 0
                },
                {
                    "sent": "And once I find a solution, I return it.",
                    "label": 0
                },
                {
                    "sent": "If I don't find a solution at the end of this process, I returned failure.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is works at Max Box at ignore this part here 'cause it's not important is really just the same algorithm, except that instead of flipping the variable that maximizes the number of satisfied formulas, it just maximizes the sum of the weights of the satisfied farmers.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can use this for me in.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's in my car logic.",
                    "label": 0
                },
                {
                    "sent": "There is a problem though, which I already alluded to, which is the memory explosion that you get when you try to ground out your network to then apply the SAT solver too, right?",
                    "label": 0
                },
                {
                    "sent": "Because if you think about it, if there are N constants in your domain and the hyest clause darity meaning the highest number of distinct variables appearing in a clauses C, The ground networks need to require the order fantasy memory.",
                    "label": 1
                },
                {
                    "sent": "OK, so suppose I have 1000 constants, which is not that much and the highest clarity is 3.",
                    "label": 0
                },
                {
                    "sent": "Again, not that much.",
                    "label": 0
                },
                {
                    "sent": "Already I have a billion ground clauses, so we need to do something about this and we have done something about it.",
                    "label": 0
                },
                {
                    "sent": "We can handle things, for example, where every word in the English language is a constant, and the way we do that is by exploiting the sparseness of relational domains.",
                    "label": 0
                },
                {
                    "sent": "If you think about it, there's 6 billion people in the world, so there are 36 billion billion possible groundings of that predicate, right?",
                    "label": 0
                },
                {
                    "sent": "But the vast majority of them are false, 'cause you know, most people only have a few dozen friends, maybe a few 100.",
                    "label": 0
                },
                {
                    "sent": "Maybe thousands at most if you're on Facebook, but that's it.",
                    "label": 0
                },
                {
                    "sent": "OK, so very very sparse.",
                    "label": 0
                },
                {
                    "sent": "Conversely, because of that, most ground clauses are trivially satisfied because their preconditions don't fire right?",
                    "label": 0
                },
                {
                    "sent": "If two people aren't friends, we don't need to worry about whether the smoking habits of running full smoking habits or the other.",
                    "label": 0
                },
                {
                    "sent": "So we've developed a lazy extension of walk set called lazy set that actually exploits this sparseness and increases the size of the means that we can scale to buy.",
                    "label": 0
                },
                {
                    "sent": "Many, many orders of magnitude and I won't go into the details here, but this is certainly an important.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Part of making this practical?",
                    "label": 0
                },
                {
                    "sent": "OK, so the other main inference problem that we usually want to solve is to actually compute the probabilities of some query predicates given some evidence once.",
                    "label": 0
                },
                {
                    "sent": "And the most general questions that you can ask of an MLN is what is the probability that some formula holds OK?",
                    "label": 0
                },
                {
                    "sent": "What is the probability that some formula holds given it some other formula holds and in principle this question is easy to answer because the probability of a formula is just the sum of the probabilities of the world where the formula is true.",
                    "label": 0
                },
                {
                    "sent": "So I can just run over each world, check whether the formula holds, add up the probabilities of the world's the probability of world can of course be computers in the formula and I'm done right?",
                    "label": 0
                },
                {
                    "sent": "However, this is of course not remotely feasible, 'cause there's an exponential number of worlds.",
                    "label": 0
                },
                {
                    "sent": "What we can do though is use a Monte Carlo scheme of 1 sort or another to sample worlds randomly, and then in each rule that I sample, I check whether the formula holds and the fraction of worlds where the formula holds.",
                    "label": 0
                },
                {
                    "sent": "Is my estimate of the probability very well established way to do probabilistic inference?",
                    "label": 0
                },
                {
                    "sent": "And if I'm conditioning on another formula, what I can do is, as I sample worlds, the first thing I do is check if the conditional formula conditioning formula holds.",
                    "label": 0
                },
                {
                    "sent": "If it doesn't hold, I ignore that world and they only count among the remainder.",
                    "label": 0
                },
                {
                    "sent": "And this gives me conditional inference.",
                    "label": 0
                },
                {
                    "sent": "OK, now this may still not be very efficient, because again, you know the network could very easily blow up, right?",
                    "label": 0
                },
                {
                    "sent": "However, there's something else that we can do if it's the case that the conditioning formula is a conjunction of ground atoms, like a set of facts, which is almost always the case, right?",
                    "label": 1
                },
                {
                    "sent": "It's like you know this is what the symptoms are.",
                    "label": 0
                },
                {
                    "sent": "This is through the friends of NR.",
                    "label": 0
                },
                {
                    "sent": "And now you know is there's any smoke?",
                    "label": 0
                },
                {
                    "sent": "Or does Anna have lung cancer?",
                    "label": 0
                },
                {
                    "sent": "And so forth.",
                    "label": 0
                },
                {
                    "sent": "In that case, what we can do is we can first construct the minimum subset of the network that is necessary to answer the query.",
                    "label": 1
                },
                {
                    "sent": "And then we do inference on that network, right?",
                    "label": 0
                },
                {
                    "sent": "This is actually what's called knowledge based model construction and what we've done is generalized some earlier ideas from from vision networks.",
                    "label": 0
                },
                {
                    "sent": "Another thing that you can do that's very exciting that I won't talk about here is do lifted inference the same way that you do lifted inference by Resolution 1st order logic do inference for whole sets of objects at once.",
                    "label": 0
                },
                {
                    "sent": "Potentially even do inference for infinite objects in finite time.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, but just going back to knowledge base model construction, how do you construct your minimum network?",
                    "label": 0
                },
                {
                    "sent": "Very simple idea.",
                    "label": 0
                },
                {
                    "sent": "I'm going to construct the network out from the query notes until I hit evidence.",
                    "label": 0
                },
                {
                    "sent": "So I start out with an empty network and the queue containing the query nodes, and then I repeatedly go to the front of the queue, take a note.",
                    "label": 0
                },
                {
                    "sent": "Add it to the network, check if it's in the evidence.",
                    "label": 0
                },
                {
                    "sent": "If it's in the evidence, I can stop.",
                    "label": 0
                },
                {
                    "sent": "If it's not in the evidence, what I do is like I get its neighbors and add those to the cube and keep going.",
                    "label": 0
                },
                {
                    "sent": "And the hope is that I will bump into a boundary of evidence that separates me from the rest of the world.",
                    "label": 0
                },
                {
                    "sent": "Of course, in the worst case, this is going to choose the whole world, but most of the time it's going to be something much, much smaller in the whole world.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "For example, if I'm asking a question about whether Anna smokes or not, I really need to know only about some small number of people that you might have interacted with, not people across the world who she's never seen.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So then you can do inference by something like Markov chain Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "The most popular algorithm very widely used is something called Gibbs sampling that many of you probably know bout again, joyfully.",
                    "label": 0
                },
                {
                    "sent": "Simple algorithm given how popular it is.",
                    "label": 0
                },
                {
                    "sent": "So here's what you do.",
                    "label": 0
                },
                {
                    "sent": "You start with a random truth assignment and then you repeat the following some number of times or until some convergence criterion is met.",
                    "label": 1
                },
                {
                    "sent": "I go to each variable in turn and I sample and you value for that variable condition on its neighbors.",
                    "label": 0
                },
                {
                    "sent": "And then I just record which fraction of states the formula that I'm interested in was true, and that's my answer.",
                    "label": 1
                },
                {
                    "sent": "OK there.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple idea, very powerful, but again there's a problem which is.",
                    "label": 0
                },
                {
                    "sent": "This is enough to handle the terministic dependency nondeterministic dependences, but if you add deterministic dependences, which is something you want to do in general, this breaks down.",
                    "label": 0
                },
                {
                    "sent": "The reason it's Rick down is that the deterministic dependences break up the state space into regions that don't communicate.",
                    "label": 0
                },
                {
                    "sent": "So if I start my Gibbs sampling over here, it never gets over there and I get the wrong answer.",
                    "label": 0
                },
                {
                    "sent": "And it's actually a longstanding problem in AI.",
                    "label": 0
                },
                {
                    "sent": "We've actually been able to make very good progress on it, using.",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly.",
                    "label": 0
                },
                {
                    "sent": "A combination of MCMC with ideas from satisfiability testing.",
                    "label": 0
                },
                {
                    "sent": "So we developed this algorithm called MC set, where the next sample.",
                    "label": 0
                },
                {
                    "sent": "Instead of being proposed the way I just described is generated by a SAT solver, and so I'm able to very efficiently jump between these modes because this is what satisfiability solvers do well.",
                    "label": 0
                },
                {
                    "sent": "And of course there's some care that has to go into making sure that you still wind up with the right distribution, and again, I won't go into the details here, but this is orders of magnitude faster than Gibbs sampling, and again is one of the things that actually makes this all feasible impact.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now let's talk a little bit about learning.",
                    "label": 0
                },
                {
                    "sent": "You know where?",
                    "label": 0
                },
                {
                    "sent": "Where do these Markov logic networks come from?",
                    "label": 1
                },
                {
                    "sent": "The formulas?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The weights of course, you could just write it all by hand, but most of the time that's probably not what you want to do.",
                    "label": 0
                },
                {
                    "sent": "You might have formulas that you or somebody wrote down and you might want to learn weights for them, or in the most general case you might actually want to learn the formulas themselves, or just take formulas that people contributed to you.",
                    "label": 0
                },
                {
                    "sent": "Again, think of something like the semantic web that may not be entirely wrong, but need to be refined and maybe you can refine them by looking at data.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can think of the data that you're going to learn from as a relational database.",
                    "label": 1
                },
                {
                    "sent": "So if you have a relational database, you can apply Markov logic winning to that before you have isn't structured information, and we're going to see an example of that here.",
                    "label": 0
                },
                {
                    "sent": "What you can do is you can represent it as a database in the most simple minded where you can think of like.",
                    "label": 0
                },
                {
                    "sent": "For example, you can think of free text as just being a sequence of predicates.",
                    "label": 0
                },
                {
                    "sent": "Thing that this token appears in this position in the document.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the data.",
                    "label": 1
                },
                {
                    "sent": "And here in this talk for simplicity I'm going to make the closed world assumption, which is that every Atom.",
                    "label": 1
                },
                {
                    "sent": "Every ground predicate that is not in the database is assumed to be false.",
                    "label": 1
                },
                {
                    "sent": "OK, this is a standard assumption in databases.",
                    "label": 0
                },
                {
                    "sent": "Often it's not appropriate, and when it's not appropriate, what's going to happen is that we're going to need to have EM versions of the algorithms that I'm going to talk about, and those are available in the software, but here to simplify, I'm going to ignore that.",
                    "label": 1
                },
                {
                    "sent": "So two main tasks, as always in machine learning, learning parameters.",
                    "label": 0
                },
                {
                    "sent": "In our case, the weights learning structure.",
                    "label": 0
                },
                {
                    "sent": "In our case, the formulas and the parameters can be learned generatively and discriminatively, and I'm going to briefly.",
                    "label": 0
                },
                {
                    "sent": "Look at each of these in turn.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do I learn weights generatively?",
                    "label": 0
                },
                {
                    "sent": "Well learning which generatively means?",
                    "label": 0
                },
                {
                    "sent": "Finding the weights that are most likely to have generated the database that you're looking at maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, there is no closed form solution right?",
                    "label": 0
                },
                {
                    "sent": "I am going to need to do an optimization process, but Fortunately likelihood is a convex function of the weights, meaning there's a single global optimum.",
                    "label": 0
                },
                {
                    "sent": "I don't have to worry about local optimum, I don't have to worry about initialization OK, and I can use, you know, any gradient descent algorithm and fast 2nd order quasi Newton methods like LBF GS to find the optimal weights and the key thing of course is what is the gradient?",
                    "label": 0
                },
                {
                    "sent": "Here's the gradient.",
                    "label": 0
                },
                {
                    "sent": "The derivative of the log likelihood with respect to await has actually very intuitive form, which is the falling.",
                    "label": 0
                },
                {
                    "sent": "It's the difference between the number of true groundings of the corresponding clause in the data and the expected number of true groundings according to the model.",
                    "label": 1
                },
                {
                    "sent": "So what happens is that if your model is predicting that a clause is true less often than it really is, the weight of that clause needs to go up if it's predicting that it's true more often than it really is, then the way it needs to go down.",
                    "label": 0
                },
                {
                    "sent": "And when all the weights and when all the counts lineup, you've learned the maximum likelihood weights and you can stop.",
                    "label": 0
                },
                {
                    "sent": "OK, so very simple.",
                    "label": 0
                },
                {
                    "sent": "At some level.",
                    "label": 0
                },
                {
                    "sent": "Very intuitive, but there's a big snag here.",
                    "label": 0
                },
                {
                    "sent": "The big snack, usually when you have a powerful language there is a snack somewhere, right?",
                    "label": 1
                },
                {
                    "sent": "The big snack here is that computing these expectations requires inference.",
                    "label": 0
                },
                {
                    "sent": "And you know, inference is still costly no matter how many tricks we do right?",
                    "label": 0
                },
                {
                    "sent": "And if you now have to do inference that every step of a gradient descent procedure, this is going to end up being very, very slow and you know we tried it and it was very, very slow, so we had to look for alternatives.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, they exist one of the alternatives is changing the subject if the thing that you're trying to optimize, it's too hard to optimize, optimize something else that's easier and that hopefully has some correspondence with the first thing.",
                    "label": 0
                },
                {
                    "sent": "And you know, in Markov networks there is this standard approach of.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's called pseudo likelihood pseudo likelihood is just the product overall variables of the probability of the variable condition on the state of its neighbors in the data.",
                    "label": 1
                },
                {
                    "sent": "And the great thing about pseudo likelihood is that you don't need the inference to compute this right.",
                    "label": 0
                },
                {
                    "sent": "This is just Markov blanket, right?",
                    "label": 0
                },
                {
                    "sent": "It's like Gibbs sampling, right?",
                    "label": 0
                },
                {
                    "sent": "And if you combine this with something like LBF GS you actually get quite fast learning and it's a consistent estimated so it has some nice difficult properties.",
                    "label": 0
                },
                {
                    "sent": "So for many problems to the likelihood is the way to go.",
                    "label": 1
                },
                {
                    "sent": "It's widely used in areas like vision, special statistics, some natural language processing as well, and and so forth.",
                    "label": 0
                },
                {
                    "sent": "However, sometimes to the likelihood will give you very bad results.",
                    "label": 0
                },
                {
                    "sent": "And this is not surprising, because if you think about it, so the likelihood is only taking local interactions into account.",
                    "label": 0
                },
                {
                    "sent": "It's not taking long range effects, so you for inference time I need to infer from some variable over there this time variable.",
                    "label": 0
                },
                {
                    "sent": "Over here I'm actually doing something that the pseudo likelihood did not optimize the parameters for.",
                    "label": 0
                },
                {
                    "sent": "OK, so for some problems this will not give good results.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fortunately for those problems, there's another thing that we can use which is very popular machine learning, which is discriminative learning.",
                    "label": 0
                },
                {
                    "sent": "What is discriminative learning?",
                    "label": 0
                },
                {
                    "sent": "Discriminative learning is something that you can do when you know in advance which variables are going to be evidence and which variables are going to be query.",
                    "label": 0
                },
                {
                    "sent": "And that's usually the case and people do discriminative learning in the main reason for this is that it just tends to give better results.",
                    "label": 0
                },
                {
                    "sent": "For us.",
                    "label": 0
                },
                {
                    "sent": "It's going to have the additional nice effect that we can.",
                    "label": 0
                },
                {
                    "sent": "You know there's other approximations that we can.",
                    "label": 0
                },
                {
                    "sent": "Exploit here, so now what we're going to do is we're going to maximize the conditional likelihood of the query given the evidence instead of their joint likelihood, right?",
                    "label": 1
                },
                {
                    "sent": "And already I saved something here, because now I don't need to model dependencies among the evidence variables because the query time they're going to be known and that would be a waste of time.",
                    "label": 0
                },
                {
                    "sent": "OK, and I can just optimize what I really care about, so I have this formula that is pretty much the same as before, except that it depends on X&Y instead of, you know on all the variables in undifferentiated.",
                    "label": 0
                },
                {
                    "sent": "But now there's another nice thing that we can do about this difficult turn here, which is the following.",
                    "label": 0
                },
                {
                    "sent": "The thing that makes inference heart is that there's usually many modes, right?",
                    "label": 0
                },
                {
                    "sent": "Then you need to find them an average over all of them.",
                    "label": 0
                },
                {
                    "sent": "But when you condition on evidence, what usually happens is that many or most of those modes go away.",
                    "label": 0
                },
                {
                    "sent": "In fact, as the condition on more and more evidence, eventually you probably wind up with just one very large mode.",
                    "label": 1
                },
                {
                    "sent": "OK, So what we can do is instead of doing a sum over all possible states, I can just find my most likely state.",
                    "label": 0
                },
                {
                    "sent": "And take the counts in that state as an approximation for the average of the count several States and if that mode is large enough, this actually will work very well, and often it is OK.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this idea was actually originally introduced in something called the voted Perceptron algorithm by Mike Collins, which was for doing, you know, HMM, things like part of speech tagging in natural language and so forth.",
                    "label": 0
                },
                {
                    "sent": "So in the voter perception, of course it's an HMM.",
                    "label": 0
                },
                {
                    "sent": "So what we assume is that the network is a linear chain like this.",
                    "label": 1
                },
                {
                    "sent": "So here are my query variables and here's my evidence.",
                    "label": 0
                },
                {
                    "sent": "And this is a sequential generalization of the good ol perceptron algorithm and the way it works is as follows are initialized, although it's to zero.",
                    "label": 0
                },
                {
                    "sent": "And then I do the following some number of times I find the most likely value of Y given X using the Viterbi algorithm.",
                    "label": 0
                },
                {
                    "sent": "And then I do the difference between the accounts in the data and accounts in that state multiplied by a learning rate update the weights.",
                    "label": 0
                },
                {
                    "sent": "Do this some number of times.",
                    "label": 0
                },
                {
                    "sent": "Average the results because that improves accuracy by fighting over fitting and I'm done.",
                    "label": 0
                },
                {
                    "sent": "OK, now what we would like to have is a similar algorithm that works for them.",
                    "label": 0
                },
                {
                    "sent": "Lens meaning when this structure is not only your chain but is arbitrary.",
                    "label": 0
                },
                {
                    "sent": "And if you think about it, we already have everything we need to do that.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because we have the Max walk set to do MVP inference, all we have to do is go to the voted Perceptron algorithm, replacing interview with Max Walk set and now we have a much more powerful algorithm to learn weights for arbitrary ambulance OK. And this and this is what we do.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let me say a little bit about structure learning.",
                    "label": 0
                },
                {
                    "sent": "Structure learning is the problem of discovering what formulas called from the data or revising correcting formulas that somebody gave you.",
                    "label": 0
                },
                {
                    "sent": "So how might you do that?",
                    "label": 0
                },
                {
                    "sent": "Well, this is if you think about it, the generalization of the well known problem of feature induction in Markov networks.",
                    "label": 1
                },
                {
                    "sent": "It's also if you think about it, a form of inductive logic programming the field of inductive logic programming is concerned with discovering prolog programs from data, discovering horn clauses from a database.",
                    "label": 0
                },
                {
                    "sent": "And this is a form of inductive logic programming, and so we can use all the good techniques that people have developed there with a couple of important differences.",
                    "label": 0
                },
                {
                    "sent": "One is that most of IRP is about learning horn clauses because it's essentially for classification.",
                    "label": 0
                },
                {
                    "sent": "But here we're trying to model the joint distribution of a bunch of variables, so we need to be able to learn any clauses, not just horn clauses.",
                    "label": 1
                },
                {
                    "sent": "This is one thing the other one is that in IRP typically your search is guided by a criterion like accuracy information gain.",
                    "label": 0
                },
                {
                    "sent": "Some combination of this with coverage.",
                    "label": 0
                },
                {
                    "sent": "But here what we're trying to do is learn a probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "So what we should be evaluating our candidates by is likelihood, right?",
                    "label": 0
                },
                {
                    "sent": "We're trying to maximize likelihood it into the deck, which sounds very straightforward until you realize that in order to evaluate the candidates structures so you know, I have my current MLN, and I'm going to try, for example, adding a little to a clause to evaluate how good this is, I have to recompute the weights for the MLN.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to have to recompute the weights every time I propose a new candidate.",
                    "label": 0
                },
                {
                    "sent": "And, you know, I could easily consider millions of candidates, so this isn't going to work.",
                    "label": 0
                },
                {
                    "sent": "OK, So what can we do?",
                    "label": 0
                },
                {
                    "sent": "Well, surprisingly, learning which which can actually turns out to not be the bottom.",
                    "label": 0
                },
                {
                    "sent": "Like if you do a couple of very simple things, one is, you can loosen your convergence thresholds, right?",
                    "label": 0
                },
                {
                    "sent": "I'm just trying to decide which one is best.",
                    "label": 0
                },
                {
                    "sent": "I don't even know exactly what the weights are, right, just enough to pick the right candidate most of the time, and then when I pick that kind of that can really run the process to convergence.",
                    "label": 0
                },
                {
                    "sent": "To make sure I have the right weights.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is one thing.",
                    "label": 0
                },
                {
                    "sent": "The other thing is when I do my knew it optimization, I should just start from the previous weights.",
                    "label": 0
                },
                {
                    "sent": "If I have a new clause, I start with the weight of 04.",
                    "label": 0
                },
                {
                    "sent": "If I'm modifying a clause, I just start with the same weight.",
                    "label": 0
                },
                {
                    "sent": "Because the chances are when I entered Issaquah, this isn't going to affect the width of most other clauses, and so if I use something like LB RTS that converges very fast when you're near the optimum and I start with the current weights off and I will get the right weights in just two or three iterations, so the bottleneck actually turns out to be something else.",
                    "label": 0
                },
                {
                    "sent": "The bottleneck is computing the number of true groundings of each clause, which surprisingly is itself an intractable problem, and in weight learning we only had to do this once at the beginning and then we were done, so it was not too bad, but now I have to.",
                    "label": 0
                },
                {
                    "sent": "This is for every new clothes that I come up with, and again this could get very very expensive.",
                    "label": 0
                },
                {
                    "sent": "But again, there's a there's a set of tricks that you can use to make this much math facture.",
                    "label": 0
                },
                {
                    "sent": "Let me mention only the most important one in terms of how much you gain from it, which is subsampling.",
                    "label": 0
                },
                {
                    "sent": "If I have a clause with the billing your trilling groundings, which can very easily happen, I don't actually need to look at every one of those groundings to estimate the number of programmers I can.",
                    "label": 1
                },
                {
                    "sent": "Random Sample 1000 of the groundings, check those and then extrapolate to the total and this you know again gives you orders of magnitude speed improvement.",
                    "label": 0
                },
                {
                    "sent": "So with these things you can actually do structure learning in Markov logic at about the same speed that you can do traditional inductive logic programming, which truth in advertising is still not all that fast.",
                    "label": 0
                },
                {
                    "sent": "So if all the things that I'm covering here today, the one that.",
                    "label": 0
                },
                {
                    "sent": "It really doesn't quite scale to large ruler problems.",
                    "label": 0
                },
                {
                    "sent": "Is the structure learning, but you know we're working on it.",
                    "label": 0
                },
                {
                    "sent": "Certainly for your doing is just revising a knowledge base and there's not too much revision to do.",
                    "label": 0
                },
                {
                    "sent": "This will probably still be fast enough.",
                    "label": 0
                },
                {
                    "sent": "OK, so now you.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have the usual choices.",
                    "label": 0
                },
                {
                    "sent": "You know it's a search process, so your initial state, your operators, your evaluation function, your search method, your initial state can just be unit clauses, or it can be a hand coded knowledge base.",
                    "label": 1
                },
                {
                    "sent": "Operators are things like available in the software, things like adding and removing literals, flipping signs of literals.",
                    "label": 0
                },
                {
                    "sent": "The evaluation function is pseudo likelihood plus the structure prior, right?",
                    "label": 1
                },
                {
                    "sent": "I need the structure prior.",
                    "label": 0
                },
                {
                    "sent": "Just isn't loading.",
                    "label": 0
                },
                {
                    "sent": "Visionaire was to make sure that I don't over fit, so I penalized for moving away from the initial knowledge base.",
                    "label": 0
                },
                {
                    "sent": "And then various search methods have been proposed by our group and others.",
                    "label": 0
                },
                {
                    "sent": "Things like beam search, shorts for search bottom up learning and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the learning.",
                    "label": 0
                },
                {
                    "sent": "Let me briefly mention the software in which all of this is.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is available, it's called alchemy.",
                    "label": 0
                },
                {
                    "sent": "It's open source.",
                    "label": 0
                },
                {
                    "sent": "It allows you to write down your knowledge or capture it from somewhere in the form of 1st order formulas, and then all the algorithms that I just described for learning and inference are available.",
                    "label": 0
                },
                {
                    "sent": "And in addition you have programming language features like you know file inclusion, typing syntactic sugar for the things that you do most often, the ability to link in external functions and predicates and so forth.",
                    "label": 1
                },
                {
                    "sent": "Here's the URL I will put it up again at the end.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's interesting to compare alchemy with the kinds of things that were available before on both the logical side and the statistical site, so comparing alchemy with Prolog in Prolog, the representation is horn clauses and then alchemy.",
                    "label": 0
                },
                {
                    "sent": "It's arbitrary 1st order formula, so alchemy is more flexible in that respect.",
                    "label": 0
                },
                {
                    "sent": "In Prolog, inference is done by theorem proving, which you know back in the 70s was the fastest form of inference available, but you know these days you know the fastest inferences by.",
                    "label": 0
                },
                {
                    "sent": "Model checking by satisfiability, and that's what we do, but of course most importantly in alchemy.",
                    "label": 0
                },
                {
                    "sent": "You have learning and handling of uncertainty right out of the box, and in Prolog you don't.",
                    "label": 0
                },
                {
                    "sent": "Correct now comparing alchemy with sort of like the software that exists out there for doing things like statistical inference and machine learning.",
                    "label": 0
                },
                {
                    "sent": "There's of course many different packages, no single one that's used for everything, but probably the most popular one by a good margin is something called bugs, meaning you know vision inference using Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "So Bugs uses Bayes Nets as the representation.",
                    "label": 0
                },
                {
                    "sent": "Malcolm uses Markov Nets in bugs influences, then by Gibbs sampling, which we already saw can get very, very slow if you have strong dependencies.",
                    "label": 1
                },
                {
                    "sent": "And how come you have Gibbs sampling in a number of other things, including empty set, which is much, much faster in bugs?",
                    "label": 0
                },
                {
                    "sent": "You can only learn parameters in alchemy.",
                    "label": 0
                },
                {
                    "sent": "You can also in structure, but of course the most important difference is that alchemy is relational.",
                    "label": 0
                },
                {
                    "sent": "In bugs there's no simple general way to handle non IID data in alchemy as we just saw.",
                    "label": 0
                },
                {
                    "sent": "This is very easy.",
                    "label": 0
                },
                {
                    "sent": "So alchemy even looking at it only as logic or only as probability, is already in many ways in advance over the state of the art.",
                    "label": 0
                },
                {
                    "sent": "But the nicest thing of course is that it seamlessly combines these two things.",
                    "label": 0
                },
                {
                    "sent": "You have a problem that has both structured and unstructured aspects.",
                    "label": 0
                },
                {
                    "sent": "You don't have to glue together pieces, you don't have to start with Prolog and try to make it probabilistic or start with bugs and try to handle relational information.",
                    "label": 0
                },
                {
                    "sent": "It's all done very uniformly and smoothly in the language of Markov logic.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So last part of the talk, let me mention some examples of what you can do with Markov logic and in some sense this is the payoff right so far we've been investing in understanding this representation and developing the arguments for it.",
                    "label": 0
                },
                {
                    "sent": "So what is it that you can do with?",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An awful lot.",
                    "label": 0
                },
                {
                    "sent": "It turns out, so here's the here's a sample of some of the things that Markov logic and alchemy, in some cases, other implementations of Markov logic that people have developed, things that have been done.",
                    "label": 0
                },
                {
                    "sent": "Information extraction is 1.",
                    "label": 0
                },
                {
                    "sent": "It's actually example that I'm going to go over here in some detail to illustrate things.",
                    "label": 0
                },
                {
                    "sent": "It's of course a very important problem.",
                    "label": 0
                },
                {
                    "sent": "There was a system by Riedel and client that actually won the Triple L 2005.",
                    "label": 0
                },
                {
                    "sent": "Information extraction competition using Markov logic.",
                    "label": 1
                },
                {
                    "sent": "This was the goal of this was to extract gene interactions from from Medline abstracts into resolution.",
                    "label": 0
                },
                {
                    "sent": "Another very important problem link prediction collective classification.",
                    "label": 1
                },
                {
                    "sent": "Things like classifying webpages, using their links as well as their content, various kinds of web mining, natural language processing.",
                    "label": 1
                },
                {
                    "sent": "I just came from the MLP where we had the paper applying Markov logic to natural language processing ontology refinement.",
                    "label": 1
                },
                {
                    "sent": "Another very important problem.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned the best paper award at CHM last year.",
                    "label": 0
                },
                {
                    "sent": "As a paper by Fei Wu and and then, well that use Markov logic to learn and refine ontologists from from Wikipedia and then other things like com, bio, social network analysis, and so forth.",
                    "label": 0
                },
                {
                    "sent": "Here's a couple of examples that I think worth mentioning, Psych, which some of you know is the world's largest knowledge base, right, but didn't exactly succeed because of the problems in first order logic.",
                    "label": 0
                },
                {
                    "sent": "The folks at Sitecore have begun to transform Psych into into an MLN by adding weights to their formulas.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Again, very straightforward thing to do.",
                    "label": 0
                },
                {
                    "sent": "And another one is at Caillou, the largest project in Darby History which is trying to build this automated personal assistant that manage is your email and your meetings for you and automatically files things in folders and what not.",
                    "label": 0
                },
                {
                    "sent": "The main inference engine that Caillou uses is based on Markov logic, and there's more of these things which hopefully and again you can go to the alchemy website and there's you know, a bunch of pointers to papers and applications and millions and databases and what not there, but you know, hopefully this will persuade you that this really is something that you can.",
                    "label": 0
                },
                {
                    "sent": "Do state of the art solutions for many important problems in information and knowledge Management Today.",
                    "label": 0
                },
                {
                    "sent": "So for concreteness, let me.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last thing in this talk, before I conclude, just show you how we can do something like information extraction in Markov logic, right?",
                    "label": 0
                },
                {
                    "sent": "And I think this is a nice illustration because information extraction is your quintessential bridge between the structure and the instruction, right?",
                    "label": 0
                },
                {
                    "sent": "You start out with in structured it's just text and you populate a database and how you can do that.",
                    "label": 0
                },
                {
                    "sent": "It based inference.",
                    "label": 0
                },
                {
                    "sent": "Then you know logical inference over it, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to use as an example the problem of extracting a database of citations from the reference lists in papers, right?",
                    "label": 0
                },
                {
                    "sent": "This is the problem that sites here.",
                    "label": 0
                },
                {
                    "sent": "And Google Scholar has to solve very problem.",
                    "label": 0
                },
                {
                    "sent": "Very popular problem among researchers today.",
                    "label": 0
                },
                {
                    "sent": "So here's a very small example and now.",
                    "label": 0
                },
                {
                    "sent": "You need to do 2 main.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As an information extraction, the first one is what's called segmentation.",
                    "label": 0
                },
                {
                    "sent": "I need to run over the text string and figure out where the fields of my database are.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example, perox, England and painted them Ingles are authors.",
                    "label": 0
                },
                {
                    "sent": "Tripoli 06 is a venue and so forth, so this is the first part, it's segmentation.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other part is internal resolution.",
                    "label": 0
                },
                {
                    "sent": "I need to figure out that triple your six and the proceedings of the 21st National Conference on Artificial Intelligence are the same thing, right?",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then I also need to figure out that these two papers are the same thing.",
                    "label": 0
                },
                {
                    "sent": "These two papers are the same and these two papers are the same.",
                    "label": 0
                },
                {
                    "sent": "Otherwise when I query my database, I'm going to get duplicate results right?",
                    "label": 0
                },
                {
                    "sent": "Which should probably seen happen when you get, you know, duplicate junk mailings to you with different misspellings of your name and so forth.",
                    "label": 0
                },
                {
                    "sent": "And if I try to do mining on top of this and the data is not properly resolved, the results will be garbage.",
                    "label": 0
                },
                {
                    "sent": "OK, so we need to be able to do this.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two things and the state of the art is basically as follows.",
                    "label": 1
                },
                {
                    "sent": "For segmentation, people use something like a hidden Markov model or a conditional random field to assign.",
                    "label": 0
                },
                {
                    "sent": "Each took into a field like you could run over the the input string into go like OK.",
                    "label": 0
                },
                {
                    "sent": "Author, author, author, author, title, title, title, title, venue.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is the segmentation part and then for the entity resolution people typically use the classifier like say logistic regression, to predict for each pair of fields, whether they're the same.",
                    "label": 0
                },
                {
                    "sent": "Right are these two the same people?",
                    "label": 0
                },
                {
                    "sent": "And for each pair of papers, whether they're the same right and then they do some kind of transitive closure?",
                    "label": 0
                },
                {
                    "sent": "Right, because if A is the same as B&B is the same as C, then a is the same as see OK and of course I'm exemplifying this here for the problem of citation matching, but you can imagine doing this for any other field like you know, extracting city names and people names from the news and so forth.",
                    "label": 0
                },
                {
                    "sent": "Now if you did this today, as you know, people at you know lots of companies like Yahoo and Google and Amazon and so forth.",
                    "label": 0
                },
                {
                    "sent": "Do and roll your own information extraction system.",
                    "label": 0
                },
                {
                    "sent": "This is going to take a long time, and it's probably going to run into the 10s of thousands of Java code and once it put in the bells and whistles even more in alchemy, you can implement the complete information extraction system in just seven formulas that actually fit on a single slide that I'm going to put up shortly.",
                    "label": 0
                },
                {
                    "sent": "So you get several orders of magnitude improvement in how quick and simple it is to program a complex, you know, application dealing with structure than.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And my structured information.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's how we do that.",
                    "label": 0
                },
                {
                    "sent": "Fields.",
                    "label": 0
                },
                {
                    "sent": "In this case, I think, like Arthur, title, venue, and there's also citations in positions.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to put this here for clarity, but you could actually infer them from.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the input database, now predicates as evidence.",
                    "label": 0
                },
                {
                    "sent": "I'm going to have just the predicate token, which says that this token appears in this position in this citation.",
                    "label": 0
                },
                {
                    "sent": "So I process the text into this form and that's why what's going to drive my.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inference, and now it's query predicates.",
                    "label": 0
                },
                {
                    "sent": "I'm going to have three things in field which says that this position in this field is part of this citation.",
                    "label": 0
                },
                {
                    "sent": "Sorry this position in this citation is part of this field.",
                    "label": 0
                },
                {
                    "sent": "So for example, position one in citation one is part of the author field.",
                    "label": 0
                },
                {
                    "sent": "OK, so this query predicate is going to do the segmentation for me.",
                    "label": 0
                },
                {
                    "sent": "Once I've inferred the truth files of this predicate, I will know which tokens belong to which fields, and then these two predicates.",
                    "label": 0
                },
                {
                    "sent": "These to query predicates do the entire resolution.",
                    "label": 0
                },
                {
                    "sent": "Same field says that for example, the authors field in Citation 1.",
                    "label": 0
                },
                {
                    "sent": "Citation two is the same or not, right?",
                    "label": 0
                },
                {
                    "sent": "Did the same people piece England proxying are the same person and same site just means that the two citations are the same?",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm going to do is I'm going to write an MLN that takes this and infers these.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the MLN.",
                    "label": 0
                },
                {
                    "sent": "As advertised, just 7 short formulas and these three formulas here do the segmentation and these four do they do the entity resolution OK?",
                    "label": 0
                },
                {
                    "sent": "And here's how this works.",
                    "label": 0
                },
                {
                    "sent": "These three formulas are all it takes to implement any.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Umm, and it works as follows.",
                    "label": 0
                },
                {
                    "sent": "This first formula says that if I have a certain token in a certain position, then that position is in a certain field and as in prolog, all free variables are universally quantified.",
                    "label": 0
                },
                {
                    "sent": "So this is going to ground out to every field and every token etc.",
                    "label": 0
                },
                {
                    "sent": "And this little plus notation is a nice piece of syntactic sugar that says we want to learn a different weight for every combination of these variables that have pluses.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to learn a different, which for each combination of token and field.",
                    "label": 0
                },
                {
                    "sent": "Which means that what this formula is going to represent is exactly.",
                    "label": 0
                },
                {
                    "sent": "The observation matrix of a hidden Markov model where the observations are tokens and the states are fields.",
                    "label": 0
                },
                {
                    "sent": "So what these words are going to capture is the correlations between tokens and fields.",
                    "label": 0
                },
                {
                    "sent": "Conference is probably indicative of venue.",
                    "label": 0
                },
                {
                    "sent": "Smith is probably indicative of author etc etc.",
                    "label": 0
                },
                {
                    "sent": "There is problem not indicative of much of anything, so that will have a little weight.",
                    "label": 0
                },
                {
                    "sent": "So this is the observation made.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tricks?",
                    "label": 0
                },
                {
                    "sent": "This here is the transition matrix.",
                    "label": 0
                },
                {
                    "sent": "It just says that if you're in the field.",
                    "label": 0
                },
                {
                    "sent": "So soon you author Field, then in the next position you're likely to be in the author field as well.",
                    "label": 0
                },
                {
                    "sent": "In Journal you could capture the complete correlation between fields, but for information extraction purposes the only thing you really need to capture is that you tend to stay in the same field.",
                    "label": 0
                },
                {
                    "sent": "So here I have F&F, but if I wanted the general transition matrix I would just have.",
                    "label": 0
                },
                {
                    "sent": "If you're in field F at position I, UN field F prime at position I plus one and then again I would have the complete matrix, OK.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this formula just says that I can't be in more than one field at once, and this is actually formula that's not needed.",
                    "label": 0
                },
                {
                    "sent": "The reason we're putting it here is that we can actually handle things that are not in any field without having to introduce another field just by allowing things to not be in any field.",
                    "label": 0
                },
                {
                    "sent": "But then we still have to say that you can't be in more than one field at the same time.",
                    "label": 0
                },
                {
                    "sent": "OK, so this does the information extraction.",
                    "label": 0
                },
                {
                    "sent": "There's the segmentation part of the information extraction using HMM.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's look at how we do the entity resolution.",
                    "label": 0
                },
                {
                    "sent": "So here's a formula that predicts whether or not two fields are the same from whether or not they contain the same tokens.",
                    "label": 0
                },
                {
                    "sent": "If the field content if a citation contains a certain token, and, say, the author field, and so does another citation, then this makes them more likely to be the same field.",
                    "label": 0
                },
                {
                    "sent": "Right field share words.",
                    "label": 0
                },
                {
                    "sent": "They are more likely to be the same, and again, I can learn weights for these because some words will be much more informative than others, so one way to think of this formula is as a similarity comparison between fields.",
                    "label": 0
                },
                {
                    "sent": "Another one is to see this as a logistic regression.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "This formula is ridiculously a logistic regression, where this is the class variable and these are the features, right?",
                    "label": 0
                },
                {
                    "sent": "So you can set up logistic regression by defining the features using these formulas, and then you know an application to the class.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a very little formula that does a lot of work.",
                    "label": 0
                },
                {
                    "sent": "It says that if the fields are the same, then the citations are the same and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So if the author field is the same and the venue field is the same and the title for this, and then probably this is the same citation and again certain authors might have a higher weight than for example, venues and titles would probably have an even higher weight, right?",
                    "label": 0
                },
                {
                    "sent": "But notice that I've put an equivalence here, which means that you can also do the other way.",
                    "label": 0
                },
                {
                    "sent": "You can say if the citations are the same, then the fields are the same and this is incredible useful for the following reason.",
                    "label": 0
                },
                {
                    "sent": "Remember the example with Tripoli eye and the National Conference on Artificial Intelligence Traditional into resolution systems will choke on this because there's no similarity between those two strings at all.",
                    "label": 0
                },
                {
                    "sent": "But if I have two papers that I conclude are the same because the authors in the titles are the same and one of them is in triple AI and the other one is in the national Conference, probably from that I can conclude that those two things are the same, and now this becomes available for a bunch of other inferences.",
                    "label": 0
                },
                {
                    "sent": "And now maybe I can decide that certain papers are the same that I couldn't before.",
                    "label": 0
                },
                {
                    "sent": "And you know this.",
                    "label": 0
                },
                {
                    "sent": "Will you know this will propagate and potentially give me much better results than it would otherwise?",
                    "label": 0
                },
                {
                    "sent": "And indeed, you know we've had papers or we will show exactly that.",
                    "label": 0
                },
                {
                    "sent": "This works very well.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally these two formulas just do the transitive closure for you.",
                    "label": 0
                },
                {
                    "sent": "This formula says if field sees the field, F is the same in C as in C prime, and it's same as in C prime is in C prime, and this is the same as in C as in C plus prime right?",
                    "label": 0
                },
                {
                    "sent": "And this is just the same thing for citations and the beautiful thing about this is that.",
                    "label": 0
                },
                {
                    "sent": "There have been no in the papers on different ethnic waits to incorporate transitive closure into anti resolution.",
                    "label": 0
                },
                {
                    "sent": "If years ago, for example, there was a widely cited paper by McCallum and Wellner which was how do we extend the CRF to do transitive closure for Inter resolution and they had to develop their representation of this inference.",
                    "label": 0
                },
                {
                    "sent": "Learning algorithms, you know was quite an achievement at the time.",
                    "label": 0
                },
                {
                    "sent": "In Markov logic.",
                    "label": 0
                },
                {
                    "sent": "All that you have to do is write down the transitivity axiom from logic.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "This is the accent of transitivity.",
                    "label": 0
                },
                {
                    "sent": "We know that you know the equality is a transitive relation.",
                    "label": 0
                },
                {
                    "sent": "Put that in the MLN and everything goes through OK, so.",
                    "label": 0
                },
                {
                    "sent": "This thing is actually, believe it or not, just these four formulas are actually pretty state of the art into the resolution system.",
                    "label": 0
                },
                {
                    "sent": "The segmentation part, however, is not so great.",
                    "label": 0
                },
                {
                    "sent": "The reason it's not so great is that it's not very good at telling where the boundaries of fields are.",
                    "label": 0
                },
                {
                    "sent": "It can propagate author, author, author, but it doesn't exactly know where the author ends in the title begins, and people have known this information extraction and what they usually do is they have rules for deciding where field begins and where field ends, like for example the whole area of wrapper, induction for extracting things from web pages based on this idea.",
                    "label": 0
                },
                {
                    "sent": "OK, now that's sort of like a rule based approach, and then there's sort of like the CRF type.",
                    "label": 0
                },
                {
                    "sent": "Properly, you know probabilistic approaches.",
                    "label": 0
                },
                {
                    "sent": "What we actually like to do is to combine both.",
                    "label": 0
                },
                {
                    "sent": "Now in Markov logic you can combine both very very simply.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, in the case of citation matching, we know that there's a really good predictor of where field ends under another one begins, which is a period right in citations.",
                    "label": 0
                },
                {
                    "sent": "Usually, you know at the end of the authors appeared and then disappeared at the end of title.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we can just modify this rule that says contiguous positions like to be in the same field, unless one of them is it.",
                    "label": 0
                },
                {
                    "sent": "OK, so the period breaks the propagation of same field and we just this little change.",
                    "label": 0
                },
                {
                    "sent": "Suddenly we get much, much better result.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example on the corner database of computer science papers.",
                    "label": 0
                },
                {
                    "sent": "Standard data set for these problems.",
                    "label": 0
                },
                {
                    "sent": "These are precision recall curves, so I would like to be up here.",
                    "label": 0
                },
                {
                    "sent": "Here's what happens when I only use the token information.",
                    "label": 0
                },
                {
                    "sent": "Here's what happens when I use sequence information, and when I add that little modification with the period, suddenly I get to this yellow curve and you find out, do the same thing for, which of course is a noisy signal but still useful signal.",
                    "label": 0
                },
                {
                    "sent": "I'm almost up there, OK?",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So and you know, again, we also get very good results on matching venues with a few more formulas, we get the state of the art system that was developed by a student in three months as opposed to five people over two years throwing everything you know.",
                    "label": 0
                },
                {
                    "sent": "But the kitchen sink.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me just briefly.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclude.",
                    "label": 0
                },
                {
                    "sent": "The structure then structured information spectrum has exploded in the last 20 years used to have either structured or unstructured information and languages for each of those and inference methods for them.",
                    "label": 0
                },
                {
                    "sent": "Now we have the whole spectrum and we need the language.",
                    "label": 0
                },
                {
                    "sent": "We need languages that can handle that in a uniform, general and simple way.",
                    "label": 1
                },
                {
                    "sent": "Markov logic is one such possible language that I described here.",
                    "label": 0
                },
                {
                    "sent": "We saw how it works, the different inference and learning algorithms for it, and what you can do with it.",
                    "label": 1
                },
                {
                    "sent": "There is of course much research left to do.",
                    "label": 0
                },
                {
                    "sent": "In terms of scaling up, difference in learning, making the algorithms more robust, right things are not yet at the push button stage where you push the button and things run.",
                    "label": 0
                },
                {
                    "sent": "You have to play with parameters.",
                    "label": 0
                },
                {
                    "sent": "Would like to make that easier, to the point where things can be used by non experts.",
                    "label": 1
                },
                {
                    "sent": "And of course there's many new applications to try and more generally I think what this points to that's very exciting is really a new way of doing computer science, which is instead of having to program everything that you do, you just define the brush structure of what you want to do is in the information extraction example.",
                    "label": 0
                },
                {
                    "sent": "And then you let the data and the probabilistic inference do the hard lifting.",
                    "label": 0
                },
                {
                    "sent": "You know to do the rest of the work again.",
                    "label": 1
                },
                {
                    "sent": "As I said, this is all available at the alchemy website datasets, MLN software, tutorials, etc.",
                    "label": 0
                },
                {
                    "sent": "So I encourage you to try it out.",
                    "label": 0
                },
                {
                    "sent": "It's at this URL and thank you very much.",
                    "label": 0
                },
                {
                    "sent": "We talked about this once before.",
                    "label": 0
                },
                {
                    "sent": "Ron Baker, my HP laptop.",
                    "label": 0
                },
                {
                    "sent": "Distributed or parallel inference procedure randecker manageable as highly considering distributed parallel inference procedures in Marklogic network.",
                    "label": 0
                },
                {
                    "sent": "Because you know, they love their theoretical properties of the network and can be used absolutely.",
                    "label": 0
                },
                {
                    "sent": "So one of the ways to scale this up to you know web scale has to be going parallel and the publicly available Alchemist software is purely sequential at this point.",
                    "label": 0
                },
                {
                    "sent": "But we already have internal parallel versions of certain things like inference and structure learning, and I am actually collaborating with Carlos Guestrin who's done a lot of work on inference.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic inference on multicore and cluster machines to paralyze alchemist.",
                    "label": 0
                },
                {
                    "sent": "So we expect this to become available in the foreseeable future.",
                    "label": 0
                },
                {
                    "sent": "And indeed it's a very important thing to do.",
                    "label": 0
                }
            ]
        }
    }
}