{
    "id": "bfzvv3cbcfwplp63brcbqfdsaabhmjq7",
    "title": "A Novel Approach for Efficient SVM Classification with Histogram Intersection Kernel",
    "info": {
        "author": [
            "Gaurav Sharma, Technicolor S.A."
        ],
        "published": "April 3, 2014",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/bmvc2013_sharma_novel_approach/",
    "segmentation": [
        [
            "Hello everyone I'm gonna Sharma and this is joint work with Frederick jury."
        ],
        [
            "So this is a brief outline.",
            "First I will give some motivation and context and then I will give the problem we address and then I'll discuss some very closely related works and then I'll give the proposed method ending with experimental results and some conclusions."
        ],
        [
            "So the topic of this talk is about histograms and SVM's.",
            "So why are we interested in these two?",
            "Well, in many classic computer vision problems, we end up with the features being histograms such as in scene classification and object image classification.",
            "We have bag of features which are histograms and then facial analysis and texture analysis.",
            "We have local patterns which are also histograms and many successful approaches to these problems.",
            "Use histograms with the corresponding histogram intersection kernel.",
            "With the SVM classifier.",
            "So hence we are interested in these."
        ],
        [
            "And the present talk.",
            "We will be particularly addressing the time and space efficiency of this setup.",
            "Soil."
        ],
        [
            "To set some context will be working with the SVM formulation and this is the primal formulation.",
            "I'm using very standard notations here, so XI is input vector.",
            "The images have already been represented.",
            "This is these are histograms, CII, pariza, label, vector and training is a vector and labeled pair which is given as a training set.",
            "W is the normal to the hyperplane, it is the parameter of the classifier which we are interested in learning.",
            "And I here in this formulation or slack variables which address the non separable case.",
            "So the basic problem, the basic optimization here is we are minimizing the classifier energy while trying to respect the constraints as much as possible."
        ],
        [
            "This is the dual formulation of the same primal assume optimization and this is more interesting and it has been."
        ],
        [
            "Often used more because it primarily depends on dot products between input vectors.",
            "So in the optimization, Now the dual variables are Alpha.",
            "We are interested in optimizing on alphas and everything else is given and the optimization depends on dot products only of the input vectors."
        ],
        [
            "And usually the classification problems in computer vision are quite complex and a linear decision boundary which is learned by the classic extreme is not sufficient.",
            "So to introduce linearity we use the so called feature map with corner Trick.",
            "So what we do is that we take the input space Rd which is shown on the left, and we use Phi an map it to a high dimensional feature space.",
            "And we hope that after this mapping, the points will be linearly separable in the high dimensional space.",
            "So in the dual we replace the exercise with fire of excise and again the dual only depends on the dot products between these vectors."
        ],
        [
            "Which enables us to use the kernel trick.",
            "So what we do is we define a kernel function between the input vectors which correspond to the DOT products in the feature space between the same feature map vectors.",
            "Now the optimization you can replace the DOT products with the kernel evaluation and you can efficiently solve this for alphas."
        ],
        [
            "But the problem with this with this method is that at Test time, the score of a new test vector which is given here by F of X.",
            "For calculate the score, you use the expression given here and you have to calculate the kernel function for this test vector with all the support vectors.",
            "And usually this support vectors are of the order of the number of training vectors, and so it is an expensive operation to score a new test example and the complexity is D times the number of support vector, where D is the dimensionality of the input space."
        ],
        [
            "So in the current, we would like to do this efficiently, so we propose to optimize the primal formulation of SVM in Rd and this primal corresponding to the histogram intersection kernel SVM indirectly, the input space without using the kernel trick or the feature map, and this will show that this leads to a fast classifier with order D test complexity."
        ],
        [
            "So first we discuss two of closely related works, so Maji at all proposed to efficiently calculated, calculate the test scoring for the histogram intersection kernel.",
            "So the same expression if you expand it, you will see that it basically depends on two submissions.",
            "The outer summation is on support vectors and the inner summation is on dimensions.",
            "So what they say that you just exchange the order of this summations and now for if you fix a dimension, you sort the support vectors in increasing order and now you just for the test vector you just want to place it at the right place in this sorted order and once you place it, you know that all the support vector on the left of it would be less.",
            "So you can cash some values and you can compute the same expression more efficiently.",
            "So the complexity, the number of support vector part of the complexity is basically replaced by a binary search and the overall test complexity becomes order of D. Cross log of NSV.",
            "And then they also propose to do some piecewise constant or linear approximations, and with this they again gain some more at Test efficiency."
        ],
        [
            "And the second approach is by Italians of seven.",
            "In this, they propose that we can directly obtain an approximate feature map for certain class of kernels.",
            "The generative kernels intersection, Hellinger and Chi Square are among these kernels.",
            "So now at Test time, what will happen at train time?",
            "What will happen?",
            "You will take all the vectors.",
            "You will explicitly map them to in a larger dimension.",
            "You will learn a linear SVM there and then at Test time when you get the test vector, you take take the test vector, map it again in the space and then use the linear SVM that you learned in that space to calculate the score.",
            "So here the test complexity is ordered.",
            "The dash where D dash is the dimension of the feature space, which is higher than the dimension of the input space."
        ],
        [
            "Now we come to the proposed method, and I again remind you that the dual is of this form and the scoring function is it depends on kernel computation with all the support vectors."
        ],
        [
            "If you look at the corresponding primal of in the feature space.",
            "Which is never really solved.",
            "We always solve the dual.",
            "This is the expression for the primal I have written the objective in in unconstrained form, and this is equivalent to the constraint form that I showed before.",
            "And the score calculation is a dot product between W 5 which is the linear decision boundary in the feature space.",
            "And this dot product is never computed explicitly.",
            "This is usually computed using the kernel kernel trick, right?",
            "With the alphas which are obtained by optimizing this function.",
            "So the keys."
        ],
        [
            "Type in the approaches.",
            "Assume that we have a pre image of this decision boundary, right?",
            "So we have a W vector in the input space for which 5 W is either directly equal to Wi-Fi and in a more general case is the closest approximation as in general the preimage of any vector in a feature space might not exist in input space.",
            "So if we do this approximation, we can rewrite the primal objective.",
            "And now it will now."
        ],
        [
            "Sure that this objective will only depend on this W, which is the preimage of the decision boundary in the feature space.",
            "So if we use the property of the intersection kernel, the regularization term can be reduced to a simple L1 regularization on W and then the score function.",
            "We can again use the histogram intersection kernel and the score function can be written in this form.",
            "So what we propose here is that we would like to solve this optimization directly in input space without any feature mapping or kernel trick."
        ],
        [
            "So this objective function is definitely non convex, But the good thing is it is still quasi convex so quasi convexity is that the function.",
            "It would have some local flat areas, so I won't give the proof here.",
            "The details are in the paper, but you can see you can have an intuition using at looking at the decision functions in one and 2D."
        ],
        [
            "One way of solving such problem would be by doing successive relaxations.",
            "So here in Blue I show the decision function corresponding to a fixed X.",
            "So the Y value is F of W. Sorry, this is a scoring function, so the Y value is F of the X axis is W and I've fixed X which is marked.",
            "The blue curve is the scoring function corresponding to the intersection kernel.",
            "The green curve is the scoring function corresponding to the linear kernel.",
            "So if we if we work with a linear kernel, the problem becomes convex.",
            "So what we propose is that you start optimizing with the linear kernel.",
            "And then successively bend the curve to match the blue curve.",
            "So the red would be an intermediate relaxation of this decision function.",
            "This would be one way of solving the problem.",
            "In"
        ],
        [
            "Practice, we find that a simple stochastic subgradient descent based solver works quite well.",
            "One of the subgradient of the objective is quite easy to define analytically.",
            "The expression is given here.",
            "And then in the final optimization algorithm we use stochastic subgradient descent with the approximate form of this subgradient."
        ],
        [
            "So I show some experimental results here."
        ],
        [
            "So we use classic computer vision databases.",
            "So the first one is the scene 15 database, so it has 15 scene categories with about 4500 images.",
            "It has a standard protocol on which you can compare with existing methods and we use the stand."
        ],
        [
            "Protocol.",
            "The second data set is Pascal.",
            "We OC 2007.",
            "Object Image classification task and there again 20 object classes.",
            "And there is a standard train well and test set and you can compare with other method using average precision for each class and mean average precision for the whole data set."
        ],
        [
            "So we use a very standard pipeline for the features, so we use VL feat library with multiscale dense sift features and a bag of features where codebook is generated using K means.",
            "We use hard quantization and we fix the vocabulary size to be 1024.",
            "So we fixed this feature representation and then we we compare the learning algorithms."
        ],
        [
            "The first set of results we show is on the scene 15 data set, so we compare with the feature map method which has already been shown to be much faster than either doing column map or doing the method of Margie at all, which I discussed before.",
            "So in terms of accuracy, we see that we do essentially the same on this database and then."
        ],
        [
            "Then on the next video, say 2007, database again in terms of performance.",
            "In this case mean average precision.",
            "We do essentially the same.",
            "And then in terms of."
        ],
        [
            "Time and memory in time and space efficiency we compare with feature map again with the typical parameter setting which gave the best performance in the original paper and we see that we are about an order faster in time and the method leads to a substantial amount of memory reduction as well.",
            "Again, compared to feature map in feature map for testing as well, you have two things.",
            "You are first mapping it into a feature space so you are taking some time there and then you're doing a dot product in this higher dimensional feature space.",
            "While in our case we're just doing a linear scan and we're doing a slightly more costly operation compared to a dot product."
        ],
        [
            "And then we evaluated the convergence for different parameter values.",
            "So we see that the method seems to converge for a range of different parameter values, but cross validation is suggested as always."
        ],
        [
            "So to conclude, we presented a method to learn SVM with the histogram intersection kernel by optimizing directly in the input space we we proposed a stochastic subgradient based learning method which is quite efficient.",
            "It takes less than a minute to train one class of Pascal Vuosi classification task.",
            "It leads to a very efficient, very nice and efficient test time classification time.",
            "And then for future work we would like to explore better training algorithms.",
            "We would also see how it generalizes to across different kernel classes and then perhaps apply it on more applications."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everyone I'm gonna Sharma and this is joint work with Frederick jury.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a brief outline.",
                    "label": 0
                },
                {
                    "sent": "First I will give some motivation and context and then I will give the problem we address and then I'll discuss some very closely related works and then I'll give the proposed method ending with experimental results and some conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the topic of this talk is about histograms and SVM's.",
                    "label": 0
                },
                {
                    "sent": "So why are we interested in these two?",
                    "label": 0
                },
                {
                    "sent": "Well, in many classic computer vision problems, we end up with the features being histograms such as in scene classification and object image classification.",
                    "label": 1
                },
                {
                    "sent": "We have bag of features which are histograms and then facial analysis and texture analysis.",
                    "label": 0
                },
                {
                    "sent": "We have local patterns which are also histograms and many successful approaches to these problems.",
                    "label": 1
                },
                {
                    "sent": "Use histograms with the corresponding histogram intersection kernel.",
                    "label": 0
                },
                {
                    "sent": "With the SVM classifier.",
                    "label": 0
                },
                {
                    "sent": "So hence we are interested in these.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the present talk.",
                    "label": 0
                },
                {
                    "sent": "We will be particularly addressing the time and space efficiency of this setup.",
                    "label": 1
                },
                {
                    "sent": "Soil.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To set some context will be working with the SVM formulation and this is the primal formulation.",
                    "label": 0
                },
                {
                    "sent": "I'm using very standard notations here, so XI is input vector.",
                    "label": 0
                },
                {
                    "sent": "The images have already been represented.",
                    "label": 0
                },
                {
                    "sent": "This is these are histograms, CII, pariza, label, vector and training is a vector and labeled pair which is given as a training set.",
                    "label": 0
                },
                {
                    "sent": "W is the normal to the hyperplane, it is the parameter of the classifier which we are interested in learning.",
                    "label": 1
                },
                {
                    "sent": "And I here in this formulation or slack variables which address the non separable case.",
                    "label": 0
                },
                {
                    "sent": "So the basic problem, the basic optimization here is we are minimizing the classifier energy while trying to respect the constraints as much as possible.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the dual formulation of the same primal assume optimization and this is more interesting and it has been.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Often used more because it primarily depends on dot products between input vectors.",
                    "label": 1
                },
                {
                    "sent": "So in the optimization, Now the dual variables are Alpha.",
                    "label": 0
                },
                {
                    "sent": "We are interested in optimizing on alphas and everything else is given and the optimization depends on dot products only of the input vectors.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And usually the classification problems in computer vision are quite complex and a linear decision boundary which is learned by the classic extreme is not sufficient.",
                    "label": 0
                },
                {
                    "sent": "So to introduce linearity we use the so called feature map with corner Trick.",
                    "label": 0
                },
                {
                    "sent": "So what we do is that we take the input space Rd which is shown on the left, and we use Phi an map it to a high dimensional feature space.",
                    "label": 0
                },
                {
                    "sent": "And we hope that after this mapping, the points will be linearly separable in the high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So in the dual we replace the exercise with fire of excise and again the dual only depends on the dot products between these vectors.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which enables us to use the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we define a kernel function between the input vectors which correspond to the DOT products in the feature space between the same feature map vectors.",
                    "label": 0
                },
                {
                    "sent": "Now the optimization you can replace the DOT products with the kernel evaluation and you can efficiently solve this for alphas.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the problem with this with this method is that at Test time, the score of a new test vector which is given here by F of X.",
                    "label": 1
                },
                {
                    "sent": "For calculate the score, you use the expression given here and you have to calculate the kernel function for this test vector with all the support vectors.",
                    "label": 1
                },
                {
                    "sent": "And usually this support vectors are of the order of the number of training vectors, and so it is an expensive operation to score a new test example and the complexity is D times the number of support vector, where D is the dimensionality of the input space.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the current, we would like to do this efficiently, so we propose to optimize the primal formulation of SVM in Rd and this primal corresponding to the histogram intersection kernel SVM indirectly, the input space without using the kernel trick or the feature map, and this will show that this leads to a fast classifier with order D test complexity.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first we discuss two of closely related works, so Maji at all proposed to efficiently calculated, calculate the test scoring for the histogram intersection kernel.",
                    "label": 1
                },
                {
                    "sent": "So the same expression if you expand it, you will see that it basically depends on two submissions.",
                    "label": 0
                },
                {
                    "sent": "The outer summation is on support vectors and the inner summation is on dimensions.",
                    "label": 0
                },
                {
                    "sent": "So what they say that you just exchange the order of this summations and now for if you fix a dimension, you sort the support vectors in increasing order and now you just for the test vector you just want to place it at the right place in this sorted order and once you place it, you know that all the support vector on the left of it would be less.",
                    "label": 0
                },
                {
                    "sent": "So you can cash some values and you can compute the same expression more efficiently.",
                    "label": 1
                },
                {
                    "sent": "So the complexity, the number of support vector part of the complexity is basically replaced by a binary search and the overall test complexity becomes order of D. Cross log of NSV.",
                    "label": 0
                },
                {
                    "sent": "And then they also propose to do some piecewise constant or linear approximations, and with this they again gain some more at Test efficiency.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the second approach is by Italians of seven.",
                    "label": 0
                },
                {
                    "sent": "In this, they propose that we can directly obtain an approximate feature map for certain class of kernels.",
                    "label": 0
                },
                {
                    "sent": "The generative kernels intersection, Hellinger and Chi Square are among these kernels.",
                    "label": 0
                },
                {
                    "sent": "So now at Test time, what will happen at train time?",
                    "label": 0
                },
                {
                    "sent": "What will happen?",
                    "label": 0
                },
                {
                    "sent": "You will take all the vectors.",
                    "label": 0
                },
                {
                    "sent": "You will explicitly map them to in a larger dimension.",
                    "label": 0
                },
                {
                    "sent": "You will learn a linear SVM there and then at Test time when you get the test vector, you take take the test vector, map it again in the space and then use the linear SVM that you learned in that space to calculate the score.",
                    "label": 1
                },
                {
                    "sent": "So here the test complexity is ordered.",
                    "label": 0
                },
                {
                    "sent": "The dash where D dash is the dimension of the feature space, which is higher than the dimension of the input space.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we come to the proposed method, and I again remind you that the dual is of this form and the scoring function is it depends on kernel computation with all the support vectors.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at the corresponding primal of in the feature space.",
                    "label": 0
                },
                {
                    "sent": "Which is never really solved.",
                    "label": 0
                },
                {
                    "sent": "We always solve the dual.",
                    "label": 0
                },
                {
                    "sent": "This is the expression for the primal I have written the objective in in unconstrained form, and this is equivalent to the constraint form that I showed before.",
                    "label": 0
                },
                {
                    "sent": "And the score calculation is a dot product between W 5 which is the linear decision boundary in the feature space.",
                    "label": 0
                },
                {
                    "sent": "And this dot product is never computed explicitly.",
                    "label": 0
                },
                {
                    "sent": "This is usually computed using the kernel kernel trick, right?",
                    "label": 0
                },
                {
                    "sent": "With the alphas which are obtained by optimizing this function.",
                    "label": 0
                },
                {
                    "sent": "So the keys.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Type in the approaches.",
                    "label": 0
                },
                {
                    "sent": "Assume that we have a pre image of this decision boundary, right?",
                    "label": 0
                },
                {
                    "sent": "So we have a W vector in the input space for which 5 W is either directly equal to Wi-Fi and in a more general case is the closest approximation as in general the preimage of any vector in a feature space might not exist in input space.",
                    "label": 0
                },
                {
                    "sent": "So if we do this approximation, we can rewrite the primal objective.",
                    "label": 0
                },
                {
                    "sent": "And now it will now.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sure that this objective will only depend on this W, which is the preimage of the decision boundary in the feature space.",
                    "label": 0
                },
                {
                    "sent": "So if we use the property of the intersection kernel, the regularization term can be reduced to a simple L1 regularization on W and then the score function.",
                    "label": 0
                },
                {
                    "sent": "We can again use the histogram intersection kernel and the score function can be written in this form.",
                    "label": 1
                },
                {
                    "sent": "So what we propose here is that we would like to solve this optimization directly in input space without any feature mapping or kernel trick.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this objective function is definitely non convex, But the good thing is it is still quasi convex so quasi convexity is that the function.",
                    "label": 1
                },
                {
                    "sent": "It would have some local flat areas, so I won't give the proof here.",
                    "label": 1
                },
                {
                    "sent": "The details are in the paper, but you can see you can have an intuition using at looking at the decision functions in one and 2D.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One way of solving such problem would be by doing successive relaxations.",
                    "label": 0
                },
                {
                    "sent": "So here in Blue I show the decision function corresponding to a fixed X.",
                    "label": 0
                },
                {
                    "sent": "So the Y value is F of W. Sorry, this is a scoring function, so the Y value is F of the X axis is W and I've fixed X which is marked.",
                    "label": 0
                },
                {
                    "sent": "The blue curve is the scoring function corresponding to the intersection kernel.",
                    "label": 0
                },
                {
                    "sent": "The green curve is the scoring function corresponding to the linear kernel.",
                    "label": 0
                },
                {
                    "sent": "So if we if we work with a linear kernel, the problem becomes convex.",
                    "label": 0
                },
                {
                    "sent": "So what we propose is that you start optimizing with the linear kernel.",
                    "label": 0
                },
                {
                    "sent": "And then successively bend the curve to match the blue curve.",
                    "label": 0
                },
                {
                    "sent": "So the red would be an intermediate relaxation of this decision function.",
                    "label": 0
                },
                {
                    "sent": "This would be one way of solving the problem.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Practice, we find that a simple stochastic subgradient descent based solver works quite well.",
                    "label": 1
                },
                {
                    "sent": "One of the subgradient of the objective is quite easy to define analytically.",
                    "label": 0
                },
                {
                    "sent": "The expression is given here.",
                    "label": 1
                },
                {
                    "sent": "And then in the final optimization algorithm we use stochastic subgradient descent with the approximate form of this subgradient.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I show some experimental results here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we use classic computer vision databases.",
                    "label": 0
                },
                {
                    "sent": "So the first one is the scene 15 database, so it has 15 scene categories with about 4500 images.",
                    "label": 0
                },
                {
                    "sent": "It has a standard protocol on which you can compare with existing methods and we use the stand.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Protocol.",
                    "label": 0
                },
                {
                    "sent": "The second data set is Pascal.",
                    "label": 0
                },
                {
                    "sent": "We OC 2007.",
                    "label": 0
                },
                {
                    "sent": "Object Image classification task and there again 20 object classes.",
                    "label": 1
                },
                {
                    "sent": "And there is a standard train well and test set and you can compare with other method using average precision for each class and mean average precision for the whole data set.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we use a very standard pipeline for the features, so we use VL feat library with multiscale dense sift features and a bag of features where codebook is generated using K means.",
                    "label": 1
                },
                {
                    "sent": "We use hard quantization and we fix the vocabulary size to be 1024.",
                    "label": 1
                },
                {
                    "sent": "So we fixed this feature representation and then we we compare the learning algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first set of results we show is on the scene 15 data set, so we compare with the feature map method which has already been shown to be much faster than either doing column map or doing the method of Margie at all, which I discussed before.",
                    "label": 0
                },
                {
                    "sent": "So in terms of accuracy, we see that we do essentially the same on this database and then.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then on the next video, say 2007, database again in terms of performance.",
                    "label": 1
                },
                {
                    "sent": "In this case mean average precision.",
                    "label": 1
                },
                {
                    "sent": "We do essentially the same.",
                    "label": 0
                },
                {
                    "sent": "And then in terms of.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time and memory in time and space efficiency we compare with feature map again with the typical parameter setting which gave the best performance in the original paper and we see that we are about an order faster in time and the method leads to a substantial amount of memory reduction as well.",
                    "label": 0
                },
                {
                    "sent": "Again, compared to feature map in feature map for testing as well, you have two things.",
                    "label": 0
                },
                {
                    "sent": "You are first mapping it into a feature space so you are taking some time there and then you're doing a dot product in this higher dimensional feature space.",
                    "label": 0
                },
                {
                    "sent": "While in our case we're just doing a linear scan and we're doing a slightly more costly operation compared to a dot product.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we evaluated the convergence for different parameter values.",
                    "label": 0
                },
                {
                    "sent": "So we see that the method seems to converge for a range of different parameter values, but cross validation is suggested as always.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, we presented a method to learn SVM with the histogram intersection kernel by optimizing directly in the input space we we proposed a stochastic subgradient based learning method which is quite efficient.",
                    "label": 1
                },
                {
                    "sent": "It takes less than a minute to train one class of Pascal Vuosi classification task.",
                    "label": 0
                },
                {
                    "sent": "It leads to a very efficient, very nice and efficient test time classification time.",
                    "label": 1
                },
                {
                    "sent": "And then for future work we would like to explore better training algorithms.",
                    "label": 0
                },
                {
                    "sent": "We would also see how it generalizes to across different kernel classes and then perhaps apply it on more applications.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}