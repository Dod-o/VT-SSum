{
    "id": "mynukhdjvbmlkwqwrq2xdr7rqss62jye",
    "title": "Provable Matrix Completion using Alternating Minimization",
    "info": {
        "author": [
            "Praneeth Netrapalli, Department of Electrical and Computer Engineering, University of Texas at Austin"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_netrapalli_minimization/",
    "segmentation": [
        [
            "Hi, I'm Praneeth and I'll be presenting our work on provable matrix completion using alternating minimization.",
            "This work is joined with Prateek and CJ."
        ],
        [
            "Alternating minimization is a popular empirical approach for solving many low rank matrix problems.",
            "Famous examples are for instance matrix completion clustering, nonnegative matrix factorization and so on.",
            "Though it has very good empirical performance, the challenge here is that there are very few theoretical guarantees on its performance, and in this talk will be.",
            "Will be shedding some light on this issue.",
            "Originating minimization relies on the matrix factorization.",
            "Any low rank matrix M can be factorized into two smaller matrices, U&V transpose, where you is an M cross key metrics and wheezing and cross key metrics where keys rank of the matrix.",
            "The generic algorithm for generic algorithm of alternating minimization is to minimize a function F of X overall ranking matrices.",
            "We do this by first fixing U and then optimizing over.",
            "And then fixing V and then optimizing over you even when F is even when this optimization is not convex because underlying set is the set of low rank matrices, each of these individual steps may be convex and hence can be solved very efficiently."
        ],
        [
            "One of the popular examples for which organization has been observed to work well is matrix completion.",
            "In matrix completion we observe partial entries of matrix and under the assumption that the matrix is low rank, we want to fill in the rest of the entries, and in this case, alternating minimization is even more appealing because of its natural decoupling.",
            "Each step in alternating minimization just decouples into M Lee squares problems over K variables, where since case the rank of the matrix, we assume that it's.",
            "Much smaller than the dimension of the matrix.",
            "And because of this we there is no need to store large matrices during the execution of the algorithm, and it's also very easily parallelizable."
        ],
        [
            "So this is a comparison of alternating minimization with one of the leading approaches for solving matrix completion, which is a nuclear norm approach.",
            "Empirically, alternating minimization from observations, we can see that it has very similar sample complexity and also has very good computational complexity.",
            "It's much faster than nuclear norm approach."
        ],
        [
            "So as I mentioned in this talk, will prove.",
            "So as I mentioned earlier, there are no very few theoretical guarantees known for alternating mission, even though empirically it's been observed to work very well, and in this talk will.",
            "Explain our results on of our organization for two problems.",
            "One is matrix sensing and the other one is matrix completion.",
            "1st I'll explain our results for the matrix sensing problem and then we'll go to matrix completion.",
            "So the matrix sensing problem is a general, even more general version of matrix completion.",
            "We have a low rank matrix X and we obtain linear measurements of this matrix, which is why and using the observations why we want to figure out X.",
            "And the algorithm, the alternating natural alternating minimization algorithm for this particular thing is we initialize, you had zero and then in each iteration we minimize the two norm of the quantity, so that quantity is just the observations minus the observations.",
            "If the hypothesis was you had T -- 1 V transpose.",
            "So we fix you had zero and then fixing you had zero.",
            "We optimize for we and then we obtain a we had and we fix.",
            "We had and then optimize for you and we get the next you had and each of these.",
            "Problems easily squares problem and can be solved very efficiently in practice.",
            "And the initialization step is to make sure that we start at a good point from which the alternating minimization approach converges to the right answer.",
            "As we can see, this is actually a non convex problem and the known convex analysis methods are not applicable here to show the convergence of this approach to global minimum."
        ],
        [
            "So the existing results for the matrix sensing problem.",
            "All of them assume our RPO conditions.",
            "A linear operator A is.",
            "Say to satisfy our peak condition, if it's an approximate isometry for low rank matrices.",
            "And the formal definition is as given and the existing.",
            "So there are two well known methods for solving this problem.",
            "The first one, which also introduces a problem using nuclear norm approach and this approach requires Delta Phi K to be less than 1 / 10 with Delta OK is as defined in the array property and the second approach, which is a projected gradient descent method.",
            "It requires Delta 2K to be less than 1 / 3.",
            "A drawback of both of these approaches is that both of them required to compute many.",
            "It is during their execution and SVD computation is usually slow in practice for large matrices."
        ],
        [
            "So our result is as follows.",
            "The alternating minimization algorithm with appropriate initialization as we showed before, if D2 K is less than one over condition number squared times 1 / 100 K. Then after log one over epsilon titrations, the output matrix will be epsilon close to the two matrix.",
            "So there are two things I want to stress here.",
            "One is that when we compare it to the previous results, Delta 2K in our case depends on the condition number, whereas in the existing results we do not have this dependence.",
            "However, this can be modified and we can get a modified alternating minimization based algorithm for which Delta we only need D2 K to be less than one over constant K square.",
            "Even this is suboptimal.",
            "The factor key and we think it can be improved, but right now this is the result we have.",
            "Information then no.",
            "And the second point I would like to notice that this we are our result also shows that alternating minimization has a linear convergence to obtain an error of epsilon, we want to obtain an accuracy of epsilon.",
            "We only need log one over epsilon iterations."
        ],
        [
            "Now moving on to matrix completion formally, the problem is.",
            "That we observe some entries of low rank matrix and we want to fill in the rest of the entries and usual assumption made in existing literature is also that we sample elements uniformly at random from all the elements and in natural approach to use here is just minimize the error on the observed entries.",
            "So P Omega of a matrix represents the entry observed entries at the absorb places and zeros everywhere else.",
            "And we just want to minimize.",
            "The error in the observed entries and under the condition that the hypothesis matrix is a low rank matrix.",
            "And this is in fact a major component of one of the winning entries of the Netflix price.",
            "But however, proving convergence for this vanilla algorithm is much harder.",
            "One is because Omega does not, in general satisfy our IP, and two is cause since we sample uniformly at random in each iteration of the alternating minimization steps, there will be dependence dependence of iterates, and it's hard to.",
            "Use the normal independence assumptions to say something about the future at rates.",
            "So in order to get over this."
        ],
        [
            "So.",
            "This difficulty we assume as follows.",
            "So say we initially have a sample set of Omega.",
            "We divided and we decide that we want to run the organization 40 iterations.",
            "We divide the entire sample set into two different sample sets, and we use each different sample in each iteration.",
            "For instance, the algorithm is given here.",
            "So in the initial initialization part, we just take the top singular vectors of P, Omega, O of M, where as I mentioned earlier, is just.",
            "Observed entries at those places and zeros everywhere else and in each of the further alternative memorization steps we use different sample sets in each of the iterations.",
            "This is so that we have independence between I traits that will help us in our analysis and disclosure for analysis.",
            "And we conjectured that we do not need this partition, but as of now we have not been able to."
        ],
        [
            "Prove it.",
            "So existing results as I mentioned earlier, assumed uniform sampling of all the entries and the incoherence of the underlying matrix M. They are a matrix me set to be incoherent.",
            "If it's singular, values are well spread out on all the components and are not concentrated intuitively.",
            "This also means that the matrix is not spiky, and it's reasonably well spread out so that uniform sampling of very few elements uses a good idea of how the matrix is structured.",
            "So the existing in the existing result existing literature, there are two known theoretical approaches that solve this problem.",
            "The first one is the nuclear norm approach.",
            "It takes CCN log constant times KN log samples to solve the matrix completion problem correctly, and the second one which is based on the which is we just optimization on.",
            "The grassman manifold requires a function of the condition number times KN log samples to solve the problem correctly.",
            "Both of these have some computational.",
            "The first approach, which is the nuclear norm approach, requires many SVD calculations during its execution and hence his floor.",
            "In practice, for large matrices and the second approach is slower in practice because it optimizes only grassmannian manifold and also does not does not have a known rate of convergence, so.",
            "That's one of the drawbacks of this."
        ],
        [
            "Our result for the alternating minimization algorithm is as follows.",
            "If the number of samples is greater than condition number to the 4K to the 4.5 N log, and then we.",
            "Then after log one over Epsilon iterations, the error in the observed.",
            "After login or epsilon iterations, the number the error in the absorbed error in or out rate matrix and the true matrix is less than epsilon and as you pointed out.",
            "So there are so the weakness of our result is the dependence on condition number, which is not there for the nuclear norm approach and we our result also depends on the required accuracy.",
            "So there is a log one over epsilon term in the P. So and.",
            "That is because of our repeated sampling for each of the iterations.",
            "So to get an excellent accuracy, we need log one over epsilon iterations, and in each iteration we need so many samples, so the log on or excellent just multiplies the total number of samples and our dependence on K is also bad compared to existing results by a factor of 2 to 3.5.",
            "However, the advantages of our method is that we have linear convergence for the nuclear norm approach, even the known convex optimization methods are first order methods which can have at most one over square root epsilon rate of convergence, whereas we have geometric convergence which takes log one over epsilon iterations and our intermediate matrices are also low rank, and whereas nuclear norm approach may have higher rank full rank matrices as intermediate matrices."
        ],
        [
            "So the main idea of the proof is that if we observe all the elements, then alternating minimization is just the power method, which is which can be used to calculate all the top singular vectors of any given matrix.",
            "How and?",
            "In fact, even if Omega is not all the elements but only a partial set of elements, we can write updates of the alternating minimization as a power update power method update plus an error term.",
            "And if the number of observations is large enough, as in the theorem over, we can show that the error term is small enough and the process the.",
            "Iteration iterates eventually converge to the top single vectors."
        ],
        [
            "So to summarize, we showed that alternating minimization is globally optimal for matrix sensing and matrix completion under standard assumptions, and I want to stress that this is the first such result for alternating minimization.",
            "The alternative minimization has been empirically popular for many other problems, including these two, and we also have a linear rate of convergence, which is that we will obtain epsilon error only in log one over epsilon iterations.",
            "However, we also noted that we require more samples as compared to.",
            "Existing results for both of these problems."
        ],
        [
            "And.",
            "As future work we are trying to remove the dependence on condition, number and desired accuracy for the completion problem and we believe that this approach also has.",
            "May have this approach may also have application for obtaining guarantees for alternating minimization.",
            "For other problems an we have a full version online.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, I'm Praneeth and I'll be presenting our work on provable matrix completion using alternating minimization.",
                    "label": 0
                },
                {
                    "sent": "This work is joined with Prateek and CJ.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alternating minimization is a popular empirical approach for solving many low rank matrix problems.",
                    "label": 1
                },
                {
                    "sent": "Famous examples are for instance matrix completion clustering, nonnegative matrix factorization and so on.",
                    "label": 1
                },
                {
                    "sent": "Though it has very good empirical performance, the challenge here is that there are very few theoretical guarantees on its performance, and in this talk will be.",
                    "label": 0
                },
                {
                    "sent": "Will be shedding some light on this issue.",
                    "label": 1
                },
                {
                    "sent": "Originating minimization relies on the matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "Any low rank matrix M can be factorized into two smaller matrices, U&V transpose, where you is an M cross key metrics and wheezing and cross key metrics where keys rank of the matrix.",
                    "label": 0
                },
                {
                    "sent": "The generic algorithm for generic algorithm of alternating minimization is to minimize a function F of X overall ranking matrices.",
                    "label": 0
                },
                {
                    "sent": "We do this by first fixing U and then optimizing over.",
                    "label": 0
                },
                {
                    "sent": "And then fixing V and then optimizing over you even when F is even when this optimization is not convex because underlying set is the set of low rank matrices, each of these individual steps may be convex and hence can be solved very efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the popular examples for which organization has been observed to work well is matrix completion.",
                    "label": 1
                },
                {
                    "sent": "In matrix completion we observe partial entries of matrix and under the assumption that the matrix is low rank, we want to fill in the rest of the entries, and in this case, alternating minimization is even more appealing because of its natural decoupling.",
                    "label": 1
                },
                {
                    "sent": "Each step in alternating minimization just decouples into M Lee squares problems over K variables, where since case the rank of the matrix, we assume that it's.",
                    "label": 0
                },
                {
                    "sent": "Much smaller than the dimension of the matrix.",
                    "label": 0
                },
                {
                    "sent": "And because of this we there is no need to store large matrices during the execution of the algorithm, and it's also very easily parallelizable.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a comparison of alternating minimization with one of the leading approaches for solving matrix completion, which is a nuclear norm approach.",
                    "label": 1
                },
                {
                    "sent": "Empirically, alternating minimization from observations, we can see that it has very similar sample complexity and also has very good computational complexity.",
                    "label": 1
                },
                {
                    "sent": "It's much faster than nuclear norm approach.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I mentioned in this talk, will prove.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned earlier, there are no very few theoretical guarantees known for alternating mission, even though empirically it's been observed to work very well, and in this talk will.",
                    "label": 0
                },
                {
                    "sent": "Explain our results on of our organization for two problems.",
                    "label": 0
                },
                {
                    "sent": "One is matrix sensing and the other one is matrix completion.",
                    "label": 1
                },
                {
                    "sent": "1st I'll explain our results for the matrix sensing problem and then we'll go to matrix completion.",
                    "label": 0
                },
                {
                    "sent": "So the matrix sensing problem is a general, even more general version of matrix completion.",
                    "label": 1
                },
                {
                    "sent": "We have a low rank matrix X and we obtain linear measurements of this matrix, which is why and using the observations why we want to figure out X.",
                    "label": 0
                },
                {
                    "sent": "And the algorithm, the alternating natural alternating minimization algorithm for this particular thing is we initialize, you had zero and then in each iteration we minimize the two norm of the quantity, so that quantity is just the observations minus the observations.",
                    "label": 0
                },
                {
                    "sent": "If the hypothesis was you had T -- 1 V transpose.",
                    "label": 0
                },
                {
                    "sent": "So we fix you had zero and then fixing you had zero.",
                    "label": 0
                },
                {
                    "sent": "We optimize for we and then we obtain a we had and we fix.",
                    "label": 0
                },
                {
                    "sent": "We had and then optimize for you and we get the next you had and each of these.",
                    "label": 0
                },
                {
                    "sent": "Problems easily squares problem and can be solved very efficiently in practice.",
                    "label": 0
                },
                {
                    "sent": "And the initialization step is to make sure that we start at a good point from which the alternating minimization approach converges to the right answer.",
                    "label": 0
                },
                {
                    "sent": "As we can see, this is actually a non convex problem and the known convex analysis methods are not applicable here to show the convergence of this approach to global minimum.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the existing results for the matrix sensing problem.",
                    "label": 0
                },
                {
                    "sent": "All of them assume our RPO conditions.",
                    "label": 0
                },
                {
                    "sent": "A linear operator A is.",
                    "label": 0
                },
                {
                    "sent": "Say to satisfy our peak condition, if it's an approximate isometry for low rank matrices.",
                    "label": 0
                },
                {
                    "sent": "And the formal definition is as given and the existing.",
                    "label": 0
                },
                {
                    "sent": "So there are two well known methods for solving this problem.",
                    "label": 0
                },
                {
                    "sent": "The first one, which also introduces a problem using nuclear norm approach and this approach requires Delta Phi K to be less than 1 / 10 with Delta OK is as defined in the array property and the second approach, which is a projected gradient descent method.",
                    "label": 0
                },
                {
                    "sent": "It requires Delta 2K to be less than 1 / 3.",
                    "label": 0
                },
                {
                    "sent": "A drawback of both of these approaches is that both of them required to compute many.",
                    "label": 0
                },
                {
                    "sent": "It is during their execution and SVD computation is usually slow in practice for large matrices.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our result is as follows.",
                    "label": 0
                },
                {
                    "sent": "The alternating minimization algorithm with appropriate initialization as we showed before, if D2 K is less than one over condition number squared times 1 / 100 K. Then after log one over epsilon titrations, the output matrix will be epsilon close to the two matrix.",
                    "label": 0
                },
                {
                    "sent": "So there are two things I want to stress here.",
                    "label": 0
                },
                {
                    "sent": "One is that when we compare it to the previous results, Delta 2K in our case depends on the condition number, whereas in the existing results we do not have this dependence.",
                    "label": 1
                },
                {
                    "sent": "However, this can be modified and we can get a modified alternating minimization based algorithm for which Delta we only need D2 K to be less than one over constant K square.",
                    "label": 0
                },
                {
                    "sent": "Even this is suboptimal.",
                    "label": 1
                },
                {
                    "sent": "The factor key and we think it can be improved, but right now this is the result we have.",
                    "label": 0
                },
                {
                    "sent": "Information then no.",
                    "label": 1
                },
                {
                    "sent": "And the second point I would like to notice that this we are our result also shows that alternating minimization has a linear convergence to obtain an error of epsilon, we want to obtain an accuracy of epsilon.",
                    "label": 0
                },
                {
                    "sent": "We only need log one over epsilon iterations.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now moving on to matrix completion formally, the problem is.",
                    "label": 0
                },
                {
                    "sent": "That we observe some entries of low rank matrix and we want to fill in the rest of the entries and usual assumption made in existing literature is also that we sample elements uniformly at random from all the elements and in natural approach to use here is just minimize the error on the observed entries.",
                    "label": 0
                },
                {
                    "sent": "So P Omega of a matrix represents the entry observed entries at the absorb places and zeros everywhere else.",
                    "label": 0
                },
                {
                    "sent": "And we just want to minimize.",
                    "label": 0
                },
                {
                    "sent": "The error in the observed entries and under the condition that the hypothesis matrix is a low rank matrix.",
                    "label": 0
                },
                {
                    "sent": "And this is in fact a major component of one of the winning entries of the Netflix price.",
                    "label": 1
                },
                {
                    "sent": "But however, proving convergence for this vanilla algorithm is much harder.",
                    "label": 0
                },
                {
                    "sent": "One is because Omega does not, in general satisfy our IP, and two is cause since we sample uniformly at random in each iteration of the alternating minimization steps, there will be dependence dependence of iterates, and it's hard to.",
                    "label": 1
                },
                {
                    "sent": "Use the normal independence assumptions to say something about the future at rates.",
                    "label": 0
                },
                {
                    "sent": "So in order to get over this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This difficulty we assume as follows.",
                    "label": 0
                },
                {
                    "sent": "So say we initially have a sample set of Omega.",
                    "label": 0
                },
                {
                    "sent": "We divided and we decide that we want to run the organization 40 iterations.",
                    "label": 0
                },
                {
                    "sent": "We divide the entire sample set into two different sample sets, and we use each different sample in each iteration.",
                    "label": 0
                },
                {
                    "sent": "For instance, the algorithm is given here.",
                    "label": 0
                },
                {
                    "sent": "So in the initial initialization part, we just take the top singular vectors of P, Omega, O of M, where as I mentioned earlier, is just.",
                    "label": 0
                },
                {
                    "sent": "Observed entries at those places and zeros everywhere else and in each of the further alternative memorization steps we use different sample sets in each of the iterations.",
                    "label": 0
                },
                {
                    "sent": "This is so that we have independence between I traits that will help us in our analysis and disclosure for analysis.",
                    "label": 0
                },
                {
                    "sent": "And we conjectured that we do not need this partition, but as of now we have not been able to.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prove it.",
                    "label": 0
                },
                {
                    "sent": "So existing results as I mentioned earlier, assumed uniform sampling of all the entries and the incoherence of the underlying matrix M. They are a matrix me set to be incoherent.",
                    "label": 0
                },
                {
                    "sent": "If it's singular, values are well spread out on all the components and are not concentrated intuitively.",
                    "label": 0
                },
                {
                    "sent": "This also means that the matrix is not spiky, and it's reasonably well spread out so that uniform sampling of very few elements uses a good idea of how the matrix is structured.",
                    "label": 0
                },
                {
                    "sent": "So the existing in the existing result existing literature, there are two known theoretical approaches that solve this problem.",
                    "label": 0
                },
                {
                    "sent": "The first one is the nuclear norm approach.",
                    "label": 0
                },
                {
                    "sent": "It takes CCN log constant times KN log samples to solve the matrix completion problem correctly, and the second one which is based on the which is we just optimization on.",
                    "label": 0
                },
                {
                    "sent": "The grassman manifold requires a function of the condition number times KN log samples to solve the problem correctly.",
                    "label": 0
                },
                {
                    "sent": "Both of these have some computational.",
                    "label": 0
                },
                {
                    "sent": "The first approach, which is the nuclear norm approach, requires many SVD calculations during its execution and hence his floor.",
                    "label": 1
                },
                {
                    "sent": "In practice, for large matrices and the second approach is slower in practice because it optimizes only grassmannian manifold and also does not does not have a known rate of convergence, so.",
                    "label": 1
                },
                {
                    "sent": "That's one of the drawbacks of this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our result for the alternating minimization algorithm is as follows.",
                    "label": 1
                },
                {
                    "sent": "If the number of samples is greater than condition number to the 4K to the 4.5 N log, and then we.",
                    "label": 0
                },
                {
                    "sent": "Then after log one over Epsilon iterations, the error in the observed.",
                    "label": 0
                },
                {
                    "sent": "After login or epsilon iterations, the number the error in the absorbed error in or out rate matrix and the true matrix is less than epsilon and as you pointed out.",
                    "label": 0
                },
                {
                    "sent": "So there are so the weakness of our result is the dependence on condition number, which is not there for the nuclear norm approach and we our result also depends on the required accuracy.",
                    "label": 1
                },
                {
                    "sent": "So there is a log one over epsilon term in the P. So and.",
                    "label": 0
                },
                {
                    "sent": "That is because of our repeated sampling for each of the iterations.",
                    "label": 0
                },
                {
                    "sent": "So to get an excellent accuracy, we need log one over epsilon iterations, and in each iteration we need so many samples, so the log on or excellent just multiplies the total number of samples and our dependence on K is also bad compared to existing results by a factor of 2 to 3.5.",
                    "label": 1
                },
                {
                    "sent": "However, the advantages of our method is that we have linear convergence for the nuclear norm approach, even the known convex optimization methods are first order methods which can have at most one over square root epsilon rate of convergence, whereas we have geometric convergence which takes log one over epsilon iterations and our intermediate matrices are also low rank, and whereas nuclear norm approach may have higher rank full rank matrices as intermediate matrices.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main idea of the proof is that if we observe all the elements, then alternating minimization is just the power method, which is which can be used to calculate all the top singular vectors of any given matrix.",
                    "label": 1
                },
                {
                    "sent": "How and?",
                    "label": 0
                },
                {
                    "sent": "In fact, even if Omega is not all the elements but only a partial set of elements, we can write updates of the alternating minimization as a power update power method update plus an error term.",
                    "label": 0
                },
                {
                    "sent": "And if the number of observations is large enough, as in the theorem over, we can show that the error term is small enough and the process the.",
                    "label": 0
                },
                {
                    "sent": "Iteration iterates eventually converge to the top single vectors.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize, we showed that alternating minimization is globally optimal for matrix sensing and matrix completion under standard assumptions, and I want to stress that this is the first such result for alternating minimization.",
                    "label": 1
                },
                {
                    "sent": "The alternative minimization has been empirically popular for many other problems, including these two, and we also have a linear rate of convergence, which is that we will obtain epsilon error only in log one over epsilon iterations.",
                    "label": 0
                },
                {
                    "sent": "However, we also noted that we require more samples as compared to.",
                    "label": 0
                },
                {
                    "sent": "Existing results for both of these problems.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "As future work we are trying to remove the dependence on condition, number and desired accuracy for the completion problem and we believe that this approach also has.",
                    "label": 1
                },
                {
                    "sent": "May have this approach may also have application for obtaining guarantees for alternating minimization.",
                    "label": 1
                },
                {
                    "sent": "For other problems an we have a full version online.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}