{
    "id": "lr67ategr6cga74nmtbu4hdq5jxd3vha",
    "title": "On the design of robust classifiers for computer vision",
    "info": {
        "author": [
            "Hamed Masnadi-Shirazi, Department of Electrical and Computer Engineering, UC San Diego"
        ],
        "published": "July 19, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Computer Vision->Sparsity & Convex Optimisation"
        ]
    },
    "url": "http://videolectures.net/cvpr2010_masnadi_shirazi_drc/",
    "segmentation": [
        [
            "Yeah, so the topic of our paper is on the design of robust classifiers for computer vision, and this is work that's been done in collaboration with Vijay Mahadevan and Professor Novascan solos.",
            "From the statistical Visual computing Lab at University of California, San Diego."
        ],
        [
            "So we're going to be talking about classification for computer vision problems.",
            "And a lot of classification algorithms.",
            "Some of the most successful ones like SVM's and boosting.",
            "Are basically based on minimizing the expected value of a loss function fee.",
            "And these last functions are both margin enforcing and based consistent.",
            "So for such a lot losses, usually a large penalty is assigned to points with a negative margin.",
            "A small penalty to point some small positive margin.",
            "And zero close to 0 penalty to points."
        ],
        [
            "Of large positive margin.",
            "Unfortunately these type of losses don't overcome some of the unique challenges posed by computer vision problems.",
            "One major difficulty is the prevalence of noise, outliers, and class ambiguity that exists in computer vision problems.",
            "So for example, here we have a picture of a famous street from San Francisco.",
            "But if we were to do Patch based image classification on this, we would get a lot of patches that have stuff like trees and um, that could or buildings cars.",
            "Even though this is supposedly an image of a street.",
            "So this simple example shows the importance of.",
            "Having robust classify."
        ],
        [
            "There's.",
            "And.",
            "The limitation of current loss functions can to some extent be attributed to the unbounded negative margin.",
            "So for this simple classification algorithm, we have three obvious outliers.",
            "Those three Green X is an for loss function like the exponential loss that we have down there.",
            "The blue dotted line is the exponential loss and you can see that it assigns an exponentially high penalty to those three outliers, so.",
            "It emphasizes and concentrates on those three points instead of disregarding them.",
            "So recent improvements have been made in terms of better loss functions.",
            "One is the linearly growing logit last used in logic boost by Friedman in 2000, and that's the green line and you can see it grows linearly.",
            "The other one is the bounded Savage last Savage boost that we came up with in 2008, and that's the blue dotted line in the bottom.",
            "You can see that that one is bounded, so these improve over such situations.",
            "But we argue that negative margins aren't the entire Prob."
        ],
        [
            "In that it's also.",
            "Required that we penalize large positive margins as well.",
            "So we consider this simple example where we have two classes that data is distributed uniformly in the vertical direction and we have two in the horizontal direction.",
            "It's Gaussian with equal variance and means plus negative 3.",
            "So it's easy to see that for this example, the Bayes Decision rule would be a line at X = 0.",
            "And actually all these previous loss functions empirically on this exact data would put the loss function would put their decision rule right there very close to the base decision rule.",
            "Now we consider the case where we add the single outlier at negative 2 and 0.",
            "I'm.",
            "What happens is that.",
            "Current loss functions.",
            "The Savage lost the log the exponential all drastically moved their decision boundaries to the new location at X equals negative 2.3, and the reason for that is that all these losses assign zero or almost zero to points that have already been classified correctly, so placing the loss at X equals negative.",
            "2.3 is where all data points can be classified correctly.",
            "On the other hand, we introduce the tangent loss which will derive during this talk, and this loss has the shape below, and it penalizes both.",
            "Large positive and negative margins.",
            "You can see that it assigns abounded loste points that have been classified correctly, so this discourages solutions that are two correct.",
            "And the red line is where the tangent solution lies, which is very close to the Bayes decision rule.",
            "Still it disregards that outlier."
        ],
        [
            "So in order to derive the tangent loss, we need to.",
            "Talk a little bit about risk minimization.",
            "We define feature vector X class label Y classifier H predictor F and loss function L. We're going to do.",
            "Minimize the risk.",
            "So we take the expected loss.",
            "Which is equivalent to minimizing the conditional risk.",
            "Overall X is and ADA is going to be defined as the probability of 1."
        ],
        [
            "Given X, so with that in mind.",
            "We also need to talk about Bayes consistent loss functions.",
            "A good example would be the commonly known exponential loss function that's used in boosting.",
            "So the so the traditional approach is to start off with a loss function.",
            "Take the expectation to find the risk which is ciofi.",
            "And then we minimize that risk to find the function F star, the predictor that minimizes the risk.",
            "So for the exponential loss, F star would be 1/2 log of the ratio of the probabilities.",
            "And the next thing we have to do is to check to see if this F star is actually implementing the Bayes Decision Rule.",
            "In other words, is it based consistent or not?",
            "So we've plotted F star here and you can see that in fact it does.",
            "It is based consistent because it assigns positive values to probabilities that are higher than half and negative values to probabilities that are less than half.",
            "So we're good there we take this F star and we plug it back into our risk and we get the minimal conditional risk.",
            "See star.",
            "And now we have to check to see whether our problem is convex or not and.",
            "In order to do that, we have to look at see Star.",
            "It can be shown that in order to have a convex optimization problem, you need to have you need to check the convexity of sea star and so that's good as well.",
            "So for the exponential loss were good, but you can see that using this traditional method it's it's problematic because it doesn't let you actually design a base consistent loss function, it only lets you check whether your loss function is based consistent or not.",
            "Um?",
            "In fact, we can say that we kind of got lucky with the exponential loss."
        ],
        [
            "These things happen.",
            "But we'd like to be able to design loss functions that are based consistent, and so in our NIPS 08 paper we propose a new path for classifier design, where you can choose a strictly concave function, C star, an invertible function F star, that such that they have these simple symmetry.",
            "Properties you take these two functions, you plug them into this formula and you get a loss function.",
            "FIA V that is guaranteed to be based consistent.",
            "So this provides a principled way of deriving designing novel based, consistent loss functions."
        ],
        [
            "So now we'd like to use this to design A robust loss of loss function that has certain properties.",
            "So from our previous discussion for a hypothetical loss function, we'd like it to have certain robust properties.",
            "One would be that we'd like it to have assign a bounded penalty for large negative margins.",
            "We'd like it to have a smaller bounded penalty for large positive margins.",
            "And we'd obviously also like it to be margin enforcing."
        ],
        [
            "We can show that under based consistency, those three properties are satisfied if and only if our choice of F star in C sterf satisfy these five requirements."
        ],
        [
            "So, but unfortunately existing link functions F starred don't comply with those five requirements, so we introduced the Tangent link, which does satisfy those requirements, and I've.",
            "Plotted.",
            "The tangent link here an it's formula.",
            "We also combine this with the least squares, minimal conditional risk.",
            "Which does also satisfy those five requirements, and it's been plotted here.",
            "You can see that it's convex.",
            "The tangent link is also.",
            "Based consistent.",
            "So we take these two functions, we plug them into the formula for finding the loss function.",
            "And we get what we call the.",
            "Tangent loss function.",
            "So this tangent loss function is that's the formula and we've plotted it and you can see that it does in fact have those.",
            "It does in fact have the shape that we'd like it too.",
            "It does have abounded.",
            "It does assign a bounded penalty to points that have been misclassified, so it disregards those type of outliers.",
            "And it also has a bounded penalty for points that have been classified correctly, so it can also deal with those other type of outliers as well.",
            "So once we have the.",
            "Tangent loss function we can.",
            "Basically use it to do variety of design.",
            "Design A variety of classification algorithms.",
            "In the paper.",
            "I won't go through the details, but in the paper you can find we've used this loss function to design A boosting algorithm we call tangent loosed.",
            "And then you can find the details of that in the paper."
        ],
        [
            "And we've also done a variety of experiments using the tangent boost algorithm.",
            "Some of those experiments have been done on multiple instance learning problems where you have the problem of ambiguity in the sense the data is unlabeled.",
            "You only have bags of labels.",
            "We've also applied it to.",
            "Image recognition problems where there might be lots of outliers.",
            "We've also done this for.",
            "Image classification.",
            "And retrieval systems, something like the initial example that I had.",
            "But here all those experiments can be found in the paper here.",
            "I'm just going to go over experiments that we did for tracking.",
            "So we use the discriminants saliency tracker of Hot Divine Investments, LLC.",
            "VPR 09.",
            "And we simply use the tangent boosts algorithm to combine the saliency Maps in a discriminate manner.",
            "In these two.",
            "Videos that I'm going to show you the red boxes for the case where the tangent loss is used and the blue dotted boxes for the tracker that uses the exponential loss.",
            "So hopefully this will work OK. That's weird, it works on my computer.",
            "Oh, it runs here.",
            "I don't know why that's Oh well that one works OK. That's still good.",
            "Yeah, you can see that using the robust tangent loss keeps track of the target, whereas the exponential loss because of all the.",
            "Outliers basically loses track of the target and wanders off and drifts off.",
            "Let's try this one again, maybe it'll work this time.",
            "No.",
            "Oh well.",
            "It was basically a guy running around the field."
        ],
        [
            "OK, well in conclusion, we argue that being too correct should be penalized for robust classification.",
            "So we derive the set of requirements that are robust, Bayes consistent loss function should have.",
            "And we from that we derive the tangent loss that is robust and based consistent.",
            "And we demonstrate that the tangent boost algorithm.",
            "As state of the art results on a variety of challenging datasets."
        ],
        [
            "That's about it, thank you.",
            "We have time for a few questions.",
            "I have one so your curve has a sense of scale and size.",
            "The number of parameters to it.",
            "How did that come about?",
            "Anne?",
            "Is it?",
            "How can you design it to fit particular data set you might have?",
            "So.",
            "Let me go back to.",
            "That so you are asking about the shape of this.",
            "Yeah so I didn't talk about this in the interest of time, but there is a parameter on this loss function that allows you to control the ratio of how much penalty is assigned to the positive side versus the negative side.",
            "So by tweaking that parameter either like through cross validation or depending on how much outliers you expect to have in your data, you can change the shape of this loss function.",
            "From anything where it penalizes the being correct as much as it does being wrong, which is of course something you don't want to do, but it can also go to the all the way to the other extreme where.",
            "It almost doesn't penalize the positives at all and starts behaving like previous loss functions so you can tweak it and change its shape so you can ask a question if you go to microphone.",
            "Please.",
            "Have you considered that the use of a Bregman divergences answer gates and this kind of thing?",
            "I actually, if you do go to our NIPS 08 paper the proof of like that formula there passes through the use of Bregman divergences and stuff like that.",
            "So all those do come into play.",
            "You can see the connections, hopefully by reading the paper, but that's how that's where this formula comes from innocence.",
            "OK, thank you, thank you.",
            "So obviously your loss function is non convex and I was wondering that how much this is going to affect your performance because obviously you are going to fall into a local minima so I get this question all the time.",
            "It's a good one but it has a simple answer.",
            "When we're doing Minimizations were not minimizing over the loss, we're minimizing over the expected loss.",
            "So the last function, as I mean we show this in our NIPS paper, but the loss function need not be convex for the optimization problem or the risk to be convex.",
            "So although this loss function is not convex, what I'm minimizing the risk is convex, and that you can.",
            "You know that can be shown.",
            "So the problem is still convex.",
            "Let's thank the speaker again.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so the topic of our paper is on the design of robust classifiers for computer vision, and this is work that's been done in collaboration with Vijay Mahadevan and Professor Novascan solos.",
                    "label": 0
                },
                {
                    "sent": "From the statistical Visual computing Lab at University of California, San Diego.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're going to be talking about classification for computer vision problems.",
                    "label": 0
                },
                {
                    "sent": "And a lot of classification algorithms.",
                    "label": 1
                },
                {
                    "sent": "Some of the most successful ones like SVM's and boosting.",
                    "label": 0
                },
                {
                    "sent": "Are basically based on minimizing the expected value of a loss function fee.",
                    "label": 1
                },
                {
                    "sent": "And these last functions are both margin enforcing and based consistent.",
                    "label": 1
                },
                {
                    "sent": "So for such a lot losses, usually a large penalty is assigned to points with a negative margin.",
                    "label": 1
                },
                {
                    "sent": "A small penalty to point some small positive margin.",
                    "label": 0
                },
                {
                    "sent": "And zero close to 0 penalty to points.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of large positive margin.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately these type of losses don't overcome some of the unique challenges posed by computer vision problems.",
                    "label": 1
                },
                {
                    "sent": "One major difficulty is the prevalence of noise, outliers, and class ambiguity that exists in computer vision problems.",
                    "label": 1
                },
                {
                    "sent": "So for example, here we have a picture of a famous street from San Francisco.",
                    "label": 0
                },
                {
                    "sent": "But if we were to do Patch based image classification on this, we would get a lot of patches that have stuff like trees and um, that could or buildings cars.",
                    "label": 0
                },
                {
                    "sent": "Even though this is supposedly an image of a street.",
                    "label": 0
                },
                {
                    "sent": "So this simple example shows the importance of.",
                    "label": 0
                },
                {
                    "sent": "Having robust classify.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The limitation of current loss functions can to some extent be attributed to the unbounded negative margin.",
                    "label": 1
                },
                {
                    "sent": "So for this simple classification algorithm, we have three obvious outliers.",
                    "label": 0
                },
                {
                    "sent": "Those three Green X is an for loss function like the exponential loss that we have down there.",
                    "label": 0
                },
                {
                    "sent": "The blue dotted line is the exponential loss and you can see that it assigns an exponentially high penalty to those three outliers, so.",
                    "label": 0
                },
                {
                    "sent": "It emphasizes and concentrates on those three points instead of disregarding them.",
                    "label": 0
                },
                {
                    "sent": "So recent improvements have been made in terms of better loss functions.",
                    "label": 1
                },
                {
                    "sent": "One is the linearly growing logit last used in logic boost by Friedman in 2000, and that's the green line and you can see it grows linearly.",
                    "label": 0
                },
                {
                    "sent": "The other one is the bounded Savage last Savage boost that we came up with in 2008, and that's the blue dotted line in the bottom.",
                    "label": 1
                },
                {
                    "sent": "You can see that that one is bounded, so these improve over such situations.",
                    "label": 0
                },
                {
                    "sent": "But we argue that negative margins aren't the entire Prob.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In that it's also.",
                    "label": 0
                },
                {
                    "sent": "Required that we penalize large positive margins as well.",
                    "label": 1
                },
                {
                    "sent": "So we consider this simple example where we have two classes that data is distributed uniformly in the vertical direction and we have two in the horizontal direction.",
                    "label": 0
                },
                {
                    "sent": "It's Gaussian with equal variance and means plus negative 3.",
                    "label": 0
                },
                {
                    "sent": "So it's easy to see that for this example, the Bayes Decision rule would be a line at X = 0.",
                    "label": 0
                },
                {
                    "sent": "And actually all these previous loss functions empirically on this exact data would put the loss function would put their decision rule right there very close to the base decision rule.",
                    "label": 0
                },
                {
                    "sent": "Now we consider the case where we add the single outlier at negative 2 and 0.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "What happens is that.",
                    "label": 0
                },
                {
                    "sent": "Current loss functions.",
                    "label": 0
                },
                {
                    "sent": "The Savage lost the log the exponential all drastically moved their decision boundaries to the new location at X equals negative 2.3, and the reason for that is that all these losses assign zero or almost zero to points that have already been classified correctly, so placing the loss at X equals negative.",
                    "label": 0
                },
                {
                    "sent": "2.3 is where all data points can be classified correctly.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we introduce the tangent loss which will derive during this talk, and this loss has the shape below, and it penalizes both.",
                    "label": 1
                },
                {
                    "sent": "Large positive and negative margins.",
                    "label": 1
                },
                {
                    "sent": "You can see that it assigns abounded loste points that have been classified correctly, so this discourages solutions that are two correct.",
                    "label": 0
                },
                {
                    "sent": "And the red line is where the tangent solution lies, which is very close to the Bayes decision rule.",
                    "label": 0
                },
                {
                    "sent": "Still it disregards that outlier.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order to derive the tangent loss, we need to.",
                    "label": 0
                },
                {
                    "sent": "Talk a little bit about risk minimization.",
                    "label": 0
                },
                {
                    "sent": "We define feature vector X class label Y classifier H predictor F and loss function L. We're going to do.",
                    "label": 1
                },
                {
                    "sent": "Minimize the risk.",
                    "label": 0
                },
                {
                    "sent": "So we take the expected loss.",
                    "label": 0
                },
                {
                    "sent": "Which is equivalent to minimizing the conditional risk.",
                    "label": 1
                },
                {
                    "sent": "Overall X is and ADA is going to be defined as the probability of 1.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Given X, so with that in mind.",
                    "label": 0
                },
                {
                    "sent": "We also need to talk about Bayes consistent loss functions.",
                    "label": 1
                },
                {
                    "sent": "A good example would be the commonly known exponential loss function that's used in boosting.",
                    "label": 0
                },
                {
                    "sent": "So the so the traditional approach is to start off with a loss function.",
                    "label": 0
                },
                {
                    "sent": "Take the expectation to find the risk which is ciofi.",
                    "label": 0
                },
                {
                    "sent": "And then we minimize that risk to find the function F star, the predictor that minimizes the risk.",
                    "label": 0
                },
                {
                    "sent": "So for the exponential loss, F star would be 1/2 log of the ratio of the probabilities.",
                    "label": 0
                },
                {
                    "sent": "And the next thing we have to do is to check to see if this F star is actually implementing the Bayes Decision Rule.",
                    "label": 0
                },
                {
                    "sent": "In other words, is it based consistent or not?",
                    "label": 0
                },
                {
                    "sent": "So we've plotted F star here and you can see that in fact it does.",
                    "label": 0
                },
                {
                    "sent": "It is based consistent because it assigns positive values to probabilities that are higher than half and negative values to probabilities that are less than half.",
                    "label": 0
                },
                {
                    "sent": "So we're good there we take this F star and we plug it back into our risk and we get the minimal conditional risk.",
                    "label": 0
                },
                {
                    "sent": "See star.",
                    "label": 0
                },
                {
                    "sent": "And now we have to check to see whether our problem is convex or not and.",
                    "label": 0
                },
                {
                    "sent": "In order to do that, we have to look at see Star.",
                    "label": 0
                },
                {
                    "sent": "It can be shown that in order to have a convex optimization problem, you need to have you need to check the convexity of sea star and so that's good as well.",
                    "label": 0
                },
                {
                    "sent": "So for the exponential loss were good, but you can see that using this traditional method it's it's problematic because it doesn't let you actually design a base consistent loss function, it only lets you check whether your loss function is based consistent or not.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "In fact, we can say that we kind of got lucky with the exponential loss.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These things happen.",
                    "label": 0
                },
                {
                    "sent": "But we'd like to be able to design loss functions that are based consistent, and so in our NIPS 08 paper we propose a new path for classifier design, where you can choose a strictly concave function, C star, an invertible function F star, that such that they have these simple symmetry.",
                    "label": 1
                },
                {
                    "sent": "Properties you take these two functions, you plug them into this formula and you get a loss function.",
                    "label": 1
                },
                {
                    "sent": "FIA V that is guaranteed to be based consistent.",
                    "label": 0
                },
                {
                    "sent": "So this provides a principled way of deriving designing novel based, consistent loss functions.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we'd like to use this to design A robust loss of loss function that has certain properties.",
                    "label": 0
                },
                {
                    "sent": "So from our previous discussion for a hypothetical loss function, we'd like it to have certain robust properties.",
                    "label": 0
                },
                {
                    "sent": "One would be that we'd like it to have assign a bounded penalty for large negative margins.",
                    "label": 1
                },
                {
                    "sent": "We'd like it to have a smaller bounded penalty for large positive margins.",
                    "label": 1
                },
                {
                    "sent": "And we'd obviously also like it to be margin enforcing.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can show that under based consistency, those three properties are satisfied if and only if our choice of F star in C sterf satisfy these five requirements.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but unfortunately existing link functions F starred don't comply with those five requirements, so we introduced the Tangent link, which does satisfy those requirements, and I've.",
                    "label": 0
                },
                {
                    "sent": "Plotted.",
                    "label": 0
                },
                {
                    "sent": "The tangent link here an it's formula.",
                    "label": 0
                },
                {
                    "sent": "We also combine this with the least squares, minimal conditional risk.",
                    "label": 0
                },
                {
                    "sent": "Which does also satisfy those five requirements, and it's been plotted here.",
                    "label": 0
                },
                {
                    "sent": "You can see that it's convex.",
                    "label": 0
                },
                {
                    "sent": "The tangent link is also.",
                    "label": 1
                },
                {
                    "sent": "Based consistent.",
                    "label": 0
                },
                {
                    "sent": "So we take these two functions, we plug them into the formula for finding the loss function.",
                    "label": 0
                },
                {
                    "sent": "And we get what we call the.",
                    "label": 0
                },
                {
                    "sent": "Tangent loss function.",
                    "label": 0
                },
                {
                    "sent": "So this tangent loss function is that's the formula and we've plotted it and you can see that it does in fact have those.",
                    "label": 0
                },
                {
                    "sent": "It does in fact have the shape that we'd like it too.",
                    "label": 0
                },
                {
                    "sent": "It does have abounded.",
                    "label": 0
                },
                {
                    "sent": "It does assign a bounded penalty to points that have been misclassified, so it disregards those type of outliers.",
                    "label": 0
                },
                {
                    "sent": "And it also has a bounded penalty for points that have been classified correctly, so it can also deal with those other type of outliers as well.",
                    "label": 0
                },
                {
                    "sent": "So once we have the.",
                    "label": 1
                },
                {
                    "sent": "Tangent loss function we can.",
                    "label": 0
                },
                {
                    "sent": "Basically use it to do variety of design.",
                    "label": 0
                },
                {
                    "sent": "Design A variety of classification algorithms.",
                    "label": 0
                },
                {
                    "sent": "In the paper.",
                    "label": 0
                },
                {
                    "sent": "I won't go through the details, but in the paper you can find we've used this loss function to design A boosting algorithm we call tangent loosed.",
                    "label": 0
                },
                {
                    "sent": "And then you can find the details of that in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we've also done a variety of experiments using the tangent boost algorithm.",
                    "label": 0
                },
                {
                    "sent": "Some of those experiments have been done on multiple instance learning problems where you have the problem of ambiguity in the sense the data is unlabeled.",
                    "label": 0
                },
                {
                    "sent": "You only have bags of labels.",
                    "label": 0
                },
                {
                    "sent": "We've also applied it to.",
                    "label": 0
                },
                {
                    "sent": "Image recognition problems where there might be lots of outliers.",
                    "label": 0
                },
                {
                    "sent": "We've also done this for.",
                    "label": 0
                },
                {
                    "sent": "Image classification.",
                    "label": 0
                },
                {
                    "sent": "And retrieval systems, something like the initial example that I had.",
                    "label": 0
                },
                {
                    "sent": "But here all those experiments can be found in the paper here.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to go over experiments that we did for tracking.",
                    "label": 0
                },
                {
                    "sent": "So we use the discriminants saliency tracker of Hot Divine Investments, LLC.",
                    "label": 0
                },
                {
                    "sent": "VPR 09.",
                    "label": 0
                },
                {
                    "sent": "And we simply use the tangent boosts algorithm to combine the saliency Maps in a discriminate manner.",
                    "label": 1
                },
                {
                    "sent": "In these two.",
                    "label": 0
                },
                {
                    "sent": "Videos that I'm going to show you the red boxes for the case where the tangent loss is used and the blue dotted boxes for the tracker that uses the exponential loss.",
                    "label": 0
                },
                {
                    "sent": "So hopefully this will work OK. That's weird, it works on my computer.",
                    "label": 0
                },
                {
                    "sent": "Oh, it runs here.",
                    "label": 0
                },
                {
                    "sent": "I don't know why that's Oh well that one works OK. That's still good.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can see that using the robust tangent loss keeps track of the target, whereas the exponential loss because of all the.",
                    "label": 0
                },
                {
                    "sent": "Outliers basically loses track of the target and wanders off and drifts off.",
                    "label": 0
                },
                {
                    "sent": "Let's try this one again, maybe it'll work this time.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Oh well.",
                    "label": 0
                },
                {
                    "sent": "It was basically a guy running around the field.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, well in conclusion, we argue that being too correct should be penalized for robust classification.",
                    "label": 1
                },
                {
                    "sent": "So we derive the set of requirements that are robust, Bayes consistent loss function should have.",
                    "label": 0
                },
                {
                    "sent": "And we from that we derive the tangent loss that is robust and based consistent.",
                    "label": 1
                },
                {
                    "sent": "And we demonstrate that the tangent boost algorithm.",
                    "label": 0
                },
                {
                    "sent": "As state of the art results on a variety of challenging datasets.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's about it, thank you.",
                    "label": 0
                },
                {
                    "sent": "We have time for a few questions.",
                    "label": 0
                },
                {
                    "sent": "I have one so your curve has a sense of scale and size.",
                    "label": 0
                },
                {
                    "sent": "The number of parameters to it.",
                    "label": 0
                },
                {
                    "sent": "How did that come about?",
                    "label": 0
                },
                {
                    "sent": "Anne?",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "How can you design it to fit particular data set you might have?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me go back to.",
                    "label": 0
                },
                {
                    "sent": "That so you are asking about the shape of this.",
                    "label": 0
                },
                {
                    "sent": "Yeah so I didn't talk about this in the interest of time, but there is a parameter on this loss function that allows you to control the ratio of how much penalty is assigned to the positive side versus the negative side.",
                    "label": 0
                },
                {
                    "sent": "So by tweaking that parameter either like through cross validation or depending on how much outliers you expect to have in your data, you can change the shape of this loss function.",
                    "label": 0
                },
                {
                    "sent": "From anything where it penalizes the being correct as much as it does being wrong, which is of course something you don't want to do, but it can also go to the all the way to the other extreme where.",
                    "label": 0
                },
                {
                    "sent": "It almost doesn't penalize the positives at all and starts behaving like previous loss functions so you can tweak it and change its shape so you can ask a question if you go to microphone.",
                    "label": 0
                },
                {
                    "sent": "Please.",
                    "label": 0
                },
                {
                    "sent": "Have you considered that the use of a Bregman divergences answer gates and this kind of thing?",
                    "label": 0
                },
                {
                    "sent": "I actually, if you do go to our NIPS 08 paper the proof of like that formula there passes through the use of Bregman divergences and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "So all those do come into play.",
                    "label": 0
                },
                {
                    "sent": "You can see the connections, hopefully by reading the paper, but that's how that's where this formula comes from innocence.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you, thank you.",
                    "label": 0
                },
                {
                    "sent": "So obviously your loss function is non convex and I was wondering that how much this is going to affect your performance because obviously you are going to fall into a local minima so I get this question all the time.",
                    "label": 0
                },
                {
                    "sent": "It's a good one but it has a simple answer.",
                    "label": 0
                },
                {
                    "sent": "When we're doing Minimizations were not minimizing over the loss, we're minimizing over the expected loss.",
                    "label": 0
                },
                {
                    "sent": "So the last function, as I mean we show this in our NIPS paper, but the loss function need not be convex for the optimization problem or the risk to be convex.",
                    "label": 0
                },
                {
                    "sent": "So although this loss function is not convex, what I'm minimizing the risk is convex, and that you can.",
                    "label": 0
                },
                {
                    "sent": "You know that can be shown.",
                    "label": 0
                },
                {
                    "sent": "So the problem is still convex.",
                    "label": 0
                },
                {
                    "sent": "Let's thank the speaker again.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}