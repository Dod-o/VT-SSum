{
    "id": "h5qz2tr3gapipdulkatnnh5qrapsbent",
    "title": "Large-scale Data Mining: MapReduce and Beyond",
    "info": {
        "author": [
            "Spiros Papadimitriou, IBM Thomas J. Watson Research Center",
            "Jimeng Sun, IBM Thomas J. Watson Research Center",
            "Rong Yan, Facebook"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/kdd2010_papadimitriou_sun_yan_lsdm/",
    "segmentation": [
        [
            "Welcome everyone and a good morning.",
            "So this is 1/2 day tutorial on large scale data mining map reduce and beyond.",
            "Technically I'm on vacation between jobs, so anything that's beautiful.",
            "Google is at an outside eye.",
            "Tutorial on Japan at IBM Research and Wrong at Facebook.",
            "So the tutorial has three parts.",
            "I'm going to be doing the basics, making sure that everyone will and uses give some basic examples of how to write programs for map reduce and then introduce you to some other tools that are part of which is an open source project.",
            "Japan is going to is going to be going over algorithms, data mining algorithms using that reduce and finally wrong will go over."
        ],
        [
            "Applications."
        ],
        [
            "So we have data everywhere.",
            "These are statistics.",
            "Actually user agreement already, so Flickr has 3 billion photos.",
            "YouTube has had a few years ago actually a couple of years ago 83 million videos, 15 hours of video uploaded for every minute.",
            "The web has 10 billion videos watched per month.",
            "There is 500 million photos.",
            "Jackel webmap is 3, three on mix, and so on.",
            "So there is a ton of data."
        ],
        [
            "Everyone right and valid question in this case is, So what?",
            "So."
        ],
        [
            "A common argument is that well, more than."
        ],
        [
            "There is obviously more."
        ],
        [
            "But more is also."
        ],
        [
            "Different.",
            "An opportune it is in this case, so we have a lot of data.",
            "We have real time access access to content, right?",
            "We have richer context from users and hyperlinks, so pretty much everything you do or a lot of the things you do these days are captured in some form online, either through your mobile phone or through Facebook and so on.",
            "And all this data gives us abundant training examples, right?",
            "So we can have now billions or trillions of training points.",
            "So in this case, brute force methods may suffice, and by brute force we mean something that has a very simple model but can be trained using billions of points.",
            "However, the challenges here are that this data is typically much much dirtier than smaller data sets of that are that are carefully cleaned and the data points are cherry picked and so on.",
            "So we need efficient algorithms that can process such huge volumes of data and we need them to be able to scale."
        ],
        [
            "Within reason."
        ],
        [
            "Cost so you see there is this dual part, so we may be able to do things by applying simpler algorithms that can use billions or trillions of data points.",
            "Use a lot of training examples.",
            "However, we need to be able to use that data.",
            "Right so."
        ],
        [
            "So."
        ],
        [
            "There is an old quote by a famous statistician George Box that says all models are wrong, but some are useful."
        ],
        [
            "And Peter Norvig at Google has recently said that all models are wrong, and you can increasingly succeed without the."
        ],
        [
            "Alright."
        ],
        [
            "There are a lot of examples that actually employ a very simple models, such As for example, Google Page rank.",
            "Google Page rank does not alter the way that ranking works.",
            "It doesn't try to understand the semantics of a page or anything like that.",
            "It's just a very simple random walk on the on the web graph language translation is another example.",
            "Instead of trying to understand grammar and sentence structure and so on, we can often go ahead and use, let's say, trigrams or pentagrams and use the entire web as a training corpus and that does better than most other method."
        ],
        [
            "And so on.",
            "Um, one small parenthesis to get over some terminology.",
            "This is perhaps slightly nitpicking, but there is a lot of talk about cloud computing, which basically"
        ],
        [
            "Means the Internet, so network connected, and by that I mean network connectivity that is present everywhere on your computer on your mobile phone, and so on, as well as standard protocols to allow communication among those connected devices."
        ],
        [
            "Plus, commoditisation and standardization of CPU of network of storage and so on."
        ],
        [
            "As Eric Smith has said, this is what he and many others have worked towards during the rent their entire careers.",
            "It's just something that is happening now, right?",
            "So cloud computing is something much bigger.",
            "It's an overloaded term.",
            "It means."
        ],
        [
            "So this tutorial is not about cloud computing, but it is about large scale data processing right?",
            "Which may or may not be related to cloud compute."
        ],
        [
            "And as I tried to make the point earlier, basically we have tons of data that we need to leverage and we need to be able to build algorithms that are designed to use tons of data and be able to scale.",
            "And we also need of course the platforms and the infrastructure to do this right to build these algorithms that will use the large volumes."
        ],
        [
            "Data, so in this tutorial we're going to show you how to do this.",
            "So as I said in the first part, we're going to go over the basic concepts and tools and introduce the basic notions of map reduce as well as distributed storage, and then introduce some other components.",
            "Higher level components for data processing such as Hadoop, such as Sorry, HKS Pig, Cascading High and then Jim is going to give you examples after the break.",
            "On how you can apply these ideas for data analysis and data mining tasks in information retrieval in graph analysis, clustering and classification, and finally, in the left part wrong with over applications in text processing, data warehousing and machine."
        ],
        [
            "Learning.",
            "So let's start with the very basics and please feel free to interrupt at any point with questions.",
            "If something is not clear."
        ],
        [
            "So what is MapReduce right?",
            "Is my producer programming model?",
            "Is it an execution environment or is it a software package?"
        ],
        [
            "And basically, depending on who you ask.",
            "People mean all of these things at different times."
        ],
        [
            "So map reduce for this talk is basically we're going to cover the programming model for distributed computation that relies on distributed storage in a cluster of thousands 10s of thousands of machines, and it also provides scheduling."
        ],
        [
            "Fault tolerance so let me introduce the programming model first with a very simple example.",
            "Let's forget about clusters and large scale and all that stuff for for a moment.",
            "And let's just say we have a file on let's say your laptop called employees TXT that has three columns.",
            "The last name, the first name and the salary of each employee and I want to do a very simple thing finding frequency computer histogram of firstname frequencies.",
            "Or count so I could write a Python script in a few minutes to do this.",
            "Here is one way you can do it in Python, but."
        ],
        [
            "Other scripting languages have similar constructs, so first you have to open your text file and in this case when you open your text file, you'll get one way to interpret the the file pointer.",
            "In this case is a nitrate Aurora sequence over the lines of the file, right?"
        ],
        [
            "And then you can take each line of the file.",
            "What you want to do first is extract.",
            "Of course the first name, so the map operation.",
            "Basically these are products from functional programming and operation will take a function and apply it to every element in a sequence and producer transformed sequence.",
            "So in this case we have a function that takes a text line split."
        ],
        [
            "Sit on the field separator and returns."
        ],
        [
            "The second field, right, which is the first name and if we apply this to every text line will get, will get a sequence of just first names."
        ],
        [
            "The reduce operation is basically an accumulation operation that will basically apply an operation on every sequence of an input file of a sequence.",
            "So now we have a sequence data immediate sequence of the first name, and we apply the accumulation function which basically has the histogram which is a hash hash table key by first name and the value is the account right?",
            "And for each name we see we go in increment by 1.",
            "Responding count.",
            "In the hip."
        ],
        [
            "Now."
        ],
        [
            "You'll notice that at every point we have three key value items, right?",
            "I regular just a fancy name for sequence, so we open the.",
            "The initial sequence doesn't have a key, has just the value, which is the text lines.",
            "We transform this two first names and then we accumulate these and we produce another sequence of key value pairs, which is basically the the histogram values, right?",
            "The key is the name and the account is the value."
        ],
        [
            "So we can do the same.",
            "This is the basic programming model for formatted use right?",
            "And I'm going to show you an example of how the same thing can be implemented in Hadoop, which is an open source implementation of map reduce to run on clusters of clusters of a large number of machines.",
            "So again in this case we have them."
        ],
        [
            "Function Now most of this is boilerplate, but this is the map function and what happens there?",
            "Basically so we have the key in the value that appears in the normal values.",
            "Aligning the textile and first name is basically a mutable string, and what we say is, well, get the value which is the text line converted to Australian splitting.",
            "The separators.",
            "Take the second field and then set the mutable string.",
            "We have first name to that value and then output this.",
            "This first thing has a key with a value of 1, so one there is a long writable, which again is a mutable login loop."
        ],
        [
            "And.",
            "So you write a matter that is type for the keys and values of both input and the output, and this applies the transformation on each item of the input and produces an item."
        ],
        [
            "In the output stream the output sequence and then we'll do the reducer, which basically adds up."
        ],
        [
            "Count so here we get the key, which would be the first name and the sequence of accounts that we have collected from for this key, right?",
            "So initially instead of admitting the instead of admitting the just the names, you see where I admitted the name in the count of 1, right?",
            "So then I just need to get old and all that ones for that name and then add them together and will get the count of all names, right?",
            "So this is what happens here.",
            "Of values is an isolator of all the values for this particular key, and then I'll add them and then output the key, which is the first name and the total sum.",
            "In fact, in Hadoop you don't really have to write this.",
            "This is provided as a library class.",
            "This is a fairly common ad."
        ],
        [
            "Location operation and then there are a few other things that we need to do.",
            "I'm not going to go over details, but basically you have to set up the job set up for the input, say which is the mapper function, the reducer."
        ],
        [
            "Function and so on and so forth, and then execute the job on the cluster.",
            "So this is about 30 lines of code, most of it is boilerplate.",
            "You can get an Eclipse plug-in to do most of this for you, plus 5 lines of actual code from the map in the reduce function."
        ],
        [
            "Before I continue one point here.",
            "So this is the map reduce is a programming model and it has been started for distributed clusters, so there is Google's original implementation and there is also an implementation in Hadoop which is an open source project, part of the Apache Software Foundation, but the same programming model has been investigated on how you can implement it on top of of different architectures.",
            "So for example, people have looked on how to use the MapReduce model for single chip.",
            "For for symmetric, multiprocessors or single chip multiprocessors the Phoenix project in Stanford there is an implementation for the cell broadband broadband broadband engine which is the CPU that the PlayStation and so on and so forth have.",
            "And then there are many other implementations in programming in script in scripting languages in libraries, even for shell scripts and."
        ],
        [
            "So on.",
            "In this tutorial, we're going to focus on map reduce for distributed clusters, so presented the basic abstraction, right programming abstraction, and then we'll see how this applies to."
        ],
        [
            "Large clusters.",
            "The point here is that I showed you originally a quick and dirty script that will take a text file of employees and produce a histogram of firstname counts and this is about 5 lines of Python or 5 lines of non boilerplate code in Hadoop.",
            "The difference here is that the Python script can run on a single machine, but if I had let's say billions or trillions of of of employees, the process for the sake of example this would start getting tricky with Hadoop.",
            "Basically I can take the same thing executed on my laptop, but I can take the same thing and transparently executed on a cluster.",
            "It has up to thousands of machines and thousands of disk drives storing huge amounts of data."
        ],
        [
            "And in order to achieve this, basically a lot of things are hidden right?",
            "So the data will be partitioned and replicated across different nodes in the cluster.",
            "The computation will actually be shipped to machines that have copies of the data of different fragments of the data.",
            "And then we have to deal with various parameters dealing with concurrency and so on in order to ensure for color.",
            "And so there's a lot of things that going on."
        ],
        [
            "Go on, but as a programmer, basically you don't need to know any of this, right?",
            "You can write it very similarly as you would write a five line Python script and then just have it execute on a huge cluster.",
            "Um?"
        ],
        [
            "So map reduce innocence is important not only for what it does, but for what it doesn't do, right?",
            "So here is what happened, right?",
            "So here is how the the the flow things go, so the input file is split into different chunks or file splits.",
            "So let's say split, split, zero is text lines one 200 split, one is text, line 101 to 200 for example, and so on, right?",
            "Now."
        ],
        [
            "The map function will be applied on each line of the inputs, right?",
            "So there will be a mapper that processes one chunk of the input file, so each chunk of the input file will be processed by a mapper, which is a separate pro."
        ],
        [
            "This is essentially just a sequential scan over the data right?",
            "That will take each input element and transform it."
        ],
        [
            "So let's say we have the text lines from different parts.",
            "Then the mapper will apply the transformation, which in this case was extracted first name and it has inherently account of 1."
        ],
        [
            "So it will do this transformation.",
            "No.",
            "What will happen next is that.",
            "Based on the key that the Mapper emits."
        ],
        [
            "So the output elements from the mappers will get hash to a certain reducer.",
            "So John old.",
            "Basically this hash function ensures that all Jones will get sent to the same reducer, right?",
            "So, whoops.",
            "So.",
            "All of the all of the map outputs that have a key of John."
        ],
        [
            "And will be sent to the same reducer and then we'll apply the reduction operation and just some of the counts.",
            "1 + 1 = 2 and then this will be the output from the reducer which."
        ],
        [
            "Will be sent to the output."
        ],
        [
            "Now here's what happens when this gets placed on our cluster of many machines.",
            "First of all, the different chunks of the input files will get replicated on many machines, so you'll see for example, that split zero is on machine 0, machine one and machine 2 split one.",
            "There is a copy of that and host zero hosts.",
            "Host three and one more machine, and so on.",
            "No."
        ],
        [
            "The mappers that each mapper as I said, processes one chunk of the input will get shipped to machines that have local copies of the data.",
            "This may not always be possible, but this is what map reduce in this kind of setting tries to do right?",
            "So computation will be Co located with data as much as possible, and when you have huge volumes of data, it's often cheaper to ship the code to the data than to ship the data to the code, right?"
        ],
        [
            "And then the reducers will also get placed in machines in the cluster.",
            "Here you'll unavoidably have to have network communicate."
        ],
        [
            "This is Racun network aware to a certain."
        ],
        [
            "And and in addition, you can have Combiners which run inside the map processes, which basically do a data reduction.",
            "So for example, let's say you had in the same chunk, you had 10 Johns instead of sending over the network to the reducer.",
            "John one, John one, John, 110 times.",
            "You'll do a pre reduction in Saint John 10 from this machine to the reducer, right?",
            "So you minimize how much data is sent to the reducer, right?",
            "And then the Reducer will collect all the jobs from all different mappers on different machines and do the final reduction step.",
            "So this is the job of the combiner.",
            "It's a, it's actually an optimization.",
            "Your program will be correct without it, but it helps a lot, and it's actually very important in practice to do this kind of simple optimism."
        ],
        [
            "So in sum."
        ],
        [
            "Free MapReduce is, first of all a simple programming model and it allows you to express computations very simply that will run on a huge cluster of machines and it provides scalability and fault tolerance.",
            "If you can express your computation in this model, then my produce will take care of of yes.",
            "Tolerance."
        ],
        [
            "And are they end minus one?",
            "Sorry the the replicas.",
            "So yes, primarily is for fault tolerance.",
            "You can set the number the replication level on a per file basis.",
            "So on HTFS you can say for this file I want to this this each chunk of the file to be replicated 1, two or three out of the box.",
            "It's three, but you can set it to any number you want and the other reason you want to have it replicated depending on how hot the data is.",
            "So data that is accessed a lot.",
            "You probably wanted to have to have it replicated.",
            "On more machines, right?",
            "It's not a property of the job, it's a property of the of the of the file on the on.",
            "HTFS on the Hadoop distributed file system.",
            "You can change it, but it takes time, right?",
            "So whenever if I say to HTFS, change the replication factor from three to five, it will take some time to make the additional 2 copies and send them over the cluster."
        ],
        [
            "Any other questions?",
            "Yeah.",
            "No, no so so.",
            "So each mapper will be so the execution engine for MapReduce will make sure that basically.",
            "For each chunk, everything will be executed just once, so you won't execute it on all of the replicas.",
            "You'll find one replica that has a copy of the data.",
            "Send the map task there, do the transformation and this data will actually be checkpointed.",
            "And in this case it actually, if your replication factor is higher, your chances of finding a machine that's free and has a copy of the data are generally higher, right?",
            "So if but basically the But basically the job of the MapReduce execution engine is to make sure that that you get the semantics that everything will be executed exactly once.",
            "Yeah.",
            "Manage the splits yourself, no well.",
            "Uh, you can.",
            "Write your own classes that determine how the data is is split and what goes into a different split.",
            "But for things like text files or binary sequence files of key value pairs, or if you're using H base for these kind of.",
            "For these kinds of inputs there are classes that will do that for you, so basically you don't need to worry, worry about it.",
            "But if there is a.",
            "First application or you can tune that part if necessary, right?",
            "So that's part of the library, but our API's to do that, yeah?",
            "Basis.",
            "So what do you can you?",
            "So what do you mean exactly by by skew so?",
            "So you're not talking about the input.",
            "The input file splits now skew.",
            "The input file splits.",
            "It's not, you can just do it based on the on.",
            "The skew is not that much of a problem in the input file size.",
            "You can basically split based on on size.",
            "Of course it doesn't necessarily mean that if I have one kilobyte of data here and another kilobyte of data, it will take an equal amount of time to process.",
            "Right so yes, in that case you could if you have an idea of two pre to estimate that in advance that would help.",
            "But if you have a huge amount of skew and one chunk will take 3 hours to process and all of the other million chunks will take one second to process, then you may have a problem.",
            "But Jenn, yeah.",
            "But generally, if you're dealing with huge volumes of data, this kind of discrepancy.",
            "The amount of time it takes to process a chunk compared to the time it takes for a job to complete is usually just a small fraction, right.",
            "Skew can be a bigger problem in the reduce.",
            "When you when you send the partition data over the over to the Reducer and I think Jim will be giving some examples on how you can deal with it in some cases, but.",
            "Then it starts becoming trickier.",
            "Um?"
        ],
        [
            "So yeah, this is ideal for preprocessing large for processing or pre processing large volumes of data.",
            "And there was the original article on map reduce from Google was invited to see ACM January 2008 and it was pre faced by David Patterson computer architecture Berkeley and he said basically if the data center is the computer then basically the equivalent of the ad machine language instruction is basically MapReduce right?",
            "So this is a low level, a simple low level programming API abstraction.",
            "To do."
        ],
        [
            "Things on a cluster on a large cluster of machines.",
            "So this is the the the the core part right?",
            "So we have distributed storage and a way to ship to to write computations on this distributed file system in a way that will the computations will get sent to the appropriate places on the file system and provide for fault tolerance and scalability.",
            "Now on top of this.",
            "But as I said this is the low level."
        ],
        [
            "Part so on top of this there are many other components.",
            "Now Hadoop is an open source project, part of the Apache Foundation and it has several different components.",
            "Many are added overtime."
        ],
        [
            "So that Hadoop stated mission.",
            "This is from an interview from one of the founders of the of the project.",
            "Cutting is to commoditize the infrastructure for web scale data intensive application."
        ],
        [
            "Who uses Hadoop?",
            "Well, it was primarily developed in Yahoo.",
            "So Yahoo is is probably the biggest user.",
            "The next biggest user is is probably Facebook.",
            "It has a large large cluster and actually wrong in the third part will be talking a little bit more about that and giving you describing how Hadoop is is actually used in Facebook last FM.",
            "The Internet radio.",
            "Service uses Hadoop.",
            "Rackspace also uses Hadoop for log analysis as well as provides Hadoop.",
            "DIG uses Hadoop Apache Nutch which is another open source project which is a web crawler and web Web Index.",
            "Basically an open source search engine also uses Hadoop.",
            "In fact Hadoop was originally built for Notch but eventually split into a separate project.",
            "So wrong will give more examples of a real world uses."
        ],
        [
            "Hadoop in the."
        ],
        [
            "The last part of the tutorial.",
            "So let's see at the different quickly see the different parts of Hadoop.",
            "First of all, there is Hadoop core which basically provides abstractions for file systems and IO as well as RPC and persistence, right?",
            "So these are the basic APIs that allow you to plug in different distributed distributed file systems and ship computations to different."
        ],
        [
            "And so on."
        ],
        [
            "There is a relatively new project.",
            "This is not widely used, but basically when you have RPC you need a cross language way to serialize and deserialize the data that you're going to send over the network.",
            "So this is the kind of the equivalent of the Google Protocol buffers.",
            "If you've heard of those or Facebook thrift."
        ],
        [
            "It's grayed out because it's not."
        ],
        [
            "That active or widely used yet MapReduce is the component that allows you to do distributed execution of large batch jobs on a cluster.",
            "So it provides the API as well as the execution."
        ],
        [
            "Engine."
        ],
        [
            "HTFS is the component that is responsible for the distributed storage, right?",
            "So managing the files split into different chunks, replicated and so on and so forth.",
            "This was inspired again by the Google File."
        ],
        [
            "System Zookeeper, which is basically a coordination service so you can do distributed lock implement distributed locking protocols as well as storing distribute configuration parameters for for for a distributed system."
        ],
        [
            "This is kind of the equivalent of Google Chubby.",
            "And then there is a space which basically provides a higher level data abstraction.",
            "The column and oriented sports store that allows both executing both bad batch jobs as well as random access in individual items in."
        ],
        [
            "In the file, this was inspired by Google picked a big table, then there is pig which is a dataflow language.",
            "It is a procedural language that inspired by by SQL and it makes expressing MapReduce jobs easy."
        ],
        [
            "Here."
        ],
        [
            "Hive is a distributed data warehouse, again implemented on top of HTFS and MapReduce that provides a SQL like query language."
        ],
        [
            "It is Choco which is relatively new.",
            "Actually, a log collection and analysis system.",
            "So the the the different components in Hadoop are growing overtime.",
            "This is a very actively developed project.",
            "The two core components actually remain produce and HTFS, but there are a lot of other things that can make your life easier."
        ],
        [
            "So map reduce.",
            "Let me recap the programming model as I said what you need there are.",
            "Basically two things that you need to specify in order to define a MapReduce program, right?",
            "So you have to define the Mapper and Reducer.",
            "The Mapper will, as I said, take a sequence of key value pairs in the input.",
            "Let's say for example in the.",
            "In the example I gave with the histograms, the key will be void now and the the value will be the line in the text line in the input right?",
            "And then it will apply.",
            "Some function will produce another.",
            "Part of key values and.",
            "Actually, for each, the mapping is not necessarily a one to one right, so it can be 120 or one or more.",
            "So for each input key value pair, the reducer, the data type for the reducer function is basically take the the key type output by the mapper and the sequence of values.",
            "For the corresponding key and then output another key 1 zero or more key value pairs right?",
            "So do a reduction operation.",
            "So for example in the histogram example it would be the first number and account a sequence of counts output by the reducer by the Mapper.",
            "And you will aggregate these and produce again the first name in the total count.",
            "As I said, you can have the combiner, which basically is can be the same as the reducer, although it doesn't have to be.",
            "So in this case you could run.",
            "The combine are inside the Mapper as I said, instead of sending John One John 110 times, you'll send John 10 and actually this is the reason that you'll see the Mapper Outputing account, so the data types are consistent and then you can also define the partitioning function which determines how the data will get partitioned on the different reducers in the cluster."
        ],
        [
            "So.",
            "The Mapper interface in the Java APIs for Hadoop basically have the input K1V1 the key value types for the input and K2V2 the key value types for the output.",
            "You can initialize.",
            "You can do job setup in a method called configure.",
            "You do clean up in close three and then the bulk of the computation happens in the map function right which will take each key value and then output.",
            "Emit key value pairs for the output right.",
            "And as I said, you can emit 01 or more output values for each."
        ],
        [
            "Input value.",
            "The reducer again the data type is K2V2 is the the types for the input key value.",
            "This has to be the same as the output from the mapper and then K3V3 is the key value for the reducer output.",
            "Again, you have a configuring the close method to do the initialization and teardown, and again now here you have keys and a nitrator of values corresponding to this key, right?",
            "So you'll get all the values for this key.",
            "And then you can emit output values.",
            "Through the output collector, again 01 or more."
        ],
        [
            "So some Canonical examples of jobs for which MapReduce was originally designed.",
            "I gave you the example of a histogram of a simple histogram of first names in employee dot T XT.",
            "But basically histogram type jobs are for example if I have a large log file and I want to construct.",
            "A graph, so let's say I have a log of security events, let's say from IP Destination IP and I want to say how many, or let's say a packet trace from source IP, destination IP and I want to construct a graph source IP, destination IP and volume of data over this pair of nodes, right?",
            "So here this is essentially a histogram job only my bucket is the edge, which is a pair of nodes.",
            "K means is another histogram type job essentially whereas based where the buckets are basically the cluster centers, right?",
            "So if I have 10 cluster centers?",
            "Um?",
            "Then I iterate over each input in the over over each point in the input and I assign it to the closest center, right?",
            "This is the the bucket it will belong in.",
            "So this is a.",
            "This is a simplification of K means of course, and Jim will go into more details in the second part.",
            "Another type of Canonical example that for which Hadoop was built is inverted indices right?",
            "So I have a list of documents and basically I want to construct so each document is a list of terms.",
            "I want to construct the inverted index which is for each term the list of documents in which it appears.",
            "If you think of this as a document term matrix, basically I want the transpose of the matrix."
        ],
        [
            "Hadoop is also sorry.",
            "MapReduce is also used for sorting.",
            "You can do equally join and more details.",
            "Again as I said will be given by G. Manganin in Part 2.",
            "Let me show you how you can do joins.",
            "This is something that gets discussed fairly frequently when MapReduce comes up.",
            "So this is one way to do joins.",
            "It's not the only way to do joins.",
            "Let's say I have now 2 two inputs right and one input is employee name and Department ID and the second input is Department name and Department ID, right?",
            "And I want to do the join and find for each employee find the name of the Department for which he or she works.",
            "So what I can do in this case basically, essentially it's a distributed hash joint implementation, right?"
        ],
        [
            "So here is what you can.",
            "So here is one way to do this.",
            "The key for the for the map output will actually be the value on which I want to do the joint, so this will be the employee ID.",
            "So each tuple I limit 7 the Department ID and then I will say this is a blue tuple which is from the first input and it has this in the value of.",
            "The remaining fields is this Smith and I'll do the same."
        ],
        [
            "For.",
            "The couples in the second input right.",
            "Another way I could do this is actually put the the input relation tag in the key."
        ],
        [
            "So I could do that, do it in this way.",
            "In this case I would have to define the partitioning function to make sure that 7 blue and seven red will get sent to the same reducer to the same machine."
        ],
        [
            "But let's say I do the 1st way right, so the map function will take each tuple from both relations and output the key.",
            "Is the join the join column and then the remaining values are in the value.",
            "Of the."
        ],
        [
            "Map output.",
            "So when these get hashed and sent to the reducer, then all of the Department #7 couples from both relations will go to the same machine and in this case you will do basically a nested loop join and then you can output.",
            "For example Smith is in the development Department."
        ],
        [
            "Jones, in the Development Department and so on.",
            "And amid this as output.",
            "Now this is a simple example to show you one way in which you can do joins.",
            "Of course, if you have skew in the distributions and so on, then this can become trickier.",
            "And there are also higher level components that will actually do joins for you, but it's good to know, at least conceptually one way in which you can implement this."
        ],
        [
            "Um, some terminology for HTFS and MapReduce that it's useful to know if you're actually running jobs on a cluster."
        ],
        [
            "Um?",
            "So on each machine on your cluster.",
            "So as I said, there is distributed storage and distributed computation for each machine on the cluster.",
            "For the distributed storage part, you have a data node, which is a process that's responsible for managing local copies of chunks, right?",
            "So the data node basically just knows I have a chunk of let's say 100 lines of a file, and it's on this machine.",
            "Order if you want to access."
        ],
        [
            "The whole file.",
            "Basically there is a name node.",
            "This is a central directory which basically says if you want employees CXT lines is a one to 100 are in machine, let's say 01 and two.",
            "So basically this distributed directory is stored in the in the name node right?",
            "So when you want to access a part of the file will ask the name node and then the client will.",
            "Directly communicate with the data node.",
            "This structure is monitor mirrored for the 4th."
        ],
        [
            "Distributed computation part.",
            "So when you're sending jobs, there is a task tracker that runs on each machine that will basically is responsible for spawning.",
            "Local jobs, right?",
            "So a map?",
            "A map task which will just process a chunk of the input is managed by the task tracker."
        ],
        [
            "And then there is a job tracker which basically coordinates all of these different.",
            "Processes on the on the cluster, right?",
            "And it's responsible for for maintaining the semantics, restarting jobs if they fail, and so."
        ],
        [
            "And so forth.",
            "So.",
            "The example I gave was with Java, but you don't have to use Java to use MapReduce.",
            "That is also Hadoop streaming.",
            "That basically is a wrapper that allows you to write a program in any language.",
            "It will basically serialize your values into just text text format, so you can write a simple script.",
            "It uses so standard input, the input for the mapper will be piped in text format to your let's say script and.",
            "The output that your script produces will be in.",
            "Standard output will actually be parsed and then sent out to the cluster.",
            "You can use any language, of course.",
            "Serialization and deserialization in text format is not very efficient, but if you want to write something quickly, this is very good.",
            "There is also Hadoop pipes which uses a binary format and it allows you to write programs in C++, but you have to use a C++ library for this."
        ],
        [
            "Um?",
            "So now for the.",
            "For the rest of this part, I'm going to briefly introduce other components of Hadoop or other tools built on top of Hadoop that make your life easier as a programmer."
        ],
        [
            "Any questions up to this point?",
            "OK, so.",
            "As I said, one of the Canonical examples for MapReduce is building an inverted index.",
            "Um?",
            "And typically a Hadoop MapReduce is good for batch computations on large datasets, right?",
            "So I have the crawler and it has collected terabytes or petabytes of web pages.",
            "And basically I want to build the inverted index saying this word on which web pages does it appear?",
            "However, in reality the crawl pages themselves are actually updated by the crawler, right?",
            "So the crawl itself, you don't really delete the whole crawl and and do everything from scratch individual pages there will get updated.",
            "It may get, they may get augmented by other parsers or analytics that may extract properties of the pages and so on and so forth."
        ],
        [
            "MapReduce and HDFS provide distributed storage and computation, but they're basically geared for batch processing.",
            "They don't provide any facilities to access or update individual items, right?",
            "So HTFS in particular is optimized for append rights and bulk reads in batch.",
            "So H base basically adds random access read and write operations for individual entries in your your files.",
            "So you can access, let's say individual topples in your input and update them.",
            "Originally it was developed by Powerset, which was a search engine company which.",
            "I think was purchased by Microsoft and it's the original inspiration comes from Google's big."
        ],
        [
            "Table.",
            "The data model for H basis is as follows.",
            "So basically you have.",
            "The units of data are rows.",
            "Each row has a unique key.",
            "Um?",
            "And then there are column families and within each column family you have columns."
        ],
        [
            "The design is so that you can have millions of let's say sorry hundreds of column families and I'm going to give a simple example of this right?",
            "So column families have to be specified in advance.",
            "These are not easy to change on the fly, however columns you can have millions of columns and they don't all have to be populated.",
            "So this is a sparse very can be a very sparse table and then rows are typical in the billions or trillions.",
            "And they are stored sorted by the the primary key.",
            "And this is typically partitioned over many nodes in a."
        ],
        [
            "Cluster.",
            "So the key and cell values are arbitrary byte arrays, right?",
            "So H space does not know anything about what the data is, so you can use a serialization deserialization library on top of this, but."
        ],
        [
            "As far as each base is concerned, he's in keys and cell values are unaffected by the Rays and you can use any underlying data store so you can use a local file system.",
            "You can use the Hadoop distributed file system.",
            "You can use the Amazon Simple Storage Service as three and so."
        ],
        [
            "On so let me give you an example.",
            "Let's say I want to store my employees.",
            "I can define a column family for the profile part and for the profile.",
            "Here I'll say the profile has a last name, first name in the salary and the values will be the corresponding strings, but."
        ],
        [
            "So this is not this is a kind of an arbitrary example.",
            "Let's say for each employee I wanted to store the the pages they've they've bookmarked, or the documents they've bookmarked.",
            "Let's say in the corporate repository, right?",
            "So I could define another column family and let's say, as I said, this example is kind of arbitrary.",
            "The column, the column.",
            "The name could be the document ID or the URL.",
            "So as you understand, this can be extremely sparse data, right?",
            "So within each call each column, each column family.",
            "Basically you can view it as a as a very sparse hash table, storing key value associations where the key is the column name and the value is the cell value."
        ],
        [
            "And you always access data by primary key.",
            "There is no support for in the indices."
        ],
        [
            "In H base.",
            "So comparison of each base versus relational database systems.",
            "This is a different solution for somewhat similar problems with very different solutions.",
            "So RDBMS is are basically row oriented.",
            "They have a fixed schema.",
            "You cannot really add add columns.",
            "On the fly, and they also provide very strong consistency guarantees.",
            "However, H space on the other hand, was designed from the ground up to scale out by adding commodity machines, right?",
            "So it provides a very simple consistency scheme.",
            "It only guarantees that any rights you do in a row if you modify key values, right column names, and the corresponding value in your in your table in within a single row.",
            "This will be atomic, but anything beyond.",
            "Across roles or anything like that, it doesn't provide any transaction or any support for this.",
            "This sort of things which can become pretty complicated if you're dealing with data replicated on a cluster and so on.",
            "The good thing, of course is by by simplifying what it provides, it can provide much better fault tolerance.",
            "It doesn't provide any real indices, and it can also.",
            "It's also designed to support batch processing over the data in addition to the individual look ups by primary key or updates."
        ],
        [
            "So let me move onto Pig."
        ],
        [
            "Which is basically a way to simplify the expression of MapReduce jobs on the cluster.",
            "So in the example I gave in the beginning, I said you have 5 lines of non boilerplate code, but you had all these class definitions.",
            "You had to worry about all the all the older low level details, right?",
            "So writing a significant single MapReduce job requires significant grunt work, right?",
            "So you have to write the code for the boilerplate code for the Mapper and Reducer, passing the parameters, then setting up the job defining the inputs, possibly defining the input and the output formats.",
            "How do I parse my data?",
            "And so on and so forth.",
            "And typically when you do a task, it won't be a single MapReduce job, right?",
            "You will have multiple MapReduce jobs doing different parts of the of the computation, and then you will connect them altogether too, actually.",
            "Achieve what it is you're trying to do.",
            "So this can get complicated and you can end up writing a lot of line."
        ],
        [
            "The code pretty easily.",
            "So Pig is a way to is it is a dataflow language.",
            "That allows you to specify these computations much more concisely.",
            "So it does provide support for data structures, and as I said, it's a dataflow language.",
            "It's inspired by SQL, but it's not decorative lights in like SQL, it's actually an imperative language, right?"
        ],
        [
            "Let me give you an example of how you would do the same thing.",
            "Compute the histogram of first names in pig.",
            "So what you say here in this case is, first of all, specify what my input is.",
            "So I'll say that my input is in that file on HTFS, and it has three columns, the last name and first name in the salary, and I can specify what the corresponding data types are, and in this case pig will be able to knows how to parse this, then for each.",
            "So this will produce a sequence of of records.",
            "Right now for this sequence of records, I'll group them by the first name.",
            "And I'll generate.",
            "A sequence which basically has.",
            "Each element is basically the group key, which is the first name and the value is basically all of the values corresponding to that group asset multi set of all the values corresponding to that group and then on this on this on the output of the group operation.",
            "Basically you say I apply.",
            "Just do the count of the first names right and group.",
            "Here is an alias for the for the group by Key and then finally just write this on in on the output and pig will actually behind the scenes map this into the appropriate MapReduce jobs and execute it on the cluster for."
        ],
        [
            "So very quickly pig schemas schema is basically a tuple, tuple, data type and they are optional, right?",
            "So the data loading step is not really required if you don't have you.",
            "If you don't have.",
            "If you don't specify the data types for the input, then basically you can access them in a similar way as you access them in AWK.",
            "Right Dollar Zero is the first field dollar one and is the 2nd field and so on.",
            "It does have support for the most common data types and does have support."
        ],
        [
            "Or for nesting values.",
            "Very quick summary of the features so you have constructs for to do data loading and data writing.",
            "You have filtering operations that will filter filter elements in your in the stream of values Group by operations, join operations, sorting operations and finally union and split combining operations."
        ],
        [
            "No.",
            "Let me move onto cascading so cascading basically is another another way which you can express jobs for express MapReduce programs.",
            "In an easier way, right?"
        ],
        [
            "It's a library.",
            "It's not a new language.",
            "So basically it's a Java.",
            "It's a Java library that you would use the higher level abstraction here is for fields and topples fields you can think of as column names and couples you can think of well as couples.",
            "And pipes are basically is an abstraction for transformations that apply on sequences of of values of couples or on groups.",
            "And so pipe supply operations to sequences of couples.",
            "And then you have tap schemes and flows.",
            "Basically these are a way to specify the input and output formats, and again the goal is to use the."
        ],
        [
            "And of of Multijob flows."
        ],
        [
            "So.",
            "This is again the same example.",
            "I'll try to give you the high level idea of how you would write cascading programming cascading.",
            "So here again I want to compute a histogram of first names.",
            "So first I create the pipe and give it to the head of the pipe and I give it a name.",
            "This is the actually should be first names, not last name.",
            "And then the first thing I'll do basically is I have a function I specify a function.",
            "There is a library of standard functions, or you can.",
            "There is a regular expression splitter ready for you.",
            "Basically, this says create a splitter that will split on the tab and will generate 3 fields, right?",
            "So the top of the names of the first, second and third element of that output tuple will be last, first in salary.",
            "That's what this says, and then the first thing that this pipe will do.",
            "As I said, a pipe is something that transform sequences of couples.",
            "Is.",
            "Apply this splitter on each text line, so this basically says for each line in the input, which is a text file, apply the splitting function right so the text line, which is a single string, will get split into first name, last name and last name, first name and salary, and the next thing you want to do is basically group by the first name, right?",
            "So that's the next thing that happens in the pipe, right?",
            "So you connect it at the tail and then.",
            "At this on the sequence now of groups generated what you want to do is aggregate.",
            "And output account value and this is account aggregation operation right?",
            "So this is expressed by by there are different classes for different aggregation operations and then you say for every group Now apply this aggregation operation and generate account outlook and then."
        ],
        [
            "Then you have to specify what kind of format you have for your input.",
            "It's a text file where it is, where is it located?",
            "It's on the Hadoop file system and then connect all these things together and create a flow which is an instance of a pipe and then."
        ],
        [
            "So.",
            "As I said, pipes transform streams of couples and there are.",
            "Basically, at each pipes which supply a function on each element of the table group by and Co Group operations core Group is a kind of join.",
            "Every applies aggregation operations on groups of couples.",
            "And so on.",
            "The operations is what is done to tackles, so you can apply functions.",
            "You can filter topples so you can apply filter operations that will drop tables and you can do aggregation operate."
        ],
        [
            "When you have groups.",
            "Anne."
        ],
        [
            "And last part 5.",
            "And actually wrong will go into a little bit more detail in the third part.",
            "So how was originally developed at Facebook?",
            "Now it is a Hadoop subproject and it's a data warehouse infrastructure.",
            "It uses MapReduce as the execution engine and Hadoop distributed file system as the storage back end.",
            "It's geared to process very large data sets, an example which wrong will cover is the Facebook daily logs from from the different web servers.",
            "So which this data grows exponentially 30 gigabytes in January 2815, terabytes last year and.",
            "I don't know how much it is this year.",
            "We'll find out later and it it has Hive query language which is a SQL like query language.",
            "Basically the goal here."
        ],
        [
            "Is to make it easy for people that are for people that are familiar with SQL and data warehouses to actually.",
            "Do these kinds of operations on on, on, on.",
            "On Hadoop?",
            "So this is what the same example again would look like.",
            "Basically, the first the first lines say.",
            "This is the text file and this is how I want you to interpret it as a table, right?",
            "So this is it has a last name or first name in the salary.",
            "It is row format, delete delimited by Newlines and the fields are terminated by tabs.",
            "It stored as a text file and its location.",
            "Is there an HTFS or whatever?",
            "So once you say how do I interpret this this text file as a table essentially?",
            "Then you can do just the SQL operation.",
            "Select the first name and account.",
            "Count starter count one from the records group by first name, and that's it.",
            "That's almost identical to the way you would do it in in SQL, right on a traditional data warehouse."
        ],
        [
            "The data should belong to tables, but you can also use as I showed pre existing data again like big data loading is optional but encouraged.",
            "There is the notion of of partitioning columns.",
            "I'll mention it here so you're aware of this.",
            "I'm not going to go into details, but basically there is two kinds of columns.",
            "The partitioning columns are basically mapped into HTFS directory.",
            "So let's say I have my log file right and for each log entry I have a data, timestamp and other values right?",
            "So for example, the user name and what he did, his actions and so on.",
            "If I decide that the partitioning columns are the date and the time, then basically the data will be partitioned in files based on these on the values and then the remaining columns will actually be stored as values in those files within those within this directory structure.",
            "Which again is stored on the distributed file system.",
            "It has support for the most common data types and it has support for pluggable serialization, so you can define your own way."
        ],
        [
            "To to read data from discount, right?",
            "It supports most of the basic SQL functions, so you can do from subqueries.",
            "You can do joins, but only Equi joins.",
            "You can do group by and also you can do sampling of couples.",
            "As I said, it's extensible so you can write your own MapReduce if something you cannot express it in this subset of SQL, you can write your own method you script, you can write your own user defined functions.",
            "For more complicated things, you can also write your user defined types as well as serializers, deserializers that determine how the data is read."
        ],
        [
            "And written right in Word format so.",
            "In summary we went over the basics.",
            "If there is one thing that you should remember from this part is the core concept right?",
            "The distributed file system and MapReduce as both are programming as abstraction as well as how it gets mapped and executed on a cluster of machines, right?",
            "So these are these are the basic things that you should remember from this.",
            "And then we briefly covered a bunch of other tools.",
            "Didn't go into detail, but.",
            "Mention things that you should be aware and try to give you a general idea of what different things are for so you can go and look look into them further.",
            "If you're interested right so."
        ],
        [
            "All of these provide scalability."
        ],
        [
            "And all of them except map reduce our higher level tools, write map, reduce as David Patterson said, it's kind of the machine language equivalent right for?"
        ],
        [
            "For for a for a cluster.",
            "Um?",
            "MapReduce itself and cascading use an existing language, right?",
            "So you can write stuff in Java, or you can write it in any language of your choice with Hadoop, St."
        ],
        [
            "And so on.",
            "Most of them have most of them support some notion of data type or or schema, and all of them have support."
        ],
        [
            "Or for user defined data types, hive and pig.",
            "Designed with easing the transition for people that are familiar with traditional data processing languages like SQL in mind."
        ],
        [
            "So related projects now at a higher level.",
            "There are other projects that again are for large scale data processing on distributed clusters that is dried and Dryad LINQ, which has been developed at Microsoft.",
            "Um, there is social, which is basically a language kind of you can think of it as a talk for for, for.",
            "For a cluster.",
            "It's developed at Google.",
            "There is also Dremel that recently came out of of of Google.",
            "I think it's going to be in this year's VLDB.",
            "This is geared for more towards real time data processing right?",
            "As opposed to writing it.",
            "So typically let's say when you do data analysis, you want a quick turn around and tried many different things, right?",
            "There is also, as I said, big table is the the original inspiration for for 4H base and disappeared in OSD I 2006 from Google and there is also another implementation open source called Hyper Table at the lower level at the storage there is yet another implementation of.",
            "Ideas similar to GFS, so our Cosmos file System is an example developed by a company called Cosmic's Virtual Storage Network.",
            "Then there is of course the elastic compute cloud and the simple storage service in Amazon which provides access to storage and computation.",
            "There is in the energy in academia there is sector and sphere again which is for large scale data processing and data mining on clusters of machines.",
            "Again this is released as open source and a lot of other."
        ],
        [
            "Projects right, but let me.",
            "This is the last slide for this part.",
            "And then we can all go and take a break.",
            "So MapReduce is basically a simplified programming model that allows you to.",
            "Express programs for execution on a cluster, and it's built from the ground up to support scalability, fault tolerance, and run on clusters of commodity hardware.",
            "Right there is, of course, a growing collection of components on top of on top of.",
            "MapReduce and and HTFS.",
            "Some of them.",
            "Part of the Hadoop project, some of them not.",
            "And again, this is a very active, very active area."
        ],
        [
            "So this concludes the first part of the talk.",
            "Algorithms and will give you will explain how.",
            "So if you want to do, let's say the inverted index computations or if you want to do page rank or if you want to do K means or if you want to do naive base and so on.",
            "What is the map function and what is the reduced function that you would write and typical it's going to be more than one map reduce jobs for this.",
            "So this is the second part of the tutorial which is going to be about one hour and then finally wrong is going to give real world examples of uses of Hadoop in both of our research setting and an industry setting.",
            "And give you examples of applications."
        ],
        [
            "Weather all this is used in practice.",
            "So thank you for any questions.",
            "Yeah.",
            "Cloud environment.",
            "Cloud computing environment.",
            "What exactly do you mean by cloud?",
            "What do you think?",
            "As I said, cloud computing is a somewhat overloaded term.",
            "I can give you my personal biased opinion, so different people mean different things with cloud, so for some people cloud means virtualization and pretty much what Amazon EC2 does.",
            "For some people, cloud means what Google does internally.",
            "For other people, cloud means.",
            "Um?",
            "Basically, the set of different protocols on the Internet and the fact that now network is a commodity and you can have distributed services running.",
            "So the point I was trying to make is that cloud computing should not be confused really with large scale data processing, right?",
            "I wanted to make the point that this is really.",
            "I wanted to get out of the cloud computing part an just because this is really an overloaded terms that that many people used to mean different things, right?",
            "As I think Larry Ellison said that Orange is the new pink is what cloud computing is about it anyway?",
            "Yeah.",
            "How?",
            "Sorry, can you speak?",
            "HFS.",
            "The reason we didn't cover Cassandra is basically it's.",
            "It's a.",
            "It's a different storage system.",
            "So within this tutorial we focused basically on map reduce and HTFS.",
            "And things built on top of this Cassandra is kind of kind of a separate.",
            "It doesn't really fit in the in the in the MapReduce.",
            "And HTFS kind of setting, right?",
            "But again, it's a it's a.",
            "It's a distributed key value storage system.",
            "That is gaining a lot of popularity so.",
            "But I would treat it as kind of separately.",
            "Yep.",
            "In practice.",
            "Well, not yet.",
            "Learn more about this next session.",
            "The pipelines yet another classic example ocean borders, you know.",
            "Essentially, word counts.",
            "The new will hear more of it.",
            "You know if I were writing in jobs other kind of want to get my networks of map reduce jobs in each other so that it would be kind of a sufficient possible so we can fill in that region best files all the time.",
            "So if anybody has any context, how did these pipelines get in trouble?",
            "Speeding in Mars.",
            "Um, so yeah.",
            "Jamaica is going to give you examples in the next section of more.",
            "Actually the the pipelines can get.",
            "Pretty deep.",
            "It's actually then the size of the pipeline is not even deterministic, right?",
            "So for example when you're doing K means each came in, the iteration will typically be one map reduce job if you so generally for clustering operations, each iteration will be a MapReduce job.",
            "Each iteration for page rank for the.",
            "For order for the first singular vector, let's say will be a MapReduce job, right?",
            "And you will repeat it to convergence.",
            "So typically these can get the chains can get pretty long.",
            "And as I said, Jim Inc is going to give examples of how you do page rank, how you do K means, and all these things in the next part.",
            "Is this a bit more efficient when in the pipeline for long as opposed to job after job after job?",
            "Well, the the different jobs are executed sequentially, right so?",
            "So the efficiencies, again different.",
            "The If you have you.",
            "Job is known to consume.",
            "Previous scheduled in the same time consuming for each others.",
            "Is that more efficient the wait so you're saying something that a job, one that produces output for a job to to run concurrently?",
            "This is not so.",
            "The output of the output of one job will go to HTFS.",
            "It's not no so, and actually, if you want to, if you want to change jobs and then fault tolerance becomes much much much trickier, because if I have a chain of all of this running concurrently and I'm 100 deep and one job in the beginning, fails.",
            "What do I do?",
            "So you need to have some kind of checkpointing in the middle, and typically what you do.",
            "Each MapReduce job will dump the output going between Mapper and Reducer.",
            "You don't go to HTFS, but when you change different jobs you do go through HDFC.",
            "So.",
            "OK, so well, if there are any other questions you can come to me will take.",
            "We have actually 15 minutes left for the break.",
            "Will start around 10:30 ten 35 and as I said you men will be giving examples of algorithms in MapReduce.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Welcome everyone and a good morning.",
                    "label": 0
                },
                {
                    "sent": "So this is 1/2 day tutorial on large scale data mining map reduce and beyond.",
                    "label": 1
                },
                {
                    "sent": "Technically I'm on vacation between jobs, so anything that's beautiful.",
                    "label": 0
                },
                {
                    "sent": "Google is at an outside eye.",
                    "label": 1
                },
                {
                    "sent": "Tutorial on Japan at IBM Research and Wrong at Facebook.",
                    "label": 0
                },
                {
                    "sent": "So the tutorial has three parts.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be doing the basics, making sure that everyone will and uses give some basic examples of how to write programs for map reduce and then introduce you to some other tools that are part of which is an open source project.",
                    "label": 0
                },
                {
                    "sent": "Japan is going to is going to be going over algorithms, data mining algorithms using that reduce and finally wrong will go over.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applications.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have data everywhere.",
                    "label": 1
                },
                {
                    "sent": "These are statistics.",
                    "label": 0
                },
                {
                    "sent": "Actually user agreement already, so Flickr has 3 billion photos.",
                    "label": 1
                },
                {
                    "sent": "YouTube has had a few years ago actually a couple of years ago 83 million videos, 15 hours of video uploaded for every minute.",
                    "label": 0
                },
                {
                    "sent": "The web has 10 billion videos watched per month.",
                    "label": 0
                },
                {
                    "sent": "There is 500 million photos.",
                    "label": 0
                },
                {
                    "sent": "Jackel webmap is 3, three on mix, and so on.",
                    "label": 0
                },
                {
                    "sent": "So there is a ton of data.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everyone right and valid question in this case is, So what?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A common argument is that well, more than.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is obviously more.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But more is also.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Different.",
                    "label": 0
                },
                {
                    "sent": "An opportune it is in this case, so we have a lot of data.",
                    "label": 0
                },
                {
                    "sent": "We have real time access access to content, right?",
                    "label": 1
                },
                {
                    "sent": "We have richer context from users and hyperlinks, so pretty much everything you do or a lot of the things you do these days are captured in some form online, either through your mobile phone or through Facebook and so on.",
                    "label": 1
                },
                {
                    "sent": "And all this data gives us abundant training examples, right?",
                    "label": 0
                },
                {
                    "sent": "So we can have now billions or trillions of training points.",
                    "label": 0
                },
                {
                    "sent": "So in this case, brute force methods may suffice, and by brute force we mean something that has a very simple model but can be trained using billions of points.",
                    "label": 0
                },
                {
                    "sent": "However, the challenges here are that this data is typically much much dirtier than smaller data sets of that are that are carefully cleaned and the data points are cherry picked and so on.",
                    "label": 0
                },
                {
                    "sent": "So we need efficient algorithms that can process such huge volumes of data and we need them to be able to scale.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Within reason.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cost so you see there is this dual part, so we may be able to do things by applying simpler algorithms that can use billions or trillions of data points.",
                    "label": 0
                },
                {
                    "sent": "Use a lot of training examples.",
                    "label": 1
                },
                {
                    "sent": "However, we need to be able to use that data.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is an old quote by a famous statistician George Box that says all models are wrong, but some are useful.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Peter Norvig at Google has recently said that all models are wrong, and you can increasingly succeed without the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are a lot of examples that actually employ a very simple models, such As for example, Google Page rank.",
                    "label": 0
                },
                {
                    "sent": "Google Page rank does not alter the way that ranking works.",
                    "label": 0
                },
                {
                    "sent": "It doesn't try to understand the semantics of a page or anything like that.",
                    "label": 0
                },
                {
                    "sent": "It's just a very simple random walk on the on the web graph language translation is another example.",
                    "label": 0
                },
                {
                    "sent": "Instead of trying to understand grammar and sentence structure and so on, we can often go ahead and use, let's say, trigrams or pentagrams and use the entire web as a training corpus and that does better than most other method.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "Um, one small parenthesis to get over some terminology.",
                    "label": 0
                },
                {
                    "sent": "This is perhaps slightly nitpicking, but there is a lot of talk about cloud computing, which basically",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Means the Internet, so network connected, and by that I mean network connectivity that is present everywhere on your computer on your mobile phone, and so on, as well as standard protocols to allow communication among those connected devices.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plus, commoditisation and standardization of CPU of network of storage and so on.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As Eric Smith has said, this is what he and many others have worked towards during the rent their entire careers.",
                    "label": 1
                },
                {
                    "sent": "It's just something that is happening now, right?",
                    "label": 0
                },
                {
                    "sent": "So cloud computing is something much bigger.",
                    "label": 0
                },
                {
                    "sent": "It's an overloaded term.",
                    "label": 0
                },
                {
                    "sent": "It means.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this tutorial is not about cloud computing, but it is about large scale data processing right?",
                    "label": 0
                },
                {
                    "sent": "Which may or may not be related to cloud compute.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as I tried to make the point earlier, basically we have tons of data that we need to leverage and we need to be able to build algorithms that are designed to use tons of data and be able to scale.",
                    "label": 0
                },
                {
                    "sent": "And we also need of course the platforms and the infrastructure to do this right to build these algorithms that will use the large volumes.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data, so in this tutorial we're going to show you how to do this.",
                    "label": 0
                },
                {
                    "sent": "So as I said in the first part, we're going to go over the basic concepts and tools and introduce the basic notions of map reduce as well as distributed storage, and then introduce some other components.",
                    "label": 1
                },
                {
                    "sent": "Higher level components for data processing such as Hadoop, such as Sorry, HKS Pig, Cascading High and then Jim is going to give you examples after the break.",
                    "label": 0
                },
                {
                    "sent": "On how you can apply these ideas for data analysis and data mining tasks in information retrieval in graph analysis, clustering and classification, and finally, in the left part wrong with over applications in text processing, data warehousing and machine.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning.",
                    "label": 0
                },
                {
                    "sent": "So let's start with the very basics and please feel free to interrupt at any point with questions.",
                    "label": 0
                },
                {
                    "sent": "If something is not clear.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is MapReduce right?",
                    "label": 1
                },
                {
                    "sent": "Is my producer programming model?",
                    "label": 0
                },
                {
                    "sent": "Is it an execution environment or is it a software package?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And basically, depending on who you ask.",
                    "label": 0
                },
                {
                    "sent": "People mean all of these things at different times.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So map reduce for this talk is basically we're going to cover the programming model for distributed computation that relies on distributed storage in a cluster of thousands 10s of thousands of machines, and it also provides scheduling.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fault tolerance so let me introduce the programming model first with a very simple example.",
                    "label": 1
                },
                {
                    "sent": "Let's forget about clusters and large scale and all that stuff for for a moment.",
                    "label": 0
                },
                {
                    "sent": "And let's just say we have a file on let's say your laptop called employees TXT that has three columns.",
                    "label": 0
                },
                {
                    "sent": "The last name, the first name and the salary of each employee and I want to do a very simple thing finding frequency computer histogram of firstname frequencies.",
                    "label": 1
                },
                {
                    "sent": "Or count so I could write a Python script in a few minutes to do this.",
                    "label": 0
                },
                {
                    "sent": "Here is one way you can do it in Python, but.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other scripting languages have similar constructs, so first you have to open your text file and in this case when you open your text file, you'll get one way to interpret the the file pointer.",
                    "label": 0
                },
                {
                    "sent": "In this case is a nitrate Aurora sequence over the lines of the file, right?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you can take each line of the file.",
                    "label": 0
                },
                {
                    "sent": "What you want to do first is extract.",
                    "label": 0
                },
                {
                    "sent": "Of course the first name, so the map operation.",
                    "label": 0
                },
                {
                    "sent": "Basically these are products from functional programming and operation will take a function and apply it to every element in a sequence and producer transformed sequence.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have a function that takes a text line split.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sit on the field separator and returns.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second field, right, which is the first name and if we apply this to every text line will get, will get a sequence of just first names.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The reduce operation is basically an accumulation operation that will basically apply an operation on every sequence of an input file of a sequence.",
                    "label": 0
                },
                {
                    "sent": "So now we have a sequence data immediate sequence of the first name, and we apply the accumulation function which basically has the histogram which is a hash hash table key by first name and the value is the account right?",
                    "label": 0
                },
                {
                    "sent": "And for each name we see we go in increment by 1.",
                    "label": 0
                },
                {
                    "sent": "Responding count.",
                    "label": 0
                },
                {
                    "sent": "In the hip.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You'll notice that at every point we have three key value items, right?",
                    "label": 0
                },
                {
                    "sent": "I regular just a fancy name for sequence, so we open the.",
                    "label": 0
                },
                {
                    "sent": "The initial sequence doesn't have a key, has just the value, which is the text lines.",
                    "label": 0
                },
                {
                    "sent": "We transform this two first names and then we accumulate these and we produce another sequence of key value pairs, which is basically the the histogram values, right?",
                    "label": 0
                },
                {
                    "sent": "The key is the name and the account is the value.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can do the same.",
                    "label": 0
                },
                {
                    "sent": "This is the basic programming model for formatted use right?",
                    "label": 0
                },
                {
                    "sent": "And I'm going to show you an example of how the same thing can be implemented in Hadoop, which is an open source implementation of map reduce to run on clusters of clusters of a large number of machines.",
                    "label": 0
                },
                {
                    "sent": "So again in this case we have them.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function Now most of this is boilerplate, but this is the map function and what happens there?",
                    "label": 0
                },
                {
                    "sent": "Basically so we have the key in the value that appears in the normal values.",
                    "label": 0
                },
                {
                    "sent": "Aligning the textile and first name is basically a mutable string, and what we say is, well, get the value which is the text line converted to Australian splitting.",
                    "label": 0
                },
                {
                    "sent": "The separators.",
                    "label": 0
                },
                {
                    "sent": "Take the second field and then set the mutable string.",
                    "label": 0
                },
                {
                    "sent": "We have first name to that value and then output this.",
                    "label": 0
                },
                {
                    "sent": "This first thing has a key with a value of 1, so one there is a long writable, which again is a mutable login loop.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So you write a matter that is type for the keys and values of both input and the output, and this applies the transformation on each item of the input and produces an item.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the output stream the output sequence and then we'll do the reducer, which basically adds up.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Count so here we get the key, which would be the first name and the sequence of accounts that we have collected from for this key, right?",
                    "label": 0
                },
                {
                    "sent": "So initially instead of admitting the instead of admitting the just the names, you see where I admitted the name in the count of 1, right?",
                    "label": 0
                },
                {
                    "sent": "So then I just need to get old and all that ones for that name and then add them together and will get the count of all names, right?",
                    "label": 0
                },
                {
                    "sent": "So this is what happens here.",
                    "label": 0
                },
                {
                    "sent": "Of values is an isolator of all the values for this particular key, and then I'll add them and then output the key, which is the first name and the total sum.",
                    "label": 0
                },
                {
                    "sent": "In fact, in Hadoop you don't really have to write this.",
                    "label": 0
                },
                {
                    "sent": "This is provided as a library class.",
                    "label": 0
                },
                {
                    "sent": "This is a fairly common ad.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Location operation and then there are a few other things that we need to do.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go over details, but basically you have to set up the job set up for the input, say which is the mapper function, the reducer.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function and so on and so forth, and then execute the job on the cluster.",
                    "label": 0
                },
                {
                    "sent": "So this is about 30 lines of code, most of it is boilerplate.",
                    "label": 0
                },
                {
                    "sent": "You can get an Eclipse plug-in to do most of this for you, plus 5 lines of actual code from the map in the reduce function.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before I continue one point here.",
                    "label": 0
                },
                {
                    "sent": "So this is the map reduce is a programming model and it has been started for distributed clusters, so there is Google's original implementation and there is also an implementation in Hadoop which is an open source project, part of the Apache Software Foundation, but the same programming model has been investigated on how you can implement it on top of of different architectures.",
                    "label": 1
                },
                {
                    "sent": "So for example, people have looked on how to use the MapReduce model for single chip.",
                    "label": 0
                },
                {
                    "sent": "For for symmetric, multiprocessors or single chip multiprocessors the Phoenix project in Stanford there is an implementation for the cell broadband broadband broadband engine which is the CPU that the PlayStation and so on and so forth have.",
                    "label": 0
                },
                {
                    "sent": "And then there are many other implementations in programming in script in scripting languages in libraries, even for shell scripts and.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So on.",
                    "label": 0
                },
                {
                    "sent": "In this tutorial, we're going to focus on map reduce for distributed clusters, so presented the basic abstraction, right programming abstraction, and then we'll see how this applies to.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Large clusters.",
                    "label": 0
                },
                {
                    "sent": "The point here is that I showed you originally a quick and dirty script that will take a text file of employees and produce a histogram of firstname counts and this is about 5 lines of Python or 5 lines of non boilerplate code in Hadoop.",
                    "label": 0
                },
                {
                    "sent": "The difference here is that the Python script can run on a single machine, but if I had let's say billions or trillions of of of employees, the process for the sake of example this would start getting tricky with Hadoop.",
                    "label": 0
                },
                {
                    "sent": "Basically I can take the same thing executed on my laptop, but I can take the same thing and transparently executed on a cluster.",
                    "label": 0
                },
                {
                    "sent": "It has up to thousands of machines and thousands of disk drives storing huge amounts of data.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in order to achieve this, basically a lot of things are hidden right?",
                    "label": 0
                },
                {
                    "sent": "So the data will be partitioned and replicated across different nodes in the cluster.",
                    "label": 0
                },
                {
                    "sent": "The computation will actually be shipped to machines that have copies of the data of different fragments of the data.",
                    "label": 0
                },
                {
                    "sent": "And then we have to deal with various parameters dealing with concurrency and so on in order to ensure for color.",
                    "label": 0
                },
                {
                    "sent": "And so there's a lot of things that going on.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go on, but as a programmer, basically you don't need to know any of this, right?",
                    "label": 0
                },
                {
                    "sent": "You can write it very similarly as you would write a five line Python script and then just have it execute on a huge cluster.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So map reduce innocence is important not only for what it does, but for what it doesn't do, right?",
                    "label": 0
                },
                {
                    "sent": "So here is what happened, right?",
                    "label": 0
                },
                {
                    "sent": "So here is how the the the flow things go, so the input file is split into different chunks or file splits.",
                    "label": 0
                },
                {
                    "sent": "So let's say split, split, zero is text lines one 200 split, one is text, line 101 to 200 for example, and so on, right?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The map function will be applied on each line of the inputs, right?",
                    "label": 0
                },
                {
                    "sent": "So there will be a mapper that processes one chunk of the input file, so each chunk of the input file will be processed by a mapper, which is a separate pro.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is essentially just a sequential scan over the data right?",
                    "label": 0
                },
                {
                    "sent": "That will take each input element and transform it.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's say we have the text lines from different parts.",
                    "label": 0
                },
                {
                    "sent": "Then the mapper will apply the transformation, which in this case was extracted first name and it has inherently account of 1.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it will do this transformation.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "What will happen next is that.",
                    "label": 0
                },
                {
                    "sent": "Based on the key that the Mapper emits.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the output elements from the mappers will get hash to a certain reducer.",
                    "label": 0
                },
                {
                    "sent": "So John old.",
                    "label": 0
                },
                {
                    "sent": "Basically this hash function ensures that all Jones will get sent to the same reducer, right?",
                    "label": 0
                },
                {
                    "sent": "So, whoops.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "All of the all of the map outputs that have a key of John.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And will be sent to the same reducer and then we'll apply the reduction operation and just some of the counts.",
                    "label": 0
                },
                {
                    "sent": "1 + 1 = 2 and then this will be the output from the reducer which.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will be sent to the output.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now here's what happens when this gets placed on our cluster of many machines.",
                    "label": 0
                },
                {
                    "sent": "First of all, the different chunks of the input files will get replicated on many machines, so you'll see for example, that split zero is on machine 0, machine one and machine 2 split one.",
                    "label": 0
                },
                {
                    "sent": "There is a copy of that and host zero hosts.",
                    "label": 0
                },
                {
                    "sent": "Host three and one more machine, and so on.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The mappers that each mapper as I said, processes one chunk of the input will get shipped to machines that have local copies of the data.",
                    "label": 0
                },
                {
                    "sent": "This may not always be possible, but this is what map reduce in this kind of setting tries to do right?",
                    "label": 0
                },
                {
                    "sent": "So computation will be Co located with data as much as possible, and when you have huge volumes of data, it's often cheaper to ship the code to the data than to ship the data to the code, right?",
                    "label": 1
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the reducers will also get placed in machines in the cluster.",
                    "label": 0
                },
                {
                    "sent": "Here you'll unavoidably have to have network communicate.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is Racun network aware to a certain.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And and in addition, you can have Combiners which run inside the map processes, which basically do a data reduction.",
                    "label": 0
                },
                {
                    "sent": "So for example, let's say you had in the same chunk, you had 10 Johns instead of sending over the network to the reducer.",
                    "label": 0
                },
                {
                    "sent": "John one, John one, John, 110 times.",
                    "label": 0
                },
                {
                    "sent": "You'll do a pre reduction in Saint John 10 from this machine to the reducer, right?",
                    "label": 0
                },
                {
                    "sent": "So you minimize how much data is sent to the reducer, right?",
                    "label": 0
                },
                {
                    "sent": "And then the Reducer will collect all the jobs from all different mappers on different machines and do the final reduction step.",
                    "label": 0
                },
                {
                    "sent": "So this is the job of the combiner.",
                    "label": 0
                },
                {
                    "sent": "It's a, it's actually an optimization.",
                    "label": 0
                },
                {
                    "sent": "Your program will be correct without it, but it helps a lot, and it's actually very important in practice to do this kind of simple optimism.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in sum.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Free MapReduce is, first of all a simple programming model and it allows you to express computations very simply that will run on a huge cluster of machines and it provides scalability and fault tolerance.",
                    "label": 0
                },
                {
                    "sent": "If you can express your computation in this model, then my produce will take care of of yes.",
                    "label": 0
                },
                {
                    "sent": "Tolerance.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And are they end minus one?",
                    "label": 0
                },
                {
                    "sent": "Sorry the the replicas.",
                    "label": 0
                },
                {
                    "sent": "So yes, primarily is for fault tolerance.",
                    "label": 0
                },
                {
                    "sent": "You can set the number the replication level on a per file basis.",
                    "label": 0
                },
                {
                    "sent": "So on HTFS you can say for this file I want to this this each chunk of the file to be replicated 1, two or three out of the box.",
                    "label": 0
                },
                {
                    "sent": "It's three, but you can set it to any number you want and the other reason you want to have it replicated depending on how hot the data is.",
                    "label": 0
                },
                {
                    "sent": "So data that is accessed a lot.",
                    "label": 0
                },
                {
                    "sent": "You probably wanted to have to have it replicated.",
                    "label": 0
                },
                {
                    "sent": "On more machines, right?",
                    "label": 0
                },
                {
                    "sent": "It's not a property of the job, it's a property of the of the of the file on the on.",
                    "label": 0
                },
                {
                    "sent": "HTFS on the Hadoop distributed file system.",
                    "label": 0
                },
                {
                    "sent": "You can change it, but it takes time, right?",
                    "label": 0
                },
                {
                    "sent": "So whenever if I say to HTFS, change the replication factor from three to five, it will take some time to make the additional 2 copies and send them over the cluster.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, no so so.",
                    "label": 0
                },
                {
                    "sent": "So each mapper will be so the execution engine for MapReduce will make sure that basically.",
                    "label": 0
                },
                {
                    "sent": "For each chunk, everything will be executed just once, so you won't execute it on all of the replicas.",
                    "label": 0
                },
                {
                    "sent": "You'll find one replica that has a copy of the data.",
                    "label": 0
                },
                {
                    "sent": "Send the map task there, do the transformation and this data will actually be checkpointed.",
                    "label": 0
                },
                {
                    "sent": "And in this case it actually, if your replication factor is higher, your chances of finding a machine that's free and has a copy of the data are generally higher, right?",
                    "label": 0
                },
                {
                    "sent": "So if but basically the But basically the job of the MapReduce execution engine is to make sure that that you get the semantics that everything will be executed exactly once.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Manage the splits yourself, no well.",
                    "label": 0
                },
                {
                    "sent": "Uh, you can.",
                    "label": 0
                },
                {
                    "sent": "Write your own classes that determine how the data is is split and what goes into a different split.",
                    "label": 0
                },
                {
                    "sent": "But for things like text files or binary sequence files of key value pairs, or if you're using H base for these kind of.",
                    "label": 0
                },
                {
                    "sent": "For these kinds of inputs there are classes that will do that for you, so basically you don't need to worry, worry about it.",
                    "label": 0
                },
                {
                    "sent": "But if there is a.",
                    "label": 0
                },
                {
                    "sent": "First application or you can tune that part if necessary, right?",
                    "label": 0
                },
                {
                    "sent": "So that's part of the library, but our API's to do that, yeah?",
                    "label": 0
                },
                {
                    "sent": "Basis.",
                    "label": 0
                },
                {
                    "sent": "So what do you can you?",
                    "label": 0
                },
                {
                    "sent": "So what do you mean exactly by by skew so?",
                    "label": 0
                },
                {
                    "sent": "So you're not talking about the input.",
                    "label": 0
                },
                {
                    "sent": "The input file splits now skew.",
                    "label": 0
                },
                {
                    "sent": "The input file splits.",
                    "label": 0
                },
                {
                    "sent": "It's not, you can just do it based on the on.",
                    "label": 0
                },
                {
                    "sent": "The skew is not that much of a problem in the input file size.",
                    "label": 0
                },
                {
                    "sent": "You can basically split based on on size.",
                    "label": 0
                },
                {
                    "sent": "Of course it doesn't necessarily mean that if I have one kilobyte of data here and another kilobyte of data, it will take an equal amount of time to process.",
                    "label": 0
                },
                {
                    "sent": "Right so yes, in that case you could if you have an idea of two pre to estimate that in advance that would help.",
                    "label": 0
                },
                {
                    "sent": "But if you have a huge amount of skew and one chunk will take 3 hours to process and all of the other million chunks will take one second to process, then you may have a problem.",
                    "label": 0
                },
                {
                    "sent": "But Jenn, yeah.",
                    "label": 0
                },
                {
                    "sent": "But generally, if you're dealing with huge volumes of data, this kind of discrepancy.",
                    "label": 0
                },
                {
                    "sent": "The amount of time it takes to process a chunk compared to the time it takes for a job to complete is usually just a small fraction, right.",
                    "label": 0
                },
                {
                    "sent": "Skew can be a bigger problem in the reduce.",
                    "label": 0
                },
                {
                    "sent": "When you when you send the partition data over the over to the Reducer and I think Jim will be giving some examples on how you can deal with it in some cases, but.",
                    "label": 0
                },
                {
                    "sent": "Then it starts becoming trickier.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, this is ideal for preprocessing large for processing or pre processing large volumes of data.",
                    "label": 1
                },
                {
                    "sent": "And there was the original article on map reduce from Google was invited to see ACM January 2008 and it was pre faced by David Patterson computer architecture Berkeley and he said basically if the data center is the computer then basically the equivalent of the ad machine language instruction is basically MapReduce right?",
                    "label": 0
                },
                {
                    "sent": "So this is a low level, a simple low level programming API abstraction.",
                    "label": 0
                },
                {
                    "sent": "To do.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things on a cluster on a large cluster of machines.",
                    "label": 0
                },
                {
                    "sent": "So this is the the the the core part right?",
                    "label": 0
                },
                {
                    "sent": "So we have distributed storage and a way to ship to to write computations on this distributed file system in a way that will the computations will get sent to the appropriate places on the file system and provide for fault tolerance and scalability.",
                    "label": 0
                },
                {
                    "sent": "Now on top of this.",
                    "label": 0
                },
                {
                    "sent": "But as I said this is the low level.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Part so on top of this there are many other components.",
                    "label": 0
                },
                {
                    "sent": "Now Hadoop is an open source project, part of the Apache Foundation and it has several different components.",
                    "label": 0
                },
                {
                    "sent": "Many are added overtime.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that Hadoop stated mission.",
                    "label": 0
                },
                {
                    "sent": "This is from an interview from one of the founders of the of the project.",
                    "label": 0
                },
                {
                    "sent": "Cutting is to commoditize the infrastructure for web scale data intensive application.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who uses Hadoop?",
                    "label": 0
                },
                {
                    "sent": "Well, it was primarily developed in Yahoo.",
                    "label": 0
                },
                {
                    "sent": "So Yahoo is is probably the biggest user.",
                    "label": 0
                },
                {
                    "sent": "The next biggest user is is probably Facebook.",
                    "label": 0
                },
                {
                    "sent": "It has a large large cluster and actually wrong in the third part will be talking a little bit more about that and giving you describing how Hadoop is is actually used in Facebook last FM.",
                    "label": 0
                },
                {
                    "sent": "The Internet radio.",
                    "label": 0
                },
                {
                    "sent": "Service uses Hadoop.",
                    "label": 0
                },
                {
                    "sent": "Rackspace also uses Hadoop for log analysis as well as provides Hadoop.",
                    "label": 0
                },
                {
                    "sent": "DIG uses Hadoop Apache Nutch which is another open source project which is a web crawler and web Web Index.",
                    "label": 0
                },
                {
                    "sent": "Basically an open source search engine also uses Hadoop.",
                    "label": 0
                },
                {
                    "sent": "In fact Hadoop was originally built for Notch but eventually split into a separate project.",
                    "label": 0
                },
                {
                    "sent": "So wrong will give more examples of a real world uses.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hadoop in the.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last part of the tutorial.",
                    "label": 0
                },
                {
                    "sent": "So let's see at the different quickly see the different parts of Hadoop.",
                    "label": 0
                },
                {
                    "sent": "First of all, there is Hadoop core which basically provides abstractions for file systems and IO as well as RPC and persistence, right?",
                    "label": 0
                },
                {
                    "sent": "So these are the basic APIs that allow you to plug in different distributed distributed file systems and ship computations to different.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so on.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a relatively new project.",
                    "label": 0
                },
                {
                    "sent": "This is not widely used, but basically when you have RPC you need a cross language way to serialize and deserialize the data that you're going to send over the network.",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of the equivalent of the Google Protocol buffers.",
                    "label": 0
                },
                {
                    "sent": "If you've heard of those or Facebook thrift.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's grayed out because it's not.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That active or widely used yet MapReduce is the component that allows you to do distributed execution of large batch jobs on a cluster.",
                    "label": 0
                },
                {
                    "sent": "So it provides the API as well as the execution.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Engine.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "HTFS is the component that is responsible for the distributed storage, right?",
                    "label": 0
                },
                {
                    "sent": "So managing the files split into different chunks, replicated and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "This was inspired again by the Google File.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "System Zookeeper, which is basically a coordination service so you can do distributed lock implement distributed locking protocols as well as storing distribute configuration parameters for for for a distributed system.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is kind of the equivalent of Google Chubby.",
                    "label": 0
                },
                {
                    "sent": "And then there is a space which basically provides a higher level data abstraction.",
                    "label": 0
                },
                {
                    "sent": "The column and oriented sports store that allows both executing both bad batch jobs as well as random access in individual items in.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the file, this was inspired by Google picked a big table, then there is pig which is a dataflow language.",
                    "label": 0
                },
                {
                    "sent": "It is a procedural language that inspired by by SQL and it makes expressing MapReduce jobs easy.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hive is a distributed data warehouse, again implemented on top of HTFS and MapReduce that provides a SQL like query language.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is Choco which is relatively new.",
                    "label": 0
                },
                {
                    "sent": "Actually, a log collection and analysis system.",
                    "label": 0
                },
                {
                    "sent": "So the the the different components in Hadoop are growing overtime.",
                    "label": 0
                },
                {
                    "sent": "This is a very actively developed project.",
                    "label": 0
                },
                {
                    "sent": "The two core components actually remain produce and HTFS, but there are a lot of other things that can make your life easier.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So map reduce.",
                    "label": 0
                },
                {
                    "sent": "Let me recap the programming model as I said what you need there are.",
                    "label": 0
                },
                {
                    "sent": "Basically two things that you need to specify in order to define a MapReduce program, right?",
                    "label": 0
                },
                {
                    "sent": "So you have to define the Mapper and Reducer.",
                    "label": 0
                },
                {
                    "sent": "The Mapper will, as I said, take a sequence of key value pairs in the input.",
                    "label": 0
                },
                {
                    "sent": "Let's say for example in the.",
                    "label": 0
                },
                {
                    "sent": "In the example I gave with the histograms, the key will be void now and the the value will be the line in the text line in the input right?",
                    "label": 0
                },
                {
                    "sent": "And then it will apply.",
                    "label": 0
                },
                {
                    "sent": "Some function will produce another.",
                    "label": 0
                },
                {
                    "sent": "Part of key values and.",
                    "label": 0
                },
                {
                    "sent": "Actually, for each, the mapping is not necessarily a one to one right, so it can be 120 or one or more.",
                    "label": 0
                },
                {
                    "sent": "So for each input key value pair, the reducer, the data type for the reducer function is basically take the the key type output by the mapper and the sequence of values.",
                    "label": 0
                },
                {
                    "sent": "For the corresponding key and then output another key 1 zero or more key value pairs right?",
                    "label": 0
                },
                {
                    "sent": "So do a reduction operation.",
                    "label": 0
                },
                {
                    "sent": "So for example in the histogram example it would be the first number and account a sequence of counts output by the reducer by the Mapper.",
                    "label": 0
                },
                {
                    "sent": "And you will aggregate these and produce again the first name in the total count.",
                    "label": 0
                },
                {
                    "sent": "As I said, you can have the combiner, which basically is can be the same as the reducer, although it doesn't have to be.",
                    "label": 0
                },
                {
                    "sent": "So in this case you could run.",
                    "label": 0
                },
                {
                    "sent": "The combine are inside the Mapper as I said, instead of sending John One John 110 times, you'll send John 10 and actually this is the reason that you'll see the Mapper Outputing account, so the data types are consistent and then you can also define the partitioning function which determines how the data will get partitioned on the different reducers in the cluster.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The Mapper interface in the Java APIs for Hadoop basically have the input K1V1 the key value types for the input and K2V2 the key value types for the output.",
                    "label": 0
                },
                {
                    "sent": "You can initialize.",
                    "label": 0
                },
                {
                    "sent": "You can do job setup in a method called configure.",
                    "label": 0
                },
                {
                    "sent": "You do clean up in close three and then the bulk of the computation happens in the map function right which will take each key value and then output.",
                    "label": 0
                },
                {
                    "sent": "Emit key value pairs for the output right.",
                    "label": 0
                },
                {
                    "sent": "And as I said, you can emit 01 or more output values for each.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Input value.",
                    "label": 0
                },
                {
                    "sent": "The reducer again the data type is K2V2 is the the types for the input key value.",
                    "label": 0
                },
                {
                    "sent": "This has to be the same as the output from the mapper and then K3V3 is the key value for the reducer output.",
                    "label": 0
                },
                {
                    "sent": "Again, you have a configuring the close method to do the initialization and teardown, and again now here you have keys and a nitrator of values corresponding to this key, right?",
                    "label": 0
                },
                {
                    "sent": "So you'll get all the values for this key.",
                    "label": 0
                },
                {
                    "sent": "And then you can emit output values.",
                    "label": 0
                },
                {
                    "sent": "Through the output collector, again 01 or more.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some Canonical examples of jobs for which MapReduce was originally designed.",
                    "label": 0
                },
                {
                    "sent": "I gave you the example of a histogram of a simple histogram of first names in employee dot T XT.",
                    "label": 0
                },
                {
                    "sent": "But basically histogram type jobs are for example if I have a large log file and I want to construct.",
                    "label": 0
                },
                {
                    "sent": "A graph, so let's say I have a log of security events, let's say from IP Destination IP and I want to say how many, or let's say a packet trace from source IP, destination IP and I want to construct a graph source IP, destination IP and volume of data over this pair of nodes, right?",
                    "label": 0
                },
                {
                    "sent": "So here this is essentially a histogram job only my bucket is the edge, which is a pair of nodes.",
                    "label": 0
                },
                {
                    "sent": "K means is another histogram type job essentially whereas based where the buckets are basically the cluster centers, right?",
                    "label": 0
                },
                {
                    "sent": "So if I have 10 cluster centers?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Then I iterate over each input in the over over each point in the input and I assign it to the closest center, right?",
                    "label": 0
                },
                {
                    "sent": "This is the the bucket it will belong in.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "This is a simplification of K means of course, and Jim will go into more details in the second part.",
                    "label": 0
                },
                {
                    "sent": "Another type of Canonical example that for which Hadoop was built is inverted indices right?",
                    "label": 0
                },
                {
                    "sent": "So I have a list of documents and basically I want to construct so each document is a list of terms.",
                    "label": 0
                },
                {
                    "sent": "I want to construct the inverted index which is for each term the list of documents in which it appears.",
                    "label": 0
                },
                {
                    "sent": "If you think of this as a document term matrix, basically I want the transpose of the matrix.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hadoop is also sorry.",
                    "label": 0
                },
                {
                    "sent": "MapReduce is also used for sorting.",
                    "label": 0
                },
                {
                    "sent": "You can do equally join and more details.",
                    "label": 0
                },
                {
                    "sent": "Again as I said will be given by G. Manganin in Part 2.",
                    "label": 0
                },
                {
                    "sent": "Let me show you how you can do joins.",
                    "label": 0
                },
                {
                    "sent": "This is something that gets discussed fairly frequently when MapReduce comes up.",
                    "label": 0
                },
                {
                    "sent": "So this is one way to do joins.",
                    "label": 0
                },
                {
                    "sent": "It's not the only way to do joins.",
                    "label": 0
                },
                {
                    "sent": "Let's say I have now 2 two inputs right and one input is employee name and Department ID and the second input is Department name and Department ID, right?",
                    "label": 0
                },
                {
                    "sent": "And I want to do the join and find for each employee find the name of the Department for which he or she works.",
                    "label": 0
                },
                {
                    "sent": "So what I can do in this case basically, essentially it's a distributed hash joint implementation, right?",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is what you can.",
                    "label": 0
                },
                {
                    "sent": "So here is one way to do this.",
                    "label": 0
                },
                {
                    "sent": "The key for the for the map output will actually be the value on which I want to do the joint, so this will be the employee ID.",
                    "label": 0
                },
                {
                    "sent": "So each tuple I limit 7 the Department ID and then I will say this is a blue tuple which is from the first input and it has this in the value of.",
                    "label": 0
                },
                {
                    "sent": "The remaining fields is this Smith and I'll do the same.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "The couples in the second input right.",
                    "label": 0
                },
                {
                    "sent": "Another way I could do this is actually put the the input relation tag in the key.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I could do that, do it in this way.",
                    "label": 0
                },
                {
                    "sent": "In this case I would have to define the partitioning function to make sure that 7 blue and seven red will get sent to the same reducer to the same machine.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But let's say I do the 1st way right, so the map function will take each tuple from both relations and output the key.",
                    "label": 0
                },
                {
                    "sent": "Is the join the join column and then the remaining values are in the value.",
                    "label": 0
                },
                {
                    "sent": "Of the.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Map output.",
                    "label": 0
                },
                {
                    "sent": "So when these get hashed and sent to the reducer, then all of the Department #7 couples from both relations will go to the same machine and in this case you will do basically a nested loop join and then you can output.",
                    "label": 0
                },
                {
                    "sent": "For example Smith is in the development Department.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jones, in the Development Department and so on.",
                    "label": 0
                },
                {
                    "sent": "And amid this as output.",
                    "label": 0
                },
                {
                    "sent": "Now this is a simple example to show you one way in which you can do joins.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you have skew in the distributions and so on, then this can become trickier.",
                    "label": 0
                },
                {
                    "sent": "And there are also higher level components that will actually do joins for you, but it's good to know, at least conceptually one way in which you can implement this.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, some terminology for HTFS and MapReduce that it's useful to know if you're actually running jobs on a cluster.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So on each machine on your cluster.",
                    "label": 0
                },
                {
                    "sent": "So as I said, there is distributed storage and distributed computation for each machine on the cluster.",
                    "label": 0
                },
                {
                    "sent": "For the distributed storage part, you have a data node, which is a process that's responsible for managing local copies of chunks, right?",
                    "label": 0
                },
                {
                    "sent": "So the data node basically just knows I have a chunk of let's say 100 lines of a file, and it's on this machine.",
                    "label": 0
                },
                {
                    "sent": "Order if you want to access.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The whole file.",
                    "label": 0
                },
                {
                    "sent": "Basically there is a name node.",
                    "label": 0
                },
                {
                    "sent": "This is a central directory which basically says if you want employees CXT lines is a one to 100 are in machine, let's say 01 and two.",
                    "label": 0
                },
                {
                    "sent": "So basically this distributed directory is stored in the in the name node right?",
                    "label": 0
                },
                {
                    "sent": "So when you want to access a part of the file will ask the name node and then the client will.",
                    "label": 0
                },
                {
                    "sent": "Directly communicate with the data node.",
                    "label": 0
                },
                {
                    "sent": "This structure is monitor mirrored for the 4th.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distributed computation part.",
                    "label": 0
                },
                {
                    "sent": "So when you're sending jobs, there is a task tracker that runs on each machine that will basically is responsible for spawning.",
                    "label": 0
                },
                {
                    "sent": "Local jobs, right?",
                    "label": 0
                },
                {
                    "sent": "So a map?",
                    "label": 0
                },
                {
                    "sent": "A map task which will just process a chunk of the input is managed by the task tracker.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there is a job tracker which basically coordinates all of these different.",
                    "label": 0
                },
                {
                    "sent": "Processes on the on the cluster, right?",
                    "label": 0
                },
                {
                    "sent": "And it's responsible for for maintaining the semantics, restarting jobs if they fail, and so.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so forth.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The example I gave was with Java, but you don't have to use Java to use MapReduce.",
                    "label": 0
                },
                {
                    "sent": "That is also Hadoop streaming.",
                    "label": 0
                },
                {
                    "sent": "That basically is a wrapper that allows you to write a program in any language.",
                    "label": 0
                },
                {
                    "sent": "It will basically serialize your values into just text text format, so you can write a simple script.",
                    "label": 0
                },
                {
                    "sent": "It uses so standard input, the input for the mapper will be piped in text format to your let's say script and.",
                    "label": 0
                },
                {
                    "sent": "The output that your script produces will be in.",
                    "label": 0
                },
                {
                    "sent": "Standard output will actually be parsed and then sent out to the cluster.",
                    "label": 0
                },
                {
                    "sent": "You can use any language, of course.",
                    "label": 0
                },
                {
                    "sent": "Serialization and deserialization in text format is not very efficient, but if you want to write something quickly, this is very good.",
                    "label": 0
                },
                {
                    "sent": "There is also Hadoop pipes which uses a binary format and it allows you to write programs in C++, but you have to use a C++ library for this.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So now for the.",
                    "label": 0
                },
                {
                    "sent": "For the rest of this part, I'm going to briefly introduce other components of Hadoop or other tools built on top of Hadoop that make your life easier as a programmer.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any questions up to this point?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "As I said, one of the Canonical examples for MapReduce is building an inverted index.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And typically a Hadoop MapReduce is good for batch computations on large datasets, right?",
                    "label": 0
                },
                {
                    "sent": "So I have the crawler and it has collected terabytes or petabytes of web pages.",
                    "label": 0
                },
                {
                    "sent": "And basically I want to build the inverted index saying this word on which web pages does it appear?",
                    "label": 0
                },
                {
                    "sent": "However, in reality the crawl pages themselves are actually updated by the crawler, right?",
                    "label": 0
                },
                {
                    "sent": "So the crawl itself, you don't really delete the whole crawl and and do everything from scratch individual pages there will get updated.",
                    "label": 0
                },
                {
                    "sent": "It may get, they may get augmented by other parsers or analytics that may extract properties of the pages and so on and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "MapReduce and HDFS provide distributed storage and computation, but they're basically geared for batch processing.",
                    "label": 0
                },
                {
                    "sent": "They don't provide any facilities to access or update individual items, right?",
                    "label": 0
                },
                {
                    "sent": "So HTFS in particular is optimized for append rights and bulk reads in batch.",
                    "label": 0
                },
                {
                    "sent": "So H base basically adds random access read and write operations for individual entries in your your files.",
                    "label": 0
                },
                {
                    "sent": "So you can access, let's say individual topples in your input and update them.",
                    "label": 0
                },
                {
                    "sent": "Originally it was developed by Powerset, which was a search engine company which.",
                    "label": 0
                },
                {
                    "sent": "I think was purchased by Microsoft and it's the original inspiration comes from Google's big.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Table.",
                    "label": 0
                },
                {
                    "sent": "The data model for H basis is as follows.",
                    "label": 0
                },
                {
                    "sent": "So basically you have.",
                    "label": 0
                },
                {
                    "sent": "The units of data are rows.",
                    "label": 0
                },
                {
                    "sent": "Each row has a unique key.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then there are column families and within each column family you have columns.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The design is so that you can have millions of let's say sorry hundreds of column families and I'm going to give a simple example of this right?",
                    "label": 0
                },
                {
                    "sent": "So column families have to be specified in advance.",
                    "label": 0
                },
                {
                    "sent": "These are not easy to change on the fly, however columns you can have millions of columns and they don't all have to be populated.",
                    "label": 0
                },
                {
                    "sent": "So this is a sparse very can be a very sparse table and then rows are typical in the billions or trillions.",
                    "label": 0
                },
                {
                    "sent": "And they are stored sorted by the the primary key.",
                    "label": 0
                },
                {
                    "sent": "And this is typically partitioned over many nodes in a.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cluster.",
                    "label": 0
                },
                {
                    "sent": "So the key and cell values are arbitrary byte arrays, right?",
                    "label": 0
                },
                {
                    "sent": "So H space does not know anything about what the data is, so you can use a serialization deserialization library on top of this, but.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As far as each base is concerned, he's in keys and cell values are unaffected by the Rays and you can use any underlying data store so you can use a local file system.",
                    "label": 0
                },
                {
                    "sent": "You can use the Hadoop distributed file system.",
                    "label": 0
                },
                {
                    "sent": "You can use the Amazon Simple Storage Service as three and so.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On so let me give you an example.",
                    "label": 0
                },
                {
                    "sent": "Let's say I want to store my employees.",
                    "label": 0
                },
                {
                    "sent": "I can define a column family for the profile part and for the profile.",
                    "label": 0
                },
                {
                    "sent": "Here I'll say the profile has a last name, first name in the salary and the values will be the corresponding strings, but.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is not this is a kind of an arbitrary example.",
                    "label": 0
                },
                {
                    "sent": "Let's say for each employee I wanted to store the the pages they've they've bookmarked, or the documents they've bookmarked.",
                    "label": 0
                },
                {
                    "sent": "Let's say in the corporate repository, right?",
                    "label": 0
                },
                {
                    "sent": "So I could define another column family and let's say, as I said, this example is kind of arbitrary.",
                    "label": 0
                },
                {
                    "sent": "The column, the column.",
                    "label": 0
                },
                {
                    "sent": "The name could be the document ID or the URL.",
                    "label": 0
                },
                {
                    "sent": "So as you understand, this can be extremely sparse data, right?",
                    "label": 0
                },
                {
                    "sent": "So within each call each column, each column family.",
                    "label": 1
                },
                {
                    "sent": "Basically you can view it as a as a very sparse hash table, storing key value associations where the key is the column name and the value is the cell value.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you always access data by primary key.",
                    "label": 0
                },
                {
                    "sent": "There is no support for in the indices.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In H base.",
                    "label": 0
                },
                {
                    "sent": "So comparison of each base versus relational database systems.",
                    "label": 0
                },
                {
                    "sent": "This is a different solution for somewhat similar problems with very different solutions.",
                    "label": 0
                },
                {
                    "sent": "So RDBMS is are basically row oriented.",
                    "label": 0
                },
                {
                    "sent": "They have a fixed schema.",
                    "label": 0
                },
                {
                    "sent": "You cannot really add add columns.",
                    "label": 0
                },
                {
                    "sent": "On the fly, and they also provide very strong consistency guarantees.",
                    "label": 0
                },
                {
                    "sent": "However, H space on the other hand, was designed from the ground up to scale out by adding commodity machines, right?",
                    "label": 0
                },
                {
                    "sent": "So it provides a very simple consistency scheme.",
                    "label": 0
                },
                {
                    "sent": "It only guarantees that any rights you do in a row if you modify key values, right column names, and the corresponding value in your in your table in within a single row.",
                    "label": 0
                },
                {
                    "sent": "This will be atomic, but anything beyond.",
                    "label": 0
                },
                {
                    "sent": "Across roles or anything like that, it doesn't provide any transaction or any support for this.",
                    "label": 0
                },
                {
                    "sent": "This sort of things which can become pretty complicated if you're dealing with data replicated on a cluster and so on.",
                    "label": 0
                },
                {
                    "sent": "The good thing, of course is by by simplifying what it provides, it can provide much better fault tolerance.",
                    "label": 0
                },
                {
                    "sent": "It doesn't provide any real indices, and it can also.",
                    "label": 0
                },
                {
                    "sent": "It's also designed to support batch processing over the data in addition to the individual look ups by primary key or updates.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me move onto Pig.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is basically a way to simplify the expression of MapReduce jobs on the cluster.",
                    "label": 0
                },
                {
                    "sent": "So in the example I gave in the beginning, I said you have 5 lines of non boilerplate code, but you had all these class definitions.",
                    "label": 0
                },
                {
                    "sent": "You had to worry about all the all the older low level details, right?",
                    "label": 0
                },
                {
                    "sent": "So writing a significant single MapReduce job requires significant grunt work, right?",
                    "label": 0
                },
                {
                    "sent": "So you have to write the code for the boilerplate code for the Mapper and Reducer, passing the parameters, then setting up the job defining the inputs, possibly defining the input and the output formats.",
                    "label": 0
                },
                {
                    "sent": "How do I parse my data?",
                    "label": 0
                },
                {
                    "sent": "And so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "And typically when you do a task, it won't be a single MapReduce job, right?",
                    "label": 0
                },
                {
                    "sent": "You will have multiple MapReduce jobs doing different parts of the of the computation, and then you will connect them altogether too, actually.",
                    "label": 0
                },
                {
                    "sent": "Achieve what it is you're trying to do.",
                    "label": 0
                },
                {
                    "sent": "So this can get complicated and you can end up writing a lot of line.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The code pretty easily.",
                    "label": 0
                },
                {
                    "sent": "So Pig is a way to is it is a dataflow language.",
                    "label": 0
                },
                {
                    "sent": "That allows you to specify these computations much more concisely.",
                    "label": 0
                },
                {
                    "sent": "So it does provide support for data structures, and as I said, it's a dataflow language.",
                    "label": 0
                },
                {
                    "sent": "It's inspired by SQL, but it's not decorative lights in like SQL, it's actually an imperative language, right?",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me give you an example of how you would do the same thing.",
                    "label": 0
                },
                {
                    "sent": "Compute the histogram of first names in pig.",
                    "label": 0
                },
                {
                    "sent": "So what you say here in this case is, first of all, specify what my input is.",
                    "label": 0
                },
                {
                    "sent": "So I'll say that my input is in that file on HTFS, and it has three columns, the last name and first name in the salary, and I can specify what the corresponding data types are, and in this case pig will be able to knows how to parse this, then for each.",
                    "label": 0
                },
                {
                    "sent": "So this will produce a sequence of of records.",
                    "label": 0
                },
                {
                    "sent": "Right now for this sequence of records, I'll group them by the first name.",
                    "label": 0
                },
                {
                    "sent": "And I'll generate.",
                    "label": 0
                },
                {
                    "sent": "A sequence which basically has.",
                    "label": 0
                },
                {
                    "sent": "Each element is basically the group key, which is the first name and the value is basically all of the values corresponding to that group asset multi set of all the values corresponding to that group and then on this on this on the output of the group operation.",
                    "label": 0
                },
                {
                    "sent": "Basically you say I apply.",
                    "label": 0
                },
                {
                    "sent": "Just do the count of the first names right and group.",
                    "label": 0
                },
                {
                    "sent": "Here is an alias for the for the group by Key and then finally just write this on in on the output and pig will actually behind the scenes map this into the appropriate MapReduce jobs and execute it on the cluster for.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So very quickly pig schemas schema is basically a tuple, tuple, data type and they are optional, right?",
                    "label": 0
                },
                {
                    "sent": "So the data loading step is not really required if you don't have you.",
                    "label": 0
                },
                {
                    "sent": "If you don't have.",
                    "label": 0
                },
                {
                    "sent": "If you don't specify the data types for the input, then basically you can access them in a similar way as you access them in AWK.",
                    "label": 0
                },
                {
                    "sent": "Right Dollar Zero is the first field dollar one and is the 2nd field and so on.",
                    "label": 0
                },
                {
                    "sent": "It does have support for the most common data types and does have support.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or for nesting values.",
                    "label": 0
                },
                {
                    "sent": "Very quick summary of the features so you have constructs for to do data loading and data writing.",
                    "label": 0
                },
                {
                    "sent": "You have filtering operations that will filter filter elements in your in the stream of values Group by operations, join operations, sorting operations and finally union and split combining operations.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Let me move onto cascading so cascading basically is another another way which you can express jobs for express MapReduce programs.",
                    "label": 0
                },
                {
                    "sent": "In an easier way, right?",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a library.",
                    "label": 0
                },
                {
                    "sent": "It's not a new language.",
                    "label": 0
                },
                {
                    "sent": "So basically it's a Java.",
                    "label": 0
                },
                {
                    "sent": "It's a Java library that you would use the higher level abstraction here is for fields and topples fields you can think of as column names and couples you can think of well as couples.",
                    "label": 0
                },
                {
                    "sent": "And pipes are basically is an abstraction for transformations that apply on sequences of of values of couples or on groups.",
                    "label": 0
                },
                {
                    "sent": "And so pipe supply operations to sequences of couples.",
                    "label": 0
                },
                {
                    "sent": "And then you have tap schemes and flows.",
                    "label": 0
                },
                {
                    "sent": "Basically these are a way to specify the input and output formats, and again the goal is to use the.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of of Multijob flows.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is again the same example.",
                    "label": 0
                },
                {
                    "sent": "I'll try to give you the high level idea of how you would write cascading programming cascading.",
                    "label": 0
                },
                {
                    "sent": "So here again I want to compute a histogram of first names.",
                    "label": 0
                },
                {
                    "sent": "So first I create the pipe and give it to the head of the pipe and I give it a name.",
                    "label": 0
                },
                {
                    "sent": "This is the actually should be first names, not last name.",
                    "label": 0
                },
                {
                    "sent": "And then the first thing I'll do basically is I have a function I specify a function.",
                    "label": 0
                },
                {
                    "sent": "There is a library of standard functions, or you can.",
                    "label": 0
                },
                {
                    "sent": "There is a regular expression splitter ready for you.",
                    "label": 0
                },
                {
                    "sent": "Basically, this says create a splitter that will split on the tab and will generate 3 fields, right?",
                    "label": 0
                },
                {
                    "sent": "So the top of the names of the first, second and third element of that output tuple will be last, first in salary.",
                    "label": 0
                },
                {
                    "sent": "That's what this says, and then the first thing that this pipe will do.",
                    "label": 0
                },
                {
                    "sent": "As I said, a pipe is something that transform sequences of couples.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Apply this splitter on each text line, so this basically says for each line in the input, which is a text file, apply the splitting function right so the text line, which is a single string, will get split into first name, last name and last name, first name and salary, and the next thing you want to do is basically group by the first name, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the next thing that happens in the pipe, right?",
                    "label": 0
                },
                {
                    "sent": "So you connect it at the tail and then.",
                    "label": 0
                },
                {
                    "sent": "At this on the sequence now of groups generated what you want to do is aggregate.",
                    "label": 0
                },
                {
                    "sent": "And output account value and this is account aggregation operation right?",
                    "label": 0
                },
                {
                    "sent": "So this is expressed by by there are different classes for different aggregation operations and then you say for every group Now apply this aggregation operation and generate account outlook and then.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you have to specify what kind of format you have for your input.",
                    "label": 0
                },
                {
                    "sent": "It's a text file where it is, where is it located?",
                    "label": 0
                },
                {
                    "sent": "It's on the Hadoop file system and then connect all these things together and create a flow which is an instance of a pipe and then.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As I said, pipes transform streams of couples and there are.",
                    "label": 0
                },
                {
                    "sent": "Basically, at each pipes which supply a function on each element of the table group by and Co Group operations core Group is a kind of join.",
                    "label": 0
                },
                {
                    "sent": "Every applies aggregation operations on groups of couples.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "The operations is what is done to tackles, so you can apply functions.",
                    "label": 0
                },
                {
                    "sent": "You can filter topples so you can apply filter operations that will drop tables and you can do aggregation operate.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When you have groups.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And last part 5.",
                    "label": 0
                },
                {
                    "sent": "And actually wrong will go into a little bit more detail in the third part.",
                    "label": 0
                },
                {
                    "sent": "So how was originally developed at Facebook?",
                    "label": 0
                },
                {
                    "sent": "Now it is a Hadoop subproject and it's a data warehouse infrastructure.",
                    "label": 0
                },
                {
                    "sent": "It uses MapReduce as the execution engine and Hadoop distributed file system as the storage back end.",
                    "label": 0
                },
                {
                    "sent": "It's geared to process very large data sets, an example which wrong will cover is the Facebook daily logs from from the different web servers.",
                    "label": 0
                },
                {
                    "sent": "So which this data grows exponentially 30 gigabytes in January 2815, terabytes last year and.",
                    "label": 0
                },
                {
                    "sent": "I don't know how much it is this year.",
                    "label": 0
                },
                {
                    "sent": "We'll find out later and it it has Hive query language which is a SQL like query language.",
                    "label": 0
                },
                {
                    "sent": "Basically the goal here.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is to make it easy for people that are for people that are familiar with SQL and data warehouses to actually.",
                    "label": 0
                },
                {
                    "sent": "Do these kinds of operations on on, on, on.",
                    "label": 0
                },
                {
                    "sent": "On Hadoop?",
                    "label": 0
                },
                {
                    "sent": "So this is what the same example again would look like.",
                    "label": 0
                },
                {
                    "sent": "Basically, the first the first lines say.",
                    "label": 0
                },
                {
                    "sent": "This is the text file and this is how I want you to interpret it as a table, right?",
                    "label": 0
                },
                {
                    "sent": "So this is it has a last name or first name in the salary.",
                    "label": 0
                },
                {
                    "sent": "It is row format, delete delimited by Newlines and the fields are terminated by tabs.",
                    "label": 0
                },
                {
                    "sent": "It stored as a text file and its location.",
                    "label": 0
                },
                {
                    "sent": "Is there an HTFS or whatever?",
                    "label": 0
                },
                {
                    "sent": "So once you say how do I interpret this this text file as a table essentially?",
                    "label": 0
                },
                {
                    "sent": "Then you can do just the SQL operation.",
                    "label": 0
                },
                {
                    "sent": "Select the first name and account.",
                    "label": 0
                },
                {
                    "sent": "Count starter count one from the records group by first name, and that's it.",
                    "label": 0
                },
                {
                    "sent": "That's almost identical to the way you would do it in in SQL, right on a traditional data warehouse.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The data should belong to tables, but you can also use as I showed pre existing data again like big data loading is optional but encouraged.",
                    "label": 0
                },
                {
                    "sent": "There is the notion of of partitioning columns.",
                    "label": 0
                },
                {
                    "sent": "I'll mention it here so you're aware of this.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into details, but basically there is two kinds of columns.",
                    "label": 0
                },
                {
                    "sent": "The partitioning columns are basically mapped into HTFS directory.",
                    "label": 0
                },
                {
                    "sent": "So let's say I have my log file right and for each log entry I have a data, timestamp and other values right?",
                    "label": 0
                },
                {
                    "sent": "So for example, the user name and what he did, his actions and so on.",
                    "label": 0
                },
                {
                    "sent": "If I decide that the partitioning columns are the date and the time, then basically the data will be partitioned in files based on these on the values and then the remaining columns will actually be stored as values in those files within those within this directory structure.",
                    "label": 0
                },
                {
                    "sent": "Which again is stored on the distributed file system.",
                    "label": 0
                },
                {
                    "sent": "It has support for the most common data types and it has support for pluggable serialization, so you can define your own way.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To to read data from discount, right?",
                    "label": 0
                },
                {
                    "sent": "It supports most of the basic SQL functions, so you can do from subqueries.",
                    "label": 0
                },
                {
                    "sent": "You can do joins, but only Equi joins.",
                    "label": 0
                },
                {
                    "sent": "You can do group by and also you can do sampling of couples.",
                    "label": 0
                },
                {
                    "sent": "As I said, it's extensible so you can write your own MapReduce if something you cannot express it in this subset of SQL, you can write your own method you script, you can write your own user defined functions.",
                    "label": 0
                },
                {
                    "sent": "For more complicated things, you can also write your user defined types as well as serializers, deserializers that determine how the data is read.",
                    "label": 0
                }
            ]
        },
        "clip_127": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And written right in Word format so.",
                    "label": 0
                },
                {
                    "sent": "In summary we went over the basics.",
                    "label": 0
                },
                {
                    "sent": "If there is one thing that you should remember from this part is the core concept right?",
                    "label": 0
                },
                {
                    "sent": "The distributed file system and MapReduce as both are programming as abstraction as well as how it gets mapped and executed on a cluster of machines, right?",
                    "label": 0
                },
                {
                    "sent": "So these are these are the basic things that you should remember from this.",
                    "label": 0
                },
                {
                    "sent": "And then we briefly covered a bunch of other tools.",
                    "label": 0
                },
                {
                    "sent": "Didn't go into detail, but.",
                    "label": 0
                },
                {
                    "sent": "Mention things that you should be aware and try to give you a general idea of what different things are for so you can go and look look into them further.",
                    "label": 0
                },
                {
                    "sent": "If you're interested right so.",
                    "label": 0
                }
            ]
        },
        "clip_128": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All of these provide scalability.",
                    "label": 0
                }
            ]
        },
        "clip_129": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And all of them except map reduce our higher level tools, write map, reduce as David Patterson said, it's kind of the machine language equivalent right for?",
                    "label": 0
                }
            ]
        },
        "clip_130": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For for a for a cluster.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "MapReduce itself and cascading use an existing language, right?",
                    "label": 0
                },
                {
                    "sent": "So you can write stuff in Java, or you can write it in any language of your choice with Hadoop, St.",
                    "label": 0
                }
            ]
        },
        "clip_131": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "Most of them have most of them support some notion of data type or or schema, and all of them have support.",
                    "label": 0
                }
            ]
        },
        "clip_132": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or for user defined data types, hive and pig.",
                    "label": 0
                },
                {
                    "sent": "Designed with easing the transition for people that are familiar with traditional data processing languages like SQL in mind.",
                    "label": 0
                }
            ]
        },
        "clip_133": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So related projects now at a higher level.",
                    "label": 0
                },
                {
                    "sent": "There are other projects that again are for large scale data processing on distributed clusters that is dried and Dryad LINQ, which has been developed at Microsoft.",
                    "label": 0
                },
                {
                    "sent": "Um, there is social, which is basically a language kind of you can think of it as a talk for for, for.",
                    "label": 0
                },
                {
                    "sent": "For a cluster.",
                    "label": 0
                },
                {
                    "sent": "It's developed at Google.",
                    "label": 0
                },
                {
                    "sent": "There is also Dremel that recently came out of of of Google.",
                    "label": 0
                },
                {
                    "sent": "I think it's going to be in this year's VLDB.",
                    "label": 0
                },
                {
                    "sent": "This is geared for more towards real time data processing right?",
                    "label": 0
                },
                {
                    "sent": "As opposed to writing it.",
                    "label": 0
                },
                {
                    "sent": "So typically let's say when you do data analysis, you want a quick turn around and tried many different things, right?",
                    "label": 0
                },
                {
                    "sent": "There is also, as I said, big table is the the original inspiration for for 4H base and disappeared in OSD I 2006 from Google and there is also another implementation open source called Hyper Table at the lower level at the storage there is yet another implementation of.",
                    "label": 0
                },
                {
                    "sent": "Ideas similar to GFS, so our Cosmos file System is an example developed by a company called Cosmic's Virtual Storage Network.",
                    "label": 0
                },
                {
                    "sent": "Then there is of course the elastic compute cloud and the simple storage service in Amazon which provides access to storage and computation.",
                    "label": 0
                },
                {
                    "sent": "There is in the energy in academia there is sector and sphere again which is for large scale data processing and data mining on clusters of machines.",
                    "label": 0
                },
                {
                    "sent": "Again this is released as open source and a lot of other.",
                    "label": 0
                }
            ]
        },
        "clip_134": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Projects right, but let me.",
                    "label": 0
                },
                {
                    "sent": "This is the last slide for this part.",
                    "label": 0
                },
                {
                    "sent": "And then we can all go and take a break.",
                    "label": 0
                },
                {
                    "sent": "So MapReduce is basically a simplified programming model that allows you to.",
                    "label": 0
                },
                {
                    "sent": "Express programs for execution on a cluster, and it's built from the ground up to support scalability, fault tolerance, and run on clusters of commodity hardware.",
                    "label": 0
                },
                {
                    "sent": "Right there is, of course, a growing collection of components on top of on top of.",
                    "label": 0
                },
                {
                    "sent": "MapReduce and and HTFS.",
                    "label": 0
                },
                {
                    "sent": "Some of them.",
                    "label": 0
                },
                {
                    "sent": "Part of the Hadoop project, some of them not.",
                    "label": 0
                },
                {
                    "sent": "And again, this is a very active, very active area.",
                    "label": 0
                }
            ]
        },
        "clip_135": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this concludes the first part of the talk.",
                    "label": 0
                },
                {
                    "sent": "Algorithms and will give you will explain how.",
                    "label": 0
                },
                {
                    "sent": "So if you want to do, let's say the inverted index computations or if you want to do page rank or if you want to do K means or if you want to do naive base and so on.",
                    "label": 0
                },
                {
                    "sent": "What is the map function and what is the reduced function that you would write and typical it's going to be more than one map reduce jobs for this.",
                    "label": 0
                },
                {
                    "sent": "So this is the second part of the tutorial which is going to be about one hour and then finally wrong is going to give real world examples of uses of Hadoop in both of our research setting and an industry setting.",
                    "label": 0
                },
                {
                    "sent": "And give you examples of applications.",
                    "label": 0
                }
            ]
        },
        "clip_136": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Weather all this is used in practice.",
                    "label": 0
                },
                {
                    "sent": "So thank you for any questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Cloud environment.",
                    "label": 0
                },
                {
                    "sent": "Cloud computing environment.",
                    "label": 0
                },
                {
                    "sent": "What exactly do you mean by cloud?",
                    "label": 0
                },
                {
                    "sent": "What do you think?",
                    "label": 0
                },
                {
                    "sent": "As I said, cloud computing is a somewhat overloaded term.",
                    "label": 0
                },
                {
                    "sent": "I can give you my personal biased opinion, so different people mean different things with cloud, so for some people cloud means virtualization and pretty much what Amazon EC2 does.",
                    "label": 0
                },
                {
                    "sent": "For some people, cloud means what Google does internally.",
                    "label": 0
                },
                {
                    "sent": "For other people, cloud means.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Basically, the set of different protocols on the Internet and the fact that now network is a commodity and you can have distributed services running.",
                    "label": 0
                },
                {
                    "sent": "So the point I was trying to make is that cloud computing should not be confused really with large scale data processing, right?",
                    "label": 0
                },
                {
                    "sent": "I wanted to make the point that this is really.",
                    "label": 0
                },
                {
                    "sent": "I wanted to get out of the cloud computing part an just because this is really an overloaded terms that that many people used to mean different things, right?",
                    "label": 0
                },
                {
                    "sent": "As I think Larry Ellison said that Orange is the new pink is what cloud computing is about it anyway?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Sorry, can you speak?",
                    "label": 0
                },
                {
                    "sent": "HFS.",
                    "label": 0
                },
                {
                    "sent": "The reason we didn't cover Cassandra is basically it's.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's a different storage system.",
                    "label": 0
                },
                {
                    "sent": "So within this tutorial we focused basically on map reduce and HTFS.",
                    "label": 0
                },
                {
                    "sent": "And things built on top of this Cassandra is kind of kind of a separate.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really fit in the in the in the MapReduce.",
                    "label": 0
                },
                {
                    "sent": "And HTFS kind of setting, right?",
                    "label": 0
                },
                {
                    "sent": "But again, it's a it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a distributed key value storage system.",
                    "label": 0
                },
                {
                    "sent": "That is gaining a lot of popularity so.",
                    "label": 0
                },
                {
                    "sent": "But I would treat it as kind of separately.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "In practice.",
                    "label": 0
                },
                {
                    "sent": "Well, not yet.",
                    "label": 0
                },
                {
                    "sent": "Learn more about this next session.",
                    "label": 0
                },
                {
                    "sent": "The pipelines yet another classic example ocean borders, you know.",
                    "label": 0
                },
                {
                    "sent": "Essentially, word counts.",
                    "label": 0
                },
                {
                    "sent": "The new will hear more of it.",
                    "label": 0
                },
                {
                    "sent": "You know if I were writing in jobs other kind of want to get my networks of map reduce jobs in each other so that it would be kind of a sufficient possible so we can fill in that region best files all the time.",
                    "label": 0
                },
                {
                    "sent": "So if anybody has any context, how did these pipelines get in trouble?",
                    "label": 0
                },
                {
                    "sent": "Speeding in Mars.",
                    "label": 0
                },
                {
                    "sent": "Um, so yeah.",
                    "label": 0
                },
                {
                    "sent": "Jamaica is going to give you examples in the next section of more.",
                    "label": 0
                },
                {
                    "sent": "Actually the the pipelines can get.",
                    "label": 0
                },
                {
                    "sent": "Pretty deep.",
                    "label": 0
                },
                {
                    "sent": "It's actually then the size of the pipeline is not even deterministic, right?",
                    "label": 0
                },
                {
                    "sent": "So for example when you're doing K means each came in, the iteration will typically be one map reduce job if you so generally for clustering operations, each iteration will be a MapReduce job.",
                    "label": 0
                },
                {
                    "sent": "Each iteration for page rank for the.",
                    "label": 0
                },
                {
                    "sent": "For order for the first singular vector, let's say will be a MapReduce job, right?",
                    "label": 0
                },
                {
                    "sent": "And you will repeat it to convergence.",
                    "label": 0
                },
                {
                    "sent": "So typically these can get the chains can get pretty long.",
                    "label": 0
                },
                {
                    "sent": "And as I said, Jim Inc is going to give examples of how you do page rank, how you do K means, and all these things in the next part.",
                    "label": 0
                },
                {
                    "sent": "Is this a bit more efficient when in the pipeline for long as opposed to job after job after job?",
                    "label": 0
                },
                {
                    "sent": "Well, the the different jobs are executed sequentially, right so?",
                    "label": 0
                },
                {
                    "sent": "So the efficiencies, again different.",
                    "label": 0
                },
                {
                    "sent": "The If you have you.",
                    "label": 0
                },
                {
                    "sent": "Job is known to consume.",
                    "label": 0
                },
                {
                    "sent": "Previous scheduled in the same time consuming for each others.",
                    "label": 0
                },
                {
                    "sent": "Is that more efficient the wait so you're saying something that a job, one that produces output for a job to to run concurrently?",
                    "label": 0
                },
                {
                    "sent": "This is not so.",
                    "label": 0
                },
                {
                    "sent": "The output of the output of one job will go to HTFS.",
                    "label": 0
                },
                {
                    "sent": "It's not no so, and actually, if you want to, if you want to change jobs and then fault tolerance becomes much much much trickier, because if I have a chain of all of this running concurrently and I'm 100 deep and one job in the beginning, fails.",
                    "label": 0
                },
                {
                    "sent": "What do I do?",
                    "label": 0
                },
                {
                    "sent": "So you need to have some kind of checkpointing in the middle, and typically what you do.",
                    "label": 0
                },
                {
                    "sent": "Each MapReduce job will dump the output going between Mapper and Reducer.",
                    "label": 0
                },
                {
                    "sent": "You don't go to HTFS, but when you change different jobs you do go through HDFC.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so well, if there are any other questions you can come to me will take.",
                    "label": 0
                },
                {
                    "sent": "We have actually 15 minutes left for the break.",
                    "label": 0
                },
                {
                    "sent": "Will start around 10:30 ten 35 and as I said you men will be giving examples of algorithms in MapReduce.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}