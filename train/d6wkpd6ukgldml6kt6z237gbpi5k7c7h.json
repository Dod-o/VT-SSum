{
    "id": "d6wkpd6ukgldml6kt6z237gbpi5k7c7h",
    "title": "Open Knowledge Extraction Challenge",
    "info": {
        "author": [
            "Rapha\u00ebl Troncy, EURECOM"
        ],
        "published": "July 10, 2017",
        "recorded": "May 2017",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2017_troncy_open_knowledge/",
    "segmentation": [
        [
            "So thanks for giving me the occasion to present Adele.",
            "So there is an adaptive entity linking framework, so I'm rougher, transient and supervisor of Julian pre PhD students was actually is work and this is a collaboration with disappear Rizzo with researcher in the SMB in Italy.",
            "So we're trying to present you what we made as novelty for this year participation, but as you will see."
        ],
        [
            "The other is actually something we're building over the years, so first then is available is available as a standalone software.",
            "Was a REST API variable for everybody to this year I we first participate in 2015 to the OK Challenge over changes and we want it in 2015.",
            "In 2016 we presented novel contributions which was mainly combining multiple.",
            "Near extractors, so for the extraction part and having strong Neil clustering components to basically detect emergent entities and although we counted and provide them a unique identifier, group them when we believe they refer to the same reward entity and coming with new overlap revolutionary heuristics.",
            "So that was in front 16 where officially we were not winning the challenge last year, but.",
            "After the education when they have revised bit, the datasets we end up actually having the best performances, but we will not pretend to have worn it, so we'll see what this year will bring."
        ],
        [
            "Just would like to emphasis what's knew we have built in France 17 the very first things we have done and that is really in the spirit of our delicacies.",
            "Are there really stands for adaptable?",
            "So we would like to have an entity linking frameworks that can be adapted to the language to the type of entity you would like to extract to the type of documents you need to analyze.",
            "Whether it's Twitter Facebook, statue stages.",
            "Whether it's a news article, whether it's subtitles of a video, those are very different type of documents and Adele can cope with all of them and also with the knowledge base which provides the reference for the disambiguation.",
            "And here this this year OK because of the different task providing disambiguations towards either DB pedia or music brands provides a perfect trial field for testing this activity feature.",
            "So the first thing we brings is a very generic index mechanism.",
            "For indexing any knowledge base of that you can disambiguate to any knowledge base and will.",
            "This will be the main details of my talk here.",
            "We did some type urbanizations because we saw that this morning.",
            "Each challenge come with their own definitions of annotation guidelines and what type they would like to have entities extracted for.",
            "And there is need of harmonization because even two datasets mentioning location, sometimes they didn't mean exactly the same thing with what they mean by location.",
            "So we also bring a new car reference extractor based on deep neural network.",
            "And finally we bring you linking methods for French based on over lexical resources and knowledge bases as well.",
            "In this case you demo but that is not really part of what was needed for."
        ],
        [
            "Is your challenge?",
            "So the very basic principles of Adele is that you really have this combinations of those two family of tools by either what we call the end to end approach that generally consider a Dictionary of mentions and links which are built jointly from a reference knowledge base.",
            "And generally you have text splits.",
            "Then in engrams where you can try to look up candidates and you will jointly extract type and disambiguate the entities versus the linguistic based approaches where you will have.",
            "The sequences of operations where you at first parse a text by typically using supervised classifier, being them condition, conditional, random field or the like where basically you get entities and then you do look up into your knowledge base to find a links that will disambiguate them."
        ],
        [
            "So from a very high level view as well as this sequence of operations, we do first extraction and if of course identity is already given then we just can just go directly to the second step.",
            "The linking step for which we need an index of the knowledge base in which you would like to disambiguate two entities, so."
        ],
        [
            "So about the extractors, the strength of Adele is that it has a lot of extractors based on the pose tag, the part of speech tag based on really near CRF model like the famous 10 for one which can be trained on different datasets and this is what we do a lot.",
            "You will see that but also based on dictionaries and overspecialized.",
            "The extractors, for example, we have specialized neck structures for dates and time.",
            "We are specializing extractors for Twitter account handlers and so on and so on.",
            "So because we have all those extractors, we have the risk of having overlaps.",
            "Two extractors that slightly but the detect the same entity but we've slightly different face form for example so."
        ],
        [
            "Did you risztics to basically decide?",
            "Ultimately, if two extractors provide and extractions how you combine them all, you merge them, and here we have developed and implemented differently.",
            "You risztics and we can also comply with what we were learning this morning that you should always take the longest substrings.",
            "This is one of the heuristics actually, but there are over you risztics implemented.",
            "Now coming back to the link."
        ],
        [
            "Import the disambiguation part so that all starts with knowledge graph and knowledge base for which we need to index it.",
            "So what we did is that typically here taking the case of both DB Pedia and Musicbrainz we will first index or properties that have sort of data type properties that.",
            "We'll have strongly towards as values I should have written as values.",
            "Actually all those properties we need at least label an.",
            "Ideally we we need at least score property, so score in the sense that like page rank like it's something that give the notion of the popularity of this property into the data sets into the Knowledge Graph.",
            "So if you do that for DPR you will find out that you can index 281 properties.",
            "That are of this type and for music brands you will be able to index seven of them.",
            "Now what does it mean?",
            "It means that for any candidate you should query all those properties wherever they can be, possibly retrieve for as a reference for this entity.",
            "And of course that is very costly to do that many queries to any index.",
            "So what we try to bring in this year is to optimize this index to have a much.",
            "Compact index so that basically you sure to still query a very limited number of properties, but you're always sure to get the good entities if they are present into the United States and in the paper you will find the algorithm that enables to optimize drastically this index and you see what I can tell you, and you can also have that in the poster will present on Thursday morning.",
            "Is that we've only four properties of the pedia.",
            "You're almost guaranteed to always be able to retrieve the candidates, the right entity, the right link that disambiguate the entity, and four properties as well.",
            "From experience, and I can tell you what are those four magic properties so?"
        ],
        [
            "The optimization is done for specific index of course, and this is a free strap.",
            "Three steps process where basically you use first the Goldstone order annotation that you have to get a list of the good pairs mentioned in link and then you will query every mentions against each property of the index to make sure.",
            "If you can find the good links querying this property.",
            "And of course at the end of the first step you would get a much smaller subset."
        ],
        [
            "So now once you get this index you need now.",
            "Stealing King formula that query this index and propose the right entity that disambiguate the right link for disambiguate density.",
            "So this is the formula we use in Adela, and it's not that good because it's largely over promote the popular entity rather than using the full context in which the entity is appearing.",
            "But we are working on improving that, so just some."
        ],
        [
            "Mary results because we don't have the test set results.",
            "So what we did here is classic.",
            "We did a full cross 44 cross validation over the training data.",
            "We use the naval scorer in this case, but we hope we get similar results using GI Bill as the official benchmarking platform for the task.",
            "One which is the focused name, entity identification and linking.",
            "So here you need to as Michael's reminded when you need to extract and link.",
            "Entities from free classes only.",
            "This is the results we have, so you see that at the extraction.",
            "And typing level, so the strong mentioned match we are pretty good in terms of precision recall and F measure.",
            "So that means we are reasonably confident we can extract a larger number of entities belonging to those three classes.",
            "Then we have a big drop in the linking and we know why we're working on it, that for the task to where you had to extract entities from many more types because there were twelve top classes and then much larger number of step classes.",
            "You see that the extraction slightly.",
            "Decrease the performance in terms of the F1.",
            "Well, simply because you have more types to consider, so of course any classification task when the more the more classes you had in, the more difficult classification is and we still observe the similar big drop in terms of at the linking stage.",
            "What did we do?"
        ],
        [
            "And we've got exercise.",
            "We try to.",
            "So yeah.",
            "First, what's the magic combinations that enables to provide those results?",
            "Is the following one.",
            "We mix up different near?",
            "Components, the ones which are pre trained on the cone container for the three datasets, the big news datasets for the free classes of Stanford 4 classes in the seven classes of Stanford and we had our own model train on each fold of the OK it from 17, although each full were very small because the training data was very small.",
            "But we tried many more combinations.",
            "We tried to add dictionary extractors for which we can gain a bit of recall.",
            "But the big drop of precisions and the and we have an explanation for that, again explaining the paper is because then we start ending up with a lot of.",
            "Common nouns like against which is also famous band, and suddenly all those strings against are attracted as entities, where often they are not.",
            "If we try to embed the pose extractors again, we will gain a bit more record, but again we will lose a lot of precision an this is an example.",
            "Typical example, if you add the pose extractors you will get you will start extracting entities such as World War Two, which is for many people consider the name entity.",
            "But was not Annotated's name entity for this particular challenge.",
            "So because of that you know you have to respect the annotation guidelines of the challenge, so that's why we finally didn't include this extractor.",
            "We try to add more models from previous years.",
            "OK 2015, 2016.",
            "And again similar observations, we gain a bit of record and massively drop of precisions.",
            "And again here you see the inconsistency in annotation rules year after year, which makes that very hard to use previous years data.",
            "And that observation is made on task one, but similar observations on task 2 details in the paper.",
            "The good news is this one or indexing formula.",
            "So that is the record at many because you can see 17.",
            "So that means what it means that.",
            "We we find in 95.75% of the cases we find the correct link, but sometimes you have to go to the proposed links ranks 1729 so it's not the first one.",
            "Of course you would like to get a very good performance at recall at one and not at that many.",
            "So sometimes we know that the correct link would be retrieved by your indexing technique, but sometimes you have to go very far in the ranking.",
            "So the big question is how you?",
            "Push and you boosted this Frank into the top positions, and this is where the linking formula plays a key role.",
            "That's it.",
            "I hope I haven't been too long."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thanks for giving me the occasion to present Adele.",
                    "label": 0
                },
                {
                    "sent": "So there is an adaptive entity linking framework, so I'm rougher, transient and supervisor of Julian pre PhD students was actually is work and this is a collaboration with disappear Rizzo with researcher in the SMB in Italy.",
                    "label": 0
                },
                {
                    "sent": "So we're trying to present you what we made as novelty for this year participation, but as you will see.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other is actually something we're building over the years, so first then is available is available as a standalone software.",
                    "label": 0
                },
                {
                    "sent": "Was a REST API variable for everybody to this year I we first participate in 2015 to the OK Challenge over changes and we want it in 2015.",
                    "label": 0
                },
                {
                    "sent": "In 2016 we presented novel contributions which was mainly combining multiple.",
                    "label": 0
                },
                {
                    "sent": "Near extractors, so for the extraction part and having strong Neil clustering components to basically detect emergent entities and although we counted and provide them a unique identifier, group them when we believe they refer to the same reward entity and coming with new overlap revolutionary heuristics.",
                    "label": 0
                },
                {
                    "sent": "So that was in front 16 where officially we were not winning the challenge last year, but.",
                    "label": 0
                },
                {
                    "sent": "After the education when they have revised bit, the datasets we end up actually having the best performances, but we will not pretend to have worn it, so we'll see what this year will bring.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just would like to emphasis what's knew we have built in France 17 the very first things we have done and that is really in the spirit of our delicacies.",
                    "label": 0
                },
                {
                    "sent": "Are there really stands for adaptable?",
                    "label": 0
                },
                {
                    "sent": "So we would like to have an entity linking frameworks that can be adapted to the language to the type of entity you would like to extract to the type of documents you need to analyze.",
                    "label": 0
                },
                {
                    "sent": "Whether it's Twitter Facebook, statue stages.",
                    "label": 0
                },
                {
                    "sent": "Whether it's a news article, whether it's subtitles of a video, those are very different type of documents and Adele can cope with all of them and also with the knowledge base which provides the reference for the disambiguation.",
                    "label": 0
                },
                {
                    "sent": "And here this this year OK because of the different task providing disambiguations towards either DB pedia or music brands provides a perfect trial field for testing this activity feature.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we brings is a very generic index mechanism.",
                    "label": 0
                },
                {
                    "sent": "For indexing any knowledge base of that you can disambiguate to any knowledge base and will.",
                    "label": 0
                },
                {
                    "sent": "This will be the main details of my talk here.",
                    "label": 0
                },
                {
                    "sent": "We did some type urbanizations because we saw that this morning.",
                    "label": 0
                },
                {
                    "sent": "Each challenge come with their own definitions of annotation guidelines and what type they would like to have entities extracted for.",
                    "label": 0
                },
                {
                    "sent": "And there is need of harmonization because even two datasets mentioning location, sometimes they didn't mean exactly the same thing with what they mean by location.",
                    "label": 0
                },
                {
                    "sent": "So we also bring a new car reference extractor based on deep neural network.",
                    "label": 1
                },
                {
                    "sent": "And finally we bring you linking methods for French based on over lexical resources and knowledge bases as well.",
                    "label": 0
                },
                {
                    "sent": "In this case you demo but that is not really part of what was needed for.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is your challenge?",
                    "label": 0
                },
                {
                    "sent": "So the very basic principles of Adele is that you really have this combinations of those two family of tools by either what we call the end to end approach that generally consider a Dictionary of mentions and links which are built jointly from a reference knowledge base.",
                    "label": 1
                },
                {
                    "sent": "And generally you have text splits.",
                    "label": 0
                },
                {
                    "sent": "Then in engrams where you can try to look up candidates and you will jointly extract type and disambiguate the entities versus the linguistic based approaches where you will have.",
                    "label": 0
                },
                {
                    "sent": "The sequences of operations where you at first parse a text by typically using supervised classifier, being them condition, conditional, random field or the like where basically you get entities and then you do look up into your knowledge base to find a links that will disambiguate them.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So from a very high level view as well as this sequence of operations, we do first extraction and if of course identity is already given then we just can just go directly to the second step.",
                    "label": 0
                },
                {
                    "sent": "The linking step for which we need an index of the knowledge base in which you would like to disambiguate two entities, so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So about the extractors, the strength of Adele is that it has a lot of extractors based on the pose tag, the part of speech tag based on really near CRF model like the famous 10 for one which can be trained on different datasets and this is what we do a lot.",
                    "label": 0
                },
                {
                    "sent": "You will see that but also based on dictionaries and overspecialized.",
                    "label": 0
                },
                {
                    "sent": "The extractors, for example, we have specialized neck structures for dates and time.",
                    "label": 0
                },
                {
                    "sent": "We are specializing extractors for Twitter account handlers and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So because we have all those extractors, we have the risk of having overlaps.",
                    "label": 0
                },
                {
                    "sent": "Two extractors that slightly but the detect the same entity but we've slightly different face form for example so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did you risztics to basically decide?",
                    "label": 0
                },
                {
                    "sent": "Ultimately, if two extractors provide and extractions how you combine them all, you merge them, and here we have developed and implemented differently.",
                    "label": 0
                },
                {
                    "sent": "You risztics and we can also comply with what we were learning this morning that you should always take the longest substrings.",
                    "label": 0
                },
                {
                    "sent": "This is one of the heuristics actually, but there are over you risztics implemented.",
                    "label": 0
                },
                {
                    "sent": "Now coming back to the link.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Import the disambiguation part so that all starts with knowledge graph and knowledge base for which we need to index it.",
                    "label": 0
                },
                {
                    "sent": "So what we did is that typically here taking the case of both DB Pedia and Musicbrainz we will first index or properties that have sort of data type properties that.",
                    "label": 1
                },
                {
                    "sent": "We'll have strongly towards as values I should have written as values.",
                    "label": 0
                },
                {
                    "sent": "Actually all those properties we need at least label an.",
                    "label": 0
                },
                {
                    "sent": "Ideally we we need at least score property, so score in the sense that like page rank like it's something that give the notion of the popularity of this property into the data sets into the Knowledge Graph.",
                    "label": 1
                },
                {
                    "sent": "So if you do that for DPR you will find out that you can index 281 properties.",
                    "label": 0
                },
                {
                    "sent": "That are of this type and for music brands you will be able to index seven of them.",
                    "label": 0
                },
                {
                    "sent": "Now what does it mean?",
                    "label": 0
                },
                {
                    "sent": "It means that for any candidate you should query all those properties wherever they can be, possibly retrieve for as a reference for this entity.",
                    "label": 0
                },
                {
                    "sent": "And of course that is very costly to do that many queries to any index.",
                    "label": 0
                },
                {
                    "sent": "So what we try to bring in this year is to optimize this index to have a much.",
                    "label": 0
                },
                {
                    "sent": "Compact index so that basically you sure to still query a very limited number of properties, but you're always sure to get the good entities if they are present into the United States and in the paper you will find the algorithm that enables to optimize drastically this index and you see what I can tell you, and you can also have that in the poster will present on Thursday morning.",
                    "label": 0
                },
                {
                    "sent": "Is that we've only four properties of the pedia.",
                    "label": 1
                },
                {
                    "sent": "You're almost guaranteed to always be able to retrieve the candidates, the right entity, the right link that disambiguate the entity, and four properties as well.",
                    "label": 0
                },
                {
                    "sent": "From experience, and I can tell you what are those four magic properties so?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The optimization is done for specific index of course, and this is a free strap.",
                    "label": 1
                },
                {
                    "sent": "Three steps process where basically you use first the Goldstone order annotation that you have to get a list of the good pairs mentioned in link and then you will query every mentions against each property of the index to make sure.",
                    "label": 1
                },
                {
                    "sent": "If you can find the good links querying this property.",
                    "label": 0
                },
                {
                    "sent": "And of course at the end of the first step you would get a much smaller subset.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now once you get this index you need now.",
                    "label": 0
                },
                {
                    "sent": "Stealing King formula that query this index and propose the right entity that disambiguate the right link for disambiguate density.",
                    "label": 0
                },
                {
                    "sent": "So this is the formula we use in Adela, and it's not that good because it's largely over promote the popular entity rather than using the full context in which the entity is appearing.",
                    "label": 0
                },
                {
                    "sent": "But we are working on improving that, so just some.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mary results because we don't have the test set results.",
                    "label": 0
                },
                {
                    "sent": "So what we did here is classic.",
                    "label": 0
                },
                {
                    "sent": "We did a full cross 44 cross validation over the training data.",
                    "label": 0
                },
                {
                    "sent": "We use the naval scorer in this case, but we hope we get similar results using GI Bill as the official benchmarking platform for the task.",
                    "label": 0
                },
                {
                    "sent": "One which is the focused name, entity identification and linking.",
                    "label": 0
                },
                {
                    "sent": "So here you need to as Michael's reminded when you need to extract and link.",
                    "label": 0
                },
                {
                    "sent": "Entities from free classes only.",
                    "label": 0
                },
                {
                    "sent": "This is the results we have, so you see that at the extraction.",
                    "label": 0
                },
                {
                    "sent": "And typing level, so the strong mentioned match we are pretty good in terms of precision recall and F measure.",
                    "label": 0
                },
                {
                    "sent": "So that means we are reasonably confident we can extract a larger number of entities belonging to those three classes.",
                    "label": 0
                },
                {
                    "sent": "Then we have a big drop in the linking and we know why we're working on it, that for the task to where you had to extract entities from many more types because there were twelve top classes and then much larger number of step classes.",
                    "label": 0
                },
                {
                    "sent": "You see that the extraction slightly.",
                    "label": 0
                },
                {
                    "sent": "Decrease the performance in terms of the F1.",
                    "label": 0
                },
                {
                    "sent": "Well, simply because you have more types to consider, so of course any classification task when the more the more classes you had in, the more difficult classification is and we still observe the similar big drop in terms of at the linking stage.",
                    "label": 0
                },
                {
                    "sent": "What did we do?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we've got exercise.",
                    "label": 0
                },
                {
                    "sent": "We try to.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "First, what's the magic combinations that enables to provide those results?",
                    "label": 0
                },
                {
                    "sent": "Is the following one.",
                    "label": 0
                },
                {
                    "sent": "We mix up different near?",
                    "label": 0
                },
                {
                    "sent": "Components, the ones which are pre trained on the cone container for the three datasets, the big news datasets for the free classes of Stanford 4 classes in the seven classes of Stanford and we had our own model train on each fold of the OK it from 17, although each full were very small because the training data was very small.",
                    "label": 1
                },
                {
                    "sent": "But we tried many more combinations.",
                    "label": 0
                },
                {
                    "sent": "We tried to add dictionary extractors for which we can gain a bit of recall.",
                    "label": 0
                },
                {
                    "sent": "But the big drop of precisions and the and we have an explanation for that, again explaining the paper is because then we start ending up with a lot of.",
                    "label": 0
                },
                {
                    "sent": "Common nouns like against which is also famous band, and suddenly all those strings against are attracted as entities, where often they are not.",
                    "label": 0
                },
                {
                    "sent": "If we try to embed the pose extractors again, we will gain a bit more record, but again we will lose a lot of precision an this is an example.",
                    "label": 0
                },
                {
                    "sent": "Typical example, if you add the pose extractors you will get you will start extracting entities such as World War Two, which is for many people consider the name entity.",
                    "label": 0
                },
                {
                    "sent": "But was not Annotated's name entity for this particular challenge.",
                    "label": 0
                },
                {
                    "sent": "So because of that you know you have to respect the annotation guidelines of the challenge, so that's why we finally didn't include this extractor.",
                    "label": 0
                },
                {
                    "sent": "We try to add more models from previous years.",
                    "label": 1
                },
                {
                    "sent": "OK 2015, 2016.",
                    "label": 0
                },
                {
                    "sent": "And again similar observations, we gain a bit of record and massively drop of precisions.",
                    "label": 0
                },
                {
                    "sent": "And again here you see the inconsistency in annotation rules year after year, which makes that very hard to use previous years data.",
                    "label": 0
                },
                {
                    "sent": "And that observation is made on task one, but similar observations on task 2 details in the paper.",
                    "label": 1
                },
                {
                    "sent": "The good news is this one or indexing formula.",
                    "label": 0
                },
                {
                    "sent": "So that is the record at many because you can see 17.",
                    "label": 0
                },
                {
                    "sent": "So that means what it means that.",
                    "label": 0
                },
                {
                    "sent": "We we find in 95.75% of the cases we find the correct link, but sometimes you have to go to the proposed links ranks 1729 so it's not the first one.",
                    "label": 0
                },
                {
                    "sent": "Of course you would like to get a very good performance at recall at one and not at that many.",
                    "label": 0
                },
                {
                    "sent": "So sometimes we know that the correct link would be retrieved by your indexing technique, but sometimes you have to go very far in the ranking.",
                    "label": 0
                },
                {
                    "sent": "So the big question is how you?",
                    "label": 0
                },
                {
                    "sent": "Push and you boosted this Frank into the top positions, and this is where the linking formula plays a key role.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "I hope I haven't been too long.",
                    "label": 0
                }
            ]
        }
    }
}