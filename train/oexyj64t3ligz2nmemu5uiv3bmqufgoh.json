{
    "id": "oexyj64t3ligz2nmemu5uiv3bmqufgoh",
    "title": "Capacity Control for Partially Ordered Feature Sets",
    "info": {
        "author": [
            "Ulrich R\u00fcckert, International Computer Science Institute, UC Berkeley"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_ruckert_ccpofs/",
    "segmentation": [
        [
            "Let's get started.",
            "So this is about capacity control for partially ordered feature sets."
        ],
        [
            "And here's a short outline of what I'm going to talk about.",
            "First of all, is going to be in short introduction, or give some background about the concepts I use in the motivation.",
            "But this is about then the actual results are in two parts.",
            "The first one is the theoretical work at it on distribution independent stuff, and also when you make some assumptions about the data generating distribution and then the second part is basically the practical part.",
            "Report about the experiments with it.",
            "And of course there's going to be a conclusion."
        ],
        [
            "So let's get started.",
            "The general setting I'm working on IS classification supervised machine learning.",
            "This classification or regression, and in particular I'd like to work on datasets where the we have the data instances are structured objects.",
            "So for instance you could have the case where the instances are molecules that have a structure given as a graph, and in order to use such datasets with propositional learning methods, what you usually do is you have to extract features you have to generate features.",
            "And one popular way to do so is.",
            "In that case is to have features that test for substructures.",
            "So for instance, in that particular case you could have a feature that tests whether this ethnic carbon ethnic group is present.",
            "Or you could check whether an oxygen autumn is there, or you could check whether this oxygen Atom together with the carbon Atom is is present and one thing that is particularly interesting about this kind of feature representation is that the features can actually be.",
            "Ordered according to partial order.",
            "So whenever you know that this partial this carbon asset group is present in a molecule, then you also know that of course this oxygen atoms present and you also know that this feature is present.",
            "So this gives you some sort of redundancy in the data set, right?",
            "Since if you know feature one is present then you also know a couple of other features are present.",
            "So one of the problems you're having here is that the datasets are by design kind of redundant and.",
            "The question I was asking is how does this redundancy effect classification, linear classifiers, and in particular how does it?"
        ],
        [
            "Um, affect capacity control behavior and so this is just an overview.",
            "What exactly is capacity control?",
            "Capacity is basically the idea.",
            "How large should the hypothesis class and the learner has to choose a classifier from?",
            "Here's a small example.",
            "We have a training set containing positive and negative examples, and if you have a learner with the let's say, a smaller, this class that only allows linear classifiers, then the best classifier you could come up with is that one over there that.",
            "Cannot explain the training set correctly.",
            "However, you could also have a learner with a more complex, more larger hypothesis class.",
            "Then you could maybe have a classifier that has a.",
            "More.",
            "Better shaped decision border that completely explains the training set.",
            "However, however, in most cases you probably would stick to to that case over there because this is most of the time this is simply overfitting because you're fitting to a random noise in the in the training set, and you're not getting the actually underlying concept.",
            "On the other hand, so in this case you prob."
        ],
        [
            "I would say this one is overfitting and this one is quite right.",
            "On the other hand, if you have a lot of data then maybe you have that much evidence for a certain really complex decision border that you could actually choose a hypothesis class that is larger, higher capacity, and that because you have enough evidence to to make sure that this is actually the right decision border.",
            "And in this case just having a linear one would actually be underfitting, right?",
            "So basically we're capacity of control is about if you want to have a learner that performs well, you need to learn.",
            "That has to have a positive classroom, exactly the right capacity.",
            "If the capacity is too large, then you're probably going to overfit.",
            "If it's too loud, you're going to underfit Anne.",
            "How do you quantify those things?",
            "The standard approach or popular approach that is using uniform convergence bounds from computer in learning theory?",
            "Basically what you do is you start with the with the hypothesis class you let the learner stuff with a fixed data generating distribution an arbitrary one, and you let the learner come up with the classifier.",
            "Then what the theory tells you is with high probability it is the case that the true error of the classifier you have the true errors this thing.",
            "So you're going to expect on the test data set that one is going to be smaller than the training set error, plus something that depends on the hypothesis class an.",
            "So depending on what kind of bond you have, we have different parameters in there, but generally at least for linear classes."
        ],
        [
            "Why is what you usually have is you have to be see dimension, which happens to be just the number of features.",
            "I'm pulling your classifiers so for linear classifiers in this capacity estimate term you have number of training instances and you have the number of features.",
            "That means the more features you have, the higher capacity is, and that means basically that the question of finding the right capacity translates in that setting into finding the right number of features.",
            "If you have millions of features, it's likely they're going to overfit.",
            "If you have just a few features, it's probably going to be under fit an now.",
            "Of course, the interesting question to me was.",
            "I'm in our setting.",
            "We also know that the features are partially ordered, so there is some redundancy in there and if you look at that, if you think about this then it should be quite obvious that the capacity with partially ordered features should actually be a little bit smaller than in the general case, and maybe we could also come up with something that qualifies the capacity depending on the actual partial order, because they are of course very impartial orders.",
            "So when I looked into this the very."
        ],
        [
            "1st results that I got was actually quite surprising to me because the very first result I got is that the capacity of linear classifiers with partially order features is exactly the same as the capacity of linear classifiers with arbitrary unordered features, so the capacity doesn't really change it all.",
            "Basically what this result, or more precisely what this result says, is that the VC dimension remains the same, so the VC dimension of an M dimensional linear classifier is still N + 1.",
            "Regardless of whether the features are partially or not, and since there are known well known results, they give up on lower bounds on the capacity estimate.",
            "That means that capacity remains approximately the same thing, so that was kind of surprising, because intuitively I thought that the capacity would go down, we get smaller, and.",
            "If you look at this a little bit more closely then.",
            "Turns out the reason why this is actually the case, the capacity remains the same is that those these bounds over there are a rather general they have to work for all possible distributions and particularly also have to work for distributions that are really weird and kind of that.",
            "Kind of burst cases, you distributions and the interesting question then, is of course do so.",
            "Do those worst case distributions also appear in practice?",
            "Are those distributions that are practically relevant and."
        ],
        [
            "When I looked at that, it turned out that I can.",
            "I can say you probably will never encounter such distributions in practice.",
            "At least I haven't seen it so far.",
            "So it turns out that in practice, most distributions, most data generating distributions have some properties that make them more favorable, and they allow a different kind of analysis, and in particular, most of these distributions have a property that I call an exponential decay in the level probabilities, so this is.",
            "OK, so here's an explanation of what these exponential decay in level probabilities means.",
            "Here's an example.",
            "Assume we have again the subgroup of graph testing features.",
            "So you have.",
            "You can actually order them by by level.",
            "So on the top you have the empty subgroup sub graph that occurs in all kinds of graphs.",
            "Then you have on level one you have all subgraphs that have exactly one note.",
            "So that's either carbon or oxygen Atom than a level 2.",
            "You have combinations of two of them on Level 3 of combinations that contains 3 nodes, and so on.",
            "So you can have lots of levels, and of course what's happening is that the number of features.",
            "Increases exponentially in each level, so that's one thing.",
            "The other thing you could do is you could actually look at how often such a feature can be observed in practice.",
            "So if you just if you have a data generating distribution, you could."
        ],
        [
            "Write down that, for instance, the the empty subgraphs occurs in all subgraphs, so this appears with probability 1.0 an.",
            "Let's say this one, the two carbon atoms appears in 60% of all cases.",
            "Sorry, probability of seeing that one would be 60%, and so on.",
            "So you can write that down for every feature, and then you also can do is you can take the maximum value of over all these occurrence probabilities over each level.",
            "So for instance.",
            "In level one we."
        ],
        [
            "22 features basically the one has probability point 91 has probability .3.",
            "So you take the maximum and there will be .9 over there and you do the same thing in Level 2, Level 3 and so on.",
            "So with that you get for each level.",
            "A number that I called the level probability and if you look at it in practice then it happens to be that this level probability in almost all cases I've seen goes really down quickly, quickly, quickly.",
            "So here's an example for the entity our data set and you see it's really almost an exponential decay you have there.",
            "Now.",
            "Why is that important?",
            "On the interesting."
        ],
        [
            "Think of is that you can actually rewrite or you can modify some of the uniform convergence bounds so that they work with the number of features and so that they work also with the level probabilities.",
            "And here's what the bounds look like.",
            "I'm the first cases for this year one loss that's very misclassification gets penalty one and a core classification gets penalty zero, and in that case the IT looks a little bit complicated, but in fact.",
            "So, so this is the level probability and this is the number of features per level.",
            "So what's happening there is that this capacity estimate increases with the number of features per level and decreases if the level probability it gets get smaller and you can actually see it way better if you don't use the 01 loss but the... loss of Chester continuous version that gives values between 0 and 1, two cases that are near the decision boundary and what you're having there is that what you do is you take for each level you take the number of features.",
            "And then you multiply it with the with the level probability, and then you add that up for all levels.",
            "That's the sum over there.",
            "So this gives you an incapacity, an estimate of the capacity for linear classifiers on partially ordered features now.",
            "Well, that means basically is."
        ],
        [
            "That if you have a little probability is decreasing rather quickly, then your capacity is smaller than it will be in the general case.",
            "Right, so let's have a look at what this looks likes and some practical.",
            "Settings.",
            "So let us assume that the the level probability has actually exponential decay, so for every level I the Lambda I is smaller than Alpha to the power of our Alpha.",
            "Being just a conservative.",
            "Actor and let's have a look at some of the some of the existing partial orders you can work with.",
            "One instance would be totally ordered features that this one.",
            "So every feature is a refinement of the of the preceding feature and their case.",
            "D is just one and what you get is a geometrical serious and that one is always smaller than 1 / 1 minus Alpha.",
            "So you always have a finite rather small capacity, even though you can have large number of features.",
            "That's not something you would probably see in practice, but at least it's most trick.",
            "Or you can have, so might be interesting from a theoretical .4 item sets.",
            "That's another nice case.",
            "So in this case the features are item sets there, checking for the appearance of items an you can also order them according to the subset of partial or and in that case you give you get 2 to the power of K features, K being the number of items an if you have the.",
            "Exponential decay again.",
            "Then you get a capacity is 1 plus Alpha to the power of K. So that means if you have very very steep decay then Alpha will be very small, almost zero.",
            "So this is really close to 1.",
            "On the other hand, if you have no decay at all, then this will be off will be one.",
            "So you have the capacity is 2 to the power of K, which is just the same capacity as in the general case.",
            "So you can really see the capacity goes between one and two to the power of.",
            "Kay has really brought range depending on the level.",
            "Um, the level probabilities.",
            "OK here the."
        ],
        [
            "Responding terms for strings, trees and graphs.",
            "What's interesting here is that for strings, it really depends on also on the Alpha and a number of characters in the alphabet, and if Alpha is small enough, and particularly if it's smaller than 1 / H, then this series converges, and then you also have a finite capacity even through you can have an infinite amount of features which might be interesting for some applications.",
            "The trees are actually just like strings, just they just have different content in there, so they're actually quite rated, which I found quite surprising.",
            "Graphs are actually very different because they also have the factorial in there.",
            "That means they they sum is divergent all the time, so you always have an infinite capacity even, even though you might have very steep level decay level probability decay.",
            "So this is the theory.",
            "Which might be nice, or maybe not nice.",
            "Of course to a practitioner would be much more interesting to see how this works."
        ],
        [
            "In practice, and to get an idea of that, I did a couple of experiments, so the basic idea was the theory tells us that the capacity of linear classifiers can be way smaller than if you have partially ordered features.",
            "So what that would mean?",
            "In principle it would mean that overfitting is probably not that much of an issue, and underfitting might be the larger issue.",
            "In such settings, an in order to to get an idea of that.",
            "I had a couple of experiments on three datasets there was exactly the setting I have as an example here, so instances are labeled graphs representing compounds, and the feature tests for the occurrence of subgraphs, and I used the sub graph money tool similar to G span to generate all the features and then uses a perfect machine to learn a classifier that predicts the biological activity of the component.",
            "So here's the first question is, are we going?",
            "Are we having overfitting with that?",
            "So what I did is I generated almost all the actually all features or non non duplicated software features.",
            "And here are the all fitting curves.",
            "So the blue one is the training."
        ],
        [
            "You see the agreement accuracy.",
            "As you can see, there is virtually no overfitting, so this seems to be consistent with theory, so there is definitely no fitting.",
            "Maybe there, so the next obvious question is, well, if there's no fitting, maybe there's underfitting.",
            "So in order to have a look at that, I.",
            "The basic idea what I actually needed is."
        ],
        [
            "I needed to choose what unfitting I need a class of features feature class that has a larger capacity.",
            "So now how can you increase the capacity?",
            "And if you look at the theory side again and you see the capacity depends on two things.",
            "First of all, the number of features per per level and the other thing being the level probabilities.",
            "So what you could use, you could try to come up with features, whether the decay of the level probabilities is not that steep.",
            "In order to do that.",
            "My idea was to have also subgroup features that have elastic edges and elastic edge means.",
            "Here's an example for such a thing with an elastic edge.",
            "So what you have is you have Justin sub graph and one of the edges, particularly that one.",
            "The Red One is an elastic edge, so in order to match that one to an instance like the instance here on the left doesn't work.",
            "Um, in order to match that, what you would do is you match just the non elastic parts like this part and you mention this part which happened to occur over there and over there so.",
            "It doesn't work, but I hope you can see that and then in the next step you try to match the elastic edge and the elastic edge can be matched with any path in the in the training instance.",
            "So in this case it is a match because the elastic was there can be matched with this path."
        ],
        [
            "Pop over there, right?",
            "So that's that's the idea that you have sub graphs that are more flexible because you can test for various components that just have to be connected over some part.",
            "And if you modify your sub graph mining tools so that it generates these kind of features then you can also run the experiments again and it turned out that doing so had significant improvements of the particular C on a test set on two of the three.",
            "That's it.",
            "Datasets, in particular on the NCR and the blood brain barrier data set, and there was no change in particularly on the yesterday I said.",
            "So it seems that Underfitting, apparently, at least when you look at it like this.",
            "Other thing was a problem on two of the three datasets.",
            "And so it was a good idea to come up with a more expressive features feature class.",
            "No, so that that sounds like good advice, right?",
            "If you're working this kind of field, and if you want to get more accuracy and you won't know what underfitting, you should just go out for feature feature set.",
            "Plus that is more expressive and it has a level probability indicator is not too steep.",
            "Unfortunately.",
            "In practice that's that advises a little bit difficult to implement because having such a feature set class means that you have tons thousand millions of.",
            "Features and the big question actually then, is how to deal with."
        ],
        [
            "Those features that classes that are so large, and particularly if you have data sets that are large, two.",
            "So for instance, I have here a data set that has 35,000 compounds and I.",
            "If you have just the sequences of length four that I think there are almost over 10,000 sequences, so that's really hard to to compute in reasonable time.",
            "So the question is, how can we work with that?",
            "And one thing I tried was to work with an approach that I have.",
            "Presented this email 2007 two years ago and the basic idea is that you don't generate all the features, you just try to generate a subset of features and basically you try to find those features that complement each other in some almost optimal way.",
            "Do that, then you can actually work with such huge datasets.",
            "So for instance, for sequences and graphs you need 28 and 68 minutes and elastic graphs are then only twice that, which is still manageable, right?",
            "So of course there's lots of lots of other work you could do.",
            "Another approaches you could use, I think that's basically an open question where you can.",
            "Work with some really large feature set classes which you do need to toward underfitting and how to do that efficiently.",
            "So that is the experiments, and here is the conclusion.",
            "So what I just presented is basically to."
        ],
        [
            "Two parts, the first one is a theoretical analysis of capacity control for linear classifiers with partially or features, and there were basically 2 main results.",
            "First one is that in the general case there is no change in the capacity in the.",
            "In the case where you have.",
            "Probably probability decline than the capacity can get considerably smaller, and in particular for item set strings trees.",
            "You can even have finite capacity even with an infinite amount of features.",
            "And that's not true for graphs and on the on the practical side, the experiment empirical experiments showed that there's practically no overfitting.",
            "There is some underfitting on two of the three datasets an in order to avoid this underfitting.",
            "It might work if you use a different.",
            "Set of features.",
            "In particular, you could use software features that have these elastic edges and the of course the one of the interesting things.",
            "You could also work on is how to implement this for datasets that are large and that.",
            "Have been lots of instances and lots of features, so that's it.",
            "Thank you."
        ],
        [
            "Hey precious.",
            "So in your theory, you use seems because they said the capacity is the number of dimensions.",
            "It doesn't seem that you use to margin bounds as their normal useless PMS, because of course if you consider linear classifiers because it is the number of dimensions.",
            "But if you just for example stick to support vector machine capacity just depends on the margin and not the number of.",
            "Dimensions and and get permits you wouldn't work with linear, not unlike anything yet.",
            "You think I don't know, yes.",
            "So that's similar to this.",
            "If you look at marching bands, yeah, you probably could do this.",
            "The trick for the marching band is that what they have is so you are saying that it does not depend on the dimension, which is not exactly true, because what you have is you have to have it in the ball and the ball depends on the number of dimensions.",
            "So the number of dimension is in there, it's just a little bit hidden.",
            "So of course you could.",
            "Also you know modify those bounds so that they include the margin.",
            "The problem is that for me, at least for me, I use the balance to get estimates of the just a number that gives me an estimation of the capacity.",
            "And if you say that depends on the March and that's something I don't know in advance, right?",
            "So that's something I really don't want to use for quantifying the capacity.",
            "More question.",
            "Yeah, it seems to be just intuitively that the dimension of the linear classifier that uses patterns rather than features should be higher than N + 1 because there's some modeling complexity in the pattern itself also.",
            "So you have a larger, let's say, flexibility to get very strange decision boundaries when your patterns become more complex.",
            "What what's your?",
            "What's your difference between pattern feature to you then?",
            "Well.",
            "The normally the feature would just be 1 area, yeah?",
            "Process, but it's just one variable that tests whether the pattern occurs or not, so it's just one variable, right?",
            "Yes, but.",
            "I mean, in your case it's structural, but if you're better would be just the two numeric variables combined.",
            "OK, that's a different thing then.",
            "Like a weird shape, yeah?",
            "Combining those will change and get something with you far more complex there.",
            "Yes, and you will probably get other capacity estimates then if we see dimension would go up how it would go up really depends on the actual kind of pattern Cuco.",
            "So that's not something you can.",
            "Do you notice anything like that when you allow more complex patterns?",
            "That no, I didn't because my parents are basically just occurrence patterns, right?",
            "Either appears or not, so it's just one variable that is either zero or one.",
            "So in my case it's we see dimension is definitely just the number of features, But if you have more complex patterns, it's only change.",
            "I actually was the same batteries or feature.",
            "Yeah yeah.",
            "Could we refine analysis by looking at Knox whether it occurs or not?",
            "But of course it might be more difficult to estimate, especially with graph because.",
            "True, true.",
            "My can happen.",
            "Which would you suggest?",
            "Is that because you have a richer?",
            "Yes.",
            "True.",
            "OK, so thank you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's get started.",
                    "label": 0
                },
                {
                    "sent": "So this is about capacity control for partially ordered feature sets.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's a short outline of what I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "First of all, is going to be in short introduction, or give some background about the concepts I use in the motivation.",
                    "label": 0
                },
                {
                    "sent": "But this is about then the actual results are in two parts.",
                    "label": 0
                },
                {
                    "sent": "The first one is the theoretical work at it on distribution independent stuff, and also when you make some assumptions about the data generating distribution and then the second part is basically the practical part.",
                    "label": 0
                },
                {
                    "sent": "Report about the experiments with it.",
                    "label": 0
                },
                {
                    "sent": "And of course there's going to be a conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's get started.",
                    "label": 0
                },
                {
                    "sent": "The general setting I'm working on IS classification supervised machine learning.",
                    "label": 1
                },
                {
                    "sent": "This classification or regression, and in particular I'd like to work on datasets where the we have the data instances are structured objects.",
                    "label": 0
                },
                {
                    "sent": "So for instance you could have the case where the instances are molecules that have a structure given as a graph, and in order to use such datasets with propositional learning methods, what you usually do is you have to extract features you have to generate features.",
                    "label": 0
                },
                {
                    "sent": "And one popular way to do so is.",
                    "label": 1
                },
                {
                    "sent": "In that case is to have features that test for substructures.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in that particular case you could have a feature that tests whether this ethnic carbon ethnic group is present.",
                    "label": 0
                },
                {
                    "sent": "Or you could check whether an oxygen autumn is there, or you could check whether this oxygen Atom together with the carbon Atom is is present and one thing that is particularly interesting about this kind of feature representation is that the features can actually be.",
                    "label": 0
                },
                {
                    "sent": "Ordered according to partial order.",
                    "label": 0
                },
                {
                    "sent": "So whenever you know that this partial this carbon asset group is present in a molecule, then you also know that of course this oxygen atoms present and you also know that this feature is present.",
                    "label": 1
                },
                {
                    "sent": "So this gives you some sort of redundancy in the data set, right?",
                    "label": 0
                },
                {
                    "sent": "Since if you know feature one is present then you also know a couple of other features are present.",
                    "label": 0
                },
                {
                    "sent": "So one of the problems you're having here is that the datasets are by design kind of redundant and.",
                    "label": 0
                },
                {
                    "sent": "The question I was asking is how does this redundancy effect classification, linear classifiers, and in particular how does it?",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um, affect capacity control behavior and so this is just an overview.",
                    "label": 0
                },
                {
                    "sent": "What exactly is capacity control?",
                    "label": 1
                },
                {
                    "sent": "Capacity is basically the idea.",
                    "label": 0
                },
                {
                    "sent": "How large should the hypothesis class and the learner has to choose a classifier from?",
                    "label": 1
                },
                {
                    "sent": "Here's a small example.",
                    "label": 0
                },
                {
                    "sent": "We have a training set containing positive and negative examples, and if you have a learner with the let's say, a smaller, this class that only allows linear classifiers, then the best classifier you could come up with is that one over there that.",
                    "label": 0
                },
                {
                    "sent": "Cannot explain the training set correctly.",
                    "label": 0
                },
                {
                    "sent": "However, you could also have a learner with a more complex, more larger hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "Then you could maybe have a classifier that has a.",
                    "label": 0
                },
                {
                    "sent": "More.",
                    "label": 0
                },
                {
                    "sent": "Better shaped decision border that completely explains the training set.",
                    "label": 0
                },
                {
                    "sent": "However, however, in most cases you probably would stick to to that case over there because this is most of the time this is simply overfitting because you're fitting to a random noise in the in the training set, and you're not getting the actually underlying concept.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, so in this case you prob.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I would say this one is overfitting and this one is quite right.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you have a lot of data then maybe you have that much evidence for a certain really complex decision border that you could actually choose a hypothesis class that is larger, higher capacity, and that because you have enough evidence to to make sure that this is actually the right decision border.",
                    "label": 0
                },
                {
                    "sent": "And in this case just having a linear one would actually be underfitting, right?",
                    "label": 0
                },
                {
                    "sent": "So basically we're capacity of control is about if you want to have a learner that performs well, you need to learn.",
                    "label": 0
                },
                {
                    "sent": "That has to have a positive classroom, exactly the right capacity.",
                    "label": 0
                },
                {
                    "sent": "If the capacity is too large, then you're probably going to overfit.",
                    "label": 0
                },
                {
                    "sent": "If it's too loud, you're going to underfit Anne.",
                    "label": 0
                },
                {
                    "sent": "How do you quantify those things?",
                    "label": 0
                },
                {
                    "sent": "The standard approach or popular approach that is using uniform convergence bounds from computer in learning theory?",
                    "label": 0
                },
                {
                    "sent": "Basically what you do is you start with the with the hypothesis class you let the learner stuff with a fixed data generating distribution an arbitrary one, and you let the learner come up with the classifier.",
                    "label": 0
                },
                {
                    "sent": "Then what the theory tells you is with high probability it is the case that the true error of the classifier you have the true errors this thing.",
                    "label": 0
                },
                {
                    "sent": "So you're going to expect on the test data set that one is going to be smaller than the training set error, plus something that depends on the hypothesis class an.",
                    "label": 0
                },
                {
                    "sent": "So depending on what kind of bond you have, we have different parameters in there, but generally at least for linear classes.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why is what you usually have is you have to be see dimension, which happens to be just the number of features.",
                    "label": 0
                },
                {
                    "sent": "I'm pulling your classifiers so for linear classifiers in this capacity estimate term you have number of training instances and you have the number of features.",
                    "label": 1
                },
                {
                    "sent": "That means the more features you have, the higher capacity is, and that means basically that the question of finding the right capacity translates in that setting into finding the right number of features.",
                    "label": 0
                },
                {
                    "sent": "If you have millions of features, it's likely they're going to overfit.",
                    "label": 0
                },
                {
                    "sent": "If you have just a few features, it's probably going to be under fit an now.",
                    "label": 0
                },
                {
                    "sent": "Of course, the interesting question to me was.",
                    "label": 0
                },
                {
                    "sent": "I'm in our setting.",
                    "label": 1
                },
                {
                    "sent": "We also know that the features are partially ordered, so there is some redundancy in there and if you look at that, if you think about this then it should be quite obvious that the capacity with partially ordered features should actually be a little bit smaller than in the general case, and maybe we could also come up with something that qualifies the capacity depending on the actual partial order, because they are of course very impartial orders.",
                    "label": 0
                },
                {
                    "sent": "So when I looked into this the very.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1st results that I got was actually quite surprising to me because the very first result I got is that the capacity of linear classifiers with partially order features is exactly the same as the capacity of linear classifiers with arbitrary unordered features, so the capacity doesn't really change it all.",
                    "label": 1
                },
                {
                    "sent": "Basically what this result, or more precisely what this result says, is that the VC dimension remains the same, so the VC dimension of an M dimensional linear classifier is still N + 1.",
                    "label": 0
                },
                {
                    "sent": "Regardless of whether the features are partially or not, and since there are known well known results, they give up on lower bounds on the capacity estimate.",
                    "label": 0
                },
                {
                    "sent": "That means that capacity remains approximately the same thing, so that was kind of surprising, because intuitively I thought that the capacity would go down, we get smaller, and.",
                    "label": 0
                },
                {
                    "sent": "If you look at this a little bit more closely then.",
                    "label": 0
                },
                {
                    "sent": "Turns out the reason why this is actually the case, the capacity remains the same is that those these bounds over there are a rather general they have to work for all possible distributions and particularly also have to work for distributions that are really weird and kind of that.",
                    "label": 0
                },
                {
                    "sent": "Kind of burst cases, you distributions and the interesting question then, is of course do so.",
                    "label": 0
                },
                {
                    "sent": "Do those worst case distributions also appear in practice?",
                    "label": 0
                },
                {
                    "sent": "Are those distributions that are practically relevant and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When I looked at that, it turned out that I can.",
                    "label": 0
                },
                {
                    "sent": "I can say you probably will never encounter such distributions in practice.",
                    "label": 0
                },
                {
                    "sent": "At least I haven't seen it so far.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that in practice, most distributions, most data generating distributions have some properties that make them more favorable, and they allow a different kind of analysis, and in particular, most of these distributions have a property that I call an exponential decay in the level probabilities, so this is.",
                    "label": 1
                },
                {
                    "sent": "OK, so here's an explanation of what these exponential decay in level probabilities means.",
                    "label": 0
                },
                {
                    "sent": "Here's an example.",
                    "label": 0
                },
                {
                    "sent": "Assume we have again the subgroup of graph testing features.",
                    "label": 0
                },
                {
                    "sent": "So you have.",
                    "label": 0
                },
                {
                    "sent": "You can actually order them by by level.",
                    "label": 0
                },
                {
                    "sent": "So on the top you have the empty subgroup sub graph that occurs in all kinds of graphs.",
                    "label": 0
                },
                {
                    "sent": "Then you have on level one you have all subgraphs that have exactly one note.",
                    "label": 0
                },
                {
                    "sent": "So that's either carbon or oxygen Atom than a level 2.",
                    "label": 0
                },
                {
                    "sent": "You have combinations of two of them on Level 3 of combinations that contains 3 nodes, and so on.",
                    "label": 1
                },
                {
                    "sent": "So you can have lots of levels, and of course what's happening is that the number of features.",
                    "label": 0
                },
                {
                    "sent": "Increases exponentially in each level, so that's one thing.",
                    "label": 0
                },
                {
                    "sent": "The other thing you could do is you could actually look at how often such a feature can be observed in practice.",
                    "label": 0
                },
                {
                    "sent": "So if you just if you have a data generating distribution, you could.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Write down that, for instance, the the empty subgraphs occurs in all subgraphs, so this appears with probability 1.0 an.",
                    "label": 0
                },
                {
                    "sent": "Let's say this one, the two carbon atoms appears in 60% of all cases.",
                    "label": 0
                },
                {
                    "sent": "Sorry, probability of seeing that one would be 60%, and so on.",
                    "label": 0
                },
                {
                    "sent": "So you can write that down for every feature, and then you also can do is you can take the maximum value of over all these occurrence probabilities over each level.",
                    "label": 0
                },
                {
                    "sent": "So for instance.",
                    "label": 0
                },
                {
                    "sent": "In level one we.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "22 features basically the one has probability point 91 has probability .3.",
                    "label": 0
                },
                {
                    "sent": "So you take the maximum and there will be .9 over there and you do the same thing in Level 2, Level 3 and so on.",
                    "label": 0
                },
                {
                    "sent": "So with that you get for each level.",
                    "label": 0
                },
                {
                    "sent": "A number that I called the level probability and if you look at it in practice then it happens to be that this level probability in almost all cases I've seen goes really down quickly, quickly, quickly.",
                    "label": 0
                },
                {
                    "sent": "So here's an example for the entity our data set and you see it's really almost an exponential decay you have there.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Why is that important?",
                    "label": 0
                },
                {
                    "sent": "On the interesting.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think of is that you can actually rewrite or you can modify some of the uniform convergence bounds so that they work with the number of features and so that they work also with the level probabilities.",
                    "label": 0
                },
                {
                    "sent": "And here's what the bounds look like.",
                    "label": 0
                },
                {
                    "sent": "I'm the first cases for this year one loss that's very misclassification gets penalty one and a core classification gets penalty zero, and in that case the IT looks a little bit complicated, but in fact.",
                    "label": 0
                },
                {
                    "sent": "So, so this is the level probability and this is the number of features per level.",
                    "label": 0
                },
                {
                    "sent": "So what's happening there is that this capacity estimate increases with the number of features per level and decreases if the level probability it gets get smaller and you can actually see it way better if you don't use the 01 loss but the... loss of Chester continuous version that gives values between 0 and 1, two cases that are near the decision boundary and what you're having there is that what you do is you take for each level you take the number of features.",
                    "label": 0
                },
                {
                    "sent": "And then you multiply it with the with the level probability, and then you add that up for all levels.",
                    "label": 0
                },
                {
                    "sent": "That's the sum over there.",
                    "label": 0
                },
                {
                    "sent": "So this gives you an incapacity, an estimate of the capacity for linear classifiers on partially ordered features now.",
                    "label": 0
                },
                {
                    "sent": "Well, that means basically is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That if you have a little probability is decreasing rather quickly, then your capacity is smaller than it will be in the general case.",
                    "label": 0
                },
                {
                    "sent": "Right, so let's have a look at what this looks likes and some practical.",
                    "label": 0
                },
                {
                    "sent": "Settings.",
                    "label": 0
                },
                {
                    "sent": "So let us assume that the the level probability has actually exponential decay, so for every level I the Lambda I is smaller than Alpha to the power of our Alpha.",
                    "label": 1
                },
                {
                    "sent": "Being just a conservative.",
                    "label": 0
                },
                {
                    "sent": "Actor and let's have a look at some of the some of the existing partial orders you can work with.",
                    "label": 1
                },
                {
                    "sent": "One instance would be totally ordered features that this one.",
                    "label": 0
                },
                {
                    "sent": "So every feature is a refinement of the of the preceding feature and their case.",
                    "label": 0
                },
                {
                    "sent": "D is just one and what you get is a geometrical serious and that one is always smaller than 1 / 1 minus Alpha.",
                    "label": 0
                },
                {
                    "sent": "So you always have a finite rather small capacity, even though you can have large number of features.",
                    "label": 1
                },
                {
                    "sent": "That's not something you would probably see in practice, but at least it's most trick.",
                    "label": 0
                },
                {
                    "sent": "Or you can have, so might be interesting from a theoretical .4 item sets.",
                    "label": 0
                },
                {
                    "sent": "That's another nice case.",
                    "label": 0
                },
                {
                    "sent": "So in this case the features are item sets there, checking for the appearance of items an you can also order them according to the subset of partial or and in that case you give you get 2 to the power of K features, K being the number of items an if you have the.",
                    "label": 0
                },
                {
                    "sent": "Exponential decay again.",
                    "label": 0
                },
                {
                    "sent": "Then you get a capacity is 1 plus Alpha to the power of K. So that means if you have very very steep decay then Alpha will be very small, almost zero.",
                    "label": 0
                },
                {
                    "sent": "So this is really close to 1.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you have no decay at all, then this will be off will be one.",
                    "label": 0
                },
                {
                    "sent": "So you have the capacity is 2 to the power of K, which is just the same capacity as in the general case.",
                    "label": 0
                },
                {
                    "sent": "So you can really see the capacity goes between one and two to the power of.",
                    "label": 0
                },
                {
                    "sent": "Kay has really brought range depending on the level.",
                    "label": 0
                },
                {
                    "sent": "Um, the level probabilities.",
                    "label": 0
                },
                {
                    "sent": "OK here the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Responding terms for strings, trees and graphs.",
                    "label": 0
                },
                {
                    "sent": "What's interesting here is that for strings, it really depends on also on the Alpha and a number of characters in the alphabet, and if Alpha is small enough, and particularly if it's smaller than 1 / H, then this series converges, and then you also have a finite capacity even through you can have an infinite amount of features which might be interesting for some applications.",
                    "label": 0
                },
                {
                    "sent": "The trees are actually just like strings, just they just have different content in there, so they're actually quite rated, which I found quite surprising.",
                    "label": 0
                },
                {
                    "sent": "Graphs are actually very different because they also have the factorial in there.",
                    "label": 0
                },
                {
                    "sent": "That means they they sum is divergent all the time, so you always have an infinite capacity even, even though you might have very steep level decay level probability decay.",
                    "label": 0
                },
                {
                    "sent": "So this is the theory.",
                    "label": 0
                },
                {
                    "sent": "Which might be nice, or maybe not nice.",
                    "label": 0
                },
                {
                    "sent": "Of course to a practitioner would be much more interesting to see how this works.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In practice, and to get an idea of that, I did a couple of experiments, so the basic idea was the theory tells us that the capacity of linear classifiers can be way smaller than if you have partially ordered features.",
                    "label": 1
                },
                {
                    "sent": "So what that would mean?",
                    "label": 0
                },
                {
                    "sent": "In principle it would mean that overfitting is probably not that much of an issue, and underfitting might be the larger issue.",
                    "label": 0
                },
                {
                    "sent": "In such settings, an in order to to get an idea of that.",
                    "label": 0
                },
                {
                    "sent": "I had a couple of experiments on three datasets there was exactly the setting I have as an example here, so instances are labeled graphs representing compounds, and the feature tests for the occurrence of subgraphs, and I used the sub graph money tool similar to G span to generate all the features and then uses a perfect machine to learn a classifier that predicts the biological activity of the component.",
                    "label": 1
                },
                {
                    "sent": "So here's the first question is, are we going?",
                    "label": 0
                },
                {
                    "sent": "Are we having overfitting with that?",
                    "label": 0
                },
                {
                    "sent": "So what I did is I generated almost all the actually all features or non non duplicated software features.",
                    "label": 0
                },
                {
                    "sent": "And here are the all fitting curves.",
                    "label": 0
                },
                {
                    "sent": "So the blue one is the training.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You see the agreement accuracy.",
                    "label": 0
                },
                {
                    "sent": "As you can see, there is virtually no overfitting, so this seems to be consistent with theory, so there is definitely no fitting.",
                    "label": 0
                },
                {
                    "sent": "Maybe there, so the next obvious question is, well, if there's no fitting, maybe there's underfitting.",
                    "label": 0
                },
                {
                    "sent": "So in order to have a look at that, I.",
                    "label": 0
                },
                {
                    "sent": "The basic idea what I actually needed is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I needed to choose what unfitting I need a class of features feature class that has a larger capacity.",
                    "label": 1
                },
                {
                    "sent": "So now how can you increase the capacity?",
                    "label": 0
                },
                {
                    "sent": "And if you look at the theory side again and you see the capacity depends on two things.",
                    "label": 1
                },
                {
                    "sent": "First of all, the number of features per per level and the other thing being the level probabilities.",
                    "label": 0
                },
                {
                    "sent": "So what you could use, you could try to come up with features, whether the decay of the level probabilities is not that steep.",
                    "label": 0
                },
                {
                    "sent": "In order to do that.",
                    "label": 1
                },
                {
                    "sent": "My idea was to have also subgroup features that have elastic edges and elastic edge means.",
                    "label": 0
                },
                {
                    "sent": "Here's an example for such a thing with an elastic edge.",
                    "label": 0
                },
                {
                    "sent": "So what you have is you have Justin sub graph and one of the edges, particularly that one.",
                    "label": 0
                },
                {
                    "sent": "The Red One is an elastic edge, so in order to match that one to an instance like the instance here on the left doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Um, in order to match that, what you would do is you match just the non elastic parts like this part and you mention this part which happened to occur over there and over there so.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work, but I hope you can see that and then in the next step you try to match the elastic edge and the elastic edge can be matched with any path in the in the training instance.",
                    "label": 0
                },
                {
                    "sent": "So in this case it is a match because the elastic was there can be matched with this path.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pop over there, right?",
                    "label": 0
                },
                {
                    "sent": "So that's that's the idea that you have sub graphs that are more flexible because you can test for various components that just have to be connected over some part.",
                    "label": 0
                },
                {
                    "sent": "And if you modify your sub graph mining tools so that it generates these kind of features then you can also run the experiments again and it turned out that doing so had significant improvements of the particular C on a test set on two of the three.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "Datasets, in particular on the NCR and the blood brain barrier data set, and there was no change in particularly on the yesterday I said.",
                    "label": 0
                },
                {
                    "sent": "So it seems that Underfitting, apparently, at least when you look at it like this.",
                    "label": 0
                },
                {
                    "sent": "Other thing was a problem on two of the three datasets.",
                    "label": 0
                },
                {
                    "sent": "And so it was a good idea to come up with a more expressive features feature class.",
                    "label": 0
                },
                {
                    "sent": "No, so that that sounds like good advice, right?",
                    "label": 0
                },
                {
                    "sent": "If you're working this kind of field, and if you want to get more accuracy and you won't know what underfitting, you should just go out for feature feature set.",
                    "label": 0
                },
                {
                    "sent": "Plus that is more expressive and it has a level probability indicator is not too steep.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately.",
                    "label": 0
                },
                {
                    "sent": "In practice that's that advises a little bit difficult to implement because having such a feature set class means that you have tons thousand millions of.",
                    "label": 0
                },
                {
                    "sent": "Features and the big question actually then, is how to deal with.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those features that classes that are so large, and particularly if you have data sets that are large, two.",
                    "label": 0
                },
                {
                    "sent": "So for instance, I have here a data set that has 35,000 compounds and I.",
                    "label": 0
                },
                {
                    "sent": "If you have just the sequences of length four that I think there are almost over 10,000 sequences, so that's really hard to to compute in reasonable time.",
                    "label": 0
                },
                {
                    "sent": "So the question is, how can we work with that?",
                    "label": 0
                },
                {
                    "sent": "And one thing I tried was to work with an approach that I have.",
                    "label": 0
                },
                {
                    "sent": "Presented this email 2007 two years ago and the basic idea is that you don't generate all the features, you just try to generate a subset of features and basically you try to find those features that complement each other in some almost optimal way.",
                    "label": 0
                },
                {
                    "sent": "Do that, then you can actually work with such huge datasets.",
                    "label": 0
                },
                {
                    "sent": "So for instance, for sequences and graphs you need 28 and 68 minutes and elastic graphs are then only twice that, which is still manageable, right?",
                    "label": 1
                },
                {
                    "sent": "So of course there's lots of lots of other work you could do.",
                    "label": 0
                },
                {
                    "sent": "Another approaches you could use, I think that's basically an open question where you can.",
                    "label": 0
                },
                {
                    "sent": "Work with some really large feature set classes which you do need to toward underfitting and how to do that efficiently.",
                    "label": 1
                },
                {
                    "sent": "So that is the experiments, and here is the conclusion.",
                    "label": 0
                },
                {
                    "sent": "So what I just presented is basically to.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two parts, the first one is a theoretical analysis of capacity control for linear classifiers with partially or features, and there were basically 2 main results.",
                    "label": 0
                },
                {
                    "sent": "First one is that in the general case there is no change in the capacity in the.",
                    "label": 1
                },
                {
                    "sent": "In the case where you have.",
                    "label": 1
                },
                {
                    "sent": "Probably probability decline than the capacity can get considerably smaller, and in particular for item set strings trees.",
                    "label": 1
                },
                {
                    "sent": "You can even have finite capacity even with an infinite amount of features.",
                    "label": 1
                },
                {
                    "sent": "And that's not true for graphs and on the on the practical side, the experiment empirical experiments showed that there's practically no overfitting.",
                    "label": 0
                },
                {
                    "sent": "There is some underfitting on two of the three datasets an in order to avoid this underfitting.",
                    "label": 0
                },
                {
                    "sent": "It might work if you use a different.",
                    "label": 0
                },
                {
                    "sent": "Set of features.",
                    "label": 0
                },
                {
                    "sent": "In particular, you could use software features that have these elastic edges and the of course the one of the interesting things.",
                    "label": 0
                },
                {
                    "sent": "You could also work on is how to implement this for datasets that are large and that.",
                    "label": 0
                },
                {
                    "sent": "Have been lots of instances and lots of features, so that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hey precious.",
                    "label": 0
                },
                {
                    "sent": "So in your theory, you use seems because they said the capacity is the number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "It doesn't seem that you use to margin bounds as their normal useless PMS, because of course if you consider linear classifiers because it is the number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "But if you just for example stick to support vector machine capacity just depends on the margin and not the number of.",
                    "label": 0
                },
                {
                    "sent": "Dimensions and and get permits you wouldn't work with linear, not unlike anything yet.",
                    "label": 0
                },
                {
                    "sent": "You think I don't know, yes.",
                    "label": 0
                },
                {
                    "sent": "So that's similar to this.",
                    "label": 0
                },
                {
                    "sent": "If you look at marching bands, yeah, you probably could do this.",
                    "label": 0
                },
                {
                    "sent": "The trick for the marching band is that what they have is so you are saying that it does not depend on the dimension, which is not exactly true, because what you have is you have to have it in the ball and the ball depends on the number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "So the number of dimension is in there, it's just a little bit hidden.",
                    "label": 0
                },
                {
                    "sent": "So of course you could.",
                    "label": 0
                },
                {
                    "sent": "Also you know modify those bounds so that they include the margin.",
                    "label": 0
                },
                {
                    "sent": "The problem is that for me, at least for me, I use the balance to get estimates of the just a number that gives me an estimation of the capacity.",
                    "label": 0
                },
                {
                    "sent": "And if you say that depends on the March and that's something I don't know in advance, right?",
                    "label": 0
                },
                {
                    "sent": "So that's something I really don't want to use for quantifying the capacity.",
                    "label": 0
                },
                {
                    "sent": "More question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it seems to be just intuitively that the dimension of the linear classifier that uses patterns rather than features should be higher than N + 1 because there's some modeling complexity in the pattern itself also.",
                    "label": 0
                },
                {
                    "sent": "So you have a larger, let's say, flexibility to get very strange decision boundaries when your patterns become more complex.",
                    "label": 0
                },
                {
                    "sent": "What what's your?",
                    "label": 0
                },
                {
                    "sent": "What's your difference between pattern feature to you then?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "The normally the feature would just be 1 area, yeah?",
                    "label": 0
                },
                {
                    "sent": "Process, but it's just one variable that tests whether the pattern occurs or not, so it's just one variable, right?",
                    "label": 0
                },
                {
                    "sent": "Yes, but.",
                    "label": 0
                },
                {
                    "sent": "I mean, in your case it's structural, but if you're better would be just the two numeric variables combined.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a different thing then.",
                    "label": 0
                },
                {
                    "sent": "Like a weird shape, yeah?",
                    "label": 0
                },
                {
                    "sent": "Combining those will change and get something with you far more complex there.",
                    "label": 0
                },
                {
                    "sent": "Yes, and you will probably get other capacity estimates then if we see dimension would go up how it would go up really depends on the actual kind of pattern Cuco.",
                    "label": 0
                },
                {
                    "sent": "So that's not something you can.",
                    "label": 0
                },
                {
                    "sent": "Do you notice anything like that when you allow more complex patterns?",
                    "label": 0
                },
                {
                    "sent": "That no, I didn't because my parents are basically just occurrence patterns, right?",
                    "label": 0
                },
                {
                    "sent": "Either appears or not, so it's just one variable that is either zero or one.",
                    "label": 0
                },
                {
                    "sent": "So in my case it's we see dimension is definitely just the number of features, But if you have more complex patterns, it's only change.",
                    "label": 0
                },
                {
                    "sent": "I actually was the same batteries or feature.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Could we refine analysis by looking at Knox whether it occurs or not?",
                    "label": 0
                },
                {
                    "sent": "But of course it might be more difficult to estimate, especially with graph because.",
                    "label": 0
                },
                {
                    "sent": "True, true.",
                    "label": 0
                },
                {
                    "sent": "My can happen.",
                    "label": 0
                },
                {
                    "sent": "Which would you suggest?",
                    "label": 0
                },
                {
                    "sent": "Is that because you have a richer?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "True.",
                    "label": 0
                },
                {
                    "sent": "OK, so thank you again.",
                    "label": 0
                }
            ]
        }
    }
}