{
    "id": "bdolhobltm5o227gwr522kb732eczyqz",
    "title": "Testing with Kernel-based Test Statistics Power Against Sequences of Local Alternatives",
    "info": {
        "author": [
            "Zaid Harchaoui, ENST Paris"
        ],
        "published": "Feb. 25, 2008",
        "recorded": "December 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/ripd07_harchaoui_tkb/",
    "segmentation": [
        [
            "So I'm going to talk about the test, the problem of testing with kernel based test statistics and discuss the issue of the power against sequences of local alternatives."
        ],
        [
            "So first of all, I will present briefly the problem of testing for homogeneity and then introduce our channel can official discriminant analysis based statistic.",
            "And then present some theoretical results which.",
            "We justify the need for student Isation and also give some insights about the power of our test statistics against sequences of local alternatives.",
            "And finally I will present some experiments on artificial data."
        ],
        [
            "So the problem of testing for homogeneity, I guess you already know it, but just give some.",
            "Additional remarks, so we have two samples and the question is whether the pool sample may be considered homogeneous and the standard formalization is to assume that the samples were drawn from IID from probability distribution and then then the hypothesis testing problem consists in deciding between the null hypothesis which is P1 equals P2 and the alternative which is P1 different from P2."
        ],
        [
            "OK, and this has many applications ranging from meditation to speaker identification, but the main issues are how to build a nonparametric test which is able to detect arbitrary differences in support and or frequency.",
            "And there is a related work which is very close to ours, which is the maximum mean discrepancy by gritton at all."
        ],
        [
            "OK, so the kernel Fisher discriminant analysis, so let me recall it in the finite dimensional case, inconsistent finding direction which maximizes the Fisher discriminant ratio, which is the ratio between the between class variance and within class variance.",
            "And.",
            "This this algorithm may be kernelized and then the official discriminants ratio turns into the between class variance over the regularised within class variance.",
            "Because of course the within class covariance operator is rank deficient.",
            "So the regularization is mandatory."
        ],
        [
            "And then the need for theoretical analysis is justified by the fact that once we have these statistics, then we the main issue is to define appropriate thresholds for prescribed false alarm probability to make decisions between the null distribution and the alternative."
        ],
        [
            "And actually, we need to standardize this test statistic first by recentering and then by rescaling it and the rescaling and re centering parameter matches what we call the degrees of freedom or effective dimension in for smoothing splines or kind of Ridge regression.",
            "So why do we need to recenter it and rescale it?"
        ],
        [
            "The thing is that if we take a closer look to the Ken official maximum, can official discriminants ratio?",
            "Its expectation is exactly the degrees of freedom plus something which is negligible when the regularization parameter goes to zero at an appropriate rate.",
            "And the rescaling is justified by once.",
            "The maximum kernel Fisher discriminant ratio is resented.",
            "Then we have to divide it by the 2nd effect.",
            "Effective degrees of freedom to get.",
            "Conversions to normal distribution, so this is proved by using tools on perturbation of operators and multiple central limit theorems.",
            "OK, so now I give the results.",
            "So there are two main approaches to this statistics.",
            "The first one is to consider the regularization parameter fixed and the other one is to consider a decaying Ruger isation parameter and this gives birth to different regimes in the asymptotics."
        ],
        [
            "So I will I give you free some results about the non partisan then step into the results about local alternatives.",
            "So under the newly purchase for a fixed regularization same scheme.",
            "Then the test statistics converge to an infinite linear combinations of chi squared distribution which is very close to the distribution of the MMD algorithm."
        ],
        [
            "And of course, this statistic is consistent in power."
        ],
        [
            "And for a decaying graduation parameter, then we can prove that the test to systec converts to a normal distribution.",
            "If the authorization parameter goes.",
            "Faster than the zero, then an exponent 1/2.",
            "And of course it is."
        ],
        [
            "Isn't power so the main issue is that usually most you know meaningful test statistics are consistent in power, so then this kind of theoretical analysis is not relevant for comparing test statistics.",
            "And there are two main approaches for alleviating this problem, so the first one is to compare for fixed alternatives.",
            "The error exponents by larger versions, so this is not the approach we have chosen, and the other one is to consider.",
            "Tentative, which goes which go closer as N goes to Infinity and to look at the power of the test.",
            "So let me recall with the power is the one minus the probability of deciding 80 instead of H1 when the alternative is true.",
            "So then the framework is is.",
            "Is testing from testing whether the Edge 0 is true for a sequence of P1 and P2.",
            "And against HN, which is a P1 different from P2 and the two probability measures get closer as N goes to Infinity.",
            "So this that means that usually you know when you have more even more samples, then it's easier to decide in favor of the alternative.",
            "So we make artificially the problem harder in order to analyze the true ability of the test statistics to discriminate between two alternatives.",
            "And the the.",
            "We can measure the hardness of the problem by the ETA N, which measures the distance between the two probability measures.",
            "So usually what we do is to fix some direction and then to get activation of the P1 probability measure and to look into a sequence of P2 N converging to P1 along this direction.",
            "Our approach is different since we consider a ball of radius 8 N around P1 and then look into the behavior of the statistics within this ball.",
            "And we."
        ],
        [
            "Consider a particular distance which is very natural for comparing the two probability distribution, which is the Chi square divergance.",
            "And then our main assumption on either end is that an Italian square divided by the much B has to be bounded and then eaten has to go 0 faster than the."
        ],
        [
            "So then our results is like if we look into the probability distribution of the test statistics against the sequences of alternatives, which makes the hypothesis problem hypothesis testing problem harder.",
            "As in goes to Infinity, then the probability distribution is as well infinite weighted linear combination of noncentral chi squared distribution.",
            "And.",
            "Yeah, it's true so we can show that for all N distributions in terms of convergence.",
            "Yeah, so in our statement what we mean is that the correct characteristic function of the TN and the right hand term gets closer as N tends to Infinity, so it's a little bit abusive, but it's right.",
            "So so and then the noncentral parameter.",
            "Is the difference between the means, but we scale by the covariance operator of the.",
            "Of the P1 distribution."
        ],
        [
            "And if the realization parameter the case to 0, then the shift the main shift is exactly the difference in mean.",
            "But we scaled by the limits of the difference of mean rescaled by the covariance operator operator regularised.",
            "And so that means that if the alternative converge to zero at a rate which satisfies this assumption, then we are consistent.",
            "Since we have shifted mean in the in the limiting distribution under the alternatives."
        ],
        [
            "So if you look into the race, then we see that we can get if you vary the K then we can get arbitrarily close to 1 / 2 photo.",
            "The parameter to which means that we can get arbitrarily close by choosing a risation sequence.",
            "To the parameter create."
        ],
        [
            "So if you compare this results to the ones of you know.",
            "On the maximum mean discrepancy.",
            "Maximum mean discrepancy and the newly purchase conversion also to an infinite linear combination of chi squared distribution.",
            "But since it's not rescaled then the distribution under the new hypothesis depends on the eigenvalues of the covariance operator, and then it depends on the kernel.",
            "So the normalization by the covariance digger regularised within class covariance operator makes it independent of the kernel.",
            "The regularization parameter goes to zero and it also allows to detect higher components in the difference between the distribution.",
            "So this is what I will."
        ],
        [
            "Guys in the experiments, so of course for standard problems we get comperable performances.",
            "When compared to MMD for comparing laptops distribution versus normal student distribution versus less.",
            "In terms."
        ],
        [
            "Power and also foreskin mixture."
        ],
        [
            "So the main example we chose is to take the periodic smoothing spline kernel for which when we take the underlying distribution to be uniform, then we know exactly the eigenvalues eigenvectors of the covariance operator.",
            "Which are the scene new and as new?"
        ],
        [
            "And we also have a closed form express."
        ],
        [
            "For the kernel in terms of the Bernoulli polynomials.",
            "So we take as a sequence of alternatives the.",
            "The uniform density but perturbed by.",
            "A little little perturbation, which is the coziness, which is the eigenvector of new N. And then we let N goes to Infinity, and we consider the sequence of eigenvalues, which has a geometric rate to 0.",
            "And we took for the polynomial kernel do be for Bernoulli polynomial.",
            "So I just underline that in this setting then MMD has no way to choose its kernel because the kernel is fixed then."
        ],
        [
            "Our results only show that we can.",
            "Properly tune the performances of our algorithm to reach to detect differences in distribution as again Gamer gets close to 0, but MMD when the kernel is fixed is not able to detect these differences.",
            "But of course, if you vary the kernel, then you should.",
            "Gets.",
            "Very different theoretical analysis of MMD and then.",
            "So it's only an example to show that this additional regularization parameter allows us to detect higher components in the probability distributions by tuning the regularization parameter."
        ],
        [
            "And so for conclusion, we try to give quantitative comparison comparison of the power of our test statistics against standard non parameter nonparametric tests and also try extensions or other settings like media sample problems and also a. Analyze whether the approximation of the limiting distribution is.",
            "Is better when we take an infinite linear combination of chi squared random variables or normal distribution.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about the test, the problem of testing with kernel based test statistics and discuss the issue of the power against sequences of local alternatives.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, I will present briefly the problem of testing for homogeneity and then introduce our channel can official discriminant analysis based statistic.",
                    "label": 1
                },
                {
                    "sent": "And then present some theoretical results which.",
                    "label": 0
                },
                {
                    "sent": "We justify the need for student Isation and also give some insights about the power of our test statistics against sequences of local alternatives.",
                    "label": 1
                },
                {
                    "sent": "And finally I will present some experiments on artificial data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the problem of testing for homogeneity, I guess you already know it, but just give some.",
                    "label": 0
                },
                {
                    "sent": "Additional remarks, so we have two samples and the question is whether the pool sample may be considered homogeneous and the standard formalization is to assume that the samples were drawn from IID from probability distribution and then then the hypothesis testing problem consists in deciding between the null hypothesis which is P1 equals P2 and the alternative which is P1 different from P2.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and this has many applications ranging from meditation to speaker identification, but the main issues are how to build a nonparametric test which is able to detect arbitrary differences in support and or frequency.",
                    "label": 0
                },
                {
                    "sent": "And there is a related work which is very close to ours, which is the maximum mean discrepancy by gritton at all.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the kernel Fisher discriminant analysis, so let me recall it in the finite dimensional case, inconsistent finding direction which maximizes the Fisher discriminant ratio, which is the ratio between the between class variance and within class variance.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This this algorithm may be kernelized and then the official discriminants ratio turns into the between class variance over the regularised within class variance.",
                    "label": 0
                },
                {
                    "sent": "Because of course the within class covariance operator is rank deficient.",
                    "label": 0
                },
                {
                    "sent": "So the regularization is mandatory.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the need for theoretical analysis is justified by the fact that once we have these statistics, then we the main issue is to define appropriate thresholds for prescribed false alarm probability to make decisions between the null distribution and the alternative.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And actually, we need to standardize this test statistic first by recentering and then by rescaling it and the rescaling and re centering parameter matches what we call the degrees of freedom or effective dimension in for smoothing splines or kind of Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "So why do we need to recenter it and rescale it?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The thing is that if we take a closer look to the Ken official maximum, can official discriminants ratio?",
                    "label": 0
                },
                {
                    "sent": "Its expectation is exactly the degrees of freedom plus something which is negligible when the regularization parameter goes to zero at an appropriate rate.",
                    "label": 0
                },
                {
                    "sent": "And the rescaling is justified by once.",
                    "label": 0
                },
                {
                    "sent": "The maximum kernel Fisher discriminant ratio is resented.",
                    "label": 1
                },
                {
                    "sent": "Then we have to divide it by the 2nd effect.",
                    "label": 0
                },
                {
                    "sent": "Effective degrees of freedom to get.",
                    "label": 0
                },
                {
                    "sent": "Conversions to normal distribution, so this is proved by using tools on perturbation of operators and multiple central limit theorems.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I give the results.",
                    "label": 0
                },
                {
                    "sent": "So there are two main approaches to this statistics.",
                    "label": 0
                },
                {
                    "sent": "The first one is to consider the regularization parameter fixed and the other one is to consider a decaying Ruger isation parameter and this gives birth to different regimes in the asymptotics.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will I give you free some results about the non partisan then step into the results about local alternatives.",
                    "label": 1
                },
                {
                    "sent": "So under the newly purchase for a fixed regularization same scheme.",
                    "label": 1
                },
                {
                    "sent": "Then the test statistics converge to an infinite linear combinations of chi squared distribution which is very close to the distribution of the MMD algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course, this statistic is consistent in power.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for a decaying graduation parameter, then we can prove that the test to systec converts to a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "If the authorization parameter goes.",
                    "label": 0
                },
                {
                    "sent": "Faster than the zero, then an exponent 1/2.",
                    "label": 0
                },
                {
                    "sent": "And of course it is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Isn't power so the main issue is that usually most you know meaningful test statistics are consistent in power, so then this kind of theoretical analysis is not relevant for comparing test statistics.",
                    "label": 1
                },
                {
                    "sent": "And there are two main approaches for alleviating this problem, so the first one is to compare for fixed alternatives.",
                    "label": 0
                },
                {
                    "sent": "The error exponents by larger versions, so this is not the approach we have chosen, and the other one is to consider.",
                    "label": 0
                },
                {
                    "sent": "Tentative, which goes which go closer as N goes to Infinity and to look at the power of the test.",
                    "label": 0
                },
                {
                    "sent": "So let me recall with the power is the one minus the probability of deciding 80 instead of H1 when the alternative is true.",
                    "label": 0
                },
                {
                    "sent": "So then the framework is is.",
                    "label": 0
                },
                {
                    "sent": "Is testing from testing whether the Edge 0 is true for a sequence of P1 and P2.",
                    "label": 0
                },
                {
                    "sent": "And against HN, which is a P1 different from P2 and the two probability measures get closer as N goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So this that means that usually you know when you have more even more samples, then it's easier to decide in favor of the alternative.",
                    "label": 0
                },
                {
                    "sent": "So we make artificially the problem harder in order to analyze the true ability of the test statistics to discriminate between two alternatives.",
                    "label": 0
                },
                {
                    "sent": "And the the.",
                    "label": 0
                },
                {
                    "sent": "We can measure the hardness of the problem by the ETA N, which measures the distance between the two probability measures.",
                    "label": 0
                },
                {
                    "sent": "So usually what we do is to fix some direction and then to get activation of the P1 probability measure and to look into a sequence of P2 N converging to P1 along this direction.",
                    "label": 0
                },
                {
                    "sent": "Our approach is different since we consider a ball of radius 8 N around P1 and then look into the behavior of the statistics within this ball.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consider a particular distance which is very natural for comparing the two probability distribution, which is the Chi square divergance.",
                    "label": 0
                },
                {
                    "sent": "And then our main assumption on either end is that an Italian square divided by the much B has to be bounded and then eaten has to go 0 faster than the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then our results is like if we look into the probability distribution of the test statistics against the sequences of alternatives, which makes the hypothesis problem hypothesis testing problem harder.",
                    "label": 1
                },
                {
                    "sent": "As in goes to Infinity, then the probability distribution is as well infinite weighted linear combination of noncentral chi squared distribution.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's true so we can show that for all N distributions in terms of convergence.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in our statement what we mean is that the correct characteristic function of the TN and the right hand term gets closer as N tends to Infinity, so it's a little bit abusive, but it's right.",
                    "label": 0
                },
                {
                    "sent": "So so and then the noncentral parameter.",
                    "label": 0
                },
                {
                    "sent": "Is the difference between the means, but we scale by the covariance operator of the.",
                    "label": 0
                },
                {
                    "sent": "Of the P1 distribution.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if the realization parameter the case to 0, then the shift the main shift is exactly the difference in mean.",
                    "label": 0
                },
                {
                    "sent": "But we scaled by the limits of the difference of mean rescaled by the covariance operator operator regularised.",
                    "label": 0
                },
                {
                    "sent": "And so that means that if the alternative converge to zero at a rate which satisfies this assumption, then we are consistent.",
                    "label": 0
                },
                {
                    "sent": "Since we have shifted mean in the in the limiting distribution under the alternatives.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you look into the race, then we see that we can get if you vary the K then we can get arbitrarily close to 1 / 2 photo.",
                    "label": 0
                },
                {
                    "sent": "The parameter to which means that we can get arbitrarily close by choosing a risation sequence.",
                    "label": 0
                },
                {
                    "sent": "To the parameter create.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you compare this results to the ones of you know.",
                    "label": 0
                },
                {
                    "sent": "On the maximum mean discrepancy.",
                    "label": 1
                },
                {
                    "sent": "Maximum mean discrepancy and the newly purchase conversion also to an infinite linear combination of chi squared distribution.",
                    "label": 0
                },
                {
                    "sent": "But since it's not rescaled then the distribution under the new hypothesis depends on the eigenvalues of the covariance operator, and then it depends on the kernel.",
                    "label": 0
                },
                {
                    "sent": "So the normalization by the covariance digger regularised within class covariance operator makes it independent of the kernel.",
                    "label": 1
                },
                {
                    "sent": "The regularization parameter goes to zero and it also allows to detect higher components in the difference between the distribution.",
                    "label": 1
                },
                {
                    "sent": "So this is what I will.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Guys in the experiments, so of course for standard problems we get comperable performances.",
                    "label": 0
                },
                {
                    "sent": "When compared to MMD for comparing laptops distribution versus normal student distribution versus less.",
                    "label": 1
                },
                {
                    "sent": "In terms.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Power and also foreskin mixture.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the main example we chose is to take the periodic smoothing spline kernel for which when we take the underlying distribution to be uniform, then we know exactly the eigenvalues eigenvectors of the covariance operator.",
                    "label": 0
                },
                {
                    "sent": "Which are the scene new and as new?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also have a closed form express.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the kernel in terms of the Bernoulli polynomials.",
                    "label": 0
                },
                {
                    "sent": "So we take as a sequence of alternatives the.",
                    "label": 0
                },
                {
                    "sent": "The uniform density but perturbed by.",
                    "label": 0
                },
                {
                    "sent": "A little little perturbation, which is the coziness, which is the eigenvector of new N. And then we let N goes to Infinity, and we consider the sequence of eigenvalues, which has a geometric rate to 0.",
                    "label": 0
                },
                {
                    "sent": "And we took for the polynomial kernel do be for Bernoulli polynomial.",
                    "label": 0
                },
                {
                    "sent": "So I just underline that in this setting then MMD has no way to choose its kernel because the kernel is fixed then.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our results only show that we can.",
                    "label": 0
                },
                {
                    "sent": "Properly tune the performances of our algorithm to reach to detect differences in distribution as again Gamer gets close to 0, but MMD when the kernel is fixed is not able to detect these differences.",
                    "label": 0
                },
                {
                    "sent": "But of course, if you vary the kernel, then you should.",
                    "label": 0
                },
                {
                    "sent": "Gets.",
                    "label": 0
                },
                {
                    "sent": "Very different theoretical analysis of MMD and then.",
                    "label": 0
                },
                {
                    "sent": "So it's only an example to show that this additional regularization parameter allows us to detect higher components in the probability distributions by tuning the regularization parameter.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so for conclusion, we try to give quantitative comparison comparison of the power of our test statistics against standard non parameter nonparametric tests and also try extensions or other settings like media sample problems and also a. Analyze whether the approximation of the limiting distribution is.",
                    "label": 0
                },
                {
                    "sent": "Is better when we take an infinite linear combination of chi squared random variables or normal distribution.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}