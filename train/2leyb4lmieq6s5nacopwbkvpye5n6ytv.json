{
    "id": "2leyb4lmieq6s5nacopwbkvpye5n6ytv",
    "title": "Extending Tables with Data from over a Million Websites",
    "info": {
        "author": [
            "Christian Bizer, Department of Information Systems, Freie Universit\u00e4t Berlin"
        ],
        "published": "Dec. 19, 2014",
        "recorded": "October 2014",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2014_bizer_extending_tables/",
    "segmentation": [
        [
            "Yes hello, the work I'm presenting is going to erase a similar direction as high cost work who's basically extending tables with data from the link data cloud.",
            "What we are doing is trying to go beyond that and extend tables with data from a really large number of sources.",
            "So we want to extend tables with data from millions of different websites.",
            "Do."
        ],
        [
            "This of course we can't restrict ourselves to link data becausw.",
            "There aren't millions of websites offering link data, so basically we try to exploit all different types of structured data that we find on the web that."
        ],
        [
            "Operations that we want to implement the first operation.",
            "Given you have a local table about an arbitrary topic and you want to extend it with additional attribute and you know the name of this attribute, you type in the name and the system searches for tables.",
            "Searches for data integrates these data and generates an additional attribute.",
            "That's the first operation."
        ],
        [
            "The second operation is more exploitive for use cases where you don't know the attributes that you want to add.",
            "For instance, your job is to explain why unemployment in these French regions is high or low.",
            "You might come up with some factors that might influence unemployment, but damn, it might be other factors you don't think about for."
        ],
        [
            "These use cases we provide the second operation, which is basically extends the table with as many attributes as possible, so all attributes at hundreds of attributes.",
            "All attributes that you can fill until a certain density threshold, and then you can use, for instance, correlation analysis to find out which attributes are the important ones."
        ],
        [
            "Let's look at the types of data we are using.",
            "On the one hand side we use Microdata, which is mostly schema org data.",
            "On the other side we use classic HTML tables.",
            "We use use or link data.",
            "We can get our hands on and we also use a set of tables from Wikipedia."
        ],
        [
            "Bit more details about the data as the big Data Set Challenge was the billion triple change before.",
            "Of course we use the billion triples and data set covering all topics that are usually covered by linked data."
        ],
        [
            "We use the Web data Commons Microdata corpus, which is a microdata corpus that we've extracted from the 2013 version of the common Crawl, which is a public web crawl consisting of two billion HTML pages.",
            "As this data covers products, reviews organization, local businesses, and events."
        ],
        [
            "SM work it was at Google has shown a couple of years ago around 1% of all HTML tables on the web are nice data tables.",
            "99% of these tables are Crip, so they're lay our tables or they contain whatever.",
            "So we also build a classifier to.",
            "We went through the common crawl as well, which contains 11 billion tables.",
            "We build a classifier that.",
            "Extract the data tables, giving us a corpus of a worldwide corpus of 147 million HTML tables that contain data.",
            "We use a subset, the English subset of this table corpus for our experiments."
        ],
        [
            "Here's some examples from this HTML table corpus.",
            "So for instance.",
            "3.7 million tables contain the price attribute.",
            "1.2 million tables contain and location attribute.",
            "If we look at the subject columns sings the tables are about, we see that.",
            "135 thousand rows in different tables describes the US or 42,000 rows in different tables.",
            "Just describe Greece for more exotic concepts like our former goalkeeper from the German national soccer team, Oliver.",
            "Can we find 700 tables that contain a row about Oliver Kahn?"
        ],
        [
            "Yeah, we also use a Wikipedia table corpus.",
            "That's not the infoboxes, but all the other tables.",
            "This corpus was extracted by Northwestern University using Wikipedia version from 2013."
        ],
        [
            "Yeah.",
            "So what do we do with all this data?",
            "We use a specific data model to internally represent it.",
            "We use entity attributes, tables or tabular structure, where we assume that each entity is described by one row, and we assume that each entity has a name.",
            "The name column Rechaza subject column and for all the web tables all in data.",
            "We assume that the tables contain names.",
            "For HTML tables to use, a simple myristic.",
            "Basically we take the most unique string column.",
            "If there are tires, we take the leftmost column.",
            "Research by Google has shown that this works in around 93% of all cases as most HTML table have these human readable subject columns for all the link data and add a an micro data we have.",
            "We also generate tables.",
            "Basically we generate one table per class per website.",
            "So for Amazon we will generate a product table and review table which would then contain or the row for each product offered by Amazon.",
            "SSA, that's some semantic agreement.",
            "In the linked data and the micro data, we can of course exploit this.",
            "For instance, there's agreement among sites about how to name the subject column.",
            "Most people, most science use RFS labels, use fourth name, or they use some other format as our name attributes.",
            "In areas where there's agreement on common vocabularies like schema org, we of course also explored this agreement in areas, whereas knowing that there is no agreement you will see in a second we step back and apply matching approaches."
        ],
        [
            "OK, so we take all this data.",
            "We apply the additional and restrictions that we only take tables that have a certain size and we end up with altogether 36 million tables which originate from around 1.5 million different websites.",
            "And if he would translate everything into triples, we would end up with three billion triples."
        ],
        [
            "OK, now our engines which does see the extension of the tables we call it search join Engine because it basically combines search from information retrieval websites with joins from databases and what it does.",
            "It first indexes all these tables.",
            "Then on the 2nd.",
            "User presented with some input table.",
            "The system searches for for tailors in the table corpus that could potentially contain additional data about the input table.",
            "It uses the top K tables.",
            "It performs a multi joint operations or builds one huge table and then some schema matching kicks in which then consolidate the data and later the data is fused.",
            "We got."
        ],
        [
            "A bit in the details about the the engine.",
            "So for table ranking we use subject column overlap, so you some fast string similarity library on the names of the things and then rank the tables by the number of string keys that overlap."
        ],
        [
            "Let me do this multi joint.",
            "We build one very wide table."
        ],
        [
            "Using this white table, we start column matching using a combination of label based an instance based schema matching techniques.",
            "So for instance we find out that certain attributes this give us the population and 25 attributes give us some.",
            "As our information.",
            "We apply a conflict resolution rather rather simple approaches.",
            "We basically do majority vote for strings for numeric values, the user can decide if he wants average, median or if he wants values to be clustered, and then Amare majority vote helps."
        ],
        [
            "Yeah, let's look at the system so we have an online demo where you can upload an arbitrary table.",
            "Wait a couple of minutes and you will get an extended table back which contains either the columns that you want or which contains lots of additional columns with which you can can then do something."
        ],
        [
            "Some examples of this.",
            "We uploaded a bunch of names of songs in ask the system to find out who's artist of these songs is so I don't know if you can read it so.",
            "Yeah.",
            "It looks OK if you look at the number of sources that are used in order to find these artists.",
            "The system used 287 different tables."
        ],
        [
            "We also have.",
            "Down here shows statistics button which shows you the distribution of sources.",
            "To find these these artist names, the system used quite some web tables is used, some tables from DB from Wikipedia, but it also uses smaller amount of tables from the billion triple Challenge data set and smaller amount of micro data tables.",
            "You can further drill."
        ],
        [
            "Down and look at provenance statistics.",
            "For instance, you see that the BTC data mostly comes from the BBC, which publishes information about songs and artists.",
            "Wrestling data on the web.",
            "You see that the micro data comes from last FM.",
            "Cool boots and Song Star, which seem to be some music sites which use Microdata markup.",
            "Also, they're very bitter.",
            "Various Wikipedia tables are used to find these artists."
        ],
        [
            "You also did a more formal evaluation using different classes of things.",
            "Books, company countries, drugs, films and songs.",
            "Usually we get rather good results, so coverage is the number of entities for which we can find a value there.",
            "We are usually around between 80 and 95% precision is manually evaluated against Wikipedia, IMDb and Amazon.",
            "There are values are also often in the 90s.",
            "It's interesting to see for some things.",
            "Like here population of countries, our position value is significantly lower.",
            "Same here for the teams of soccer players.",
            "Reason for this is that this is our time varying attributes and we don't deal with the time dimension yet.",
            "So we should also would need to try to extract that this table refers to that point in time, but for attributes that are not time to time dependent and not to lie here for entities which are kind of head entities, the system works quite well if you ask the system for the price of carrots office near supermarket around the corner.",
            "One finds a prize, but it has potentially the chance to find suppliers.",
            "If your supermarket uses microdata markup, so.",
            "Yes."
        ],
        [
            "That's an example of the extent with many columns operation we use upload a countries table and the system adds 500 knew columns to the table.",
            "The columns are filled with data from altogether 2000 different tables.",
            "If you again look at the distribute."
        ],
        [
            "Of the sources we see a bit different picture, so most of the data on the yellow part comes from web tables.",
            "Some data comes from the BTC tables and about 14% of the data comes from Wikipedia table, so all the different sources are used again."
        ],
        [
            "To conclude, the operations that we implemented here, such join Joints bring together websites with joints seen from the database point of view.",
            "The prototype shows that we disjoint actually work for simple queries using large amounts of.",
            "Data from the web.",
            "We've tested the system with web data.",
            "Always, when we talked to industries, industry likes it not for web data but for tables that are spread around in their Internet.",
            "So a second I really big application domain would be augmentation of tables based on data that somewhere in the Internet of some companies, especially market research companies.",
            "This really seem to drown in piles of XL files or as knowing year somewhere inside the company.",
            "Somebody else did the same market research, but had not not having the slightest chance to find these XL files produced by somebody.",
            "So that's another application domain.",
            "If we put it more in the context of big data in general, big data as many of the approaches in the area they focus on data volumes, so they're able to do the deal with them.",
            "Tremendous amounts of data.",
            "They focus on, velocity of data changes, the focus of our application armazi overlooked vis of big data.",
            "So data variety.",
            "We deal with the heterogeneity that exists on the web between millions of data sources and we can deal with data veracity.",
            "So the fact that the web usually doesn't agree on a single fact fact by doing voting between the values that we get from 50 from hundred sources for a specific topic we.",
            "Basically, you filter out the majority opinion about a topic which can be the right thing, but of course for other topics can also could also lead to problems.",
            "Yes, that's it.",
            "I had two questions.",
            "The first was you mentioned that the first column has to be named entity.",
            "Yeah, and I wondered if you were taking a narrow view of this or a broadview of this.",
            "Could it be, for example, a noun that denotes a disease or a language?",
            "And the second question would be if you find a potential value in one insert more than one table.",
            "How do you decide the values are different, like maybe for population of the city?",
            "How do you decide?",
            "Which one to take?",
            "OK, so first question.",
            "We take a rather broad volume.",
            "So basically uses her Ristic to determine the subject columns, and then we consider everything.",
            "A subject that's in this columns basically going for the low hanging fruit.",
            "Obvious problem.",
            "For instance, for people there could be a first name and the last name column.",
            "The.",
            "Empirical results show that in HTML tables is seemed to be that.",
            "Most people are named with a single name column.",
            "Oh, so this problem that could theoretically happen in the wild doesn't happen too often, or at least happens.",
            "There are always enough tables that fit these strings.",
            "We're actually's name from the input table.",
            "We are actually looking for.",
            "Second question the.",
            "How we choose values of the data Fusion part we currently implement rather simply simple approach.",
            "Basically doing voting 'em.",
            "For strings or some media and majority Mahtomedi and or clustering and vote for numeric values.",
            "Of course you can think about more sophisticated approaches.",
            "For instance, we get our data from the common crawl, so we also have siblings besides the pages.",
            "So for instance, we could rank pages using Pagerank can assign different trust rewards to the different values based on page rank.",
            "Or we could go down the direction of.",
            "Being better on the time dimension.",
            "Which then would also kind of influence our results because we would be able to better filter out historic data.",
            "So.",
            "Just I would say the system shows that things are feasible.",
            "That's a good point.",
            "It also opens up various avenues where you could still improve it and it basically goes for.",
            "Goes for the low hanging fruits, so we just exploit the fact that, for instance, for populations more websites publish the current population of a place, then publish some historic populations.",
            "How do you represent same as link between entities?",
            "That's actually a good point.",
            "We currently don't exploit same as links.",
            "As like we exploit existing.",
            "As the the reason for it is rather cool.",
            "The reason is that.",
            "There isn't that matching data out there.",
            "So if you'd like to get stuff on me to say this, but actually, if we if we look at the amount of data sources, she go into the millions.",
            "As if we have an empirical paper tomorrow where we we really did across the web data cloud and we end up with thousand data sources.",
            "OK, some of the data sources are really high quality sources.",
            "The Pedia, UK government data and so on.",
            "But if you take what's this system that you take serves a broader view and.",
            "You start working with millions of sources.",
            "Link data place.",
            "Sad to say, minor roller know.",
            "But are you alright?",
            "Sean and I can't really be basically do string matching.",
            "Of course, any kind of.",
            "The system could be extended with better types of identity resolution, high risks for calculating links, and the system should also explore it out.",
            "Some slings where people publish stuff on the web.",
            "One so from the evaluation the result looks almost perfect.",
            "I mean something like 90% some 'cause you got 100%, but if I get it right, I think your methods does not seem to be always ambiguity.",
            "So is it not a problem in this task?",
            "Can you comment on that?",
            "Note it's basically.",
            "The like.",
            "We rank the tables by key overlap.",
            "So basically the Co occurrence of the entities on the web.",
            "Partly, does a disambiguation for us because there are more tables on the web where I don't know.",
            "Paris, France, London and Berlin, Germany, Co occur, then tables, where London Paris and Berlin, Minnesota coworkers assist as part of the immigration.",
            "As a part of the Disambig, not of the disambiguation, but as a problem that we partly get things wrong, is later fixed by the voting mechanism.",
            "So it's basically the the big.",
            "The big data aspect of the singer.",
            "Once you have enough info data, you can be rather relaxed on many problems.",
            "That if you only have three data sources, you need to put Raza a lot of intelligence into it.",
            "Hi I have another question which is related to the data Fusion problem.",
            "So is your system aware of if you have numerical values and the values depend on the units in which?",
            "Our ideal with euros and dollars.",
            "Let's say our system is a rather large library of.",
            "Basically, rules for recognizing units of measurements and for normalizing them.",
            "So we use the same libraries that we use for DB pedia.",
            "So we recognize several hundred types of units of measurements and we normalize them.",
            "And you recognize this from the titles of the tables.",
            "Yes, title of the tables are invalid, like values inside the cells."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes hello, the work I'm presenting is going to erase a similar direction as high cost work who's basically extending tables with data from the link data cloud.",
                    "label": 1
                },
                {
                    "sent": "What we are doing is trying to go beyond that and extend tables with data from a really large number of sources.",
                    "label": 0
                },
                {
                    "sent": "So we want to extend tables with data from millions of different websites.",
                    "label": 0
                },
                {
                    "sent": "Do.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This of course we can't restrict ourselves to link data becausw.",
                    "label": 0
                },
                {
                    "sent": "There aren't millions of websites offering link data, so basically we try to exploit all different types of structured data that we find on the web that.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Operations that we want to implement the first operation.",
                    "label": 0
                },
                {
                    "sent": "Given you have a local table about an arbitrary topic and you want to extend it with additional attribute and you know the name of this attribute, you type in the name and the system searches for tables.",
                    "label": 1
                },
                {
                    "sent": "Searches for data integrates these data and generates an additional attribute.",
                    "label": 0
                },
                {
                    "sent": "That's the first operation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second operation is more exploitive for use cases where you don't know the attributes that you want to add.",
                    "label": 0
                },
                {
                    "sent": "For instance, your job is to explain why unemployment in these French regions is high or low.",
                    "label": 0
                },
                {
                    "sent": "You might come up with some factors that might influence unemployment, but damn, it might be other factors you don't think about for.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These use cases we provide the second operation, which is basically extends the table with as many attributes as possible, so all attributes at hundreds of attributes.",
                    "label": 0
                },
                {
                    "sent": "All attributes that you can fill until a certain density threshold, and then you can use, for instance, correlation analysis to find out which attributes are the important ones.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's look at the types of data we are using.",
                    "label": 0
                },
                {
                    "sent": "On the one hand side we use Microdata, which is mostly schema org data.",
                    "label": 0
                },
                {
                    "sent": "On the other side we use classic HTML tables.",
                    "label": 1
                },
                {
                    "sent": "We use use or link data.",
                    "label": 0
                },
                {
                    "sent": "We can get our hands on and we also use a set of tables from Wikipedia.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bit more details about the data as the big Data Set Challenge was the billion triple change before.",
                    "label": 0
                },
                {
                    "sent": "Of course we use the billion triples and data set covering all topics that are usually covered by linked data.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use the Web data Commons Microdata corpus, which is a microdata corpus that we've extracted from the 2013 version of the common Crawl, which is a public web crawl consisting of two billion HTML pages.",
                    "label": 0
                },
                {
                    "sent": "As this data covers products, reviews organization, local businesses, and events.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "SM work it was at Google has shown a couple of years ago around 1% of all HTML tables on the web are nice data tables.",
                    "label": 1
                },
                {
                    "sent": "99% of these tables are Crip, so they're lay our tables or they contain whatever.",
                    "label": 0
                },
                {
                    "sent": "So we also build a classifier to.",
                    "label": 0
                },
                {
                    "sent": "We went through the common crawl as well, which contains 11 billion tables.",
                    "label": 0
                },
                {
                    "sent": "We build a classifier that.",
                    "label": 0
                },
                {
                    "sent": "Extract the data tables, giving us a corpus of a worldwide corpus of 147 million HTML tables that contain data.",
                    "label": 0
                },
                {
                    "sent": "We use a subset, the English subset of this table corpus for our experiments.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's some examples from this HTML table corpus.",
                    "label": 0
                },
                {
                    "sent": "So for instance.",
                    "label": 0
                },
                {
                    "sent": "3.7 million tables contain the price attribute.",
                    "label": 0
                },
                {
                    "sent": "1.2 million tables contain and location attribute.",
                    "label": 0
                },
                {
                    "sent": "If we look at the subject columns sings the tables are about, we see that.",
                    "label": 0
                },
                {
                    "sent": "135 thousand rows in different tables describes the US or 42,000 rows in different tables.",
                    "label": 0
                },
                {
                    "sent": "Just describe Greece for more exotic concepts like our former goalkeeper from the German national soccer team, Oliver.",
                    "label": 0
                },
                {
                    "sent": "Can we find 700 tables that contain a row about Oliver Kahn?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, we also use a Wikipedia table corpus.",
                    "label": 0
                },
                {
                    "sent": "That's not the infoboxes, but all the other tables.",
                    "label": 0
                },
                {
                    "sent": "This corpus was extracted by Northwestern University using Wikipedia version from 2013.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So what do we do with all this data?",
                    "label": 0
                },
                {
                    "sent": "We use a specific data model to internally represent it.",
                    "label": 0
                },
                {
                    "sent": "We use entity attributes, tables or tabular structure, where we assume that each entity is described by one row, and we assume that each entity has a name.",
                    "label": 0
                },
                {
                    "sent": "The name column Rechaza subject column and for all the web tables all in data.",
                    "label": 0
                },
                {
                    "sent": "We assume that the tables contain names.",
                    "label": 0
                },
                {
                    "sent": "For HTML tables to use, a simple myristic.",
                    "label": 1
                },
                {
                    "sent": "Basically we take the most unique string column.",
                    "label": 1
                },
                {
                    "sent": "If there are tires, we take the leftmost column.",
                    "label": 0
                },
                {
                    "sent": "Research by Google has shown that this works in around 93% of all cases as most HTML table have these human readable subject columns for all the link data and add a an micro data we have.",
                    "label": 0
                },
                {
                    "sent": "We also generate tables.",
                    "label": 0
                },
                {
                    "sent": "Basically we generate one table per class per website.",
                    "label": 1
                },
                {
                    "sent": "So for Amazon we will generate a product table and review table which would then contain or the row for each product offered by Amazon.",
                    "label": 1
                },
                {
                    "sent": "SSA, that's some semantic agreement.",
                    "label": 0
                },
                {
                    "sent": "In the linked data and the micro data, we can of course exploit this.",
                    "label": 0
                },
                {
                    "sent": "For instance, there's agreement among sites about how to name the subject column.",
                    "label": 0
                },
                {
                    "sent": "Most people, most science use RFS labels, use fourth name, or they use some other format as our name attributes.",
                    "label": 0
                },
                {
                    "sent": "In areas where there's agreement on common vocabularies like schema org, we of course also explored this agreement in areas, whereas knowing that there is no agreement you will see in a second we step back and apply matching approaches.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we take all this data.",
                    "label": 0
                },
                {
                    "sent": "We apply the additional and restrictions that we only take tables that have a certain size and we end up with altogether 36 million tables which originate from around 1.5 million different websites.",
                    "label": 0
                },
                {
                    "sent": "And if he would translate everything into triples, we would end up with three billion triples.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now our engines which does see the extension of the tables we call it search join Engine because it basically combines search from information retrieval websites with joins from databases and what it does.",
                    "label": 0
                },
                {
                    "sent": "It first indexes all these tables.",
                    "label": 0
                },
                {
                    "sent": "Then on the 2nd.",
                    "label": 0
                },
                {
                    "sent": "User presented with some input table.",
                    "label": 0
                },
                {
                    "sent": "The system searches for for tailors in the table corpus that could potentially contain additional data about the input table.",
                    "label": 0
                },
                {
                    "sent": "It uses the top K tables.",
                    "label": 0
                },
                {
                    "sent": "It performs a multi joint operations or builds one huge table and then some schema matching kicks in which then consolidate the data and later the data is fused.",
                    "label": 0
                },
                {
                    "sent": "We got.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bit in the details about the the engine.",
                    "label": 0
                },
                {
                    "sent": "So for table ranking we use subject column overlap, so you some fast string similarity library on the names of the things and then rank the tables by the number of string keys that overlap.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me do this multi joint.",
                    "label": 0
                },
                {
                    "sent": "We build one very wide table.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using this white table, we start column matching using a combination of label based an instance based schema matching techniques.",
                    "label": 1
                },
                {
                    "sent": "So for instance we find out that certain attributes this give us the population and 25 attributes give us some.",
                    "label": 0
                },
                {
                    "sent": "As our information.",
                    "label": 1
                },
                {
                    "sent": "We apply a conflict resolution rather rather simple approaches.",
                    "label": 1
                },
                {
                    "sent": "We basically do majority vote for strings for numeric values, the user can decide if he wants average, median or if he wants values to be clustered, and then Amare majority vote helps.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, let's look at the system so we have an online demo where you can upload an arbitrary table.",
                    "label": 0
                },
                {
                    "sent": "Wait a couple of minutes and you will get an extended table back which contains either the columns that you want or which contains lots of additional columns with which you can can then do something.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some examples of this.",
                    "label": 0
                },
                {
                    "sent": "We uploaded a bunch of names of songs in ask the system to find out who's artist of these songs is so I don't know if you can read it so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It looks OK if you look at the number of sources that are used in order to find these artists.",
                    "label": 0
                },
                {
                    "sent": "The system used 287 different tables.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also have.",
                    "label": 0
                },
                {
                    "sent": "Down here shows statistics button which shows you the distribution of sources.",
                    "label": 0
                },
                {
                    "sent": "To find these these artist names, the system used quite some web tables is used, some tables from DB from Wikipedia, but it also uses smaller amount of tables from the billion triple Challenge data set and smaller amount of micro data tables.",
                    "label": 0
                },
                {
                    "sent": "You can further drill.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Down and look at provenance statistics.",
                    "label": 0
                },
                {
                    "sent": "For instance, you see that the BTC data mostly comes from the BBC, which publishes information about songs and artists.",
                    "label": 0
                },
                {
                    "sent": "Wrestling data on the web.",
                    "label": 0
                },
                {
                    "sent": "You see that the micro data comes from last FM.",
                    "label": 0
                },
                {
                    "sent": "Cool boots and Song Star, which seem to be some music sites which use Microdata markup.",
                    "label": 0
                },
                {
                    "sent": "Also, they're very bitter.",
                    "label": 0
                },
                {
                    "sent": "Various Wikipedia tables are used to find these artists.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You also did a more formal evaluation using different classes of things.",
                    "label": 0
                },
                {
                    "sent": "Books, company countries, drugs, films and songs.",
                    "label": 0
                },
                {
                    "sent": "Usually we get rather good results, so coverage is the number of entities for which we can find a value there.",
                    "label": 1
                },
                {
                    "sent": "We are usually around between 80 and 95% precision is manually evaluated against Wikipedia, IMDb and Amazon.",
                    "label": 0
                },
                {
                    "sent": "There are values are also often in the 90s.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to see for some things.",
                    "label": 0
                },
                {
                    "sent": "Like here population of countries, our position value is significantly lower.",
                    "label": 0
                },
                {
                    "sent": "Same here for the teams of soccer players.",
                    "label": 0
                },
                {
                    "sent": "Reason for this is that this is our time varying attributes and we don't deal with the time dimension yet.",
                    "label": 0
                },
                {
                    "sent": "So we should also would need to try to extract that this table refers to that point in time, but for attributes that are not time to time dependent and not to lie here for entities which are kind of head entities, the system works quite well if you ask the system for the price of carrots office near supermarket around the corner.",
                    "label": 0
                },
                {
                    "sent": "One finds a prize, but it has potentially the chance to find suppliers.",
                    "label": 0
                },
                {
                    "sent": "If your supermarket uses microdata markup, so.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's an example of the extent with many columns operation we use upload a countries table and the system adds 500 knew columns to the table.",
                    "label": 0
                },
                {
                    "sent": "The columns are filled with data from altogether 2000 different tables.",
                    "label": 1
                },
                {
                    "sent": "If you again look at the distribute.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the sources we see a bit different picture, so most of the data on the yellow part comes from web tables.",
                    "label": 0
                },
                {
                    "sent": "Some data comes from the BTC tables and about 14% of the data comes from Wikipedia table, so all the different sources are used again.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To conclude, the operations that we implemented here, such join Joints bring together websites with joints seen from the database point of view.",
                    "label": 0
                },
                {
                    "sent": "The prototype shows that we disjoint actually work for simple queries using large amounts of.",
                    "label": 1
                },
                {
                    "sent": "Data from the web.",
                    "label": 0
                },
                {
                    "sent": "We've tested the system with web data.",
                    "label": 0
                },
                {
                    "sent": "Always, when we talked to industries, industry likes it not for web data but for tables that are spread around in their Internet.",
                    "label": 0
                },
                {
                    "sent": "So a second I really big application domain would be augmentation of tables based on data that somewhere in the Internet of some companies, especially market research companies.",
                    "label": 0
                },
                {
                    "sent": "This really seem to drown in piles of XL files or as knowing year somewhere inside the company.",
                    "label": 0
                },
                {
                    "sent": "Somebody else did the same market research, but had not not having the slightest chance to find these XL files produced by somebody.",
                    "label": 0
                },
                {
                    "sent": "So that's another application domain.",
                    "label": 0
                },
                {
                    "sent": "If we put it more in the context of big data in general, big data as many of the approaches in the area they focus on data volumes, so they're able to do the deal with them.",
                    "label": 0
                },
                {
                    "sent": "Tremendous amounts of data.",
                    "label": 0
                },
                {
                    "sent": "They focus on, velocity of data changes, the focus of our application armazi overlooked vis of big data.",
                    "label": 0
                },
                {
                    "sent": "So data variety.",
                    "label": 0
                },
                {
                    "sent": "We deal with the heterogeneity that exists on the web between millions of data sources and we can deal with data veracity.",
                    "label": 0
                },
                {
                    "sent": "So the fact that the web usually doesn't agree on a single fact fact by doing voting between the values that we get from 50 from hundred sources for a specific topic we.",
                    "label": 0
                },
                {
                    "sent": "Basically, you filter out the majority opinion about a topic which can be the right thing, but of course for other topics can also could also lead to problems.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's it.",
                    "label": 0
                },
                {
                    "sent": "I had two questions.",
                    "label": 0
                },
                {
                    "sent": "The first was you mentioned that the first column has to be named entity.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and I wondered if you were taking a narrow view of this or a broadview of this.",
                    "label": 0
                },
                {
                    "sent": "Could it be, for example, a noun that denotes a disease or a language?",
                    "label": 0
                },
                {
                    "sent": "And the second question would be if you find a potential value in one insert more than one table.",
                    "label": 0
                },
                {
                    "sent": "How do you decide the values are different, like maybe for population of the city?",
                    "label": 0
                },
                {
                    "sent": "How do you decide?",
                    "label": 0
                },
                {
                    "sent": "Which one to take?",
                    "label": 0
                },
                {
                    "sent": "OK, so first question.",
                    "label": 0
                },
                {
                    "sent": "We take a rather broad volume.",
                    "label": 0
                },
                {
                    "sent": "So basically uses her Ristic to determine the subject columns, and then we consider everything.",
                    "label": 0
                },
                {
                    "sent": "A subject that's in this columns basically going for the low hanging fruit.",
                    "label": 0
                },
                {
                    "sent": "Obvious problem.",
                    "label": 0
                },
                {
                    "sent": "For instance, for people there could be a first name and the last name column.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Empirical results show that in HTML tables is seemed to be that.",
                    "label": 0
                },
                {
                    "sent": "Most people are named with a single name column.",
                    "label": 0
                },
                {
                    "sent": "Oh, so this problem that could theoretically happen in the wild doesn't happen too often, or at least happens.",
                    "label": 0
                },
                {
                    "sent": "There are always enough tables that fit these strings.",
                    "label": 0
                },
                {
                    "sent": "We're actually's name from the input table.",
                    "label": 0
                },
                {
                    "sent": "We are actually looking for.",
                    "label": 0
                },
                {
                    "sent": "Second question the.",
                    "label": 0
                },
                {
                    "sent": "How we choose values of the data Fusion part we currently implement rather simply simple approach.",
                    "label": 0
                },
                {
                    "sent": "Basically doing voting 'em.",
                    "label": 0
                },
                {
                    "sent": "For strings or some media and majority Mahtomedi and or clustering and vote for numeric values.",
                    "label": 0
                },
                {
                    "sent": "Of course you can think about more sophisticated approaches.",
                    "label": 0
                },
                {
                    "sent": "For instance, we get our data from the common crawl, so we also have siblings besides the pages.",
                    "label": 0
                },
                {
                    "sent": "So for instance, we could rank pages using Pagerank can assign different trust rewards to the different values based on page rank.",
                    "label": 0
                },
                {
                    "sent": "Or we could go down the direction of.",
                    "label": 0
                },
                {
                    "sent": "Being better on the time dimension.",
                    "label": 0
                },
                {
                    "sent": "Which then would also kind of influence our results because we would be able to better filter out historic data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just I would say the system shows that things are feasible.",
                    "label": 0
                },
                {
                    "sent": "That's a good point.",
                    "label": 0
                },
                {
                    "sent": "It also opens up various avenues where you could still improve it and it basically goes for.",
                    "label": 0
                },
                {
                    "sent": "Goes for the low hanging fruits, so we just exploit the fact that, for instance, for populations more websites publish the current population of a place, then publish some historic populations.",
                    "label": 0
                },
                {
                    "sent": "How do you represent same as link between entities?",
                    "label": 0
                },
                {
                    "sent": "That's actually a good point.",
                    "label": 0
                },
                {
                    "sent": "We currently don't exploit same as links.",
                    "label": 0
                },
                {
                    "sent": "As like we exploit existing.",
                    "label": 0
                },
                {
                    "sent": "As the the reason for it is rather cool.",
                    "label": 0
                },
                {
                    "sent": "The reason is that.",
                    "label": 0
                },
                {
                    "sent": "There isn't that matching data out there.",
                    "label": 0
                },
                {
                    "sent": "So if you'd like to get stuff on me to say this, but actually, if we if we look at the amount of data sources, she go into the millions.",
                    "label": 0
                },
                {
                    "sent": "As if we have an empirical paper tomorrow where we we really did across the web data cloud and we end up with thousand data sources.",
                    "label": 0
                },
                {
                    "sent": "OK, some of the data sources are really high quality sources.",
                    "label": 0
                },
                {
                    "sent": "The Pedia, UK government data and so on.",
                    "label": 0
                },
                {
                    "sent": "But if you take what's this system that you take serves a broader view and.",
                    "label": 0
                },
                {
                    "sent": "You start working with millions of sources.",
                    "label": 0
                },
                {
                    "sent": "Link data place.",
                    "label": 0
                },
                {
                    "sent": "Sad to say, minor roller know.",
                    "label": 0
                },
                {
                    "sent": "But are you alright?",
                    "label": 0
                },
                {
                    "sent": "Sean and I can't really be basically do string matching.",
                    "label": 0
                },
                {
                    "sent": "Of course, any kind of.",
                    "label": 0
                },
                {
                    "sent": "The system could be extended with better types of identity resolution, high risks for calculating links, and the system should also explore it out.",
                    "label": 0
                },
                {
                    "sent": "Some slings where people publish stuff on the web.",
                    "label": 0
                },
                {
                    "sent": "One so from the evaluation the result looks almost perfect.",
                    "label": 0
                },
                {
                    "sent": "I mean something like 90% some 'cause you got 100%, but if I get it right, I think your methods does not seem to be always ambiguity.",
                    "label": 0
                },
                {
                    "sent": "So is it not a problem in this task?",
                    "label": 0
                },
                {
                    "sent": "Can you comment on that?",
                    "label": 0
                },
                {
                    "sent": "Note it's basically.",
                    "label": 0
                },
                {
                    "sent": "The like.",
                    "label": 0
                },
                {
                    "sent": "We rank the tables by key overlap.",
                    "label": 0
                },
                {
                    "sent": "So basically the Co occurrence of the entities on the web.",
                    "label": 0
                },
                {
                    "sent": "Partly, does a disambiguation for us because there are more tables on the web where I don't know.",
                    "label": 0
                },
                {
                    "sent": "Paris, France, London and Berlin, Germany, Co occur, then tables, where London Paris and Berlin, Minnesota coworkers assist as part of the immigration.",
                    "label": 0
                },
                {
                    "sent": "As a part of the Disambig, not of the disambiguation, but as a problem that we partly get things wrong, is later fixed by the voting mechanism.",
                    "label": 0
                },
                {
                    "sent": "So it's basically the the big.",
                    "label": 0
                },
                {
                    "sent": "The big data aspect of the singer.",
                    "label": 0
                },
                {
                    "sent": "Once you have enough info data, you can be rather relaxed on many problems.",
                    "label": 0
                },
                {
                    "sent": "That if you only have three data sources, you need to put Raza a lot of intelligence into it.",
                    "label": 0
                },
                {
                    "sent": "Hi I have another question which is related to the data Fusion problem.",
                    "label": 0
                },
                {
                    "sent": "So is your system aware of if you have numerical values and the values depend on the units in which?",
                    "label": 0
                },
                {
                    "sent": "Our ideal with euros and dollars.",
                    "label": 0
                },
                {
                    "sent": "Let's say our system is a rather large library of.",
                    "label": 0
                },
                {
                    "sent": "Basically, rules for recognizing units of measurements and for normalizing them.",
                    "label": 0
                },
                {
                    "sent": "So we use the same libraries that we use for DB pedia.",
                    "label": 0
                },
                {
                    "sent": "So we recognize several hundred types of units of measurements and we normalize them.",
                    "label": 0
                },
                {
                    "sent": "And you recognize this from the titles of the tables.",
                    "label": 0
                },
                {
                    "sent": "Yes, title of the tables are invalid, like values inside the cells.",
                    "label": 0
                }
            ]
        }
    }
}