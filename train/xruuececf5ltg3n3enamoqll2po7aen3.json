{
    "id": "xruuececf5ltg3n3enamoqll2po7aen3",
    "title": "Walks on Networks",
    "info": {
        "author": [
            "Richard Wilson, Department of Computer Science, University of York"
        ],
        "published": "July 2, 2012",
        "recorded": "May 2012",
        "category": [
            "Top->Computer Science->Network Analysis",
            "Top->Mathematics->Graph Theory"
        ]
    },
    "url": "http://videolectures.net/complexnetworks2012_wilson_walks/",
    "segmentation": [
        [
            "So."
        ],
        [
            "What I really want to do is try and give a kind of overview of the work we've done at York on discrete time random walks, so would be a fairly high level overview.",
            "We really come from a kind of background of pattern recognition and computer vision, so we're interested in applying random walk methods to problems like embedding of graphs, matching graphs, together characterization or description of graphs in some way, and also in particular graph based processing problems.",
            "So, for example, we might have an underlying computer vision problem that we can convert into a graph problem.",
            "Like image smoothing or something like that where we have a graph based representation of the data.",
            "So of course random walks are closely related to other things that we've done quite a bit of work on in the group, so spectral methods and also a lot of work on diffusion, and he processes on graphs."
        ],
        [
            "OK, so these are some of the algorithms that we can.",
            "Actually addressed with random walk based algorithms, so image smoothing problems are just mentioned where we can define a heat kernel on a graph that represents the image in some way and then use that as an isotropic smoothing filter to preserve structure in the image in some way.",
            "Gross simplification, we can use ideas from random walks to turn grass into trees or Siri, ate grass to turn them into strings in order to convert difficult problems into easier ones so they can be more easily manipulated.",
            "Embedding is also an important problem for us, so we can take graph, embed them into space, and usually when you have a geometric representation in a space in some way, we can do things a lot more easily than we can do in the combinatoric's of graph space.",
            "So examples of that.",
            "I'll mention commute time a little bit in a moment.",
            "Also preserving things like diffusion distance with our embedding characterization.",
            "One of the things we've done in the group are look at permutation invariant.",
            "So characterizations of graph that don't depend on the way that you label the graph, and you can derive those also from the structure of random walks directly.",
            "And things like consistent labeling, so we might have graph representing some data set where the labels on different verses of the graph have to be consistent with each other.",
            "And so you have a labeling problem with constraints which are represented by the graph structure."
        ],
        [
            "OK, so of course there's a very rich literature out there on graph analysis with random walks.",
            "Serve only highlighted some very small section of that literature, but there being various random walk algorithms for problems in pattern recognition in computer vision.",
            "So graph matching using random walks, for example Seriation work.",
            "I just briefly mentioned robust Kelly and Hancock looked at that and also random walk view of spectral segmentation.",
            "So the problem of segmenting images into parts you can take a random walk view of.",
            "That process"
        ],
        [
            "Embeddings also there's a lot of literature out there and how to embed graphs into space, preserving various different properties of the graphs, so I'll talk about random walk kernels in a little bit more detail.",
            "In a moment.",
            "You got things like diffusion map, where you're trying to preserve the diffusion time between the vertices of the graph in your embedding an commute time embedding for things like image segmentation and so on."
        ],
        [
            "OK, so first very brief introduction.",
            "I'm sure everybody's familiar.",
            "The concept of random walks.",
            "I won't talk too much about it, so the idea is to define a transition matrix representing the steps that you can take across the graph, and then the time evolution in a discrete time random walk basically is governed by this transition matrix that you define.",
            "So you start up with initial probability distribution on the vertices of the graph, and at the next step you simply multiply by the transition matrix to get the.",
            "Probability distribution at the next timestep?",
            "So the time Evolution, the interesting things about the random walk on the graph are represented in this matrix T, and in particular, the eigensystem of this matrix is really the key thing that you have to look at to understand the structure of the random walk.",
            "So of course there's a steady state distribution for the random walk on a graph, so the steady state is determined by the leading eigenvector for this matrix T so large times you know that the random walk will settle down into a particular configuration that doesn't depend on the.",
            "Initial configuration that you started at just governed by the leading eigenvector of this matrix T. So there's a direct connection there between random walks, an spectral theory."
        ],
        [
            "OK, so more recently people have been interested in the idea of defining graph kernels.",
            "So graph kernel is essentially a function which measures the similarity between two graphs.",
            "So it tells you how similar graphs Geworden G2 are to each other.",
            "So the idea of random walk kernel is to count the number of matching walks which exist in both of these graphs.",
            "So if you can find a walk in one graph and a walk of the same length and the other graph, then you've counted one random walk and if you count up all those possibilities.",
            "You get a count of how similar the two graphs are in terms of the random walk kernel.",
            "So the whole thing looks like this.",
            "So this thing in here.",
            "This is the product graph of G1 and G2, so that's simply formed by taking pairs of vertices, one from each of the two graphs, and you connect those vertices if there exists two consistent edges between vertices and both of the graphs.",
            "So by taking powers of the adjacency matrix of the product graph, you can count how many walks are consistent in both of the two graphs, and we sum those up or over all possible lengths of paths.",
            "So from K is not to Infinity and that will tell us the number of matching random walks in the two graphs.",
            "So obviously this as it is this whole thing won't converge because the number of walks that you get goes up very rapidly with increasing K. So you have to introduce this epsilon factor here.",
            "This is really kind of something that you can fiddle the statistics with in order to make it converge, so you start this large and then decrease it rapidly in order to truncate the series at some point.",
            "And that's kind of an empirical set of parameters that you need to set for particular types of graph.",
            "OK, so that's the idea of a random walk kernel.",
            "It's gotta some problems with it.",
            "So one of the problems with these large number of parts that you get and you have to count up the other one problem is the following problem.",
            "So if I start to walk there an I take a five step walk across this graph and I can go down this edge then go back along that edge again and then back along the edge and back to that final point there.",
            "So rather like a professor had particularly good conference dinner.",
            "This is called tottering, so he's going backwards and forwards along the edges.",
            "And eventually ends up at the same place that you could get to just by taking a three step walk.",
            "So that kind of tottering step kind of tends to mask some of the structural differences between different graphs, so you can get very long walks that don't actually traverse very much of the graph, and therefore you don't get a very clean separation of graphs that might have different structures in them."
        ],
        [
            "So one way around that that we've been exploring York is the idea of a back track list.",
            "Random walk.",
            "So essentially idea of back track list.",
            "Random Walk is to introduce a condition that you can't go back down the same age that you just walked down on the previous step.",
            "So we eliminate edges.",
            "They look like this where the previous edge that we've reversed is the same as the next edge in the sequence.",
            "So we can do that by creating the line graph of the original graph.",
            "So if this is the original graph we're interested in in order to form the line graph, we simply replace each of the edges by a pair of directed edges representing the steps in each direction, and then from this we take each edge out of this graph and create a vertex in our line graph an we connect the vertices where one of these edges with the head of one of these edges meets the tale of the next edge.",
            "So by doing that we end up with another type of work was essentially the same as the.",
            "Original walk, so the steps we can take on the line graph are the same as the original walk and then when we end up with this line graph over here, we can actually delete the edges of which represent a backtracking step because we've taken the steps out of here as edges.",
            "Each step represents a pair of transition from one edge to another, so we can now find the edges which represent the backtracking step and eliminate them from this final graph.",
            "So then if we do a walk on this graph, then we'll have illuminated this tottering problem from the."
        ],
        [
            "Graph.",
            "So the back track list random walk kernel is essentially the same as the as the random walk kernel, except we do it on the product graph of the oriented line graphs that we create from the original graphs.",
            "OK, so this right this random walk kernel will eliminate the backtracking step and the tottering step, but this also has a problem with it, and that's the fact that we've gone to these oriented line graphs.",
            "So from going from the edges of the original graph to vertices in the new graph, and essentially we've squared the problem, we might have the square of the number of vertices that we had before, and therefore this product graph here becomes very large very rapidly.",
            "So one of the things that we've done in the group most recently, so we publish this in a Simbad workshop last year is to provide an efficient method for computing this result with this kernel, without actually computing oriented line graph.",
            "So we have an iteration on the adjacency matrices of the original graph which will compute the same thing that we get in this backtrack.",
            "This random walk kernel and we show that this is a better way of characterizing the graph structure actually does a better job on the certain sets of graph that we looked at.",
            "In that paper."
        ],
        [
            "OK, so the other thing we spent quite a bit of time on looking at the group is the idea of commute time and I compute commute time embedding.",
            "So this is a very similar idea to the idea of using a random walk to characterize graphs.",
            "So the idea of a compute time embedding is to use the time it takes to get from one vertex of the graph to another via random walk.",
            "So first of all the hitting time the hitting time is just the expected time to arrive at a vertex V. Once you've started from a vertex, you and from that you can compute compute the commute time.",
            "So the commute time between U&V is just the roundtrip time, so it's the time you take to get there, and then the time added on the time you take to get back again to the original point.",
            "So this commute time is quite an interesting method of embedding because it has some nice properties which get around some of the problems you get with paths.",
            "So if I go from vertex you to vertex V and I have a graph that may be subject to errors, so edges might be deleted at random from so on.",
            "If I look at the commute time, there are lots of different paths from getting from U to V and back again, so it eliminates some of the problems with getting noise in your graphs and random education by going down all the different paths.",
            "And be more robust to those kind of errors.",
            "So we can actually compute the computer hitting time very easily by using the Greens function of the graph.",
            "So we construct the Greens function from their passion.",
            "Essentially it's the pseudoinverse, the Laplacian.",
            "So we take the inverse.",
            "That'll pass in matrix with the spectral decomposition, but we simply ignore the eigenvalue that's in the Laplacian to give us the Greens function.",
            "And then there's a straightforward relationship between the Greens function for U&V and the hitting time between the two vertices."
        ],
        [
            "So from the hitting time we can construct the commute time so it turns out the commute time has this nice formula here, so this is immediately reminiscent of the distance between two points in space, so this bit here.",
            "This is like the squared distance between 2 points, and it turns out there's a very straightforward embedding that you can construct from this commute time, which looks like this.",
            "So it's simply dependent on the eigensystem of the Laplacian matrix of the graph.",
            "So this embedding here.",
            "This will preserve commute time, so we have points in space Now where the distance between them is equal to the commute time in the original graph, and that turns out to be quite a robust embedding process, 'cause it's resistant to this random deletion of edges in the graph.",
            "So commute time.",
            "We also looked at quantum commute Time, which I went talk about here, which is based on similar principles but use this quantum walks which I'm going to talk about in a second."
        ],
        [
            "OK, so the second part of my work is going to be about quantum walks, which we've also done a fair bit of work on in the group, so a quantum walk is different from the random walk in that the quantum Walker obeys the laws of quantum mechanics.",
            "So first of all, our state of our system will be described by complex wave function rather than a probability of being each vertex, and that means we can have negative amplitudes on the vertices and state probability is essentially the square of the amplitude.",
            "Being each particular vertex.",
            "Because it's a quantum quantum walk, we have to obey the rules of quantum mechanics, and that means that the evolution of the work has to be reversible.",
            "So if we take a step in the graph, we must know how to go back by reversing if we reverse, time will end up back in the state that we started with.",
            "And Thirdly, if we observe where are quantum warcries, then it will collapse the wave function.",
            "So if I observe my quantum Walker to be at a particular vertex in the graph, then the state of the system will then collapse to quantum Walker being at that particular vertex at that particular time.",
            "So quantum walks are interesting because it turns out they have a richer structure than the classic random walk, and the reason for that is because of interference.",
            "So because we can have negative amplitudes and positive amplitudes, then the Walker may create positive negative amplitudes at the same vertex and create destructive interference, for example which give us different types of properties than we get from around normal random walk."
        ],
        [
            "OK, so this is a similar kind of quick summary of quantum walk evolution as compared to the classical random walk evolution.",
            "So the evolution matrix.",
            "So this is the equivalent of the transition matrix T for the classic random walk.",
            "So rather than being a stochastic matrix as it is for the random walk, this is a unitary matrix an its unitary becausw the evolution of the quantum walk has to be invertible, so we have to be able to invert this matrix you.",
            "It's basically constructed from two parts, so there's a coin matrix.",
            "Here we use the Grover coin matrix, so the coin matrix gives you the random choice between different steps that you're going to take at the next time step, and the transition matrix is essentially the analogue of transition matrix in a classical random walks that tells you they allowed steps that you're allowed to take in the next step of the graph.",
            "So essentially what the edges are in the graph.",
            "The time evolution of the wavefunction is essentially identical to what you get in the classical random walk, so you just multiply by you and it gives you in the next state of the system.",
            "So straightaway, there's actually a very big difference between the quantum walk in the classical random walks on the quantum walk.",
            "There's no steady state, so at large times it won't settle down into a particular configuration.",
            "The walks reversible and that means that each step of the walk has to be labeled actually by two things.",
            "So you have to know both the vertex that you've arrived at, which is V, and then the vertex which you've come from in the previous step.",
            "If you don't know both of those things, and you can't reverse the walk, you can't go backwards in time.",
            "So in effect, the quantum walk is actually on the edges of the graph rather than the vertices."
        ],
        [
            "So this is a quick example just to explain what it looks like.",
            "So in my initial state here, I assume this central vertex E, having come previously at previous time, step from the vertex A and the graph.",
            "So in effect my current state of the system is a pair AE representing where I've come from and where have arrived at this time step.",
            "If I take a step from this vertex, see here, then it since we're using Grover coin, that means that all of these three possibilities, or these outgoing edges are equally likely to have the same amplitude as each other, and in this case the amplitude gets 1/2.",
            "So for the next state you get a superposition of all the possible steps you can take next time going to from E to BE to see Anita D and they all have an amplitude of plus 1/2.",
            "This backward step in this particular case is different from these three steps, and you can tell it's different because you know that you came previously from this vertex, and in this case we end up with the amplitude of minus 1/2 for the backward step.",
            "So this is an example.",
            "You can get these different sign amplitudes and this is the thing that really makes the difference between the classical walk and the quantum walk gives you different types of structures."
        ],
        [
            "So what about the structure of the quantum walk?",
            "So we did a fair bit of analysis with David.",
            "EMS is a student in a group and also with Simoni.",
            "Submarine is at the back there on what the actual structure.",
            "This will, particularly in terms of the spectrum that you get from the transitions matrix of the walk and so on.",
            "So if you analyze the spectrum of this matrix, you representing the evolution of the walk, it turns out that it's completely dependent on the spectrum of the transition matrix of the classical walk.",
            "So you can find out the spectrum if you know the spectrum eigenvalues of the original classical random walk.",
            "So there's no difference there, which is obviously a bit disappointing, given that we know there's structural differences.",
            "So essentially that means that if we look at powers of you, we're not going to find anything that's for any different in terms of the spectrum from a normal walk.",
            "So another thing we can do is try and separate out the contributions of different amplitude states of the graph so we can look at something called the positive support of you.",
            "So positive support of you is simply defined as one.",
            "If you is greater than zero and zero otherwise.",
            "So what this represents is the paths in the graph which have a positive amplitude a particular time, so we throw away the ones I have negative amplitude and we only look at the ones that have a positive amplitude.",
            "By doing that, we can see the difference between the kind of structures from a normal walk.",
            "And from a quantum walk.",
            "So this positive support of you actually encodes interference effects in an interesting way.",
            "And it turns out that if you look at the positive support of you with the Grover coin, as I just talked about in the previous slide, then the graph you get.",
            "So this represents a graph with vertices and edges connecting them.",
            "The graph you get here is exactly the same as the oriented line graph that we used for the back track list random walk.",
            "So there's a direct connection between these quantum walks and the idea of a back track list.",
            "Random Walk on the graph."
        ],
        [
            "OK, so we need some interesting test cases where it's difficult to determine the difference between different graphs and have the same kind of structure we use strongly regular graphs for these, so if you're not familiar with these, these are graphs which have a very great deal of symmetry and similarity between them.",
            "So strongly regular graph from a particular family first has the same number of vertices as the same number of connections for every node, which is K. So it's a regular graph.",
            "Degree K and if you take a pair of vertices joined by an edge in here, then if they joined by an edge they always have L neighbors.",
            "Every single pair of vertices which is joined by an edge Hasel neighbors if they're not joined by an edge, then they have em neighbors, so they have very strong irregularities in the minutes very constrained set.",
            "So this is actually interesting because there's no known polynomial time algorithm for determining whether these graphs are the same as each other or not."
        ],
        [
            "OK, so if we look at the spectrum of the positive support of you unfortunate doesn't distinguish strongly regular graphs from the same family, so you can show that these spectrum of these things is exactly the same as it is with any of the classical random walk based methods.",
            "The positive, supportive you square doesn't separate them either, so that represents two step path which had positive amplitude.",
            "But if you look at three step paths with positive amplitude then you do get different Spectra for these different graphs.",
            "So just as an example for you, this family of strongly regular graphs here it has more than 32,000 members, but if you take any pair of Members from that family, they always have different spectrum from each other.",
            "So you can distinguish them using this method.",
            "So this is an interesting result that you and U squared don't distinguish, but you cube does.",
            "So the rationale behind that, I think is really that that's the first step number first Alright path where you can get positive and negative amplitudes actually interfering with each other at the same part.",
            "Because that means that you've actually walked around a loop triangle in the graph and come back to the original point where you can get positive and negative interference.",
            "From this on the same vertex."
        ],
        [
            "So unfortunately it doesn't separate all grass from each other by their spectrum.",
            "If it did, that would be a very interesting result, but if you look for example at cospectral trees where there was known to be a big problem with Co spectrality, then you get a very good result, so you only get less than 300 trees which are in Co spectral pairs.",
            "When you have 24 vertices in your tree as compared to more than 3000.",
            "If you look at the spectrum of the Laplacian for example, so it's a very good separator of.",
            "Different trees in terms of the spectrum."
        ],
        [
            "So the other thing we looked at where with quantum walks is the idea of trying to do graph matching.",
            "So we're comparing two rows with each other.",
            "So the idea here is to take two graphs to be matched so we have graph graph be here and join them with Alteryx vertices.",
            "So the vertices here are connected pair of vertices, one from graphite won't from graph be so essentially.",
            "These are the vertices of the project graph as we had in their back track list random walk, but we connect these in a different way so we connect to each one to the vertices that corresponds to in the two graphs.",
            "So the idea here is that you start a quantum walk on graph and graph be in the same configuration, except that you make the amplitude of the one graph be negative.",
            "The negative amplitude of the one graph A.",
            "Then if you allow the walk to proceed.",
            "If these two graphs are isomorphic to each other, then the amplitudes that you'll get on the matching vertices on these auxiliary verses in the center should be 0.",
            "'cause you get a positive contribution from walk in a negative from the other.",
            "So if there are isomorphic you can look at these auditory vertices and pick out the ones with zero.",
            "Choose all times and they should correspond to the matches between those two graphs.",
            "If they're not isomorphic if they're similar, so in the normal performance we're looking at, we're trying to find graphs which are very similar to each other, but not exactly the same.",
            "You would hope that they have small amplitude rather than zero amplitudes, so this then becomes a statistical game about whether you can separate these two things from each other."
        ],
        [
            "So if you look at the statistics, it turns out that the non matches have a different distribution to the true matches, so there not matches have this nice Gaussian distribution where we get an exponential distribution for the ones which should match with each other.",
            "So to some extent we can use the idea of a quantum walk to distinguish between matching and non matching versus by looking at the values the amplitudes on the auxiliary vertices."
        ],
        [
            "OK, so just to finish off, just want to go on a little bit of a tangent, something that we're interested in now so.",
            "One of the things we've looked at in quite a bit of detail is the idea of diffusion process on a graph.",
            "So now we're into the domain of continuous time walks, so the idea of diffusion process is very straightforward.",
            "We use the discrete Laplacian of a graph as an analog for the Laplace Beltrami operator on a manifold, and then we can define a differential equation representing the heat flow over the graph.",
            "So it's a very straightforward equation and we can solve this to find the heat kernel of the graph, which is an interesting representation of the graph.",
            "So essentially that's the same thing as a particular limit of a particular continuous time random walk over the graph.",
            "Other things we can do, we can look at solutions of Schroedinger equation over a similar way.",
            "Of course, in this case will get wavelike solutions, 'cause we're talking about essentially a graph which is in free space.",
            "So these type of processes.",
            "These are fairly well understood.",
            "Now we're using a discrete Laplacian, so that means we have connectivity, so we can have weights on the edges, but we don't have any concept of length on these edges, so there's just connections between the different verses with different weights, but no concept of how far apart they are.",
            "No geometry on the graph."
        ],
        [
            "So.",
            "What I'm going to talk about now really stems from some of the ideas of Joel Friedman here with Jean Pierre Tillisch.",
            "So his idea was to take a geometric realization of the graph, an associated length, an interval with each edge, and then we're introducing a kind of metric structure into the graph, and we have distances between the vertices.",
            "So then we can start to talk about how long it takes signals, for example, to propagate.",
            "So how long it takes diffusion processes to propagate down the length of edges and so on.",
            "So if you follow that idea, then you end up with this idea of a two part.",
            "Fashion so vertex based part was essentially the same idea as a discrete Laplacian and an edge based part which has much more interesting properties.",
            "So for the edge based part we're talking about functions which now exist on the edges on this interval associated with the edges."
        ],
        [
            "So from that edge base of passing we can define the various analogs of these things that we had before, but edge based in edge based sense.",
            "So we can have an edge based heat kernel.",
            "An edge based diffusion.",
            "We can define a wave equation now directly on these edges and we can even have things like a relativistic heat equation.",
            "So the heat equation, which has finite propagation speed across the graph.",
            "So I think these are interesting to us at the moment because in contrast to the discrete Laplacian, these last two equations exhibit finite propagation speeds, so there might be particularly interesting for modeling processes on graphs where the transmission time is of the same order as they kind of effects that you're looking at.",
            "So if you use something with a finite speed, you can see different type of timing effects that you might not see using a discrete Laplacian."
        ],
        [
            "So the reason why I mentioned at the end of this talk is it turns out there's actually a connection between this edge based Laplacian and these discrete time equate continuous time equations and the type of walks that I've already been talking about.",
            "So the eigensystem of this edge based lapasan comes in two parts.",
            "So the first part sets of eigenfunctions which are supported on the vertices, so they have non zero values on the vertices and you can show that these eigenvalues and eigenfunctions are essentially determined.",
            "By the structure of the random walk matrix T. So the eigenfunction, the eigenvectors, the random walk transition matrix, determine what these eigen functions are.",
            "There's another set of Eigen functions which are zero on the vertices, so the unsupported eigen functions on the vertices actually turned out to be determined by the back track list.",
            "Random Walk on the adjacency matrix of the oriented line graph.",
            "So if we look at the eigen structure of the adjacency matrix, it tells us what these eigenfunctions are, which are not supported on the vertices, so this eigensystem.",
            "This edge base Laplacian actually contains structure from both the classical random walk on the back track list random walk, so it's a particularly flexible approach.",
            "To modeling the signal propagation on graphs."
        ],
        [
            "K so just finish off with just kind of summarize what I said.",
            "So random walks are very powerful tools for analyzing network structure, and we've done a lot of work in Group at your car using random walks of various processes on graphs.",
            "So summarize what we've done in various spheres with a random walk back track list, random walk and quantum walks, and this is kind of a more forward looking.",
            "The edge based Laplace.",
            "There's something that we're interested in at the moment because firstly it contains structure from both of these types of walks.",
            "The random walk in the back track list walk an because of this finite speed of signal propagation bias, various differential equations.",
            "So we're interested modeling systems where the transmission time across the network actually matters to the dynamics.",
            "Right, so that's it.",
            "Thank you.",
            "Question.",
            "You talked about how?",
            "Be insensitive to noise.",
            "Algebra Replaces Zero bayawan in the matrix.",
            "It doesn't change the spectrum very much.",
            "Is there a general theory for that, or is it just not relation?",
            "So well, I was I was talking particularly about the commute time in that context.",
            "So we look if you look at individual parts then you can destroy path by deleting an edge for example.",
            "So it's not very noise resistant.",
            "But in order to answer that second question.",
            "So when you're looking at powers, basically it sums of powers of the adjacency matrix.",
            "So once you do that process, I think it is.",
            "I don't know exactly what the theorem is, I'm not mathematician, so not well versed in that, but.",
            "01 So it's another one change to a matrix.",
            "Send it yeah.",
            "So it's not the case actually, so it depends exactly on the graph in the spectrum of the graph.",
            "It depends on the eigen gaps between the various eigenvalues, so if it turns out that those eigen gaps actually closely related to the symmetry factors of the graph.",
            "So if you have two graphs which don't have many automorphisms and they don't have any symmetry factors, then usually have a nicely spread spectrum and you can compare them.",
            "They basically result in a nice eigensystem in space.",
            "If they have a lot of symmetry then you can completely destroy that symmetry just by deleting the edges and you get a massive, flipping the eigensystem and they can end up looking completely different from each other.",
            "So I think that's a general problem for these eigen based methods that they can't cope with certain types of graph they can cope with the type of graphs that you might normally see.",
            "So if it's A kind of Erdos Renyi type graph, then you don't normally have any problem with your system.",
            "The.",
            "PD is known as the telegraphers equation.",
            "So this is one of my."
        ],
        [
            "Differential equations.",
            "One of these.",
            "Yeah, the number 3.",
            "Interesting that in all sorts of weird applications, including the highly channel and flow of radionuclides through groundwater and things like that where normal dispersion just isn't observed there very fast.",
            "It's kind of interesting that turns up here, and that's something that we probably have some conversation about environmental applications.",
            "And also I mean this.",
            "This third equation has a kind of a bit of a checkered history.",
            "So I mean my.",
            "So my my original background surface of 1st degree was in physics, so obviously we did.",
            "Far from it, so I guess that's true.",
            "Quite a few people actually.",
            "So obviously we study things like heat equation and we don't study things like this.",
            "So the thing about this is that it doesn't have a particularly strong physical basis, and you can get unusual solution.",
            "So this will also have wavelike solutions in certain cases.",
            "And so that's something which is not people don't perceive as being a very heat like type of behavior.",
            "It doesn't behave like diffusion.",
            "Yeah, exactly so.",
            "You know, I mean, if you have a system that's generally governed by an equation like this, then this is.",
            "This is what you need to use, so I guess it's in that sense.",
            "It's a very interesting question.",
            "Good, I think it's very interesting equation.",
            "Particularly this picture we could compare to networks and you could say whether they are the same, so as more frequently, yeah.",
            "So just.",
            "Real Life Network network network.",
            "What?",
            "So determine the similarity of networks.",
            "So graph matching graph comparison is something that we've done a huge amount of work on over the last 20 years or so.",
            "So not just by these type of methods.",
            "I was just using that as a kind of example to motivate the quantum walk.",
            "So the problem of determining whether two graphs are similar or not.",
            "It's a very, very interesting problem and quite hard problem.",
            "I think we have very good methods for graphs which are not so large, so in the order of hundreds of vertices, perhaps maybe up to 1000.",
            "But when you're talking bout, presumably you're talking about working with very large graphs in the order, maybe.",
            "Five site yeah, so once you go up to very large networks, 10s of thousands or up to millions of vertices then you need to adopt.",
            "These methods are very efficient and then it becomes a very much more difficult problem.",
            "So there I think you need to use much simpler characterizations of the graph rather than these type of matching methods.",
            "So here we're looking for an explicit correspondence between the different vertices.",
            "I think if you were talking about very large systems you need to not worry about that and just look for a comparator like like the graph kernels for example, which would be more efficient.",
            "Jordan or.",
            "It seems like everything you've done it in this talk has been deterministic.",
            "If we thought about throwing a bit of noise into the graphs and then define their tests.",
            "I say we haven't.",
            "I think it's a good idea.",
            "But I found this stuff that Mark was talking about about grass networks that evolve overtime fascinating, so I think would be very interested in looking at those type of problems, whether the structure might change from one time step to the next and what effect that has on walks for example.",
            "So I think that's a very interesting question and one.",
            "Rather than buy up.",
            "Terministic take statistics about what happens in various equations.",
            "OK, I think that's that's an interesting question.",
            "It doesn't, I can't immediately say that that's going to give any positive benefit or not.",
            "OK, so so many probably knows better than me.",
            "In fact, started noise on this table.",
            "Yeah, OK yeah yeah, interesting idea.",
            "Too short.",
            "The.",
            "Additional information that come from work brings compared to the other work I understand, for example, that under more schedule information about the structure it was even used in some algorithm performed over there, not detect some communities.",
            "So I I didn't understand what additional information we have about this picture.",
            "When you look to wave equation or something that is 1 equation that we are really excited about is the FKP equation which is not linear also have reaction term in addition to the diffusion.",
            "It's important because it's this type of disease starts.",
            "To start with this and something that.",
            "That's the fucking point diffusion.",
            "Fisher it shouldn't have linearity, OK?",
            "IS 1 -- F term OK?",
            "I'm not familiar with that to be honest, so I think that second question is a difficult one for me to answer.",
            "So as well the first question was.",
            "Is yeah.",
            "I didn't.",
            "So I mean, I don't know how familiar with quantum algorithms in general, but.",
            "Quantum evolution of the system seems to be more calm, potentially powerful in some ways than classical evolution of the system.",
            "So in this case we're comparing a classical random walk to a quantum walk.",
            "It's essentially interference patterns that we are on the graph, which will reveal more information about the structure.",
            "So for example, there's a particular special case where you glue true trees together, so the top of one tree at one side and the top of another sign.",
            "Then you blew the leaves together and you can show for a binary tree that a quantum walk will end up at the other side.",
            "Exponentially more quickly than a classical random walk.",
            "So essentially you can probe the structure of a particular types of graph more efficiently with quantum.",
            "What the account with the classical walk, and that's because of this interference patterns that you get in the graph.",
            "OK, so sort.",
            "If I understand correctly.",
            "So and in the classical random walk you are completely so the person that the Walker is completely stupid.",
            "So in the right of one note and can return in the in the same way to the to the previous node, but in the in the quantum one you penalize in some way to return to the previous one.",
            "So it's in some way guided to go to the target.",
            "This is that correct?",
            "Um?",
            "I think it's no.",
            "Probability is the same.",
            "Negative.",
            "But the probability is the square of the amplitude is you just have this interference effect?",
            "So I mean it's to take a physical analogy.",
            "It's like the idea of quantum tunneling, so quantum particle couldn't pass through a barrier in arriving at the side button necessarily being observed in between the two, whereas a classical part of would just be stopped and can't penetrate the other side, so the interference patterns allow you to penetrate through certain types of graphs more quickly than the classical random walk, and that's.",
            "Regulation for example.",
            "Population, the declaration threshold tells you OK if I'm below the threshold, there is a giant component which arose from one side to the other one, and that's fine.",
            "Now, if you take the point of declaration program which is not solved knowledge, there is a constant declaration threshold which is larger than the classical threshold, and the difference is due because there's no conduction because of interference effect.",
            "But I still don't understand what what you learn from that.",
            "You know there's no what's.",
            "Do you know something about the giant destruction of design cluster or you know?",
            "I think it's a nontrivial problem to actually connect the properties of a walk in general, except in certain special cases and the structure of the graph, so I think if you're looking for a clear kind of message about what, what can I use the quantum?",
            "What type of structure can I probe with the quantum walk?",
            "I can't with the classical work, for example, I don't think there's an easy answer to that yet.",
            "I mean, these ideas are still maturing, I think to a certain extent, and I'm not aware of any clear definition of exactly what this type of structure that you can probe in one way with the quantum Walker.",
            "And then you will get a different answer for other work.",
            "I think at the moment we're still limited to the type of things I've showed you here, right.",
            "Particular examples where the quantum walk behaves differently from the classical walk like the one I showed you here, or like this idea of being able to pass through certain graphs more quickly than the classical walk.",
            "I don't think there's a kind of general answer to that problem yet."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I really want to do is try and give a kind of overview of the work we've done at York on discrete time random walks, so would be a fairly high level overview.",
                    "label": 0
                },
                {
                    "sent": "We really come from a kind of background of pattern recognition and computer vision, so we're interested in applying random walk methods to problems like embedding of graphs, matching graphs, together characterization or description of graphs in some way, and also in particular graph based processing problems.",
                    "label": 0
                },
                {
                    "sent": "So, for example, we might have an underlying computer vision problem that we can convert into a graph problem.",
                    "label": 0
                },
                {
                    "sent": "Like image smoothing or something like that where we have a graph based representation of the data.",
                    "label": 0
                },
                {
                    "sent": "So of course random walks are closely related to other things that we've done quite a bit of work on in the group, so spectral methods and also a lot of work on diffusion, and he processes on graphs.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so these are some of the algorithms that we can.",
                    "label": 0
                },
                {
                    "sent": "Actually addressed with random walk based algorithms, so image smoothing problems are just mentioned where we can define a heat kernel on a graph that represents the image in some way and then use that as an isotropic smoothing filter to preserve structure in the image in some way.",
                    "label": 1
                },
                {
                    "sent": "Gross simplification, we can use ideas from random walks to turn grass into trees or Siri, ate grass to turn them into strings in order to convert difficult problems into easier ones so they can be more easily manipulated.",
                    "label": 1
                },
                {
                    "sent": "Embedding is also an important problem for us, so we can take graph, embed them into space, and usually when you have a geometric representation in a space in some way, we can do things a lot more easily than we can do in the combinatoric's of graph space.",
                    "label": 1
                },
                {
                    "sent": "So examples of that.",
                    "label": 0
                },
                {
                    "sent": "I'll mention commute time a little bit in a moment.",
                    "label": 0
                },
                {
                    "sent": "Also preserving things like diffusion distance with our embedding characterization.",
                    "label": 1
                },
                {
                    "sent": "One of the things we've done in the group are look at permutation invariant.",
                    "label": 0
                },
                {
                    "sent": "So characterizations of graph that don't depend on the way that you label the graph, and you can derive those also from the structure of random walks directly.",
                    "label": 0
                },
                {
                    "sent": "And things like consistent labeling, so we might have graph representing some data set where the labels on different verses of the graph have to be consistent with each other.",
                    "label": 0
                },
                {
                    "sent": "And so you have a labeling problem with constraints which are represented by the graph structure.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so of course there's a very rich literature out there on graph analysis with random walks.",
                    "label": 0
                },
                {
                    "sent": "Serve only highlighted some very small section of that literature, but there being various random walk algorithms for problems in pattern recognition in computer vision.",
                    "label": 0
                },
                {
                    "sent": "So graph matching using random walks, for example Seriation work.",
                    "label": 1
                },
                {
                    "sent": "I just briefly mentioned robust Kelly and Hancock looked at that and also random walk view of spectral segmentation.",
                    "label": 0
                },
                {
                    "sent": "So the problem of segmenting images into parts you can take a random walk view of.",
                    "label": 0
                },
                {
                    "sent": "That process",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Embeddings also there's a lot of literature out there and how to embed graphs into space, preserving various different properties of the graphs, so I'll talk about random walk kernels in a little bit more detail.",
                    "label": 0
                },
                {
                    "sent": "In a moment.",
                    "label": 0
                },
                {
                    "sent": "You got things like diffusion map, where you're trying to preserve the diffusion time between the vertices of the graph in your embedding an commute time embedding for things like image segmentation and so on.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so first very brief introduction.",
                    "label": 0
                },
                {
                    "sent": "I'm sure everybody's familiar.",
                    "label": 0
                },
                {
                    "sent": "The concept of random walks.",
                    "label": 0
                },
                {
                    "sent": "I won't talk too much about it, so the idea is to define a transition matrix representing the steps that you can take across the graph, and then the time evolution in a discrete time random walk basically is governed by this transition matrix that you define.",
                    "label": 0
                },
                {
                    "sent": "So you start up with initial probability distribution on the vertices of the graph, and at the next step you simply multiply by the transition matrix to get the.",
                    "label": 0
                },
                {
                    "sent": "Probability distribution at the next timestep?",
                    "label": 0
                },
                {
                    "sent": "So the time Evolution, the interesting things about the random walk on the graph are represented in this matrix T, and in particular, the eigensystem of this matrix is really the key thing that you have to look at to understand the structure of the random walk.",
                    "label": 0
                },
                {
                    "sent": "So of course there's a steady state distribution for the random walk on a graph, so the steady state is determined by the leading eigenvector for this matrix T so large times you know that the random walk will settle down into a particular configuration that doesn't depend on the.",
                    "label": 1
                },
                {
                    "sent": "Initial configuration that you started at just governed by the leading eigenvector of this matrix T. So there's a direct connection there between random walks, an spectral theory.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so more recently people have been interested in the idea of defining graph kernels.",
                    "label": 0
                },
                {
                    "sent": "So graph kernel is essentially a function which measures the similarity between two graphs.",
                    "label": 0
                },
                {
                    "sent": "So it tells you how similar graphs Geworden G2 are to each other.",
                    "label": 0
                },
                {
                    "sent": "So the idea of random walk kernel is to count the number of matching walks which exist in both of these graphs.",
                    "label": 1
                },
                {
                    "sent": "So if you can find a walk in one graph and a walk of the same length and the other graph, then you've counted one random walk and if you count up all those possibilities.",
                    "label": 0
                },
                {
                    "sent": "You get a count of how similar the two graphs are in terms of the random walk kernel.",
                    "label": 0
                },
                {
                    "sent": "So the whole thing looks like this.",
                    "label": 0
                },
                {
                    "sent": "So this thing in here.",
                    "label": 1
                },
                {
                    "sent": "This is the product graph of G1 and G2, so that's simply formed by taking pairs of vertices, one from each of the two graphs, and you connect those vertices if there exists two consistent edges between vertices and both of the graphs.",
                    "label": 0
                },
                {
                    "sent": "So by taking powers of the adjacency matrix of the product graph, you can count how many walks are consistent in both of the two graphs, and we sum those up or over all possible lengths of paths.",
                    "label": 0
                },
                {
                    "sent": "So from K is not to Infinity and that will tell us the number of matching random walks in the two graphs.",
                    "label": 0
                },
                {
                    "sent": "So obviously this as it is this whole thing won't converge because the number of walks that you get goes up very rapidly with increasing K. So you have to introduce this epsilon factor here.",
                    "label": 0
                },
                {
                    "sent": "This is really kind of something that you can fiddle the statistics with in order to make it converge, so you start this large and then decrease it rapidly in order to truncate the series at some point.",
                    "label": 0
                },
                {
                    "sent": "And that's kind of an empirical set of parameters that you need to set for particular types of graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the idea of a random walk kernel.",
                    "label": 0
                },
                {
                    "sent": "It's gotta some problems with it.",
                    "label": 0
                },
                {
                    "sent": "So one of the problems with these large number of parts that you get and you have to count up the other one problem is the following problem.",
                    "label": 0
                },
                {
                    "sent": "So if I start to walk there an I take a five step walk across this graph and I can go down this edge then go back along that edge again and then back along the edge and back to that final point there.",
                    "label": 0
                },
                {
                    "sent": "So rather like a professor had particularly good conference dinner.",
                    "label": 0
                },
                {
                    "sent": "This is called tottering, so he's going backwards and forwards along the edges.",
                    "label": 0
                },
                {
                    "sent": "And eventually ends up at the same place that you could get to just by taking a three step walk.",
                    "label": 0
                },
                {
                    "sent": "So that kind of tottering step kind of tends to mask some of the structural differences between different graphs, so you can get very long walks that don't actually traverse very much of the graph, and therefore you don't get a very clean separation of graphs that might have different structures in them.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one way around that that we've been exploring York is the idea of a back track list.",
                    "label": 0
                },
                {
                    "sent": "Random walk.",
                    "label": 0
                },
                {
                    "sent": "So essentially idea of back track list.",
                    "label": 0
                },
                {
                    "sent": "Random Walk is to introduce a condition that you can't go back down the same age that you just walked down on the previous step.",
                    "label": 0
                },
                {
                    "sent": "So we eliminate edges.",
                    "label": 0
                },
                {
                    "sent": "They look like this where the previous edge that we've reversed is the same as the next edge in the sequence.",
                    "label": 0
                },
                {
                    "sent": "So we can do that by creating the line graph of the original graph.",
                    "label": 0
                },
                {
                    "sent": "So if this is the original graph we're interested in in order to form the line graph, we simply replace each of the edges by a pair of directed edges representing the steps in each direction, and then from this we take each edge out of this graph and create a vertex in our line graph an we connect the vertices where one of these edges with the head of one of these edges meets the tale of the next edge.",
                    "label": 0
                },
                {
                    "sent": "So by doing that we end up with another type of work was essentially the same as the.",
                    "label": 0
                },
                {
                    "sent": "Original walk, so the steps we can take on the line graph are the same as the original walk and then when we end up with this line graph over here, we can actually delete the edges of which represent a backtracking step because we've taken the steps out of here as edges.",
                    "label": 0
                },
                {
                    "sent": "Each step represents a pair of transition from one edge to another, so we can now find the edges which represent the backtracking step and eliminate them from this final graph.",
                    "label": 0
                },
                {
                    "sent": "So then if we do a walk on this graph, then we'll have illuminated this tottering problem from the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Graph.",
                    "label": 0
                },
                {
                    "sent": "So the back track list random walk kernel is essentially the same as the as the random walk kernel, except we do it on the product graph of the oriented line graphs that we create from the original graphs.",
                    "label": 1
                },
                {
                    "sent": "OK, so this right this random walk kernel will eliminate the backtracking step and the tottering step, but this also has a problem with it, and that's the fact that we've gone to these oriented line graphs.",
                    "label": 0
                },
                {
                    "sent": "So from going from the edges of the original graph to vertices in the new graph, and essentially we've squared the problem, we might have the square of the number of vertices that we had before, and therefore this product graph here becomes very large very rapidly.",
                    "label": 0
                },
                {
                    "sent": "So one of the things that we've done in the group most recently, so we publish this in a Simbad workshop last year is to provide an efficient method for computing this result with this kernel, without actually computing oriented line graph.",
                    "label": 0
                },
                {
                    "sent": "So we have an iteration on the adjacency matrices of the original graph which will compute the same thing that we get in this backtrack.",
                    "label": 0
                },
                {
                    "sent": "This random walk kernel and we show that this is a better way of characterizing the graph structure actually does a better job on the certain sets of graph that we looked at.",
                    "label": 0
                },
                {
                    "sent": "In that paper.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the other thing we spent quite a bit of time on looking at the group is the idea of commute time and I compute commute time embedding.",
                    "label": 0
                },
                {
                    "sent": "So this is a very similar idea to the idea of using a random walk to characterize graphs.",
                    "label": 0
                },
                {
                    "sent": "So the idea of a compute time embedding is to use the time it takes to get from one vertex of the graph to another via random walk.",
                    "label": 0
                },
                {
                    "sent": "So first of all the hitting time the hitting time is just the expected time to arrive at a vertex V. Once you've started from a vertex, you and from that you can compute compute the commute time.",
                    "label": 1
                },
                {
                    "sent": "So the commute time between U&V is just the roundtrip time, so it's the time you take to get there, and then the time added on the time you take to get back again to the original point.",
                    "label": 0
                },
                {
                    "sent": "So this commute time is quite an interesting method of embedding because it has some nice properties which get around some of the problems you get with paths.",
                    "label": 0
                },
                {
                    "sent": "So if I go from vertex you to vertex V and I have a graph that may be subject to errors, so edges might be deleted at random from so on.",
                    "label": 0
                },
                {
                    "sent": "If I look at the commute time, there are lots of different paths from getting from U to V and back again, so it eliminates some of the problems with getting noise in your graphs and random education by going down all the different paths.",
                    "label": 0
                },
                {
                    "sent": "And be more robust to those kind of errors.",
                    "label": 0
                },
                {
                    "sent": "So we can actually compute the computer hitting time very easily by using the Greens function of the graph.",
                    "label": 0
                },
                {
                    "sent": "So we construct the Greens function from their passion.",
                    "label": 0
                },
                {
                    "sent": "Essentially it's the pseudoinverse, the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So we take the inverse.",
                    "label": 0
                },
                {
                    "sent": "That'll pass in matrix with the spectral decomposition, but we simply ignore the eigenvalue that's in the Laplacian to give us the Greens function.",
                    "label": 1
                },
                {
                    "sent": "And then there's a straightforward relationship between the Greens function for U&V and the hitting time between the two vertices.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So from the hitting time we can construct the commute time so it turns out the commute time has this nice formula here, so this is immediately reminiscent of the distance between two points in space, so this bit here.",
                    "label": 0
                },
                {
                    "sent": "This is like the squared distance between 2 points, and it turns out there's a very straightforward embedding that you can construct from this commute time, which looks like this.",
                    "label": 0
                },
                {
                    "sent": "So it's simply dependent on the eigensystem of the Laplacian matrix of the graph.",
                    "label": 0
                },
                {
                    "sent": "So this embedding here.",
                    "label": 0
                },
                {
                    "sent": "This will preserve commute time, so we have points in space Now where the distance between them is equal to the commute time in the original graph, and that turns out to be quite a robust embedding process, 'cause it's resistant to this random deletion of edges in the graph.",
                    "label": 0
                },
                {
                    "sent": "So commute time.",
                    "label": 0
                },
                {
                    "sent": "We also looked at quantum commute Time, which I went talk about here, which is based on similar principles but use this quantum walks which I'm going to talk about in a second.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the second part of my work is going to be about quantum walks, which we've also done a fair bit of work on in the group, so a quantum walk is different from the random walk in that the quantum Walker obeys the laws of quantum mechanics.",
                    "label": 1
                },
                {
                    "sent": "So first of all, our state of our system will be described by complex wave function rather than a probability of being each vertex, and that means we can have negative amplitudes on the vertices and state probability is essentially the square of the amplitude.",
                    "label": 0
                },
                {
                    "sent": "Being each particular vertex.",
                    "label": 0
                },
                {
                    "sent": "Because it's a quantum quantum walk, we have to obey the rules of quantum mechanics, and that means that the evolution of the work has to be reversible.",
                    "label": 0
                },
                {
                    "sent": "So if we take a step in the graph, we must know how to go back by reversing if we reverse, time will end up back in the state that we started with.",
                    "label": 0
                },
                {
                    "sent": "And Thirdly, if we observe where are quantum warcries, then it will collapse the wave function.",
                    "label": 0
                },
                {
                    "sent": "So if I observe my quantum Walker to be at a particular vertex in the graph, then the state of the system will then collapse to quantum Walker being at that particular vertex at that particular time.",
                    "label": 0
                },
                {
                    "sent": "So quantum walks are interesting because it turns out they have a richer structure than the classic random walk, and the reason for that is because of interference.",
                    "label": 0
                },
                {
                    "sent": "So because we can have negative amplitudes and positive amplitudes, then the Walker may create positive negative amplitudes at the same vertex and create destructive interference, for example which give us different types of properties than we get from around normal random walk.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is a similar kind of quick summary of quantum walk evolution as compared to the classical random walk evolution.",
                    "label": 0
                },
                {
                    "sent": "So the evolution matrix.",
                    "label": 0
                },
                {
                    "sent": "So this is the equivalent of the transition matrix T for the classic random walk.",
                    "label": 0
                },
                {
                    "sent": "So rather than being a stochastic matrix as it is for the random walk, this is a unitary matrix an its unitary becausw the evolution of the quantum walk has to be invertible, so we have to be able to invert this matrix you.",
                    "label": 0
                },
                {
                    "sent": "It's basically constructed from two parts, so there's a coin matrix.",
                    "label": 1
                },
                {
                    "sent": "Here we use the Grover coin matrix, so the coin matrix gives you the random choice between different steps that you're going to take at the next time step, and the transition matrix is essentially the analogue of transition matrix in a classical random walks that tells you they allowed steps that you're allowed to take in the next step of the graph.",
                    "label": 0
                },
                {
                    "sent": "So essentially what the edges are in the graph.",
                    "label": 0
                },
                {
                    "sent": "The time evolution of the wavefunction is essentially identical to what you get in the classical random walk, so you just multiply by you and it gives you in the next state of the system.",
                    "label": 0
                },
                {
                    "sent": "So straightaway, there's actually a very big difference between the quantum walk in the classical random walks on the quantum walk.",
                    "label": 0
                },
                {
                    "sent": "There's no steady state, so at large times it won't settle down into a particular configuration.",
                    "label": 1
                },
                {
                    "sent": "The walks reversible and that means that each step of the walk has to be labeled actually by two things.",
                    "label": 0
                },
                {
                    "sent": "So you have to know both the vertex that you've arrived at, which is V, and then the vertex which you've come from in the previous step.",
                    "label": 0
                },
                {
                    "sent": "If you don't know both of those things, and you can't reverse the walk, you can't go backwards in time.",
                    "label": 0
                },
                {
                    "sent": "So in effect, the quantum walk is actually on the edges of the graph rather than the vertices.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a quick example just to explain what it looks like.",
                    "label": 0
                },
                {
                    "sent": "So in my initial state here, I assume this central vertex E, having come previously at previous time, step from the vertex A and the graph.",
                    "label": 0
                },
                {
                    "sent": "So in effect my current state of the system is a pair AE representing where I've come from and where have arrived at this time step.",
                    "label": 0
                },
                {
                    "sent": "If I take a step from this vertex, see here, then it since we're using Grover coin, that means that all of these three possibilities, or these outgoing edges are equally likely to have the same amplitude as each other, and in this case the amplitude gets 1/2.",
                    "label": 0
                },
                {
                    "sent": "So for the next state you get a superposition of all the possible steps you can take next time going to from E to BE to see Anita D and they all have an amplitude of plus 1/2.",
                    "label": 0
                },
                {
                    "sent": "This backward step in this particular case is different from these three steps, and you can tell it's different because you know that you came previously from this vertex, and in this case we end up with the amplitude of minus 1/2 for the backward step.",
                    "label": 0
                },
                {
                    "sent": "So this is an example.",
                    "label": 0
                },
                {
                    "sent": "You can get these different sign amplitudes and this is the thing that really makes the difference between the classical walk and the quantum walk gives you different types of structures.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what about the structure of the quantum walk?",
                    "label": 1
                },
                {
                    "sent": "So we did a fair bit of analysis with David.",
                    "label": 0
                },
                {
                    "sent": "EMS is a student in a group and also with Simoni.",
                    "label": 0
                },
                {
                    "sent": "Submarine is at the back there on what the actual structure.",
                    "label": 0
                },
                {
                    "sent": "This will, particularly in terms of the spectrum that you get from the transitions matrix of the walk and so on.",
                    "label": 0
                },
                {
                    "sent": "So if you analyze the spectrum of this matrix, you representing the evolution of the walk, it turns out that it's completely dependent on the spectrum of the transition matrix of the classical walk.",
                    "label": 0
                },
                {
                    "sent": "So you can find out the spectrum if you know the spectrum eigenvalues of the original classical random walk.",
                    "label": 0
                },
                {
                    "sent": "So there's no difference there, which is obviously a bit disappointing, given that we know there's structural differences.",
                    "label": 0
                },
                {
                    "sent": "So essentially that means that if we look at powers of you, we're not going to find anything that's for any different in terms of the spectrum from a normal walk.",
                    "label": 0
                },
                {
                    "sent": "So another thing we can do is try and separate out the contributions of different amplitude states of the graph so we can look at something called the positive support of you.",
                    "label": 0
                },
                {
                    "sent": "So positive support of you is simply defined as one.",
                    "label": 0
                },
                {
                    "sent": "If you is greater than zero and zero otherwise.",
                    "label": 0
                },
                {
                    "sent": "So what this represents is the paths in the graph which have a positive amplitude a particular time, so we throw away the ones I have negative amplitude and we only look at the ones that have a positive amplitude.",
                    "label": 0
                },
                {
                    "sent": "By doing that, we can see the difference between the kind of structures from a normal walk.",
                    "label": 1
                },
                {
                    "sent": "And from a quantum walk.",
                    "label": 1
                },
                {
                    "sent": "So this positive support of you actually encodes interference effects in an interesting way.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that if you look at the positive support of you with the Grover coin, as I just talked about in the previous slide, then the graph you get.",
                    "label": 1
                },
                {
                    "sent": "So this represents a graph with vertices and edges connecting them.",
                    "label": 0
                },
                {
                    "sent": "The graph you get here is exactly the same as the oriented line graph that we used for the back track list random walk.",
                    "label": 1
                },
                {
                    "sent": "So there's a direct connection between these quantum walks and the idea of a back track list.",
                    "label": 0
                },
                {
                    "sent": "Random Walk on the graph.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we need some interesting test cases where it's difficult to determine the difference between different graphs and have the same kind of structure we use strongly regular graphs for these, so if you're not familiar with these, these are graphs which have a very great deal of symmetry and similarity between them.",
                    "label": 1
                },
                {
                    "sent": "So strongly regular graph from a particular family first has the same number of vertices as the same number of connections for every node, which is K. So it's a regular graph.",
                    "label": 1
                },
                {
                    "sent": "Degree K and if you take a pair of vertices joined by an edge in here, then if they joined by an edge they always have L neighbors.",
                    "label": 1
                },
                {
                    "sent": "Every single pair of vertices which is joined by an edge Hasel neighbors if they're not joined by an edge, then they have em neighbors, so they have very strong irregularities in the minutes very constrained set.",
                    "label": 0
                },
                {
                    "sent": "So this is actually interesting because there's no known polynomial time algorithm for determining whether these graphs are the same as each other or not.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so if we look at the spectrum of the positive support of you unfortunate doesn't distinguish strongly regular graphs from the same family, so you can show that these spectrum of these things is exactly the same as it is with any of the classical random walk based methods.",
                    "label": 1
                },
                {
                    "sent": "The positive, supportive you square doesn't separate them either, so that represents two step path which had positive amplitude.",
                    "label": 1
                },
                {
                    "sent": "But if you look at three step paths with positive amplitude then you do get different Spectra for these different graphs.",
                    "label": 0
                },
                {
                    "sent": "So just as an example for you, this family of strongly regular graphs here it has more than 32,000 members, but if you take any pair of Members from that family, they always have different spectrum from each other.",
                    "label": 0
                },
                {
                    "sent": "So you can distinguish them using this method.",
                    "label": 0
                },
                {
                    "sent": "So this is an interesting result that you and U squared don't distinguish, but you cube does.",
                    "label": 0
                },
                {
                    "sent": "So the rationale behind that, I think is really that that's the first step number first Alright path where you can get positive and negative amplitudes actually interfering with each other at the same part.",
                    "label": 0
                },
                {
                    "sent": "Because that means that you've actually walked around a loop triangle in the graph and come back to the original point where you can get positive and negative interference.",
                    "label": 0
                },
                {
                    "sent": "From this on the same vertex.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So unfortunately it doesn't separate all grass from each other by their spectrum.",
                    "label": 0
                },
                {
                    "sent": "If it did, that would be a very interesting result, but if you look for example at cospectral trees where there was known to be a big problem with Co spectrality, then you get a very good result, so you only get less than 300 trees which are in Co spectral pairs.",
                    "label": 0
                },
                {
                    "sent": "When you have 24 vertices in your tree as compared to more than 3000.",
                    "label": 0
                },
                {
                    "sent": "If you look at the spectrum of the Laplacian for example, so it's a very good separator of.",
                    "label": 0
                },
                {
                    "sent": "Different trees in terms of the spectrum.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the other thing we looked at where with quantum walks is the idea of trying to do graph matching.",
                    "label": 0
                },
                {
                    "sent": "So we're comparing two rows with each other.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is to take two graphs to be matched so we have graph graph be here and join them with Alteryx vertices.",
                    "label": 1
                },
                {
                    "sent": "So the vertices here are connected pair of vertices, one from graphite won't from graph be so essentially.",
                    "label": 0
                },
                {
                    "sent": "These are the vertices of the project graph as we had in their back track list random walk, but we connect these in a different way so we connect to each one to the vertices that corresponds to in the two graphs.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that you start a quantum walk on graph and graph be in the same configuration, except that you make the amplitude of the one graph be negative.",
                    "label": 0
                },
                {
                    "sent": "The negative amplitude of the one graph A.",
                    "label": 0
                },
                {
                    "sent": "Then if you allow the walk to proceed.",
                    "label": 1
                },
                {
                    "sent": "If these two graphs are isomorphic to each other, then the amplitudes that you'll get on the matching vertices on these auxiliary verses in the center should be 0.",
                    "label": 0
                },
                {
                    "sent": "'cause you get a positive contribution from walk in a negative from the other.",
                    "label": 0
                },
                {
                    "sent": "So if there are isomorphic you can look at these auditory vertices and pick out the ones with zero.",
                    "label": 0
                },
                {
                    "sent": "Choose all times and they should correspond to the matches between those two graphs.",
                    "label": 0
                },
                {
                    "sent": "If they're not isomorphic if they're similar, so in the normal performance we're looking at, we're trying to find graphs which are very similar to each other, but not exactly the same.",
                    "label": 0
                },
                {
                    "sent": "You would hope that they have small amplitude rather than zero amplitudes, so this then becomes a statistical game about whether you can separate these two things from each other.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you look at the statistics, it turns out that the non matches have a different distribution to the true matches, so there not matches have this nice Gaussian distribution where we get an exponential distribution for the ones which should match with each other.",
                    "label": 0
                },
                {
                    "sent": "So to some extent we can use the idea of a quantum walk to distinguish between matching and non matching versus by looking at the values the amplitudes on the auxiliary vertices.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to finish off, just want to go on a little bit of a tangent, something that we're interested in now so.",
                    "label": 0
                },
                {
                    "sent": "One of the things we've looked at in quite a bit of detail is the idea of diffusion process on a graph.",
                    "label": 0
                },
                {
                    "sent": "So now we're into the domain of continuous time walks, so the idea of diffusion process is very straightforward.",
                    "label": 0
                },
                {
                    "sent": "We use the discrete Laplacian of a graph as an analog for the Laplace Beltrami operator on a manifold, and then we can define a differential equation representing the heat flow over the graph.",
                    "label": 0
                },
                {
                    "sent": "So it's a very straightforward equation and we can solve this to find the heat kernel of the graph, which is an interesting representation of the graph.",
                    "label": 0
                },
                {
                    "sent": "So essentially that's the same thing as a particular limit of a particular continuous time random walk over the graph.",
                    "label": 1
                },
                {
                    "sent": "Other things we can do, we can look at solutions of Schroedinger equation over a similar way.",
                    "label": 0
                },
                {
                    "sent": "Of course, in this case will get wavelike solutions, 'cause we're talking about essentially a graph which is in free space.",
                    "label": 0
                },
                {
                    "sent": "So these type of processes.",
                    "label": 0
                },
                {
                    "sent": "These are fairly well understood.",
                    "label": 0
                },
                {
                    "sent": "Now we're using a discrete Laplacian, so that means we have connectivity, so we can have weights on the edges, but we don't have any concept of length on these edges, so there's just connections between the different verses with different weights, but no concept of how far apart they are.",
                    "label": 0
                },
                {
                    "sent": "No geometry on the graph.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to talk about now really stems from some of the ideas of Joel Friedman here with Jean Pierre Tillisch.",
                    "label": 0
                },
                {
                    "sent": "So his idea was to take a geometric realization of the graph, an associated length, an interval with each edge, and then we're introducing a kind of metric structure into the graph, and we have distances between the vertices.",
                    "label": 1
                },
                {
                    "sent": "So then we can start to talk about how long it takes signals, for example, to propagate.",
                    "label": 0
                },
                {
                    "sent": "So how long it takes diffusion processes to propagate down the length of edges and so on.",
                    "label": 1
                },
                {
                    "sent": "So if you follow that idea, then you end up with this idea of a two part.",
                    "label": 1
                },
                {
                    "sent": "Fashion so vertex based part was essentially the same idea as a discrete Laplacian and an edge based part which has much more interesting properties.",
                    "label": 0
                },
                {
                    "sent": "So for the edge based part we're talking about functions which now exist on the edges on this interval associated with the edges.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So from that edge base of passing we can define the various analogs of these things that we had before, but edge based in edge based sense.",
                    "label": 0
                },
                {
                    "sent": "So we can have an edge based heat kernel.",
                    "label": 1
                },
                {
                    "sent": "An edge based diffusion.",
                    "label": 0
                },
                {
                    "sent": "We can define a wave equation now directly on these edges and we can even have things like a relativistic heat equation.",
                    "label": 1
                },
                {
                    "sent": "So the heat equation, which has finite propagation speed across the graph.",
                    "label": 0
                },
                {
                    "sent": "So I think these are interesting to us at the moment because in contrast to the discrete Laplacian, these last two equations exhibit finite propagation speeds, so there might be particularly interesting for modeling processes on graphs where the transmission time is of the same order as they kind of effects that you're looking at.",
                    "label": 1
                },
                {
                    "sent": "So if you use something with a finite speed, you can see different type of timing effects that you might not see using a discrete Laplacian.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the reason why I mentioned at the end of this talk is it turns out there's actually a connection between this edge based Laplacian and these discrete time equate continuous time equations and the type of walks that I've already been talking about.",
                    "label": 0
                },
                {
                    "sent": "So the eigensystem of this edge based lapasan comes in two parts.",
                    "label": 1
                },
                {
                    "sent": "So the first part sets of eigenfunctions which are supported on the vertices, so they have non zero values on the vertices and you can show that these eigenvalues and eigenfunctions are essentially determined.",
                    "label": 1
                },
                {
                    "sent": "By the structure of the random walk matrix T. So the eigenfunction, the eigenvectors, the random walk transition matrix, determine what these eigen functions are.",
                    "label": 1
                },
                {
                    "sent": "There's another set of Eigen functions which are zero on the vertices, so the unsupported eigen functions on the vertices actually turned out to be determined by the back track list.",
                    "label": 0
                },
                {
                    "sent": "Random Walk on the adjacency matrix of the oriented line graph.",
                    "label": 0
                },
                {
                    "sent": "So if we look at the eigen structure of the adjacency matrix, it tells us what these eigenfunctions are, which are not supported on the vertices, so this eigensystem.",
                    "label": 0
                },
                {
                    "sent": "This edge base Laplacian actually contains structure from both the classical random walk on the back track list random walk, so it's a particularly flexible approach.",
                    "label": 0
                },
                {
                    "sent": "To modeling the signal propagation on graphs.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "K so just finish off with just kind of summarize what I said.",
                    "label": 0
                },
                {
                    "sent": "So random walks are very powerful tools for analyzing network structure, and we've done a lot of work in Group at your car using random walks of various processes on graphs.",
                    "label": 1
                },
                {
                    "sent": "So summarize what we've done in various spheres with a random walk back track list, random walk and quantum walks, and this is kind of a more forward looking.",
                    "label": 0
                },
                {
                    "sent": "The edge based Laplace.",
                    "label": 0
                },
                {
                    "sent": "There's something that we're interested in at the moment because firstly it contains structure from both of these types of walks.",
                    "label": 1
                },
                {
                    "sent": "The random walk in the back track list walk an because of this finite speed of signal propagation bias, various differential equations.",
                    "label": 1
                },
                {
                    "sent": "So we're interested modeling systems where the transmission time across the network actually matters to the dynamics.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "You talked about how?",
                    "label": 0
                },
                {
                    "sent": "Be insensitive to noise.",
                    "label": 0
                },
                {
                    "sent": "Algebra Replaces Zero bayawan in the matrix.",
                    "label": 0
                },
                {
                    "sent": "It doesn't change the spectrum very much.",
                    "label": 0
                },
                {
                    "sent": "Is there a general theory for that, or is it just not relation?",
                    "label": 0
                },
                {
                    "sent": "So well, I was I was talking particularly about the commute time in that context.",
                    "label": 0
                },
                {
                    "sent": "So we look if you look at individual parts then you can destroy path by deleting an edge for example.",
                    "label": 0
                },
                {
                    "sent": "So it's not very noise resistant.",
                    "label": 0
                },
                {
                    "sent": "But in order to answer that second question.",
                    "label": 0
                },
                {
                    "sent": "So when you're looking at powers, basically it sums of powers of the adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "So once you do that process, I think it is.",
                    "label": 0
                },
                {
                    "sent": "I don't know exactly what the theorem is, I'm not mathematician, so not well versed in that, but.",
                    "label": 0
                },
                {
                    "sent": "01 So it's another one change to a matrix.",
                    "label": 0
                },
                {
                    "sent": "Send it yeah.",
                    "label": 0
                },
                {
                    "sent": "So it's not the case actually, so it depends exactly on the graph in the spectrum of the graph.",
                    "label": 0
                },
                {
                    "sent": "It depends on the eigen gaps between the various eigenvalues, so if it turns out that those eigen gaps actually closely related to the symmetry factors of the graph.",
                    "label": 0
                },
                {
                    "sent": "So if you have two graphs which don't have many automorphisms and they don't have any symmetry factors, then usually have a nicely spread spectrum and you can compare them.",
                    "label": 0
                },
                {
                    "sent": "They basically result in a nice eigensystem in space.",
                    "label": 0
                },
                {
                    "sent": "If they have a lot of symmetry then you can completely destroy that symmetry just by deleting the edges and you get a massive, flipping the eigensystem and they can end up looking completely different from each other.",
                    "label": 0
                },
                {
                    "sent": "So I think that's a general problem for these eigen based methods that they can't cope with certain types of graph they can cope with the type of graphs that you might normally see.",
                    "label": 0
                },
                {
                    "sent": "So if it's A kind of Erdos Renyi type graph, then you don't normally have any problem with your system.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "PD is known as the telegraphers equation.",
                    "label": 0
                },
                {
                    "sent": "So this is one of my.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Differential equations.",
                    "label": 0
                },
                {
                    "sent": "One of these.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the number 3.",
                    "label": 0
                },
                {
                    "sent": "Interesting that in all sorts of weird applications, including the highly channel and flow of radionuclides through groundwater and things like that where normal dispersion just isn't observed there very fast.",
                    "label": 0
                },
                {
                    "sent": "It's kind of interesting that turns up here, and that's something that we probably have some conversation about environmental applications.",
                    "label": 0
                },
                {
                    "sent": "And also I mean this.",
                    "label": 0
                },
                {
                    "sent": "This third equation has a kind of a bit of a checkered history.",
                    "label": 0
                },
                {
                    "sent": "So I mean my.",
                    "label": 0
                },
                {
                    "sent": "So my my original background surface of 1st degree was in physics, so obviously we did.",
                    "label": 0
                },
                {
                    "sent": "Far from it, so I guess that's true.",
                    "label": 0
                },
                {
                    "sent": "Quite a few people actually.",
                    "label": 0
                },
                {
                    "sent": "So obviously we study things like heat equation and we don't study things like this.",
                    "label": 0
                },
                {
                    "sent": "So the thing about this is that it doesn't have a particularly strong physical basis, and you can get unusual solution.",
                    "label": 0
                },
                {
                    "sent": "So this will also have wavelike solutions in certain cases.",
                    "label": 0
                },
                {
                    "sent": "And so that's something which is not people don't perceive as being a very heat like type of behavior.",
                    "label": 0
                },
                {
                    "sent": "It doesn't behave like diffusion.",
                    "label": 0
                },
                {
                    "sent": "Yeah, exactly so.",
                    "label": 0
                },
                {
                    "sent": "You know, I mean, if you have a system that's generally governed by an equation like this, then this is.",
                    "label": 0
                },
                {
                    "sent": "This is what you need to use, so I guess it's in that sense.",
                    "label": 0
                },
                {
                    "sent": "It's a very interesting question.",
                    "label": 0
                },
                {
                    "sent": "Good, I think it's very interesting equation.",
                    "label": 0
                },
                {
                    "sent": "Particularly this picture we could compare to networks and you could say whether they are the same, so as more frequently, yeah.",
                    "label": 0
                },
                {
                    "sent": "So just.",
                    "label": 0
                },
                {
                    "sent": "Real Life Network network network.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "So determine the similarity of networks.",
                    "label": 0
                },
                {
                    "sent": "So graph matching graph comparison is something that we've done a huge amount of work on over the last 20 years or so.",
                    "label": 0
                },
                {
                    "sent": "So not just by these type of methods.",
                    "label": 0
                },
                {
                    "sent": "I was just using that as a kind of example to motivate the quantum walk.",
                    "label": 0
                },
                {
                    "sent": "So the problem of determining whether two graphs are similar or not.",
                    "label": 0
                },
                {
                    "sent": "It's a very, very interesting problem and quite hard problem.",
                    "label": 0
                },
                {
                    "sent": "I think we have very good methods for graphs which are not so large, so in the order of hundreds of vertices, perhaps maybe up to 1000.",
                    "label": 0
                },
                {
                    "sent": "But when you're talking bout, presumably you're talking about working with very large graphs in the order, maybe.",
                    "label": 0
                },
                {
                    "sent": "Five site yeah, so once you go up to very large networks, 10s of thousands or up to millions of vertices then you need to adopt.",
                    "label": 0
                },
                {
                    "sent": "These methods are very efficient and then it becomes a very much more difficult problem.",
                    "label": 0
                },
                {
                    "sent": "So there I think you need to use much simpler characterizations of the graph rather than these type of matching methods.",
                    "label": 0
                },
                {
                    "sent": "So here we're looking for an explicit correspondence between the different vertices.",
                    "label": 0
                },
                {
                    "sent": "I think if you were talking about very large systems you need to not worry about that and just look for a comparator like like the graph kernels for example, which would be more efficient.",
                    "label": 0
                },
                {
                    "sent": "Jordan or.",
                    "label": 0
                },
                {
                    "sent": "It seems like everything you've done it in this talk has been deterministic.",
                    "label": 0
                },
                {
                    "sent": "If we thought about throwing a bit of noise into the graphs and then define their tests.",
                    "label": 0
                },
                {
                    "sent": "I say we haven't.",
                    "label": 0
                },
                {
                    "sent": "I think it's a good idea.",
                    "label": 0
                },
                {
                    "sent": "But I found this stuff that Mark was talking about about grass networks that evolve overtime fascinating, so I think would be very interested in looking at those type of problems, whether the structure might change from one time step to the next and what effect that has on walks for example.",
                    "label": 0
                },
                {
                    "sent": "So I think that's a very interesting question and one.",
                    "label": 0
                },
                {
                    "sent": "Rather than buy up.",
                    "label": 0
                },
                {
                    "sent": "Terministic take statistics about what happens in various equations.",
                    "label": 0
                },
                {
                    "sent": "OK, I think that's that's an interesting question.",
                    "label": 0
                },
                {
                    "sent": "It doesn't, I can't immediately say that that's going to give any positive benefit or not.",
                    "label": 0
                },
                {
                    "sent": "OK, so so many probably knows better than me.",
                    "label": 0
                },
                {
                    "sent": "In fact, started noise on this table.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK yeah yeah, interesting idea.",
                    "label": 0
                },
                {
                    "sent": "Too short.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Additional information that come from work brings compared to the other work I understand, for example, that under more schedule information about the structure it was even used in some algorithm performed over there, not detect some communities.",
                    "label": 0
                },
                {
                    "sent": "So I I didn't understand what additional information we have about this picture.",
                    "label": 0
                },
                {
                    "sent": "When you look to wave equation or something that is 1 equation that we are really excited about is the FKP equation which is not linear also have reaction term in addition to the diffusion.",
                    "label": 0
                },
                {
                    "sent": "It's important because it's this type of disease starts.",
                    "label": 0
                },
                {
                    "sent": "To start with this and something that.",
                    "label": 0
                },
                {
                    "sent": "That's the fucking point diffusion.",
                    "label": 0
                },
                {
                    "sent": "Fisher it shouldn't have linearity, OK?",
                    "label": 0
                },
                {
                    "sent": "IS 1 -- F term OK?",
                    "label": 0
                },
                {
                    "sent": "I'm not familiar with that to be honest, so I think that second question is a difficult one for me to answer.",
                    "label": 0
                },
                {
                    "sent": "So as well the first question was.",
                    "label": 0
                },
                {
                    "sent": "Is yeah.",
                    "label": 0
                },
                {
                    "sent": "I didn't.",
                    "label": 0
                },
                {
                    "sent": "So I mean, I don't know how familiar with quantum algorithms in general, but.",
                    "label": 0
                },
                {
                    "sent": "Quantum evolution of the system seems to be more calm, potentially powerful in some ways than classical evolution of the system.",
                    "label": 0
                },
                {
                    "sent": "So in this case we're comparing a classical random walk to a quantum walk.",
                    "label": 0
                },
                {
                    "sent": "It's essentially interference patterns that we are on the graph, which will reveal more information about the structure.",
                    "label": 0
                },
                {
                    "sent": "So for example, there's a particular special case where you glue true trees together, so the top of one tree at one side and the top of another sign.",
                    "label": 0
                },
                {
                    "sent": "Then you blew the leaves together and you can show for a binary tree that a quantum walk will end up at the other side.",
                    "label": 0
                },
                {
                    "sent": "Exponentially more quickly than a classical random walk.",
                    "label": 0
                },
                {
                    "sent": "So essentially you can probe the structure of a particular types of graph more efficiently with quantum.",
                    "label": 0
                },
                {
                    "sent": "What the account with the classical walk, and that's because of this interference patterns that you get in the graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so sort.",
                    "label": 0
                },
                {
                    "sent": "If I understand correctly.",
                    "label": 0
                },
                {
                    "sent": "So and in the classical random walk you are completely so the person that the Walker is completely stupid.",
                    "label": 0
                },
                {
                    "sent": "So in the right of one note and can return in the in the same way to the to the previous node, but in the in the quantum one you penalize in some way to return to the previous one.",
                    "label": 0
                },
                {
                    "sent": "So it's in some way guided to go to the target.",
                    "label": 0
                },
                {
                    "sent": "This is that correct?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I think it's no.",
                    "label": 0
                },
                {
                    "sent": "Probability is the same.",
                    "label": 0
                },
                {
                    "sent": "Negative.",
                    "label": 0
                },
                {
                    "sent": "But the probability is the square of the amplitude is you just have this interference effect?",
                    "label": 0
                },
                {
                    "sent": "So I mean it's to take a physical analogy.",
                    "label": 0
                },
                {
                    "sent": "It's like the idea of quantum tunneling, so quantum particle couldn't pass through a barrier in arriving at the side button necessarily being observed in between the two, whereas a classical part of would just be stopped and can't penetrate the other side, so the interference patterns allow you to penetrate through certain types of graphs more quickly than the classical random walk, and that's.",
                    "label": 0
                },
                {
                    "sent": "Regulation for example.",
                    "label": 0
                },
                {
                    "sent": "Population, the declaration threshold tells you OK if I'm below the threshold, there is a giant component which arose from one side to the other one, and that's fine.",
                    "label": 0
                },
                {
                    "sent": "Now, if you take the point of declaration program which is not solved knowledge, there is a constant declaration threshold which is larger than the classical threshold, and the difference is due because there's no conduction because of interference effect.",
                    "label": 0
                },
                {
                    "sent": "But I still don't understand what what you learn from that.",
                    "label": 0
                },
                {
                    "sent": "You know there's no what's.",
                    "label": 0
                },
                {
                    "sent": "Do you know something about the giant destruction of design cluster or you know?",
                    "label": 0
                },
                {
                    "sent": "I think it's a nontrivial problem to actually connect the properties of a walk in general, except in certain special cases and the structure of the graph, so I think if you're looking for a clear kind of message about what, what can I use the quantum?",
                    "label": 0
                },
                {
                    "sent": "What type of structure can I probe with the quantum walk?",
                    "label": 0
                },
                {
                    "sent": "I can't with the classical work, for example, I don't think there's an easy answer to that yet.",
                    "label": 0
                },
                {
                    "sent": "I mean, these ideas are still maturing, I think to a certain extent, and I'm not aware of any clear definition of exactly what this type of structure that you can probe in one way with the quantum Walker.",
                    "label": 0
                },
                {
                    "sent": "And then you will get a different answer for other work.",
                    "label": 0
                },
                {
                    "sent": "I think at the moment we're still limited to the type of things I've showed you here, right.",
                    "label": 0
                },
                {
                    "sent": "Particular examples where the quantum walk behaves differently from the classical walk like the one I showed you here, or like this idea of being able to pass through certain graphs more quickly than the classical walk.",
                    "label": 0
                },
                {
                    "sent": "I don't think there's a kind of general answer to that problem yet.",
                    "label": 0
                }
            ]
        }
    }
}