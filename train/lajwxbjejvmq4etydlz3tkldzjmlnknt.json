{
    "id": "lajwxbjejvmq4etydlz3tkldzjmlnknt",
    "title": "Algorithms and Hardness for Robust Subspace Recovery",
    "info": {
        "author": [
            "Moritz Hardt, Google, Inc."
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_hardt_algorithms/",
    "segmentation": [
        [
            "Alright yeah, thanks for coming.",
            "So this works about algorithms and hardness for robust subspace recovery and this is joint work with Encore Moitra.",
            "So let me tell you about the very basic geometric."
        ],
        [
            "Problem that will look at.",
            "In this talk we call it robust subspace recovery and the problem is very simple to state your given endpoints in RN so endpoints and N dimensional space and the points of the property that some fraction of them are so called inliers, and that means they are contained in a lower dimensional subspace, and that subspace is unknown.",
            "And I'll denote the dimension of it by D. So think of D is less than the host dimension, or it's less than North and think of it as much less than N. So maybe an over 100.",
            "And you don't know what the inliers are, and you don't know the subspace and the remaining points.",
            "Anything that's not an inlier is an outlier, and the only thing that I'll assume about them is that they were sort of general positions, so they don't any any of them spend our end, so they're not in any lower dimensional subspace, but other than that, they're just completely adversarial.",
            "OK, so we're starting a very strong model where the where the outliers are completely corrupted and adversarially corrupted points.",
            "OK, and the goal is to recover the subspace.",
            "So in two dimensions this is relatively easy."
        ],
        [
            "You can see that here five points are in a lower dimensional subspace in a 1 dimensional subspace and the other points are outliers.",
            "Unfortunately, when we go to higher dimension."
        ],
        [
            "The dilemma is that all the algorithms we know for this problem are either not robust to these kind of strong adversarial outliers, or they're not computationally efficient.",
            "So this is exactly what this talk is about asking if we can have both.",
            "So let me give you to reinforce this dilemma.",
            "Let me give you 2 examples of classical algorithm."
        ],
        [
            "So the first one is the famous least squares algorithm, so you want to find the subspace that minimize the sum of squared Euclidean distances to the points.",
            "You could talk about other distances, it wouldn't matter for this example.",
            "So if the points that look something like this, it works pretty well.",
            "We find the subspace, but it's great because it sufficient we know how to do this in polynomial time.",
            "Unfortunately, it just takes one adversarial outlier to completely perturb the subspace that this algorithm finds to something that's not reasonable.",
            "OK, so it's not robust in that sense.",
            "It's not robust.",
            "Even a single adversarial outlier that shows you how strong the outlier model is."
        ],
        [
            "The other hand, so there was a whole community of people in robust statistics who looked at this and studied this robust subspace recovery problem from the 70s on.",
            "And there's there's a famous heuristic.",
            "They're called the least median of scores.",
            "Heuristic Detour, CF, which says instead of minimizing the sum of Euclidean distances, just minimize the median of Euclidean distances and median sounds good.",
            "That sounds like it will be more robust, and indeed, if this is the point that I can throw in almost 1/2 fraction of outliers.",
            "And I do not perturb the I don't perturb the subspace, so this is great this robust.",
            "Unfortunately, it's computationally infeasible to two in general to compute the least median of square subspace, so this is a heuristic, but we cannot solve this in polynomial time in general, so these two, these two examples motivate the following."
        ],
        [
            "Definitions that already appeared and the robust stats, terminology, and the first ones that have a breakdown point.",
            "So we're going to say that an estimator by which I mean just an algorithm has a breakdown point of P. So P is a real number.",
            "If it recovers the subspace from any P fraction of outliers.",
            "So you throw in any P fraction of adversarial outliers, it'll still be able to recover the subspace.",
            "So that's the breakdown point.",
            "And the statistical threshold is the highest breakdown point of any statistical estimator.",
            "So any algorithm, no matter what the running time is.",
            "So for example, we're saves algorithm is a statistical estimator, so that's valid for the.",
            "In this notion, statistical threshold.",
            "But we're interested in this talk, is computational complexity, and in particular what we'll call the computational threshold, which is the highest breakdown point of any computationally efficient estimator.",
            "So it should be clear to you that the.",
            "Computational thresholds always less than this statistical threshold can never be larger, so the question is what is it?",
            "And the first the first, the first notion of statistical threshold has been very well studied in robust statistics and the second one is not well understood.",
            "So that's what we'll look at in this talk and."
        ],
        [
            "In fact, we will manage to pin down the precise computational threshold of robust subspace recovery and will show that it's 1 / 1 -- D over North.",
            "Remember these the dimension of the subspace and is the host space dimension.",
            "So to give you an example of these, just an over 100, then we can recover.",
            "We have an algorithm that recovers from 99% outliers, but we also show that recovering from 99.1% outliers for this setting of parameters is intractable.",
            "OK, makes sense.",
            "Sunjoy I felt it.",
            "So it is but.",
            "Yeah, I don't.",
            "I don't think it has a very nice expression like this depends, but you can, for example always get 50%, whereas the computational threshold isn't isn't always 50%.",
            "So yeah, so in particular this result separates the computational statistical threshold, which is similar to what we saw in Quentin stock.",
            "So OK and of course we never show anything in computational complexity, we just derive it from complexity theoretic assumptions, and so there again, there's an implausible complexity assumption hidden here that that is, you know that we used to get a hardness result.",
            "So in particular, what you'll see in this talk is that we get new algorithms, so we've got an efficient algorithm that recovers the subspace from 1 -- 2 / N fraction of outliers.",
            "So this gives us the upper bound that we need, and for the hardness result will use this problem known as small set expansion and will show that if you manage to improve over this 1 -- 3 / N threshold by just some positive constant epsilon then you will refute what's known as the small Set expansion conjecture in complexity theory, which is plausible hardness assumption.",
            "OK, so at the very least, if you make progress on this question it will be a major algorithmic breakthrough.",
            "OK, and there is also something I won't talk about, which is a really interesting new convex program for this problem and a duality principle which shows that the feasibility of robust subspace recovery is actually dual to a very natural geometric condition that arises in functional analysis and that gives some insight into the problem that we thought was interesting.",
            "OK, so this is roughly what I'll talk about if the results are clear, all move on.",
            "There's a lot of related work and the set."
        ],
        [
            "So people have studied robust, robust PCA before and have tried to come up with efficient algorithms for it.",
            "For example, there's the well known work by Condessa Del.",
            "That's the first one I mentioned here, where you have a low rank matrix with arbitrarily corrupted entries, but the location of the entries is uniformly at random chosen, and they managed to recover the original matrix.",
            "So here in our setting we have arbitrarily corrupted column so that we have like 99% of the points are adversarially corrupted, so it's a different setting.",
            "There's also paperback Sue at all, where they also consider corrupted points, but there's a different objective function so they don't recover the subspace.",
            "Rather they find a subspace which has similar variance to the original subspace.",
            "They're also different assumptions here that lead to different results.",
            "OK, and there are many variants of least squares.",
            "They all have the same behavior in terms of breakdown point, so there are more robust, maybe in like certain settings, but for this notion of outlier there are not.",
            "So this is just some of the some of the related work.",
            "If you have more questions about that, come talk to me.",
            "There's a lot of work here.",
            "You didn't mention that one regression, right?",
            "So L1 regression.",
            "If you look at L1 norm in several two norm, it has the same behavior in terms of breakdown point.",
            "So it just takes one arbitrary adversarial outlier to perturb it, so the breakdown point is 0, even though for like small perturbations it's more robust than Euclidean norm.",
            "The points are inexact subspace.",
            "That's right, that's right, they are inside.",
            "But we allow lots and lots of outliers.",
            "Right, so this is analogous to the first kind of set of results.",
            "We have a low rank matrix plus a sparse matrix.",
            "Here we have a low rank matrix with lots of corrupted columns.",
            "Quinton open note exactly inside the right so you could look at probabilistic models, but under these assumptions that we have the weak assumptions on the outliers.",
            "Even just adding a little bit of Gaussian noise to the inliers will make it hard to recover so."
        ],
        [
            "Let me give you an overview of what I'll talk about in this for the remainder of the talk, I'll mention a randomized the simple randomized algorithm that gives us the upper bound.",
            "Then I'll come to the connection to small set expansion, tell you about the hardness result, and then I'll give you your homework assignment, which is to look at the paper and read up on the convex program and duality.",
            "But I won't have time to talk about it.",
            "OK."
        ],
        [
            "So here is the simple randomized algorithm, and here's what it gives.",
            "So it shows that if a set of points contains at least Dearborn fraction of inliers, an meets a condition one that I'm about to state, then we can find the hidden subspace and randomized polynomial time.",
            "So what's the condition?",
            "It's basically just what I indicated before a set of endpoints is linearly independent if and only if it contains.",
            "At most the inliers.",
            "So that sort of says the only way endpoints could have a linear dependencies but containing too many inliers, and that just means that outliers stoned don't have linear dependencies, so it's sort of general position.",
            "But it's a, it's a, it's a mild.",
            "Deterministic assumption in particular, is no random randomness involved in this assumption.",
            "OK, and so under this condition, which essentially just the general position assumption, we can recover from a dealer.",
            "In fraction of inliers.",
            "We can also discuss 2 back to contents question.",
            "We can relax a little bit if you have a separation and determinant so you don't really need linear dependencies.",
            "You can talk about determinants instead.",
            "Gives you some approximate condition.",
            "I don't really want to get into it, but you can relax it a little bit, but if you go too far, the results start to breakdown.",
            "OK, so let me tell you what this album?"
        ],
        [
            "So this is the entire algorithm.",
            "It's very simple.",
            "So you now think of the input as a matrix and the columns X one through XMR.",
            "The input points, so it's an N by N matrix.",
            "It has N rows and N columns, and all the algorithm is doing.",
            "Essentially, is it repeatedly samples a set of endpoints, and it checks for a linear dependence.",
            "So we sample set V of random, upsize North from the set of columns and we look if the minor of a determined by these set of endpoints.",
            "So then by and minor is rank deficient.",
            "So if it has a linear dependence.",
            "Then we say, OK fine, there's a non zero vector in the kernel of that matrix.",
            "So we find any such vector and we look at IT support and then all the algorithm does is it says OK take the columns corresponding to the support of the linear dependence and that defines the basis of your subspace.",
            "Alright, so output the subspace spanned by the corresponding columns.",
            "And we repeat that until you know, we find a linear dependence, so I need to show you two things I need to show you that.",
            "First of all, we do find a linear dependence in a reasonable amount of time and then second from the linear dependence we can in fact correctly recover the subspace.",
            "Alright, so let's see first why we find a linear dependence.",
            "So it should be clear to you that the expected number of inliers, in the sample V, is exactly deep.",
            "And that's because the fraction of inliers is deal ran, and we sample endpoints.",
            "So the expectation of the number of inliers is just exactly D very simple.",
            "Argument and.",
            "Remember that if the sample does not contain D. Inliers, but D + 1 inliers, since you're all in a D dimensional subspace, then we must have a linear dependency.",
            "OK, so as soon as we accidentally sample a little bit more than expected inliers, we have a linear dependency.",
            "So sampling D plus one or more inliers will lead to linear dependency, and will put us into this case.",
            "OK, and that happens with pretty good probability, typically constant for reasonable parameters.",
            "OK, and then I we also need to show that the support of the dependence reveals the inliers reveals the location of the inliers, and this is where we use condition.",
            "Want what we're going to argue is that the support of the linear dependence can only be like on points that correspond to inliers on coordinates.",
            "That correspond to inliers, and the reason is just that by this condition, one outliers cannot participate in the linear dependencies.",
            "So the support of the non zero vector in the kernel must, in fact the only supported on.",
            "On end layers.",
            "And there's a little bit of an argument, but if you think about it for awhile, you'll see why this is true.",
            "OK, so this is all it takes to analyze this algorithm.",
            "This gives a very simple, very simple upper bound and will next will show that it's actually optimal so.",
            "That's the algorithm, let me."
        ],
        [
            "About hardness, and I don't want to give you the formal statement wider way, but the informal statement would be that improving over the threshold of 1 -- 3 / N would refute the small set expansion hypothesis that there was recently introducing complexity theory, and so I want to tell you what the small set expansion hypothesis is and what the small expansion problem is.",
            "How many people have you have heard about the small set expansion problem?",
            "Not too many, so it's good that I'll go over this slowly."
        ],
        [
            "OK, so this is the small set expansion problem.",
            "You're given a graph G and let's assume for the purpose of this talk that it's regular.",
            "So the degree is exactly capital, Delta, and.",
            "The expansion of a set S, so if S is set of vertices, the expansion is just the number of outgoing edges normalized by the largest possible number of outgoing edges, which is Delta times S, so that's a number between zero and one.",
            "It's the fraction of outgoing edges, and that's the expansion of a set S. OK, and the small set expansion problem, which I'll denote by SSC.",
            "It has two parameters, Epsilon, Delta that are not so important.",
            "It says given a graph G, decide if you know which of the following two cases is true and the first case is basically that fall sets S that are size Delta times the number of vertices.",
            "So think of this as all small sets fall small sets.",
            "It's true that the expansion of that set is at least one minus epsilon.",
            "So the first case is that almost all small sets have almost all of their at sorry.",
            "All small sets have almost all of their edges going out.",
            "OK, this is just what this says and Delta controls how small the set sizes and epsilon controls how many edges are going out.",
            "OK, this is the first case.",
            "All small sets are expanding and then no cases.",
            "The other extreme where there exists a set S small set which has almost all of its edges staying inside, so only an epsilon fraction of the edges are going out.",
            "So the small set expansion problem is to decide between these two extreme cases, and I don't care what you do in between.",
            "And the hypothesis.",
            "It's a pretty strong hardness conjecture.",
            "It says that even though this these cases are so extreme for every epsilon, there is a Delta such that the corresponding small set expansion problem is NP hard.",
            "Sorry, I have a gaps written right here.",
            "Don't just delete that.",
            "It's the same problem.",
            "OK, so the conjecture just says that this small set expansion problem is hard.",
            "OK, so how do we use this for our problem?"
        ],
        [
            "Let me just give you the intuition.",
            "So we need to reduce the small set expansion problem to the robust subspace recovery problem.",
            "So we need to go from a graph to a matrix and the matrix.",
            "Very simple, it's just a vbe matrix where every column corresponds to an edge.",
            "So the points just have two non 00 in coordinates and I'll choose the coefficients of these coordinates at random.",
            "So these are just random numbers between zero and one doesn't really matter and that's my that's my reduction.",
            "So just from the graph I designed this rectangular matrix where every column is an edge whose nonzero coordinates are the endpoints of the edge with random coefficients.",
            "Very simple reduction and the reason why I picked the end points at random so that the only linear dependencies.",
            "In this matrices in this matrix are due to coordinate subspaces.",
            "OK, so these random coefficients have have a useful role."
        ],
        [
            "So what do I need to show?",
            "I need to show two things.",
            "One is the completeness direction, which is, it turns out to be easy, which is that if you have a small nonexpanding set in your graph, OK, so set of small size which has almost all its edges staying inside like this one.",
            "Then you want to argue that there exists a low dimensional coordinate subspace containing many points and it's very to do so.",
            "It's very easy to do so the subspace given by the set is just all the edges and S that stay inside.",
            "OK, so you look at all the columns for which the corresponding edges are inside as.",
            "OK, so this gives you a small low dimensional coordinate subspace that contains too many points and you can plug in the parameter and figure it out.",
            "Sound is the other direction which turns out to be a bit more difficult.",
            "You would want to say that if there is a subspace containing many points then there's actually a coordinate subspace containing many points and from this coordinate subspace you can find a small nonexpanding set so the other direction I won't be able to give you the formal proof, but it's it's not too hard, so let me."
        ],
        [
            "Come up, what do we do?",
            "We pin down the computational threshold of a natural formalization of robust subspace recovery.",
            "And it was important for us to get the tight result.",
            "There is also important to us that we had no distributional assumptions here, and we allowed very strong outliers.",
            "Adversarial outliers.",
            "OK, and there are many interesting connections here.",
            "Some I wasn't able to talk about others I showed you like the connection to small set expansion, so it's worth worth exploring this further.",
            "And finally, you know this came up during the talk.",
            "Can we relax the exact subspace containment to something like approximate subspace containment?",
            "Is it possible to analyze the computational threshold of these relaxations so that's a natural question?",
            "But I tell you I just want to give you an impression for what kind of difficulties you're going to run into so."
        ],
        [
            "The challenge with extending our results.",
            "So if you want to have adversarial or deterministic assumptions on the points, then it's difficult design algorithms with provable guarantees, so the algorithms are very likely to breakdown, and you're not going to have.",
            "Algorithms under these weak assumptions, on the other hand, because your assumptions are so weak, it's sort of feasible to come up with the hardness result.",
            "You're talking about.",
            "A worst case problem, so you might be able to prove hardness.",
            "Even though that's also not always easy.",
            "On the other hand, if you go to distributional assumptions on points, then you have the opposite problem.",
            "It's easy to design algorithms, but it's much harder to prove hardness results because you now have this nasty average case problem, and there are very few average case reductions in complexity theory.",
            "So you somehow need to maneuver these two extremes and we've sort of found a sweet spot where we can have matching upper and lower bounds.",
            "So open."
        ],
        [
            "Problems there.",
            "I think it's a beautiful area that many open questions here.",
            "Pick your favorite problem and unsupervised learning introduce adverse aerial outliers.",
            "Try to understand the computational threshold.",
            "Very little is known, and I think there are all of these problems are very difficult and beautiful.",
            "So there are many other problems you could be looking at.",
            "Other formalizations of robust PCA, and I think it's quite worthwhile and more broadly, what we kind of want to understand is the role of complexity theory and unsupervised learning.",
            "So what is the tradeoff between robust robustness and computational efficiency in unsupervised learning?",
            "And there's clearly a tension between between the two, and it be nice to have more results that get the tradeoff between the two, so that's all I wanted to say."
        ],
        [
            "Questions, let me know."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright yeah, thanks for coming.",
                    "label": 0
                },
                {
                    "sent": "So this works about algorithms and hardness for robust subspace recovery and this is joint work with Encore Moitra.",
                    "label": 1
                },
                {
                    "sent": "So let me tell you about the very basic geometric.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem that will look at.",
                    "label": 0
                },
                {
                    "sent": "In this talk we call it robust subspace recovery and the problem is very simple to state your given endpoints in RN so endpoints and N dimensional space and the points of the property that some fraction of them are so called inliers, and that means they are contained in a lower dimensional subspace, and that subspace is unknown.",
                    "label": 1
                },
                {
                    "sent": "And I'll denote the dimension of it by D. So think of D is less than the host dimension, or it's less than North and think of it as much less than N. So maybe an over 100.",
                    "label": 0
                },
                {
                    "sent": "And you don't know what the inliers are, and you don't know the subspace and the remaining points.",
                    "label": 0
                },
                {
                    "sent": "Anything that's not an inlier is an outlier, and the only thing that I'll assume about them is that they were sort of general positions, so they don't any any of them spend our end, so they're not in any lower dimensional subspace, but other than that, they're just completely adversarial.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're starting a very strong model where the where the outliers are completely corrupted and adversarially corrupted points.",
                    "label": 0
                },
                {
                    "sent": "OK, and the goal is to recover the subspace.",
                    "label": 0
                },
                {
                    "sent": "So in two dimensions this is relatively easy.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see that here five points are in a lower dimensional subspace in a 1 dimensional subspace and the other points are outliers.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, when we go to higher dimension.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The dilemma is that all the algorithms we know for this problem are either not robust to these kind of strong adversarial outliers, or they're not computationally efficient.",
                    "label": 1
                },
                {
                    "sent": "So this is exactly what this talk is about asking if we can have both.",
                    "label": 0
                },
                {
                    "sent": "So let me give you to reinforce this dilemma.",
                    "label": 0
                },
                {
                    "sent": "Let me give you 2 examples of classical algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first one is the famous least squares algorithm, so you want to find the subspace that minimize the sum of squared Euclidean distances to the points.",
                    "label": 1
                },
                {
                    "sent": "You could talk about other distances, it wouldn't matter for this example.",
                    "label": 0
                },
                {
                    "sent": "So if the points that look something like this, it works pretty well.",
                    "label": 0
                },
                {
                    "sent": "We find the subspace, but it's great because it sufficient we know how to do this in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, it just takes one adversarial outlier to completely perturb the subspace that this algorithm finds to something that's not reasonable.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not robust in that sense.",
                    "label": 0
                },
                {
                    "sent": "It's not robust.",
                    "label": 0
                },
                {
                    "sent": "Even a single adversarial outlier that shows you how strong the outlier model is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other hand, so there was a whole community of people in robust statistics who looked at this and studied this robust subspace recovery problem from the 70s on.",
                    "label": 0
                },
                {
                    "sent": "And there's there's a famous heuristic.",
                    "label": 0
                },
                {
                    "sent": "They're called the least median of scores.",
                    "label": 1
                },
                {
                    "sent": "Heuristic Detour, CF, which says instead of minimizing the sum of Euclidean distances, just minimize the median of Euclidean distances and median sounds good.",
                    "label": 0
                },
                {
                    "sent": "That sounds like it will be more robust, and indeed, if this is the point that I can throw in almost 1/2 fraction of outliers.",
                    "label": 0
                },
                {
                    "sent": "And I do not perturb the I don't perturb the subspace, so this is great this robust.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, it's computationally infeasible to two in general to compute the least median of square subspace, so this is a heuristic, but we cannot solve this in polynomial time in general, so these two, these two examples motivate the following.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Definitions that already appeared and the robust stats, terminology, and the first ones that have a breakdown point.",
                    "label": 0
                },
                {
                    "sent": "So we're going to say that an estimator by which I mean just an algorithm has a breakdown point of P. So P is a real number.",
                    "label": 1
                },
                {
                    "sent": "If it recovers the subspace from any P fraction of outliers.",
                    "label": 1
                },
                {
                    "sent": "So you throw in any P fraction of adversarial outliers, it'll still be able to recover the subspace.",
                    "label": 0
                },
                {
                    "sent": "So that's the breakdown point.",
                    "label": 1
                },
                {
                    "sent": "And the statistical threshold is the highest breakdown point of any statistical estimator.",
                    "label": 0
                },
                {
                    "sent": "So any algorithm, no matter what the running time is.",
                    "label": 0
                },
                {
                    "sent": "So for example, we're saves algorithm is a statistical estimator, so that's valid for the.",
                    "label": 1
                },
                {
                    "sent": "In this notion, statistical threshold.",
                    "label": 0
                },
                {
                    "sent": "But we're interested in this talk, is computational complexity, and in particular what we'll call the computational threshold, which is the highest breakdown point of any computationally efficient estimator.",
                    "label": 0
                },
                {
                    "sent": "So it should be clear to you that the.",
                    "label": 0
                },
                {
                    "sent": "Computational thresholds always less than this statistical threshold can never be larger, so the question is what is it?",
                    "label": 0
                },
                {
                    "sent": "And the first the first, the first notion of statistical threshold has been very well studied in robust statistics and the second one is not well understood.",
                    "label": 0
                },
                {
                    "sent": "So that's what we'll look at in this talk and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In fact, we will manage to pin down the precise computational threshold of robust subspace recovery and will show that it's 1 / 1 -- D over North.",
                    "label": 1
                },
                {
                    "sent": "Remember these the dimension of the subspace and is the host space dimension.",
                    "label": 0
                },
                {
                    "sent": "So to give you an example of these, just an over 100, then we can recover.",
                    "label": 0
                },
                {
                    "sent": "We have an algorithm that recovers from 99% outliers, but we also show that recovering from 99.1% outliers for this setting of parameters is intractable.",
                    "label": 0
                },
                {
                    "sent": "OK, makes sense.",
                    "label": 0
                },
                {
                    "sent": "Sunjoy I felt it.",
                    "label": 0
                },
                {
                    "sent": "So it is but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't think it has a very nice expression like this depends, but you can, for example always get 50%, whereas the computational threshold isn't isn't always 50%.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so in particular this result separates the computational statistical threshold, which is similar to what we saw in Quentin stock.",
                    "label": 0
                },
                {
                    "sent": "So OK and of course we never show anything in computational complexity, we just derive it from complexity theoretic assumptions, and so there again, there's an implausible complexity assumption hidden here that that is, you know that we used to get a hardness result.",
                    "label": 0
                },
                {
                    "sent": "So in particular, what you'll see in this talk is that we get new algorithms, so we've got an efficient algorithm that recovers the subspace from 1 -- 2 / N fraction of outliers.",
                    "label": 1
                },
                {
                    "sent": "So this gives us the upper bound that we need, and for the hardness result will use this problem known as small set expansion and will show that if you manage to improve over this 1 -- 3 / N threshold by just some positive constant epsilon then you will refute what's known as the small Set expansion conjecture in complexity theory, which is plausible hardness assumption.",
                    "label": 0
                },
                {
                    "sent": "OK, so at the very least, if you make progress on this question it will be a major algorithmic breakthrough.",
                    "label": 1
                },
                {
                    "sent": "OK, and there is also something I won't talk about, which is a really interesting new convex program for this problem and a duality principle which shows that the feasibility of robust subspace recovery is actually dual to a very natural geometric condition that arises in functional analysis and that gives some insight into the problem that we thought was interesting.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is roughly what I'll talk about if the results are clear, all move on.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of related work and the set.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So people have studied robust, robust PCA before and have tried to come up with efficient algorithms for it.",
                    "label": 0
                },
                {
                    "sent": "For example, there's the well known work by Condessa Del.",
                    "label": 0
                },
                {
                    "sent": "That's the first one I mentioned here, where you have a low rank matrix with arbitrarily corrupted entries, but the location of the entries is uniformly at random chosen, and they managed to recover the original matrix.",
                    "label": 0
                },
                {
                    "sent": "So here in our setting we have arbitrarily corrupted column so that we have like 99% of the points are adversarially corrupted, so it's a different setting.",
                    "label": 0
                },
                {
                    "sent": "There's also paperback Sue at all, where they also consider corrupted points, but there's a different objective function so they don't recover the subspace.",
                    "label": 0
                },
                {
                    "sent": "Rather they find a subspace which has similar variance to the original subspace.",
                    "label": 0
                },
                {
                    "sent": "They're also different assumptions here that lead to different results.",
                    "label": 0
                },
                {
                    "sent": "OK, and there are many variants of least squares.",
                    "label": 1
                },
                {
                    "sent": "They all have the same behavior in terms of breakdown point, so there are more robust, maybe in like certain settings, but for this notion of outlier there are not.",
                    "label": 0
                },
                {
                    "sent": "So this is just some of the some of the related work.",
                    "label": 0
                },
                {
                    "sent": "If you have more questions about that, come talk to me.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of work here.",
                    "label": 0
                },
                {
                    "sent": "You didn't mention that one regression, right?",
                    "label": 0
                },
                {
                    "sent": "So L1 regression.",
                    "label": 0
                },
                {
                    "sent": "If you look at L1 norm in several two norm, it has the same behavior in terms of breakdown point.",
                    "label": 1
                },
                {
                    "sent": "So it just takes one arbitrary adversarial outlier to perturb it, so the breakdown point is 0, even though for like small perturbations it's more robust than Euclidean norm.",
                    "label": 0
                },
                {
                    "sent": "The points are inexact subspace.",
                    "label": 1
                },
                {
                    "sent": "That's right, that's right, they are inside.",
                    "label": 0
                },
                {
                    "sent": "But we allow lots and lots of outliers.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is analogous to the first kind of set of results.",
                    "label": 0
                },
                {
                    "sent": "We have a low rank matrix plus a sparse matrix.",
                    "label": 0
                },
                {
                    "sent": "Here we have a low rank matrix with lots of corrupted columns.",
                    "label": 0
                },
                {
                    "sent": "Quinton open note exactly inside the right so you could look at probabilistic models, but under these assumptions that we have the weak assumptions on the outliers.",
                    "label": 0
                },
                {
                    "sent": "Even just adding a little bit of Gaussian noise to the inliers will make it hard to recover so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me give you an overview of what I'll talk about in this for the remainder of the talk, I'll mention a randomized the simple randomized algorithm that gives us the upper bound.",
                    "label": 0
                },
                {
                    "sent": "Then I'll come to the connection to small set expansion, tell you about the hardness result, and then I'll give you your homework assignment, which is to look at the paper and read up on the convex program and duality.",
                    "label": 1
                },
                {
                    "sent": "But I won't have time to talk about it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the simple randomized algorithm, and here's what it gives.",
                    "label": 0
                },
                {
                    "sent": "So it shows that if a set of points contains at least Dearborn fraction of inliers, an meets a condition one that I'm about to state, then we can find the hidden subspace and randomized polynomial time.",
                    "label": 1
                },
                {
                    "sent": "So what's the condition?",
                    "label": 1
                },
                {
                    "sent": "It's basically just what I indicated before a set of endpoints is linearly independent if and only if it contains.",
                    "label": 0
                },
                {
                    "sent": "At most the inliers.",
                    "label": 0
                },
                {
                    "sent": "So that sort of says the only way endpoints could have a linear dependencies but containing too many inliers, and that just means that outliers stoned don't have linear dependencies, so it's sort of general position.",
                    "label": 0
                },
                {
                    "sent": "But it's a, it's a, it's a mild.",
                    "label": 0
                },
                {
                    "sent": "Deterministic assumption in particular, is no random randomness involved in this assumption.",
                    "label": 0
                },
                {
                    "sent": "OK, and so under this condition, which essentially just the general position assumption, we can recover from a dealer.",
                    "label": 0
                },
                {
                    "sent": "In fraction of inliers.",
                    "label": 0
                },
                {
                    "sent": "We can also discuss 2 back to contents question.",
                    "label": 0
                },
                {
                    "sent": "We can relax a little bit if you have a separation and determinant so you don't really need linear dependencies.",
                    "label": 0
                },
                {
                    "sent": "You can talk about determinants instead.",
                    "label": 0
                },
                {
                    "sent": "Gives you some approximate condition.",
                    "label": 0
                },
                {
                    "sent": "I don't really want to get into it, but you can relax it a little bit, but if you go too far, the results start to breakdown.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me tell you what this album?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the entire algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                },
                {
                    "sent": "So you now think of the input as a matrix and the columns X one through XMR.",
                    "label": 0
                },
                {
                    "sent": "The input points, so it's an N by N matrix.",
                    "label": 0
                },
                {
                    "sent": "It has N rows and N columns, and all the algorithm is doing.",
                    "label": 0
                },
                {
                    "sent": "Essentially, is it repeatedly samples a set of endpoints, and it checks for a linear dependence.",
                    "label": 0
                },
                {
                    "sent": "So we sample set V of random, upsize North from the set of columns and we look if the minor of a determined by these set of endpoints.",
                    "label": 1
                },
                {
                    "sent": "So then by and minor is rank deficient.",
                    "label": 0
                },
                {
                    "sent": "So if it has a linear dependence.",
                    "label": 0
                },
                {
                    "sent": "Then we say, OK fine, there's a non zero vector in the kernel of that matrix.",
                    "label": 0
                },
                {
                    "sent": "So we find any such vector and we look at IT support and then all the algorithm does is it says OK take the columns corresponding to the support of the linear dependence and that defines the basis of your subspace.",
                    "label": 0
                },
                {
                    "sent": "Alright, so output the subspace spanned by the corresponding columns.",
                    "label": 0
                },
                {
                    "sent": "And we repeat that until you know, we find a linear dependence, so I need to show you two things I need to show you that.",
                    "label": 1
                },
                {
                    "sent": "First of all, we do find a linear dependence in a reasonable amount of time and then second from the linear dependence we can in fact correctly recover the subspace.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let's see first why we find a linear dependence.",
                    "label": 1
                },
                {
                    "sent": "So it should be clear to you that the expected number of inliers, in the sample V, is exactly deep.",
                    "label": 0
                },
                {
                    "sent": "And that's because the fraction of inliers is deal ran, and we sample endpoints.",
                    "label": 0
                },
                {
                    "sent": "So the expectation of the number of inliers is just exactly D very simple.",
                    "label": 0
                },
                {
                    "sent": "Argument and.",
                    "label": 0
                },
                {
                    "sent": "Remember that if the sample does not contain D. Inliers, but D + 1 inliers, since you're all in a D dimensional subspace, then we must have a linear dependency.",
                    "label": 0
                },
                {
                    "sent": "OK, so as soon as we accidentally sample a little bit more than expected inliers, we have a linear dependency.",
                    "label": 1
                },
                {
                    "sent": "So sampling D plus one or more inliers will lead to linear dependency, and will put us into this case.",
                    "label": 0
                },
                {
                    "sent": "OK, and that happens with pretty good probability, typically constant for reasonable parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, and then I we also need to show that the support of the dependence reveals the inliers reveals the location of the inliers, and this is where we use condition.",
                    "label": 0
                },
                {
                    "sent": "Want what we're going to argue is that the support of the linear dependence can only be like on points that correspond to inliers on coordinates.",
                    "label": 0
                },
                {
                    "sent": "That correspond to inliers, and the reason is just that by this condition, one outliers cannot participate in the linear dependencies.",
                    "label": 0
                },
                {
                    "sent": "So the support of the non zero vector in the kernel must, in fact the only supported on.",
                    "label": 0
                },
                {
                    "sent": "On end layers.",
                    "label": 0
                },
                {
                    "sent": "And there's a little bit of an argument, but if you think about it for awhile, you'll see why this is true.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is all it takes to analyze this algorithm.",
                    "label": 0
                },
                {
                    "sent": "This gives a very simple, very simple upper bound and will next will show that it's actually optimal so.",
                    "label": 0
                },
                {
                    "sent": "That's the algorithm, let me.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About hardness, and I don't want to give you the formal statement wider way, but the informal statement would be that improving over the threshold of 1 -- 3 / N would refute the small set expansion hypothesis that there was recently introducing complexity theory, and so I want to tell you what the small set expansion hypothesis is and what the small expansion problem is.",
                    "label": 0
                },
                {
                    "sent": "How many people have you have heard about the small set expansion problem?",
                    "label": 1
                },
                {
                    "sent": "Not too many, so it's good that I'll go over this slowly.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the small set expansion problem.",
                    "label": 0
                },
                {
                    "sent": "You're given a graph G and let's assume for the purpose of this talk that it's regular.",
                    "label": 0
                },
                {
                    "sent": "So the degree is exactly capital, Delta, and.",
                    "label": 0
                },
                {
                    "sent": "The expansion of a set S, so if S is set of vertices, the expansion is just the number of outgoing edges normalized by the largest possible number of outgoing edges, which is Delta times S, so that's a number between zero and one.",
                    "label": 0
                },
                {
                    "sent": "It's the fraction of outgoing edges, and that's the expansion of a set S. OK, and the small set expansion problem, which I'll denote by SSC.",
                    "label": 0
                },
                {
                    "sent": "It has two parameters, Epsilon, Delta that are not so important.",
                    "label": 0
                },
                {
                    "sent": "It says given a graph G, decide if you know which of the following two cases is true and the first case is basically that fall sets S that are size Delta times the number of vertices.",
                    "label": 0
                },
                {
                    "sent": "So think of this as all small sets fall small sets.",
                    "label": 0
                },
                {
                    "sent": "It's true that the expansion of that set is at least one minus epsilon.",
                    "label": 0
                },
                {
                    "sent": "So the first case is that almost all small sets have almost all of their at sorry.",
                    "label": 0
                },
                {
                    "sent": "All small sets have almost all of their edges going out.",
                    "label": 0
                },
                {
                    "sent": "OK, this is just what this says and Delta controls how small the set sizes and epsilon controls how many edges are going out.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the first case.",
                    "label": 0
                },
                {
                    "sent": "All small sets are expanding and then no cases.",
                    "label": 0
                },
                {
                    "sent": "The other extreme where there exists a set S small set which has almost all of its edges staying inside, so only an epsilon fraction of the edges are going out.",
                    "label": 0
                },
                {
                    "sent": "So the small set expansion problem is to decide between these two extreme cases, and I don't care what you do in between.",
                    "label": 0
                },
                {
                    "sent": "And the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "It's a pretty strong hardness conjecture.",
                    "label": 0
                },
                {
                    "sent": "It says that even though this these cases are so extreme for every epsilon, there is a Delta such that the corresponding small set expansion problem is NP hard.",
                    "label": 1
                },
                {
                    "sent": "Sorry, I have a gaps written right here.",
                    "label": 0
                },
                {
                    "sent": "Don't just delete that.",
                    "label": 0
                },
                {
                    "sent": "It's the same problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so the conjecture just says that this small set expansion problem is hard.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we use this for our problem?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me just give you the intuition.",
                    "label": 0
                },
                {
                    "sent": "So we need to reduce the small set expansion problem to the robust subspace recovery problem.",
                    "label": 0
                },
                {
                    "sent": "So we need to go from a graph to a matrix and the matrix.",
                    "label": 0
                },
                {
                    "sent": "Very simple, it's just a vbe matrix where every column corresponds to an edge.",
                    "label": 0
                },
                {
                    "sent": "So the points just have two non 00 in coordinates and I'll choose the coefficients of these coordinates at random.",
                    "label": 0
                },
                {
                    "sent": "So these are just random numbers between zero and one doesn't really matter and that's my that's my reduction.",
                    "label": 0
                },
                {
                    "sent": "So just from the graph I designed this rectangular matrix where every column is an edge whose nonzero coordinates are the endpoints of the edge with random coefficients.",
                    "label": 0
                },
                {
                    "sent": "Very simple reduction and the reason why I picked the end points at random so that the only linear dependencies.",
                    "label": 0
                },
                {
                    "sent": "In this matrices in this matrix are due to coordinate subspaces.",
                    "label": 1
                },
                {
                    "sent": "OK, so these random coefficients have have a useful role.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do I need to show?",
                    "label": 0
                },
                {
                    "sent": "I need to show two things.",
                    "label": 0
                },
                {
                    "sent": "One is the completeness direction, which is, it turns out to be easy, which is that if you have a small nonexpanding set in your graph, OK, so set of small size which has almost all its edges staying inside like this one.",
                    "label": 0
                },
                {
                    "sent": "Then you want to argue that there exists a low dimensional coordinate subspace containing many points and it's very to do so.",
                    "label": 1
                },
                {
                    "sent": "It's very easy to do so the subspace given by the set is just all the edges and S that stay inside.",
                    "label": 0
                },
                {
                    "sent": "OK, so you look at all the columns for which the corresponding edges are inside as.",
                    "label": 0
                },
                {
                    "sent": "OK, so this gives you a small low dimensional coordinate subspace that contains too many points and you can plug in the parameter and figure it out.",
                    "label": 0
                },
                {
                    "sent": "Sound is the other direction which turns out to be a bit more difficult.",
                    "label": 0
                },
                {
                    "sent": "You would want to say that if there is a subspace containing many points then there's actually a coordinate subspace containing many points and from this coordinate subspace you can find a small nonexpanding set so the other direction I won't be able to give you the formal proof, but it's it's not too hard, so let me.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Come up, what do we do?",
                    "label": 0
                },
                {
                    "sent": "We pin down the computational threshold of a natural formalization of robust subspace recovery.",
                    "label": 1
                },
                {
                    "sent": "And it was important for us to get the tight result.",
                    "label": 0
                },
                {
                    "sent": "There is also important to us that we had no distributional assumptions here, and we allowed very strong outliers.",
                    "label": 0
                },
                {
                    "sent": "Adversarial outliers.",
                    "label": 0
                },
                {
                    "sent": "OK, and there are many interesting connections here.",
                    "label": 0
                },
                {
                    "sent": "Some I wasn't able to talk about others I showed you like the connection to small set expansion, so it's worth worth exploring this further.",
                    "label": 0
                },
                {
                    "sent": "And finally, you know this came up during the talk.",
                    "label": 0
                },
                {
                    "sent": "Can we relax the exact subspace containment to something like approximate subspace containment?",
                    "label": 1
                },
                {
                    "sent": "Is it possible to analyze the computational threshold of these relaxations so that's a natural question?",
                    "label": 0
                },
                {
                    "sent": "But I tell you I just want to give you an impression for what kind of difficulties you're going to run into so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The challenge with extending our results.",
                    "label": 1
                },
                {
                    "sent": "So if you want to have adversarial or deterministic assumptions on the points, then it's difficult design algorithms with provable guarantees, so the algorithms are very likely to breakdown, and you're not going to have.",
                    "label": 1
                },
                {
                    "sent": "Algorithms under these weak assumptions, on the other hand, because your assumptions are so weak, it's sort of feasible to come up with the hardness result.",
                    "label": 0
                },
                {
                    "sent": "You're talking about.",
                    "label": 0
                },
                {
                    "sent": "A worst case problem, so you might be able to prove hardness.",
                    "label": 0
                },
                {
                    "sent": "Even though that's also not always easy.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you go to distributional assumptions on points, then you have the opposite problem.",
                    "label": 0
                },
                {
                    "sent": "It's easy to design algorithms, but it's much harder to prove hardness results because you now have this nasty average case problem, and there are very few average case reductions in complexity theory.",
                    "label": 0
                },
                {
                    "sent": "So you somehow need to maneuver these two extremes and we've sort of found a sweet spot where we can have matching upper and lower bounds.",
                    "label": 0
                },
                {
                    "sent": "So open.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problems there.",
                    "label": 0
                },
                {
                    "sent": "I think it's a beautiful area that many open questions here.",
                    "label": 0
                },
                {
                    "sent": "Pick your favorite problem and unsupervised learning introduce adverse aerial outliers.",
                    "label": 0
                },
                {
                    "sent": "Try to understand the computational threshold.",
                    "label": 1
                },
                {
                    "sent": "Very little is known, and I think there are all of these problems are very difficult and beautiful.",
                    "label": 0
                },
                {
                    "sent": "So there are many other problems you could be looking at.",
                    "label": 0
                },
                {
                    "sent": "Other formalizations of robust PCA, and I think it's quite worthwhile and more broadly, what we kind of want to understand is the role of complexity theory and unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So what is the tradeoff between robust robustness and computational efficiency in unsupervised learning?",
                    "label": 1
                },
                {
                    "sent": "And there's clearly a tension between between the two, and it be nice to have more results that get the tradeoff between the two, so that's all I wanted to say.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions, let me know.",
                    "label": 0
                }
            ]
        }
    }
}