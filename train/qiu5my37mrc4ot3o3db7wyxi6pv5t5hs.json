{
    "id": "qiu5my37mrc4ot3o3db7wyxi6pv5t5hs",
    "title": "Kernels for Link Prediction with Latent Feature Models",
    "info": {
        "author": [
            "Canh Hao Nguyen, School of Knowledge Science, Japan Advanced Institute of Science and Technology (JAIST)"
        ],
        "published": "Oct. 3, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_nguyen_kernels/",
    "segmentation": [
        [
            "Good morning everybody.",
            "My name is going home when it is only done with Hiroshima Metrika.",
            "We are from my informatics center, Kyoto University, Japan.",
            "The title is kernel for link prediction with latent feature models."
        ],
        [
            "OK so I gave a brief interaction about the link prediction problems and I after that I do latent feature model to model the structure of the network that we want to predict the links and then because we cannot change that model.",
            "So we use kernels for that to change that and we do some experiments and come."
        ],
        [
            "Blue tint.",
            "OK, this is a link prediction problems that we are supposed to have.",
            "Graph the networks we want to graph site layout Gray color network over there.",
            "What we want to put some more links into that later right links over there according to some model.",
            "That is, we have to design and it is certainly you hear a lot about this before and it is our application actually in systems biology.",
            "Anne.",
            "Usually in the Sky problem there are two kind of information people to predict links.",
            "One is the information of the nodes like you even two nodes having certain information and you say OK given that you know that they have link between them just so you can kind of information in the new network structure that OK you're given this network structure of them must be LinkedIn here because Instructure had to follow certain patterns and we actually following the second part."
        ],
        [
            "OK, one of the fundamental assumption is link prediction is that one of the user way is that they represent links into some.",
            "Space somewhere, so because the link is a pen object, so actually you'd be on the path of taking some space and you do some usual learning problem that so when you do the usual learning problem that zip user assumption.",
            "If you have an idea assumption of the links.",
            "Actually when you take every pairs with emoji, assume that they are they have an IID distribution this."
        ],
        [
            "Actually.",
            "You some sort of network structure you cannot take into account.",
            "Have representation, that's why we network structure."
        ],
        [
            "And certainly network structure is not strength.",
            "We hear every time about scale free network here bout CLT network whereas an exchange network means that the two nodes are similar.",
            "And we also hear about Bipartite network collaborative filtering center and what we want to do with that.",
            "We want to model the network with latent feature models from our data of course, and we want to be able to say that some of the biological networks, like PPI Network's in regulatory network should follow this model."
        ],
        [
            "OK, what we want to do today is that we want to model your network structure or network topology with latent features and then we had to drive the data and picture model for networks that we actually targeting.",
            "Non semantic networks.",
            "So the network data LinkedIn network doesn't mean that just similar.",
            "It had to be something not similar.",
            "And then we try to approximate the model using kernels and then we learn the model optimally and efficiently and after that we apply the link prediction problem on.",
            "Biological networks, so the framework is lighted, so would you have a you are given?",
            "OK, so you are given an adjacency matrix here.",
            "This is where supposed to encode the network structure.",
            "And then you are going to put a model into that, and according to the model you are not able to put every links here into a space.",
            "And every sales here that's supposed to not have link into a space.",
            "And we're going to do when I do a classification here and using this model will not go back and say OK, this is about to be a link over there.",
            "So that is a framework."
        ],
        [
            "OK, so I'm going to talk about the latent feature model that is a generative model."
        ],
        [
            "Network structure.",
            "OK, I will.",
            "Motivation is from protein protein interaction that you know, that is the proteins like this and protein slices sometime one put in doc and other proteins and discreetly create a physical interaction with the two proteins and it is hosting create the whole Putin Putin interaction networks.",
            "Anne.",
            "We know that Putin.",
            "Putin is based on domain or interaction site, so this part of an interaction sites.",
            "The reason they are able to interact with each other because they are complementary in shape.",
            "So when they're talking to each other, so this shape here, actually a latent feature of the protein, you know, some of them we don't know too much about them so.",
            "Yeah, latent feature where we want to be able to use the network structure to infer the Skype thing.",
            "And."
        ],
        [
            "This is a model.",
            "Actually we have certain latent feature of the protein somewhere, then some of them actually interact following these rules because they complement each other in the shapes.",
            "And then if you have a set of nodes in the networks and it snowed this aback latent features and you look around and see OK this guy and this guy having the pair of latent features that interact, so is there going to be a links in here color coded with the.",
            "Interaction they have here so GD basically the generative process of our.",
            "Network."
        ],
        [
            "Structure.",
            "OK, so in the.",
            "Model there are two matrices.",
            "One is a latent feature matrix.",
            "It is magic encoding.",
            "What latent feature nodes have.",
            "An example here.",
            "They have their first latent feature, but not the others.",
            "The second kind of information we need to take into the model with the feature interaction matrix.",
            "So it's actually saying here is the first latent feature interact with the second latent feature, and this thing is this thing together going to produce model like this.",
            "So we will have multiple F is latent feature matrix multiplied with.",
            "Teacher interaction matrix and is a chance for work.",
            "Latent feature metrican product of these three matrix is going to produce.",
            "This big matrix is supposed to be the adjacency matrix according to the model."
        ],
        [
            "Influenza model is one of the newspaper as far as I know is they do some Indian buffet process is to enforce model.",
            "Let's suppose that you're going to sample the matrixes and to group these nodes together and saying that this node having one latent feature in common and then join.",
            "So saying that this group having one latent feature in common, so that's how they sample the latent feature matrix.",
            "And then there's you see there are many links between the two groups here.",
            "You're saying OK, this latent feature and latent Fisher?",
            "Tend to interact with each other, so we infer that there's a rest.",
            "OK, these remaining links have to be there in the model, and they do it using Gibbs Sampling's and one of.",
            "The problem with that is very time consuming and scalable to lay some 100 nodes, and we are modeling abilities and."
        ],
        [
            "OK, so since we have a problem come with the model, we try to encode them in kernel instead."
        ],
        [
            "So just record that kernel support.",
            "We import similarity between object in the same class.",
            "Here we want to classify links into link klappan non links collapse like rectify pairs of nodes into links and links.",
            "So what we want we want to have a similarity between the links and similarity between Zenon links class and as the link is a passive node.",
            "So what we want from the very beginning they're going to change the two nodes are similar.",
            "We want to have a kernel on the nodes and saying that the notes are similar they share.",
            "Manylath"
        ],
        [
            "And teacher, so ideally we would like to have some kernel wins and load something later.",
            "That is a great number.",
            "A common latent feature they have, but this is something that we want to have, but we don't have because we don't have the model."
        ],
        [
            "An after that we want to design the kernel in that it should be closed.",
            "Do the ideal corner.",
            "Hello guys, this is magically.",
            "Here is a diagonal matrix encoding the weights of big features and we are assuming that the feature."
        ],
        [
            "OK, so we don't have latent feature, but what we can observe with the what we want to happen.",
            "The node are similar if they have more latent feature in common, but we observed with a note when they have more latent feature incumbent they have more neighborhood in common.",
            "So what we use something very very simple.",
            "That's where the efficiency metrics saying it is.",
            "Amount of common neighborhood they share."
        ],
        [
            "Actually, the good news is that in and we're working with sparse networks.",
            "In order to model sparse network, we have to use powers models and we show some results in the end.",
            "In sparse model model, the kernel redefined here actually close to the ideal kernel that we want to have."
        ],
        [
            "OK, so we having a kernels on the nodes and we just follow what people propose compare quite Standard Time to defy the symmetric between links by middle of similarity between nodes by region.",
            "Try to measure different alignments and multiply the kernel and add them together.",
            "And this is basically proposed before."
        ],
        [
            "So we want to demonstrate the ideas of our kernel computed from our data.",
            "Suppose that we have a kernel model like this, so this matrix is going to generate the.",
            "Adjacency matrix does it, but we don't have the full magic.",
            "But what will happen and incomplete observation?",
            "So this is what we have.",
            "The data we built our kernel function load on this and we do a kernel PCA just to visualize it and see where the links and grass and on links and as the missing links and we see here is on the blue line here as further links club and we see a grouping in the three patterns.",
            "The links here actually the misspelling according to the model.",
            "So happy to see that there actually.",
            "Very similar patterns as a link to the testing data following the same distribution.",
            "That training data.",
            "Here we also look at the other class of non links laptop here and we see that in Arlington Lab following a different pattern like this and here you have different patterns.",
            "So by doing that we are able to money to put on the notes.",
            "Actually when you look at Baxter model and see which other note that have the same features, we look at that lately.",
            "Second feature for example, you see the agent node with the second picture actually group in.",
            "Here your reasoning that they share many common neighborhood and just first feature is going to be something in here that's going to be in here.",
            "This node shared by the have two latent features, so we have to stay somewhere in between.",
            "So we we don't generate the binary feature but we actually put them.",
            "Somewhat close in some space."
        ],
        [
            "So we show some properties about kernel, for example, positivities.",
            "We're saying that if they share.",
            "Some latent feature according to the model.",
            "Then we should be able.",
            "To recognize that by giving a non positive values on that the second property like we.",
            "Under certain condition caused when they them or no later than the more neighborhood they have anymore color values we have so actually the kernel we define here up data actually reflects some properties of the ideas journals, so that close in some sense."
        ],
        [
            "We actually tried to study when the grandeur Colonel generated from data according to the model clauses.",
            "Actually idea code at exactly what we want and we show the condition when I will define kernel.",
            "Actually idea Colonel will show that there's some formula.",
            "Actually this matrix actually have own logo vector and correlated.",
            "So we also show some qualities of this thing.",
            "And I'm say some in some usual situation that we actually.",
            "Then have the ideas they know by just computing it from data.",
            "When we try to work on the sparse network and sparse network is model.",
            "With sparse model an we what we found here that we have one way to parameterise how spa model is.",
            "By by this kind of formula, so I don't talk about that, but it is one of the way, not the only way to say how sparse model is.",
            "And then we can manage to limit when this one is small enough.",
            "Then the kernel modify is close enough to the idea of Colonel, so I've actually right about the difference between our kernels and ideas going to based on the sparsity of the model.",
            "According to our parameterisation here."
        ],
        [
            "OK, so that's the properties of the kernels that we try to device for the links according to the latent teacher models."
        ],
        [
            "So.",
            "We do some experiment on that so we don't try to be anything funny, so we actually use the protein, protein interaction networks and we extracted them from DDI database, have hand curated experimentally validated data and largest possible understand we want to go to work with the network website 6000 or something like that and we show running time we compare the assumptions.",
            "An prediction you see on them."
        ],
        [
            "OK, so why we need to use latent feature model?",
            "Because a lot of people are using the simple like similarity assumption on the kernels like you see the work some parts we just compare simple whether the similarity assumption work here and actually when we plot AUC, doing a nearest neighbor classifier.",
            "Here we see that.",
            "Symmetry assumption needed.",
            "Something up here and we use a latent teacher model.",
            "We are going up here to 87% of AUC.",
            "So what we learn from here?",
            "OK, similarly, Simpson is significantly better than random that in certain part of General Network we had some celebrity in there, but the rest of them, this part, you know, we get some better performance because some part of the network there is no similarity in their cause.",
            "We managed to that because the latent teacher model actually can encode certain type of similarity network as well."
        ],
        [
            "About we try to compare the time of our method with the proposed method using Indian buffet process is and what we see that this is her own.",
            "The network we have.",
            "This is a network.",
            "We figure out the small decrease less than 10 per sample and we have a small network here which is like 700 nodes.",
            "Something too.",
            "We have something like 6000 nodes along the line.",
            "Here the time we run our method actually from 90 seconds to less than 500 seconds.",
            "So there's 6 minutes.",
            "On SVM and when we try to use Indian buffet process on this smallest subnetwork of 700, something notes what we we?",
            "What we plot here is that they look like uh-huh the model and see how it converts overtime an until the time that we actually get some rather stable.",
            "We see on the test set it is about 1/2 day for the smallest subnetwork.",
            "So our method actually."
        ],
        [
            "Faster.",
            "We do apply link prediction on East PPI Networks and digital result so we can so use a linear kernel or that we put a Gaussian kernel on top of our defined kernel and what we expected result is this Gaussian kernels is linear kernel.",
            "There is also.",
            "Indian buffet process somewhere here with slightly less.",
            "That's not that much less, but The thing is that the reason is missing.",
            "Your reason is that we let them run for days and it doesn't converge to live at one model.",
            "So we just give up so the rest are missing here.",
            "We also tried to compare the.",
            "Prediction based on network structure with the other method.",
            "Prediction based on the attributes of the node.",
            "Here, Attribution nodes are the sequence and we use spectrum kernels and for the problem we usually get you zero.",
            "71% GT is with like 50% less than what we are doing here.",
            "So the reason we thought is the sequence light spectrum corner they use on the sub sequences so they have too much redundant information in there and they just really hinder the performance in here because network actually encode what is relevant on the network.",
            "So we actually.",
            "I need to get the relevant information from net."
        ],
        [
            "Exactly the same story can be seen from fruit fly prepare network that OK in in October 2nd little bit less, but doesn't scale projects of the data and we managed to have something like that.",
            "Rather high is much higher than random.",
            "So we also say that these the network structure network do have structure and we managed to get them.",
            "To make some, you see much better than 0.5 and also didn't so better than using spectrum color, which is like 65% something."
        ],
        [
            "OK, So what we try to do?",
            "We try to argue to use the network structure to predict links and for our problem we relate and teacher model to model their network structure and then in order to train that we will actually try to change your latent teacher model.",
            "But we try to enforce the property of latent teacher model using kernels and we show some good results in terms of times an performance.",
            "On paper net."
        ],
        [
            "So our method is efficient and effective for latent feature models.",
            "So sparse network actually.",
            "OK, so that's it for me.",
            "Thank you very much.",
            "Questions.",
            "Thank you actually I have clarification question.",
            "Essentially you input is a similarity matrix.",
            "And at some point you said that we you used to compare, you used features of nodes.",
            "So my question, can you combine both?",
            "So if there is any way to inject node features in your link prediction kernel, have you ever tried and this kind of combination?",
            "Yes we do that.",
            "OK, we have a kind of based on the network structure and then so we have kernel based on attributes of the nodes and we just do multiple color learning on top of that.",
            "We tried that.",
            "The thing is that the spectrum kernels for the node using those information they contained too much information and they absorb into things when you combine them just getting worse, but it should be.",
            "Shouldn't be a problem actually problem, but it doesn't get better.",
            "That's the thing.",
            "But still I mean it looks a bit strange that you can't manage, because sometimes this information is complementary.",
            "What gets from the network?",
            "It might be not always the same, but you get in features, so they should help each other to some point.",
            "Our hypothesis is that when the info only information like in the application, we have to mention that should be included in the sequence.",
            "Usually some people try to do that from sequences.",
            "What part of the sequences response without we don't know.",
            "So actually using the network itself, we're actually selecting only the relevant information.",
            "OK, thank you.",
            "Other questions.",
            "Well, I have one so I was wondering if you see room in your methods to incorporate some domain knowledge, for instance, well, you worked with protein protein interaction but.",
            "If you were to work with some models of social networks, you know we hear that there are these laws, you know, preferential attachment.",
            "What have you.",
            "So I think in certain types of networks you would have some sort of domain knowledge about the type of growth that you can expect, and could you incorporate that with the kernel.",
            "This type of girl?",
            "Yeah, I would say yes.",
            "You have to look at what kind of domain knowledge we have in order to put into an.",
            "We talked about certain kind of domain knowledge, something like that.",
            "We're trying to classify different type of interaction or different type type of links and if we have that kind of pattern then we might be able to put it in there.",
            "It is possible, but it could be difficult elsewhere.",
            "OK, let's thank the speaker again.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good morning everybody.",
                    "label": 0
                },
                {
                    "sent": "My name is going home when it is only done with Hiroshima Metrika.",
                    "label": 0
                },
                {
                    "sent": "We are from my informatics center, Kyoto University, Japan.",
                    "label": 0
                },
                {
                    "sent": "The title is kernel for link prediction with latent feature models.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I gave a brief interaction about the link prediction problems and I after that I do latent feature model to model the structure of the network that we want to predict the links and then because we cannot change that model.",
                    "label": 0
                },
                {
                    "sent": "So we use kernels for that to change that and we do some experiments and come.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Blue tint.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a link prediction problems that we are supposed to have.",
                    "label": 1
                },
                {
                    "sent": "Graph the networks we want to graph site layout Gray color network over there.",
                    "label": 0
                },
                {
                    "sent": "What we want to put some more links into that later right links over there according to some model.",
                    "label": 0
                },
                {
                    "sent": "That is, we have to design and it is certainly you hear a lot about this before and it is our application actually in systems biology.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Usually in the Sky problem there are two kind of information people to predict links.",
                    "label": 0
                },
                {
                    "sent": "One is the information of the nodes like you even two nodes having certain information and you say OK given that you know that they have link between them just so you can kind of information in the new network structure that OK you're given this network structure of them must be LinkedIn here because Instructure had to follow certain patterns and we actually following the second part.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, one of the fundamental assumption is link prediction is that one of the user way is that they represent links into some.",
                    "label": 1
                },
                {
                    "sent": "Space somewhere, so because the link is a pen object, so actually you'd be on the path of taking some space and you do some usual learning problem that so when you do the usual learning problem that zip user assumption.",
                    "label": 1
                },
                {
                    "sent": "If you have an idea assumption of the links.",
                    "label": 0
                },
                {
                    "sent": "Actually when you take every pairs with emoji, assume that they are they have an IID distribution this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "You some sort of network structure you cannot take into account.",
                    "label": 0
                },
                {
                    "sent": "Have representation, that's why we network structure.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And certainly network structure is not strength.",
                    "label": 0
                },
                {
                    "sent": "We hear every time about scale free network here bout CLT network whereas an exchange network means that the two nodes are similar.",
                    "label": 0
                },
                {
                    "sent": "And we also hear about Bipartite network collaborative filtering center and what we want to do with that.",
                    "label": 1
                },
                {
                    "sent": "We want to model the network with latent feature models from our data of course, and we want to be able to say that some of the biological networks, like PPI Network's in regulatory network should follow this model.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, what we want to do today is that we want to model your network structure or network topology with latent features and then we had to drive the data and picture model for networks that we actually targeting.",
                    "label": 0
                },
                {
                    "sent": "Non semantic networks.",
                    "label": 0
                },
                {
                    "sent": "So the network data LinkedIn network doesn't mean that just similar.",
                    "label": 0
                },
                {
                    "sent": "It had to be something not similar.",
                    "label": 0
                },
                {
                    "sent": "And then we try to approximate the model using kernels and then we learn the model optimally and efficiently and after that we apply the link prediction problem on.",
                    "label": 1
                },
                {
                    "sent": "Biological networks, so the framework is lighted, so would you have a you are given?",
                    "label": 0
                },
                {
                    "sent": "OK, so you are given an adjacency matrix here.",
                    "label": 0
                },
                {
                    "sent": "This is where supposed to encode the network structure.",
                    "label": 0
                },
                {
                    "sent": "And then you are going to put a model into that, and according to the model you are not able to put every links here into a space.",
                    "label": 0
                },
                {
                    "sent": "And every sales here that's supposed to not have link into a space.",
                    "label": 0
                },
                {
                    "sent": "And we're going to do when I do a classification here and using this model will not go back and say OK, this is about to be a link over there.",
                    "label": 0
                },
                {
                    "sent": "So that is a framework.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to talk about the latent feature model that is a generative model.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Network structure.",
                    "label": 0
                },
                {
                    "sent": "OK, I will.",
                    "label": 0
                },
                {
                    "sent": "Motivation is from protein protein interaction that you know, that is the proteins like this and protein slices sometime one put in doc and other proteins and discreetly create a physical interaction with the two proteins and it is hosting create the whole Putin Putin interaction networks.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We know that Putin.",
                    "label": 0
                },
                {
                    "sent": "Putin is based on domain or interaction site, so this part of an interaction sites.",
                    "label": 1
                },
                {
                    "sent": "The reason they are able to interact with each other because they are complementary in shape.",
                    "label": 0
                },
                {
                    "sent": "So when they're talking to each other, so this shape here, actually a latent feature of the protein, you know, some of them we don't know too much about them so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, latent feature where we want to be able to use the network structure to infer the Skype thing.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a model.",
                    "label": 0
                },
                {
                    "sent": "Actually we have certain latent feature of the protein somewhere, then some of them actually interact following these rules because they complement each other in the shapes.",
                    "label": 0
                },
                {
                    "sent": "And then if you have a set of nodes in the networks and it snowed this aback latent features and you look around and see OK this guy and this guy having the pair of latent features that interact, so is there going to be a links in here color coded with the.",
                    "label": 0
                },
                {
                    "sent": "Interaction they have here so GD basically the generative process of our.",
                    "label": 0
                },
                {
                    "sent": "Network.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Structure.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the.",
                    "label": 0
                },
                {
                    "sent": "Model there are two matrices.",
                    "label": 0
                },
                {
                    "sent": "One is a latent feature matrix.",
                    "label": 1
                },
                {
                    "sent": "It is magic encoding.",
                    "label": 0
                },
                {
                    "sent": "What latent feature nodes have.",
                    "label": 0
                },
                {
                    "sent": "An example here.",
                    "label": 0
                },
                {
                    "sent": "They have their first latent feature, but not the others.",
                    "label": 0
                },
                {
                    "sent": "The second kind of information we need to take into the model with the feature interaction matrix.",
                    "label": 0
                },
                {
                    "sent": "So it's actually saying here is the first latent feature interact with the second latent feature, and this thing is this thing together going to produce model like this.",
                    "label": 0
                },
                {
                    "sent": "So we will have multiple F is latent feature matrix multiplied with.",
                    "label": 0
                },
                {
                    "sent": "Teacher interaction matrix and is a chance for work.",
                    "label": 1
                },
                {
                    "sent": "Latent feature metrican product of these three matrix is going to produce.",
                    "label": 0
                },
                {
                    "sent": "This big matrix is supposed to be the adjacency matrix according to the model.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Influenza model is one of the newspaper as far as I know is they do some Indian buffet process is to enforce model.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose that you're going to sample the matrixes and to group these nodes together and saying that this node having one latent feature in common and then join.",
                    "label": 0
                },
                {
                    "sent": "So saying that this group having one latent feature in common, so that's how they sample the latent feature matrix.",
                    "label": 1
                },
                {
                    "sent": "And then there's you see there are many links between the two groups here.",
                    "label": 0
                },
                {
                    "sent": "You're saying OK, this latent feature and latent Fisher?",
                    "label": 0
                },
                {
                    "sent": "Tend to interact with each other, so we infer that there's a rest.",
                    "label": 0
                },
                {
                    "sent": "OK, these remaining links have to be there in the model, and they do it using Gibbs Sampling's and one of.",
                    "label": 0
                },
                {
                    "sent": "The problem with that is very time consuming and scalable to lay some 100 nodes, and we are modeling abilities and.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so since we have a problem come with the model, we try to encode them in kernel instead.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just record that kernel support.",
                    "label": 0
                },
                {
                    "sent": "We import similarity between object in the same class.",
                    "label": 1
                },
                {
                    "sent": "Here we want to classify links into link klappan non links collapse like rectify pairs of nodes into links and links.",
                    "label": 0
                },
                {
                    "sent": "So what we want we want to have a similarity between the links and similarity between Zenon links class and as the link is a passive node.",
                    "label": 1
                },
                {
                    "sent": "So what we want from the very beginning they're going to change the two nodes are similar.",
                    "label": 0
                },
                {
                    "sent": "We want to have a kernel on the nodes and saying that the notes are similar they share.",
                    "label": 1
                },
                {
                    "sent": "Manylath",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And teacher, so ideally we would like to have some kernel wins and load something later.",
                    "label": 0
                },
                {
                    "sent": "That is a great number.",
                    "label": 0
                },
                {
                    "sent": "A common latent feature they have, but this is something that we want to have, but we don't have because we don't have the model.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An after that we want to design the kernel in that it should be closed.",
                    "label": 0
                },
                {
                    "sent": "Do the ideal corner.",
                    "label": 0
                },
                {
                    "sent": "Hello guys, this is magically.",
                    "label": 0
                },
                {
                    "sent": "Here is a diagonal matrix encoding the weights of big features and we are assuming that the feature.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we don't have latent feature, but what we can observe with the what we want to happen.",
                    "label": 0
                },
                {
                    "sent": "The node are similar if they have more latent feature in common, but we observed with a note when they have more latent feature incumbent they have more neighborhood in common.",
                    "label": 1
                },
                {
                    "sent": "So what we use something very very simple.",
                    "label": 0
                },
                {
                    "sent": "That's where the efficiency metrics saying it is.",
                    "label": 0
                },
                {
                    "sent": "Amount of common neighborhood they share.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, the good news is that in and we're working with sparse networks.",
                    "label": 0
                },
                {
                    "sent": "In order to model sparse network, we have to use powers models and we show some results in the end.",
                    "label": 0
                },
                {
                    "sent": "In sparse model model, the kernel redefined here actually close to the ideal kernel that we want to have.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we having a kernels on the nodes and we just follow what people propose compare quite Standard Time to defy the symmetric between links by middle of similarity between nodes by region.",
                    "label": 0
                },
                {
                    "sent": "Try to measure different alignments and multiply the kernel and add them together.",
                    "label": 0
                },
                {
                    "sent": "And this is basically proposed before.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we want to demonstrate the ideas of our kernel computed from our data.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we have a kernel model like this, so this matrix is going to generate the.",
                    "label": 0
                },
                {
                    "sent": "Adjacency matrix does it, but we don't have the full magic.",
                    "label": 0
                },
                {
                    "sent": "But what will happen and incomplete observation?",
                    "label": 0
                },
                {
                    "sent": "So this is what we have.",
                    "label": 0
                },
                {
                    "sent": "The data we built our kernel function load on this and we do a kernel PCA just to visualize it and see where the links and grass and on links and as the missing links and we see here is on the blue line here as further links club and we see a grouping in the three patterns.",
                    "label": 0
                },
                {
                    "sent": "The links here actually the misspelling according to the model.",
                    "label": 0
                },
                {
                    "sent": "So happy to see that there actually.",
                    "label": 0
                },
                {
                    "sent": "Very similar patterns as a link to the testing data following the same distribution.",
                    "label": 0
                },
                {
                    "sent": "That training data.",
                    "label": 0
                },
                {
                    "sent": "Here we also look at the other class of non links laptop here and we see that in Arlington Lab following a different pattern like this and here you have different patterns.",
                    "label": 0
                },
                {
                    "sent": "So by doing that we are able to money to put on the notes.",
                    "label": 0
                },
                {
                    "sent": "Actually when you look at Baxter model and see which other note that have the same features, we look at that lately.",
                    "label": 0
                },
                {
                    "sent": "Second feature for example, you see the agent node with the second picture actually group in.",
                    "label": 0
                },
                {
                    "sent": "Here your reasoning that they share many common neighborhood and just first feature is going to be something in here that's going to be in here.",
                    "label": 0
                },
                {
                    "sent": "This node shared by the have two latent features, so we have to stay somewhere in between.",
                    "label": 0
                },
                {
                    "sent": "So we we don't generate the binary feature but we actually put them.",
                    "label": 0
                },
                {
                    "sent": "Somewhat close in some space.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we show some properties about kernel, for example, positivities.",
                    "label": 0
                },
                {
                    "sent": "We're saying that if they share.",
                    "label": 1
                },
                {
                    "sent": "Some latent feature according to the model.",
                    "label": 1
                },
                {
                    "sent": "Then we should be able.",
                    "label": 0
                },
                {
                    "sent": "To recognize that by giving a non positive values on that the second property like we.",
                    "label": 0
                },
                {
                    "sent": "Under certain condition caused when they them or no later than the more neighborhood they have anymore color values we have so actually the kernel we define here up data actually reflects some properties of the ideas journals, so that close in some sense.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We actually tried to study when the grandeur Colonel generated from data according to the model clauses.",
                    "label": 0
                },
                {
                    "sent": "Actually idea code at exactly what we want and we show the condition when I will define kernel.",
                    "label": 0
                },
                {
                    "sent": "Actually idea Colonel will show that there's some formula.",
                    "label": 0
                },
                {
                    "sent": "Actually this matrix actually have own logo vector and correlated.",
                    "label": 0
                },
                {
                    "sent": "So we also show some qualities of this thing.",
                    "label": 0
                },
                {
                    "sent": "And I'm say some in some usual situation that we actually.",
                    "label": 0
                },
                {
                    "sent": "Then have the ideas they know by just computing it from data.",
                    "label": 0
                },
                {
                    "sent": "When we try to work on the sparse network and sparse network is model.",
                    "label": 0
                },
                {
                    "sent": "With sparse model an we what we found here that we have one way to parameterise how spa model is.",
                    "label": 0
                },
                {
                    "sent": "By by this kind of formula, so I don't talk about that, but it is one of the way, not the only way to say how sparse model is.",
                    "label": 0
                },
                {
                    "sent": "And then we can manage to limit when this one is small enough.",
                    "label": 0
                },
                {
                    "sent": "Then the kernel modify is close enough to the idea of Colonel, so I've actually right about the difference between our kernels and ideas going to based on the sparsity of the model.",
                    "label": 0
                },
                {
                    "sent": "According to our parameterisation here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's the properties of the kernels that we try to device for the links according to the latent teacher models.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We do some experiment on that so we don't try to be anything funny, so we actually use the protein, protein interaction networks and we extracted them from DDI database, have hand curated experimentally validated data and largest possible understand we want to go to work with the network website 6000 or something like that and we show running time we compare the assumptions.",
                    "label": 0
                },
                {
                    "sent": "An prediction you see on them.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so why we need to use latent feature model?",
                    "label": 1
                },
                {
                    "sent": "Because a lot of people are using the simple like similarity assumption on the kernels like you see the work some parts we just compare simple whether the similarity assumption work here and actually when we plot AUC, doing a nearest neighbor classifier.",
                    "label": 1
                },
                {
                    "sent": "Here we see that.",
                    "label": 0
                },
                {
                    "sent": "Symmetry assumption needed.",
                    "label": 0
                },
                {
                    "sent": "Something up here and we use a latent teacher model.",
                    "label": 0
                },
                {
                    "sent": "We are going up here to 87% of AUC.",
                    "label": 0
                },
                {
                    "sent": "So what we learn from here?",
                    "label": 0
                },
                {
                    "sent": "OK, similarly, Simpson is significantly better than random that in certain part of General Network we had some celebrity in there, but the rest of them, this part, you know, we get some better performance because some part of the network there is no similarity in their cause.",
                    "label": 0
                },
                {
                    "sent": "We managed to that because the latent teacher model actually can encode certain type of similarity network as well.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About we try to compare the time of our method with the proposed method using Indian buffet process is and what we see that this is her own.",
                    "label": 1
                },
                {
                    "sent": "The network we have.",
                    "label": 0
                },
                {
                    "sent": "This is a network.",
                    "label": 0
                },
                {
                    "sent": "We figure out the small decrease less than 10 per sample and we have a small network here which is like 700 nodes.",
                    "label": 0
                },
                {
                    "sent": "Something too.",
                    "label": 0
                },
                {
                    "sent": "We have something like 6000 nodes along the line.",
                    "label": 1
                },
                {
                    "sent": "Here the time we run our method actually from 90 seconds to less than 500 seconds.",
                    "label": 1
                },
                {
                    "sent": "So there's 6 minutes.",
                    "label": 0
                },
                {
                    "sent": "On SVM and when we try to use Indian buffet process on this smallest subnetwork of 700, something notes what we we?",
                    "label": 0
                },
                {
                    "sent": "What we plot here is that they look like uh-huh the model and see how it converts overtime an until the time that we actually get some rather stable.",
                    "label": 1
                },
                {
                    "sent": "We see on the test set it is about 1/2 day for the smallest subnetwork.",
                    "label": 0
                },
                {
                    "sent": "So our method actually.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Faster.",
                    "label": 0
                },
                {
                    "sent": "We do apply link prediction on East PPI Networks and digital result so we can so use a linear kernel or that we put a Gaussian kernel on top of our defined kernel and what we expected result is this Gaussian kernels is linear kernel.",
                    "label": 0
                },
                {
                    "sent": "There is also.",
                    "label": 0
                },
                {
                    "sent": "Indian buffet process somewhere here with slightly less.",
                    "label": 0
                },
                {
                    "sent": "That's not that much less, but The thing is that the reason is missing.",
                    "label": 0
                },
                {
                    "sent": "Your reason is that we let them run for days and it doesn't converge to live at one model.",
                    "label": 0
                },
                {
                    "sent": "So we just give up so the rest are missing here.",
                    "label": 0
                },
                {
                    "sent": "We also tried to compare the.",
                    "label": 0
                },
                {
                    "sent": "Prediction based on network structure with the other method.",
                    "label": 0
                },
                {
                    "sent": "Prediction based on the attributes of the node.",
                    "label": 0
                },
                {
                    "sent": "Here, Attribution nodes are the sequence and we use spectrum kernels and for the problem we usually get you zero.",
                    "label": 0
                },
                {
                    "sent": "71% GT is with like 50% less than what we are doing here.",
                    "label": 0
                },
                {
                    "sent": "So the reason we thought is the sequence light spectrum corner they use on the sub sequences so they have too much redundant information in there and they just really hinder the performance in here because network actually encode what is relevant on the network.",
                    "label": 0
                },
                {
                    "sent": "So we actually.",
                    "label": 0
                },
                {
                    "sent": "I need to get the relevant information from net.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exactly the same story can be seen from fruit fly prepare network that OK in in October 2nd little bit less, but doesn't scale projects of the data and we managed to have something like that.",
                    "label": 0
                },
                {
                    "sent": "Rather high is much higher than random.",
                    "label": 0
                },
                {
                    "sent": "So we also say that these the network structure network do have structure and we managed to get them.",
                    "label": 0
                },
                {
                    "sent": "To make some, you see much better than 0.5 and also didn't so better than using spectrum color, which is like 65% something.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what we try to do?",
                    "label": 0
                },
                {
                    "sent": "We try to argue to use the network structure to predict links and for our problem we relate and teacher model to model their network structure and then in order to train that we will actually try to change your latent teacher model.",
                    "label": 1
                },
                {
                    "sent": "But we try to enforce the property of latent teacher model using kernels and we show some good results in terms of times an performance.",
                    "label": 0
                },
                {
                    "sent": "On paper net.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our method is efficient and effective for latent feature models.",
                    "label": 1
                },
                {
                    "sent": "So sparse network actually.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's it for me.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you actually I have clarification question.",
                    "label": 0
                },
                {
                    "sent": "Essentially you input is a similarity matrix.",
                    "label": 0
                },
                {
                    "sent": "And at some point you said that we you used to compare, you used features of nodes.",
                    "label": 0
                },
                {
                    "sent": "So my question, can you combine both?",
                    "label": 0
                },
                {
                    "sent": "So if there is any way to inject node features in your link prediction kernel, have you ever tried and this kind of combination?",
                    "label": 0
                },
                {
                    "sent": "Yes we do that.",
                    "label": 0
                },
                {
                    "sent": "OK, we have a kind of based on the network structure and then so we have kernel based on attributes of the nodes and we just do multiple color learning on top of that.",
                    "label": 0
                },
                {
                    "sent": "We tried that.",
                    "label": 0
                },
                {
                    "sent": "The thing is that the spectrum kernels for the node using those information they contained too much information and they absorb into things when you combine them just getting worse, but it should be.",
                    "label": 0
                },
                {
                    "sent": "Shouldn't be a problem actually problem, but it doesn't get better.",
                    "label": 0
                },
                {
                    "sent": "That's the thing.",
                    "label": 0
                },
                {
                    "sent": "But still I mean it looks a bit strange that you can't manage, because sometimes this information is complementary.",
                    "label": 1
                },
                {
                    "sent": "What gets from the network?",
                    "label": 0
                },
                {
                    "sent": "It might be not always the same, but you get in features, so they should help each other to some point.",
                    "label": 0
                },
                {
                    "sent": "Our hypothesis is that when the info only information like in the application, we have to mention that should be included in the sequence.",
                    "label": 0
                },
                {
                    "sent": "Usually some people try to do that from sequences.",
                    "label": 0
                },
                {
                    "sent": "What part of the sequences response without we don't know.",
                    "label": 0
                },
                {
                    "sent": "So actually using the network itself, we're actually selecting only the relevant information.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Well, I have one so I was wondering if you see room in your methods to incorporate some domain knowledge, for instance, well, you worked with protein protein interaction but.",
                    "label": 0
                },
                {
                    "sent": "If you were to work with some models of social networks, you know we hear that there are these laws, you know, preferential attachment.",
                    "label": 0
                },
                {
                    "sent": "What have you.",
                    "label": 0
                },
                {
                    "sent": "So I think in certain types of networks you would have some sort of domain knowledge about the type of growth that you can expect, and could you incorporate that with the kernel.",
                    "label": 0
                },
                {
                    "sent": "This type of girl?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I would say yes.",
                    "label": 0
                },
                {
                    "sent": "You have to look at what kind of domain knowledge we have in order to put into an.",
                    "label": 0
                },
                {
                    "sent": "We talked about certain kind of domain knowledge, something like that.",
                    "label": 0
                },
                {
                    "sent": "We're trying to classify different type of interaction or different type type of links and if we have that kind of pattern then we might be able to put it in there.",
                    "label": 0
                },
                {
                    "sent": "It is possible, but it could be difficult elsewhere.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank the speaker again.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}