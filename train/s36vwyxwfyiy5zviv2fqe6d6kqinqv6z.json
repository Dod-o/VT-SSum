{
    "id": "s36vwyxwfyiy5zviv2fqe6d6kqinqv6z",
    "title": "What is Machine Learning?",
    "info": {
        "author": [
            "Neil D. Lawrence, Department of Computer Science, University of Sheffield"
        ],
        "published": "May 13, 2013",
        "recorded": "April 2012",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2012_lawrence_machine_learning/",
    "segmentation": [
        [
            "So this is a 2 hour lecture that I wrote across the last two hours.",
            "But it is based on some material I've had before it occurred to me, so I think I went.",
            "For example, I assumed too much in terms of some of the Bayesian learning.",
            "I thought Mark was going to do a bit more to introduce some Bayesian learning concepts, so I thought it might be fun to actually.",
            "Go back to basics a little bit, which we didn't really do at the start of the summer school and try and motivate machine learning a bit, and perhaps what I like to call learning with probabilities.",
            "Or in this talk."
        ],
        [
            "That's what I'm calling it.",
            "So the slides might be a bit funny because I've been slapping them together and not proofreading them as I go through, but I'm sure it won't matter too much."
        ],
        [
            "So I thought I'd start with these actually.",
            "Just some sort of general stuff on what I think machine learning is, because some people tell you things like machine learning is just statistics.",
            "And I think you know you could also, people could perhaps argue machine learning is just optimization.",
            "What is machine learning?",
            "How?",
            "Why was the new field machine learning?",
            "Required in the UK there was actually.",
            "In the 1970s, I think it was there was something called the Lighthill report that really set back AI research in the UK for many many years because it said there was nothing being done in AI research that couldn't be done in a pre existing field such as signal processing or statistics.",
            "So there was no need for a new field of AI and that was in the time of, you know, logic based AI.",
            "But you know you could maybe try and make that argument today.",
            "Why is there a need for field machine learning when there's so much overlap between what we do?",
            "And what many other fields do?",
            "So I want to talk a little bit about that.",
            "So we want to endow the computers with ability to learn from data.",
            "So we present data from sensors, the Internet experiments, and then we expect the computer to make sensible decisions where sensible.",
            "I don't know what sensible means, but you know, informally, that's what we're expecting.",
            "And of course, there are three traditional categories of machine learning, supervised learning, classification and regression.",
            "Now, I would argue that these were certainly the areas when I started machine learning classification was still considered a big challenge.",
            "Traditional classification, I'm presented with a set of inputs and a set of targets training data, and I expect to be tested on inputs drawn from a similar distribution and targets drawn from the same sort of distribution.",
            "People did this with neural networks using.",
            "Backpropagation to learn them, and in some sense the success of neural networks standard feedforward neural networks, really was the core of the machine learning community.",
            "And then of course, you've heard about the support vector machine an.",
            "I would argue myself.",
            "My own perspective is that classification in that way is almost no longer interesting because it's a solved.",
            "Solved problem from a research point of view.",
            "I mean there are definitely interesting aspects such as semi supervised learning where you don't have.",
            "You've got a lot of data without labels.",
            "Can you use that to improve your classification?",
            "Multitask learning.",
            "Where you've done some learning from some related tasks and you want to know what can you take information about learning what cows look like to help you understand sheep things like that?",
            "That's definitely unresolved.",
            "But the classical thing we used to look at.",
            "The M list data set.",
            "Here is a bunch of handwritten digits.",
            "What's the class of the digits?",
            "As Bernard said, we can get to even slightly better than human performance on that data, and that was a big problem.",
            "You couldn't do that in 1996 when I started the machine learning regression as well.",
            "Standard regression.",
            "I think we have good methods for dealing with that.",
            "Personally, I use Gaussian process.",
            "Is there are many variants of regression that are interesting, but in some sense I would say that the classical standard supervised learning.",
            "Scenarios are things that are being less studied at the leading conferences such as NIPS, ICML, UI, the leading journals with more into sort of things which might be thinking of unsupervised learning such as deep learning.",
            "Of course, I've talked about dimensionality reduction already clustering, Bayesian nonparametric models.",
            "You're going to hear about from Zubin.",
            "And then I think probably one of the most challenging areas is reinforcement learning.",
            "And that's an area where there's still room for enormous amount of progress.",
            "So learning from delayed feedback where you know a robot has to learn to make you a Cup of tea.",
            "But it doesn't, it doesn't really know much about the problem, it just gets a reward at the end.",
            "It's how do you do that sort of thing?",
            "That's really big challenge, and it's an area of a lot of ongoing research.",
            "So that's my sort of overview."
        ],
        [
            "And there's a few different examples of learning.",
            "So."
        ],
        [
            "What's the history of machine learning?",
            "Well, I'd give you my personal perspective on what machine learning where it comes from, and I think it comes out of this connection.",
            "This movement in artificial intelligence.",
            "So there were there was this area of artificial intelligence that was about specifying logical rules.",
            "Expert systems that really dominated in the 1970s.",
            "The early machine learning people were actually a lot overlapped a lot with people in psychology.",
            "For example, Jeff Hinton.",
            "A background is a psychologist.",
            "Also, Mike Jordan has a background as a psychologist and there was this group of people they call themselves the parallel distributed processing group and there was this interesting mix at the time with the idea that in the brain you had lots of distributed processing elements with high connectivity and it was very robust so you could take out cells and the system would still work and the system would learn.",
            "So people were interested in trying to build.",
            "Later models that had similar characteristics and the movement was called Connectionism.",
            "And you can read about it on Wikipedia.",
            "Connectionism was the dominant force in machine learning, say in the late 1990s.",
            "But and it was actually sort of driven by ideas of models of the brain and in."
        ],
        [
            "Killer Rosenblatt, who built the so called Perceptron.",
            "So it was.",
            "I mean, I think there may even be videos of this.",
            "It was in 1962 and a learning system which could try and classify digits very simple, one, that you may have heard about before, and it was even based on a very simple model of a neuron.",
            "So this is kind of quite powerfully exciting stuff.",
            "In 1962 you can do a bit of learning based on an existing model of the brain.",
            "Now I think any neuro scientist would say it's a rubbish model of the brain.",
            "And as machine learning people actually, it is still quite for such a simple learning rule.",
            "If you Colonel eyes it, you can do some quite powerful things, so, but it's a very simple system.",
            "I think that that area was inspirational for the connectionist.",
            "Some people were looking at extending the perceptron and the big sort of step forward was the multilayer perceptrons or the neural network."
        ],
        [
            "People could use to do nonlinear classification.",
            "So to understand that joke.",
            "You have to have a weird geeky overlap mix between some sort of Japanese computer game from the 1980s.",
            "Plus an understanding of risk, empirical risk and that makes generalization for bounds based on VC dimension.",
            "And Daniella couldn't set up this photo so but I'm using this photo to represent.",
            "What happened to the machine learning community was that I mean?",
            "So how does machine learning differ?",
            "Why am I very proud of what the community I came to at that time did?",
            "In some sense, I think it's a very dangerous thing to be one of these communities that gets inspired by how the brain works and we're going to be like, you know, we're going to use this stuff to do learning, and you know you can also do it for evolution.",
            "You get genetic algorithms or artificial life.",
            "You get particle swarm optimization blah blah blah blah.",
            "It can be useful to be inspired by biological systems.",
            "Absolutely.",
            "There is always a danger, though, that a large part of the research moves away from the theoretical side.",
            "It's frightened of the mathematical side, and it just ends up hacking.",
            "Playing with algorithms without any motivation.",
            "What I think was very good about the connectionists, and you know you can mention people like Mike Jordan and Jeff Hinton in this.",
            "They were never afraid of theoretical people that came on board.",
            "So you ended up in the early days, so Jeff Hinton used to go.",
            "They did this Boltzmann machine thing, which was cognitive model, inspired by Hopfield networks.",
            "I believe with Terry Sadowski, but then they went out and told physicists about this model and its related to icing models.",
            "So you got statistical physicists coming into the community and rather than saying oh that's all too complex and doesn't matter in practice they were absorbed into the community.",
            "So by the time I was in the community there was this mix of engineers, psychologists, very few computer scientists actually.",
            "And statistical physicists mathematicians interested in icing models interested in this book.",
            "Hertz Gordon Palmer, is basically a statistical physics perspective on the sort of models people were looking at at the time, and that was the leading textbook, I guess until 1995, when Chris Bishop's first book came out and related everything to statistical pattern recognition, which is another step forward.",
            "So when I came in to the community, there was this big transition where people started looking so.",
            "That makes work support vector machines.",
            "Kernel methods were all just emerging in 1996 OK, and that really revolutionized the field, so it went from being.",
            "You know, when I came in, the reason I could get involved in machine learning is I understood the chain rule for differentiation.",
            "OK, that's why it's quite complex, but that's how backpropagation worked.",
            "If you look at the mathematical concepts you're hearing about here, now you're hearing Mark talking about.",
            "Information geometry of parameter space.",
            "For Bayesian modeling, you've got David Mackay in 1993.",
            "I guess starting to publish on Bayesian models for neural networks, so you really have all this theoretical stuff being absorbed by the Community and the Community moving forward to an extent such that I think in many areas we actually.",
            "Overtook the fields, we were competing."
        ],
        [
            "With in terms of statistics, which at one stage was regarded as more rigorous than machine learning, but actually machine learning has really, you know it's putting a lot back in towards statistics.",
            "So my personal view is that machine learning benefited greatly by incorporating these early ideas from psychology, but not being afraid to incorporate rigorous theory, and one of the things you're seeing now is this return to those type of models with deep learning.",
            "These models of difficult to analyze models, but they're doing amazing stuff algorithmically.",
            "And actually doing some pretty incredible unsupervised learning with these models.",
            "But instead of just people playing with those things and saying algorithmically, here's this.",
            "You've got people like Ian Murray doing great analysis of trying to do annealed importance sampling to estimate marginal likelihoods in these very complex models to try and understand better what's going on.",
            "And I think it's a very powerful."
        ],
        [
            "Community as a result of that.",
            "So I think early machine learning was viewed with a lot of skepticism by the statistics community.",
            "But certainly for me, say in the UK, alot of the people I'm most interested in talking to, our statisticians doing Bayesian learning in or what I think of Bayesian modeling.",
            "And there's now enormous.",
            "Interaction between machine learning and statistics, but I think there is still a difference.",
            "So when people tell you what's going on, they say to you it's just just statistics done in a hacky way or whatever.",
            "I think that's not true.",
            "My personal view is that the philosophy of the two fields is fundamentally different.",
            "Whatever you're doing at the moment, whatever your tasks are, if you are doing machine learning, you want to replace the human in the processing of your data set.",
            "So there was this.",
            "I had lunch at Nips.",
            "We were doing a Gaussian process Workshop and Zubin Ghahramani and Tony O'hagan were there.",
            "Tony O'hagan's, a leading statistician who did a lot to introduce Gaussian process to the statistics community and a very well known Bayesian statistician and Tony sort of said I don't believe you can ever take a data set and.",
            "Um?",
            "Just analyze it by computer without a human being involved in the loop to interpret the analysis, which I sort of agree with in terms of the technology we have today.",
            "But Zuben immediately said well, but unless you believe that the humans doing anything magical in less you believe there's a ghost in the machine, to put it one way, you should be able to replace whatever the human is doing as well.",
            "Now I think that statistics as a community is about summarizing data to the extent that humans.",
            "Can interpret the data so you try and say the mean of this data is 7.",
            "So if you look if you go back to when statistics became field in its own right, they were often working with social science.",
            "So if you want to understand poverty in Manchester versus poverty in London, you could take the average income of the people in Manchester and find that it's 12 shillings and sixpence per year, and you can take the average income of people in London and find it.",
            "18 shillings per year, and then you want to know the difference.",
            "The fact that these numbers are different doesn't mean anything.",
            "Are these populations different?",
            "So statistics itself?",
            "We know what that's like baseball.",
            "You say his batting average is well, so it runs batted in is .4 or in cricket you know you say the batting averages 30 runs.",
            "That's what statistics means when we say statistics were talking mathematical statistics, which is really the study of whether the fact that those numbers are different means that the populations are different.",
            "And that's what statistics were set up to do.",
            "So it's set up to feed to understand the numbers you're feeding to humans.",
            "But machine learning set up to.",
            "Eliminate humans.",
            "Kind of fundamentally, we're aiming for the AI singularity OK, I mean.",
            "Fortunately, anyone in the field knows this so far away from it, but we don't have to worry yet.",
            "It's like the sun going out, but that is really our aim that we don't want.",
            "You know, I'm I presented the Gaussian process latent variable model to the start group in Sheffield.",
            "Uh, once and Tony O'hagan was there actually, but at the end of it, one of the lecturers in the group say, OK, that's OK, that's fine, but what do you tell the client?",
            "I'm like who's the client?",
            "Statisticians are always worrying about a client.",
            "There's someone who's comes to them with data, and they're trying to say something about the data.",
            "Tell the client computer P value.",
            "I don't think we care about the client, because in some sense we're just trying to integrate it into computer program, which I'm trying to provide a module in a computer program that has an end result, like those faces.",
            "You can animate the faces, right?",
            "The client there is graphics guy who's drawing, so I don't tell him anything.",
            "The computer does all of it, so I think that's the."
        ],
        [
            "Difference, but for the moment the two overlap very strongly.",
            "Why do they overlap at the moment?",
            "Because we're not capable of replacing the human so very often we need to say something to the human about the data, so we're interested in computing these numbers and the sort of things statisticians do.",
            "I think also.",
            "One of the main roles of statistics.",
            "Again, these are personal views, but you can disagree or agree or whatever.",
            "A lot of statistics in terms of what the field was set up to do with solved quite early on, so in terms of being able to infer draw conclusions about data about if you've got two sets of crops and you're fertilizing them in different ways, you want to know which fertilizer was better.",
            "Randomizing the samples.",
            "These sort of ideas came out early there sort of model free.",
            "They allow you to infer things about the data clinical trials.",
            "A lot of those questions again are solved and statisticians have moved into things which are much more model based, which originally the field tried to avoid.",
            "So Bayesian I don't think Bayesian statistics personally is a complete oxymoron because statistics set itself up as a field to avoid modeling.",
            "They didn't want to write down what was going on in the data because they were looking at social data where there was no physical model.",
            "What physicists were doing at the time were saying I have a model of the universe, or have Newton's equations.",
            "I can observe planets, I can infer the way to the planet, they had a physical model.",
            "Statistics set it up to self up, say well, we don't understand anything about social interactions, but we can compute numbers and ask questions in the Model 3 way.",
            "So Bayesian statistics seems like an oxymoron to me because in Bayesian in the Bayesian world you always be explicit about your model and that's what physicists do actually ended before statistics came and continue to do in parallel with statistics.",
            "So that sort of personal feeling in philosophy.",
            "But actually that doesn't matter because today.",
            "Statisticians are doing Bayesian modeling all the time, and they're doing it really, really well, so you know it's you know a lot of statisticians would disagree with me there.",
            "So.",
            "I think machine learning has this overlap with cognitive science, which is helpful, and it brings in a lot of insight, and I think particularly this.",
            "This this.",
            "This little point here is important, so you have to be careful about getting tide up in mathematical formalisms because they can be misleading.",
            "Fallacy that aerodynamically a bumblebee can't fly.",
            "I mean people say, did you know that aerodynamically a bumblebee can't fly?",
            "OK, that's the dumbest thing to say ever.",
            "Because it's a limitation of the model, not a fact.",
            "Yeah, so and I was driving at this.",
            "You know there was a theme to some of the stuff I was talking about in dimensionality reduction about the curse of dimensionality, so and so forth you can get lost in mathematical proofs about what happens in these things, which actually betray what your own instincts are.",
            "If you look at what humans are doing.",
            "So a lot of problems that we study are apparently NP hard or whatever it's in.",
            "The computer vision is insolvable, or this sort of thing, but in practice cognitive science shows us that we're solving these problems all the time.",
            "Or were capable of generalizing over few examples, despite the fact that the statistical underpinnings are not strong, so that's clearly a limitation of the model rather than the fact that this is a bumblebee effect.",
            "Obviously a bumblebee can fly, and the reason people say aerodynamic you can't fly is what they mean is if you apply the aerodynamic rules that you apply to a jumbo jet under those rules, the bumblebee can't fly, but of course the jumbo jet is flying under very different.",
            "Reynolds numbers to the bumblebee.",
            "The bumblebee is the effective viscosity to the bumblebee is much higher, so it's different.",
            "You need a different model, so mathematical foundations are still really important because they understand help us understand the capabilities of of our algorithms.",
            "But I think one thing that machine learning does really well is.",
            "It's very good at going out empirically and saying look at this result.",
            "I've created this crazy funky deep hierarchy of Boltzmann machines and I can get this kind of supervised learning result even though I can't prove much about when this will converge.",
            "So we mustn't restrict our ambitions to the limitations of current mathematical formalisms, and humans can give us a lot of inspiration in terms of what we can do in order to go and seek beyond.",
            "This."
        ],
        [
            "With standard mathematical models.",
            "So I guess I've already said this early statistics had a lot of success with the idea of a statistical sort of proof and inverted commas.",
            "I computed the mean of these two tables of numbers.",
            "There are different.",
            "Does this prove anything?",
            "Well, it depends on the answer depends on how many numbers generated, how many are and how big the differences, and we randomized your samples so and so forth, whether it was correlation.",
            "At least to hypothesis testing, and I'm very subjected to this now because I work in an Institute with a lot of doctors and they always come into my office and say, can you compute the P value for this because?",
            "Every experiment they do their expected to provide AP value, even whether it's interesting or not.",
            "It's it's important, though it's an important area, it's not what I'm interested in, and classical hypothesis testing.",
            "The questions you can ask really about your data are quite limiting, so that can have the effect of limiting science too, but there are many, many successes, so who knows who student worked for?",
            "I mean, his name was actually got it, but Guinness.",
            "Yeah, there you go.",
            "Guinness is what one of the largest brewing corporations in the world.",
            "While they wanted the largest brewing corporations in the world.",
            "The early on adapted statistical methodology's to make sure they could brew beer consistently.",
            "Why is it that you can pick up wine from Australia?",
            "That is very, very high quality because people did analysis of what was important in the creation of why these little variations that were occurring on a year on year basis.",
            "People get out.",
            "People avoid them now because they understand.",
            "Even though they don't understand necessarily the system of what's going on in the brewing, precisely, they can check variations.",
            "If I add this much yeast.",
            "If I put it this temperature, they can check which is significant with randomization and hypothesis testing.",
            "So Gossett work for Guinness and Guinness is one of the largest brewing companies in the world.",
            "He came up with the student T he came up with T tests.",
            "These sort of things.",
            "Basically, in an effort to help them ensure they brewing a consistent brew, now I don't know the details of the story, but I think a nice ending to that story would be that statistics enabled.",
            "Guinness to be the large massive Brewing Company it is today.",
            "Certainly stout is overrated, sorry need so it can't be their fundamental product.",
            "I mean, I say they advertise it with Italians and Spanish and funky weird adverts all the time so it can't be that it tastes good 'cause they keep on having these weird things to advertise it, so it must be statistics.",
            "But there are many open questions.",
            "For example, I think that when you talk to statistician, not all statisticians but graduate level educated statisticians about causality, they are paranoid.",
            "They say you can't infer causality.",
            "It's not possible to infer causality yet.",
            "You know when I did my first lecture only work anymore, but I kicked that thing there and the thing went off right.",
            "That was the plug connector.",
            "I immediately inferred causality from one data point, so we know humans need causality an we may not always get it right.",
            "But statisticians became afraid of causality because correlation and causality doesn't necessarily imply causality.",
            "So they almost eliminate it.",
            "Causality mean there's many papers on it saying you know, you remove causality in any analysis of data.",
            "If that's the case, then all the human reasoning that we're doing where we're on furring causes, whether they're wrong or right.",
            "But to help us with understanding a system, we do look for causality.",
            "It's very different, it's very.",
            "Wrong according to statistics, but of course it's not wrong.",
            "It's an important open question and you know something that Bernards looking at extensively gave a nips invited talk on it last year and a lot of leading statisticians are looking at it as well.",
            "But traditionally, statistics avoided causality."
        ],
        [
            "So there's William Sealy Gossett and they were sort of.",
            "I like to think of the early statisticians there, sort of Edwardian English gentleman.",
            "That's a classic Edwardian English gentleman.",
            "I'm glad I didn't live in those days 'cause I could never have grown that mustache.",
            "This is like 8 days growth.",
            "So I'm glad I'm not Edwardian."
        ],
        [
            "So, and as I mentioned already, that.",
            "The statistics is really about computing numbers and really."
        ],
        [
            "Matt Mathematical statistics is the world that we are interested in.",
            "Some machine learning and probability.",
            "Where does probability coming into machine learning well, the world is an uncertain place.",
            "For example, I got up today expecting to sit quietly in the audience.",
            "For six hours of lectures and you know things happen, things change and you need to deal with that.",
            "So epistemic uncertainty is 1 type of uncertainty, so this is uncertainty arising through lack of knowledge.",
            "So what color socks is that person wearing?",
            "So what color socks am I wearing?",
            "Very good you see.",
            "Normally that works better in England when you're giving your lectures in the middle of the winter.",
            "And then aleatoric uncertainty is uncertainty arising through an underlying stochastic system.",
            "So if I if I take a sheet of paper here and I drop it, where will it fall?",
            "Well, I could do it.",
            "Similar way many many times and I wouldn't be able to say.",
            "Now those are the two broad categories of uncertainty, and here's the real.",
            "This is the right way to think about these uncertainties, right so?",
            "Two ways of watching a football match, right?",
            "You watch it live.",
            "And that's aleatoric uncertainty.",
            "The other one is you can't watch it live because you're at work or something and you record it.",
            "On your video recorder or your Sky plus box or whatever, and then you come back later or it was on really early in the morning and you play it as if it was live.",
            "Now as far as you're concerned, if you don't know the result, it's a bit spoiled.",
            "If someone walks in and says oh, they want, you know, but as long as you can maintain the illusion that you don't know the result, the uncertainties the same.",
            "There is an actual difference, and this difference was brought home to me when I was doing this with rugby, England versus France in the World Cup semifinal.",
            "I think it was.",
            "I've tried to forget about it.",
            "We had a really good quarter finals.",
            "It was, I think, to get to the semifinal.",
            "We had an excellent run to the team was rubbish but they just had to beat France that weren't looking that good and I started watching the game and they were just appalling and they were down by a certain number of points and I thought I can't.",
            "You know I'm just going to check what the final score was so I left and went and looked.",
            "Obviously you can't do that if it's aleatoric.",
            "If you're watching it live, but I was watching it recorded so I do a lot of watching recorded and that epistemic uncertainty, because I can always go and find out the result is known.",
            "I just don't know it.",
            "With Aleatoric is the result is unknown.",
            "Now there's a sort of overlap thing here, which is that why does this thing drop in different ways?",
            "Every time I drop it, it's I assume it's slightly different initial conditions.",
            "Chaos theory, yeah, very small variations in initial conditions.",
            "Even in a deterministic system, so actually Allie at all, I think pretty much unless you get down to the quantum level and I don't understand quantum mechanics enough to make a claim about that, but let's ignore quantum levels.",
            "Let's just assume that this is chaos causing these things.",
            "All aleatoric uncertainty in some senses, epistemic uncertainty.",
            "If you knew precisely the initial conditions with which I'm dropping it, and the state of everything in the room, then we would be able to compute where that would land.",
            "But.",
            "Such small variations effectively make this statistical noise aleatoric uncertainty.",
            "So Laplace this is called Laplace indeterminism.",
            "To sum it, well, that's really about epistemic uncertainty, but the two overlap a little bit more, I think than we're being honest about here, because fundamentally, the aleatoric uncertainty is something that is occurring because I don't understand the initial conditions.",
            "But let's think about, I mean, but let's just make the division.",
            "Let's assume that is genuinely stochastic and epistemic ISM."
        ],
        [
            "OK, so we need a framework to characterize this uncertainty and we use."
        ],
        [
            "Ability for doing that was one of the frameworks.",
            "So people talk about Thomas Base, but number one.",
            "We don't really have a picture of Thomas Bayes.",
            "The one people show is unlikely to be Thomas Bayes 'cause Thomas Bayes was a non conformist minister.",
            "And that's what non conformist ministers look like.",
            "They have big wigs.",
            "And this is Richard Price, who was a Welsh philosopher, an essay writer.",
            "And if you read Thomas Bayes essay your hips as a long introduction to it, and then there's an essay, and the introduction is written by price, who also edited the essay.",
            "The introduction is really easy to understand and the essay isn't so a lot of the claims for what's going on the essay or written by this guy.",
            "Richard Price, who we know are not more about.",
            "He's also political essayist.",
            "He's involved in, you know, he was writing about the American Revolution.",
            "He's interesting.",
            "He's more like a Benjamin Franklin type figure.",
            "Where is Thomas Bayes?",
            "We know very little about, but he was the one that presented it to the Royal Society and you can read about Richard Price Online.",
            "He was Welsh and he came to London and one of my best friends is called Jonathan."
        ],
        [
            "Price an I don't know if he's related or not, probably not.",
            "But then who's the man?",
            "This is to me.",
            "Is the man Laplace?",
            "So Laplace actually didn't know about Thomas Bayes, but he came to similar conclusions about how you should try and deal with data and dealing with uncertainty independently an.",
            "I think he almost did.",
            "If you read the plus, I don't think I do anything that Laplace hadn't thought of.",
            "Of course I have a computer so I can.",
            "Empirically, do things that he thought of but couldn't do in practice.",
            "The class was interested in.",
            "Initially he was interested in gambling, so he was interested in if a dice was unfair.",
            "For example, how could you tell when the dice was unfair and the French called this an English dice?",
            "I don't know what the English called an unfair dice, but actually he reduces.",
            "He looks at binomial, he looks at a coin coin, tosses and he looks at he does.",
            "He does a Bayesian treatment of coin tosses he doesn't know about beta distributions, so he can't computer beta posterior and what he actually does is he's the second person to write down the Gaussian distribution after Dimov and he writes it down in computing a Bayesian posterior.",
            "Over dice flip using Laplace's approximation.",
            "The 2nd order expansion about the mode, and he's doing that in.",
            "I think the 17 late 1700s.",
            "That's pretty incredible that he you know, so Laplace approximation, which is still one of the standard methods of approximating Bayesian inference.",
            "He invented the Gaussian distribution to do it, so I think that's pretty cool that the Gaussian distribution was invented for Bayesian inference.",
            "He also shows in his paper, which again the translation is quite readable.",
            "Annotation is very modern, so if you're French, I suspect the original paper is quite readable.",
            "He shows that in the limit, as the data goes high, the Laplace approximation becomes correct.",
            "So, and he's considering that limit.",
            "So for him it's not even.",
            "An approximation, but he didn't know about the beta distribution.",
            "The system he looks at it completely analytic for us today, but I guess the normalizer of the beta distribution wasn't known at that time."
        ],
        [
            "OK, so the classic sort of thing we tend to do in machine learning, moving away a bit from motivation and philosophy.",
            "I forgot to look at time.",
            "I have no idea of time, so let me just.",
            "That's not too bad, is it?",
            "Is inputs and targets.",
            "So we've got some set of inputs and we want to predict a target, and for binary classification that should either be one."
        ],
        [
            "Or minus one.",
            "So the sort of examples we've already talked about document categorization who have failed detected face belongs to detecting faces, and if."
        ],
        [
            "Ages classifying digits from binary data.",
            "Now the perceptron is really the foundation of machine learning in the sense that you take a data point XI and you predict that it belongs to a class.",
            "Why I if the sum of some weights times that input plus a bias is greater than 0?",
            "So I like the vector notation, so there it is in vector notation.",
            "The inner product between the weights and the input plus a bias is greater than 0, otherwise we assume it's minus one.",
            "That was what Rosen black sort of said in 1957.",
            "He died quite young.",
            "Unfortunately there's a memorial to him."
        ],
        [
            "At his old institution, so the perceptron algorithm is really quite cool.",
            "It's actually can be solved by linear programming, and it is just a linear program, because unfortunately the one bit they did manage to wipe off.",
            "When you look at the perceptual algorithm is just looking for a feasible region.",
            "In the W space.",
            "So you can draw it in two ways you can draw.",
            "The vector W in a 2 dimensional space.",
            "I shouldn't put those in 'cause that's like maximum margin.",
            "You're looking for a linearly separable region.",
            "You're satisfied you're feasible if you can get all the negative points in one side and all the positive points on the other side.",
            "But there's another way of drawing it, so in this case you've got 1 W and you draw the W and you're drawing in X space and the exes are points in the space.",
            "Right now what Robert was talking about, I believe he was drawing earlier is the what we would feasible region, which we would think of as the version space.",
            "This is sometimes called.",
            "So these are now linear constraints.",
            "Every data point is providing a linear constraint.",
            "If you draw in W space, then the weight vector you're interested in is now a point as the X is.",
            "We're in here and every data point provides.",
            "Well, actually it's the data times.",
            "Its label provides a linear constraint.",
            "OK, so every datapoint is giving you a linear constraint, which gives you a feasible region, and the perceptron algorithm is actually.",
            "An odd way, which I think I can't.",
            "Robert you know about its worst case performance or anything.",
            "It's got weird worst case, but it's average case I think is good.",
            "Yeah, it has been analyzed a bit in terms of linear programming.",
            "I'm not sure of the details, but it has, I think, very bad.",
            "Worst case performance, but quite good.",
            "Average case performance.",
            "So you've got all these data points here.",
            "These zeros you've got the Y which dictates which direction this arrow is pointing in.",
            "If you for example had this data point change label like this, then that region becomes and then you've got no feasible region.",
            "Yeah, so that's an inseparable problem in the perceptron algorithm.",
            "The other weird thing about when you do have an inseparable problems.",
            "Within the perceptron algorithm, you do end up with a reasonable solution.",
            "If there's a learning rate, and if you reduce the learning rate.",
            "In a slow enough way, you will actually find the optimal hyperplane.",
            "This sort of thing that was known in 19.",
            "Well, it was known easily by the 1990s when I came into the field and the algorithm is basically you.",
            "You take a wait and you add to the weight.",
            "Some learning rate times the label value times the datapoint value.",
            "What that says.",
            "So if the so you iterate you select and it not an increment K and increment.",
            "Eater and you select a misclassified point, you ignore any points that are correctly classified.",
            "So for example, if you're here, if your weight solution is here, then all these solutions are fine.",
            "It's only this one that you're infringing, so you select this guy and you update in that direction basically, so you may actually go beyond.",
            "You don't do anything intelligent.",
            "You just take a learning rate I guess in the.",
            "I don't really understand enough in the simplex method to understand, but you could it perhaps the case that Gardiner you're doing something definitely more intelligent.",
            "The simplex method you won't overshoot here.",
            "You could potentially overshoot, but then you look at this one and you go back again.",
            "Yeah, so I've got a little."
        ],
        [
            "Visualization of that.",
            "This is all historic stuff, so here's a simple data set and what we do is we select initially some data point and we build the initial decision boundary based on that data point only, so.",
            "We start with the weights being zero and for the first iteration we set the weight vector."
        ],
        [
            "One of the data point values, which in this case that I've selected that data point I circled, and that's the result.",
            "So we multiply the observed value of the data point times."
        ],
        [
            "It's class.",
            "Then we go through selecting incorrectly classified data points.",
            "So I've circled one there.",
            "Which is incorrectly classified and we do this update.",
            "We multiply that incorrectly classified data point."
        ],
        [
            "By its label.",
            "And."
        ],
        [
            "Just the weight vector."
        ],
        [
            "Now very often."
        ],
        [
            "This converges very quickly.",
            "So there's that one green point there.",
            "That is now in."
        ],
        [
            "Reclassified yeah.",
            "And then.",
            "All data are now correctly classified.",
            "Now that's just an example I set up that is separable that has two calcium Gaussian distributions.",
            "The feasible region would be any region in here that we can bend that.",
            "Point, and not without misclassifying that or misclassifying that.",
            "So there's a small feasible region that we could find the support vector machine.",
            "Work is about making that solution unique by adding a quadratic term so that you get a quadratic programming program with linear constraints, and you look for the maximum margin separation.",
            "Now the fun thing about Rosenblatt, when he specifies this learning rule is I don't think he's why.",
            "I haven't read the original paper, but I think it's just an intuitive learning rule.",
            "That you know that you should if you've got something wrong, you should look at the thing you've got wrong and you should add it into your weight vector.",
            "But it still works and it leads to things like Hebbian learning which people use for Hopfield networks, but they were and heavy and low."
        ],
        [
            "It's something that I believe there's some evidence for existing between neurons in the brain, so there's that nice connection there.",
            "So regression examples predict why given."
        ],
        [
            "X in the Y given X we define an error function now which is.",
            "So we have some in the linear case we say that we've got some function of X which is gradient plus an offset.",
            "And then we might define an error function in that case, which is the distance between our data and our target function."
        ],
        [
            "So we can add a portion of the error to the bias and what we then also try and do is add.",
            "Some of the error to the gradient.",
            "Now this we can do iterative learning of these things by updating the bias and the Intercept."
        ],
        [
            "In a very similar way to the perceptron."
        ],
        [
            "And."
        ],
        [
            "You get."
        ],
        [
            "The same sort of effect."
        ],
        [
            "We select every."
        ],
        [
            "Datapoint in turn."
        ],
        [
            "We can actually update."
        ],
        [
            "8."
        ],
        [
            "A linear regression."
        ],
        [
            "You all know."
        ],
        [
            "So that this isn't."
        ],
        [
            "Standard way of."
        ],
        [
            "Doing linear regression."
        ],
        [
            "But this is a sort of."
        ],
        [
            "So."
        ],
        [
            "So castec."
        ],
        [
            "Gradient."
        ],
        [
            "Sent version of linear regression.",
            "So we are updating and actually an underlying error function now.",
            "But I think people didn't think about that so much."
        ],
        [
            "In the early days of."
        ],
        [
            "In learning so."
        ],
        [
            "So you do it until.",
            "You have."
        ],
        [
            "A solution that fits now of course that's solvable with a little bit of linear algebra, and that's how we all do it today, but I think in early machine learning days people were interested in learning rules.",
            "More than they were interested in objective function because they wanted to know how neurons were updating their own weights.",
            "And of course we can go to nonlinear."
        ],
        [
            "Faces so if the X is not linearly related to why, then we create a feature space and you've already seen me do this before.",
            "So here we've got a function of X which is equal to some inner product between the basis functions and weights.",
            "And if we apply the same sort of rules to those weights, we."
        ],
        [
            "Can also do it for them."
        ],
        [
            "Sean, so here's a."
        ],
        [
            "Basis this is a quadratic basis, so it's got three different parts."
        ],
        [
            "And we can sample from the quadratic basis by so we can look at."
        ],
        [
            "So I was doing in the kernel PCA lecture.",
            "We can say well for some random values of W. What sort of functions can this produce?",
            "And they're obviously all quadratics."
        ],
        [
            "So we sample 3 random values of W. Here we're sampling W 1, which is the offset is .87 W 2 is minus .38835 and W 3 is minus 2.58 and these are the basis.",
            "And this is the weighted sum of those basis.",
            "So we've got some positive weight on W one so .87, so around there.",
            "So that value should be .87.",
            "At zero there we've got a negative weight linear function, so it's slightly negative going down in this way, so you can see that this side of the quadratic is actually lower than this site, even though the axes are symmetric.",
            "And then we've got a negative weight minus two on the quadratic term, so the quadratic form is showing quite strongly double what it was in the first one.",
            "Well, it should be quadruple or something, so that's about 7:00 or something, so it's been.",
            "Then it's got the linear terminated, so below are still and we can sum those three things together.",
            "We get a function that's nonlinear.",
            "And that's what we think of as the."
        ],
        [
            "Feature space and Bernard talked about that, so here's another example where we've got a much lower weight on the quadratic term, so the nonlinearity."
        ],
        [
            "Is less and another example with a positive weight on the quadratic term Anna negative weight on the constant term and a slightly negative on the linear so you can sample different functions?"
        ],
        [
            "According to this system.",
            "Now, the systems that I like a bit more than that because they're local are the radial basis function type systems we've already talked about.",
            "And in this case."
        ],
        [
            "We've got for example, 3."
        ],
        [
            "Aces and the three bases can be summed together in a similar way, so we've got basis function.",
            "One has this.",
            "Center here minus one basis function two has a center of zero and basis function 3 has a center of one.",
            "And what we're doing to create function in this case is a weighted sum.",
            "For example, with a Gaussian sample from W to generate random function."
        ],
        [
            "Which gives us functions like this.",
            "So here we've got a small negative weight on the 1st basis, so yeah, that's leading to this small bump.",
            "We've got a small negative way on the second basis, but it's smaller, so we actually go up a little bit and a largest negative weight on the second or third basis, and we can keep doing."
        ],
        [
            "Fun."
        ],
        [
            "Actions like that with similar's."
        ],
        [
            "So in that case, the weight on the second basis is very small."
        ],
        [
            "So it hardly appears there we've got a 3."
        ],
        [
            "That if waits.",
            "And we can even learn in simple systems like this using the same learning rule we had before.",
            "And I like these examples.",
            "This is not how you do."
        ],
        [
            "Function learning, But in this case you can show how fun."
        ],
        [
            "And respond slow."
        ],
        [
            "Oakley so this."
        ],
        [
            "Because of the learning rule is now based on the basis, then you only up wait things which are nearby your data point.",
            "And this is the sort of thing we used to look at a lot.",
            "Then what the nature of the learning rule was an iteratively update."
        ],
        [
            "One data point data."
        ],
        [
            "Time, but we."
        ],
        [
            "Doing it in more comp."
        ],
        [
            "Systems like."
        ],
        [
            "Realnetworks."
        ],
        [
            "The Lynette."
        ],
        [
            "Items that were state of."
        ],
        [
            "We are on."
        ],
        [
            "Digit recognize."
        ],
        [
            "And we're doing."
        ],
        [
            "This."
        ],
        [
            "One data point at a."
        ],
        [
            "Time."
        ],
        [
            "In"
        ],
        [
            "Stead of doing what you might."
        ],
        [
            "I think it's the right thing to."
        ],
        [
            "2."
        ],
        [
            "It would be certainly for."
        ],
        [
            "This model which."
        ],
        [
            "Is to try and do something."
        ],
        [
            "Batch up."
        ],
        [
            "Nization"
        ],
        [
            "Some."
        ],
        [
            "2nd order method."
        ],
        [
            "Or"
        ],
        [
            "So eventually"
        ],
        [
            "The function does."
        ],
        [
            "Change."
        ],
        [
            "But really, what's going on in these systems?",
            "What's the mathematical interpretation?",
            "So we like looking at those learning rules, I think, particularly 'cause there's a cognitive science inspiration for machine learning, but fundamentally these learning rules are minimizing this error function, right?",
            "So this is an error function that is the classic sum of squares that Robert talked about some across data points between the squared difference between the output of the function and data point, why I?",
            "And that's the sum of squares error.",
            "Now I always get confused who invented the sum of squares error.",
            "Anyone, I mean someone did, but anyone know?",
            "Square.",
            "Mr Square Middle East and Mr. Square work together.",
            "I'm asking this privately 'cause I always forget there's too many Frenchmen starting with lemon law and I get confused between them.",
            "I think it was lizandra, but it might have been a garage.",
            "Come on, someone looking up someone with Internet.",
            "Look it up.",
            "OK, well actually I want to come back to this point again later.",
            "Why did they invent least squares?",
            "They invented least squares because they were making observations of planets and they had more unknowns.",
            "Sorry more observations than unknowns, so they knew about solving systems of simultaneous equations, but they had more observations than unknown.",
            "So which solution should they use?",
            "And least squares?",
            "Was the approach for.",
            "Doing that now, Gauss also claims to have invented least squares, and he claims he found the Planet series its orbit by applying these squared before LeBron Alexandra.",
            "Published or LaGrange, whichever one."
        ],
        [
            "So here there's now a cost function.",
            "And we can build design matrices and the cost function expresses the mismatch between our prediction and reality.",
            "And here I'm defining in the red fiai as being the evaluation of the basis function for.",
            "Each of well, for each of the basis functions, so there's this replace."
        ],
        [
            "In effect, so I'm moving to vector notation, which you know as you know I like, so I'm replacing that with an inner product.",
            "So W is your vector of."
        ],
        [
            "Parameters anfi is the basis function values for one data point."
        ],
        [
            "So we can minimize that we can compute its air."
        ],
        [
            "And what we find is that the gradient of the basis function is equal to the current error.",
            "So you always updating according to the error in both those perception examples and the regression examples.",
            "And in this case this sort of error the current error.",
            "This isn't the error function, but what the people used to call the error was.",
            "The difference between the observation and the target value so.",
            "That's the gradient."
        ],
        [
            "For all the data points and the gradient for what we can do is minimize that by steepest descent.",
            "So we initialize the algorithm with some value W, and we compute the gradient and go downhill.",
            "Of course, again, this is analytic to optimize."
        ],
        [
            "But Illustratively speaking, this is a sort of."
        ],
        [
            "Cheapest."
        ],
        [
            "Sent."
        ],
        [
            "Type approach, so you compute the gradient.",
            "And this is the quadratic form of the error function and you start heading down."
        ],
        [
            "Towards the minimum."
        ],
        [
            "Fortunately."
        ],
        [
            "Actually goes."
        ],
        [
            "Very, very slow."
        ],
        [
            "Be."
        ],
        [
            "Once it's done."
        ],
        [
            "It's getting OK, so the jump there wasn't 'cause it started going fast."
        ],
        [
            "That's nine."
        ],
        [
            "Orationes that's 10."
        ],
        [
            "28 or."
        ],
        [
            "Asians?"
        ],
        [
            "That's 32."
        ],
        [
            "Nations 42"
        ],
        [
            "5000 iterations 150 so here you're using the learning rule of to take the weight and update it by the sum of."
        ],
        [
            "All the gradients of each datapoint."
        ],
        [
            "So it can take a very long time.",
            "Now stochastic gradient descent is something that machine learning people started doing back in the 90s and do an enormous amount now.",
            "And the reason is because it tends to give you much faster solutions for very large datasets.",
            "So in stochastic gradient descent, that's the what the algorithm we affectively saw for the perceptron and the regression problems earlier that you take one data point and you look at its error and you update your weight function according to that.",
            "So stochastic gradient descent works.",
            "Very well if you've got 25 million data points, because if you've got 25 million data points.",
            "You need to compute the actual gradient to sum up the gradient with respect to each of those data points.",
            "25,000,000 sum but stochastic gradient says says don't bother doing that.",
            "Just go in an update according to the gradient of each individual data point.",
            "Now stochastic gradient descent has this."
        ],
        [
            "Learning rate parameter.",
            "OK, so just this is this is standard gradient descent with the sum over all data points.",
            "So your update rule is the weight vector is equal to the old weight vector minus the gradient.",
            "You descend the gradient.",
            "Stochastic gradient descent says."
        ],
        [
            "Well, let's.",
            "Change look at that.",
            "OK, so.",
            "Let's take that function there and say that's the error that been read."
        ],
        [
            "Find that is the Delta Y that we've seen before."
        ],
        [
            "And then let's replace the Delta, why I?"
        ],
        [
            "Some over North with multiple repeats of doing it for each individual data point, so this isn't exactly the same as that because here we are updating the weight function.",
            "Once for all N here we're updating the weight function for the first W and then that changes this Delta Yi.",
            "So every time you iterate, you're updating W, so these aren't exactly the same thing, But if you have a, if you have a sort of reduction criterion for your steepest descent, your learning rate.",
            "If you reduce this in the right way, it turns out that this probably converges what the right way is is an open question, and very often I think.",
            "Normal convergence on these things.",
            "I don't know the details of how people claim they've achieved it, but certainly in practice."
        ],
        [
            "On that.",
            "Fuck the function we looked at before."
        ],
        [
            "Or this is sort of effect you get now we're not following exactly the gradient we're following.",
            "This is the same example I showed you earlier with the basis functions with following that gradient with respect to one."
        ],
        [
            "Data point."
        ],
        [
            "And then you see."
        ],
        [
            "Little wiggles around."
        ],
        [
            "A bit broad."
        ],
        [
            "Speaking, it goes in there."
        ],
        [
            "That way, sometimes with."
        ],
        [
            "Full step sometimes."
        ],
        [
            "Large 'cause it dips."
        ],
        [
            "Things on the Val."
        ],
        [
            "You have the basis funk."
        ],
        [
            "At each step."
        ],
        [
            "So."
        ],
        [
            "While there are many iterations here."
        ],
        [
            "Each iteration is."
        ],
        [
            "Only looking at one data point, so the value."
        ],
        [
            "Two of these comes."
        ],
        [
            "When you've got very."
        ],
        [
            "Large datasets."
        ],
        [
            "Look down."
        ],
        [
            "Valley below it's actually wiggling around.",
            "That's why it's sort of called stochastic.",
            "On average, it tends to go in the right direction.",
            "But for very long."
        ],
        [
            "State sets which."
        ],
        [
            "This isn't one."
        ],
        [
            "This tends to be much faster than computing the correct gradient at each data point, and you'll see people using that leave.",
            "Did I?",
            "Oh, is there a typo?",
            "Yeah, there probably was a typo.",
            "Yes, sorry yes, that's a typo.",
            "Yeah, thanks leave that this here there's my mouse over there it is."
        ],
        [
            "That should be a negative OK, 'cause we're descending the gradient, sorry, thanks.",
            "So we can get a an approximation to the gradient with stochastic gradient descent, which is often much faster to use in practice, and this is how, say, Yan Lacune was doing MNIST on 60,000 digits in the late 1990s or even early 1990s.",
            "You could do it because you didn't have to solve a large update looking at quadratic error, you were looking at 60,000 digits.",
            "You present each digit one after the other, update the weight function of the weights of the neural networks.",
            "That was an adaptive basis system which he constructed to include translation invariants is in various things.",
            "It was very impressive piece of work in the late 90s, and it all worked because."
        ],
        [
            "Rustic gradient descent.",
            "As a field, we sort of kind of forgot about stochastic gradient descent for awhile, but it's now come back in.",
            "I think in a big way for deep learning.",
            "People are using a lot for deep learning.",
            "It's difficult to analyze.",
            "We used to call it online learning.",
            "Why don't we call it online learning anymore?",
            "OK, I guess 'cause that means doing a course on the Internet.",
            "But we didn't have the Internet back then, so we call it online learning and a lot of the statistical physics analysis was about trying to understand the characteristics of that in certain systems.",
            "For example, it's known that it often plateaus, so you start really nicely.",
            "Often the error function goes down, and then it seems to plateau, and then it goes down again in neural networks and people did a lot of analysis.",
            "In fact, my close colleague Magnus Rattray has some classic papers on on this plateau effect, and it's important because you want a long learning rate in this region and a short learning.",
            "It's in the other region and if you read Geoff Hinton's papers still today.",
            "If you read this knee paper, there's a large amount in the paper about how he started off with this learning rate, and then he switched to that learning rate and old papers used to have a lot about that, but it was sort of little bit heuristic.",
            "And.",
            "Less difficult to formalize some of the minimizations, so the modern view of error functions is, of course.",
            "One of the following.",
            "Either the error function has a probabilistic interpretation, in which case you're typically doing maximum likelihood, and this is the modern view.",
            "When I say that I mean from the perspective machine learning Gaussian, Laplace knew that, so that's not that modern.",
            "The error function is an actual loss function that you want to minimize, so that's empirical risk minimization which Bernard talked about, and I think that you can broadly see that the field is has been split along those two lines.",
            "People either take one perspective or the other.",
            "I mean, this is a broad generalization, so I tend to think are the error functions are probabilistic thing.",
            "So what I should really do is worry about my probabilistic model.",
            "If you bring an application to me, I start writing down.",
            "Probabilistic model.",
            "Where is a sort of Barnard would probably think more in these points of view, so if you bring an application to him, he would typically start writing down an optimization problem.",
            "Now the two things that then happen is you either can't solve the probabilistic model because it's non analytic or the optimization is NP hard.",
            "So what I do is I learn, look for either a simpler version of the probabilistic model that is analytic or I look to do approximate inference by sampling or variational methods or the Laplace approximation.",
            "What I think the optimization people do is they tend to relax the optimization so they say, well, I really want an A classic example.",
            "Is L1 minimization type problems, so they say I really want to minimize to find a sparse solution and the way I should find a sparse solution.",
            "Is by using the L0 norm on the parameters of my regression, but that's not tractable because which the L0 norm is drawn.",
            "In the white space.",
            "So again, you can think that this is like the in the version space.",
            "The L0 norm is stacked up along the.",
            "Axes, so you can't even really draw it properly.",
            "It's like a sort of a.",
            "If everything's the L0 norm counts the number of non zero terms, so every weight in your vector is 0, then the L0 norm is like a pointy stick sticking out of the axis, and then if this one here is 0 but the others are non zero you get these lines going along the axis.",
            "So it's basically a star.",
            "In the weight space it says I prefer things close to the center on the axis, and people tend to relax that to the L1 norm, which is actually a square and it leads to convex optimization.",
            "So that's sort of relaxation.",
            "People use for sparse models, but people use very many different relaxations for clustering.",
            "Spectral clustering is formulated around a relaxation from a discrete allocation of data points to continuous version of the allocation.",
            "So that's my broad perspective on.",
            "I don't do that, so.",
            "I'm not an expert, so I'm making that up a bit, but I definitely do this an in this case.",
            "We do we look at the interactive abilities and we try and approximate in one of a number of ways.",
            "So if you look at the.",
            "Last 15 million years of machine learning research has mainly focused on probabilistic interpretation.",
            "So let's graphical models are actually a way of doing probabilistic graphical models of either doing exactly with the junction tree algorithms.",
            "Graphical models over high dimensional discrete variables by specifying conditional independence ease.",
            "Gaussian processes are a way of doing probabilistic modeling over functions.",
            "Semidefinite programming has become a big thing as a relaxation of certain or even as the thing you want to do.",
            "The objective function you want to minimize?",
            "I mean people are doing a quadratic programming.",
            "Of course, for the support vector machine, so you can broadly see I think most of the last 15 years of machine learning as focusing."
        ],
        [
            "One of those two things.",
            "So the things that I'm not been talking about is how you do that.",
            "Some of that is being covered by Robert 2nd order methods.",
            "Conjugate gradient causing Newton and Newton effective heuristics, such as people when they do online gradient descent, they use something called momentum."
        ],
        [
            "Or stochastic gradient descent I should say.",
            "OK, so.",
            "Unsupervised learning.",
            "K means clustering is the I guess the classic example.",
            "I just want to talk about briefly in unsupervised learning.",
            "So in clustering, which is a classical machine learning.",
            "Again, it's also used in statistics requirement where you want to divide groups into according to different characteristics of the groups.",
            "So an example there would be different animal species or different political parties, and what we want to do is determine which groups they move into and what's harder."
        ],
        [
            "Perhaps the number of different groups, so for K means clustering, we might require a set of K cluster centers.",
            "This is the classic algorithm we want to know where the center of those clusters are and the assignment of each data point.",
            "To those centers.",
            "So the algorithm there simple algorithm that I want to sort of.",
            "Talk about we get onto it, but in terms of how it turns when you consider probability, initialize the cluster centers on data points.",
            "Assign each data point to the nearest cluster center an update each cluster center by setting."
        ],
        [
            "The mean of assigned data points.",
            "So the objective that people do in K means clustering is they have a number of clusters K. Which is associated with centers and then a number of data Y.",
            "And what you try and do is you minimize the sum across all clusters of the data points allocated to that cluster.",
            "So for those eyes that are allocated to J of the squared distance between.",
            "The mean and the data points.",
            "So this minimizes the sum of the Euclidean squared distance between point points and their associated cluster centers.",
            "K means clustering is not guaranteed to be a global minimum or unique.",
            "When you apply this algorithm, and this is a non convex optimization problem so."
        ],
        [
            "This is difficult to solve, but it's actually fast to solve.",
            "If you use such an algorithm.",
            "So here we've got some data."
        ],
        [
            "We initialize clusters randomly from the data."
        ],
        [
            "We then allocate all data points to their closest cluster center."
        ],
        [
            "We update the cluster centers to the mean of their allocated."
        ],
        [
            "Data points."
        ],
        [
            "And then, uh."
        ],
        [
            "Date each center according to the."
        ],
        [
            "Knew mean."
        ],
        [
            "And we repeat that."
        ],
        [
            "Process."
        ],
        [
            "And then eventually you don't get no change in cluster allocation, so I wanted to talk about this briefly because it's another simple algorithm that actually has a more complex interpretation.",
            "When you look at it probabilistically and we'll come back to that after I try and motivate why you go to probability instead of standard error functions so.",
            "He's got 2 steps.",
            "One is an update of the means and one is an update of the allocations and what I want to do later is sort of point out that the update of the allocations is like the eastep and any EM algorithm and the update of the means is like an M step in the EM algorithm.",
            "Yeah, OK.",
            "So I think I've gone slightly over my first half, so that will be I'll do next is."
        ],
        [
            "Introduce OK. Well I just finished with this.",
            "Clustering approaches and machine learning.",
            "I did have a section so I didn't introduce this section property.",
            "I moved from supervised learning to unsupervised learning and started talking about clustering without properly introducing it because I put these slides together an hour ago.",
            "But other I'm not going to talk about dimensionality reduction.",
            "Anyone know why?",
            "I'm not going to talk about dimensionality reduction?",
            "'cause you've had six hours on that already.",
            "So other clustering approaches in machine learning, spectral clustering.",
            "This is really cool.",
            "It allows clusters which aren't convex hulls.",
            "It's based on a relaxation of an optimization problem initially proposed by Shi and Malik.",
            "An important work in machine learning done by Andrew."
        ],
        [
            "Mike Jordan and someone else who I'm forgetting.",
            "This form of clustering only allows clusters that in the data space are convex hulls, so you can't have a cluster, for example that spins around in a circle or something, and that's when we look at clustering we think are these data are close together.",
            "They should be part of the same cluster, but this doesn't allow for that just allows data which are near incomes of Euclidean distance.",
            "So spectral clustering is a really cool way of doing that.",
            "Relaxation of an optimization problem."
        ],
        [
            "And then directly process is you're going to hear about from Zubin and Good point to say that at 5:00 o'clock DeLand is going to start going to talk to you about Kingmans coalescence, which is related to Bayesian nonparametric methods.",
            "So Dylan is going to come forward and give a talk.",
            "Then with Fernando not being here so to reach their processes are probabilistic formulation.",
            "So this is an optimization formulation and this is a probabilistic formulation.",
            "For a clustering albums that nonparametric nonparametric's, I think one of the really exciting areas of Bayesian learning.",
            "But Zubin's going to talk to you a lot about them as well as Peter Banks and John coming in and in and so I won't say too much more about."
        ],
        [
            "About so after the break, we'll come back and talk about maximum likelihood regression from a probabilistic perspective and want to do a bit more.",
            "One of the things I think is interesting is what people think Bayesian modeling is versus what it actually is.",
            "Bayesian modeling is not using Bayes rule.",
            "OK, I mean it does use Bayes rule, but if you use Bayes rule, you're not being Bayesian.",
            "OK people, some people disagree with Bayesian modeling.",
            "You can't disagree with Bayes rule.",
            "It's an obvious consequence of the laws of probability.",
            "So if you say I'm not going to use Bayes rule, you're just being dumb.",
            "But there is a difference between the Bayesian and frequentist approach, and I'll try and highlight where that comes from and what it is.",
            "Philosophically, it's associated with this epistemic and aleatoric uncertainty, and I'll use regression for motivating that first by maximum likelihood and then by Bayesian perspective.",
            "And if this time at the end, I'll briefly talk about EM algorithms and try and introduce variational methods briefly through the EM algorithm.",
            "OK, so back in 5 minutes I guess."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a 2 hour lecture that I wrote across the last two hours.",
                    "label": 0
                },
                {
                    "sent": "But it is based on some material I've had before it occurred to me, so I think I went.",
                    "label": 0
                },
                {
                    "sent": "For example, I assumed too much in terms of some of the Bayesian learning.",
                    "label": 0
                },
                {
                    "sent": "I thought Mark was going to do a bit more to introduce some Bayesian learning concepts, so I thought it might be fun to actually.",
                    "label": 0
                },
                {
                    "sent": "Go back to basics a little bit, which we didn't really do at the start of the summer school and try and motivate machine learning a bit, and perhaps what I like to call learning with probabilities.",
                    "label": 1
                },
                {
                    "sent": "Or in this talk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's what I'm calling it.",
                    "label": 0
                },
                {
                    "sent": "So the slides might be a bit funny because I've been slapping them together and not proofreading them as I go through, but I'm sure it won't matter too much.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I thought I'd start with these actually.",
                    "label": 0
                },
                {
                    "sent": "Just some sort of general stuff on what I think machine learning is, because some people tell you things like machine learning is just statistics.",
                    "label": 0
                },
                {
                    "sent": "And I think you know you could also, people could perhaps argue machine learning is just optimization.",
                    "label": 0
                },
                {
                    "sent": "What is machine learning?",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Why was the new field machine learning?",
                    "label": 0
                },
                {
                    "sent": "Required in the UK there was actually.",
                    "label": 0
                },
                {
                    "sent": "In the 1970s, I think it was there was something called the Lighthill report that really set back AI research in the UK for many many years because it said there was nothing being done in AI research that couldn't be done in a pre existing field such as signal processing or statistics.",
                    "label": 0
                },
                {
                    "sent": "So there was no need for a new field of AI and that was in the time of, you know, logic based AI.",
                    "label": 0
                },
                {
                    "sent": "But you know you could maybe try and make that argument today.",
                    "label": 0
                },
                {
                    "sent": "Why is there a need for field machine learning when there's so much overlap between what we do?",
                    "label": 0
                },
                {
                    "sent": "And what many other fields do?",
                    "label": 0
                },
                {
                    "sent": "So I want to talk a little bit about that.",
                    "label": 0
                },
                {
                    "sent": "So we want to endow the computers with ability to learn from data.",
                    "label": 0
                },
                {
                    "sent": "So we present data from sensors, the Internet experiments, and then we expect the computer to make sensible decisions where sensible.",
                    "label": 1
                },
                {
                    "sent": "I don't know what sensible means, but you know, informally, that's what we're expecting.",
                    "label": 0
                },
                {
                    "sent": "And of course, there are three traditional categories of machine learning, supervised learning, classification and regression.",
                    "label": 0
                },
                {
                    "sent": "Now, I would argue that these were certainly the areas when I started machine learning classification was still considered a big challenge.",
                    "label": 0
                },
                {
                    "sent": "Traditional classification, I'm presented with a set of inputs and a set of targets training data, and I expect to be tested on inputs drawn from a similar distribution and targets drawn from the same sort of distribution.",
                    "label": 0
                },
                {
                    "sent": "People did this with neural networks using.",
                    "label": 0
                },
                {
                    "sent": "Backpropagation to learn them, and in some sense the success of neural networks standard feedforward neural networks, really was the core of the machine learning community.",
                    "label": 0
                },
                {
                    "sent": "And then of course, you've heard about the support vector machine an.",
                    "label": 0
                },
                {
                    "sent": "I would argue myself.",
                    "label": 0
                },
                {
                    "sent": "My own perspective is that classification in that way is almost no longer interesting because it's a solved.",
                    "label": 0
                },
                {
                    "sent": "Solved problem from a research point of view.",
                    "label": 0
                },
                {
                    "sent": "I mean there are definitely interesting aspects such as semi supervised learning where you don't have.",
                    "label": 0
                },
                {
                    "sent": "You've got a lot of data without labels.",
                    "label": 0
                },
                {
                    "sent": "Can you use that to improve your classification?",
                    "label": 0
                },
                {
                    "sent": "Multitask learning.",
                    "label": 0
                },
                {
                    "sent": "Where you've done some learning from some related tasks and you want to know what can you take information about learning what cows look like to help you understand sheep things like that?",
                    "label": 0
                },
                {
                    "sent": "That's definitely unresolved.",
                    "label": 0
                },
                {
                    "sent": "But the classical thing we used to look at.",
                    "label": 0
                },
                {
                    "sent": "The M list data set.",
                    "label": 0
                },
                {
                    "sent": "Here is a bunch of handwritten digits.",
                    "label": 0
                },
                {
                    "sent": "What's the class of the digits?",
                    "label": 0
                },
                {
                    "sent": "As Bernard said, we can get to even slightly better than human performance on that data, and that was a big problem.",
                    "label": 0
                },
                {
                    "sent": "You couldn't do that in 1996 when I started the machine learning regression as well.",
                    "label": 0
                },
                {
                    "sent": "Standard regression.",
                    "label": 0
                },
                {
                    "sent": "I think we have good methods for dealing with that.",
                    "label": 0
                },
                {
                    "sent": "Personally, I use Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Is there are many variants of regression that are interesting, but in some sense I would say that the classical standard supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Scenarios are things that are being less studied at the leading conferences such as NIPS, ICML, UI, the leading journals with more into sort of things which might be thinking of unsupervised learning such as deep learning.",
                    "label": 0
                },
                {
                    "sent": "Of course, I've talked about dimensionality reduction already clustering, Bayesian nonparametric models.",
                    "label": 0
                },
                {
                    "sent": "You're going to hear about from Zubin.",
                    "label": 0
                },
                {
                    "sent": "And then I think probably one of the most challenging areas is reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And that's an area where there's still room for enormous amount of progress.",
                    "label": 0
                },
                {
                    "sent": "So learning from delayed feedback where you know a robot has to learn to make you a Cup of tea.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't, it doesn't really know much about the problem, it just gets a reward at the end.",
                    "label": 0
                },
                {
                    "sent": "It's how do you do that sort of thing?",
                    "label": 0
                },
                {
                    "sent": "That's really big challenge, and it's an area of a lot of ongoing research.",
                    "label": 0
                },
                {
                    "sent": "So that's my sort of overview.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's a few different examples of learning.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's the history of machine learning?",
                    "label": 1
                },
                {
                    "sent": "Well, I'd give you my personal perspective on what machine learning where it comes from, and I think it comes out of this connection.",
                    "label": 1
                },
                {
                    "sent": "This movement in artificial intelligence.",
                    "label": 0
                },
                {
                    "sent": "So there were there was this area of artificial intelligence that was about specifying logical rules.",
                    "label": 0
                },
                {
                    "sent": "Expert systems that really dominated in the 1970s.",
                    "label": 0
                },
                {
                    "sent": "The early machine learning people were actually a lot overlapped a lot with people in psychology.",
                    "label": 0
                },
                {
                    "sent": "For example, Jeff Hinton.",
                    "label": 0
                },
                {
                    "sent": "A background is a psychologist.",
                    "label": 0
                },
                {
                    "sent": "Also, Mike Jordan has a background as a psychologist and there was this group of people they call themselves the parallel distributed processing group and there was this interesting mix at the time with the idea that in the brain you had lots of distributed processing elements with high connectivity and it was very robust so you could take out cells and the system would still work and the system would learn.",
                    "label": 0
                },
                {
                    "sent": "So people were interested in trying to build.",
                    "label": 0
                },
                {
                    "sent": "Later models that had similar characteristics and the movement was called Connectionism.",
                    "label": 0
                },
                {
                    "sent": "And you can read about it on Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "Connectionism was the dominant force in machine learning, say in the late 1990s.",
                    "label": 1
                },
                {
                    "sent": "But and it was actually sort of driven by ideas of models of the brain and in.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Killer Rosenblatt, who built the so called Perceptron.",
                    "label": 0
                },
                {
                    "sent": "So it was.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think there may even be videos of this.",
                    "label": 0
                },
                {
                    "sent": "It was in 1962 and a learning system which could try and classify digits very simple, one, that you may have heard about before, and it was even based on a very simple model of a neuron.",
                    "label": 1
                },
                {
                    "sent": "So this is kind of quite powerfully exciting stuff.",
                    "label": 0
                },
                {
                    "sent": "In 1962 you can do a bit of learning based on an existing model of the brain.",
                    "label": 0
                },
                {
                    "sent": "Now I think any neuro scientist would say it's a rubbish model of the brain.",
                    "label": 0
                },
                {
                    "sent": "And as machine learning people actually, it is still quite for such a simple learning rule.",
                    "label": 0
                },
                {
                    "sent": "If you Colonel eyes it, you can do some quite powerful things, so, but it's a very simple system.",
                    "label": 0
                },
                {
                    "sent": "I think that that area was inspirational for the connectionist.",
                    "label": 0
                },
                {
                    "sent": "Some people were looking at extending the perceptron and the big sort of step forward was the multilayer perceptrons or the neural network.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "People could use to do nonlinear classification.",
                    "label": 0
                },
                {
                    "sent": "So to understand that joke.",
                    "label": 0
                },
                {
                    "sent": "You have to have a weird geeky overlap mix between some sort of Japanese computer game from the 1980s.",
                    "label": 0
                },
                {
                    "sent": "Plus an understanding of risk, empirical risk and that makes generalization for bounds based on VC dimension.",
                    "label": 0
                },
                {
                    "sent": "And Daniella couldn't set up this photo so but I'm using this photo to represent.",
                    "label": 0
                },
                {
                    "sent": "What happened to the machine learning community was that I mean?",
                    "label": 0
                },
                {
                    "sent": "So how does machine learning differ?",
                    "label": 1
                },
                {
                    "sent": "Why am I very proud of what the community I came to at that time did?",
                    "label": 0
                },
                {
                    "sent": "In some sense, I think it's a very dangerous thing to be one of these communities that gets inspired by how the brain works and we're going to be like, you know, we're going to use this stuff to do learning, and you know you can also do it for evolution.",
                    "label": 0
                },
                {
                    "sent": "You get genetic algorithms or artificial life.",
                    "label": 0
                },
                {
                    "sent": "You get particle swarm optimization blah blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "It can be useful to be inspired by biological systems.",
                    "label": 0
                },
                {
                    "sent": "Absolutely.",
                    "label": 0
                },
                {
                    "sent": "There is always a danger, though, that a large part of the research moves away from the theoretical side.",
                    "label": 0
                },
                {
                    "sent": "It's frightened of the mathematical side, and it just ends up hacking.",
                    "label": 0
                },
                {
                    "sent": "Playing with algorithms without any motivation.",
                    "label": 0
                },
                {
                    "sent": "What I think was very good about the connectionists, and you know you can mention people like Mike Jordan and Jeff Hinton in this.",
                    "label": 0
                },
                {
                    "sent": "They were never afraid of theoretical people that came on board.",
                    "label": 0
                },
                {
                    "sent": "So you ended up in the early days, so Jeff Hinton used to go.",
                    "label": 0
                },
                {
                    "sent": "They did this Boltzmann machine thing, which was cognitive model, inspired by Hopfield networks.",
                    "label": 0
                },
                {
                    "sent": "I believe with Terry Sadowski, but then they went out and told physicists about this model and its related to icing models.",
                    "label": 0
                },
                {
                    "sent": "So you got statistical physicists coming into the community and rather than saying oh that's all too complex and doesn't matter in practice they were absorbed into the community.",
                    "label": 0
                },
                {
                    "sent": "So by the time I was in the community there was this mix of engineers, psychologists, very few computer scientists actually.",
                    "label": 0
                },
                {
                    "sent": "And statistical physicists mathematicians interested in icing models interested in this book.",
                    "label": 0
                },
                {
                    "sent": "Hertz Gordon Palmer, is basically a statistical physics perspective on the sort of models people were looking at at the time, and that was the leading textbook, I guess until 1995, when Chris Bishop's first book came out and related everything to statistical pattern recognition, which is another step forward.",
                    "label": 0
                },
                {
                    "sent": "So when I came in to the community, there was this big transition where people started looking so.",
                    "label": 0
                },
                {
                    "sent": "That makes work support vector machines.",
                    "label": 0
                },
                {
                    "sent": "Kernel methods were all just emerging in 1996 OK, and that really revolutionized the field, so it went from being.",
                    "label": 0
                },
                {
                    "sent": "You know, when I came in, the reason I could get involved in machine learning is I understood the chain rule for differentiation.",
                    "label": 0
                },
                {
                    "sent": "OK, that's why it's quite complex, but that's how backpropagation worked.",
                    "label": 0
                },
                {
                    "sent": "If you look at the mathematical concepts you're hearing about here, now you're hearing Mark talking about.",
                    "label": 0
                },
                {
                    "sent": "Information geometry of parameter space.",
                    "label": 0
                },
                {
                    "sent": "For Bayesian modeling, you've got David Mackay in 1993.",
                    "label": 0
                },
                {
                    "sent": "I guess starting to publish on Bayesian models for neural networks, so you really have all this theoretical stuff being absorbed by the Community and the Community moving forward to an extent such that I think in many areas we actually.",
                    "label": 0
                },
                {
                    "sent": "Overtook the fields, we were competing.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With in terms of statistics, which at one stage was regarded as more rigorous than machine learning, but actually machine learning has really, you know it's putting a lot back in towards statistics.",
                    "label": 0
                },
                {
                    "sent": "So my personal view is that machine learning benefited greatly by incorporating these early ideas from psychology, but not being afraid to incorporate rigorous theory, and one of the things you're seeing now is this return to those type of models with deep learning.",
                    "label": 1
                },
                {
                    "sent": "These models of difficult to analyze models, but they're doing amazing stuff algorithmically.",
                    "label": 0
                },
                {
                    "sent": "And actually doing some pretty incredible unsupervised learning with these models.",
                    "label": 0
                },
                {
                    "sent": "But instead of just people playing with those things and saying algorithmically, here's this.",
                    "label": 0
                },
                {
                    "sent": "You've got people like Ian Murray doing great analysis of trying to do annealed importance sampling to estimate marginal likelihoods in these very complex models to try and understand better what's going on.",
                    "label": 0
                },
                {
                    "sent": "And I think it's a very powerful.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Community as a result of that.",
                    "label": 0
                },
                {
                    "sent": "So I think early machine learning was viewed with a lot of skepticism by the statistics community.",
                    "label": 1
                },
                {
                    "sent": "But certainly for me, say in the UK, alot of the people I'm most interested in talking to, our statisticians doing Bayesian learning in or what I think of Bayesian modeling.",
                    "label": 0
                },
                {
                    "sent": "And there's now enormous.",
                    "label": 0
                },
                {
                    "sent": "Interaction between machine learning and statistics, but I think there is still a difference.",
                    "label": 1
                },
                {
                    "sent": "So when people tell you what's going on, they say to you it's just just statistics done in a hacky way or whatever.",
                    "label": 1
                },
                {
                    "sent": "I think that's not true.",
                    "label": 0
                },
                {
                    "sent": "My personal view is that the philosophy of the two fields is fundamentally different.",
                    "label": 0
                },
                {
                    "sent": "Whatever you're doing at the moment, whatever your tasks are, if you are doing machine learning, you want to replace the human in the processing of your data set.",
                    "label": 1
                },
                {
                    "sent": "So there was this.",
                    "label": 0
                },
                {
                    "sent": "I had lunch at Nips.",
                    "label": 0
                },
                {
                    "sent": "We were doing a Gaussian process Workshop and Zubin Ghahramani and Tony O'hagan were there.",
                    "label": 0
                },
                {
                    "sent": "Tony O'hagan's, a leading statistician who did a lot to introduce Gaussian process to the statistics community and a very well known Bayesian statistician and Tony sort of said I don't believe you can ever take a data set and.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Just analyze it by computer without a human being involved in the loop to interpret the analysis, which I sort of agree with in terms of the technology we have today.",
                    "label": 0
                },
                {
                    "sent": "But Zuben immediately said well, but unless you believe that the humans doing anything magical in less you believe there's a ghost in the machine, to put it one way, you should be able to replace whatever the human is doing as well.",
                    "label": 0
                },
                {
                    "sent": "Now I think that statistics as a community is about summarizing data to the extent that humans.",
                    "label": 0
                },
                {
                    "sent": "Can interpret the data so you try and say the mean of this data is 7.",
                    "label": 0
                },
                {
                    "sent": "So if you look if you go back to when statistics became field in its own right, they were often working with social science.",
                    "label": 0
                },
                {
                    "sent": "So if you want to understand poverty in Manchester versus poverty in London, you could take the average income of the people in Manchester and find that it's 12 shillings and sixpence per year, and you can take the average income of people in London and find it.",
                    "label": 0
                },
                {
                    "sent": "18 shillings per year, and then you want to know the difference.",
                    "label": 0
                },
                {
                    "sent": "The fact that these numbers are different doesn't mean anything.",
                    "label": 0
                },
                {
                    "sent": "Are these populations different?",
                    "label": 0
                },
                {
                    "sent": "So statistics itself?",
                    "label": 0
                },
                {
                    "sent": "We know what that's like baseball.",
                    "label": 0
                },
                {
                    "sent": "You say his batting average is well, so it runs batted in is .4 or in cricket you know you say the batting averages 30 runs.",
                    "label": 1
                },
                {
                    "sent": "That's what statistics means when we say statistics were talking mathematical statistics, which is really the study of whether the fact that those numbers are different means that the populations are different.",
                    "label": 0
                },
                {
                    "sent": "And that's what statistics were set up to do.",
                    "label": 0
                },
                {
                    "sent": "So it's set up to feed to understand the numbers you're feeding to humans.",
                    "label": 0
                },
                {
                    "sent": "But machine learning set up to.",
                    "label": 0
                },
                {
                    "sent": "Eliminate humans.",
                    "label": 0
                },
                {
                    "sent": "Kind of fundamentally, we're aiming for the AI singularity OK, I mean.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, anyone in the field knows this so far away from it, but we don't have to worry yet.",
                    "label": 0
                },
                {
                    "sent": "It's like the sun going out, but that is really our aim that we don't want.",
                    "label": 0
                },
                {
                    "sent": "You know, I'm I presented the Gaussian process latent variable model to the start group in Sheffield.",
                    "label": 0
                },
                {
                    "sent": "Uh, once and Tony O'hagan was there actually, but at the end of it, one of the lecturers in the group say, OK, that's OK, that's fine, but what do you tell the client?",
                    "label": 0
                },
                {
                    "sent": "I'm like who's the client?",
                    "label": 0
                },
                {
                    "sent": "Statisticians are always worrying about a client.",
                    "label": 0
                },
                {
                    "sent": "There's someone who's comes to them with data, and they're trying to say something about the data.",
                    "label": 0
                },
                {
                    "sent": "Tell the client computer P value.",
                    "label": 0
                },
                {
                    "sent": "I don't think we care about the client, because in some sense we're just trying to integrate it into computer program, which I'm trying to provide a module in a computer program that has an end result, like those faces.",
                    "label": 0
                },
                {
                    "sent": "You can animate the faces, right?",
                    "label": 0
                },
                {
                    "sent": "The client there is graphics guy who's drawing, so I don't tell him anything.",
                    "label": 0
                },
                {
                    "sent": "The computer does all of it, so I think that's the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Difference, but for the moment the two overlap very strongly.",
                    "label": 1
                },
                {
                    "sent": "Why do they overlap at the moment?",
                    "label": 0
                },
                {
                    "sent": "Because we're not capable of replacing the human so very often we need to say something to the human about the data, so we're interested in computing these numbers and the sort of things statisticians do.",
                    "label": 0
                },
                {
                    "sent": "I think also.",
                    "label": 0
                },
                {
                    "sent": "One of the main roles of statistics.",
                    "label": 0
                },
                {
                    "sent": "Again, these are personal views, but you can disagree or agree or whatever.",
                    "label": 0
                },
                {
                    "sent": "A lot of statistics in terms of what the field was set up to do with solved quite early on, so in terms of being able to infer draw conclusions about data about if you've got two sets of crops and you're fertilizing them in different ways, you want to know which fertilizer was better.",
                    "label": 0
                },
                {
                    "sent": "Randomizing the samples.",
                    "label": 0
                },
                {
                    "sent": "These sort of ideas came out early there sort of model free.",
                    "label": 0
                },
                {
                    "sent": "They allow you to infer things about the data clinical trials.",
                    "label": 0
                },
                {
                    "sent": "A lot of those questions again are solved and statisticians have moved into things which are much more model based, which originally the field tried to avoid.",
                    "label": 0
                },
                {
                    "sent": "So Bayesian I don't think Bayesian statistics personally is a complete oxymoron because statistics set itself up as a field to avoid modeling.",
                    "label": 0
                },
                {
                    "sent": "They didn't want to write down what was going on in the data because they were looking at social data where there was no physical model.",
                    "label": 0
                },
                {
                    "sent": "What physicists were doing at the time were saying I have a model of the universe, or have Newton's equations.",
                    "label": 0
                },
                {
                    "sent": "I can observe planets, I can infer the way to the planet, they had a physical model.",
                    "label": 0
                },
                {
                    "sent": "Statistics set it up to self up, say well, we don't understand anything about social interactions, but we can compute numbers and ask questions in the Model 3 way.",
                    "label": 0
                },
                {
                    "sent": "So Bayesian statistics seems like an oxymoron to me because in Bayesian in the Bayesian world you always be explicit about your model and that's what physicists do actually ended before statistics came and continue to do in parallel with statistics.",
                    "label": 0
                },
                {
                    "sent": "So that sort of personal feeling in philosophy.",
                    "label": 0
                },
                {
                    "sent": "But actually that doesn't matter because today.",
                    "label": 0
                },
                {
                    "sent": "Statisticians are doing Bayesian modeling all the time, and they're doing it really, really well, so you know it's you know a lot of statisticians would disagree with me there.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I think machine learning has this overlap with cognitive science, which is helpful, and it brings in a lot of insight, and I think particularly this.",
                    "label": 0
                },
                {
                    "sent": "This this.",
                    "label": 0
                },
                {
                    "sent": "This little point here is important, so you have to be careful about getting tide up in mathematical formalisms because they can be misleading.",
                    "label": 0
                },
                {
                    "sent": "Fallacy that aerodynamically a bumblebee can't fly.",
                    "label": 0
                },
                {
                    "sent": "I mean people say, did you know that aerodynamically a bumblebee can't fly?",
                    "label": 0
                },
                {
                    "sent": "OK, that's the dumbest thing to say ever.",
                    "label": 1
                },
                {
                    "sent": "Because it's a limitation of the model, not a fact.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so and I was driving at this.",
                    "label": 0
                },
                {
                    "sent": "You know there was a theme to some of the stuff I was talking about in dimensionality reduction about the curse of dimensionality, so and so forth you can get lost in mathematical proofs about what happens in these things, which actually betray what your own instincts are.",
                    "label": 0
                },
                {
                    "sent": "If you look at what humans are doing.",
                    "label": 0
                },
                {
                    "sent": "So a lot of problems that we study are apparently NP hard or whatever it's in.",
                    "label": 0
                },
                {
                    "sent": "The computer vision is insolvable, or this sort of thing, but in practice cognitive science shows us that we're solving these problems all the time.",
                    "label": 0
                },
                {
                    "sent": "Or were capable of generalizing over few examples, despite the fact that the statistical underpinnings are not strong, so that's clearly a limitation of the model rather than the fact that this is a bumblebee effect.",
                    "label": 0
                },
                {
                    "sent": "Obviously a bumblebee can fly, and the reason people say aerodynamic you can't fly is what they mean is if you apply the aerodynamic rules that you apply to a jumbo jet under those rules, the bumblebee can't fly, but of course the jumbo jet is flying under very different.",
                    "label": 0
                },
                {
                    "sent": "Reynolds numbers to the bumblebee.",
                    "label": 0
                },
                {
                    "sent": "The bumblebee is the effective viscosity to the bumblebee is much higher, so it's different.",
                    "label": 0
                },
                {
                    "sent": "You need a different model, so mathematical foundations are still really important because they understand help us understand the capabilities of of our algorithms.",
                    "label": 1
                },
                {
                    "sent": "But I think one thing that machine learning does really well is.",
                    "label": 0
                },
                {
                    "sent": "It's very good at going out empirically and saying look at this result.",
                    "label": 0
                },
                {
                    "sent": "I've created this crazy funky deep hierarchy of Boltzmann machines and I can get this kind of supervised learning result even though I can't prove much about when this will converge.",
                    "label": 1
                },
                {
                    "sent": "So we mustn't restrict our ambitions to the limitations of current mathematical formalisms, and humans can give us a lot of inspiration in terms of what we can do in order to go and seek beyond.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With standard mathematical models.",
                    "label": 0
                },
                {
                    "sent": "So I guess I've already said this early statistics had a lot of success with the idea of a statistical sort of proof and inverted commas.",
                    "label": 1
                },
                {
                    "sent": "I computed the mean of these two tables of numbers.",
                    "label": 1
                },
                {
                    "sent": "There are different.",
                    "label": 0
                },
                {
                    "sent": "Does this prove anything?",
                    "label": 1
                },
                {
                    "sent": "Well, it depends on the answer depends on how many numbers generated, how many are and how big the differences, and we randomized your samples so and so forth, whether it was correlation.",
                    "label": 0
                },
                {
                    "sent": "At least to hypothesis testing, and I'm very subjected to this now because I work in an Institute with a lot of doctors and they always come into my office and say, can you compute the P value for this because?",
                    "label": 0
                },
                {
                    "sent": "Every experiment they do their expected to provide AP value, even whether it's interesting or not.",
                    "label": 0
                },
                {
                    "sent": "It's it's important, though it's an important area, it's not what I'm interested in, and classical hypothesis testing.",
                    "label": 0
                },
                {
                    "sent": "The questions you can ask really about your data are quite limiting, so that can have the effect of limiting science too, but there are many, many successes, so who knows who student worked for?",
                    "label": 1
                },
                {
                    "sent": "I mean, his name was actually got it, but Guinness.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there you go.",
                    "label": 0
                },
                {
                    "sent": "Guinness is what one of the largest brewing corporations in the world.",
                    "label": 0
                },
                {
                    "sent": "While they wanted the largest brewing corporations in the world.",
                    "label": 0
                },
                {
                    "sent": "The early on adapted statistical methodology's to make sure they could brew beer consistently.",
                    "label": 0
                },
                {
                    "sent": "Why is it that you can pick up wine from Australia?",
                    "label": 0
                },
                {
                    "sent": "That is very, very high quality because people did analysis of what was important in the creation of why these little variations that were occurring on a year on year basis.",
                    "label": 0
                },
                {
                    "sent": "People get out.",
                    "label": 0
                },
                {
                    "sent": "People avoid them now because they understand.",
                    "label": 0
                },
                {
                    "sent": "Even though they don't understand necessarily the system of what's going on in the brewing, precisely, they can check variations.",
                    "label": 0
                },
                {
                    "sent": "If I add this much yeast.",
                    "label": 0
                },
                {
                    "sent": "If I put it this temperature, they can check which is significant with randomization and hypothesis testing.",
                    "label": 0
                },
                {
                    "sent": "So Gossett work for Guinness and Guinness is one of the largest brewing companies in the world.",
                    "label": 0
                },
                {
                    "sent": "He came up with the student T he came up with T tests.",
                    "label": 0
                },
                {
                    "sent": "These sort of things.",
                    "label": 0
                },
                {
                    "sent": "Basically, in an effort to help them ensure they brewing a consistent brew, now I don't know the details of the story, but I think a nice ending to that story would be that statistics enabled.",
                    "label": 1
                },
                {
                    "sent": "Guinness to be the large massive Brewing Company it is today.",
                    "label": 0
                },
                {
                    "sent": "Certainly stout is overrated, sorry need so it can't be their fundamental product.",
                    "label": 0
                },
                {
                    "sent": "I mean, I say they advertise it with Italians and Spanish and funky weird adverts all the time so it can't be that it tastes good 'cause they keep on having these weird things to advertise it, so it must be statistics.",
                    "label": 0
                },
                {
                    "sent": "But there are many open questions.",
                    "label": 0
                },
                {
                    "sent": "For example, I think that when you talk to statistician, not all statisticians but graduate level educated statisticians about causality, they are paranoid.",
                    "label": 0
                },
                {
                    "sent": "They say you can't infer causality.",
                    "label": 0
                },
                {
                    "sent": "It's not possible to infer causality yet.",
                    "label": 0
                },
                {
                    "sent": "You know when I did my first lecture only work anymore, but I kicked that thing there and the thing went off right.",
                    "label": 0
                },
                {
                    "sent": "That was the plug connector.",
                    "label": 0
                },
                {
                    "sent": "I immediately inferred causality from one data point, so we know humans need causality an we may not always get it right.",
                    "label": 0
                },
                {
                    "sent": "But statisticians became afraid of causality because correlation and causality doesn't necessarily imply causality.",
                    "label": 0
                },
                {
                    "sent": "So they almost eliminate it.",
                    "label": 0
                },
                {
                    "sent": "Causality mean there's many papers on it saying you know, you remove causality in any analysis of data.",
                    "label": 0
                },
                {
                    "sent": "If that's the case, then all the human reasoning that we're doing where we're on furring causes, whether they're wrong or right.",
                    "label": 0
                },
                {
                    "sent": "But to help us with understanding a system, we do look for causality.",
                    "label": 0
                },
                {
                    "sent": "It's very different, it's very.",
                    "label": 0
                },
                {
                    "sent": "Wrong according to statistics, but of course it's not wrong.",
                    "label": 0
                },
                {
                    "sent": "It's an important open question and you know something that Bernards looking at extensively gave a nips invited talk on it last year and a lot of leading statisticians are looking at it as well.",
                    "label": 0
                },
                {
                    "sent": "But traditionally, statistics avoided causality.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's William Sealy Gossett and they were sort of.",
                    "label": 0
                },
                {
                    "sent": "I like to think of the early statisticians there, sort of Edwardian English gentleman.",
                    "label": 0
                },
                {
                    "sent": "That's a classic Edwardian English gentleman.",
                    "label": 1
                },
                {
                    "sent": "I'm glad I didn't live in those days 'cause I could never have grown that mustache.",
                    "label": 0
                },
                {
                    "sent": "This is like 8 days growth.",
                    "label": 0
                },
                {
                    "sent": "So I'm glad I'm not Edwardian.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, and as I mentioned already, that.",
                    "label": 0
                },
                {
                    "sent": "The statistics is really about computing numbers and really.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Matt Mathematical statistics is the world that we are interested in.",
                    "label": 0
                },
                {
                    "sent": "Some machine learning and probability.",
                    "label": 1
                },
                {
                    "sent": "Where does probability coming into machine learning well, the world is an uncertain place.",
                    "label": 1
                },
                {
                    "sent": "For example, I got up today expecting to sit quietly in the audience.",
                    "label": 0
                },
                {
                    "sent": "For six hours of lectures and you know things happen, things change and you need to deal with that.",
                    "label": 1
                },
                {
                    "sent": "So epistemic uncertainty is 1 type of uncertainty, so this is uncertainty arising through lack of knowledge.",
                    "label": 1
                },
                {
                    "sent": "So what color socks is that person wearing?",
                    "label": 0
                },
                {
                    "sent": "So what color socks am I wearing?",
                    "label": 0
                },
                {
                    "sent": "Very good you see.",
                    "label": 0
                },
                {
                    "sent": "Normally that works better in England when you're giving your lectures in the middle of the winter.",
                    "label": 1
                },
                {
                    "sent": "And then aleatoric uncertainty is uncertainty arising through an underlying stochastic system.",
                    "label": 1
                },
                {
                    "sent": "So if I if I take a sheet of paper here and I drop it, where will it fall?",
                    "label": 0
                },
                {
                    "sent": "Well, I could do it.",
                    "label": 0
                },
                {
                    "sent": "Similar way many many times and I wouldn't be able to say.",
                    "label": 0
                },
                {
                    "sent": "Now those are the two broad categories of uncertainty, and here's the real.",
                    "label": 0
                },
                {
                    "sent": "This is the right way to think about these uncertainties, right so?",
                    "label": 0
                },
                {
                    "sent": "Two ways of watching a football match, right?",
                    "label": 0
                },
                {
                    "sent": "You watch it live.",
                    "label": 0
                },
                {
                    "sent": "And that's aleatoric uncertainty.",
                    "label": 0
                },
                {
                    "sent": "The other one is you can't watch it live because you're at work or something and you record it.",
                    "label": 0
                },
                {
                    "sent": "On your video recorder or your Sky plus box or whatever, and then you come back later or it was on really early in the morning and you play it as if it was live.",
                    "label": 0
                },
                {
                    "sent": "Now as far as you're concerned, if you don't know the result, it's a bit spoiled.",
                    "label": 0
                },
                {
                    "sent": "If someone walks in and says oh, they want, you know, but as long as you can maintain the illusion that you don't know the result, the uncertainties the same.",
                    "label": 0
                },
                {
                    "sent": "There is an actual difference, and this difference was brought home to me when I was doing this with rugby, England versus France in the World Cup semifinal.",
                    "label": 0
                },
                {
                    "sent": "I think it was.",
                    "label": 0
                },
                {
                    "sent": "I've tried to forget about it.",
                    "label": 0
                },
                {
                    "sent": "We had a really good quarter finals.",
                    "label": 0
                },
                {
                    "sent": "It was, I think, to get to the semifinal.",
                    "label": 0
                },
                {
                    "sent": "We had an excellent run to the team was rubbish but they just had to beat France that weren't looking that good and I started watching the game and they were just appalling and they were down by a certain number of points and I thought I can't.",
                    "label": 0
                },
                {
                    "sent": "You know I'm just going to check what the final score was so I left and went and looked.",
                    "label": 0
                },
                {
                    "sent": "Obviously you can't do that if it's aleatoric.",
                    "label": 0
                },
                {
                    "sent": "If you're watching it live, but I was watching it recorded so I do a lot of watching recorded and that epistemic uncertainty, because I can always go and find out the result is known.",
                    "label": 0
                },
                {
                    "sent": "I just don't know it.",
                    "label": 0
                },
                {
                    "sent": "With Aleatoric is the result is unknown.",
                    "label": 0
                },
                {
                    "sent": "Now there's a sort of overlap thing here, which is that why does this thing drop in different ways?",
                    "label": 0
                },
                {
                    "sent": "Every time I drop it, it's I assume it's slightly different initial conditions.",
                    "label": 0
                },
                {
                    "sent": "Chaos theory, yeah, very small variations in initial conditions.",
                    "label": 0
                },
                {
                    "sent": "Even in a deterministic system, so actually Allie at all, I think pretty much unless you get down to the quantum level and I don't understand quantum mechanics enough to make a claim about that, but let's ignore quantum levels.",
                    "label": 0
                },
                {
                    "sent": "Let's just assume that this is chaos causing these things.",
                    "label": 0
                },
                {
                    "sent": "All aleatoric uncertainty in some senses, epistemic uncertainty.",
                    "label": 0
                },
                {
                    "sent": "If you knew precisely the initial conditions with which I'm dropping it, and the state of everything in the room, then we would be able to compute where that would land.",
                    "label": 0
                },
                {
                    "sent": "But.",
                    "label": 0
                },
                {
                    "sent": "Such small variations effectively make this statistical noise aleatoric uncertainty.",
                    "label": 0
                },
                {
                    "sent": "So Laplace this is called Laplace indeterminism.",
                    "label": 0
                },
                {
                    "sent": "To sum it, well, that's really about epistemic uncertainty, but the two overlap a little bit more, I think than we're being honest about here, because fundamentally, the aleatoric uncertainty is something that is occurring because I don't understand the initial conditions.",
                    "label": 0
                },
                {
                    "sent": "But let's think about, I mean, but let's just make the division.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that is genuinely stochastic and epistemic ISM.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we need a framework to characterize this uncertainty and we use.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ability for doing that was one of the frameworks.",
                    "label": 0
                },
                {
                    "sent": "So people talk about Thomas Base, but number one.",
                    "label": 0
                },
                {
                    "sent": "We don't really have a picture of Thomas Bayes.",
                    "label": 0
                },
                {
                    "sent": "The one people show is unlikely to be Thomas Bayes 'cause Thomas Bayes was a non conformist minister.",
                    "label": 0
                },
                {
                    "sent": "And that's what non conformist ministers look like.",
                    "label": 0
                },
                {
                    "sent": "They have big wigs.",
                    "label": 0
                },
                {
                    "sent": "And this is Richard Price, who was a Welsh philosopher, an essay writer.",
                    "label": 1
                },
                {
                    "sent": "And if you read Thomas Bayes essay your hips as a long introduction to it, and then there's an essay, and the introduction is written by price, who also edited the essay.",
                    "label": 0
                },
                {
                    "sent": "The introduction is really easy to understand and the essay isn't so a lot of the claims for what's going on the essay or written by this guy.",
                    "label": 0
                },
                {
                    "sent": "Richard Price, who we know are not more about.",
                    "label": 0
                },
                {
                    "sent": "He's also political essayist.",
                    "label": 0
                },
                {
                    "sent": "He's involved in, you know, he was writing about the American Revolution.",
                    "label": 0
                },
                {
                    "sent": "He's interesting.",
                    "label": 0
                },
                {
                    "sent": "He's more like a Benjamin Franklin type figure.",
                    "label": 0
                },
                {
                    "sent": "Where is Thomas Bayes?",
                    "label": 0
                },
                {
                    "sent": "We know very little about, but he was the one that presented it to the Royal Society and you can read about Richard Price Online.",
                    "label": 0
                },
                {
                    "sent": "He was Welsh and he came to London and one of my best friends is called Jonathan.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Price an I don't know if he's related or not, probably not.",
                    "label": 0
                },
                {
                    "sent": "But then who's the man?",
                    "label": 0
                },
                {
                    "sent": "This is to me.",
                    "label": 0
                },
                {
                    "sent": "Is the man Laplace?",
                    "label": 0
                },
                {
                    "sent": "So Laplace actually didn't know about Thomas Bayes, but he came to similar conclusions about how you should try and deal with data and dealing with uncertainty independently an.",
                    "label": 0
                },
                {
                    "sent": "I think he almost did.",
                    "label": 0
                },
                {
                    "sent": "If you read the plus, I don't think I do anything that Laplace hadn't thought of.",
                    "label": 0
                },
                {
                    "sent": "Of course I have a computer so I can.",
                    "label": 0
                },
                {
                    "sent": "Empirically, do things that he thought of but couldn't do in practice.",
                    "label": 0
                },
                {
                    "sent": "The class was interested in.",
                    "label": 0
                },
                {
                    "sent": "Initially he was interested in gambling, so he was interested in if a dice was unfair.",
                    "label": 0
                },
                {
                    "sent": "For example, how could you tell when the dice was unfair and the French called this an English dice?",
                    "label": 0
                },
                {
                    "sent": "I don't know what the English called an unfair dice, but actually he reduces.",
                    "label": 0
                },
                {
                    "sent": "He looks at binomial, he looks at a coin coin, tosses and he looks at he does.",
                    "label": 0
                },
                {
                    "sent": "He does a Bayesian treatment of coin tosses he doesn't know about beta distributions, so he can't computer beta posterior and what he actually does is he's the second person to write down the Gaussian distribution after Dimov and he writes it down in computing a Bayesian posterior.",
                    "label": 0
                },
                {
                    "sent": "Over dice flip using Laplace's approximation.",
                    "label": 0
                },
                {
                    "sent": "The 2nd order expansion about the mode, and he's doing that in.",
                    "label": 0
                },
                {
                    "sent": "I think the 17 late 1700s.",
                    "label": 0
                },
                {
                    "sent": "That's pretty incredible that he you know, so Laplace approximation, which is still one of the standard methods of approximating Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "He invented the Gaussian distribution to do it, so I think that's pretty cool that the Gaussian distribution was invented for Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "He also shows in his paper, which again the translation is quite readable.",
                    "label": 0
                },
                {
                    "sent": "Annotation is very modern, so if you're French, I suspect the original paper is quite readable.",
                    "label": 0
                },
                {
                    "sent": "He shows that in the limit, as the data goes high, the Laplace approximation becomes correct.",
                    "label": 0
                },
                {
                    "sent": "So, and he's considering that limit.",
                    "label": 0
                },
                {
                    "sent": "So for him it's not even.",
                    "label": 0
                },
                {
                    "sent": "An approximation, but he didn't know about the beta distribution.",
                    "label": 0
                },
                {
                    "sent": "The system he looks at it completely analytic for us today, but I guess the normalizer of the beta distribution wasn't known at that time.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the classic sort of thing we tend to do in machine learning, moving away a bit from motivation and philosophy.",
                    "label": 0
                },
                {
                    "sent": "I forgot to look at time.",
                    "label": 0
                },
                {
                    "sent": "I have no idea of time, so let me just.",
                    "label": 0
                },
                {
                    "sent": "That's not too bad, is it?",
                    "label": 0
                },
                {
                    "sent": "Is inputs and targets.",
                    "label": 0
                },
                {
                    "sent": "So we've got some set of inputs and we want to predict a target, and for binary classification that should either be one.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or minus one.",
                    "label": 0
                },
                {
                    "sent": "So the sort of examples we've already talked about document categorization who have failed detected face belongs to detecting faces, and if.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ages classifying digits from binary data.",
                    "label": 0
                },
                {
                    "sent": "Now the perceptron is really the foundation of machine learning in the sense that you take a data point XI and you predict that it belongs to a class.",
                    "label": 1
                },
                {
                    "sent": "Why I if the sum of some weights times that input plus a bias is greater than 0?",
                    "label": 0
                },
                {
                    "sent": "So I like the vector notation, so there it is in vector notation.",
                    "label": 0
                },
                {
                    "sent": "The inner product between the weights and the input plus a bias is greater than 0, otherwise we assume it's minus one.",
                    "label": 0
                },
                {
                    "sent": "That was what Rosen black sort of said in 1957.",
                    "label": 0
                },
                {
                    "sent": "He died quite young.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately there's a memorial to him.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At his old institution, so the perceptron algorithm is really quite cool.",
                    "label": 0
                },
                {
                    "sent": "It's actually can be solved by linear programming, and it is just a linear program, because unfortunately the one bit they did manage to wipe off.",
                    "label": 0
                },
                {
                    "sent": "When you look at the perceptual algorithm is just looking for a feasible region.",
                    "label": 0
                },
                {
                    "sent": "In the W space.",
                    "label": 0
                },
                {
                    "sent": "So you can draw it in two ways you can draw.",
                    "label": 0
                },
                {
                    "sent": "The vector W in a 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "I shouldn't put those in 'cause that's like maximum margin.",
                    "label": 0
                },
                {
                    "sent": "You're looking for a linearly separable region.",
                    "label": 0
                },
                {
                    "sent": "You're satisfied you're feasible if you can get all the negative points in one side and all the positive points on the other side.",
                    "label": 0
                },
                {
                    "sent": "But there's another way of drawing it, so in this case you've got 1 W and you draw the W and you're drawing in X space and the exes are points in the space.",
                    "label": 0
                },
                {
                    "sent": "Right now what Robert was talking about, I believe he was drawing earlier is the what we would feasible region, which we would think of as the version space.",
                    "label": 0
                },
                {
                    "sent": "This is sometimes called.",
                    "label": 0
                },
                {
                    "sent": "So these are now linear constraints.",
                    "label": 0
                },
                {
                    "sent": "Every data point is providing a linear constraint.",
                    "label": 0
                },
                {
                    "sent": "If you draw in W space, then the weight vector you're interested in is now a point as the X is.",
                    "label": 0
                },
                {
                    "sent": "We're in here and every data point provides.",
                    "label": 0
                },
                {
                    "sent": "Well, actually it's the data times.",
                    "label": 0
                },
                {
                    "sent": "Its label provides a linear constraint.",
                    "label": 0
                },
                {
                    "sent": "OK, so every datapoint is giving you a linear constraint, which gives you a feasible region, and the perceptron algorithm is actually.",
                    "label": 0
                },
                {
                    "sent": "An odd way, which I think I can't.",
                    "label": 0
                },
                {
                    "sent": "Robert you know about its worst case performance or anything.",
                    "label": 0
                },
                {
                    "sent": "It's got weird worst case, but it's average case I think is good.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it has been analyzed a bit in terms of linear programming.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure of the details, but it has, I think, very bad.",
                    "label": 0
                },
                {
                    "sent": "Worst case performance, but quite good.",
                    "label": 0
                },
                {
                    "sent": "Average case performance.",
                    "label": 0
                },
                {
                    "sent": "So you've got all these data points here.",
                    "label": 0
                },
                {
                    "sent": "These zeros you've got the Y which dictates which direction this arrow is pointing in.",
                    "label": 0
                },
                {
                    "sent": "If you for example had this data point change label like this, then that region becomes and then you've got no feasible region.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's an inseparable problem in the perceptron algorithm.",
                    "label": 0
                },
                {
                    "sent": "The other weird thing about when you do have an inseparable problems.",
                    "label": 0
                },
                {
                    "sent": "Within the perceptron algorithm, you do end up with a reasonable solution.",
                    "label": 0
                },
                {
                    "sent": "If there's a learning rate, and if you reduce the learning rate.",
                    "label": 0
                },
                {
                    "sent": "In a slow enough way, you will actually find the optimal hyperplane.",
                    "label": 0
                },
                {
                    "sent": "This sort of thing that was known in 19.",
                    "label": 0
                },
                {
                    "sent": "Well, it was known easily by the 1990s when I came into the field and the algorithm is basically you.",
                    "label": 0
                },
                {
                    "sent": "You take a wait and you add to the weight.",
                    "label": 0
                },
                {
                    "sent": "Some learning rate times the label value times the datapoint value.",
                    "label": 0
                },
                {
                    "sent": "What that says.",
                    "label": 0
                },
                {
                    "sent": "So if the so you iterate you select and it not an increment K and increment.",
                    "label": 1
                },
                {
                    "sent": "Eater and you select a misclassified point, you ignore any points that are correctly classified.",
                    "label": 1
                },
                {
                    "sent": "So for example, if you're here, if your weight solution is here, then all these solutions are fine.",
                    "label": 0
                },
                {
                    "sent": "It's only this one that you're infringing, so you select this guy and you update in that direction basically, so you may actually go beyond.",
                    "label": 0
                },
                {
                    "sent": "You don't do anything intelligent.",
                    "label": 0
                },
                {
                    "sent": "You just take a learning rate I guess in the.",
                    "label": 0
                },
                {
                    "sent": "I don't really understand enough in the simplex method to understand, but you could it perhaps the case that Gardiner you're doing something definitely more intelligent.",
                    "label": 0
                },
                {
                    "sent": "The simplex method you won't overshoot here.",
                    "label": 0
                },
                {
                    "sent": "You could potentially overshoot, but then you look at this one and you go back again.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I've got a little.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Visualization of that.",
                    "label": 0
                },
                {
                    "sent": "This is all historic stuff, so here's a simple data set and what we do is we select initially some data point and we build the initial decision boundary based on that data point only, so.",
                    "label": 0
                },
                {
                    "sent": "We start with the weights being zero and for the first iteration we set the weight vector.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the data point values, which in this case that I've selected that data point I circled, and that's the result.",
                    "label": 0
                },
                {
                    "sent": "So we multiply the observed value of the data point times.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's class.",
                    "label": 0
                },
                {
                    "sent": "Then we go through selecting incorrectly classified data points.",
                    "label": 0
                },
                {
                    "sent": "So I've circled one there.",
                    "label": 0
                },
                {
                    "sent": "Which is incorrectly classified and we do this update.",
                    "label": 0
                },
                {
                    "sent": "We multiply that incorrectly classified data point.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By its label.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just the weight vector.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now very often.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This converges very quickly.",
                    "label": 0
                },
                {
                    "sent": "So there's that one green point there.",
                    "label": 0
                },
                {
                    "sent": "That is now in.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reclassified yeah.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "All data are now correctly classified.",
                    "label": 1
                },
                {
                    "sent": "Now that's just an example I set up that is separable that has two calcium Gaussian distributions.",
                    "label": 0
                },
                {
                    "sent": "The feasible region would be any region in here that we can bend that.",
                    "label": 0
                },
                {
                    "sent": "Point, and not without misclassifying that or misclassifying that.",
                    "label": 0
                },
                {
                    "sent": "So there's a small feasible region that we could find the support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Work is about making that solution unique by adding a quadratic term so that you get a quadratic programming program with linear constraints, and you look for the maximum margin separation.",
                    "label": 0
                },
                {
                    "sent": "Now the fun thing about Rosenblatt, when he specifies this learning rule is I don't think he's why.",
                    "label": 0
                },
                {
                    "sent": "I haven't read the original paper, but I think it's just an intuitive learning rule.",
                    "label": 0
                },
                {
                    "sent": "That you know that you should if you've got something wrong, you should look at the thing you've got wrong and you should add it into your weight vector.",
                    "label": 0
                },
                {
                    "sent": "But it still works and it leads to things like Hebbian learning which people use for Hopfield networks, but they were and heavy and low.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's something that I believe there's some evidence for existing between neurons in the brain, so there's that nice connection there.",
                    "label": 0
                },
                {
                    "sent": "So regression examples predict why given.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "X in the Y given X we define an error function now which is.",
                    "label": 1
                },
                {
                    "sent": "So we have some in the linear case we say that we've got some function of X which is gradient plus an offset.",
                    "label": 0
                },
                {
                    "sent": "And then we might define an error function in that case, which is the distance between our data and our target function.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can add a portion of the error to the bias and what we then also try and do is add.",
                    "label": 1
                },
                {
                    "sent": "Some of the error to the gradient.",
                    "label": 0
                },
                {
                    "sent": "Now this we can do iterative learning of these things by updating the bias and the Intercept.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a very similar way to the perceptron.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You get.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same sort of effect.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We select every.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Datapoint in turn.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can actually update.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "8.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A linear regression.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You all know.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that this isn't.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Standard way of.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing linear regression.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this is a sort of.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So castec.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gradient.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sent version of linear regression.",
                    "label": 0
                },
                {
                    "sent": "So we are updating and actually an underlying error function now.",
                    "label": 0
                },
                {
                    "sent": "But I think people didn't think about that so much.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the early days of.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In learning so.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you do it until.",
                    "label": 0
                },
                {
                    "sent": "You have.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A solution that fits now of course that's solvable with a little bit of linear algebra, and that's how we all do it today, but I think in early machine learning days people were interested in learning rules.",
                    "label": 0
                },
                {
                    "sent": "More than they were interested in objective function because they wanted to know how neurons were updating their own weights.",
                    "label": 0
                },
                {
                    "sent": "And of course we can go to nonlinear.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Faces so if the X is not linearly related to why, then we create a feature space and you've already seen me do this before.",
                    "label": 0
                },
                {
                    "sent": "So here we've got a function of X which is equal to some inner product between the basis functions and weights.",
                    "label": 0
                },
                {
                    "sent": "And if we apply the same sort of rules to those weights, we.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can also do it for them.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sean, so here's a.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basis this is a quadratic basis, so it's got three different parts.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can sample from the quadratic basis by so we can look at.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I was doing in the kernel PCA lecture.",
                    "label": 0
                },
                {
                    "sent": "We can say well for some random values of W. What sort of functions can this produce?",
                    "label": 0
                },
                {
                    "sent": "And they're obviously all quadratics.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we sample 3 random values of W. Here we're sampling W 1, which is the offset is .87 W 2 is minus .38835 and W 3 is minus 2.58 and these are the basis.",
                    "label": 0
                },
                {
                    "sent": "And this is the weighted sum of those basis.",
                    "label": 0
                },
                {
                    "sent": "So we've got some positive weight on W one so .87, so around there.",
                    "label": 0
                },
                {
                    "sent": "So that value should be .87.",
                    "label": 0
                },
                {
                    "sent": "At zero there we've got a negative weight linear function, so it's slightly negative going down in this way, so you can see that this side of the quadratic is actually lower than this site, even though the axes are symmetric.",
                    "label": 0
                },
                {
                    "sent": "And then we've got a negative weight minus two on the quadratic term, so the quadratic form is showing quite strongly double what it was in the first one.",
                    "label": 0
                },
                {
                    "sent": "Well, it should be quadruple or something, so that's about 7:00 or something, so it's been.",
                    "label": 0
                },
                {
                    "sent": "Then it's got the linear terminated, so below are still and we can sum those three things together.",
                    "label": 0
                },
                {
                    "sent": "We get a function that's nonlinear.",
                    "label": 0
                },
                {
                    "sent": "And that's what we think of as the.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Feature space and Bernard talked about that, so here's another example where we've got a much lower weight on the quadratic term, so the nonlinearity.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is less and another example with a positive weight on the quadratic term Anna negative weight on the constant term and a slightly negative on the linear so you can sample different functions?",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "According to this system.",
                    "label": 0
                },
                {
                    "sent": "Now, the systems that I like a bit more than that because they're local are the radial basis function type systems we've already talked about.",
                    "label": 0
                },
                {
                    "sent": "And in this case.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've got for example, 3.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Aces and the three bases can be summed together in a similar way, so we've got basis function.",
                    "label": 0
                },
                {
                    "sent": "One has this.",
                    "label": 0
                },
                {
                    "sent": "Center here minus one basis function two has a center of zero and basis function 3 has a center of one.",
                    "label": 0
                },
                {
                    "sent": "And what we're doing to create function in this case is a weighted sum.",
                    "label": 0
                },
                {
                    "sent": "For example, with a Gaussian sample from W to generate random function.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which gives us functions like this.",
                    "label": 0
                },
                {
                    "sent": "So here we've got a small negative weight on the 1st basis, so yeah, that's leading to this small bump.",
                    "label": 0
                },
                {
                    "sent": "We've got a small negative way on the second basis, but it's smaller, so we actually go up a little bit and a largest negative weight on the second or third basis, and we can keep doing.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fun.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actions like that with similar's.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in that case, the weight on the second basis is very small.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it hardly appears there we've got a 3.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That if waits.",
                    "label": 0
                },
                {
                    "sent": "And we can even learn in simple systems like this using the same learning rule we had before.",
                    "label": 0
                },
                {
                    "sent": "And I like these examples.",
                    "label": 0
                },
                {
                    "sent": "This is not how you do.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function learning, But in this case you can show how fun.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And respond slow.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oakley so this.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because of the learning rule is now based on the basis, then you only up wait things which are nearby your data point.",
                    "label": 0
                },
                {
                    "sent": "And this is the sort of thing we used to look at a lot.",
                    "label": 0
                },
                {
                    "sent": "Then what the nature of the learning rule was an iteratively update.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One data point data.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time, but we.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing it in more comp.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Systems like.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Realnetworks.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Lynette.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Items that were state of.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are on.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Digit recognize.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we're doing.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One data point at a.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stead of doing what you might.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think it's the right thing to.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It would be certainly for.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This model which.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is to try and do something.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Batch up.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nization",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2nd order method.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So eventually",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The function does.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Change.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But really, what's going on in these systems?",
                    "label": 0
                },
                {
                    "sent": "What's the mathematical interpretation?",
                    "label": 0
                },
                {
                    "sent": "So we like looking at those learning rules, I think, particularly 'cause there's a cognitive science inspiration for machine learning, but fundamentally these learning rules are minimizing this error function, right?",
                    "label": 0
                },
                {
                    "sent": "So this is an error function that is the classic sum of squares that Robert talked about some across data points between the squared difference between the output of the function and data point, why I?",
                    "label": 0
                },
                {
                    "sent": "And that's the sum of squares error.",
                    "label": 0
                },
                {
                    "sent": "Now I always get confused who invented the sum of squares error.",
                    "label": 0
                },
                {
                    "sent": "Anyone, I mean someone did, but anyone know?",
                    "label": 0
                },
                {
                    "sent": "Square.",
                    "label": 0
                },
                {
                    "sent": "Mr Square Middle East and Mr. Square work together.",
                    "label": 0
                },
                {
                    "sent": "I'm asking this privately 'cause I always forget there's too many Frenchmen starting with lemon law and I get confused between them.",
                    "label": 0
                },
                {
                    "sent": "I think it was lizandra, but it might have been a garage.",
                    "label": 0
                },
                {
                    "sent": "Come on, someone looking up someone with Internet.",
                    "label": 0
                },
                {
                    "sent": "Look it up.",
                    "label": 0
                },
                {
                    "sent": "OK, well actually I want to come back to this point again later.",
                    "label": 0
                },
                {
                    "sent": "Why did they invent least squares?",
                    "label": 0
                },
                {
                    "sent": "They invented least squares because they were making observations of planets and they had more unknowns.",
                    "label": 0
                },
                {
                    "sent": "Sorry more observations than unknowns, so they knew about solving systems of simultaneous equations, but they had more observations than unknown.",
                    "label": 0
                },
                {
                    "sent": "So which solution should they use?",
                    "label": 0
                },
                {
                    "sent": "And least squares?",
                    "label": 0
                },
                {
                    "sent": "Was the approach for.",
                    "label": 0
                },
                {
                    "sent": "Doing that now, Gauss also claims to have invented least squares, and he claims he found the Planet series its orbit by applying these squared before LeBron Alexandra.",
                    "label": 0
                },
                {
                    "sent": "Published or LaGrange, whichever one.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here there's now a cost function.",
                    "label": 0
                },
                {
                    "sent": "And we can build design matrices and the cost function expresses the mismatch between our prediction and reality.",
                    "label": 0
                },
                {
                    "sent": "And here I'm defining in the red fiai as being the evaluation of the basis function for.",
                    "label": 0
                },
                {
                    "sent": "Each of well, for each of the basis functions, so there's this replace.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In effect, so I'm moving to vector notation, which you know as you know I like, so I'm replacing that with an inner product.",
                    "label": 0
                },
                {
                    "sent": "So W is your vector of.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parameters anfi is the basis function values for one data point.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can minimize that we can compute its air.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we find is that the gradient of the basis function is equal to the current error.",
                    "label": 0
                },
                {
                    "sent": "So you always updating according to the error in both those perception examples and the regression examples.",
                    "label": 0
                },
                {
                    "sent": "And in this case this sort of error the current error.",
                    "label": 0
                },
                {
                    "sent": "This isn't the error function, but what the people used to call the error was.",
                    "label": 0
                },
                {
                    "sent": "The difference between the observation and the target value so.",
                    "label": 0
                },
                {
                    "sent": "That's the gradient.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For all the data points and the gradient for what we can do is minimize that by steepest descent.",
                    "label": 0
                },
                {
                    "sent": "So we initialize the algorithm with some value W, and we compute the gradient and go downhill.",
                    "label": 0
                },
                {
                    "sent": "Of course, again, this is analytic to optimize.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But Illustratively speaking, this is a sort of.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cheapest.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sent.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Type approach, so you compute the gradient.",
                    "label": 0
                },
                {
                    "sent": "And this is the quadratic form of the error function and you start heading down.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Towards the minimum.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fortunately.",
                    "label": 0
                }
            ]
        },
        "clip_127": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually goes.",
                    "label": 0
                }
            ]
        },
        "clip_128": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very, very slow.",
                    "label": 0
                }
            ]
        },
        "clip_129": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be.",
                    "label": 0
                }
            ]
        },
        "clip_130": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once it's done.",
                    "label": 0
                }
            ]
        },
        "clip_131": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's getting OK, so the jump there wasn't 'cause it started going fast.",
                    "label": 0
                }
            ]
        },
        "clip_132": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's nine.",
                    "label": 0
                }
            ]
        },
        "clip_133": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Orationes that's 10.",
                    "label": 0
                }
            ]
        },
        "clip_134": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "28 or.",
                    "label": 0
                }
            ]
        },
        "clip_135": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Asians?",
                    "label": 0
                }
            ]
        },
        "clip_136": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's 32.",
                    "label": 0
                }
            ]
        },
        "clip_137": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nations 42",
                    "label": 0
                }
            ]
        },
        "clip_138": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "5000 iterations 150 so here you're using the learning rule of to take the weight and update it by the sum of.",
                    "label": 0
                }
            ]
        },
        "clip_139": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the gradients of each datapoint.",
                    "label": 0
                }
            ]
        },
        "clip_140": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it can take a very long time.",
                    "label": 0
                },
                {
                    "sent": "Now stochastic gradient descent is something that machine learning people started doing back in the 90s and do an enormous amount now.",
                    "label": 0
                },
                {
                    "sent": "And the reason is because it tends to give you much faster solutions for very large datasets.",
                    "label": 0
                },
                {
                    "sent": "So in stochastic gradient descent, that's the what the algorithm we affectively saw for the perceptron and the regression problems earlier that you take one data point and you look at its error and you update your weight function according to that.",
                    "label": 0
                },
                {
                    "sent": "So stochastic gradient descent works.",
                    "label": 1
                },
                {
                    "sent": "Very well if you've got 25 million data points, because if you've got 25 million data points.",
                    "label": 0
                },
                {
                    "sent": "You need to compute the actual gradient to sum up the gradient with respect to each of those data points.",
                    "label": 0
                },
                {
                    "sent": "25,000,000 sum but stochastic gradient says says don't bother doing that.",
                    "label": 0
                },
                {
                    "sent": "Just go in an update according to the gradient of each individual data point.",
                    "label": 0
                },
                {
                    "sent": "Now stochastic gradient descent has this.",
                    "label": 0
                }
            ]
        },
        "clip_141": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning rate parameter.",
                    "label": 0
                },
                {
                    "sent": "OK, so just this is this is standard gradient descent with the sum over all data points.",
                    "label": 0
                },
                {
                    "sent": "So your update rule is the weight vector is equal to the old weight vector minus the gradient.",
                    "label": 0
                },
                {
                    "sent": "You descend the gradient.",
                    "label": 0
                },
                {
                    "sent": "Stochastic gradient descent says.",
                    "label": 0
                }
            ]
        },
        "clip_142": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, let's.",
                    "label": 0
                },
                {
                    "sent": "Change look at that.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Let's take that function there and say that's the error that been read.",
                    "label": 0
                }
            ]
        },
        "clip_143": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find that is the Delta Y that we've seen before.",
                    "label": 0
                }
            ]
        },
        "clip_144": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then let's replace the Delta, why I?",
                    "label": 0
                }
            ]
        },
        "clip_145": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some over North with multiple repeats of doing it for each individual data point, so this isn't exactly the same as that because here we are updating the weight function.",
                    "label": 0
                },
                {
                    "sent": "Once for all N here we're updating the weight function for the first W and then that changes this Delta Yi.",
                    "label": 0
                },
                {
                    "sent": "So every time you iterate, you're updating W, so these aren't exactly the same thing, But if you have a, if you have a sort of reduction criterion for your steepest descent, your learning rate.",
                    "label": 0
                },
                {
                    "sent": "If you reduce this in the right way, it turns out that this probably converges what the right way is is an open question, and very often I think.",
                    "label": 0
                },
                {
                    "sent": "Normal convergence on these things.",
                    "label": 0
                },
                {
                    "sent": "I don't know the details of how people claim they've achieved it, but certainly in practice.",
                    "label": 0
                }
            ]
        },
        "clip_146": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On that.",
                    "label": 0
                },
                {
                    "sent": "Fuck the function we looked at before.",
                    "label": 0
                }
            ]
        },
        "clip_147": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or this is sort of effect you get now we're not following exactly the gradient we're following.",
                    "label": 0
                },
                {
                    "sent": "This is the same example I showed you earlier with the basis functions with following that gradient with respect to one.",
                    "label": 0
                }
            ]
        },
        "clip_148": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data point.",
                    "label": 0
                }
            ]
        },
        "clip_149": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you see.",
                    "label": 0
                }
            ]
        },
        "clip_150": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little wiggles around.",
                    "label": 0
                }
            ]
        },
        "clip_151": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bit broad.",
                    "label": 0
                }
            ]
        },
        "clip_152": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Speaking, it goes in there.",
                    "label": 0
                }
            ]
        },
        "clip_153": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That way, sometimes with.",
                    "label": 0
                }
            ]
        },
        "clip_154": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Full step sometimes.",
                    "label": 0
                }
            ]
        },
        "clip_155": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Large 'cause it dips.",
                    "label": 0
                }
            ]
        },
        "clip_156": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things on the Val.",
                    "label": 0
                }
            ]
        },
        "clip_157": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have the basis funk.",
                    "label": 0
                }
            ]
        },
        "clip_158": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At each step.",
                    "label": 0
                }
            ]
        },
        "clip_159": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_160": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "While there are many iterations here.",
                    "label": 0
                }
            ]
        },
        "clip_161": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each iteration is.",
                    "label": 0
                }
            ]
        },
        "clip_162": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only looking at one data point, so the value.",
                    "label": 0
                }
            ]
        },
        "clip_163": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two of these comes.",
                    "label": 0
                }
            ]
        },
        "clip_164": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When you've got very.",
                    "label": 0
                }
            ]
        },
        "clip_165": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Large datasets.",
                    "label": 0
                }
            ]
        },
        "clip_166": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look down.",
                    "label": 0
                }
            ]
        },
        "clip_167": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Valley below it's actually wiggling around.",
                    "label": 0
                },
                {
                    "sent": "That's why it's sort of called stochastic.",
                    "label": 0
                },
                {
                    "sent": "On average, it tends to go in the right direction.",
                    "label": 0
                },
                {
                    "sent": "But for very long.",
                    "label": 0
                }
            ]
        },
        "clip_168": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "State sets which.",
                    "label": 0
                }
            ]
        },
        "clip_169": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This isn't one.",
                    "label": 0
                }
            ]
        },
        "clip_170": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This tends to be much faster than computing the correct gradient at each data point, and you'll see people using that leave.",
                    "label": 0
                },
                {
                    "sent": "Did I?",
                    "label": 0
                },
                {
                    "sent": "Oh, is there a typo?",
                    "label": 0
                },
                {
                    "sent": "Yeah, there probably was a typo.",
                    "label": 0
                },
                {
                    "sent": "Yes, sorry yes, that's a typo.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thanks leave that this here there's my mouse over there it is.",
                    "label": 0
                }
            ]
        },
        "clip_171": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That should be a negative OK, 'cause we're descending the gradient, sorry, thanks.",
                    "label": 0
                },
                {
                    "sent": "So we can get a an approximation to the gradient with stochastic gradient descent, which is often much faster to use in practice, and this is how, say, Yan Lacune was doing MNIST on 60,000 digits in the late 1990s or even early 1990s.",
                    "label": 1
                },
                {
                    "sent": "You could do it because you didn't have to solve a large update looking at quadratic error, you were looking at 60,000 digits.",
                    "label": 0
                },
                {
                    "sent": "You present each digit one after the other, update the weight function of the weights of the neural networks.",
                    "label": 0
                },
                {
                    "sent": "That was an adaptive basis system which he constructed to include translation invariants is in various things.",
                    "label": 0
                },
                {
                    "sent": "It was very impressive piece of work in the late 90s, and it all worked because.",
                    "label": 0
                }
            ]
        },
        "clip_172": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rustic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "As a field, we sort of kind of forgot about stochastic gradient descent for awhile, but it's now come back in.",
                    "label": 1
                },
                {
                    "sent": "I think in a big way for deep learning.",
                    "label": 0
                },
                {
                    "sent": "People are using a lot for deep learning.",
                    "label": 0
                },
                {
                    "sent": "It's difficult to analyze.",
                    "label": 0
                },
                {
                    "sent": "We used to call it online learning.",
                    "label": 0
                },
                {
                    "sent": "Why don't we call it online learning anymore?",
                    "label": 0
                },
                {
                    "sent": "OK, I guess 'cause that means doing a course on the Internet.",
                    "label": 0
                },
                {
                    "sent": "But we didn't have the Internet back then, so we call it online learning and a lot of the statistical physics analysis was about trying to understand the characteristics of that in certain systems.",
                    "label": 0
                },
                {
                    "sent": "For example, it's known that it often plateaus, so you start really nicely.",
                    "label": 0
                },
                {
                    "sent": "Often the error function goes down, and then it seems to plateau, and then it goes down again in neural networks and people did a lot of analysis.",
                    "label": 0
                },
                {
                    "sent": "In fact, my close colleague Magnus Rattray has some classic papers on on this plateau effect, and it's important because you want a long learning rate in this region and a short learning.",
                    "label": 0
                },
                {
                    "sent": "It's in the other region and if you read Geoff Hinton's papers still today.",
                    "label": 0
                },
                {
                    "sent": "If you read this knee paper, there's a large amount in the paper about how he started off with this learning rate, and then he switched to that learning rate and old papers used to have a lot about that, but it was sort of little bit heuristic.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Less difficult to formalize some of the minimizations, so the modern view of error functions is, of course.",
                    "label": 0
                },
                {
                    "sent": "One of the following.",
                    "label": 0
                },
                {
                    "sent": "Either the error function has a probabilistic interpretation, in which case you're typically doing maximum likelihood, and this is the modern view.",
                    "label": 0
                },
                {
                    "sent": "When I say that I mean from the perspective machine learning Gaussian, Laplace knew that, so that's not that modern.",
                    "label": 0
                },
                {
                    "sent": "The error function is an actual loss function that you want to minimize, so that's empirical risk minimization which Bernard talked about, and I think that you can broadly see that the field is has been split along those two lines.",
                    "label": 0
                },
                {
                    "sent": "People either take one perspective or the other.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is a broad generalization, so I tend to think are the error functions are probabilistic thing.",
                    "label": 0
                },
                {
                    "sent": "So what I should really do is worry about my probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "If you bring an application to me, I start writing down.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "Where is a sort of Barnard would probably think more in these points of view, so if you bring an application to him, he would typically start writing down an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Now the two things that then happen is you either can't solve the probabilistic model because it's non analytic or the optimization is NP hard.",
                    "label": 0
                },
                {
                    "sent": "So what I do is I learn, look for either a simpler version of the probabilistic model that is analytic or I look to do approximate inference by sampling or variational methods or the Laplace approximation.",
                    "label": 0
                },
                {
                    "sent": "What I think the optimization people do is they tend to relax the optimization so they say, well, I really want an A classic example.",
                    "label": 0
                },
                {
                    "sent": "Is L1 minimization type problems, so they say I really want to minimize to find a sparse solution and the way I should find a sparse solution.",
                    "label": 0
                },
                {
                    "sent": "Is by using the L0 norm on the parameters of my regression, but that's not tractable because which the L0 norm is drawn.",
                    "label": 0
                },
                {
                    "sent": "In the white space.",
                    "label": 0
                },
                {
                    "sent": "So again, you can think that this is like the in the version space.",
                    "label": 0
                },
                {
                    "sent": "The L0 norm is stacked up along the.",
                    "label": 0
                },
                {
                    "sent": "Axes, so you can't even really draw it properly.",
                    "label": 0
                },
                {
                    "sent": "It's like a sort of a.",
                    "label": 0
                },
                {
                    "sent": "If everything's the L0 norm counts the number of non zero terms, so every weight in your vector is 0, then the L0 norm is like a pointy stick sticking out of the axis, and then if this one here is 0 but the others are non zero you get these lines going along the axis.",
                    "label": 0
                },
                {
                    "sent": "So it's basically a star.",
                    "label": 0
                },
                {
                    "sent": "In the weight space it says I prefer things close to the center on the axis, and people tend to relax that to the L1 norm, which is actually a square and it leads to convex optimization.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of relaxation.",
                    "label": 0
                },
                {
                    "sent": "People use for sparse models, but people use very many different relaxations for clustering.",
                    "label": 0
                },
                {
                    "sent": "Spectral clustering is formulated around a relaxation from a discrete allocation of data points to continuous version of the allocation.",
                    "label": 0
                },
                {
                    "sent": "So that's my broad perspective on.",
                    "label": 0
                },
                {
                    "sent": "I don't do that, so.",
                    "label": 0
                },
                {
                    "sent": "I'm not an expert, so I'm making that up a bit, but I definitely do this an in this case.",
                    "label": 0
                },
                {
                    "sent": "We do we look at the interactive abilities and we try and approximate in one of a number of ways.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the.",
                    "label": 0
                },
                {
                    "sent": "Last 15 million years of machine learning research has mainly focused on probabilistic interpretation.",
                    "label": 0
                },
                {
                    "sent": "So let's graphical models are actually a way of doing probabilistic graphical models of either doing exactly with the junction tree algorithms.",
                    "label": 0
                },
                {
                    "sent": "Graphical models over high dimensional discrete variables by specifying conditional independence ease.",
                    "label": 0
                },
                {
                    "sent": "Gaussian processes are a way of doing probabilistic modeling over functions.",
                    "label": 0
                },
                {
                    "sent": "Semidefinite programming has become a big thing as a relaxation of certain or even as the thing you want to do.",
                    "label": 0
                },
                {
                    "sent": "The objective function you want to minimize?",
                    "label": 0
                },
                {
                    "sent": "I mean people are doing a quadratic programming.",
                    "label": 0
                },
                {
                    "sent": "Of course, for the support vector machine, so you can broadly see I think most of the last 15 years of machine learning as focusing.",
                    "label": 0
                }
            ]
        },
        "clip_173": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of those two things.",
                    "label": 0
                },
                {
                    "sent": "So the things that I'm not been talking about is how you do that.",
                    "label": 0
                },
                {
                    "sent": "Some of that is being covered by Robert 2nd order methods.",
                    "label": 0
                },
                {
                    "sent": "Conjugate gradient causing Newton and Newton effective heuristics, such as people when they do online gradient descent, they use something called momentum.",
                    "label": 1
                }
            ]
        },
        "clip_174": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or stochastic gradient descent I should say.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "K means clustering is the I guess the classic example.",
                    "label": 0
                },
                {
                    "sent": "I just want to talk about briefly in unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So in clustering, which is a classical machine learning.",
                    "label": 0
                },
                {
                    "sent": "Again, it's also used in statistics requirement where you want to divide groups into according to different characteristics of the groups.",
                    "label": 0
                },
                {
                    "sent": "So an example there would be different animal species or different political parties, and what we want to do is determine which groups they move into and what's harder.",
                    "label": 0
                }
            ]
        },
        "clip_175": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Perhaps the number of different groups, so for K means clustering, we might require a set of K cluster centers.",
                    "label": 0
                },
                {
                    "sent": "This is the classic algorithm we want to know where the center of those clusters are and the assignment of each data point.",
                    "label": 0
                },
                {
                    "sent": "To those centers.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm there simple algorithm that I want to sort of.",
                    "label": 0
                },
                {
                    "sent": "Talk about we get onto it, but in terms of how it turns when you consider probability, initialize the cluster centers on data points.",
                    "label": 0
                },
                {
                    "sent": "Assign each data point to the nearest cluster center an update each cluster center by setting.",
                    "label": 0
                }
            ]
        },
        "clip_176": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The mean of assigned data points.",
                    "label": 0
                },
                {
                    "sent": "So the objective that people do in K means clustering is they have a number of clusters K. Which is associated with centers and then a number of data Y.",
                    "label": 0
                },
                {
                    "sent": "And what you try and do is you minimize the sum across all clusters of the data points allocated to that cluster.",
                    "label": 0
                },
                {
                    "sent": "So for those eyes that are allocated to J of the squared distance between.",
                    "label": 0
                },
                {
                    "sent": "The mean and the data points.",
                    "label": 0
                },
                {
                    "sent": "So this minimizes the sum of the Euclidean squared distance between point points and their associated cluster centers.",
                    "label": 0
                },
                {
                    "sent": "K means clustering is not guaranteed to be a global minimum or unique.",
                    "label": 0
                },
                {
                    "sent": "When you apply this algorithm, and this is a non convex optimization problem so.",
                    "label": 0
                }
            ]
        },
        "clip_177": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is difficult to solve, but it's actually fast to solve.",
                    "label": 0
                },
                {
                    "sent": "If you use such an algorithm.",
                    "label": 0
                },
                {
                    "sent": "So here we've got some data.",
                    "label": 0
                }
            ]
        },
        "clip_178": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We initialize clusters randomly from the data.",
                    "label": 0
                }
            ]
        },
        "clip_179": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We then allocate all data points to their closest cluster center.",
                    "label": 0
                }
            ]
        },
        "clip_180": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We update the cluster centers to the mean of their allocated.",
                    "label": 0
                }
            ]
        },
        "clip_181": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data points.",
                    "label": 0
                }
            ]
        },
        "clip_182": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then, uh.",
                    "label": 0
                }
            ]
        },
        "clip_183": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Date each center according to the.",
                    "label": 0
                }
            ]
        },
        "clip_184": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Knew mean.",
                    "label": 0
                }
            ]
        },
        "clip_185": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we repeat that.",
                    "label": 0
                }
            ]
        },
        "clip_186": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Process.",
                    "label": 0
                }
            ]
        },
        "clip_187": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then eventually you don't get no change in cluster allocation, so I wanted to talk about this briefly because it's another simple algorithm that actually has a more complex interpretation.",
                    "label": 0
                },
                {
                    "sent": "When you look at it probabilistically and we'll come back to that after I try and motivate why you go to probability instead of standard error functions so.",
                    "label": 0
                },
                {
                    "sent": "He's got 2 steps.",
                    "label": 0
                },
                {
                    "sent": "One is an update of the means and one is an update of the allocations and what I want to do later is sort of point out that the update of the allocations is like the eastep and any EM algorithm and the update of the means is like an M step in the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK.",
                    "label": 0
                },
                {
                    "sent": "So I think I've gone slightly over my first half, so that will be I'll do next is.",
                    "label": 0
                }
            ]
        },
        "clip_188": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Introduce OK. Well I just finished with this.",
                    "label": 0
                },
                {
                    "sent": "Clustering approaches and machine learning.",
                    "label": 0
                },
                {
                    "sent": "I did have a section so I didn't introduce this section property.",
                    "label": 0
                },
                {
                    "sent": "I moved from supervised learning to unsupervised learning and started talking about clustering without properly introducing it because I put these slides together an hour ago.",
                    "label": 0
                },
                {
                    "sent": "But other I'm not going to talk about dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "Anyone know why?",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about dimensionality reduction?",
                    "label": 0
                },
                {
                    "sent": "'cause you've had six hours on that already.",
                    "label": 0
                },
                {
                    "sent": "So other clustering approaches in machine learning, spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "This is really cool.",
                    "label": 0
                },
                {
                    "sent": "It allows clusters which aren't convex hulls.",
                    "label": 0
                },
                {
                    "sent": "It's based on a relaxation of an optimization problem initially proposed by Shi and Malik.",
                    "label": 0
                },
                {
                    "sent": "An important work in machine learning done by Andrew.",
                    "label": 0
                }
            ]
        },
        "clip_189": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mike Jordan and someone else who I'm forgetting.",
                    "label": 0
                },
                {
                    "sent": "This form of clustering only allows clusters that in the data space are convex hulls, so you can't have a cluster, for example that spins around in a circle or something, and that's when we look at clustering we think are these data are close together.",
                    "label": 0
                },
                {
                    "sent": "They should be part of the same cluster, but this doesn't allow for that just allows data which are near incomes of Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "So spectral clustering is a really cool way of doing that.",
                    "label": 0
                },
                {
                    "sent": "Relaxation of an optimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_190": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then directly process is you're going to hear about from Zubin and Good point to say that at 5:00 o'clock DeLand is going to start going to talk to you about Kingmans coalescence, which is related to Bayesian nonparametric methods.",
                    "label": 0
                },
                {
                    "sent": "So Dylan is going to come forward and give a talk.",
                    "label": 0
                },
                {
                    "sent": "Then with Fernando not being here so to reach their processes are probabilistic formulation.",
                    "label": 0
                },
                {
                    "sent": "So this is an optimization formulation and this is a probabilistic formulation.",
                    "label": 0
                },
                {
                    "sent": "For a clustering albums that nonparametric nonparametric's, I think one of the really exciting areas of Bayesian learning.",
                    "label": 0
                },
                {
                    "sent": "But Zubin's going to talk to you a lot about them as well as Peter Banks and John coming in and in and so I won't say too much more about.",
                    "label": 0
                }
            ]
        },
        "clip_191": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About so after the break, we'll come back and talk about maximum likelihood regression from a probabilistic perspective and want to do a bit more.",
                    "label": 0
                },
                {
                    "sent": "One of the things I think is interesting is what people think Bayesian modeling is versus what it actually is.",
                    "label": 0
                },
                {
                    "sent": "Bayesian modeling is not using Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "OK, I mean it does use Bayes rule, but if you use Bayes rule, you're not being Bayesian.",
                    "label": 0
                },
                {
                    "sent": "OK people, some people disagree with Bayesian modeling.",
                    "label": 0
                },
                {
                    "sent": "You can't disagree with Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "It's an obvious consequence of the laws of probability.",
                    "label": 0
                },
                {
                    "sent": "So if you say I'm not going to use Bayes rule, you're just being dumb.",
                    "label": 0
                },
                {
                    "sent": "But there is a difference between the Bayesian and frequentist approach, and I'll try and highlight where that comes from and what it is.",
                    "label": 0
                },
                {
                    "sent": "Philosophically, it's associated with this epistemic and aleatoric uncertainty, and I'll use regression for motivating that first by maximum likelihood and then by Bayesian perspective.",
                    "label": 0
                },
                {
                    "sent": "And if this time at the end, I'll briefly talk about EM algorithms and try and introduce variational methods briefly through the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so back in 5 minutes I guess.",
                    "label": 0
                }
            ]
        }
    }
}