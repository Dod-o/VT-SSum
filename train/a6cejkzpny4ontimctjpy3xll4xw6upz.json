{
    "id": "a6cejkzpny4ontimctjpy3xll4xw6upz",
    "title": "The Many Faces of Optimism: a Unifying Approach",
    "info": {
        "author": [
            "Istvan Szita, E\u00f6tv\u00f6s Lor\u00e1nd University"
        ],
        "published": "Aug. 6, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_szita_mfo/",
    "segmentation": [
        [
            "So of course I'm going to talk about reinforcement learning and the efficient exploration problem, and after giving you a quick background of the models I use and quick and fairly biased overview of exploration methods so far, I will show you how to get a new one, which is probably better and I will talk about why we like it, why.",
            "Is that good?"
        ],
        [
            "So we were going to talk about Markov decision processes, finite, discounted, boring, but.",
            "Yeah man, let's wait until the end of the talk for some outlook and.",
            "We're talking about value function based methods.",
            "More specifically Q values.",
            "So just to let you know and we want to explore efficiently like this.",
            "Barely visible cats doing in the fish tank."
        ],
        [
            "So if you want exploration, then of course we have epsilon really exploration, which is extremely simple.",
            "Sometimes you make random action like this cat.",
            "Sorry for my cat fixation, couldn't help it.",
            "And so epsilon greedy exploration is sufficient for convergence in the limit, if that's what you want.",
            "And yeah, we have proofs for many classical methods, but we are not so patient to wait until the limit we want convergence first.",
            "So."
        ],
        [
            "No who?",
            "Let's look for something better and we are in a lucky situation because there are loads of better exploration methods and almost all of them says then says that if you are uncertain then you should be optimistic.",
            "How to do?",
            "How to be optimistic?",
            "Well that Verizon lot and I'm going to talk about four ways to do that.",
            "And of course there are many others.",
            "But I said before that this will be a very big."
        ],
        [
            "Overview and the first method that I will be talking about is our Max, which builds a model of the NDP from the observations and it built it so that that model is optimistic.",
            "It says that all the model says that all the unknown transitions go to some garden of Eden State who which, which is a hypothetical state, but it still gives a huge reward and the model says that you will go there and initially all the transitions are considered unknown and if you visit the transition many many, many times then the aggregate will say OK.",
            "This is already known.",
            "We can use the real values in.",
            "Instead of this hypothesis.",
            "Well, we like our mix becausw it convergence in polynomial time to some near optimal policy with high probability."
        ],
        [
            "Optimistic initial values is something very different and, well, the name says it all.",
            "So what we basically do is set the Q values to some very high values and then boot that rather encourages exploration.",
            "'cause well, if you haven't been to a state, then this Q values are still high.",
            "And the big plus of optimistic initial values is that you have no extra work after this initialization.",
            "Well, of course it can be, and usually it is combined with other techniques like epsilon, greedy.",
            "But you don't have to do that, and you can still have convergence.",
            "We have two problems with optimistic initial values.",
            "One is that so you basically you want to set the initial value really high.",
            "And talking to boost expiration.",
            "But if you do that then it takes lots of lots of time until it where the initial effect wears out.",
            "And the other problem is that it will not work with the model building.",
            "Because just imagine that you have a model you have set your initial values really high.",
            "Then you solve the model MDP, and, well, initial effect initial conditions have no effect.",
            "You get some solution so that initialization has been wide up, wiped out."
        ],
        [
            "Exploration bonus methods are rather large family of methods and they basically say that you should give reward for interesting states where interesting can be pretty much everything, like rarely giving reward for rarely visited states or states with large temporal difference, error or anything you'd like the exact form and.",
            "The size of this box is Verizon lot, and in fact you're not even constrained to let these bonuses to zero.",
            "You can give rewards that are that oscillate.",
            "Oh, which of course makes it more difficult to provide convergence proofs, but they can be really efficient in practice and well, of course, in the end we want some optimal policies, so standard technique is to divide the very function into two parts and also collect the bonus rewards in a separate value.",
            "Functions that can guide.",
            "Agent, but you can throw it away if you already with expression."
        ],
        [
            "And still another method of exploration is modernized model based into estimation, which also builds model from observations.",
            "And in fact it can be considered as a special case of expiration bonds methods.",
            "Well we build model.",
            "We estimate confidence intervals of transition probabilities of rewards for calculated confidence intervals for.",
            "State values and.",
            "Well, we give the exploration bonus is simply the half of the.",
            "In confidence intervals.",
            "So we explore a lot.",
            "If we are not confident where we are and we can.",
            "We can prove polynomial time convergence."
        ],
        [
            "As well.",
            "OK, so we have since the algorithm and now let's check if we can take the pluses from from this and somehow eliminate the minus is not.",
            "Let's check if it's possible.",
            "So start with model estimation.",
            "Well model estimation is good if we want to be sample efficient.",
            "Pretty standard, we keep several counters like how many times we've been to triplet and state action pair and how much reward we've collected so far in the triplet, and so we can get rid.",
            "Of course, really easily estimates of the rewards and the transition probabilities.",
            "Well, all of this is pretty."
        ],
        [
            "And the next step is that we put optimism into this model and how to make this model optimist.",
            "Well, we introduce this Garden of Eden State.",
            "And we say that, say, the model that we've been to this Garden of Eden State from each state action pair once.",
            "Well, of course we can put that into the formulas very easily, and we can see that that's really what we want.",
            "'cause then the initial model says that the reward for going to the Garden of Eden states are Max and probability of doing that is 1 and.",
            "All the other things are zero, and so if we calculate the optimal value function then we can see that it's really optimistic it predicts some high reward."
        ],
        [
            "Now look at this.",
            "Thing once more so we have optimism in the model just just the same kind.",
            "That optimistic initial values did.",
            "But now we have it in the model.",
            "And the good thing is that we will have no extra work after initialization and well, that Garden of Eden State is the same as we had in our mix."
        ],
        [
            "OK, so we have this optimistic model so we can start our standard reinforcement learning cycle that for each step T we select some action greedily.",
            "Corresponding to our current value function and then perform that action.",
            "Observed state and reward update the model counters and update model parameters and then we have new more we have to solve that.",
            "But we have that can sound scary.",
            "But that can be done pretty fast 'cause the model doesn't change very much and it can be done with a few steps of value iteration or.",
            "A synchronously and we get a new value of the Q function and we can iterate them until until we are satisfied."
        ],
        [
            "There's one potential problem with that.",
            "That we have one parameter in the other, which is our Max and we want to set that really high.",
            "Well, but then we have the problem that it takes long time before it wears out.",
            "So of course the solution is easy.",
            "We separate the reward counters so that in one of the counters we collect all the real rewards and the other counter.",
            "We collect all the hypothetical reward.",
            "And of course if you have that then we can calculate the separate reorder estimates and separate value functions.",
            "We're going to use this for action selection, but at anytime you can say that.",
            "OK, so.",
            "We don't want to use this anymore, we just need the real rewards you want.",
            "So."
        ],
        [
            "Take a look at this again.",
            "This separation is the trick from exploration, bones, methods, and in fact if you look at this separation, the this extra reward can be considered itself as an exploration bonus."
        ],
        [
            "So.",
            "Somehow we have collected several ideas and we have new algorithm and we would like to prove convergence.",
            "Well we have one parameter and we showed that if we send that to some high value then then the algorithm that I've described before converges to near optimal solution with high probability and the proof is.",
            "Basically identical to model based interval estimations proof.",
            "That's where we use that algorithm.",
            "And, well, so there's not many new tricks in that.",
            "We got a slightly looser bound, but we might have get us compensation for this let's."
        ],
        [
            "Look at this a little bit later so we have polynomial.",
            "Time convergence.",
            "We want to check how.",
            "What is the performance in real life and well we.",
            "Snatched pretty much all the benchmark data that we could get for for testing exploration methods and.",
            "We tested, tested our optimistic initial model algorithm on that and.",
            "It's like a slight problem was that.",
            "Evaluation criteria were different or more between different benchmarks.",
            "So what we did is we tested our methods in for each benchmark.",
            "According to the rules of that particular comparison, because that's the only thing we could do in a fair setting, because if we would change that to a uniform evaluation criterion, then you could have said that, OK, we selected this I valuation criterion so that it's good for us, but."
        ],
        [
            "How we try to avoid this and what you could see is optimistic initial model performs better than any of the.",
            "Mmath is found in the literature."
        ],
        [
            "Oh well, except for one exception of Bayesian dynamic programming on one benchmark problem, but based on dynamic programming is prettier has pretty high computational complexity."
        ],
        [
            "And well, this is a benchmark where smaller numbers are better.",
            "And, uh, optimistic and model is.",
            "Quite good in this benchmark as well.",
            "So you can say that it's."
        ],
        [
            "Quite efficient.",
            "So we have more time convergence.",
            "We have convincing performance in practice, but what I considered the largest advantage of this method that it's extremely simple to implement, 'cause if you do model estimation then then basically the only thing you have to do is model initialization and you have no work after that.",
            "And, well, you decision-making is pretty easy, you always do the greedy action selection and the MATLAB source code will be.",
            "Least soon, Ann."
        ],
        [
            "And now it's time for a little bit of outlook.",
            "Oh well, oh when we originally started to work on this project, we didn't want to solve M DPS.",
            "We wanted to solve factored MDP.",
            "Sorry, fast and well we had to start with in depth.",
            "But now the extension to factored MDP's is basically ready.",
            "We have to run some benchmark test to see how well it performs on that domain, and we'd like to extend the method to general general function approximation.",
            "With MDP's"
        ],
        [
            "Oh, thank you for you.",
            "Do you have any questions?",
            "Sorry.",
            "So.",
            "I knew that someone will ask that question and I have a spare slide that shows."
        ],
        [
            "Not this one.",
            "Yep, this one, so it's pretty ugly, and in fact it could be.",
            "So yeah, X is the state space H as well some constant.",
            "And I could have simplified the bitter about the epsilons, but I didn't want to risk that.",
            "The Fifth Times newer version.",
            "Yeah, so basically the.",
            "A theoretical bound is pretty loose, but what you can say is still polynomial and I think it could be made a little bit better.",
            "But if you have we had to talk about that with marketing and OOS.",
            "We concluded that.",
            "You have to pay for a better polynomial bound in the worst case for an error with the worst performance in the average case.",
            "So we have a better performance in the average case.",
            "It seems like.",
            "So it's really been great to see all this work on exploration, exploitation, all showing.",
            "Proving ideas, but you know it's still puzzling, though a little bit, because polynomial and that's great.",
            "I you know after one showed that polynomial, the next interesting question becoming becomes can we do it in real problems where you have to use function approximation?",
            "Yes, it's a bit frustrating that we community and not making progress and looking at exploration in function approximations because until we do that, all of this is, you know whether it's 2.3 the PowerPoint Viewer 2.1 or yeah.",
            "Not really going to make a difference, so anyway.",
            "I mean there's a lot of people here who are doing research and exploration, exploitation, and I think we should try to make progress function.",
            "Proximation you mentioned that you're looking at that.",
            "So as a question then, are there talks about how to make progress?",
            "Oh well, oh so when you're when you deal with factored Markov decision processes, then you're basically doing some kind of function approximation, and for those those function approximation.",
            "But the that kind of function approximation this method can be extended really easily if you have some general linear function approximators, well then.",
            "And then we we are a little bit, so we have several ideas, but we haven't found the real solution yet.",
            "We don't.",
            "Basically, we don't want to discretize the state space because then you lose.",
            "One answer might be that the idea is already there, optimistic initialization, which we could implement function approximation.",
            "We just haven't done the analysis, so so I guess the question I'm really asking is, do you think we need new algorithmic ideas or is and we're just or we're just missing the analysis of the function approximation part, but the idea is already there because all these share the same idea, yes?",
            "Implementing that this function approximation.",
            "Exactly, but there are several problems if you want to do that with this function approximation function approximators because you know.",
            "Well, basically at some point you have to do some kind of counting of whether you've been to this state where, and that's that's pretty much impossible, but I think this might have a good good chance to go there.",
            "Go there becausw.",
            "Well, it's so because of its simplicity, so.",
            "New experiments, how did you choose the parameters for algorithms for my algorithm?",
            "So for the other algorithms that were optimized by the other researchers where we took the data for our algorithm, we have one parameter and we did that with really crude optimization, like trying our Max.",
            "It has to 1 to 10 to 100 like this and choose the best.",
            "Big enough, yes.",
            "You make me smile.",
            "Oh, OK.",
            "Try to.",
            "Yep.",
            "Best value for what is arguing that?",
            "Would you explain people is minimal value for matches?",
            "So well, this is our Mexico is abound on their rewards in the MVP.",
            "This is this is the OR Max indoor Air origin alarm.",
            "And well, we have then H which is just a horizontal length until the algorithm for the discounting forgets everything.",
            "Until epsilon precision.",
            "What else do we have there?",
            "Well, we have state space, size, action, space, size and that's some constant.",
            "So basically, that's that's the worst case bound, so you really don't want to set this high becausw.",
            "Then it will take more time to commercial.",
            "Love you so my experience in working with generalization for exploration is that some of the techniques do.",
            "Transferring some of them don't.",
            "So what you mentioned this.",
            "Equality issues for state to think is a fundamental right now.",
            "This is techniques algorithm algorithm and I think that because you never returned to the same state, an error in a rich.",
            "Observation space we need different algorithms.",
            "Things which don't rely on returning the same observation, but optimistic initial value functions shouldn't really rely on the factorial.",
            "Yeah, that's what we hope.",
            "These are really pretty strong quality.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of course I'm going to talk about reinforcement learning and the efficient exploration problem, and after giving you a quick background of the models I use and quick and fairly biased overview of exploration methods so far, I will show you how to get a new one, which is probably better and I will talk about why we like it, why.",
                    "label": 0
                },
                {
                    "sent": "Is that good?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we were going to talk about Markov decision processes, finite, discounted, boring, but.",
                    "label": 1
                },
                {
                    "sent": "Yeah man, let's wait until the end of the talk for some outlook and.",
                    "label": 1
                },
                {
                    "sent": "We're talking about value function based methods.",
                    "label": 0
                },
                {
                    "sent": "More specifically Q values.",
                    "label": 0
                },
                {
                    "sent": "So just to let you know and we want to explore efficiently like this.",
                    "label": 0
                },
                {
                    "sent": "Barely visible cats doing in the fish tank.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you want exploration, then of course we have epsilon really exploration, which is extremely simple.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you make random action like this cat.",
                    "label": 0
                },
                {
                    "sent": "Sorry for my cat fixation, couldn't help it.",
                    "label": 0
                },
                {
                    "sent": "And so epsilon greedy exploration is sufficient for convergence in the limit, if that's what you want.",
                    "label": 1
                },
                {
                    "sent": "And yeah, we have proofs for many classical methods, but we are not so patient to wait until the limit we want convergence first.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No who?",
                    "label": 0
                },
                {
                    "sent": "Let's look for something better and we are in a lucky situation because there are loads of better exploration methods and almost all of them says then says that if you are uncertain then you should be optimistic.",
                    "label": 0
                },
                {
                    "sent": "How to do?",
                    "label": 0
                },
                {
                    "sent": "How to be optimistic?",
                    "label": 0
                },
                {
                    "sent": "Well that Verizon lot and I'm going to talk about four ways to do that.",
                    "label": 0
                },
                {
                    "sent": "And of course there are many others.",
                    "label": 1
                },
                {
                    "sent": "But I said before that this will be a very big.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overview and the first method that I will be talking about is our Max, which builds a model of the NDP from the observations and it built it so that that model is optimistic.",
                    "label": 0
                },
                {
                    "sent": "It says that all the model says that all the unknown transitions go to some garden of Eden State who which, which is a hypothetical state, but it still gives a huge reward and the model says that you will go there and initially all the transitions are considered unknown and if you visit the transition many many, many times then the aggregate will say OK.",
                    "label": 1
                },
                {
                    "sent": "This is already known.",
                    "label": 0
                },
                {
                    "sent": "We can use the real values in.",
                    "label": 0
                },
                {
                    "sent": "Instead of this hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Well, we like our mix becausw it convergence in polynomial time to some near optimal policy with high probability.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Optimistic initial values is something very different and, well, the name says it all.",
                    "label": 0
                },
                {
                    "sent": "So what we basically do is set the Q values to some very high values and then boot that rather encourages exploration.",
                    "label": 0
                },
                {
                    "sent": "'cause well, if you haven't been to a state, then this Q values are still high.",
                    "label": 0
                },
                {
                    "sent": "And the big plus of optimistic initial values is that you have no extra work after this initialization.",
                    "label": 1
                },
                {
                    "sent": "Well, of course it can be, and usually it is combined with other techniques like epsilon, greedy.",
                    "label": 1
                },
                {
                    "sent": "But you don't have to do that, and you can still have convergence.",
                    "label": 0
                },
                {
                    "sent": "We have two problems with optimistic initial values.",
                    "label": 0
                },
                {
                    "sent": "One is that so you basically you want to set the initial value really high.",
                    "label": 0
                },
                {
                    "sent": "And talking to boost expiration.",
                    "label": 0
                },
                {
                    "sent": "But if you do that then it takes lots of lots of time until it where the initial effect wears out.",
                    "label": 0
                },
                {
                    "sent": "And the other problem is that it will not work with the model building.",
                    "label": 0
                },
                {
                    "sent": "Because just imagine that you have a model you have set your initial values really high.",
                    "label": 0
                },
                {
                    "sent": "Then you solve the model MDP, and, well, initial effect initial conditions have no effect.",
                    "label": 0
                },
                {
                    "sent": "You get some solution so that initialization has been wide up, wiped out.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Exploration bonus methods are rather large family of methods and they basically say that you should give reward for interesting states where interesting can be pretty much everything, like rarely giving reward for rarely visited states or states with large temporal difference, error or anything you'd like the exact form and.",
                    "label": 1
                },
                {
                    "sent": "The size of this box is Verizon lot, and in fact you're not even constrained to let these bonuses to zero.",
                    "label": 0
                },
                {
                    "sent": "You can give rewards that are that oscillate.",
                    "label": 0
                },
                {
                    "sent": "Oh, which of course makes it more difficult to provide convergence proofs, but they can be really efficient in practice and well, of course, in the end we want some optimal policies, so standard technique is to divide the very function into two parts and also collect the bonus rewards in a separate value.",
                    "label": 0
                },
                {
                    "sent": "Functions that can guide.",
                    "label": 0
                },
                {
                    "sent": "Agent, but you can throw it away if you already with expression.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And still another method of exploration is modernized model based into estimation, which also builds model from observations.",
                    "label": 1
                },
                {
                    "sent": "And in fact it can be considered as a special case of expiration bonds methods.",
                    "label": 0
                },
                {
                    "sent": "Well we build model.",
                    "label": 1
                },
                {
                    "sent": "We estimate confidence intervals of transition probabilities of rewards for calculated confidence intervals for.",
                    "label": 0
                },
                {
                    "sent": "State values and.",
                    "label": 0
                },
                {
                    "sent": "Well, we give the exploration bonus is simply the half of the.",
                    "label": 0
                },
                {
                    "sent": "In confidence intervals.",
                    "label": 0
                },
                {
                    "sent": "So we explore a lot.",
                    "label": 0
                },
                {
                    "sent": "If we are not confident where we are and we can.",
                    "label": 0
                },
                {
                    "sent": "We can prove polynomial time convergence.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have since the algorithm and now let's check if we can take the pluses from from this and somehow eliminate the minus is not.",
                    "label": 0
                },
                {
                    "sent": "Let's check if it's possible.",
                    "label": 0
                },
                {
                    "sent": "So start with model estimation.",
                    "label": 1
                },
                {
                    "sent": "Well model estimation is good if we want to be sample efficient.",
                    "label": 0
                },
                {
                    "sent": "Pretty standard, we keep several counters like how many times we've been to triplet and state action pair and how much reward we've collected so far in the triplet, and so we can get rid.",
                    "label": 0
                },
                {
                    "sent": "Of course, really easily estimates of the rewards and the transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "Well, all of this is pretty.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the next step is that we put optimism into this model and how to make this model optimist.",
                    "label": 0
                },
                {
                    "sent": "Well, we introduce this Garden of Eden State.",
                    "label": 0
                },
                {
                    "sent": "And we say that, say, the model that we've been to this Garden of Eden State from each state action pair once.",
                    "label": 1
                },
                {
                    "sent": "Well, of course we can put that into the formulas very easily, and we can see that that's really what we want.",
                    "label": 1
                },
                {
                    "sent": "'cause then the initial model says that the reward for going to the Garden of Eden states are Max and probability of doing that is 1 and.",
                    "label": 0
                },
                {
                    "sent": "All the other things are zero, and so if we calculate the optimal value function then we can see that it's really optimistic it predicts some high reward.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now look at this.",
                    "label": 0
                },
                {
                    "sent": "Thing once more so we have optimism in the model just just the same kind.",
                    "label": 0
                },
                {
                    "sent": "That optimistic initial values did.",
                    "label": 1
                },
                {
                    "sent": "But now we have it in the model.",
                    "label": 0
                },
                {
                    "sent": "And the good thing is that we will have no extra work after initialization and well, that Garden of Eden State is the same as we had in our mix.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we have this optimistic model so we can start our standard reinforcement learning cycle that for each step T we select some action greedily.",
                    "label": 0
                },
                {
                    "sent": "Corresponding to our current value function and then perform that action.",
                    "label": 0
                },
                {
                    "sent": "Observed state and reward update the model counters and update model parameters and then we have new more we have to solve that.",
                    "label": 1
                },
                {
                    "sent": "But we have that can sound scary.",
                    "label": 0
                },
                {
                    "sent": "But that can be done pretty fast 'cause the model doesn't change very much and it can be done with a few steps of value iteration or.",
                    "label": 1
                },
                {
                    "sent": "A synchronously and we get a new value of the Q function and we can iterate them until until we are satisfied.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's one potential problem with that.",
                    "label": 1
                },
                {
                    "sent": "That we have one parameter in the other, which is our Max and we want to set that really high.",
                    "label": 0
                },
                {
                    "sent": "Well, but then we have the problem that it takes long time before it wears out.",
                    "label": 0
                },
                {
                    "sent": "So of course the solution is easy.",
                    "label": 0
                },
                {
                    "sent": "We separate the reward counters so that in one of the counters we collect all the real rewards and the other counter.",
                    "label": 0
                },
                {
                    "sent": "We collect all the hypothetical reward.",
                    "label": 1
                },
                {
                    "sent": "And of course if you have that then we can calculate the separate reorder estimates and separate value functions.",
                    "label": 0
                },
                {
                    "sent": "We're going to use this for action selection, but at anytime you can say that.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 1
                },
                {
                    "sent": "We don't want to use this anymore, we just need the real rewards you want.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take a look at this again.",
                    "label": 0
                },
                {
                    "sent": "This separation is the trick from exploration, bones, methods, and in fact if you look at this separation, the this extra reward can be considered itself as an exploration bonus.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Somehow we have collected several ideas and we have new algorithm and we would like to prove convergence.",
                    "label": 0
                },
                {
                    "sent": "Well we have one parameter and we showed that if we send that to some high value then then the algorithm that I've described before converges to near optimal solution with high probability and the proof is.",
                    "label": 1
                },
                {
                    "sent": "Basically identical to model based interval estimations proof.",
                    "label": 0
                },
                {
                    "sent": "That's where we use that algorithm.",
                    "label": 0
                },
                {
                    "sent": "And, well, so there's not many new tricks in that.",
                    "label": 1
                },
                {
                    "sent": "We got a slightly looser bound, but we might have get us compensation for this let's.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at this a little bit later so we have polynomial.",
                    "label": 0
                },
                {
                    "sent": "Time convergence.",
                    "label": 0
                },
                {
                    "sent": "We want to check how.",
                    "label": 0
                },
                {
                    "sent": "What is the performance in real life and well we.",
                    "label": 0
                },
                {
                    "sent": "Snatched pretty much all the benchmark data that we could get for for testing exploration methods and.",
                    "label": 0
                },
                {
                    "sent": "We tested, tested our optimistic initial model algorithm on that and.",
                    "label": 0
                },
                {
                    "sent": "It's like a slight problem was that.",
                    "label": 0
                },
                {
                    "sent": "Evaluation criteria were different or more between different benchmarks.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we tested our methods in for each benchmark.",
                    "label": 0
                },
                {
                    "sent": "According to the rules of that particular comparison, because that's the only thing we could do in a fair setting, because if we would change that to a uniform evaluation criterion, then you could have said that, OK, we selected this I valuation criterion so that it's good for us, but.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How we try to avoid this and what you could see is optimistic initial model performs better than any of the.",
                    "label": 0
                },
                {
                    "sent": "Mmath is found in the literature.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh well, except for one exception of Bayesian dynamic programming on one benchmark problem, but based on dynamic programming is prettier has pretty high computational complexity.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And well, this is a benchmark where smaller numbers are better.",
                    "label": 0
                },
                {
                    "sent": "And, uh, optimistic and model is.",
                    "label": 0
                },
                {
                    "sent": "Quite good in this benchmark as well.",
                    "label": 0
                },
                {
                    "sent": "So you can say that it's.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quite efficient.",
                    "label": 0
                },
                {
                    "sent": "So we have more time convergence.",
                    "label": 0
                },
                {
                    "sent": "We have convincing performance in practice, but what I considered the largest advantage of this method that it's extremely simple to implement, 'cause if you do model estimation then then basically the only thing you have to do is model initialization and you have no work after that.",
                    "label": 1
                },
                {
                    "sent": "And, well, you decision-making is pretty easy, you always do the greedy action selection and the MATLAB source code will be.",
                    "label": 0
                },
                {
                    "sent": "Least soon, Ann.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now it's time for a little bit of outlook.",
                    "label": 0
                },
                {
                    "sent": "Oh well, oh when we originally started to work on this project, we didn't want to solve M DPS.",
                    "label": 0
                },
                {
                    "sent": "We wanted to solve factored MDP.",
                    "label": 0
                },
                {
                    "sent": "Sorry, fast and well we had to start with in depth.",
                    "label": 0
                },
                {
                    "sent": "But now the extension to factored MDP's is basically ready.",
                    "label": 1
                },
                {
                    "sent": "We have to run some benchmark test to see how well it performs on that domain, and we'd like to extend the method to general general function approximation.",
                    "label": 0
                },
                {
                    "sent": "With MDP's",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh, thank you for you.",
                    "label": 1
                },
                {
                    "sent": "Do you have any questions?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I knew that someone will ask that question and I have a spare slide that shows.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not this one.",
                    "label": 0
                },
                {
                    "sent": "Yep, this one, so it's pretty ugly, and in fact it could be.",
                    "label": 0
                },
                {
                    "sent": "So yeah, X is the state space H as well some constant.",
                    "label": 0
                },
                {
                    "sent": "And I could have simplified the bitter about the epsilons, but I didn't want to risk that.",
                    "label": 0
                },
                {
                    "sent": "The Fifth Times newer version.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so basically the.",
                    "label": 0
                },
                {
                    "sent": "A theoretical bound is pretty loose, but what you can say is still polynomial and I think it could be made a little bit better.",
                    "label": 0
                },
                {
                    "sent": "But if you have we had to talk about that with marketing and OOS.",
                    "label": 0
                },
                {
                    "sent": "We concluded that.",
                    "label": 0
                },
                {
                    "sent": "You have to pay for a better polynomial bound in the worst case for an error with the worst performance in the average case.",
                    "label": 0
                },
                {
                    "sent": "So we have a better performance in the average case.",
                    "label": 0
                },
                {
                    "sent": "It seems like.",
                    "label": 0
                },
                {
                    "sent": "So it's really been great to see all this work on exploration, exploitation, all showing.",
                    "label": 0
                },
                {
                    "sent": "Proving ideas, but you know it's still puzzling, though a little bit, because polynomial and that's great.",
                    "label": 0
                },
                {
                    "sent": "I you know after one showed that polynomial, the next interesting question becoming becomes can we do it in real problems where you have to use function approximation?",
                    "label": 0
                },
                {
                    "sent": "Yes, it's a bit frustrating that we community and not making progress and looking at exploration in function approximations because until we do that, all of this is, you know whether it's 2.3 the PowerPoint Viewer 2.1 or yeah.",
                    "label": 0
                },
                {
                    "sent": "Not really going to make a difference, so anyway.",
                    "label": 0
                },
                {
                    "sent": "I mean there's a lot of people here who are doing research and exploration, exploitation, and I think we should try to make progress function.",
                    "label": 0
                },
                {
                    "sent": "Proximation you mentioned that you're looking at that.",
                    "label": 0
                },
                {
                    "sent": "So as a question then, are there talks about how to make progress?",
                    "label": 0
                },
                {
                    "sent": "Oh well, oh so when you're when you deal with factored Markov decision processes, then you're basically doing some kind of function approximation, and for those those function approximation.",
                    "label": 0
                },
                {
                    "sent": "But the that kind of function approximation this method can be extended really easily if you have some general linear function approximators, well then.",
                    "label": 0
                },
                {
                    "sent": "And then we we are a little bit, so we have several ideas, but we haven't found the real solution yet.",
                    "label": 0
                },
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "Basically, we don't want to discretize the state space because then you lose.",
                    "label": 0
                },
                {
                    "sent": "One answer might be that the idea is already there, optimistic initialization, which we could implement function approximation.",
                    "label": 0
                },
                {
                    "sent": "We just haven't done the analysis, so so I guess the question I'm really asking is, do you think we need new algorithmic ideas or is and we're just or we're just missing the analysis of the function approximation part, but the idea is already there because all these share the same idea, yes?",
                    "label": 0
                },
                {
                    "sent": "Implementing that this function approximation.",
                    "label": 0
                },
                {
                    "sent": "Exactly, but there are several problems if you want to do that with this function approximation function approximators because you know.",
                    "label": 0
                },
                {
                    "sent": "Well, basically at some point you have to do some kind of counting of whether you've been to this state where, and that's that's pretty much impossible, but I think this might have a good good chance to go there.",
                    "label": 0
                },
                {
                    "sent": "Go there becausw.",
                    "label": 0
                },
                {
                    "sent": "Well, it's so because of its simplicity, so.",
                    "label": 0
                },
                {
                    "sent": "New experiments, how did you choose the parameters for algorithms for my algorithm?",
                    "label": 0
                },
                {
                    "sent": "So for the other algorithms that were optimized by the other researchers where we took the data for our algorithm, we have one parameter and we did that with really crude optimization, like trying our Max.",
                    "label": 0
                },
                {
                    "sent": "It has to 1 to 10 to 100 like this and choose the best.",
                    "label": 0
                },
                {
                    "sent": "Big enough, yes.",
                    "label": 0
                },
                {
                    "sent": "You make me smile.",
                    "label": 0
                },
                {
                    "sent": "Oh, OK.",
                    "label": 0
                },
                {
                    "sent": "Try to.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Best value for what is arguing that?",
                    "label": 0
                },
                {
                    "sent": "Would you explain people is minimal value for matches?",
                    "label": 0
                },
                {
                    "sent": "So well, this is our Mexico is abound on their rewards in the MVP.",
                    "label": 0
                },
                {
                    "sent": "This is this is the OR Max indoor Air origin alarm.",
                    "label": 0
                },
                {
                    "sent": "And well, we have then H which is just a horizontal length until the algorithm for the discounting forgets everything.",
                    "label": 0
                },
                {
                    "sent": "Until epsilon precision.",
                    "label": 0
                },
                {
                    "sent": "What else do we have there?",
                    "label": 0
                },
                {
                    "sent": "Well, we have state space, size, action, space, size and that's some constant.",
                    "label": 0
                },
                {
                    "sent": "So basically, that's that's the worst case bound, so you really don't want to set this high becausw.",
                    "label": 0
                },
                {
                    "sent": "Then it will take more time to commercial.",
                    "label": 0
                },
                {
                    "sent": "Love you so my experience in working with generalization for exploration is that some of the techniques do.",
                    "label": 1
                },
                {
                    "sent": "Transferring some of them don't.",
                    "label": 0
                },
                {
                    "sent": "So what you mentioned this.",
                    "label": 0
                },
                {
                    "sent": "Equality issues for state to think is a fundamental right now.",
                    "label": 0
                },
                {
                    "sent": "This is techniques algorithm algorithm and I think that because you never returned to the same state, an error in a rich.",
                    "label": 0
                },
                {
                    "sent": "Observation space we need different algorithms.",
                    "label": 0
                },
                {
                    "sent": "Things which don't rely on returning the same observation, but optimistic initial value functions shouldn't really rely on the factorial.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's what we hope.",
                    "label": 0
                },
                {
                    "sent": "These are really pretty strong quality.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}