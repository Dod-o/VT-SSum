{
    "id": "xlgsjbubcioocj6ad7tmjxuepfn5bkeo",
    "title": "Classification Calibration Dimension for General Multiclass Losses",
    "info": {
        "author": [
            "Harish Guruprasad, Department of Computer Science and Automation (CSA), Indian Institute of Science Bangalore"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/machine_guruprasad_general/",
    "segmentation": [
        [
            "The subject of this work is awesome Toric properties of supervised machine learning algorithms.",
            "Let me give some background."
        ],
        [
            "There has been much interest in recent years in understanding when a surrogate loss such as the exponential of the hinge loss, is consistent with respect to a target loss, such as the binary 01 loss.",
            "In other words, understanding when minimizing a surrogate loss also leads to minimizing the target loss.",
            "In the final output, setting aside satisfying this property is set to be classification calibrated with respect to the target loss.",
            "For example, in the case of binary classification, there is no almost complete understanding of when a surrogate is classification calibrated with respect to the binary 01 loss.",
            "There's also been work on other target losses, such as the multiclass, your own loss, the Hamming loss, uncertain ranking, losses.",
            "In this work we developed some tools for understanding this question.",
            "For a general loss described by general loss matrix, different loss matrices arise naturally in different tasks.",
            "For example, in a document classification task, you may want to classify a newspaper article into one of three classes.",
            "Politics, sports and business unit of 1.",
            "If you predict wrongly and 0 otherwise.",
            "In a movie rating prediction task, you may want to predict the rating of a movie from one star to three stars.",
            "The loss here might be the difference between the actual and predicted number of stars.",
            "In a document classification task with a reject option, you may be allowed to abstain on an instance, in which case you incur a loss of, say, 1/2."
        ],
        [
            "In general, there could be some in classes and K possible predictions, and you're given a loss matrix where elijay is the loss incurred on predicting J when the true classes I.",
            "We study consistency with respect to such general last matrices and give some necessary conditions and sufficient conditions for win.",
            "A surrogate is classification calibrated with respect to such a general loss matrix.",
            "We then go on to define a new notion called the classification calibration dimension of a loss matrix, which is essentially the smallest dimension of our prediction space, such that there exists a surrogate that is both convex and classification calibrated.",
            "With respect to this loss matrix.",
            "We then give Upper and lower bounds on this quantity.",
            "For example, one result we show is that the classification calibration dimension of a loss matrix is upper bounded by its rank."
        ],
        [
            "As one interesting application, we apply the lower bound of the classification calibration dimension to the Paradise subset ranking loss.",
            "In Paradise upset ranking, the classes are preference graphs on some objects we have article to four here.",
            "This could be our documents and the predictions are permutations on these are objects.",
            "The power BI subset ranking loss on predicting accommodation Sigma where the true preference graphs G simply counts the number of edges in G that are incorrectly ordered by Sigma.",
            "We show that the classification calibration dimension of this loss is greater than R, which means in particular that any algorithm that learns our score functions by minimizing a convex surrogate cannot be classification calibrated with respect to this loss.",
            "This gives an alternate route for analyzing the difficulty of designing consistent contact surrogates for this loss from the recent result of two chain colleagues.",
            "Fortunately I'm out of time, but I would love to give you more look at it or results.",
            "So please do come to poster 70 today."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The subject of this work is awesome Toric properties of supervised machine learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Let me give some background.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There has been much interest in recent years in understanding when a surrogate loss such as the exponential of the hinge loss, is consistent with respect to a target loss, such as the binary 01 loss.",
                    "label": 0
                },
                {
                    "sent": "In other words, understanding when minimizing a surrogate loss also leads to minimizing the target loss.",
                    "label": 0
                },
                {
                    "sent": "In the final output, setting aside satisfying this property is set to be classification calibrated with respect to the target loss.",
                    "label": 0
                },
                {
                    "sent": "For example, in the case of binary classification, there is no almost complete understanding of when a surrogate is classification calibrated with respect to the binary 01 loss.",
                    "label": 0
                },
                {
                    "sent": "There's also been work on other target losses, such as the multiclass, your own loss, the Hamming loss, uncertain ranking, losses.",
                    "label": 0
                },
                {
                    "sent": "In this work we developed some tools for understanding this question.",
                    "label": 0
                },
                {
                    "sent": "For a general loss described by general loss matrix, different loss matrices arise naturally in different tasks.",
                    "label": 0
                },
                {
                    "sent": "For example, in a document classification task, you may want to classify a newspaper article into one of three classes.",
                    "label": 0
                },
                {
                    "sent": "Politics, sports and business unit of 1.",
                    "label": 0
                },
                {
                    "sent": "If you predict wrongly and 0 otherwise.",
                    "label": 0
                },
                {
                    "sent": "In a movie rating prediction task, you may want to predict the rating of a movie from one star to three stars.",
                    "label": 0
                },
                {
                    "sent": "The loss here might be the difference between the actual and predicted number of stars.",
                    "label": 0
                },
                {
                    "sent": "In a document classification task with a reject option, you may be allowed to abstain on an instance, in which case you incur a loss of, say, 1/2.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In general, there could be some in classes and K possible predictions, and you're given a loss matrix where elijay is the loss incurred on predicting J when the true classes I.",
                    "label": 0
                },
                {
                    "sent": "We study consistency with respect to such general last matrices and give some necessary conditions and sufficient conditions for win.",
                    "label": 0
                },
                {
                    "sent": "A surrogate is classification calibrated with respect to such a general loss matrix.",
                    "label": 1
                },
                {
                    "sent": "We then go on to define a new notion called the classification calibration dimension of a loss matrix, which is essentially the smallest dimension of our prediction space, such that there exists a surrogate that is both convex and classification calibrated.",
                    "label": 0
                },
                {
                    "sent": "With respect to this loss matrix.",
                    "label": 0
                },
                {
                    "sent": "We then give Upper and lower bounds on this quantity.",
                    "label": 1
                },
                {
                    "sent": "For example, one result we show is that the classification calibration dimension of a loss matrix is upper bounded by its rank.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As one interesting application, we apply the lower bound of the classification calibration dimension to the Paradise subset ranking loss.",
                    "label": 1
                },
                {
                    "sent": "In Paradise upset ranking, the classes are preference graphs on some objects we have article to four here.",
                    "label": 0
                },
                {
                    "sent": "This could be our documents and the predictions are permutations on these are objects.",
                    "label": 0
                },
                {
                    "sent": "The power BI subset ranking loss on predicting accommodation Sigma where the true preference graphs G simply counts the number of edges in G that are incorrectly ordered by Sigma.",
                    "label": 0
                },
                {
                    "sent": "We show that the classification calibration dimension of this loss is greater than R, which means in particular that any algorithm that learns our score functions by minimizing a convex surrogate cannot be classification calibrated with respect to this loss.",
                    "label": 0
                },
                {
                    "sent": "This gives an alternate route for analyzing the difficulty of designing consistent contact surrogates for this loss from the recent result of two chain colleagues.",
                    "label": 0
                },
                {
                    "sent": "Fortunately I'm out of time, but I would love to give you more look at it or results.",
                    "label": 0
                },
                {
                    "sent": "So please do come to poster 70 today.",
                    "label": 0
                }
            ]
        }
    }
}