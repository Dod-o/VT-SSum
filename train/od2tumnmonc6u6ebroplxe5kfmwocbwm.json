{
    "id": "od2tumnmonc6u6ebroplxe5kfmwocbwm",
    "title": "Media Stream",
    "info": {
        "author": [
            "Bla\u017e Fortuna, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Dec. 23, 2011",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Network Analysis",
            "Top->Computer Science->Web Mining"
        ]
    },
    "url": "http://videolectures.net/coinplanetdataschool2011_fortuna_stream/",
    "segmentation": [
        [
            "So hello, I'm black Fortuna that were said to come from artificial intelligence laboratory from GSI and the work will be showing today was mostly done together with the rest of our team there mostly to mark a global economic.",
            "And I'll be that discussion talk about."
        ],
        [
            "There is a showing some things about streams and start with some short introduction of what do we.",
            "By why do we want to mind streams?",
            "That's what's that language of doing that, but the challenges are doing that, and we'll have two kind of case studies.",
            "One is analyzing document stream analyzing and visualizing.",
            "And the second one will be mining web clickstream.",
            "So."
        ],
        [
            "Just start introduction.",
            "So first of all, why would we even want to deal with streams or deal with real time information?",
            "And there's some same goes the time equals money and if you can react in if you connect fast two events then we can get some value out of it.",
            "For example if you are talking about reading earnings reports, then we could get some advantage on the Stock Exchange.",
            "If you are talking about monitoring quality of service then reacting.",
            "Properly to any failures in the network increases quality of service, again with fraud detection.",
            "So can we react in real time to any detected?",
            "Events that appear like fruit, so this brings value.",
            "And in general direction speed, this case means value and we want to react as fast as possible."
        ],
        [
            "There are a couple of areas dealing with such things, so there will be focusing most on this, called streamlining the subfield of data mining and we deals with the incoming data streams so meaning.",
            "The points that are the data points that are coming to our systems are coming.",
            "As a time series more less.",
            "Oh point with the timestamp and we have to react to them, react to them or deal with them as they come in another area that you already saw in the morning tutorial about.",
            "It's called complex in detection and.",
            "This is just more fun."
        ],
        [
            "Permission.",
            "Now what are the standard approaches or what is that?",
            "What are the challenges when dealing with stream data?",
            "So one thing is so, first of all, streams can be if you have a small stream, then it's not really a problem, so we can we'll deal with them with off the shelf data mining methods may be a bit adapted for the.",
            "Temporal scenario, but that's about it.",
            "The problem becomes sadder when the data streams come become too intense to be handled by the shelves.",
            "Data mining methods, or when the events that are coming through or operations that we want to apply on their streams becomes too complex.",
            "So another example example of what would complex operation mean?",
            "So if a document comes in.",
            "Understanding what the document document means relating it to the existing documents in the stream and so on, can require parts and computation, and we need some special data structures to deal with such things.",
            "And also since the data is coming in in real time through streams, we can have this shifting distribution.",
            "So what's the topic slightly changed?",
            "So if you would look at example articles about politics than the topic slowly changed the election.",
            "Through no budget periods and so on, and we have to our models or other operations on the stream have to take this into account.",
            "This changing distributions a couple other features of streams most.",
            "Often they are too big to be stored, so we don't really want to store them all the data and we don't usually don't even need to store all the data.",
            "We just need to have ability to take into account any changes in the stream and to keep some.",
            "For example sliding window through the data and being able to adapt the models on the fly according to the new events introduced into the time window.",
            "You also, since the data is coming through slowly through time we have.",
            "We don't really have a luxury to look at it twice often, so we can't.",
            "We have to be as the credit card transaction comes through, we see it only once and then we can react to whether it's fraud or not.",
            "And after two years it's very hard to go back and recheck it because of the computational constraints usually.",
            "This is nonstationary data, so that changes in the distributions through time.",
            "And one thing is that we usually don't want to store the data explicitly, so we want to store them in some aggregate form, so we don't want to store each particular document.",
            "Maybe, but just the aggregates of the documents.",
            "A lot of the main keywords.",
            "What are the main topics?",
            "But other entities in them?",
            "And then be able to update these aggregates on the fly."
        ],
        [
            "And this is some kind of genetic infrastructure which usually we use parts of it when developing real time streaming systems.",
            "So we get on the input.",
            "We get some events which we have to capture either by some agents.",
            "Deploy it on the networking equipment.",
            "Detecting alarm source example JavaScript on the web page collecting the data.",
            "Then we have to transform this.",
            "Put it into its get some standardized form for our system.",
            "And then send this to the model which is updated based on the.",
            "New data.",
            "And this model can then be used either query to predict either for the future events either for the given events or reason about the stream.",
            "So this is more like more or less generic."
        ],
        [
            "About streams and I will go check them in more details on two case studies.",
            "The first one is focusing on analyzing document streams."
        ],
        [
            "So what do we mean by document stream?",
            "So we mean collect large collection of documents, so we employ millions of documents produced by news outlets, news publishers and what happens then?",
            "Slowly?",
            "So when we get to the millions of documents we don't, we can't really focus too much on the linguistic structure of 1 particular document.",
            "We have to start looking documents in the aggregate forms.",
            "And that means extracting keywords and entities and so on.",
            "And the extra step sentences so the orders of the words in the documents start mattering less and less once we increase the scale of the documents.",
            "And usually we are interested not in the documents themselves, but in the information that's contained in this document.",
            "So this can be extracted either by extracting entities, so people organization companies that are mentioned in the documents and the relationships between these entities.",
            "So if company a acquires Company B so that kind of just simple statement can summarize quite well quite many documents, news articles that appear on the business."
        ],
        [
            "OK.",
            "So first.",
            "We have to represent the documents.",
            "So how do we represent the documents?",
            "One of the standard things.",
            "One of the most under this text mining approach for representing documents is to represent it as a vector in some vector space and the vector space.",
            "So we start by checking the detecting the vocabulary of the documents.",
            "So we checked the corpus or at least the corpus that is also far and extract all the unique words and we assign the unique number to each word and this forms our basis.",
            "So this work.",
            "1.",
            "So each.",
            "Word has its own basis which dimension in director space and we represent the document as a vector in this space where we have a sum value non 0 value for the words that occur in the document and zero value for other words.",
            "So here is an example.",
            "So we have three very simple documents.",
            "If you check the.",
            "What is the vocabulary that occurs in these documents?",
            "We get these 6 words.",
            "This gives us this task that our vectors this text document vectors are 6 dimensional.",
            "And we put account for each vector for each document.",
            "For each word that occurs in the document.",
            "For example, in the second document, numerical analysis appear and we look at.",
            "Here numerical is in the 4th place.",
            "An analysis is in the first place.",
            "That means we have one count, one on the 1st and the 4th place and zero everywhere else.",
            "Now if you have a bit more complex or longer documents then these are not only zeros and ones we have.",
            "We can be a bit more complex, complex on.",
            "A bit more smarter about estimating the weights of the words, but it is important that the weight in the vector corresponds to importance in a way to importance of the word in the document.",
            "And once we have this representation, we can start comparing the documents and tech checking what's similar and what's not, and we do that by checking the distance in this space in this vector space.",
            "So for example, if you want to see what's the difference between X1 document one and document two, we just took this to take these two vectors and which take the cosine distance, causing similarity between these two, which means so there are two vectors in a vector space.",
            "We just take the calls in between the angle between vectors.",
            "And this turned out to be quite a good measure of similarity between documents.",
            "In a bitter naive explanation, we could just say that we are checking the overlap between the documents and the more important words overlap, the higher the similarity between these documents and the similarities.",
            "Usually something between zero and one.",
            "So 1 means exactly the same as zero means no work."
        ],
        [
            "Connor now will so one thing that is a.",
            "So it's useful to do with a large collections is visualization and we go now through some procedure for visualizing the document corpus first on the offline setting, and then we move it on the online setting.",
            "So as we saw the effuse back and forth representation for documents, each document ends up living in some high dimensional space.",
            "So it's a vector in 10 thousands of more dimensional space.",
            "And if you want to visualize them.",
            "So if you want to plot kind of map of these documents on the computer screen, then we are very limited with the two dimensions.",
            "And now the question is how do we?",
            "Transform documents that are positioned in this high dimensional space to just two dimensions."
        ],
        [
            "And we have a developed procedure that does this quite efficiently.",
            "So we start with approach called Latin semantic indexing that can take.",
            "Dimensional space reduce it down to a couple 100."
        ],
        [
            "Nations and notice that in semantic indexing.",
            "So it's a linear technique provide and it identifies sports that usually Co occur together.",
            "Then and if you have a large enough corpus disco occurrence over many documents usually means they are related so worth.",
            "Computer software hardware.",
            "This kind of works with usually Co occur and we can squeeze them together into one dimension and this lattice semantic indexing can identify these groups of words.",
            "And one group of words.",
            "We call it concept or latent variable and it's just a linear combination of these words from the original space and how to compute this so the back end is just a singular value decomposition composition where we take the metrics with all the documents as Rose.",
            "We decompose it using similar relative composition.",
            "And these Canonical dimensions.",
            "We use them as the hour basis for the documents.",
            "So, but in essence, this is the.",
            "According to Shen, so grouping words with similar meaning that Coker together often and using this approach we can get down to."
        ],
        [
            "100 dimensions and if but if you would try to just take 2 dimensions then we throw away too much information because the linear model so linear just taking linear combination of words is too weak to capture to capture this.",
            "More topics.",
            "So what we do we use another technique called multidimensional scaling to get from this few hundreds to dementia."
        ],
        [
            "And multidimensional scaling is actually a nonlinear technique and it starts by compute the distance between similarity between each two points in the original space and this.",
            "Latin space we developed we constructed here.",
            "And then we try to we run an optimization procedure which derives.",
            "Output positions of points in two dimensions in such ways that the position on 2 dimensional plane matches as much as possible to original distance.",
            "Now, if you want to, if you run this optimization over the complete data set, then close document.",
            "Similar documents are grouped together on the map and not so similar documents or not.",
            "For the report.",
            "In this can be efficiently implemented as well for larger data."
        ],
        [
            "Now the at the end.",
            "Once we get these two dimensions, we have to display it and when we are displaying we can add a couple of tools."
        ],
        [
            "So, just to reiterate a bit, so here we have, uh, this is the visualizations of each point corresponds to 1 document, and these two points, for example, are very close together.",
            "That means that the distance in the original space was quite close, so which means they had a high overlap of important keywords, whereas for example, this document in these documents up here are farther apart, which means that the distance in the original space was quite longer.",
            "They were not so similar and.",
            "Which corresponds to low overlap of the important keywords.",
            "Now that we have these positions, we can estimate the density so you can see this lighter area correspond to the more dense area of the map.",
            "And the darker area correspond to less dense areas and.",
            "Can do this user keeps which then kind of show the clusters of the documents.",
            "Will see latency later in the actual examples or this.",
            "How can we identify clusters from this?"
        ],
        [
            "Another thing we can do, we can compute keywords for the map.",
            "So if you take these documents from here, but the most important keywords that define the document there, we can just take the what we do.",
            "We take the area around here, check the words in this bag of words, vectors and take the one with the highest weight.",
            "If you average these documents and display them on the screen so.",
            "This I think these points here are descriptions of European projects.",
            "From FB 6 and you can see for example this part versus semantic web projects.",
            "This was about.",
            "Services web services.",
            "I think then medical learning, machine learning, cognitive science and so on.",
            "And we can see that just by glimpsing at this map, not needing to check the actual content of."
        ],
        [
            "Documents.",
            "We can also focus.",
            "So here we went with the mouse and we circled this area here and you can get a more detailed list of keywords for that area.",
            "And one now.",
            "Now this all this was done on a static map."
        ],
        [
            "But if we start, if you have a times time points attached to the documents, we can start seeing how the documents move through such map overtime, so we'll see now in a demo.",
            "So here are one point corresponds to 1 document and this was in year 2004 and we see that we can see.",
            "The gradient of change, so the longer the arrow, the more bigger the distance the point will move in the next year.",
            "So switch to the demo."
        ],
        [
            "OK, so this is the map, so the data set here is we have a.",
            "Collection of papers from a machine learning that machine learning data mining community.",
            "So from Pascal Network we have all the papers, so for the group of people produced in 2004 and in 2005, so each point.",
            "Which point corresponds to one person?",
            "So, for example, this point here corresponds to Andreas.",
            "Opold from University of Leuven and these are the abstracts of the papers that he quoted in that year.",
            "And this we do that for each person.",
            "Now just by running the optimizations that we went through in the previous slides, we derive this map.",
            "And looking at this we can see there are a couple of clusters.",
            "So this is one group of similar.",
            "People, this is another group, and here is another bigger group.",
            "Plus these people in the center.",
            "So central position usually corresponds to somebody that is between multiple areas.",
            "Now if you display the.",
            "Common words.",
            "Criticize.",
            "So we can see that these people here are working on the images, so computer vision.",
            "These are text mining people, natural language processing people, whereas here it was more.",
            "This computational learning theory, kernel methods and so on.",
            "How come there is duplicates on this map?",
            "After all, the vector analysis that we've done before, so duplicating this keywords appear, so each keyword is computed independently and it's just by randomly sample points over the map and compute the most important keyword for that area of the map.",
            "And if it appears more time, it just means that the word is important in more areas.",
            "So we could also had some versions where we were trying to remove these duplicates, but then it increases the cost in the.",
            "Computation.",
            "What about documents are in different language?",
            "So if if the documents provide that would be in different language, yes.",
            "So then we would need.",
            "So we had some experience with this on.",
            "Constitutions, so if you have documents from different languages, then you have to first match the words from different languages other than dictionary, either using some automatic methods like Canonical correlation analysis is 1 method method for detecting correlated words across the languages.",
            "So if you train these systems by having a long set of parallel sentences.",
            "And you can then.",
            "Identify this concept.",
            "This like we have this Latin semantic indexing here.",
            "There are techniques that can take different corpuses in different languages and map them down to the same space where each dimension corresponds to the same thing in different languages.",
            "What about if you have, let's say document that isn't alphabetic, for example numbers.",
            "Numbers so here.",
            "So the in the first step, when we take the document, it's already transformed into a vector and from then on everything works on a medical object we had another.",
            "For example another.",
            "Then move it on.",
            "This is using images, so from images you can extract common markers either histogram based or some other feature based, and you can then also get similar Maps and then you would have a here you would end up with pictures of Flowers of cities and so on, but this is it's possible to do it on non textual data as well.",
            "Thank you.",
            "OK, so now if you turn on this magnifying so this is if I move the mouse.",
            "So we see this circle and these keywords are computed as the top keywords from this circle and now we see if you move around.",
            "Around this area, images is always the top keywords and that's why it was already select multiple times.",
            "And if you go here with its documents text this day is here.",
            "There were a couple of French articles.",
            "That's why this, I think appears here, but we didn't control for that.",
            "And this is unable to submit more detailed analysis.",
            "We can also.",
            "Display the names of the people, so here are just rank them by the number of publications.",
            "So we see this.",
            "The top people did publish most articles in that year from that network and we can show relations so.",
            "This shows the Co authors of this person.",
            "Now if we talked about before about the temporal dimension, so we can move to year 2005.",
            "Now map changes a bit.",
            "We see that this images area gets a bit more isolated.",
            "This kernel methods area gets a bit smaller and these documents area gets a bit more prominent here as well.",
            "And we can see for each other if you go back.",
            "And we turn this gradient on.",
            "We can see what was the direction of the people.",
            "So how these areas here moved more towards images or more towards document processing.",
            "So here So what are the tendencies of the people?",
            "From the changes from their publishing.",
            "Abstracts from 2004 and 2005.",
            "We can also see the actual trace.",
            "So this see this person went here from.",
            "Completely left to the center of documents cluster in 2005.",
            "However, some person here, for example.",
            "Just moved a little bit to the left.",
            "OK.",
            "So this is one data set.",
            "If you go back."
        ],
        [
            "This is this was this demo now here.",
            "We had a very specific data set where people were easy to extract, so person and we had a describe.",
            "The person with the abstract within the model generic scenario when you would have a document.",
            "So let's say typical.",
            "Documents and we want to do the similar graphs we first have to extract entities and this is where this service we develop called enricher comes handy.",
            "So let me show you demo."
        ],
        [
            "It's a bit easier.",
            "So we start by loading the article, so this is a web service.",
            "If you go to.",
            "And Richard Rodgers.",
            "Notice I get to this live demo.",
            "We can load the text in, so here are some.",
            "Upload that articles and if you click and rich now the system goes and takes this article and identifies all the entities that occur in this document.",
            "So you see Brazil.",
            "Italy, Germany so on identifies the top keywords that describe this article, so we see a sports.",
            "Soccer was one of the main keywords here and most teams is 1 big taxonomy categories for this article and for each entity also tell what it is.",
            "So we see that Brazil.",
            "Is actually.",
            "Country Brazil, so if you go.",
            "It would follow this link to get and we see also cycle concept.",
            "So you suck at the beginning of the week.",
            "So we gotta link to the open cycle concept of this entity.",
            "Other names for Brazil.",
            "And categories, we see that it's a Latin American country, and so on.",
            "So we do this by linking to think, taking the context of the Brazilian this document and identifying, resolving it into some external ontology.",
            "In this case, the pedia.",
            "So, but now if you do this for each article on the stream, then we can slowly start getting snippets about Russia.",
            "For example, if you always take the sentences around that article about that entity and we can use this to describe the entity so we can describe the entity by the sentences or paragraphs in which it occurs.",
            "And we can do this similar visualizations as we did before for the this research network.",
            "So.",
            "Oh, and if you go."
        ],
        [
            "Demo and if you do that will show now in a demo of.",
            "Built on top of Reuters news corpus.",
            "So this is a standard to Corpus Reuters, released for the research community and it contains news articles, all news articles, writers published from Summer 2006, Summer 96 to 797.",
            "Now if you go here so we see.",
            "So now we start to the end of the August and these are the entities that occur in that article.",
            "The top entities that occur in the articles you see US.",
            "And yours is mostly around center 'cause it's present in many articles.",
            "And then here you would have this.",
            "North America Pacific region.",
            "Here we would have more.",
            "Business area and here it's about more South America.",
            "Entities and what we can do is we see for example, Bill Clinton.",
            "And if you change, we can see how he moved around through the map so.",
            "How did entities how occurs in different contexts as the time went on?",
            "And we can see, for example, the trace.",
            "We can see that he started started here.",
            "And then he went to he was in some business Contacts.",
            "Then he went to think he went for a visit to China or somewhere there.",
            "So he occurs here for some time and then went back down.",
            "Whereas if you look at the USA it's mostly traveling here in the center, so 'cause it occurs in many articles and it occurs in many topics or Argentina and other hand, it's very local and it would occur only in the context of soft America.",
            "Now this is just taking the entity and positioning it against the context of the topic topic context.",
            "We can also check the relations so we can see relationship between Bill Clinton for example Washington.",
            "And at that time it was.",
            "Election campaign and Bob Dole was one of the candidates.",
            "NBC, the relationship between Bill Clinton and Washington contains keyword like Clinton Dole, Washington Republican Convention and so on.",
            "What is relationship to Russia is about?",
            "Clinton, Yeltsin, Chechnya, Grozny, and so on.",
            "Another example is towards Mexico, so here it's about.",
            "Some.",
            "Drug dealing.",
            "Whereas if you go through the end.",
            "He's not even in a relationship with Mexico anymore.",
            "'cause he was not mentioned in any context.",
            "Another very interesting example is interesting is.",
            "Kids.",
            "Can't see it now, so at some point he's in a relationship with a hot Chicago is here.",
            "So he has a relationship with Chicago because somebody had some Democratic Democratic convention there.",
            "But as time goes on, the relationship disappeared because he's not.",
            "Bill Clinton is not mentioned in context of Chicago anymore.",
            "OK, So what did this so this weekend?",
            "Basically what we this tool shows that we can extract profiles of entities, position them in the topical context and also identify relationships just by looking at this stream of documents that's coming through and we can visualize them quite easily."
        ],
        [
            "So this is just."
        ],
        [
            "This."
        ],
        [
            "And I want a last example is on document is called the News Analyzer.",
            "So here we have the same data set but we are visualizing such queries.",
            "So here we see we can put in the query we get list of all the results, then the topics as they evolve through that data set that may go."
        ],
        [
            "Is analyzer.",
            "So let's say we start it, Clinton.",
            "So these are all the articles from Reuters Corpus in which Clinton occurred.",
            "And here we have a graph that shows a timeline from August 96 to Aug 97 and we have a split automatically.",
            "This set of documents into five clusters and here they are displayed.",
            "The point is a bit small, so the first the Orange one corresponds to the campaign.",
            "So we can see how the campaign.",
            "Election campaign was happening on in November.",
            "The election actually happened and then the topic died out.",
            "On the other hand, this budget topic started after the election, so around New York and it was moving on from the most of the first half of the year.",
            "There are also this rapid blue cluster.",
            "Here is about not oh I think then it was discussions about not extending to East Europe, so they had quite a big.",
            "Quite some coverage on the news now if you go.",
            "Here we see also that Clinton is quite uniformly covered throughout the year.",
            "Now if you say.",
            "Instead of Clinton, Clinton, and Dole, so it's a bit more focused.",
            "No.",
            "So this is running on my laptop.",
            "OK. We see the Clinton and Dole occurred together at the beginning during the election campaign, but once it was done, the topic models died out.",
            "So you can also see other more seasonal things if you search for influencer.",
            "Pop.",
            "Should work.",
            "Let's restart.",
            "We can see so influenza, flu or has a spike around winter.",
            "And we can see also what are the topics.",
            "So let's vaccine.",
            "There are some poultry China ban and the biggest one.",
            "I guess there are some influence of flow on this long World Cup or one very interesting one is also ski jumps.",
            "Ski jumps so they're there.",
            "Again, it's seasonal sports in winter it's quite big and we see the biggest cluster is called Peter Castle.",
            "Slovenian ski jumper was quite good demo.",
            "So this this kind of tools allow us to explore this temporal aspects of data sets.",
            "OK."
        ],
        [
            "And here are some couple of examples using the same tool on top of New York Times archive.",
            "So for the period of Second World War, and if you search for Pearl Harbor, we see that first two years it was not much mentioned, but on the attack happened it was around here somewhere.",
            "The topic models explode."
        ],
        [
            "Also, the opposite was about Belgrade, so there was quite some topic.",
            "Deal 2000 till 41.",
            "Whether you would like your Belgrade will join on the website of the will join the war.",
            "But once the decision was taken and the new Slavia capitulated, that was more or less out of music."
        ],
        [
            "Relevant anymore.",
            "And the phone is this Normandy we see there is a bit of a spike at the beginning, mostly about sailing, so people selling probably across the channel and then it was a topic completely diet until 44 when the."
        ],
        [
            "They happened.",
            "OK, So what we are doing with this now so this is more like a infrastructural slide.",
            "So we are connecting.",
            "We have we're using Spinner so it's a service for.",
            "It crawls mainstream news blogs microblogs like Twitter and they collect around 30 million items per day.",
            "And we are developing infrastructure for automatically handling, processing and indexing this stream.",
            "But each article is processed by Enricher as we showed before, and the idea is that we have a.",
            "To provide similar functionalities as shown in the previous slide.",
            "On top of this, and this was done.",
            "All this is happening in the context of render project.",
            "So if you're interested more about this, if you can Google for a render project on the Inter."
        ],
        [
            "And one last example of our handling clusters scanning document stream.",
            "So this is stream clustering.",
            "So in the previous example we're visualization.",
            "We are mostly focused on the top keywords of the articles or positioning them or visualizing them.",
            "But if we want to derive topics on the fly then we can have things called string clustering where we start with some.",
            "Initial buffer of documents, for example, to get this is also done under writers to get four clusters and as new documents coming in, and we position them in this clusters, we have to further extent.",
            "Or delete concepts from it so and more details are in large box paper from 2008.",
            "So this is 1 example of.",
            "Taking a standard approach to clustering and adapting it to working on streams."
        ],
        [
            "OK, so this was the first case study and I will go through the second one which is a web click streamlining."
        ],
        [
            "So what's the idea here?",
            "Here we have, so before we had only one stream of documents, now we have a couple of streams, so we have a stream of content of news articles that are coming that are published on the web.",
            "We have another stream of click events that are happening, so people visiting particular page and sometimes you also have some user data or more more and more often.",
            "As we saw yesterday the privacy talk and now the task is can be.",
            "Use this data to model particular user segments and also to recommend contents to the."
        ],
        [
            "Users.",
            "OK, so.",
            "For the overview of the system, and then we'll go piece by Piece Street.",
            "So we started by access locks, so this is standard.",
            "For example Apache Web server collecting all the streams, all the clicks pages that happen and we store them in some file.",
            "And then we also have articles or web pages that are visited by these people.",
            "And sometimes for some users we have their data user data, either if they registered.",
            "For example, we would know their demographics, age, income and so on.",
            "So we take all these three sources of data, combine them together in an index.",
            "And then the implication is that some domain experts can define the user segment.",
            "We use this data from here to construct a model from the user segment.",
            "Either visualize the segment to see what are their characteristics or apply it, for example, for other for recommending additional articles either for.",
            "Targeting news to them or so on."
        ],
        [
            "So now step by step.",
            "But what do you mean by access log?",
            "So we're typically web pages would be logging the user interaction with the websites, so that means for each page view we would have some user ID, which is typically done by dropping a cookie on the user side, date and time of when it happened.",
            "So either local time so including the location of the user, either the some Standard Time GMT for example, then often we can get location from the IP.",
            "So looking at the back of the user.",
            "Please come from we can know what he requested.",
            "So which pages viewing where he came from.",
            "So after that he came from Facebook or this came from some home page front page.",
            "If they come from Google, we can also see or from search engine we can see a search query so we can extract it from front page and from the user agent we can get which browser device operating system he's using.",
            "Then this tracking of the user, so keeping ideas is usually done by dropping cookies on the user."
        ],
        [
            "One example of this is so this is some ID with some IP and you can if you go do Geo location on this IP we can see that it corresponds to Beijing, China.",
            "We get some, you requested some URL from where he came from when it happened, that was the user agent and from here you can see that it's somebody was using Safari on Windows.",
            "Then so this is one step."
        ],
        [
            "The second stream is articles, so we can take the what we see from the article as we saw before.",
            "We already know from the content we can extract just the keyboard, so get this bag of words vector.",
            "We can also extract named entities or topics using services such and richer and often web pages will also have metadata which is.",
            "Attach them such as outdoor publishing date and so on, and this is a.",
            "Major newspapers are quite careful about this because of the IT can or blocks as well, 'cause it good metadata can help their search engine optimization efforts."
        ],
        [
            "OK. And the first data source was user data, so we have a.",
            "We were using data set from some major website news publishing website.",
            "And in that case, 20% of the users were registrated and will see that using machine learning we can quite well generalize from this 20% to some other users from this registration data and they would.",
            "During registration they would enter either, but they should enter Italy, gender, year of birth, household income and some other categories.",
            "And one problem here is that it can be quite noisy, so people could just click the first option just to move on.",
            "With the registration.",
            "Sample of somebody registering so some Mail from 65 born in 65 with from this zip code.",
            "United States household."
        ],
        [
            "Income and so on."
        ],
        [
            "Now what we want to do so."
        ],
        [
            "Using this user data, we want the domain experts be able to define user segments on top of this data and what we mean by user segments.",
            "It's a segment subset of users.",
            "Which are sharing some common characteristics and characteristics can be many things.",
            "It can be gender, so somebody give one segment would be all the male visitors or all the visitors with the age about 40 or coming from Facebook interested in topics, topic, travel, or any combination of these things.",
            "So when we let the two in order for the domain experts enter this into the system, the.",
            "Formulated the problem in the form of a faceted search or a search query so the people just use it will just enter these things in a search engine.",
            "And the results would be the users that correspond to the particular segment.",
            "And then we would use this retrieved users as a model their segments so to do machine learn."
        ],
        [
            "On top of them.",
            "And for indexing wind we can from one visit or from 1 user from page we can index many things and these are all the parameters over which will allow people to define their segments.",
            "So starting from domain sub domain over the URL of the page that was visited or we can filter over meta tags on the page, the title or the content they discovered them entities search terms that people use to find come to the website.",
            "Or the domain from where they came.",
            "Also the other location when they visit, so give me all the people that come to our website on Friday evenings from Uganda.",
            "For example, this could be a query.",
            "User agent income.",
            "H and gender.",
            "So this last three.",
            "Are extracted from the we not only for registered."
        ],
        [
            "Others.",
            "So one example.",
            "So here is some common examples.",
            "So all the females that are between 100 and hundred $50,000 per year that are interested in fashion.",
            "Or all the people that are match at least one of these criteria that you rather, CEO, CFO, awesome, or Vice President.",
            "So give me all the decision makers in the company.",
            "Or all the people that are interested in reading about Obama, Obama, Healthcare and they came to our page website."
        ],
        [
            "Twitter.",
            "Now we have a set of users that correspond to this segment.",
            "We first construct the feature space and feature, so we describe each users with some feature vector and we do that similarly in a similar way.",
            "Similar way as we did that before from the documents tax documents.",
            "Only here we have many more fields and we do that just by concatenating the feature vectors together so the user just clicks by this.",
            "Which parameters should be active for modeling and these authentic into account for the feature vector.",
            "The training set is either one visit or one user, so depending on what you're modeling, if you're interested in user segments and one example would be 1 user, which is described as a center it as an average vector of all his visits.",
            "And all the people that match our query are defined as positive class and everybody else that didn't correspond to this query is defined as negative class and we use support vector classification algorithm to technique to learn a model that can predict then user belongs to the class or not.",
            "And this can be can we can scale this quite easily using stochastic gradient descent implementation as FCM.",
            "So we can run it on millions of users without."
        ],
        [
            "Much effort.",
            "OK, so now that we have the segment, we can visualize it by displaying top features from the feature space that are relevant for this segment so.",
            "I believe we are using content ornamented is we can show the top keywords that make somebody be positively classified in a particular segment or top entities that are relevant.",
            "That particular segment and this can be served as a useful information for the website editor.",
            "So if they want to target develop content, target the particular domain.",
            "And one example is this.",
            "If you have a females that are more than 100,000 per year, their household income earning 100,000 and are interested in style.",
            "They would be reading articles with such keyboard children studies.",
            "I think family health so."
        ],
        [
            "And I'll be sure couple of experiments."
        ],
        [
            "So this is 1 example, so using these segments."
        ],
        [
            "Visualizations with what we can do also is we can.",
            "Use these segments to predict.",
            "The registration fields for new users, so for example.",
            "So we have again the same data set, so from one major publishing website we have 5,000,000 users, out of which one million was registered and we are testing how how easy it is to how well can we generalize from this.",
            "1,000,000, how well can we generalize either gender H or income to the non registered users?",
            "And we see these are the categories we had.",
            "So either male female for age we had, we split by decades.",
            "For earnings which took this classification scale."
        ],
        [
            "So we use the feature vectors as we talked before we use.",
            "Either just text features are just entities that according to documents either just metadata.",
            "So the category, the authors and stuff like that.",
            "And then we also tried combining all the features to see.",
            "If this can."
        ],
        [
            "Help.",
            "So this is the results for gender.",
            "So if you have more than 10 visits, so if you take people that did more than 10 visits.",
            "And we use example.",
            "Context would be just the time, time and geographical location.",
            "If you use the text feature, so the content the words in the articles they read after 10 visits you can be.",
            "You can come to 6065% accuracy with regards to whether somebody is male or female.",
            "If you take all the metadata you can get almost to 70%.",
            "You see just after 10 visits."
        ],
        [
            "Can see now if you have over 50 visits we can come to almost 90.",
            "Almost 80% accuracy for predicting somebody is a male.",
            "Which can be.",
            "Quite well.",
            "Quite significant, so this 50% here is.",
            "This would be random."
        ],
        [
            "Now if you look at H, it gets a bit harder, so we have this parameter segmentation.",
            "So if you take this group till 40, it's quite easy to extract.",
            "Or if you take the group from 60 on, it's quite easy to extract.",
            "But people between 30 and 60 are quite hard to distinguish and at least checking, but for the important features it was that here.",
            "So this young people are quite specific and mostly retired people are also quite specific in and reading habits, whereas here so people that are have a.",
            "There's still work that are working they their interests with more correspond to their job profession versus their H. So that's why the reading habits are not so distinctive between here.",
            "So, but here again, the baseline is 20%, which is a random and you can see that here we can get quite high for after 50 visits for.",
            "Older young people."
        ],
        [
            "Now that's again split by the features we see that name entities are very important for detecting or.",
            "The most useful thing for detecting the H so."
        ],
        [
            "Which means here young people would read about different entities than old people or than this."
        ],
        [
            "During work.",
            "And for the income, it's a bit surprising.",
            "Again.",
            "See people with high incomes are quite here.",
            "The baseline was very low, so 14% would be random.",
            "And people with a high income or prices can be.",
            "Twice as good as random for them, but for everybody else it's very hard to be better than random."
        ],
        [
            "OK, now again to the system over user.",
            "How would you use this?",
            "Can use this for example for advertisers or for analysts trying to target.",
            "Did they want to target users target the non registered users as well so?",
            "Is a really so before if you have 80% of people for whom you don't know the gender, then being at least 80% sure for somebody is already better than not knowing that all."
        ],
        [
            "Another example is new."
        ],
        [
            "Recommendation, so here the task is at the moment of the visit, so this is again real time.",
            "So you come to the website you're visiting.",
            "Particular article or page, and we want to display the recommendations so the top 4 pages that user would like to click and we formulated this as a task.",
            "So we want to predict the category of the article that the user is most interested in and then we display the articles from that.",
            "How we do that?",
            "So we have a.",
            "We take the history of the article of the user, so everything he had in the past plus the current article.",
            "And from this we extract feature features such as history.",
            "So this would be all the articles that he ever read Geo.",
            "So from here he comes.",
            "So what he's reading at the moment, where is he coming at the moment?",
            "And what time?",
            "So it's Monday morning.",
            "Maybe you would read different topics in Friday evening.",
            "And then we check during training procedure we check what the user actually read.",
            "And take the category from this.",
            "And now we do this.",
            "We can generate the training training data for learning a classification model.",
            "So we can classify the user into categories.",
            "Now if you are running actually running life, we don't know the next article.",
            "So we use this same feature vector to classify the user into one of the categories style and soil and select article from these categories as the recommendations."
        ],
        [
            "And we can do this develop this running life we can see.",
            "For example, you can log the visit.",
            "When the user comes, it's independent of.",
            "The visit is locked, stored for heaven sent to the recommendation in the engine to update the index so it takes less than a.",
            "It's a couple of seconds from the click for the index update and independently the call is made to the recommendation and engine and then back for the.",
            "To the website."
        ],
        [
            "OK, so this is about the replica mining.",
            "Cannot one more short example where again we have a stream of network on that."
        ],
        [
            "Vacation network.",
            "Here the data is.",
            "We have a large telecom with more than $25,000 devices and we're getting in alarms.",
            "So alarms would mean the operator installed some agent on each of these 25,000 devices which monitors the log files and whenever a\nAppears in the log file, it's forwarded to some central location and this goes from 10 to hundreds of alarms, events per second, and typically it's not uniform, because when something goes wrong there would be many more events happening per second.",
            "And what we can do so we developed a system that can take this data and do a couple of scenarios on top of that.",
            "Money is root cause analysis so.",
            "Simulating based on historical data to see Co occurrence if for example in the past you saw that once the backup server is not responding then each device that is using that backup to store their daily backups on it will start complaining and next time this same event happens we can already tell after seeing.",
            "After seeing a couple of these devices we can say OK, this is probably the root cause.",
            "Last time in a similar scenario the backup server is the problem, so we can match back to the existing old cases.",
            "Another thing is we can do this short term prediction in the in the future.",
            "So we can say OK Now this.",
            "This device at the backup server is not available, but we know that usually all these 20 device is also complain at the same time, so these are most likely to also issue events in the next short period and long term protection anomaly detection, so this is long-term detecting long-term trends on the network, so if there is usually the operators that are handling these alarms are too focused on solving these short-term fixes that they don't detect this slow long term.",
            "Increases, for example in number of faults in a particular system, but.",
            "You can detect these things quite easily.",
            "And this is this system is."
        ],
        [
            "Diploid as well.",
            "OK, so."
        ],
        [
            "It's also just to conclude, so we had a.",
            "We showed a couple of systems, a couple of studies working on different modalities of stream, so we had a most of the time was spent on this document streams.",
            "After which was extracted this social network.",
            "So this whole code, which entity code, which entity?",
            "And in the second half we focused on this click stream and at the end also event streams in the network.",
            "So.",
            "Show a couple of application on top of it.",
            "So what's the?",
            "Taking her so she would extend these things to other modalities or to other similar systems and so on.",
            "What are the main channel challenges?",
            "Some things that this setup so they are still much work on a large scale.",
            "It's not very commoditized at the moment, so it still require lots of customization fixings and so on to be able to handle different modalities.",
            "So once we can do it, documents or moving to social networks requires quite some changes even though the many.",
            "Distributions and so on are shared between them.",
            "Another challenge is dealing with unstructured data.",
            "So.",
            "How to extract some kind of highly structured events from use example we one way of dealing with this is by using entities as a proxy for what are we tracking in the stream so, but if you would want to go on a bit lower level than this, things becomes much harder.",
            "Another thing is also on extracting semantic stream.",
            "So if you have a for example a sensor network with thousands or 10s of thousands of sensors.",
            "So extracting some.",
            "From this lower level, events which are measured on the level of a particular sensor.",
            "So trying to aggregate this to some bigger events.",
            "This so this complex event processing is one way of trying to tackle these things, but there are still probably some things that can be done once we start talking how automatically identifying related sensors."
        ],
        [
            "That's it.",
            "Any questions?",
            "Think about."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So hello, I'm black Fortuna that were said to come from artificial intelligence laboratory from GSI and the work will be showing today was mostly done together with the rest of our team there mostly to mark a global economic.",
                    "label": 0
                },
                {
                    "sent": "And I'll be that discussion talk about.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is a showing some things about streams and start with some short introduction of what do we.",
                    "label": 0
                },
                {
                    "sent": "By why do we want to mind streams?",
                    "label": 0
                },
                {
                    "sent": "That's what's that language of doing that, but the challenges are doing that, and we'll have two kind of case studies.",
                    "label": 0
                },
                {
                    "sent": "One is analyzing document stream analyzing and visualizing.",
                    "label": 1
                },
                {
                    "sent": "And the second one will be mining web clickstream.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just start introduction.",
                    "label": 0
                },
                {
                    "sent": "So first of all, why would we even want to deal with streams or deal with real time information?",
                    "label": 0
                },
                {
                    "sent": "And there's some same goes the time equals money and if you can react in if you connect fast two events then we can get some value out of it.",
                    "label": 0
                },
                {
                    "sent": "For example if you are talking about reading earnings reports, then we could get some advantage on the Stock Exchange.",
                    "label": 0
                },
                {
                    "sent": "If you are talking about monitoring quality of service then reacting.",
                    "label": 0
                },
                {
                    "sent": "Properly to any failures in the network increases quality of service, again with fraud detection.",
                    "label": 0
                },
                {
                    "sent": "So can we react in real time to any detected?",
                    "label": 0
                },
                {
                    "sent": "Events that appear like fruit, so this brings value.",
                    "label": 0
                },
                {
                    "sent": "And in general direction speed, this case means value and we want to react as fast as possible.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are a couple of areas dealing with such things, so there will be focusing most on this, called streamlining the subfield of data mining and we deals with the incoming data streams so meaning.",
                    "label": 1
                },
                {
                    "sent": "The points that are the data points that are coming to our systems are coming.",
                    "label": 0
                },
                {
                    "sent": "As a time series more less.",
                    "label": 0
                },
                {
                    "sent": "Oh point with the timestamp and we have to react to them, react to them or deal with them as they come in another area that you already saw in the morning tutorial about.",
                    "label": 0
                },
                {
                    "sent": "It's called complex in detection and.",
                    "label": 0
                },
                {
                    "sent": "This is just more fun.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Permission.",
                    "label": 0
                },
                {
                    "sent": "Now what are the standard approaches or what is that?",
                    "label": 0
                },
                {
                    "sent": "What are the challenges when dealing with stream data?",
                    "label": 1
                },
                {
                    "sent": "So one thing is so, first of all, streams can be if you have a small stream, then it's not really a problem, so we can we'll deal with them with off the shelf data mining methods may be a bit adapted for the.",
                    "label": 0
                },
                {
                    "sent": "Temporal scenario, but that's about it.",
                    "label": 0
                },
                {
                    "sent": "The problem becomes sadder when the data streams come become too intense to be handled by the shelves.",
                    "label": 0
                },
                {
                    "sent": "Data mining methods, or when the events that are coming through or operations that we want to apply on their streams becomes too complex.",
                    "label": 0
                },
                {
                    "sent": "So another example example of what would complex operation mean?",
                    "label": 0
                },
                {
                    "sent": "So if a document comes in.",
                    "label": 0
                },
                {
                    "sent": "Understanding what the document document means relating it to the existing documents in the stream and so on, can require parts and computation, and we need some special data structures to deal with such things.",
                    "label": 0
                },
                {
                    "sent": "And also since the data is coming in in real time through streams, we can have this shifting distribution.",
                    "label": 0
                },
                {
                    "sent": "So what's the topic slightly changed?",
                    "label": 0
                },
                {
                    "sent": "So if you would look at example articles about politics than the topic slowly changed the election.",
                    "label": 0
                },
                {
                    "sent": "Through no budget periods and so on, and we have to our models or other operations on the stream have to take this into account.",
                    "label": 0
                },
                {
                    "sent": "This changing distributions a couple other features of streams most.",
                    "label": 1
                },
                {
                    "sent": "Often they are too big to be stored, so we don't really want to store them all the data and we don't usually don't even need to store all the data.",
                    "label": 1
                },
                {
                    "sent": "We just need to have ability to take into account any changes in the stream and to keep some.",
                    "label": 0
                },
                {
                    "sent": "For example sliding window through the data and being able to adapt the models on the fly according to the new events introduced into the time window.",
                    "label": 1
                },
                {
                    "sent": "You also, since the data is coming through slowly through time we have.",
                    "label": 0
                },
                {
                    "sent": "We don't really have a luxury to look at it twice often, so we can't.",
                    "label": 0
                },
                {
                    "sent": "We have to be as the credit card transaction comes through, we see it only once and then we can react to whether it's fraud or not.",
                    "label": 0
                },
                {
                    "sent": "And after two years it's very hard to go back and recheck it because of the computational constraints usually.",
                    "label": 0
                },
                {
                    "sent": "This is nonstationary data, so that changes in the distributions through time.",
                    "label": 0
                },
                {
                    "sent": "And one thing is that we usually don't want to store the data explicitly, so we want to store them in some aggregate form, so we don't want to store each particular document.",
                    "label": 0
                },
                {
                    "sent": "Maybe, but just the aggregates of the documents.",
                    "label": 0
                },
                {
                    "sent": "A lot of the main keywords.",
                    "label": 0
                },
                {
                    "sent": "What are the main topics?",
                    "label": 0
                },
                {
                    "sent": "But other entities in them?",
                    "label": 0
                },
                {
                    "sent": "And then be able to update these aggregates on the fly.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is some kind of genetic infrastructure which usually we use parts of it when developing real time streaming systems.",
                    "label": 0
                },
                {
                    "sent": "So we get on the input.",
                    "label": 1
                },
                {
                    "sent": "We get some events which we have to capture either by some agents.",
                    "label": 0
                },
                {
                    "sent": "Deploy it on the networking equipment.",
                    "label": 0
                },
                {
                    "sent": "Detecting alarm source example JavaScript on the web page collecting the data.",
                    "label": 0
                },
                {
                    "sent": "Then we have to transform this.",
                    "label": 0
                },
                {
                    "sent": "Put it into its get some standardized form for our system.",
                    "label": 0
                },
                {
                    "sent": "And then send this to the model which is updated based on the.",
                    "label": 0
                },
                {
                    "sent": "New data.",
                    "label": 0
                },
                {
                    "sent": "And this model can then be used either query to predict either for the future events either for the given events or reason about the stream.",
                    "label": 0
                },
                {
                    "sent": "So this is more like more or less generic.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About streams and I will go check them in more details on two case studies.",
                    "label": 0
                },
                {
                    "sent": "The first one is focusing on analyzing document streams.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do we mean by document stream?",
                    "label": 0
                },
                {
                    "sent": "So we mean collect large collection of documents, so we employ millions of documents produced by news outlets, news publishers and what happens then?",
                    "label": 0
                },
                {
                    "sent": "Slowly?",
                    "label": 0
                },
                {
                    "sent": "So when we get to the millions of documents we don't, we can't really focus too much on the linguistic structure of 1 particular document.",
                    "label": 0
                },
                {
                    "sent": "We have to start looking documents in the aggregate forms.",
                    "label": 1
                },
                {
                    "sent": "And that means extracting keywords and entities and so on.",
                    "label": 0
                },
                {
                    "sent": "And the extra step sentences so the orders of the words in the documents start mattering less and less once we increase the scale of the documents.",
                    "label": 0
                },
                {
                    "sent": "And usually we are interested not in the documents themselves, but in the information that's contained in this document.",
                    "label": 1
                },
                {
                    "sent": "So this can be extracted either by extracting entities, so people organization companies that are mentioned in the documents and the relationships between these entities.",
                    "label": 0
                },
                {
                    "sent": "So if company a acquires Company B so that kind of just simple statement can summarize quite well quite many documents, news articles that appear on the business.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So first.",
                    "label": 0
                },
                {
                    "sent": "We have to represent the documents.",
                    "label": 0
                },
                {
                    "sent": "So how do we represent the documents?",
                    "label": 0
                },
                {
                    "sent": "One of the standard things.",
                    "label": 0
                },
                {
                    "sent": "One of the most under this text mining approach for representing documents is to represent it as a vector in some vector space and the vector space.",
                    "label": 0
                },
                {
                    "sent": "So we start by checking the detecting the vocabulary of the documents.",
                    "label": 0
                },
                {
                    "sent": "So we checked the corpus or at least the corpus that is also far and extract all the unique words and we assign the unique number to each word and this forms our basis.",
                    "label": 0
                },
                {
                    "sent": "So this work.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "So each.",
                    "label": 0
                },
                {
                    "sent": "Word has its own basis which dimension in director space and we represent the document as a vector in this space where we have a sum value non 0 value for the words that occur in the document and zero value for other words.",
                    "label": 0
                },
                {
                    "sent": "So here is an example.",
                    "label": 0
                },
                {
                    "sent": "So we have three very simple documents.",
                    "label": 0
                },
                {
                    "sent": "If you check the.",
                    "label": 0
                },
                {
                    "sent": "What is the vocabulary that occurs in these documents?",
                    "label": 0
                },
                {
                    "sent": "We get these 6 words.",
                    "label": 0
                },
                {
                    "sent": "This gives us this task that our vectors this text document vectors are 6 dimensional.",
                    "label": 0
                },
                {
                    "sent": "And we put account for each vector for each document.",
                    "label": 0
                },
                {
                    "sent": "For each word that occurs in the document.",
                    "label": 0
                },
                {
                    "sent": "For example, in the second document, numerical analysis appear and we look at.",
                    "label": 0
                },
                {
                    "sent": "Here numerical is in the 4th place.",
                    "label": 0
                },
                {
                    "sent": "An analysis is in the first place.",
                    "label": 0
                },
                {
                    "sent": "That means we have one count, one on the 1st and the 4th place and zero everywhere else.",
                    "label": 0
                },
                {
                    "sent": "Now if you have a bit more complex or longer documents then these are not only zeros and ones we have.",
                    "label": 0
                },
                {
                    "sent": "We can be a bit more complex, complex on.",
                    "label": 0
                },
                {
                    "sent": "A bit more smarter about estimating the weights of the words, but it is important that the weight in the vector corresponds to importance in a way to importance of the word in the document.",
                    "label": 0
                },
                {
                    "sent": "And once we have this representation, we can start comparing the documents and tech checking what's similar and what's not, and we do that by checking the distance in this space in this vector space.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you want to see what's the difference between X1 document one and document two, we just took this to take these two vectors and which take the cosine distance, causing similarity between these two, which means so there are two vectors in a vector space.",
                    "label": 0
                },
                {
                    "sent": "We just take the calls in between the angle between vectors.",
                    "label": 0
                },
                {
                    "sent": "And this turned out to be quite a good measure of similarity between documents.",
                    "label": 0
                },
                {
                    "sent": "In a bitter naive explanation, we could just say that we are checking the overlap between the documents and the more important words overlap, the higher the similarity between these documents and the similarities.",
                    "label": 0
                },
                {
                    "sent": "Usually something between zero and one.",
                    "label": 0
                },
                {
                    "sent": "So 1 means exactly the same as zero means no work.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Connor now will so one thing that is a.",
                    "label": 0
                },
                {
                    "sent": "So it's useful to do with a large collections is visualization and we go now through some procedure for visualizing the document corpus first on the offline setting, and then we move it on the online setting.",
                    "label": 0
                },
                {
                    "sent": "So as we saw the effuse back and forth representation for documents, each document ends up living in some high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So it's a vector in 10 thousands of more dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And if you want to visualize them.",
                    "label": 0
                },
                {
                    "sent": "So if you want to plot kind of map of these documents on the computer screen, then we are very limited with the two dimensions.",
                    "label": 0
                },
                {
                    "sent": "And now the question is how do we?",
                    "label": 0
                },
                {
                    "sent": "Transform documents that are positioned in this high dimensional space to just two dimensions.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we have a developed procedure that does this quite efficiently.",
                    "label": 0
                },
                {
                    "sent": "So we start with approach called Latin semantic indexing that can take.",
                    "label": 1
                },
                {
                    "sent": "Dimensional space reduce it down to a couple 100.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nations and notice that in semantic indexing.",
                    "label": 0
                },
                {
                    "sent": "So it's a linear technique provide and it identifies sports that usually Co occur together.",
                    "label": 1
                },
                {
                    "sent": "Then and if you have a large enough corpus disco occurrence over many documents usually means they are related so worth.",
                    "label": 0
                },
                {
                    "sent": "Computer software hardware.",
                    "label": 0
                },
                {
                    "sent": "This kind of works with usually Co occur and we can squeeze them together into one dimension and this lattice semantic indexing can identify these groups of words.",
                    "label": 0
                },
                {
                    "sent": "And one group of words.",
                    "label": 1
                },
                {
                    "sent": "We call it concept or latent variable and it's just a linear combination of these words from the original space and how to compute this so the back end is just a singular value decomposition composition where we take the metrics with all the documents as Rose.",
                    "label": 1
                },
                {
                    "sent": "We decompose it using similar relative composition.",
                    "label": 1
                },
                {
                    "sent": "And these Canonical dimensions.",
                    "label": 0
                },
                {
                    "sent": "We use them as the hour basis for the documents.",
                    "label": 0
                },
                {
                    "sent": "So, but in essence, this is the.",
                    "label": 1
                },
                {
                    "sent": "According to Shen, so grouping words with similar meaning that Coker together often and using this approach we can get down to.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "100 dimensions and if but if you would try to just take 2 dimensions then we throw away too much information because the linear model so linear just taking linear combination of words is too weak to capture to capture this.",
                    "label": 0
                },
                {
                    "sent": "More topics.",
                    "label": 0
                },
                {
                    "sent": "So what we do we use another technique called multidimensional scaling to get from this few hundreds to dementia.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And multidimensional scaling is actually a nonlinear technique and it starts by compute the distance between similarity between each two points in the original space and this.",
                    "label": 1
                },
                {
                    "sent": "Latin space we developed we constructed here.",
                    "label": 0
                },
                {
                    "sent": "And then we try to we run an optimization procedure which derives.",
                    "label": 0
                },
                {
                    "sent": "Output positions of points in two dimensions in such ways that the position on 2 dimensional plane matches as much as possible to original distance.",
                    "label": 1
                },
                {
                    "sent": "Now, if you want to, if you run this optimization over the complete data set, then close document.",
                    "label": 0
                },
                {
                    "sent": "Similar documents are grouped together on the map and not so similar documents or not.",
                    "label": 0
                },
                {
                    "sent": "For the report.",
                    "label": 0
                },
                {
                    "sent": "In this can be efficiently implemented as well for larger data.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the at the end.",
                    "label": 0
                },
                {
                    "sent": "Once we get these two dimensions, we have to display it and when we are displaying we can add a couple of tools.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, just to reiterate a bit, so here we have, uh, this is the visualizations of each point corresponds to 1 document, and these two points, for example, are very close together.",
                    "label": 0
                },
                {
                    "sent": "That means that the distance in the original space was quite close, so which means they had a high overlap of important keywords, whereas for example, this document in these documents up here are farther apart, which means that the distance in the original space was quite longer.",
                    "label": 0
                },
                {
                    "sent": "They were not so similar and.",
                    "label": 0
                },
                {
                    "sent": "Which corresponds to low overlap of the important keywords.",
                    "label": 0
                },
                {
                    "sent": "Now that we have these positions, we can estimate the density so you can see this lighter area correspond to the more dense area of the map.",
                    "label": 0
                },
                {
                    "sent": "And the darker area correspond to less dense areas and.",
                    "label": 0
                },
                {
                    "sent": "Can do this user keeps which then kind of show the clusters of the documents.",
                    "label": 0
                },
                {
                    "sent": "Will see latency later in the actual examples or this.",
                    "label": 0
                },
                {
                    "sent": "How can we identify clusters from this?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing we can do, we can compute keywords for the map.",
                    "label": 0
                },
                {
                    "sent": "So if you take these documents from here, but the most important keywords that define the document there, we can just take the what we do.",
                    "label": 0
                },
                {
                    "sent": "We take the area around here, check the words in this bag of words, vectors and take the one with the highest weight.",
                    "label": 0
                },
                {
                    "sent": "If you average these documents and display them on the screen so.",
                    "label": 0
                },
                {
                    "sent": "This I think these points here are descriptions of European projects.",
                    "label": 0
                },
                {
                    "sent": "From FB 6 and you can see for example this part versus semantic web projects.",
                    "label": 0
                },
                {
                    "sent": "This was about.",
                    "label": 0
                },
                {
                    "sent": "Services web services.",
                    "label": 0
                },
                {
                    "sent": "I think then medical learning, machine learning, cognitive science and so on.",
                    "label": 0
                },
                {
                    "sent": "And we can see that just by glimpsing at this map, not needing to check the actual content of.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Documents.",
                    "label": 0
                },
                {
                    "sent": "We can also focus.",
                    "label": 0
                },
                {
                    "sent": "So here we went with the mouse and we circled this area here and you can get a more detailed list of keywords for that area.",
                    "label": 0
                },
                {
                    "sent": "And one now.",
                    "label": 0
                },
                {
                    "sent": "Now this all this was done on a static map.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if we start, if you have a times time points attached to the documents, we can start seeing how the documents move through such map overtime, so we'll see now in a demo.",
                    "label": 0
                },
                {
                    "sent": "So here are one point corresponds to 1 document and this was in year 2004 and we see that we can see.",
                    "label": 0
                },
                {
                    "sent": "The gradient of change, so the longer the arrow, the more bigger the distance the point will move in the next year.",
                    "label": 0
                },
                {
                    "sent": "So switch to the demo.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the map, so the data set here is we have a.",
                    "label": 0
                },
                {
                    "sent": "Collection of papers from a machine learning that machine learning data mining community.",
                    "label": 0
                },
                {
                    "sent": "So from Pascal Network we have all the papers, so for the group of people produced in 2004 and in 2005, so each point.",
                    "label": 0
                },
                {
                    "sent": "Which point corresponds to one person?",
                    "label": 0
                },
                {
                    "sent": "So, for example, this point here corresponds to Andreas.",
                    "label": 0
                },
                {
                    "sent": "Opold from University of Leuven and these are the abstracts of the papers that he quoted in that year.",
                    "label": 0
                },
                {
                    "sent": "And this we do that for each person.",
                    "label": 0
                },
                {
                    "sent": "Now just by running the optimizations that we went through in the previous slides, we derive this map.",
                    "label": 0
                },
                {
                    "sent": "And looking at this we can see there are a couple of clusters.",
                    "label": 0
                },
                {
                    "sent": "So this is one group of similar.",
                    "label": 0
                },
                {
                    "sent": "People, this is another group, and here is another bigger group.",
                    "label": 0
                },
                {
                    "sent": "Plus these people in the center.",
                    "label": 0
                },
                {
                    "sent": "So central position usually corresponds to somebody that is between multiple areas.",
                    "label": 0
                },
                {
                    "sent": "Now if you display the.",
                    "label": 0
                },
                {
                    "sent": "Common words.",
                    "label": 0
                },
                {
                    "sent": "Criticize.",
                    "label": 0
                },
                {
                    "sent": "So we can see that these people here are working on the images, so computer vision.",
                    "label": 0
                },
                {
                    "sent": "These are text mining people, natural language processing people, whereas here it was more.",
                    "label": 0
                },
                {
                    "sent": "This computational learning theory, kernel methods and so on.",
                    "label": 0
                },
                {
                    "sent": "How come there is duplicates on this map?",
                    "label": 0
                },
                {
                    "sent": "After all, the vector analysis that we've done before, so duplicating this keywords appear, so each keyword is computed independently and it's just by randomly sample points over the map and compute the most important keyword for that area of the map.",
                    "label": 0
                },
                {
                    "sent": "And if it appears more time, it just means that the word is important in more areas.",
                    "label": 0
                },
                {
                    "sent": "So we could also had some versions where we were trying to remove these duplicates, but then it increases the cost in the.",
                    "label": 0
                },
                {
                    "sent": "Computation.",
                    "label": 0
                },
                {
                    "sent": "What about documents are in different language?",
                    "label": 0
                },
                {
                    "sent": "So if if the documents provide that would be in different language, yes.",
                    "label": 0
                },
                {
                    "sent": "So then we would need.",
                    "label": 0
                },
                {
                    "sent": "So we had some experience with this on.",
                    "label": 0
                },
                {
                    "sent": "Constitutions, so if you have documents from different languages, then you have to first match the words from different languages other than dictionary, either using some automatic methods like Canonical correlation analysis is 1 method method for detecting correlated words across the languages.",
                    "label": 0
                },
                {
                    "sent": "So if you train these systems by having a long set of parallel sentences.",
                    "label": 0
                },
                {
                    "sent": "And you can then.",
                    "label": 0
                },
                {
                    "sent": "Identify this concept.",
                    "label": 0
                },
                {
                    "sent": "This like we have this Latin semantic indexing here.",
                    "label": 0
                },
                {
                    "sent": "There are techniques that can take different corpuses in different languages and map them down to the same space where each dimension corresponds to the same thing in different languages.",
                    "label": 0
                },
                {
                    "sent": "What about if you have, let's say document that isn't alphabetic, for example numbers.",
                    "label": 0
                },
                {
                    "sent": "Numbers so here.",
                    "label": 0
                },
                {
                    "sent": "So the in the first step, when we take the document, it's already transformed into a vector and from then on everything works on a medical object we had another.",
                    "label": 0
                },
                {
                    "sent": "For example another.",
                    "label": 0
                },
                {
                    "sent": "Then move it on.",
                    "label": 0
                },
                {
                    "sent": "This is using images, so from images you can extract common markers either histogram based or some other feature based, and you can then also get similar Maps and then you would have a here you would end up with pictures of Flowers of cities and so on, but this is it's possible to do it on non textual data as well.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, so now if you turn on this magnifying so this is if I move the mouse.",
                    "label": 0
                },
                {
                    "sent": "So we see this circle and these keywords are computed as the top keywords from this circle and now we see if you move around.",
                    "label": 0
                },
                {
                    "sent": "Around this area, images is always the top keywords and that's why it was already select multiple times.",
                    "label": 0
                },
                {
                    "sent": "And if you go here with its documents text this day is here.",
                    "label": 0
                },
                {
                    "sent": "There were a couple of French articles.",
                    "label": 0
                },
                {
                    "sent": "That's why this, I think appears here, but we didn't control for that.",
                    "label": 0
                },
                {
                    "sent": "And this is unable to submit more detailed analysis.",
                    "label": 0
                },
                {
                    "sent": "We can also.",
                    "label": 0
                },
                {
                    "sent": "Display the names of the people, so here are just rank them by the number of publications.",
                    "label": 0
                },
                {
                    "sent": "So we see this.",
                    "label": 0
                },
                {
                    "sent": "The top people did publish most articles in that year from that network and we can show relations so.",
                    "label": 0
                },
                {
                    "sent": "This shows the Co authors of this person.",
                    "label": 0
                },
                {
                    "sent": "Now if we talked about before about the temporal dimension, so we can move to year 2005.",
                    "label": 0
                },
                {
                    "sent": "Now map changes a bit.",
                    "label": 0
                },
                {
                    "sent": "We see that this images area gets a bit more isolated.",
                    "label": 0
                },
                {
                    "sent": "This kernel methods area gets a bit smaller and these documents area gets a bit more prominent here as well.",
                    "label": 0
                },
                {
                    "sent": "And we can see for each other if you go back.",
                    "label": 0
                },
                {
                    "sent": "And we turn this gradient on.",
                    "label": 0
                },
                {
                    "sent": "We can see what was the direction of the people.",
                    "label": 0
                },
                {
                    "sent": "So how these areas here moved more towards images or more towards document processing.",
                    "label": 0
                },
                {
                    "sent": "So here So what are the tendencies of the people?",
                    "label": 0
                },
                {
                    "sent": "From the changes from their publishing.",
                    "label": 0
                },
                {
                    "sent": "Abstracts from 2004 and 2005.",
                    "label": 0
                },
                {
                    "sent": "We can also see the actual trace.",
                    "label": 0
                },
                {
                    "sent": "So this see this person went here from.",
                    "label": 0
                },
                {
                    "sent": "Completely left to the center of documents cluster in 2005.",
                    "label": 0
                },
                {
                    "sent": "However, some person here, for example.",
                    "label": 0
                },
                {
                    "sent": "Just moved a little bit to the left.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is one data set.",
                    "label": 0
                },
                {
                    "sent": "If you go back.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is this was this demo now here.",
                    "label": 0
                },
                {
                    "sent": "We had a very specific data set where people were easy to extract, so person and we had a describe.",
                    "label": 0
                },
                {
                    "sent": "The person with the abstract within the model generic scenario when you would have a document.",
                    "label": 0
                },
                {
                    "sent": "So let's say typical.",
                    "label": 0
                },
                {
                    "sent": "Documents and we want to do the similar graphs we first have to extract entities and this is where this service we develop called enricher comes handy.",
                    "label": 0
                },
                {
                    "sent": "So let me show you demo.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a bit easier.",
                    "label": 0
                },
                {
                    "sent": "So we start by loading the article, so this is a web service.",
                    "label": 0
                },
                {
                    "sent": "If you go to.",
                    "label": 0
                },
                {
                    "sent": "And Richard Rodgers.",
                    "label": 0
                },
                {
                    "sent": "Notice I get to this live demo.",
                    "label": 0
                },
                {
                    "sent": "We can load the text in, so here are some.",
                    "label": 0
                },
                {
                    "sent": "Upload that articles and if you click and rich now the system goes and takes this article and identifies all the entities that occur in this document.",
                    "label": 0
                },
                {
                    "sent": "So you see Brazil.",
                    "label": 0
                },
                {
                    "sent": "Italy, Germany so on identifies the top keywords that describe this article, so we see a sports.",
                    "label": 0
                },
                {
                    "sent": "Soccer was one of the main keywords here and most teams is 1 big taxonomy categories for this article and for each entity also tell what it is.",
                    "label": 0
                },
                {
                    "sent": "So we see that Brazil.",
                    "label": 0
                },
                {
                    "sent": "Is actually.",
                    "label": 0
                },
                {
                    "sent": "Country Brazil, so if you go.",
                    "label": 0
                },
                {
                    "sent": "It would follow this link to get and we see also cycle concept.",
                    "label": 0
                },
                {
                    "sent": "So you suck at the beginning of the week.",
                    "label": 0
                },
                {
                    "sent": "So we gotta link to the open cycle concept of this entity.",
                    "label": 0
                },
                {
                    "sent": "Other names for Brazil.",
                    "label": 0
                },
                {
                    "sent": "And categories, we see that it's a Latin American country, and so on.",
                    "label": 0
                },
                {
                    "sent": "So we do this by linking to think, taking the context of the Brazilian this document and identifying, resolving it into some external ontology.",
                    "label": 0
                },
                {
                    "sent": "In this case, the pedia.",
                    "label": 0
                },
                {
                    "sent": "So, but now if you do this for each article on the stream, then we can slowly start getting snippets about Russia.",
                    "label": 0
                },
                {
                    "sent": "For example, if you always take the sentences around that article about that entity and we can use this to describe the entity so we can describe the entity by the sentences or paragraphs in which it occurs.",
                    "label": 0
                },
                {
                    "sent": "And we can do this similar visualizations as we did before for the this research network.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Oh, and if you go.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Demo and if you do that will show now in a demo of.",
                    "label": 0
                },
                {
                    "sent": "Built on top of Reuters news corpus.",
                    "label": 0
                },
                {
                    "sent": "So this is a standard to Corpus Reuters, released for the research community and it contains news articles, all news articles, writers published from Summer 2006, Summer 96 to 797.",
                    "label": 0
                },
                {
                    "sent": "Now if you go here so we see.",
                    "label": 0
                },
                {
                    "sent": "So now we start to the end of the August and these are the entities that occur in that article.",
                    "label": 0
                },
                {
                    "sent": "The top entities that occur in the articles you see US.",
                    "label": 0
                },
                {
                    "sent": "And yours is mostly around center 'cause it's present in many articles.",
                    "label": 0
                },
                {
                    "sent": "And then here you would have this.",
                    "label": 0
                },
                {
                    "sent": "North America Pacific region.",
                    "label": 0
                },
                {
                    "sent": "Here we would have more.",
                    "label": 0
                },
                {
                    "sent": "Business area and here it's about more South America.",
                    "label": 0
                },
                {
                    "sent": "Entities and what we can do is we see for example, Bill Clinton.",
                    "label": 0
                },
                {
                    "sent": "And if you change, we can see how he moved around through the map so.",
                    "label": 0
                },
                {
                    "sent": "How did entities how occurs in different contexts as the time went on?",
                    "label": 0
                },
                {
                    "sent": "And we can see, for example, the trace.",
                    "label": 0
                },
                {
                    "sent": "We can see that he started started here.",
                    "label": 0
                },
                {
                    "sent": "And then he went to he was in some business Contacts.",
                    "label": 0
                },
                {
                    "sent": "Then he went to think he went for a visit to China or somewhere there.",
                    "label": 0
                },
                {
                    "sent": "So he occurs here for some time and then went back down.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you look at the USA it's mostly traveling here in the center, so 'cause it occurs in many articles and it occurs in many topics or Argentina and other hand, it's very local and it would occur only in the context of soft America.",
                    "label": 0
                },
                {
                    "sent": "Now this is just taking the entity and positioning it against the context of the topic topic context.",
                    "label": 0
                },
                {
                    "sent": "We can also check the relations so we can see relationship between Bill Clinton for example Washington.",
                    "label": 0
                },
                {
                    "sent": "And at that time it was.",
                    "label": 0
                },
                {
                    "sent": "Election campaign and Bob Dole was one of the candidates.",
                    "label": 0
                },
                {
                    "sent": "NBC, the relationship between Bill Clinton and Washington contains keyword like Clinton Dole, Washington Republican Convention and so on.",
                    "label": 0
                },
                {
                    "sent": "What is relationship to Russia is about?",
                    "label": 0
                },
                {
                    "sent": "Clinton, Yeltsin, Chechnya, Grozny, and so on.",
                    "label": 0
                },
                {
                    "sent": "Another example is towards Mexico, so here it's about.",
                    "label": 0
                },
                {
                    "sent": "Some.",
                    "label": 0
                },
                {
                    "sent": "Drug dealing.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you go through the end.",
                    "label": 0
                },
                {
                    "sent": "He's not even in a relationship with Mexico anymore.",
                    "label": 0
                },
                {
                    "sent": "'cause he was not mentioned in any context.",
                    "label": 0
                },
                {
                    "sent": "Another very interesting example is interesting is.",
                    "label": 0
                },
                {
                    "sent": "Kids.",
                    "label": 0
                },
                {
                    "sent": "Can't see it now, so at some point he's in a relationship with a hot Chicago is here.",
                    "label": 1
                },
                {
                    "sent": "So he has a relationship with Chicago because somebody had some Democratic Democratic convention there.",
                    "label": 0
                },
                {
                    "sent": "But as time goes on, the relationship disappeared because he's not.",
                    "label": 0
                },
                {
                    "sent": "Bill Clinton is not mentioned in context of Chicago anymore.",
                    "label": 0
                },
                {
                    "sent": "OK, So what did this so this weekend?",
                    "label": 0
                },
                {
                    "sent": "Basically what we this tool shows that we can extract profiles of entities, position them in the topical context and also identify relationships just by looking at this stream of documents that's coming through and we can visualize them quite easily.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I want a last example is on document is called the News Analyzer.",
                    "label": 0
                },
                {
                    "sent": "So here we have the same data set but we are visualizing such queries.",
                    "label": 0
                },
                {
                    "sent": "So here we see we can put in the query we get list of all the results, then the topics as they evolve through that data set that may go.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is analyzer.",
                    "label": 0
                },
                {
                    "sent": "So let's say we start it, Clinton.",
                    "label": 0
                },
                {
                    "sent": "So these are all the articles from Reuters Corpus in which Clinton occurred.",
                    "label": 0
                },
                {
                    "sent": "And here we have a graph that shows a timeline from August 96 to Aug 97 and we have a split automatically.",
                    "label": 0
                },
                {
                    "sent": "This set of documents into five clusters and here they are displayed.",
                    "label": 0
                },
                {
                    "sent": "The point is a bit small, so the first the Orange one corresponds to the campaign.",
                    "label": 0
                },
                {
                    "sent": "So we can see how the campaign.",
                    "label": 0
                },
                {
                    "sent": "Election campaign was happening on in November.",
                    "label": 0
                },
                {
                    "sent": "The election actually happened and then the topic died out.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, this budget topic started after the election, so around New York and it was moving on from the most of the first half of the year.",
                    "label": 0
                },
                {
                    "sent": "There are also this rapid blue cluster.",
                    "label": 0
                },
                {
                    "sent": "Here is about not oh I think then it was discussions about not extending to East Europe, so they had quite a big.",
                    "label": 0
                },
                {
                    "sent": "Quite some coverage on the news now if you go.",
                    "label": 0
                },
                {
                    "sent": "Here we see also that Clinton is quite uniformly covered throughout the year.",
                    "label": 0
                },
                {
                    "sent": "Now if you say.",
                    "label": 0
                },
                {
                    "sent": "Instead of Clinton, Clinton, and Dole, so it's a bit more focused.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So this is running on my laptop.",
                    "label": 0
                },
                {
                    "sent": "OK. We see the Clinton and Dole occurred together at the beginning during the election campaign, but once it was done, the topic models died out.",
                    "label": 0
                },
                {
                    "sent": "So you can also see other more seasonal things if you search for influencer.",
                    "label": 0
                },
                {
                    "sent": "Pop.",
                    "label": 0
                },
                {
                    "sent": "Should work.",
                    "label": 0
                },
                {
                    "sent": "Let's restart.",
                    "label": 0
                },
                {
                    "sent": "We can see so influenza, flu or has a spike around winter.",
                    "label": 0
                },
                {
                    "sent": "And we can see also what are the topics.",
                    "label": 0
                },
                {
                    "sent": "So let's vaccine.",
                    "label": 0
                },
                {
                    "sent": "There are some poultry China ban and the biggest one.",
                    "label": 0
                },
                {
                    "sent": "I guess there are some influence of flow on this long World Cup or one very interesting one is also ski jumps.",
                    "label": 0
                },
                {
                    "sent": "Ski jumps so they're there.",
                    "label": 0
                },
                {
                    "sent": "Again, it's seasonal sports in winter it's quite big and we see the biggest cluster is called Peter Castle.",
                    "label": 0
                },
                {
                    "sent": "Slovenian ski jumper was quite good demo.",
                    "label": 0
                },
                {
                    "sent": "So this this kind of tools allow us to explore this temporal aspects of data sets.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here are some couple of examples using the same tool on top of New York Times archive.",
                    "label": 0
                },
                {
                    "sent": "So for the period of Second World War, and if you search for Pearl Harbor, we see that first two years it was not much mentioned, but on the attack happened it was around here somewhere.",
                    "label": 0
                },
                {
                    "sent": "The topic models explode.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, the opposite was about Belgrade, so there was quite some topic.",
                    "label": 0
                },
                {
                    "sent": "Deal 2000 till 41.",
                    "label": 0
                },
                {
                    "sent": "Whether you would like your Belgrade will join on the website of the will join the war.",
                    "label": 0
                },
                {
                    "sent": "But once the decision was taken and the new Slavia capitulated, that was more or less out of music.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Relevant anymore.",
                    "label": 0
                },
                {
                    "sent": "And the phone is this Normandy we see there is a bit of a spike at the beginning, mostly about sailing, so people selling probably across the channel and then it was a topic completely diet until 44 when the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They happened.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we are doing with this now so this is more like a infrastructural slide.",
                    "label": 0
                },
                {
                    "sent": "So we are connecting.",
                    "label": 0
                },
                {
                    "sent": "We have we're using Spinner so it's a service for.",
                    "label": 0
                },
                {
                    "sent": "It crawls mainstream news blogs microblogs like Twitter and they collect around 30 million items per day.",
                    "label": 0
                },
                {
                    "sent": "And we are developing infrastructure for automatically handling, processing and indexing this stream.",
                    "label": 0
                },
                {
                    "sent": "But each article is processed by Enricher as we showed before, and the idea is that we have a.",
                    "label": 0
                },
                {
                    "sent": "To provide similar functionalities as shown in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "On top of this, and this was done.",
                    "label": 0
                },
                {
                    "sent": "All this is happening in the context of render project.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested more about this, if you can Google for a render project on the Inter.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And one last example of our handling clusters scanning document stream.",
                    "label": 0
                },
                {
                    "sent": "So this is stream clustering.",
                    "label": 0
                },
                {
                    "sent": "So in the previous example we're visualization.",
                    "label": 0
                },
                {
                    "sent": "We are mostly focused on the top keywords of the articles or positioning them or visualizing them.",
                    "label": 0
                },
                {
                    "sent": "But if we want to derive topics on the fly then we can have things called string clustering where we start with some.",
                    "label": 0
                },
                {
                    "sent": "Initial buffer of documents, for example, to get this is also done under writers to get four clusters and as new documents coming in, and we position them in this clusters, we have to further extent.",
                    "label": 0
                },
                {
                    "sent": "Or delete concepts from it so and more details are in large box paper from 2008.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 example of.",
                    "label": 0
                },
                {
                    "sent": "Taking a standard approach to clustering and adapting it to working on streams.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this was the first case study and I will go through the second one which is a web click streamlining.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's the idea here?",
                    "label": 0
                },
                {
                    "sent": "Here we have, so before we had only one stream of documents, now we have a couple of streams, so we have a stream of content of news articles that are coming that are published on the web.",
                    "label": 0
                },
                {
                    "sent": "We have another stream of click events that are happening, so people visiting particular page and sometimes you also have some user data or more more and more often.",
                    "label": 0
                },
                {
                    "sent": "As we saw yesterday the privacy talk and now the task is can be.",
                    "label": 0
                },
                {
                    "sent": "Use this data to model particular user segments and also to recommend contents to the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Users.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "For the overview of the system, and then we'll go piece by Piece Street.",
                    "label": 0
                },
                {
                    "sent": "So we started by access locks, so this is standard.",
                    "label": 0
                },
                {
                    "sent": "For example Apache Web server collecting all the streams, all the clicks pages that happen and we store them in some file.",
                    "label": 0
                },
                {
                    "sent": "And then we also have articles or web pages that are visited by these people.",
                    "label": 0
                },
                {
                    "sent": "And sometimes for some users we have their data user data, either if they registered.",
                    "label": 1
                },
                {
                    "sent": "For example, we would know their demographics, age, income and so on.",
                    "label": 0
                },
                {
                    "sent": "So we take all these three sources of data, combine them together in an index.",
                    "label": 0
                },
                {
                    "sent": "And then the implication is that some domain experts can define the user segment.",
                    "label": 0
                },
                {
                    "sent": "We use this data from here to construct a model from the user segment.",
                    "label": 0
                },
                {
                    "sent": "Either visualize the segment to see what are their characteristics or apply it, for example, for other for recommending additional articles either for.",
                    "label": 0
                },
                {
                    "sent": "Targeting news to them or so on.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now step by step.",
                    "label": 0
                },
                {
                    "sent": "But what do you mean by access log?",
                    "label": 0
                },
                {
                    "sent": "So we're typically web pages would be logging the user interaction with the websites, so that means for each page view we would have some user ID, which is typically done by dropping a cookie on the user side, date and time of when it happened.",
                    "label": 0
                },
                {
                    "sent": "So either local time so including the location of the user, either the some Standard Time GMT for example, then often we can get location from the IP.",
                    "label": 0
                },
                {
                    "sent": "So looking at the back of the user.",
                    "label": 0
                },
                {
                    "sent": "Please come from we can know what he requested.",
                    "label": 0
                },
                {
                    "sent": "So which pages viewing where he came from.",
                    "label": 0
                },
                {
                    "sent": "So after that he came from Facebook or this came from some home page front page.",
                    "label": 0
                },
                {
                    "sent": "If they come from Google, we can also see or from search engine we can see a search query so we can extract it from front page and from the user agent we can get which browser device operating system he's using.",
                    "label": 0
                },
                {
                    "sent": "Then this tracking of the user, so keeping ideas is usually done by dropping cookies on the user.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One example of this is so this is some ID with some IP and you can if you go do Geo location on this IP we can see that it corresponds to Beijing, China.",
                    "label": 0
                },
                {
                    "sent": "We get some, you requested some URL from where he came from when it happened, that was the user agent and from here you can see that it's somebody was using Safari on Windows.",
                    "label": 0
                },
                {
                    "sent": "Then so this is one step.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second stream is articles, so we can take the what we see from the article as we saw before.",
                    "label": 0
                },
                {
                    "sent": "We already know from the content we can extract just the keyboard, so get this bag of words vector.",
                    "label": 0
                },
                {
                    "sent": "We can also extract named entities or topics using services such and richer and often web pages will also have metadata which is.",
                    "label": 0
                },
                {
                    "sent": "Attach them such as outdoor publishing date and so on, and this is a.",
                    "label": 0
                },
                {
                    "sent": "Major newspapers are quite careful about this because of the IT can or blocks as well, 'cause it good metadata can help their search engine optimization efforts.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. And the first data source was user data, so we have a.",
                    "label": 0
                },
                {
                    "sent": "We were using data set from some major website news publishing website.",
                    "label": 0
                },
                {
                    "sent": "And in that case, 20% of the users were registrated and will see that using machine learning we can quite well generalize from this 20% to some other users from this registration data and they would.",
                    "label": 0
                },
                {
                    "sent": "During registration they would enter either, but they should enter Italy, gender, year of birth, household income and some other categories.",
                    "label": 0
                },
                {
                    "sent": "And one problem here is that it can be quite noisy, so people could just click the first option just to move on.",
                    "label": 0
                },
                {
                    "sent": "With the registration.",
                    "label": 0
                },
                {
                    "sent": "Sample of somebody registering so some Mail from 65 born in 65 with from this zip code.",
                    "label": 0
                },
                {
                    "sent": "United States household.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Income and so on.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now what we want to do so.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using this user data, we want the domain experts be able to define user segments on top of this data and what we mean by user segments.",
                    "label": 0
                },
                {
                    "sent": "It's a segment subset of users.",
                    "label": 0
                },
                {
                    "sent": "Which are sharing some common characteristics and characteristics can be many things.",
                    "label": 0
                },
                {
                    "sent": "It can be gender, so somebody give one segment would be all the male visitors or all the visitors with the age about 40 or coming from Facebook interested in topics, topic, travel, or any combination of these things.",
                    "label": 0
                },
                {
                    "sent": "So when we let the two in order for the domain experts enter this into the system, the.",
                    "label": 0
                },
                {
                    "sent": "Formulated the problem in the form of a faceted search or a search query so the people just use it will just enter these things in a search engine.",
                    "label": 0
                },
                {
                    "sent": "And the results would be the users that correspond to the particular segment.",
                    "label": 0
                },
                {
                    "sent": "And then we would use this retrieved users as a model their segments so to do machine learn.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On top of them.",
                    "label": 0
                },
                {
                    "sent": "And for indexing wind we can from one visit or from 1 user from page we can index many things and these are all the parameters over which will allow people to define their segments.",
                    "label": 0
                },
                {
                    "sent": "So starting from domain sub domain over the URL of the page that was visited or we can filter over meta tags on the page, the title or the content they discovered them entities search terms that people use to find come to the website.",
                    "label": 0
                },
                {
                    "sent": "Or the domain from where they came.",
                    "label": 0
                },
                {
                    "sent": "Also the other location when they visit, so give me all the people that come to our website on Friday evenings from Uganda.",
                    "label": 0
                },
                {
                    "sent": "For example, this could be a query.",
                    "label": 0
                },
                {
                    "sent": "User agent income.",
                    "label": 0
                },
                {
                    "sent": "H and gender.",
                    "label": 0
                },
                {
                    "sent": "So this last three.",
                    "label": 0
                },
                {
                    "sent": "Are extracted from the we not only for registered.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Others.",
                    "label": 0
                },
                {
                    "sent": "So one example.",
                    "label": 0
                },
                {
                    "sent": "So here is some common examples.",
                    "label": 0
                },
                {
                    "sent": "So all the females that are between 100 and hundred $50,000 per year that are interested in fashion.",
                    "label": 0
                },
                {
                    "sent": "Or all the people that are match at least one of these criteria that you rather, CEO, CFO, awesome, or Vice President.",
                    "label": 0
                },
                {
                    "sent": "So give me all the decision makers in the company.",
                    "label": 0
                },
                {
                    "sent": "Or all the people that are interested in reading about Obama, Obama, Healthcare and they came to our page website.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Twitter.",
                    "label": 0
                },
                {
                    "sent": "Now we have a set of users that correspond to this segment.",
                    "label": 0
                },
                {
                    "sent": "We first construct the feature space and feature, so we describe each users with some feature vector and we do that similarly in a similar way.",
                    "label": 0
                },
                {
                    "sent": "Similar way as we did that before from the documents tax documents.",
                    "label": 0
                },
                {
                    "sent": "Only here we have many more fields and we do that just by concatenating the feature vectors together so the user just clicks by this.",
                    "label": 0
                },
                {
                    "sent": "Which parameters should be active for modeling and these authentic into account for the feature vector.",
                    "label": 0
                },
                {
                    "sent": "The training set is either one visit or one user, so depending on what you're modeling, if you're interested in user segments and one example would be 1 user, which is described as a center it as an average vector of all his visits.",
                    "label": 0
                },
                {
                    "sent": "And all the people that match our query are defined as positive class and everybody else that didn't correspond to this query is defined as negative class and we use support vector classification algorithm to technique to learn a model that can predict then user belongs to the class or not.",
                    "label": 0
                },
                {
                    "sent": "And this can be can we can scale this quite easily using stochastic gradient descent implementation as FCM.",
                    "label": 0
                },
                {
                    "sent": "So we can run it on millions of users without.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Much effort.",
                    "label": 0
                },
                {
                    "sent": "OK, so now that we have the segment, we can visualize it by displaying top features from the feature space that are relevant for this segment so.",
                    "label": 1
                },
                {
                    "sent": "I believe we are using content ornamented is we can show the top keywords that make somebody be positively classified in a particular segment or top entities that are relevant.",
                    "label": 0
                },
                {
                    "sent": "That particular segment and this can be served as a useful information for the website editor.",
                    "label": 0
                },
                {
                    "sent": "So if they want to target develop content, target the particular domain.",
                    "label": 0
                },
                {
                    "sent": "And one example is this.",
                    "label": 0
                },
                {
                    "sent": "If you have a females that are more than 100,000 per year, their household income earning 100,000 and are interested in style.",
                    "label": 0
                },
                {
                    "sent": "They would be reading articles with such keyboard children studies.",
                    "label": 0
                },
                {
                    "sent": "I think family health so.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'll be sure couple of experiments.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is 1 example, so using these segments.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Visualizations with what we can do also is we can.",
                    "label": 0
                },
                {
                    "sent": "Use these segments to predict.",
                    "label": 0
                },
                {
                    "sent": "The registration fields for new users, so for example.",
                    "label": 0
                },
                {
                    "sent": "So we have again the same data set, so from one major publishing website we have 5,000,000 users, out of which one million was registered and we are testing how how easy it is to how well can we generalize from this.",
                    "label": 0
                },
                {
                    "sent": "1,000,000, how well can we generalize either gender H or income to the non registered users?",
                    "label": 0
                },
                {
                    "sent": "And we see these are the categories we had.",
                    "label": 0
                },
                {
                    "sent": "So either male female for age we had, we split by decades.",
                    "label": 0
                },
                {
                    "sent": "For earnings which took this classification scale.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we use the feature vectors as we talked before we use.",
                    "label": 0
                },
                {
                    "sent": "Either just text features are just entities that according to documents either just metadata.",
                    "label": 0
                },
                {
                    "sent": "So the category, the authors and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "And then we also tried combining all the features to see.",
                    "label": 0
                },
                {
                    "sent": "If this can.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Help.",
                    "label": 0
                },
                {
                    "sent": "So this is the results for gender.",
                    "label": 0
                },
                {
                    "sent": "So if you have more than 10 visits, so if you take people that did more than 10 visits.",
                    "label": 0
                },
                {
                    "sent": "And we use example.",
                    "label": 0
                },
                {
                    "sent": "Context would be just the time, time and geographical location.",
                    "label": 0
                },
                {
                    "sent": "If you use the text feature, so the content the words in the articles they read after 10 visits you can be.",
                    "label": 1
                },
                {
                    "sent": "You can come to 6065% accuracy with regards to whether somebody is male or female.",
                    "label": 0
                },
                {
                    "sent": "If you take all the metadata you can get almost to 70%.",
                    "label": 0
                },
                {
                    "sent": "You see just after 10 visits.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can see now if you have over 50 visits we can come to almost 90.",
                    "label": 0
                },
                {
                    "sent": "Almost 80% accuracy for predicting somebody is a male.",
                    "label": 0
                },
                {
                    "sent": "Which can be.",
                    "label": 0
                },
                {
                    "sent": "Quite well.",
                    "label": 0
                },
                {
                    "sent": "Quite significant, so this 50% here is.",
                    "label": 0
                },
                {
                    "sent": "This would be random.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now if you look at H, it gets a bit harder, so we have this parameter segmentation.",
                    "label": 0
                },
                {
                    "sent": "So if you take this group till 40, it's quite easy to extract.",
                    "label": 0
                },
                {
                    "sent": "Or if you take the group from 60 on, it's quite easy to extract.",
                    "label": 0
                },
                {
                    "sent": "But people between 30 and 60 are quite hard to distinguish and at least checking, but for the important features it was that here.",
                    "label": 0
                },
                {
                    "sent": "So this young people are quite specific and mostly retired people are also quite specific in and reading habits, whereas here so people that are have a.",
                    "label": 0
                },
                {
                    "sent": "There's still work that are working they their interests with more correspond to their job profession versus their H. So that's why the reading habits are not so distinctive between here.",
                    "label": 0
                },
                {
                    "sent": "So, but here again, the baseline is 20%, which is a random and you can see that here we can get quite high for after 50 visits for.",
                    "label": 0
                },
                {
                    "sent": "Older young people.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now that's again split by the features we see that name entities are very important for detecting or.",
                    "label": 0
                },
                {
                    "sent": "The most useful thing for detecting the H so.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which means here young people would read about different entities than old people or than this.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "During work.",
                    "label": 0
                },
                {
                    "sent": "And for the income, it's a bit surprising.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "See people with high incomes are quite here.",
                    "label": 0
                },
                {
                    "sent": "The baseline was very low, so 14% would be random.",
                    "label": 0
                },
                {
                    "sent": "And people with a high income or prices can be.",
                    "label": 0
                },
                {
                    "sent": "Twice as good as random for them, but for everybody else it's very hard to be better than random.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now again to the system over user.",
                    "label": 0
                },
                {
                    "sent": "How would you use this?",
                    "label": 0
                },
                {
                    "sent": "Can use this for example for advertisers or for analysts trying to target.",
                    "label": 0
                },
                {
                    "sent": "Did they want to target users target the non registered users as well so?",
                    "label": 0
                },
                {
                    "sent": "Is a really so before if you have 80% of people for whom you don't know the gender, then being at least 80% sure for somebody is already better than not knowing that all.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another example is new.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recommendation, so here the task is at the moment of the visit, so this is again real time.",
                    "label": 0
                },
                {
                    "sent": "So you come to the website you're visiting.",
                    "label": 0
                },
                {
                    "sent": "Particular article or page, and we want to display the recommendations so the top 4 pages that user would like to click and we formulated this as a task.",
                    "label": 0
                },
                {
                    "sent": "So we want to predict the category of the article that the user is most interested in and then we display the articles from that.",
                    "label": 0
                },
                {
                    "sent": "How we do that?",
                    "label": 0
                },
                {
                    "sent": "So we have a.",
                    "label": 0
                },
                {
                    "sent": "We take the history of the article of the user, so everything he had in the past plus the current article.",
                    "label": 0
                },
                {
                    "sent": "And from this we extract feature features such as history.",
                    "label": 0
                },
                {
                    "sent": "So this would be all the articles that he ever read Geo.",
                    "label": 0
                },
                {
                    "sent": "So from here he comes.",
                    "label": 0
                },
                {
                    "sent": "So what he's reading at the moment, where is he coming at the moment?",
                    "label": 0
                },
                {
                    "sent": "And what time?",
                    "label": 0
                },
                {
                    "sent": "So it's Monday morning.",
                    "label": 0
                },
                {
                    "sent": "Maybe you would read different topics in Friday evening.",
                    "label": 0
                },
                {
                    "sent": "And then we check during training procedure we check what the user actually read.",
                    "label": 0
                },
                {
                    "sent": "And take the category from this.",
                    "label": 0
                },
                {
                    "sent": "And now we do this.",
                    "label": 0
                },
                {
                    "sent": "We can generate the training training data for learning a classification model.",
                    "label": 0
                },
                {
                    "sent": "So we can classify the user into categories.",
                    "label": 0
                },
                {
                    "sent": "Now if you are running actually running life, we don't know the next article.",
                    "label": 0
                },
                {
                    "sent": "So we use this same feature vector to classify the user into one of the categories style and soil and select article from these categories as the recommendations.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can do this develop this running life we can see.",
                    "label": 0
                },
                {
                    "sent": "For example, you can log the visit.",
                    "label": 0
                },
                {
                    "sent": "When the user comes, it's independent of.",
                    "label": 0
                },
                {
                    "sent": "The visit is locked, stored for heaven sent to the recommendation in the engine to update the index so it takes less than a.",
                    "label": 0
                },
                {
                    "sent": "It's a couple of seconds from the click for the index update and independently the call is made to the recommendation and engine and then back for the.",
                    "label": 0
                },
                {
                    "sent": "To the website.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is about the replica mining.",
                    "label": 0
                },
                {
                    "sent": "Cannot one more short example where again we have a stream of network on that.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vacation network.",
                    "label": 0
                },
                {
                    "sent": "Here the data is.",
                    "label": 0
                },
                {
                    "sent": "We have a large telecom with more than $25,000 devices and we're getting in alarms.",
                    "label": 0
                },
                {
                    "sent": "So alarms would mean the operator installed some agent on each of these 25,000 devices which monitors the log files and whenever a\nAppears in the log file, it's forwarded to some central location and this goes from 10 to hundreds of alarms, events per second, and typically it's not uniform, because when something goes wrong there would be many more events happening per second.",
                    "label": 0
                },
                {
                    "sent": "And what we can do so we developed a system that can take this data and do a couple of scenarios on top of that.",
                    "label": 0
                },
                {
                    "sent": "Money is root cause analysis so.",
                    "label": 0
                },
                {
                    "sent": "Simulating based on historical data to see Co occurrence if for example in the past you saw that once the backup server is not responding then each device that is using that backup to store their daily backups on it will start complaining and next time this same event happens we can already tell after seeing.",
                    "label": 0
                },
                {
                    "sent": "After seeing a couple of these devices we can say OK, this is probably the root cause.",
                    "label": 0
                },
                {
                    "sent": "Last time in a similar scenario the backup server is the problem, so we can match back to the existing old cases.",
                    "label": 0
                },
                {
                    "sent": "Another thing is we can do this short term prediction in the in the future.",
                    "label": 0
                },
                {
                    "sent": "So we can say OK Now this.",
                    "label": 0
                },
                {
                    "sent": "This device at the backup server is not available, but we know that usually all these 20 device is also complain at the same time, so these are most likely to also issue events in the next short period and long term protection anomaly detection, so this is long-term detecting long-term trends on the network, so if there is usually the operators that are handling these alarms are too focused on solving these short-term fixes that they don't detect this slow long term.",
                    "label": 0
                },
                {
                    "sent": "Increases, for example in number of faults in a particular system, but.",
                    "label": 0
                },
                {
                    "sent": "You can detect these things quite easily.",
                    "label": 0
                },
                {
                    "sent": "And this is this system is.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Diploid as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's also just to conclude, so we had a.",
                    "label": 0
                },
                {
                    "sent": "We showed a couple of systems, a couple of studies working on different modalities of stream, so we had a most of the time was spent on this document streams.",
                    "label": 0
                },
                {
                    "sent": "After which was extracted this social network.",
                    "label": 0
                },
                {
                    "sent": "So this whole code, which entity code, which entity?",
                    "label": 0
                },
                {
                    "sent": "And in the second half we focused on this click stream and at the end also event streams in the network.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Show a couple of application on top of it.",
                    "label": 0
                },
                {
                    "sent": "So what's the?",
                    "label": 0
                },
                {
                    "sent": "Taking her so she would extend these things to other modalities or to other similar systems and so on.",
                    "label": 0
                },
                {
                    "sent": "What are the main channel challenges?",
                    "label": 0
                },
                {
                    "sent": "Some things that this setup so they are still much work on a large scale.",
                    "label": 0
                },
                {
                    "sent": "It's not very commoditized at the moment, so it still require lots of customization fixings and so on to be able to handle different modalities.",
                    "label": 0
                },
                {
                    "sent": "So once we can do it, documents or moving to social networks requires quite some changes even though the many.",
                    "label": 0
                },
                {
                    "sent": "Distributions and so on are shared between them.",
                    "label": 0
                },
                {
                    "sent": "Another challenge is dealing with unstructured data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How to extract some kind of highly structured events from use example we one way of dealing with this is by using entities as a proxy for what are we tracking in the stream so, but if you would want to go on a bit lower level than this, things becomes much harder.",
                    "label": 0
                },
                {
                    "sent": "Another thing is also on extracting semantic stream.",
                    "label": 0
                },
                {
                    "sent": "So if you have a for example a sensor network with thousands or 10s of thousands of sensors.",
                    "label": 0
                },
                {
                    "sent": "So extracting some.",
                    "label": 0
                },
                {
                    "sent": "From this lower level, events which are measured on the level of a particular sensor.",
                    "label": 0
                },
                {
                    "sent": "So trying to aggregate this to some bigger events.",
                    "label": 0
                },
                {
                    "sent": "This so this complex event processing is one way of trying to tackle these things, but there are still probably some things that can be done once we start talking how automatically identifying related sensors.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Think about.",
                    "label": 0
                }
            ]
        }
    }
}