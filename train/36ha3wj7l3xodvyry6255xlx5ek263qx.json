{
    "id": "36ha3wj7l3xodvyry6255xlx5ek263qx",
    "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning",
    "info": {
        "author": [
            "Stephane Ross, Robotics Institute, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/aistats2011_ross_reduction/",
    "segmentation": [
        [
            "So Steven, I'm going to talk about the novel Imitation learning approach that's related to no regret online learning, and that's joint work with my advisor, Drew Bagnell and Jeff Gordon.",
            "OK."
        ],
        [
            "So in imitation learning, what we're basically trying to do is make a robot learn how to perform some task by watching an expert doing it, so might have a mobile robot like this and would like it to be able to traverse some terrain and reach some goal location.",
            "So we might have an expert you can still operate the robot around and so out to navigate across the terrain efficiently, and So what we'd like to be able to do is apply some kind of machine learning algorithm that can take these expert demonstrations as input.",
            "And output the policy that we can just run on the robot and the robot will be able to achieve that task by itself.",
            "So there's been many."
        ],
        [
            "Successful applications of imitation learning.",
            "In recent years, these are some examples, and so the sort of main advantage of imitation learning over other, more traditional control methods is here.",
            "You don't have to provide any sort of complete or accurate model of the world, and you don't have to be able to end too, and all the parameters of your cost function so that you get the proper behavior out of a planning algorithm.",
            "For instance, all they need to be able to do is."
        ],
        [
            "Demonstrate how to perform the task, and that's often much easier for humans to do, especially on those complex robotic systems.",
            "So now let's look at the particular example and see how we can learn a policy for robot.",
            "From these expert demonstrations.",
            "So let's look at the following scenario.",
            "Let's say I have a car and I want it to be able to drive itself on the road and my car has a camera sensor so it can see what's in front of the vehicle, and So what I'd like to do is learn some policy that can take these input camera images and predict what kind of steering it should do in the environment to stay near the center of the road.",
            "There's sort of like a very trivial answer."
        ],
        [
            "Full training procedure you could think of to do this and what you could do is just have the human drive the car around the environment and as the human is driving you record these images that are being seen by the camera and also the steering being executed by the expert.",
            "And So what you do is you build this big data set of input images that you see with their corresponding associated steering.",
            "And so now you could just treat that as a standard supervised learning problem.",
            "So apply your favorite classification or regression algorithm and you will get some policy out of it that can take any input image and predict some kind of steering angle.",
            "So this sounds like a reasonable approach."
        ],
        [
            "Notes right?",
            "Well, turns out that if you do that in practice, you'll often get very bad performance at the task, so I'm going to show you a video that illustrates what goes wrong with this approach.",
            "So hear what you're saying is me playing this racing game, collecting training data to learn a controller that can steer the cart in this game.",
            "What you can see is when I'm driving, I'm driving mostly near the center of the road, so all the examples that the learning algorithm sees is what to do near the center of the road.",
            "But now when you run the learn policy.",
            "What happens is it's not perfect.",
            "It still makes some mistakes.",
            "So what happens is it ends up in those new situations near the side of the road where it doesn't have any training data, so it doesn't know what to recover when it's in those bad situations, so just makes more mistakes and falls off the road.",
            "So what we can see in this video is there is this key distinction between supervised learning an imitation learning.",
            "In supervised learning.",
            "We assume that our test data it comes ID from the same distribution as our training data, but that's not the case here.",
            "It what happens is the predictions that you do at Test time changes the future observations or states that you're going to see in the future.",
            "And we tested that.",
            "So the test data is not ID and depends on the actual policy that you run at Test time.",
            "So because of this, you can actually show that in theory."
        ],
        [
            "This supervised learning approach has pretty poor performance guarantees, so in particular, if you learn some policy that makes mistake with some small property epsilon under the states that are visited by the expert.",
            "During training.",
            "And you run that policy 40 time steps at execution time, then under its own trajectories it could be making as much as T squared epsilon mistakes and expectation.",
            "So that's pretty bad, because if you compare this to the standard ID setting, you expect to make T epsilon mistakes.",
            "If you have to classify T new test examples.",
            "And that's also both an upper bound and lower bound, because we can construct examples where this holds with equality.",
            "So sort of natural question you can ask is well can we develop some new learning algorithms that can provide better guarantees in this in this imitation learning setting?",
            "It's a result we can if we allow the learning algorithm to sort of do his own actions during training and collect more data about the expert behavior about what to do after you make those mistakes in the environment.",
            "So that's the kind of strategies will be employed by the learning approaches.",
            "I'm going to show you.",
            "And the way we're going to."
        ],
        [
            "Arise these approaches is doing this kind of reduction based analysis, so the idea of a reduction is you can take some are learning problem and you reduce it to solving a sequence of simpler or easier related problems.",
            "So an example that you might be familiar with is reduction of multiclass classification to binary classification.",
            "So in that case, what you do is you solve these sequence of binary classification tasks, from which you can construct a multiclass classifier.",
            "And then what you can also do is bound the performance on the original multiclass classification problem as a function of how well you can do at those binary classification tasks.",
            "So we'll do a similar idea here.",
            "Will will reduce our art imitation learning problem to solving a sequence of classification problems, and we'll be able to bound our.",
            "Well, we can do at the original imitation learning problem as a function of how well we can do at those at this sequence of classification problems.",
            "So I'm going to briefly mention two previous approaches we've proposed here last year that can guarantee better performance guarantee."
        ],
        [
            "Then this supervised learning approach I showed you earlier, but so this approach are nice in theory, but they have some drawbacks.",
            "In practice that our new approach will be able to deal with.",
            "So first one is called afford training and the idea here is, instead of learning a single fixed policy, you're going to learn a sequence of policy.",
            "So you train a non stationary policy and the way you train that non stationary policy is you learn one policy for each step in sequence by starting at learning the first policy for the first step.",
            "So what you do is you just collect data of the 1st place you see in the environment.",
            "What the expert does at those States and you train a force policy on that data and then to train your second policy.",
            "Now what you do is you simulate your first policy.",
            "You learn for the first step to see what kind of states you end up at the second step, and then you ask the expert what to do there and you train your second policy on that data and you just keep going on like this.",
            "So train one policy for iteration of the algorithm for one step.",
            "And So what you can show about this approach is now when you're going to run that sequence of policies that you've learned at Test time, your expected number of mistakes is linear in T and scales with epsilon, the average error rate of the classifiers that you've learned.",
            "So that's really nice, because now we achieve the same bound that we could guarantee in the ID setting, even though this is like an an ID setting.",
            "So it's pretty good, but the sort of drawback of this approach is it's impractical when T is large or unknown.",
            "So for many tasks we might not know in advance how long we're going to have to run our policy, so we don't know how many policies we have to learn, or if I have to run my policy 4 million iterations and F to run this algorithm, 4 million iterations and learn a million policy's just to get my complete policy to run at testing."
        ],
        [
            "So often we can't do it this approach.",
            "So sort of addressed this problem.",
            "We also proposed another approach last year which was called Smile, and that's the similar approach to CERN, or conservative policy iteration.",
            "If you're familiar with that.",
            "The idea here is we train a stationary stochastic policy over several iterations by making small changes to the policy at each iteration.",
            "And then with this approach work is you essentially use your current policy at each iteration.",
            "What you do is you use your current policy to collect more data about the expert in those states that you encounter by running that trend policy.",
            "And then you find the best possible mimicking the expert in that data, and then you add some small probability of running that new policy to update your current policy.",
            "And you just do that over many iterations.",
            "If you do that for long enough and make that parameter Alpha small enough.",
            "Then you can also guarantee good performance guarantees with this approach.",
            "What was bad about this is you get this stochastic policy out of it, and often when you want to run this on the like on a robot in the real world, you don't want to run this stochastic policy.",
            "Don't want to start picking some bad actions some of the time.",
            "So that's what our new algorithm will allow us to do.",
            "Will be able to learn a deterministic stationary policy that's much better in practice.",
            "So now I'm going to."
        ],
        [
            "We present our new approach with this called Dagger for data set aggregation.",
            "And it's very simple to implement.",
            "An will be able to provide this sort of same guarantees as the in the ID setting, so errors are."
        ],
        [
            "Works at the first iteration you do."
        ],
        [
            "Exactly the same thing as the supervised learning approach I showed you earlier, so you just have the expert drive the car and the environment color."
        ],
        [
            "Data while the expert is driving.",
            "And then you'll get the first data set.",
            "That you train your first policy on, but now what you do is you iterate so the second iteration you're going to run that first policy that you learn.",
            "So now it's like the computer is driving the car.",
            "And as the computer is driving the car, you still ask the expert what it would do in those states or observations that you see.",
            "So it's like the computer is driving and the expert says a turn more to the right.",
            "No turn watching right?",
            "No really, you should really."
        ],
        [
            "Turn to the right and so you get this new data set of."
        ],
        [
            "Examples an hour."
        ],
        [
            "Do is you aggregate that new data set with your previous data set that you collected for the first iteration?"
        ],
        [
            "And you train your next policy on that giant data set of all the data is collected so far.",
            "And so the algorithm just keep going on like this.",
            "So the next iteration user current."
        ],
        [
            "Else run it in the environment to collect more data about the experts behavior.",
            "You add that data to your giant data set of all the data."
        ],
        [
            "So far, entering your next policy and so on.",
            "So you do that for some large number of iterations, and at the end you could just return the policy that did best at mimicking the expert while you were running it in the environment, so under its own trajectories.",
            "So now you might be wondering how is that related to online learning?",
            "Because that doesn't seem apparent at all from this, so that's what I'm going to show you next and this relation to online learning is what will allow us to provide some good guarantees for this."
        ],
        [
            "Roads.",
            "So, and I'm going to sort of do a quick introduction to online learning.",
            "For those of you might not be familiar with this, and so you can think of online learning as essentially a game between two players.",
            "On one side you have a learner, and on the other side you have an evil adversary that tries to make the learner look bad.",
            "The way this game is."
        ],
        [
            "Wade is at each iteration you have the learners.",
            "They pick a classifier based on its current data."
        ],
        [
            "And then the adversary can observe that classifier and."
        ],
        [
            "Test instance where that classifier will."
        ],
        [
            "We evaluated and that defines some loss function over a space of classifiers.",
            "That's being picked by the adversary.",
            "And as the learner ensured that loss, then he observes the true label of that instance and you can update is hypothesis based on that new data.",
            "For the next round.",
            "So the game just keep playing on like this.",
            "So what we're interested in online learning is minimize this notion of regret."
        ],
        [
            "But as the other talks talk previously, then this notion of regret is basically just the difference between the some of the losses that you incur by playing this sequence of classifiers to classify your test instances that are picked by the adversary versus how well you could do if you add known all these tests examples in advance and pick the best classifier in your class.",
            "So the average regret is just this regret divided by the number of rounds an.",
            "And then no regret algorithm is basically an algorithm that can guarantee that this goes to 0 as a number of iterations goes to Infinity.",
            "So how does that relate to Dagger?"
        ],
        [
            "Well, you can essentially think of each round of online learning or of dagger as a round of online learning.",
            "So here a classifier corresponds to a policy.",
            "An you can if you remember when we were running dagger at each iteration we're collecting a new small data set of of examples by running the current policy.",
            "So you can think of these new examples that you collect as the test instances that are picked by the adversary that defines the law."
        ],
        [
            "Function that that iteration.",
            "So and with Dagger was doing with that new data is it was aggregating it with this data aggregate data set of all the data I've collected so far and then picking the new best policy on that entire data set?",
            "So what this corresponds to air is just picking the policy that minimizes this.",
            "Some of the losses so far.",
            "So for those of you familiar with online learning, you'll recognize that this is just a folder leader type approach.",
            "So you can essentially think of Dagger as running this folder leader online learning algorithm on this."
        ],
        [
            "Your sequence of loss functions and we will be able to say is if this folder leader algorithm is essentially no regret, will be able to provide some good guarantees for the policy that we learn with Dagger."
        ],
        [
            "So in particular, what we can do is relate the performance of the policy, learn with dagger at Test time to two quantities that are related to how well we can do on that online learning problem.",
            "So one of those quantity is the average regret I was talking about.",
            "The other one is how well you can do at classifying that aggregate data set, which are class of policies."
        ],
        [
            "So we notice for follow the leader.",
            "It's actually a no regret algorithm for strongly convex loss functions.",
            "So if your classification last include some type of regularization will usually be a strongly convex loss function.",
            "And what that means is all these other terms will vanish to zero as you iterate dagger for long enough, and so you'll be able to guarantee that you'll learn some policy.",
            "That basically have a number of mistakes at Test time that's linear in T and scales with our we can do at classifying that aggregate data set."
        ],
        [
            "And the other important point here that we made with this analysis is that basically these results just follow from the no regret properties of follow the leader.",
            "So what that means is you could run dagger with any other online learning algorithm to pick this sequence of policy's, and as long as it's no regret will be able to guarantee a similar bound like this.",
            "So you could imagine like a version of Dagger that does like online gradient descent on the same sequence of loss functions that we've defined and will be able to have the same guarantees.",
            "So the key part of the reduction is how we define those loss functions, and as long as we do any kind of no regret algorithm on this will have good performance guarantees."
        ],
        [
            "So the bounds I just showed you was for this sort of infinite sample case, where you would sample an infinite number of trajectories at each iteration of dagger.",
            "So we can also provide high probability bounds when you just sample a finite number of trajectories.",
            "And in particular, to just sample one trajectory per iteration.",
            "An run dagger for order T squared iteration.",
            "Then you get a similar bound at all with high probability.",
            "OK, so now."
        ],
        [
            "I want to show you some cool experiments that we did with Dagger.",
            "First one is this racing game I should be at the beginning, so remember we're trying to learn is some policy that can map these images to the steering issued.",
            "Execute to stay near the center of the road."
        ],
        [
            "So I'm going to show you a video of our dagger did after 20 iterations of training.",
            "And basically each iteration of training was one more lap of training data that we were collecting that corresponds to about 1000 time steps each.",
            "So it's about like 1000 training samples that you add to your data set at each iteration, and all we're learning is a linear mapping from the image features which were just color values in the resized image to this steering.",
            "So we're basically doing linear regression.",
            "And So what you can see is with Dagger, we can perform actually really well at that task, we learn some policy that never falls off the track.",
            "Over an entire."
        ],
        [
            "But but if we compare that to other approaches, what we can see is the smile, both smile and the supervised learning approach didn't do that well.",
            "So particular with supervised learning, the performance wasn't even improving as we were collecting more and more data with the experts policy."
        ],
        [
            "So let's experiment is on Super Mario Bros.",
            "So here we're trying to make the computer learn how to play this game by watching a planning algorithm that had full access to the internal game set of the game and could basically beat any stage of the game.",
            "And here we're using a simulator from this module AI competition in 2009 that could generate random stages.",
            "So I'm assuming most of you are familiar with that game, but if you're not, the goal is basically to move the character Mario through the stage by avoiding hitting enemies and falling into holes, and then reach the end of this stage before running out of time.",
            "And you can do this for actions, jump right, left, or use the speed button, or any combination of those at the same time.",
            "And here the features we're using is just a giant binary vector that describes the objects that were around Mario."
        ],
        [
            "And so now I'm going to show you another video of the performance with different approaches.",
            "So first here this is the expert playing, so it's the planning algorithm who's playing the game.",
            "As you can see, it's pretty good, can basically beat any stage of the game, and now this is the supervised learning approach and what you can see is that it's not doing very well.",
            "It gets stuck in the level pretty often, or also often falls into holes.",
            "As you can see there.",
            "Now this is the performance with Dagger, same total number of data over 20 iterations and you can see it's doing much better.",
            "It actually completes an entire stage pretty often.",
            "As you can see here.",
            "But it's also not perfect, so still it's enemies quite often."
        ],
        [
            "But so if we compare the average distance travelled by Mario over over like this random distribution of stages that we can generate.",
            "Where we can see that dagger does much better than all the other approaches?",
            "So error 3000 corresponds to about 70% of a stage on average.",
            "That's being completed.",
            "And so we compared here with smile CERN and also supervised learning.",
            "And each algorithm was run for 20 iterations that we're collecting 5000 training samples per iteration.",
            "So in conclusion."
        ],
        [
            "Well, the sort of take home message of this talk is you can do these simple iterative procedures to obtain much better performance, practice and theory on these kind of non ID supervised learning problem like imitation learning.",
            "Is something I didn't have time to talk about is that you can actually apply this to do structured prediction.",
            "So one way you can do this is by doing in their process similar to CERN where you transform your structured prediction problem into like a sequential prediction problem.",
            "That's sort of a general form of imitation learning.",
            "And then you can apply dagger to learn the classifier.",
            "That can essentially construct the structured output by making this sequence of predictions.",
            "And so we have actually structured prediction application in the paper on in writing recognition, and we also have a new paper coming up in the Computer Vision conference doing as seen labeling applique."
        ],
        [
            "Oceans.",
            "So I'll just stop here.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Steven, I'm going to talk about the novel Imitation learning approach that's related to no regret online learning, and that's joint work with my advisor, Drew Bagnell and Jeff Gordon.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in imitation learning, what we're basically trying to do is make a robot learn how to perform some task by watching an expert doing it, so might have a mobile robot like this and would like it to be able to traverse some terrain and reach some goal location.",
                    "label": 0
                },
                {
                    "sent": "So we might have an expert you can still operate the robot around and so out to navigate across the terrain efficiently, and So what we'd like to be able to do is apply some kind of machine learning algorithm that can take these expert demonstrations as input.",
                    "label": 1
                },
                {
                    "sent": "And output the policy that we can just run on the robot and the robot will be able to achieve that task by itself.",
                    "label": 0
                },
                {
                    "sent": "So there's been many.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Successful applications of imitation learning.",
                    "label": 1
                },
                {
                    "sent": "In recent years, these are some examples, and so the sort of main advantage of imitation learning over other, more traditional control methods is here.",
                    "label": 0
                },
                {
                    "sent": "You don't have to provide any sort of complete or accurate model of the world, and you don't have to be able to end too, and all the parameters of your cost function so that you get the proper behavior out of a planning algorithm.",
                    "label": 0
                },
                {
                    "sent": "For instance, all they need to be able to do is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Demonstrate how to perform the task, and that's often much easier for humans to do, especially on those complex robotic systems.",
                    "label": 0
                },
                {
                    "sent": "So now let's look at the particular example and see how we can learn a policy for robot.",
                    "label": 0
                },
                {
                    "sent": "From these expert demonstrations.",
                    "label": 0
                },
                {
                    "sent": "So let's look at the following scenario.",
                    "label": 0
                },
                {
                    "sent": "Let's say I have a car and I want it to be able to drive itself on the road and my car has a camera sensor so it can see what's in front of the vehicle, and So what I'd like to do is learn some policy that can take these input camera images and predict what kind of steering it should do in the environment to stay near the center of the road.",
                    "label": 0
                },
                {
                    "sent": "There's sort of like a very trivial answer.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Full training procedure you could think of to do this and what you could do is just have the human drive the car around the environment and as the human is driving you record these images that are being seen by the camera and also the steering being executed by the expert.",
                    "label": 0
                },
                {
                    "sent": "And So what you do is you build this big data set of input images that you see with their corresponding associated steering.",
                    "label": 0
                },
                {
                    "sent": "And so now you could just treat that as a standard supervised learning problem.",
                    "label": 0
                },
                {
                    "sent": "So apply your favorite classification or regression algorithm and you will get some policy out of it that can take any input image and predict some kind of steering angle.",
                    "label": 0
                },
                {
                    "sent": "So this sounds like a reasonable approach.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Notes right?",
                    "label": 0
                },
                {
                    "sent": "Well, turns out that if you do that in practice, you'll often get very bad performance at the task, so I'm going to show you a video that illustrates what goes wrong with this approach.",
                    "label": 1
                },
                {
                    "sent": "So hear what you're saying is me playing this racing game, collecting training data to learn a controller that can steer the cart in this game.",
                    "label": 0
                },
                {
                    "sent": "What you can see is when I'm driving, I'm driving mostly near the center of the road, so all the examples that the learning algorithm sees is what to do near the center of the road.",
                    "label": 0
                },
                {
                    "sent": "But now when you run the learn policy.",
                    "label": 0
                },
                {
                    "sent": "What happens is it's not perfect.",
                    "label": 0
                },
                {
                    "sent": "It still makes some mistakes.",
                    "label": 0
                },
                {
                    "sent": "So what happens is it ends up in those new situations near the side of the road where it doesn't have any training data, so it doesn't know what to recover when it's in those bad situations, so just makes more mistakes and falls off the road.",
                    "label": 0
                },
                {
                    "sent": "So what we can see in this video is there is this key distinction between supervised learning an imitation learning.",
                    "label": 0
                },
                {
                    "sent": "In supervised learning.",
                    "label": 0
                },
                {
                    "sent": "We assume that our test data it comes ID from the same distribution as our training data, but that's not the case here.",
                    "label": 0
                },
                {
                    "sent": "It what happens is the predictions that you do at Test time changes the future observations or states that you're going to see in the future.",
                    "label": 0
                },
                {
                    "sent": "And we tested that.",
                    "label": 0
                },
                {
                    "sent": "So the test data is not ID and depends on the actual policy that you run at Test time.",
                    "label": 0
                },
                {
                    "sent": "So because of this, you can actually show that in theory.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This supervised learning approach has pretty poor performance guarantees, so in particular, if you learn some policy that makes mistake with some small property epsilon under the states that are visited by the expert.",
                    "label": 0
                },
                {
                    "sent": "During training.",
                    "label": 0
                },
                {
                    "sent": "And you run that policy 40 time steps at execution time, then under its own trajectories it could be making as much as T squared epsilon mistakes and expectation.",
                    "label": 0
                },
                {
                    "sent": "So that's pretty bad, because if you compare this to the standard ID setting, you expect to make T epsilon mistakes.",
                    "label": 0
                },
                {
                    "sent": "If you have to classify T new test examples.",
                    "label": 0
                },
                {
                    "sent": "And that's also both an upper bound and lower bound, because we can construct examples where this holds with equality.",
                    "label": 0
                },
                {
                    "sent": "So sort of natural question you can ask is well can we develop some new learning algorithms that can provide better guarantees in this in this imitation learning setting?",
                    "label": 0
                },
                {
                    "sent": "It's a result we can if we allow the learning algorithm to sort of do his own actions during training and collect more data about the expert behavior about what to do after you make those mistakes in the environment.",
                    "label": 0
                },
                {
                    "sent": "So that's the kind of strategies will be employed by the learning approaches.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you.",
                    "label": 0
                },
                {
                    "sent": "And the way we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Arise these approaches is doing this kind of reduction based analysis, so the idea of a reduction is you can take some are learning problem and you reduce it to solving a sequence of simpler or easier related problems.",
                    "label": 0
                },
                {
                    "sent": "So an example that you might be familiar with is reduction of multiclass classification to binary classification.",
                    "label": 1
                },
                {
                    "sent": "So in that case, what you do is you solve these sequence of binary classification tasks, from which you can construct a multiclass classifier.",
                    "label": 0
                },
                {
                    "sent": "And then what you can also do is bound the performance on the original multiclass classification problem as a function of how well you can do at those binary classification tasks.",
                    "label": 0
                },
                {
                    "sent": "So we'll do a similar idea here.",
                    "label": 0
                },
                {
                    "sent": "Will will reduce our art imitation learning problem to solving a sequence of classification problems, and we'll be able to bound our.",
                    "label": 0
                },
                {
                    "sent": "Well, we can do at the original imitation learning problem as a function of how well we can do at those at this sequence of classification problems.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to briefly mention two previous approaches we've proposed here last year that can guarantee better performance guarantee.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then this supervised learning approach I showed you earlier, but so this approach are nice in theory, but they have some drawbacks.",
                    "label": 0
                },
                {
                    "sent": "In practice that our new approach will be able to deal with.",
                    "label": 0
                },
                {
                    "sent": "So first one is called afford training and the idea here is, instead of learning a single fixed policy, you're going to learn a sequence of policy.",
                    "label": 0
                },
                {
                    "sent": "So you train a non stationary policy and the way you train that non stationary policy is you learn one policy for each step in sequence by starting at learning the first policy for the first step.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you just collect data of the 1st place you see in the environment.",
                    "label": 0
                },
                {
                    "sent": "What the expert does at those States and you train a force policy on that data and then to train your second policy.",
                    "label": 0
                },
                {
                    "sent": "Now what you do is you simulate your first policy.",
                    "label": 0
                },
                {
                    "sent": "You learn for the first step to see what kind of states you end up at the second step, and then you ask the expert what to do there and you train your second policy on that data and you just keep going on like this.",
                    "label": 0
                },
                {
                    "sent": "So train one policy for iteration of the algorithm for one step.",
                    "label": 0
                },
                {
                    "sent": "And So what you can show about this approach is now when you're going to run that sequence of policies that you've learned at Test time, your expected number of mistakes is linear in T and scales with epsilon, the average error rate of the classifiers that you've learned.",
                    "label": 0
                },
                {
                    "sent": "So that's really nice, because now we achieve the same bound that we could guarantee in the ID setting, even though this is like an an ID setting.",
                    "label": 0
                },
                {
                    "sent": "So it's pretty good, but the sort of drawback of this approach is it's impractical when T is large or unknown.",
                    "label": 0
                },
                {
                    "sent": "So for many tasks we might not know in advance how long we're going to have to run our policy, so we don't know how many policies we have to learn, or if I have to run my policy 4 million iterations and F to run this algorithm, 4 million iterations and learn a million policy's just to get my complete policy to run at testing.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So often we can't do it this approach.",
                    "label": 0
                },
                {
                    "sent": "So sort of addressed this problem.",
                    "label": 0
                },
                {
                    "sent": "We also proposed another approach last year which was called Smile, and that's the similar approach to CERN, or conservative policy iteration.",
                    "label": 0
                },
                {
                    "sent": "If you're familiar with that.",
                    "label": 0
                },
                {
                    "sent": "The idea here is we train a stationary stochastic policy over several iterations by making small changes to the policy at each iteration.",
                    "label": 0
                },
                {
                    "sent": "And then with this approach work is you essentially use your current policy at each iteration.",
                    "label": 0
                },
                {
                    "sent": "What you do is you use your current policy to collect more data about the expert in those states that you encounter by running that trend policy.",
                    "label": 0
                },
                {
                    "sent": "And then you find the best possible mimicking the expert in that data, and then you add some small probability of running that new policy to update your current policy.",
                    "label": 0
                },
                {
                    "sent": "And you just do that over many iterations.",
                    "label": 0
                },
                {
                    "sent": "If you do that for long enough and make that parameter Alpha small enough.",
                    "label": 0
                },
                {
                    "sent": "Then you can also guarantee good performance guarantees with this approach.",
                    "label": 0
                },
                {
                    "sent": "What was bad about this is you get this stochastic policy out of it, and often when you want to run this on the like on a robot in the real world, you don't want to run this stochastic policy.",
                    "label": 0
                },
                {
                    "sent": "Don't want to start picking some bad actions some of the time.",
                    "label": 0
                },
                {
                    "sent": "So that's what our new algorithm will allow us to do.",
                    "label": 0
                },
                {
                    "sent": "Will be able to learn a deterministic stationary policy that's much better in practice.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We present our new approach with this called Dagger for data set aggregation.",
                    "label": 0
                },
                {
                    "sent": "And it's very simple to implement.",
                    "label": 0
                },
                {
                    "sent": "An will be able to provide this sort of same guarantees as the in the ID setting, so errors are.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Works at the first iteration you do.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exactly the same thing as the supervised learning approach I showed you earlier, so you just have the expert drive the car and the environment color.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data while the expert is driving.",
                    "label": 0
                },
                {
                    "sent": "And then you'll get the first data set.",
                    "label": 0
                },
                {
                    "sent": "That you train your first policy on, but now what you do is you iterate so the second iteration you're going to run that first policy that you learn.",
                    "label": 0
                },
                {
                    "sent": "So now it's like the computer is driving the car.",
                    "label": 0
                },
                {
                    "sent": "And as the computer is driving the car, you still ask the expert what it would do in those states or observations that you see.",
                    "label": 0
                },
                {
                    "sent": "So it's like the computer is driving and the expert says a turn more to the right.",
                    "label": 0
                },
                {
                    "sent": "No turn watching right?",
                    "label": 0
                },
                {
                    "sent": "No really, you should really.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turn to the right and so you get this new data set of.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples an hour.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do is you aggregate that new data set with your previous data set that you collected for the first iteration?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you train your next policy on that giant data set of all the data is collected so far.",
                    "label": 0
                },
                {
                    "sent": "And so the algorithm just keep going on like this.",
                    "label": 0
                },
                {
                    "sent": "So the next iteration user current.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Else run it in the environment to collect more data about the experts behavior.",
                    "label": 0
                },
                {
                    "sent": "You add that data to your giant data set of all the data.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So far, entering your next policy and so on.",
                    "label": 0
                },
                {
                    "sent": "So you do that for some large number of iterations, and at the end you could just return the policy that did best at mimicking the expert while you were running it in the environment, so under its own trajectories.",
                    "label": 0
                },
                {
                    "sent": "So now you might be wondering how is that related to online learning?",
                    "label": 0
                },
                {
                    "sent": "Because that doesn't seem apparent at all from this, so that's what I'm going to show you next and this relation to online learning is what will allow us to provide some good guarantees for this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Roads.",
                    "label": 0
                },
                {
                    "sent": "So, and I'm going to sort of do a quick introduction to online learning.",
                    "label": 1
                },
                {
                    "sent": "For those of you might not be familiar with this, and so you can think of online learning as essentially a game between two players.",
                    "label": 0
                },
                {
                    "sent": "On one side you have a learner, and on the other side you have an evil adversary that tries to make the learner look bad.",
                    "label": 0
                },
                {
                    "sent": "The way this game is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wade is at each iteration you have the learners.",
                    "label": 0
                },
                {
                    "sent": "They pick a classifier based on its current data.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the adversary can observe that classifier and.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Test instance where that classifier will.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We evaluated and that defines some loss function over a space of classifiers.",
                    "label": 0
                },
                {
                    "sent": "That's being picked by the adversary.",
                    "label": 0
                },
                {
                    "sent": "And as the learner ensured that loss, then he observes the true label of that instance and you can update is hypothesis based on that new data.",
                    "label": 0
                },
                {
                    "sent": "For the next round.",
                    "label": 0
                },
                {
                    "sent": "So the game just keep playing on like this.",
                    "label": 0
                },
                {
                    "sent": "So what we're interested in online learning is minimize this notion of regret.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But as the other talks talk previously, then this notion of regret is basically just the difference between the some of the losses that you incur by playing this sequence of classifiers to classify your test instances that are picked by the adversary versus how well you could do if you add known all these tests examples in advance and pick the best classifier in your class.",
                    "label": 0
                },
                {
                    "sent": "So the average regret is just this regret divided by the number of rounds an.",
                    "label": 0
                },
                {
                    "sent": "And then no regret algorithm is basically an algorithm that can guarantee that this goes to 0 as a number of iterations goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So how does that relate to Dagger?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, you can essentially think of each round of online learning or of dagger as a round of online learning.",
                    "label": 1
                },
                {
                    "sent": "So here a classifier corresponds to a policy.",
                    "label": 0
                },
                {
                    "sent": "An you can if you remember when we were running dagger at each iteration we're collecting a new small data set of of examples by running the current policy.",
                    "label": 0
                },
                {
                    "sent": "So you can think of these new examples that you collect as the test instances that are picked by the adversary that defines the law.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Function that that iteration.",
                    "label": 0
                },
                {
                    "sent": "So and with Dagger was doing with that new data is it was aggregating it with this data aggregate data set of all the data I've collected so far and then picking the new best policy on that entire data set?",
                    "label": 0
                },
                {
                    "sent": "So what this corresponds to air is just picking the policy that minimizes this.",
                    "label": 0
                },
                {
                    "sent": "Some of the losses so far.",
                    "label": 0
                },
                {
                    "sent": "So for those of you familiar with online learning, you'll recognize that this is just a folder leader type approach.",
                    "label": 0
                },
                {
                    "sent": "So you can essentially think of Dagger as running this folder leader online learning algorithm on this.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your sequence of loss functions and we will be able to say is if this folder leader algorithm is essentially no regret, will be able to provide some good guarantees for the policy that we learn with Dagger.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in particular, what we can do is relate the performance of the policy, learn with dagger at Test time to two quantities that are related to how well we can do on that online learning problem.",
                    "label": 0
                },
                {
                    "sent": "So one of those quantity is the average regret I was talking about.",
                    "label": 0
                },
                {
                    "sent": "The other one is how well you can do at classifying that aggregate data set, which are class of policies.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we notice for follow the leader.",
                    "label": 0
                },
                {
                    "sent": "It's actually a no regret algorithm for strongly convex loss functions.",
                    "label": 1
                },
                {
                    "sent": "So if your classification last include some type of regularization will usually be a strongly convex loss function.",
                    "label": 0
                },
                {
                    "sent": "And what that means is all these other terms will vanish to zero as you iterate dagger for long enough, and so you'll be able to guarantee that you'll learn some policy.",
                    "label": 0
                },
                {
                    "sent": "That basically have a number of mistakes at Test time that's linear in T and scales with our we can do at classifying that aggregate data set.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the other important point here that we made with this analysis is that basically these results just follow from the no regret properties of follow the leader.",
                    "label": 0
                },
                {
                    "sent": "So what that means is you could run dagger with any other online learning algorithm to pick this sequence of policy's, and as long as it's no regret will be able to guarantee a similar bound like this.",
                    "label": 0
                },
                {
                    "sent": "So you could imagine like a version of Dagger that does like online gradient descent on the same sequence of loss functions that we've defined and will be able to have the same guarantees.",
                    "label": 0
                },
                {
                    "sent": "So the key part of the reduction is how we define those loss functions, and as long as we do any kind of no regret algorithm on this will have good performance guarantees.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the bounds I just showed you was for this sort of infinite sample case, where you would sample an infinite number of trajectories at each iteration of dagger.",
                    "label": 1
                },
                {
                    "sent": "So we can also provide high probability bounds when you just sample a finite number of trajectories.",
                    "label": 0
                },
                {
                    "sent": "And in particular, to just sample one trajectory per iteration.",
                    "label": 0
                },
                {
                    "sent": "An run dagger for order T squared iteration.",
                    "label": 0
                },
                {
                    "sent": "Then you get a similar bound at all with high probability.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to show you some cool experiments that we did with Dagger.",
                    "label": 0
                },
                {
                    "sent": "First one is this racing game I should be at the beginning, so remember we're trying to learn is some policy that can map these images to the steering issued.",
                    "label": 0
                },
                {
                    "sent": "Execute to stay near the center of the road.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to show you a video of our dagger did after 20 iterations of training.",
                    "label": 0
                },
                {
                    "sent": "And basically each iteration of training was one more lap of training data that we were collecting that corresponds to about 1000 time steps each.",
                    "label": 0
                },
                {
                    "sent": "So it's about like 1000 training samples that you add to your data set at each iteration, and all we're learning is a linear mapping from the image features which were just color values in the resized image to this steering.",
                    "label": 0
                },
                {
                    "sent": "So we're basically doing linear regression.",
                    "label": 0
                },
                {
                    "sent": "And So what you can see is with Dagger, we can perform actually really well at that task, we learn some policy that never falls off the track.",
                    "label": 0
                },
                {
                    "sent": "Over an entire.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But but if we compare that to other approaches, what we can see is the smile, both smile and the supervised learning approach didn't do that well.",
                    "label": 0
                },
                {
                    "sent": "So particular with supervised learning, the performance wasn't even improving as we were collecting more and more data with the experts policy.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's experiment is on Super Mario Bros.",
                    "label": 1
                },
                {
                    "sent": "So here we're trying to make the computer learn how to play this game by watching a planning algorithm that had full access to the internal game set of the game and could basically beat any stage of the game.",
                    "label": 0
                },
                {
                    "sent": "And here we're using a simulator from this module AI competition in 2009 that could generate random stages.",
                    "label": 0
                },
                {
                    "sent": "So I'm assuming most of you are familiar with that game, but if you're not, the goal is basically to move the character Mario through the stage by avoiding hitting enemies and falling into holes, and then reach the end of this stage before running out of time.",
                    "label": 0
                },
                {
                    "sent": "And you can do this for actions, jump right, left, or use the speed button, or any combination of those at the same time.",
                    "label": 0
                },
                {
                    "sent": "And here the features we're using is just a giant binary vector that describes the objects that were around Mario.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so now I'm going to show you another video of the performance with different approaches.",
                    "label": 0
                },
                {
                    "sent": "So first here this is the expert playing, so it's the planning algorithm who's playing the game.",
                    "label": 0
                },
                {
                    "sent": "As you can see, it's pretty good, can basically beat any stage of the game, and now this is the supervised learning approach and what you can see is that it's not doing very well.",
                    "label": 0
                },
                {
                    "sent": "It gets stuck in the level pretty often, or also often falls into holes.",
                    "label": 0
                },
                {
                    "sent": "As you can see there.",
                    "label": 0
                },
                {
                    "sent": "Now this is the performance with Dagger, same total number of data over 20 iterations and you can see it's doing much better.",
                    "label": 0
                },
                {
                    "sent": "It actually completes an entire stage pretty often.",
                    "label": 0
                },
                {
                    "sent": "As you can see here.",
                    "label": 0
                },
                {
                    "sent": "But it's also not perfect, so still it's enemies quite often.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But so if we compare the average distance travelled by Mario over over like this random distribution of stages that we can generate.",
                    "label": 0
                },
                {
                    "sent": "Where we can see that dagger does much better than all the other approaches?",
                    "label": 0
                },
                {
                    "sent": "So error 3000 corresponds to about 70% of a stage on average.",
                    "label": 0
                },
                {
                    "sent": "That's being completed.",
                    "label": 0
                },
                {
                    "sent": "And so we compared here with smile CERN and also supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And each algorithm was run for 20 iterations that we're collecting 5000 training samples per iteration.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, the sort of take home message of this talk is you can do these simple iterative procedures to obtain much better performance, practice and theory on these kind of non ID supervised learning problem like imitation learning.",
                    "label": 1
                },
                {
                    "sent": "Is something I didn't have time to talk about is that you can actually apply this to do structured prediction.",
                    "label": 0
                },
                {
                    "sent": "So one way you can do this is by doing in their process similar to CERN where you transform your structured prediction problem into like a sequential prediction problem.",
                    "label": 0
                },
                {
                    "sent": "That's sort of a general form of imitation learning.",
                    "label": 0
                },
                {
                    "sent": "And then you can apply dagger to learn the classifier.",
                    "label": 0
                },
                {
                    "sent": "That can essentially construct the structured output by making this sequence of predictions.",
                    "label": 0
                },
                {
                    "sent": "And so we have actually structured prediction application in the paper on in writing recognition, and we also have a new paper coming up in the Computer Vision conference doing as seen labeling applique.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oceans.",
                    "label": 0
                },
                {
                    "sent": "So I'll just stop here.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}