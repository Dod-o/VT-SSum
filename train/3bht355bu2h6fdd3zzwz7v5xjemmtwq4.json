{
    "id": "3bht355bu2h6fdd3zzwz7v5xjemmtwq4",
    "title": "On Consistent Surrogate Risk Minimization and Property Elicitation",
    "info": {
        "author": [
            "Shivani Agarwal, Department of Computer Science and Automation (CSA), Indian Institute of Science Bangalore"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_agarwal_property_elicitation/",
    "segmentation": [
        [
            "So how many people here know what surrogate risk minimization is?",
            "How many people know what property Visitation is?",
            "Well, yes, we just saw some of that in refiles talk.",
            "OK, So what we do in this paper is we show that good surrogates in supervised learning can essentially be viewed as eliciting certain properties of the conditional label distribution, from which one can recover a Bayes optimal prediction.",
            "So.",
            "We're all familiar with.",
            "So."
        ],
        [
            "Targets in supervised learning.",
            "For example, in binary classification with 01 loss, when doesn't in general minimize a 01 loss directly, but rather minimizes some surrogate loss, usually convex on the real line?",
            "For example, the hinge loss."
        ],
        [
            "And then predicts according to."
        ],
        [
            "Sign."
        ],
        [
            "In multiclass classification with 01 loss."
        ],
        [
            "One might minimize the."
        ],
        [
            "I'm missing a surrogate or the lead and Bob's surrogate which foreign in class Problem act on the in dimensional Euclidean space?",
            "And then predicted."
        ],
        [
            "The argmax chooses the class that has the highest real value."
        ],
        [
            "And in general we might have any number of classes in here in his four.",
            "Any loss matrix?",
            "And one might have a surrogate and."
        ],
        [
            "Any D dimensional space here these two.",
            "And what might partition the dimensions?",
            "Are good space in any manner in order to predict one of these classes?",
            "No good surrogate is one that is calibrated for the target loss.",
            "I won't get into formal definitions here, but intuitively, minimizing a calibrated surrogate allows you to recover a Bayes optimal classifier for the target loss and has been much interesting reasons."
        ],
        [
            "In designing comixology Centers for various learning problems.",
            "Proper."
        ],
        [
            "Dissertation is a classical topic in probability forecasting, statistics, economics, machine learning.",
            "In the classical setting, one has any possible outcomes here in his four, and one is interested in eliciting probability distribution over these in outcomes.",
            "In property licitation"
        ],
        [
            "And wants to elicit or estimate just some property or statistic of the distribution.",
            "And this could live in some smaller dimensional space.",
            "So here the dimension."
        ],
        [
            "2.",
            "And in order to do this, one uses a scoring rule that assigns a score to every outcome property value pair.",
            "A good scoring."
        ],
        [
            "Is one that is strictly proper for the target property?",
            "And again, I won't get into the definitions, but intuitively, if the true outcome is drawn according to probability vector P. Then the expected value of the strictly proper scoring rule as a function of the D dimensional space is uniquely minimized at the correct property value gamma of P. And I."
        ],
        [
            "There's been a lot of interest in recent years in understanding in designing strictly proper scoring rules for various properties of interest."
        ],
        [
            "What we do here is we define what we call calibrated properties for a target loss target loss matrix in so intuitively and L calibrated property is just any property of the conditional label distribution from which one can recover a Bayes optimal prediction under that loss L. So more specifically."
        ],
        [
            "For any in class loss matrix L. The in simplex can be partitioned into N trigger probabilities, it's.",
            "So the I TH such set is just the set of all.",
            "Conditional label probability vectors for which predicting class I is optimal andrel.",
            "And then a calibrated property is."
        ],
        [
            "Any property for which.",
            "Just by reading off the property value, we can tell which of the trigger probabilities its contains.",
            "The underlying condition label probability vector."
        ],
        [
            "What we show."
        ],
        [
            "Is that any strictly proper scoring rule for calibrated property?",
            "Forms a calibrated surrogate.",
            "In fact, one could also in the reverse direction.",
            "And so this gives us a nice unified framework as well as a convenient tool for designing convex, celebrated surrogates.",
            "And I won't have much time for details here."
        ],
        [
            "But just briefly, as it turns out, many convex calibrations arrested in proposed in recent years, including surrogates, are calibrated for various ranking losses as well as the generic collaborative ramsammy at all can be viewed as strictly proper scoring rules that isn't certain linear properties.",
            "Media properties and ice.",
            "They well understood."
        ],
        [
            "But it turns out that they may not always give the smallest dimension, and in such cases one can potentially design lower dimensional calibrated surrogates by turning to nonlinear properties and to just briefly."
        ],
        [
            "As one example, we show that by using strictly proper scoring rules for vectors of quantile properties, one can often design convex calibrated surrogates in lower dimensions that are calibrated under fairly broad conditions.",
            "So for example, for the multiclass 0."
        ],
        [
            "One loss.",
            "We get a log in dimensional convex surrogate that is calibrated, Andrew.",
            "Different set of conditions and the interventional crimethink is related.",
            "In some sense a larger set of conditions.",
            "That was the trailer for the movie."
        ],
        [
            "For more details come to oppose this afternoon, talk to our pit.",
            "Who's in the audience or myself or read our paper."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how many people here know what surrogate risk minimization is?",
                    "label": 1
                },
                {
                    "sent": "How many people know what property Visitation is?",
                    "label": 0
                },
                {
                    "sent": "Well, yes, we just saw some of that in refiles talk.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we do in this paper is we show that good surrogates in supervised learning can essentially be viewed as eliciting certain properties of the conditional label distribution, from which one can recover a Bayes optimal prediction.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We're all familiar with.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Targets in supervised learning.",
                    "label": 0
                },
                {
                    "sent": "For example, in binary classification with 01 loss, when doesn't in general minimize a 01 loss directly, but rather minimizes some surrogate loss, usually convex on the real line?",
                    "label": 0
                },
                {
                    "sent": "For example, the hinge loss.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then predicts according to.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sign.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In multiclass classification with 01 loss.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One might minimize the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm missing a surrogate or the lead and Bob's surrogate which foreign in class Problem act on the in dimensional Euclidean space?",
                    "label": 0
                },
                {
                    "sent": "And then predicted.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The argmax chooses the class that has the highest real value.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in general we might have any number of classes in here in his four.",
                    "label": 0
                },
                {
                    "sent": "Any loss matrix?",
                    "label": 0
                },
                {
                    "sent": "And one might have a surrogate and.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Any D dimensional space here these two.",
                    "label": 0
                },
                {
                    "sent": "And what might partition the dimensions?",
                    "label": 0
                },
                {
                    "sent": "Are good space in any manner in order to predict one of these classes?",
                    "label": 0
                },
                {
                    "sent": "No good surrogate is one that is calibrated for the target loss.",
                    "label": 1
                },
                {
                    "sent": "I won't get into formal definitions here, but intuitively, minimizing a calibrated surrogate allows you to recover a Bayes optimal classifier for the target loss and has been much interesting reasons.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In designing comixology Centers for various learning problems.",
                    "label": 0
                },
                {
                    "sent": "Proper.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dissertation is a classical topic in probability forecasting, statistics, economics, machine learning.",
                    "label": 0
                },
                {
                    "sent": "In the classical setting, one has any possible outcomes here in his four, and one is interested in eliciting probability distribution over these in outcomes.",
                    "label": 0
                },
                {
                    "sent": "In property licitation",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And wants to elicit or estimate just some property or statistic of the distribution.",
                    "label": 0
                },
                {
                    "sent": "And this could live in some smaller dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So here the dimension.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "And in order to do this, one uses a scoring rule that assigns a score to every outcome property value pair.",
                    "label": 0
                },
                {
                    "sent": "A good scoring.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is one that is strictly proper for the target property?",
                    "label": 0
                },
                {
                    "sent": "And again, I won't get into the definitions, but intuitively, if the true outcome is drawn according to probability vector P. Then the expected value of the strictly proper scoring rule as a function of the D dimensional space is uniquely minimized at the correct property value gamma of P. And I.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's been a lot of interest in recent years in understanding in designing strictly proper scoring rules for various properties of interest.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we do here is we define what we call calibrated properties for a target loss target loss matrix in so intuitively and L calibrated property is just any property of the conditional label distribution from which one can recover a Bayes optimal prediction under that loss L. So more specifically.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For any in class loss matrix L. The in simplex can be partitioned into N trigger probabilities, it's.",
                    "label": 0
                },
                {
                    "sent": "So the I TH such set is just the set of all.",
                    "label": 0
                },
                {
                    "sent": "Conditional label probability vectors for which predicting class I is optimal andrel.",
                    "label": 0
                },
                {
                    "sent": "And then a calibrated property is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any property for which.",
                    "label": 0
                },
                {
                    "sent": "Just by reading off the property value, we can tell which of the trigger probabilities its contains.",
                    "label": 0
                },
                {
                    "sent": "The underlying condition label probability vector.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we show.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that any strictly proper scoring rule for calibrated property?",
                    "label": 0
                },
                {
                    "sent": "Forms a calibrated surrogate.",
                    "label": 0
                },
                {
                    "sent": "In fact, one could also in the reverse direction.",
                    "label": 0
                },
                {
                    "sent": "And so this gives us a nice unified framework as well as a convenient tool for designing convex, celebrated surrogates.",
                    "label": 0
                },
                {
                    "sent": "And I won't have much time for details here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But just briefly, as it turns out, many convex calibrations arrested in proposed in recent years, including surrogates, are calibrated for various ranking losses as well as the generic collaborative ramsammy at all can be viewed as strictly proper scoring rules that isn't certain linear properties.",
                    "label": 0
                },
                {
                    "sent": "Media properties and ice.",
                    "label": 0
                },
                {
                    "sent": "They well understood.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it turns out that they may not always give the smallest dimension, and in such cases one can potentially design lower dimensional calibrated surrogates by turning to nonlinear properties and to just briefly.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As one example, we show that by using strictly proper scoring rules for vectors of quantile properties, one can often design convex calibrated surrogates in lower dimensions that are calibrated under fairly broad conditions.",
                    "label": 0
                },
                {
                    "sent": "So for example, for the multiclass 0.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One loss.",
                    "label": 0
                },
                {
                    "sent": "We get a log in dimensional convex surrogate that is calibrated, Andrew.",
                    "label": 0
                },
                {
                    "sent": "Different set of conditions and the interventional crimethink is related.",
                    "label": 0
                },
                {
                    "sent": "In some sense a larger set of conditions.",
                    "label": 0
                },
                {
                    "sent": "That was the trailer for the movie.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For more details come to oppose this afternoon, talk to our pit.",
                    "label": 0
                },
                {
                    "sent": "Who's in the audience or myself or read our paper.",
                    "label": 0
                }
            ]
        }
    }
}