{
    "id": "uykttvbbuvuzdg35llyi72agzz5jrjel",
    "title": "Information Rates and Optimal Decoding in Large Neural Populations",
    "info": {
        "author": [
            "David Pfau, Neuroscience, Columbia University Medical Center, Columbia University"
        ],
        "published": "Sept. 6, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Neural Networks",
            "Top->Medicine->Neuroscience"
        ]
    },
    "url": "http://videolectures.net/nips2011_pfau_decoding/",
    "segmentation": [
        [
            "So in this work we look at.",
            "Analytic results for optimal neural decoding in a novel intermediate signal to noise regime.",
            "By neural decoding, I mean there's some stimulus such as the intensity of light falling on a retina.",
            "We are able to record neural spike trains and we want to estimate either the original stimulus or the amount of information about the stimulus encoded in the network."
        ],
        [
            "So analytic results are very difficult to derive for this problem.",
            "For realistic models, most work has been in either a high or a low signal to noise regime.",
            "People have looked at what happens when for a fixed population of neurons, what happens when the sensitivity to the stimulus is very low, or for a fixed sensitivity in each neuron?",
            "What the optimal decoding is in the limit of a large population?",
            "By contrast, we look in an intermediate signal to noise regime.",
            "Where as the number of neurons grows in the population, the sensitivity of each neuron to the stimulus decreases in a balanced way, so that even in the limit of very large networks, the total amount of information about the stimulus remains finite."
        ],
        [
            "So individual neurons convey only a small amount of information.",
            "The population as a whole conveys a finite amount of information and at least for independent neurons, directly from the theory of asymptotics, for maximum likelihood, you can show that a simple linear combination of filtered versions of these spike trains is a sufficient statistic for this stimulus, so you can throw out all the information, but this simple statistic.",
            "And this statistic is Gaussian distributed, which means that analytically decoding is very simple.",
            "In this work, we show that this result holds even for more realistic weakly coupled networks and networks with nontrivial history dependencies such as refractoriness or bursting.",
            "So with this sufficient statistic we can analytically calculate the optimal estimate of the original stimulus or the amount of information in the network about the stimulus.",
            "Not only are these results exact in the limit of large networks, they also work very well on model data for."
        ],
        [
            "Reasonably sized networks, so this is showing going down, increasing numbers of neurons in the network.",
            "This is model data where the neurons are weakly coupled.",
            "Poisson firing history terms for refractory.",
            "And we generate model spikes.",
            "We derive this sufficient statistic for networks of different sizes to the right of that and we compare this simple Gaussian decoder in.",
            "Blue, with the exactly optimal Bayesian decoder in red and find that even for moderately sized networks such as 20 neurons, that decoding is very close to optimal with a much simpler decoder analytically tractable.",
            "So in summary.",
            "We investigate a novel, limit a novel signal to noise regime.",
            "Calculate the optimal decoding and find that a simple sufficient statistic works well both for calculating information rates and in practice explains many recent results in the literature suggesting linear decoding works very well, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this work we look at.",
                    "label": 0
                },
                {
                    "sent": "Analytic results for optimal neural decoding in a novel intermediate signal to noise regime.",
                    "label": 0
                },
                {
                    "sent": "By neural decoding, I mean there's some stimulus such as the intensity of light falling on a retina.",
                    "label": 0
                },
                {
                    "sent": "We are able to record neural spike trains and we want to estimate either the original stimulus or the amount of information about the stimulus encoded in the network.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So analytic results are very difficult to derive for this problem.",
                    "label": 0
                },
                {
                    "sent": "For realistic models, most work has been in either a high or a low signal to noise regime.",
                    "label": 0
                },
                {
                    "sent": "People have looked at what happens when for a fixed population of neurons, what happens when the sensitivity to the stimulus is very low, or for a fixed sensitivity in each neuron?",
                    "label": 0
                },
                {
                    "sent": "What the optimal decoding is in the limit of a large population?",
                    "label": 0
                },
                {
                    "sent": "By contrast, we look in an intermediate signal to noise regime.",
                    "label": 0
                },
                {
                    "sent": "Where as the number of neurons grows in the population, the sensitivity of each neuron to the stimulus decreases in a balanced way, so that even in the limit of very large networks, the total amount of information about the stimulus remains finite.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So individual neurons convey only a small amount of information.",
                    "label": 1
                },
                {
                    "sent": "The population as a whole conveys a finite amount of information and at least for independent neurons, directly from the theory of asymptotics, for maximum likelihood, you can show that a simple linear combination of filtered versions of these spike trains is a sufficient statistic for this stimulus, so you can throw out all the information, but this simple statistic.",
                    "label": 0
                },
                {
                    "sent": "And this statistic is Gaussian distributed, which means that analytically decoding is very simple.",
                    "label": 0
                },
                {
                    "sent": "In this work, we show that this result holds even for more realistic weakly coupled networks and networks with nontrivial history dependencies such as refractoriness or bursting.",
                    "label": 0
                },
                {
                    "sent": "So with this sufficient statistic we can analytically calculate the optimal estimate of the original stimulus or the amount of information in the network about the stimulus.",
                    "label": 1
                },
                {
                    "sent": "Not only are these results exact in the limit of large networks, they also work very well on model data for.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reasonably sized networks, so this is showing going down, increasing numbers of neurons in the network.",
                    "label": 0
                },
                {
                    "sent": "This is model data where the neurons are weakly coupled.",
                    "label": 0
                },
                {
                    "sent": "Poisson firing history terms for refractory.",
                    "label": 0
                },
                {
                    "sent": "And we generate model spikes.",
                    "label": 0
                },
                {
                    "sent": "We derive this sufficient statistic for networks of different sizes to the right of that and we compare this simple Gaussian decoder in.",
                    "label": 0
                },
                {
                    "sent": "Blue, with the exactly optimal Bayesian decoder in red and find that even for moderately sized networks such as 20 neurons, that decoding is very close to optimal with a much simpler decoder analytically tractable.",
                    "label": 0
                },
                {
                    "sent": "So in summary.",
                    "label": 0
                },
                {
                    "sent": "We investigate a novel, limit a novel signal to noise regime.",
                    "label": 0
                },
                {
                    "sent": "Calculate the optimal decoding and find that a simple sufficient statistic works well both for calculating information rates and in practice explains many recent results in the literature suggesting linear decoding works very well, thank you.",
                    "label": 0
                }
            ]
        }
    }
}