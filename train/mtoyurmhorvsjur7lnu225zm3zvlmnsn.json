{
    "id": "mtoyurmhorvsjur7lnu225zm3zvlmnsn",
    "title": "A Case Study: Privacy Preserving Release of Spa9o-\u00ad\u2010temporal Density in Paris",
    "info": {
        "author": [
            "Gergely \u00c1cs, INRIA Grenoble Rh\u00f4ne-Alpes"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_acs_spatio_temporal_density/",
    "segmentation": [
        [
            "This is joint work with the cloud Castelluccio who is also from India as well as me.",
            "So."
        ],
        [
            "So the motivation of the of our paper is that there is a French national project which is called X data, where X stands for crossing different datasets.",
            "So there is a French national project which is called X data, where X stands for crossing different datasets.",
            "So in particular there are several big data providers in France, like lampposts, which provides the Postal data of individuals or EDF, which provides the electricity consumption or orange which provides the call data record of.",
            "Different users and the idea is that these big companies wants to combine these datasets in order to come up with new services.",
            "For example, knowing the special temporal density of Paris, which is computed for from the call data record of users, we can identify the sportswear which should deploy, deploy new services, for example bakeries.",
            "And these things."
        ],
        [
            "However, the problem is that the European data protection law, which is stronger than the US equivalent parole, says that all data sets have to be anonymized such that the data subjects or the individuals are no longer identifiable.",
            "So what does it mean?",
            "Identifiable that the law is very loose?",
            "Defining identifiability actually doesn't dictate any privacy specific privacy model to be used, but does it mean it just says that?",
            "All attacks should be considered, which has reasonable power and uses reasonable tools and tries to re identify the users in the data set.",
            "If it cannot be done using these adversaries then the data is not.",
            "The users are not identifiable.",
            "However, the problem is that in our case we have several big datasets, and even if the user is not identifiable in any of them individually, if we combine them together, it can happen that he becomes identifiable.",
            "So we reveal personal data about the individuals and because of that.",
            "The data has to be anonymized before cross processing, combining them.",
            "In practice, at least in France, the French data Protection Office called Clear checks if the data analyzation is correct and obeys the rule."
        ],
        [
            "So because of this, we choose a privacy model which supports this.",
            "Cross processing or the combination, the composability and because of that we use differential privacy model.",
            "So just in a short differential privacy says that if you have two datasets which differ only in a single individual, these are called neighboring datasets, then anonymization algorithm is differential private, so it's a property of Dynamization algorithm if it produces indistinguishable outputs on neighboring datasets.",
            "So in other words, it means that the algorithm will be insensitive to the change of.",
            "Any single individual in the data set.",
            "More precisely, it means that if we consider any possible output.",
            "Then the output probability of these output will be more or less the same on neighboring datasets, so it can be bounded with the function of epsilon.",
            "If epsilon is smaller then we will have stronger privacy guarantee, but if it's larger like Rachel and .5 or one, then we have much weaker privacy guarantee but larger utility.",
            "So actually epsilon measures the information leakage about any single individual.",
            "Indirectly.",
            "So as I mentioned, the very nice property of differential privacy and the reason why we used it is that it composes securely.",
            "So if you have several datasets, we individually sanitize them.",
            "And for each of them we get an epsilon value and we want to measure the information leakage about any single individual which can present in the intersection of these datasets.",
            "Then just simply have to add up all the individual epsilons and the sum of the epsilons will measure the information leakage indirectly."
        ],
        [
            "So in this presentation I'm going to focus on the data which is provided by Orange.",
            "It's the call data record, a simplified call data record."
        ],
        [
            "Simplified, because it means that we store the events of individuals by even timing that the income.",
            "So we have a huge table and each local response to one event.",
            "They even can be incoming or outgoing outgoing call or SMS of the user.",
            "We also store the phone number of the user and the GPS tower position the.",
            "The position of the GSM tower.",
            "And also the timestamp of the event.",
            "Although these are provided by Orange Descrambled, the phone numbers so they are removed if you will.",
            "But as we know it doesn't preserve too much privacy in practice."
        ],
        [
            "So this is the data set that we had.",
            "It's contains roughly 2 million users in the Metropolitan area of Paris.",
            "Where there are a bit more than 1300 GPS towers.",
            "You can see the towers in this picture.",
            "We had the events of these users in one week in 2007, where the mean number of events per user was 13.",
            "However, the inverse case, the maximum number of events was more than 700.",
            "It's important in our case because it influences privacy and actually it shows that this data is sensitive.",
            "I will clarify later why exactly."
        ],
        [
            "So it's very important to note that what we anonymize is not the serial data set, but the special temporal density.",
            "So it means that Paris can be divided into a bit more than 900 non overlapping areas.",
            "These are not the GPS towers.",
            "These are independent from that.",
            "These are called.",
            "Iris says these are the from the statistics of Statistics Institute of France and we want to know how many people visited each of these cell.",
            "In any hour of the whole week.",
            "So in other words, we can build a time series for each cell and we will have a bunch of time series.",
            "As you can see here, and we want to anonymize these time series and we want to release this analyze time series in a differential private way.",
            "The challenge here is that.",
            "It can be considered as a large dimensional data, at least in privacy.",
            "Becausw for one user, we can assign a vector which has the number of coordinates equal equal to the number of hours in the week, which is 168."
        ],
        [
            "So let's see how we anonymize the data set.",
            "We took a three step approach and it's important to note that it's only the highlight of the solution.",
            "So much more details are in the paper.",
            "First, we recognize that there are no need to keep all the visits or the events per users.",
            "We just have to.",
            "Keep only more or less 30.",
            "Events for users and we sample.",
            "This is uniformly per random producer.",
            "Having this truncated Cdr data set, we created the time series, so we mapped the Cdr data to the special temporal density.",
            "And having that I'm serious, we perturb this time theorems in order in order to guarantee differential privacy.",
            "So let's see the second step.",
            "How we created the time series."
        ],
        [
            "Well, from one hand we have the Cdr data.",
            "From which we can compute the number of visits of each GPS tower in Paris in any hour of the vehicle.",
            "On the other hand, we have the iris US and we want to compute the visit number of the iris cells.",
            "So we did a very simple approach and we created a voluntary tessellation of the GPS towers where the center of the voluntary polygons are the towers and after that we for each iris cell we checked, we identified.",
            "They've only polygons which overlap with this particular iris cell, and these Voronoi.",
            "Polygons the corresponding GPS towers contributed the counts to these iris.",
            "So in other words we sent over or overlapping.",
            "My polygons we summed up their contribution where the contribution is proportional to the overlapping area.",
            "And using this simple approach, we computed the time series of each I reset.",
            "So the count of each iris selling.",
            "During the whole week."
        ],
        [
            "So the third step is that having these time series we have to put or get in order to guarantee differential privacy."
        ],
        [
            "My differential privacy is a very simple approach to purtle this time series.",
            "Namely, we can add the IID Laplace noise to each count and two each.",
            "To each count of each size, Iris says.",
            "And here you can see the result is that the the red curve.",
            "Is the noisy the particular time series by the blue one is the origin, is the original 1.",
            "And as you can see, there is a pretty bad.",
            "The problem is that the original count are very small and the magnitude of the noise is comparable to the current value.",
            "So basically the noise destroys all the almost all the information and even the trends cannot be identified from the noisy data.",
            "So instead of this we take took a more sophisticated approach.",
            "And.",
            "We observed that the geographically close I research have similar time series.",
            "So what we did is that we clustered the clauses which have small counts.",
            "And we did it until the aggregated time series, so the cluster had, which is simply the sum of the time series inside the clusters.",
            "Are have sufficiently large counts, and then it's true.",
            "Then we added noise to this aggregated time series.",
            "After having the cluster head the noisy cluster head, so the aggregated time series.",
            "We assume that all the all the cells have exactly the same trends like the aggregated time series.",
            "So as the cluster head but with different magnitude.",
            "So what we did is that we normalized the noisy aggregated time series by simply dividing the total visit in all the cells inside the cluster.",
            "And after that.",
            "And after that we scaled back.",
            "These normalized time series, with the total opposite of the individual.",
            "Says so.",
            "In other words, it means that.",
            "In the inside one cluster each time series have exactly the same trends but with different magnitude.",
            "And here you can see the result for one cell where the epsilon is set to 0.3.",
            "Actually, I forgot to tell you, but the Laplace noise that we add is calibrated to this epsilon and the maximum contribution of a user.",
            "So this cat parameter of the Laplace noise and this is what determines the magnitude of the noise and 0.3 epsilon value stands for pretty good privacy.",
            "So as you can see in our approach.",
            "The.",
            "The results are incomparably better compared to the naive approach."
        ],
        [
            "In particular, we did 2.",
            "From a simulation test.",
            "The first is that we measure the mean relative error.",
            "So for each cell in Paris, will compute the mean relative error between the noisy and the original time series, and here you can see the result where the color corresponds to the error for each cell.",
            "So this is the result for the naive approach.",
            "Many at the lab plus noise to each count.",
            "And as you can see, the error is more than 100 percentage.",
            "Compared to this, with our scheme, the error the average error over OSS is less than 17, so it's very closed.",
            "The.",
            "It's more or less practical results, at least in the project."
        ],
        [
            "One day on the other test, we computed the Christian correlation in the same manner and as you can see in our approach to Pearson, correlation is almost perfect, so we managed to preserve the trends of all time series, almost in Paris.",
            "So."
        ],
        [
            "The conclusion is that first the secure composability is an implicit requirement in the European data protection law, and these favors randomization based notions of privacy, such as differential privacy.",
            "2nd, we obtain very good results using differential privacy.",
            "For large dimensional data and 3rd which is the most important message is that there are no universal enemies animations solution that fits all the application.",
            "So we have to tailor randomization solution not only to the application requirements but also to the public characteristics of the data set.",
            "So if it's periodic, if it's can be compressed by clustering and so on.",
            "Thank you this is."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with the cloud Castelluccio who is also from India as well as me.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the motivation of the of our paper is that there is a French national project which is called X data, where X stands for crossing different datasets.",
                    "label": 0
                },
                {
                    "sent": "So there is a French national project which is called X data, where X stands for crossing different datasets.",
                    "label": 0
                },
                {
                    "sent": "So in particular there are several big data providers in France, like lampposts, which provides the Postal data of individuals or EDF, which provides the electricity consumption or orange which provides the call data record of.",
                    "label": 1
                },
                {
                    "sent": "Different users and the idea is that these big companies wants to combine these datasets in order to come up with new services.",
                    "label": 0
                },
                {
                    "sent": "For example, knowing the special temporal density of Paris, which is computed for from the call data record of users, we can identify the sportswear which should deploy, deploy new services, for example bakeries.",
                    "label": 0
                },
                {
                    "sent": "And these things.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, the problem is that the European data protection law, which is stronger than the US equivalent parole, says that all data sets have to be anonymized such that the data subjects or the individuals are no longer identifiable.",
                    "label": 1
                },
                {
                    "sent": "So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "Identifiable that the law is very loose?",
                    "label": 0
                },
                {
                    "sent": "Defining identifiability actually doesn't dictate any privacy specific privacy model to be used, but does it mean it just says that?",
                    "label": 0
                },
                {
                    "sent": "All attacks should be considered, which has reasonable power and uses reasonable tools and tries to re identify the users in the data set.",
                    "label": 0
                },
                {
                    "sent": "If it cannot be done using these adversaries then the data is not.",
                    "label": 0
                },
                {
                    "sent": "The users are not identifiable.",
                    "label": 0
                },
                {
                    "sent": "However, the problem is that in our case we have several big datasets, and even if the user is not identifiable in any of them individually, if we combine them together, it can happen that he becomes identifiable.",
                    "label": 0
                },
                {
                    "sent": "So we reveal personal data about the individuals and because of that.",
                    "label": 0
                },
                {
                    "sent": "The data has to be anonymized before cross processing, combining them.",
                    "label": 1
                },
                {
                    "sent": "In practice, at least in France, the French data Protection Office called Clear checks if the data analyzation is correct and obeys the rule.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So because of this, we choose a privacy model which supports this.",
                    "label": 0
                },
                {
                    "sent": "Cross processing or the combination, the composability and because of that we use differential privacy model.",
                    "label": 1
                },
                {
                    "sent": "So just in a short differential privacy says that if you have two datasets which differ only in a single individual, these are called neighboring datasets, then anonymization algorithm is differential private, so it's a property of Dynamization algorithm if it produces indistinguishable outputs on neighboring datasets.",
                    "label": 0
                },
                {
                    "sent": "So in other words, it means that the algorithm will be insensitive to the change of.",
                    "label": 1
                },
                {
                    "sent": "Any single individual in the data set.",
                    "label": 0
                },
                {
                    "sent": "More precisely, it means that if we consider any possible output.",
                    "label": 0
                },
                {
                    "sent": "Then the output probability of these output will be more or less the same on neighboring datasets, so it can be bounded with the function of epsilon.",
                    "label": 0
                },
                {
                    "sent": "If epsilon is smaller then we will have stronger privacy guarantee, but if it's larger like Rachel and .5 or one, then we have much weaker privacy guarantee but larger utility.",
                    "label": 0
                },
                {
                    "sent": "So actually epsilon measures the information leakage about any single individual.",
                    "label": 0
                },
                {
                    "sent": "Indirectly.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned, the very nice property of differential privacy and the reason why we used it is that it composes securely.",
                    "label": 0
                },
                {
                    "sent": "So if you have several datasets, we individually sanitize them.",
                    "label": 0
                },
                {
                    "sent": "And for each of them we get an epsilon value and we want to measure the information leakage about any single individual which can present in the intersection of these datasets.",
                    "label": 0
                },
                {
                    "sent": "Then just simply have to add up all the individual epsilons and the sum of the epsilons will measure the information leakage indirectly.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this presentation I'm going to focus on the data which is provided by Orange.",
                    "label": 0
                },
                {
                    "sent": "It's the call data record, a simplified call data record.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simplified, because it means that we store the events of individuals by even timing that the income.",
                    "label": 0
                },
                {
                    "sent": "So we have a huge table and each local response to one event.",
                    "label": 0
                },
                {
                    "sent": "They even can be incoming or outgoing outgoing call or SMS of the user.",
                    "label": 1
                },
                {
                    "sent": "We also store the phone number of the user and the GPS tower position the.",
                    "label": 0
                },
                {
                    "sent": "The position of the GSM tower.",
                    "label": 0
                },
                {
                    "sent": "And also the timestamp of the event.",
                    "label": 1
                },
                {
                    "sent": "Although these are provided by Orange Descrambled, the phone numbers so they are removed if you will.",
                    "label": 0
                },
                {
                    "sent": "But as we know it doesn't preserve too much privacy in practice.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the data set that we had.",
                    "label": 0
                },
                {
                    "sent": "It's contains roughly 2 million users in the Metropolitan area of Paris.",
                    "label": 0
                },
                {
                    "sent": "Where there are a bit more than 1300 GPS towers.",
                    "label": 0
                },
                {
                    "sent": "You can see the towers in this picture.",
                    "label": 0
                },
                {
                    "sent": "We had the events of these users in one week in 2007, where the mean number of events per user was 13.",
                    "label": 0
                },
                {
                    "sent": "However, the inverse case, the maximum number of events was more than 700.",
                    "label": 0
                },
                {
                    "sent": "It's important in our case because it influences privacy and actually it shows that this data is sensitive.",
                    "label": 0
                },
                {
                    "sent": "I will clarify later why exactly.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's very important to note that what we anonymize is not the serial data set, but the special temporal density.",
                    "label": 0
                },
                {
                    "sent": "So it means that Paris can be divided into a bit more than 900 non overlapping areas.",
                    "label": 0
                },
                {
                    "sent": "These are not the GPS towers.",
                    "label": 0
                },
                {
                    "sent": "These are independent from that.",
                    "label": 0
                },
                {
                    "sent": "These are called.",
                    "label": 0
                },
                {
                    "sent": "Iris says these are the from the statistics of Statistics Institute of France and we want to know how many people visited each of these cell.",
                    "label": 0
                },
                {
                    "sent": "In any hour of the whole week.",
                    "label": 0
                },
                {
                    "sent": "So in other words, we can build a time series for each cell and we will have a bunch of time series.",
                    "label": 0
                },
                {
                    "sent": "As you can see here, and we want to anonymize these time series and we want to release this analyze time series in a differential private way.",
                    "label": 0
                },
                {
                    "sent": "The challenge here is that.",
                    "label": 0
                },
                {
                    "sent": "It can be considered as a large dimensional data, at least in privacy.",
                    "label": 0
                },
                {
                    "sent": "Becausw for one user, we can assign a vector which has the number of coordinates equal equal to the number of hours in the week, which is 168.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see how we anonymize the data set.",
                    "label": 0
                },
                {
                    "sent": "We took a three step approach and it's important to note that it's only the highlight of the solution.",
                    "label": 0
                },
                {
                    "sent": "So much more details are in the paper.",
                    "label": 0
                },
                {
                    "sent": "First, we recognize that there are no need to keep all the visits or the events per users.",
                    "label": 0
                },
                {
                    "sent": "We just have to.",
                    "label": 0
                },
                {
                    "sent": "Keep only more or less 30.",
                    "label": 0
                },
                {
                    "sent": "Events for users and we sample.",
                    "label": 0
                },
                {
                    "sent": "This is uniformly per random producer.",
                    "label": 0
                },
                {
                    "sent": "Having this truncated Cdr data set, we created the time series, so we mapped the Cdr data to the special temporal density.",
                    "label": 0
                },
                {
                    "sent": "And having that I'm serious, we perturb this time theorems in order in order to guarantee differential privacy.",
                    "label": 0
                },
                {
                    "sent": "So let's see the second step.",
                    "label": 0
                },
                {
                    "sent": "How we created the time series.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, from one hand we have the Cdr data.",
                    "label": 0
                },
                {
                    "sent": "From which we can compute the number of visits of each GPS tower in Paris in any hour of the vehicle.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we have the iris US and we want to compute the visit number of the iris cells.",
                    "label": 1
                },
                {
                    "sent": "So we did a very simple approach and we created a voluntary tessellation of the GPS towers where the center of the voluntary polygons are the towers and after that we for each iris cell we checked, we identified.",
                    "label": 0
                },
                {
                    "sent": "They've only polygons which overlap with this particular iris cell, and these Voronoi.",
                    "label": 0
                },
                {
                    "sent": "Polygons the corresponding GPS towers contributed the counts to these iris.",
                    "label": 0
                },
                {
                    "sent": "So in other words we sent over or overlapping.",
                    "label": 0
                },
                {
                    "sent": "My polygons we summed up their contribution where the contribution is proportional to the overlapping area.",
                    "label": 0
                },
                {
                    "sent": "And using this simple approach, we computed the time series of each I reset.",
                    "label": 1
                },
                {
                    "sent": "So the count of each iris selling.",
                    "label": 0
                },
                {
                    "sent": "During the whole week.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the third step is that having these time series we have to put or get in order to guarantee differential privacy.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My differential privacy is a very simple approach to purtle this time series.",
                    "label": 0
                },
                {
                    "sent": "Namely, we can add the IID Laplace noise to each count and two each.",
                    "label": 1
                },
                {
                    "sent": "To each count of each size, Iris says.",
                    "label": 0
                },
                {
                    "sent": "And here you can see the result is that the the red curve.",
                    "label": 0
                },
                {
                    "sent": "Is the noisy the particular time series by the blue one is the origin, is the original 1.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, there is a pretty bad.",
                    "label": 0
                },
                {
                    "sent": "The problem is that the original count are very small and the magnitude of the noise is comparable to the current value.",
                    "label": 0
                },
                {
                    "sent": "So basically the noise destroys all the almost all the information and even the trends cannot be identified from the noisy data.",
                    "label": 0
                },
                {
                    "sent": "So instead of this we take took a more sophisticated approach.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We observed that the geographically close I research have similar time series.",
                    "label": 0
                },
                {
                    "sent": "So what we did is that we clustered the clauses which have small counts.",
                    "label": 0
                },
                {
                    "sent": "And we did it until the aggregated time series, so the cluster had, which is simply the sum of the time series inside the clusters.",
                    "label": 0
                },
                {
                    "sent": "Are have sufficiently large counts, and then it's true.",
                    "label": 0
                },
                {
                    "sent": "Then we added noise to this aggregated time series.",
                    "label": 1
                },
                {
                    "sent": "After having the cluster head the noisy cluster head, so the aggregated time series.",
                    "label": 0
                },
                {
                    "sent": "We assume that all the all the cells have exactly the same trends like the aggregated time series.",
                    "label": 0
                },
                {
                    "sent": "So as the cluster head but with different magnitude.",
                    "label": 0
                },
                {
                    "sent": "So what we did is that we normalized the noisy aggregated time series by simply dividing the total visit in all the cells inside the cluster.",
                    "label": 0
                },
                {
                    "sent": "And after that.",
                    "label": 0
                },
                {
                    "sent": "And after that we scaled back.",
                    "label": 1
                },
                {
                    "sent": "These normalized time series, with the total opposite of the individual.",
                    "label": 0
                },
                {
                    "sent": "Says so.",
                    "label": 0
                },
                {
                    "sent": "In other words, it means that.",
                    "label": 0
                },
                {
                    "sent": "In the inside one cluster each time series have exactly the same trends but with different magnitude.",
                    "label": 1
                },
                {
                    "sent": "And here you can see the result for one cell where the epsilon is set to 0.3.",
                    "label": 0
                },
                {
                    "sent": "Actually, I forgot to tell you, but the Laplace noise that we add is calibrated to this epsilon and the maximum contribution of a user.",
                    "label": 0
                },
                {
                    "sent": "So this cat parameter of the Laplace noise and this is what determines the magnitude of the noise and 0.3 epsilon value stands for pretty good privacy.",
                    "label": 0
                },
                {
                    "sent": "So as you can see in our approach.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The results are incomparably better compared to the naive approach.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In particular, we did 2.",
                    "label": 0
                },
                {
                    "sent": "From a simulation test.",
                    "label": 0
                },
                {
                    "sent": "The first is that we measure the mean relative error.",
                    "label": 0
                },
                {
                    "sent": "So for each cell in Paris, will compute the mean relative error between the noisy and the original time series, and here you can see the result where the color corresponds to the error for each cell.",
                    "label": 0
                },
                {
                    "sent": "So this is the result for the naive approach.",
                    "label": 1
                },
                {
                    "sent": "Many at the lab plus noise to each count.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, the error is more than 100 percentage.",
                    "label": 1
                },
                {
                    "sent": "Compared to this, with our scheme, the error the average error over OSS is less than 17, so it's very closed.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "It's more or less practical results, at least in the project.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One day on the other test, we computed the Christian correlation in the same manner and as you can see in our approach to Pearson, correlation is almost perfect, so we managed to preserve the trends of all time series, almost in Paris.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The conclusion is that first the secure composability is an implicit requirement in the European data protection law, and these favors randomization based notions of privacy, such as differential privacy.",
                    "label": 1
                },
                {
                    "sent": "2nd, we obtain very good results using differential privacy.",
                    "label": 1
                },
                {
                    "sent": "For large dimensional data and 3rd which is the most important message is that there are no universal enemies animations solution that fits all the application.",
                    "label": 1
                },
                {
                    "sent": "So we have to tailor randomization solution not only to the application requirements but also to the public characteristics of the data set.",
                    "label": 0
                },
                {
                    "sent": "So if it's periodic, if it's can be compressed by clustering and so on.",
                    "label": 0
                },
                {
                    "sent": "Thank you this is.",
                    "label": 0
                }
            ]
        }
    }
}