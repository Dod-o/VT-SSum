{
    "id": "x7gl7mhq5jxznwyqvy746pybdjlc7zdt",
    "title": "Invited Talk: Towards Theoretical Understanding of Domain Adaptation Learning",
    "info": {
        "author": [
            "Shai Ben-David, David R. Cheriton School of Computer Science, University of Waterloo"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_ben_david_ttu/",
    "segmentation": [
        [
            "OK, so so the emphasis of this talk is domain adaptation and theoretical analysis of that so."
        ],
        [
            "I think that if we, I think this slide is going to be my first slide tomorrow also.",
            "But we can see that the.",
            "To ethical research in machine learning has contributed some very important tools to practical machine learning like boosting and support vector machines are a.",
            "Very prominent examples of such contributions that grew out of theory and became important tools.",
            "Essential tools in applications.",
            "However, the there is a problem that the mainstream machine learning is based on simplifying assumptions that many potential applications fail to meet so that we always in order to do theory, we need to simplify the world and the mainstream theoretical machine learning makes very strong simplifying assumption about the world.",
            "The problem is.",
            "That in many potential applications you cannot apply the theory because it doesn't meet those assumptions, so it's a major challenge to extend our machine learning theory to cover a wider variety of realistic settings, and I think this is a big program.",
            "This is direction that I think then theoretical machine learning should."
        ],
        [
            "Pushed out so.",
            "One such limitation is that this is the central issue of this content of this workshop is the major common assumption is that the data.",
            "Generating distribution is stationary that we make the assumptions that we trained on the same data that we're going to use in learning.",
            "And of course we need to make some kind of such an assumption in order to make learning possible.",
            "If all our experience in the past has no relevance to the future, then of course there's no point in trying to learn anything but the strong assumption here is that not only that our experience is relevant to our future, but also that.",
            "It's exactly the same distribution that generated the data in the training set that we're going to be tested on, and that's a very strong assumption.",
            "So.",
            "Formally, it is common to assume that both the training and the test examples are generated.",
            "I I did, it is identically independently distributed by the same fixed probability distribution and.",
            "This is unrealistic in many machine learning applications as they leave I mentioned and therefore I think it's great to have this workshop that focuses on this important.",
            "Short full off the current theory of machine learning."
        ],
        [
            "So what I'm going to do in this talk, I'm first going to talk the the central point of the talk is going to be domain adaptation and adaptation situation where we make relax the IID assumptions.",
            "But we make another kind of simplifying assumption.",
            "We assume that there are just two distributions, one on which we train and one on which we have to perform and test our performance.",
            "And so I'll first talk about some general issues and some principles.",
            "That we have to address, then I'll give an example of 1 success.",
            "Analysis of domain adaptation where the theory could contribute to practical algorithms.",
            "And then I'll discuss some inherent limitations in the approaches that we currently have told them in adaptations.",
            "So there are some very.",
            "Strong limits to what we can do unless we come up with.",
            "Bright new innovations and then conclude with talking about open problems."
        ],
        [
            "So.",
            "What can we do when we want to learn when the training and test distributions are different?",
            "Anne.",
            "So I mean.",
            "Of course, I mean my examples are more ways.",
            "No toy real world examples in your practical examples, but one such example is driving lessons.",
            "And if you, I mean, I've just been with this with my kids.",
            "They learn to drive on your own car and then they go and do the test on a different car.",
            "So you have to adapt what you learn to do on one car to what you have to do in on the other car.",
            "We all do it intuitively, but we have to realize that it's there is a problem here of domain adaptation.",
            "Either a mole computer cyantific problem is spam filters.",
            "If I'm trying to train a spam filter then I have this tension between.",
            "I can give it a lot of training, a lot of on my email and have a lot of data.",
            "But then I want to apply the spam filter.",
            "I want to sell it to you and this part filter will detect spam on your email and distribution of your emails is very different than distribution of my emails.",
            "Still, we believe that there is some relationship.",
            "That will make it relevant to you.",
            "The lesson that I learned on training it on my email so spam filters are another very prominent.",
            "Examples of it?",
            "Difference between training and test distributions.",
            "Another example that I'll talk about is in natural language processing task.",
            "In natural language processing we usually train on one corpora of data and we have some type of documents that we train on, say biomedical documents.",
            "But then we want that the lessons that we've learned will be relevant to other types of data.",
            "So if I want to adapt some tool from.",
            "Training it on legal documents and then I want to apply it to.",
            "Say.",
            "Well, Street Journal articles and so on.",
            "Will the language, of course, is slightly different.",
            "The domain is slightly different, but on the other hand we believe that there is relevance to the lessons that we learn from one domain and apply it to the other windows."
        ],
        [
            "With our motivating examples.",
            "And the goal is to figure out how and when will domain adaptation learning succeed.",
            "I mean of course the main adaptation learning will not succeed always if the two tasks are not related or what we're trying to transfer is exactly the aspect of the task that is not.",
            "The same then we may fail, so we have to.",
            "Learn when and then how will the meditation succeed?",
            "And the problem is one of those areas where there is a big gap between what is happening in practice and what is happening in theory.",
            "So in practice people applied to manipulation all the time.",
            "The problem is that we do not have satisfactory theoretical justification for that.",
            "So practitioners, I don't know this audience how practician oriented value, but practitioners may say why should we care about theoretical justification.",
            "If something works, why bother with philosophising?",
            "About it, but so I give you some reasons.",
            "If anybody needs those reasons.",
            "So I think that what we gain from theory is that ability to provide success guarantees to common risztics.",
            "I mean, if you just see lots of.",
            "I see Mail papers that say here is my new you Ristic, and here is this data that I tried it on and here is the graphs that show you my big success.",
            "But of course the data that you want to try it on is a different one than what they did.",
            "Do you have any guarantee that was succeeded on their experiments?",
            "Will work for your data, and that's exactly where the theory comes in and can provide you with some kind of guarantees.",
            "Another aspect is of course to understand under which circumstances will your algorithm work.",
            "I mean, every algorithm works on the certain circumstances.",
            "May fail in the others.",
            "It's important for us to know.",
            "Under which the contest will our algorithm work and this will help us choose the right learning paradigm for a given task.",
            "If we have a long selection of different algorithms, what is the best one for my task?",
            "If I have theoretical understanding of both the algorithms and posted tasks, then I can do this matching most successfully.",
            "And of course to guide the development of new algorithms and so on and so forth so.",
            "I hope that.",
            "I managed to convince of that people are convinced already that we do need to, right?",
            "It's not enough that something works we.",
            "Do you have a benefits from having theoretical understanding of?",
            "Why it works?",
            "So this is what we're trying to do.",
            "So domain adaptation works all the time and we try to understand theoretically why."
        ],
        [
            "An under what circumstances it will work, so my I'll take a very simplistic model and the Convention will be that the learning task is a distribution of points and labels.",
            "So we just take the simplistic classification tasks.",
            "You have domain points.",
            "Each of them is you have to predict the label and the label is binary is a 01, and we assume that there is some underlying probability distribution that generates the points and generates their labels.",
            "So I don't assume here there is a fixed correct label for every point.",
            "So it but that just the distribution that for every domain point there is some probability they'll get label zero and some probability will get label one.",
            "So for me a learning task is such a probability distribution over domain and labels.",
            "OK, so that's just the convention and so now.",
            "If we want to to worry about domain adaptation, the most important issue is how do we model task relatedness?",
            "I mean domain adaptation will only work when tasks are related.",
            "But how do we mathematically?",
            "Measure this relatedness.",
            "What parameters of relatedness should we use?",
            "So if we view a task as probability distribution over domain and labels, then some very natural, very obvious notions of relatedness just suggest themselves, so one of them is saying.",
            "Let us assume that in both tasks they may differ over the distribution over there points.",
            "One of them gives some points, other give me another points, but the labels are the same.",
            "That is that for every point, the probability that for every point on every label, the probability I'll get the label 1, four point X is the same in the training data and the same in the sector in the test data.",
            "So you can think about, say, trying to learn spam filters.",
            "I want to adapt my spam filters to your email.",
            "We can make a strong assumption that what I will consider spam is exactly what you will consider spam, although this is not a very realistic assumption, this is definitely something that we may say.",
            "I mean, if this is the situation, then the tasks are well related and maybe learning to detect spam on my email will help me detect spam on your email.",
            "So this kind of assumption is called the covariate shift assumption and it was quite popular in research until quite recently.",
            "Anne.",
            "The point to note about this is that, as is, this assumption is backwards.",
            "I mean, if we just assume that.",
            "The two tasks are related to the sense that for every point in every label, the probability of the label and the one task is same as probability of the label on the add task.",
            "But we don't assume anything about the relatedness of how points are distributed then.",
            "Obviously it may be that you know.",
            "I say the points are like this.",
            "I mean you have 10 points, my 10 fingers, but these are the points that come up on my data.",
            "These are the points that come up on your data and these points are labeled one and disposal able 0 for both of us.",
            "But when I train on my data I only see the points which are labeled one and then I'll come up with the predictor that states all one.",
            "But on your points.",
            "The table zero and my prediction predictor will fail on your points, so just assuming that both of us have the same, given this point, both of us will label one, and given that point, most of us will label it 0 does not yet give me sufficient grounds to guarantee the success of domain adaptation, and I will.",
            "I will demonstrate it more formally later, so this is the covariate shift assumption and it's very tempting assumption to make.",
            "But it suffers from 2 problems one of.",
            "Is it's not realistic and the other problem is that it's useless.",
            "So we have to be careful about this assumptions.",
            "I was worried when I prepare the talk that I'll look at the program and I'll see that someone after me later talks about the Covenant shift, but I Luckily I don't think there's a situation.",
            "So what do we need further so is this the problem with my covariate shift?",
            "Was that you see my distribution?",
            "Wasn't this eight fingers and one data set just gave me just those points and the other data sets that gave me those points and they had different labels?",
            "But the covariate shift assumption held because both of them had the function which is 01010101.",
            "So what?",
            "How should I strengthen the covariate shift assumption to make it useful to make a stronger assumption that will allow me to do the main adaptation?",
            "I have to also assume that it could not be the case that I only see one set of points at the training and completely different set of points under the test.",
            "So I have to assume the resulted resulting negative assumption.",
            "Is we have to assume that there is relatedness not only in the labels but also relatedness on the unlabeled domains.",
            "The points that will come up in the unlabeled domain on the 1 task related to the distribution, the unstable distribution in the other task.",
            "So we have to make some kind of assumptions about labels and we have to make some kind of an assumptions about relatedness of the unlabeled distribution as well.",
            "End.",
            "We also consider relaxation of the covariate shift because the covariate shift.",
            "In itself, as we said, is 2.",
            "Strong in the sense of not being realistic, so we're going to make relaxations that are realistic and combine it with assumptions about the unlabeled distributions that will allow us to get some kind of guarantees.",
            "So this is the kind of assumptions that I will consider about task relatedness.",
            "And as I said, task relatedness is the key to domain adaptation.",
            "We have to understand how to measure the relatedness between tasks before we can do any analysis of domain adaptation."
        ],
        [
            "So here is our success example that several we have.",
            "Considered several models in which we could prove that under those models we have success of certain domain adaptations algorithms, and that's one of them.",
            "If I have time, I'll also mention the different one.",
            "So the example here is an example of.",
            "Part of speech tagging.",
            "And in part of speech tagging, doing inductive transfer between one domain to another domain.",
            "So part of speech tagging is the common preprocessing step in natural language processing in part of speech tagging you take a sentence and you have to predict what's the subject.",
            "What's the object?",
            "What's the verb?",
            "What's the noun and so on.",
            "So you have to.",
            "Figure out the grammatical roles of every word in your text, and we want to train.",
            "So what will we do in training is?",
            "We have students that sit down and tag every word in a big corpora of text and we pay them by the page and then we have this training data we fitted to our machine and we try to come up with some rules that will allow us to do automatic part of speech tagging.",
            "The problem is that we are training it on one type of.",
            "At.",
            "Articles and we want to apply to different kind of text and we need to do the main adaptation or inductive transfer.",
            "So this was this, a practical task and the question is can automatic part of its Tiger that was trained on one domain say legal documents.",
            "Can it be used on a different domain, say biomedical abstracts?",
            "And the issue is the discrepancy between the two.",
            "Domains, both in terms of classification in terms of the distribution of the unlabeled data.",
            "And for this problem we have the extra information we can get a lot of unlabeled target data.",
            "So if I know that I want to apply my part of speech tagger, I trained it on legal documents.",
            "I know that I want to apply it to biomedical abstract.",
            "I can go and get lots of biomedical extra without labeling.",
            "I want I don't want to sit my students again to label it and to pay them, but I can get a lot of unlabeled data.",
            "So the question is.",
            "Is this ability to get a lot of unlabeled data of the target domain?",
            "How much will it help me to adapt my part of speech tagger from one domain to the other domain?"
        ],
        [
            "So.",
            "Hey, this group in a Penn State tablets are McDonald and Pereira came up with an algorithm that they called structural correspondence learning and so they had a practical algorithm that worked.",
            "And then we came and tried to provide.",
            "A theoretical justification and understand why this algorithm works and how can we improve it.",
            "So I'm going to describe now the algorithm.",
            "So what was the algorithm the algorithm was say work like this?",
            "You choose a set of pre vote words, so you choose the kind of words which they called people words which are words that occur in both.",
            "Types of articles with similar frequency, like the terminals proposed, prepositions like end, then our it if and so on.",
            "All those words that are.",
            "Content Lestin will probably occur similarly in both types of documents.",
            "They choose those people words and then they represent every word in the text as a vector of this correlation with a small window around it with those pivot words.",
            "So let me describe it.",
            "Let me show you."
        ],
        [
            "There.",
            "So here is a an example.",
            "So what I do is if I want to.",
            "Look at this world I as well I don't view it as the word toughness, but just look at.",
            "If I take a window of three words around it, how likely what's the probability I'll see in a what's the probability that see in you?",
            "So I have a vector of probabilities over all those pivot words for every word I map it to the vector of probability of each word.",
            "What is the probability that it will occur in a small window around my world?",
            "So every word now is replaced by a vector of probabilities over the peoples words.",
            "I do it in my first domain and I do the same in my second domain.",
            "So again I mean I have here some kind of I take a window around everywhere.",
            "Dan I say if I go over my data, what's the probability that my peoples words will occur in a small window around this world?",
            "So every word isn't just now mapped to a vector of probabilities over pivot words and now I have a mapping of both domains.",
            "Into a mutual domain initial representation.",
            "The presentation of every word is a vector of probabilities and then I treat the two domains as if they were one domain.",
            "So now that I kind of cleaned the differences between the two domains, I treat I just trained my tagger on the these vectors.",
            "They trained the linear separator on this vectors in the training domain and applied it to the representation of words in the.",
            "In the target domain and it turned out."
        ],
        [
            "To work nicely.",
            "So we are trying to understand to vertically why this.",
            "A method worked.",
            "So here I just introduced a little bit of formalism.",
            "So the learner has access to a labeled training set, which I denote by South of randomly chosen a.",
            "Points generated by the training distributions, so the trend distribution I call it P of SS for source and the target distribution.",
            "I call it P of TT for target.",
            "So we get labeled distributions or labeled sample according to the domain to the source distribution and we get an unlabeled sample which is generated by the test distribution.",
            "As we said in this kind of applications you can get a lot of cheaply.",
            "You can get a lot of unlabeled target data.",
            "So we have these two types of samples and the learners task is to predict the labels and succeed according to the target distribution.",
            "So we want to be.",
            "Accurate according to the Target label."
        ],
        [
            "That's the formal setting.",
            "And now what is the?",
            "If I'm trying to abstract the algorithm and describe it in more general terms, what is doing is we are searching to embed?",
            "We want to find an embedding of both domains into some fixed feature space and we want this embedding to satisfy two requirements.",
            "One requirement is that under this embedding the two tags looks similar.",
            "The other requirement is that we have not lost a lot of information in this embedding.",
            "So let me let me explain it in a different example, so assume that I was.",
            "I mean, I until I was 30 I was only riding my bicycle.",
            "I never drove a car.",
            "Then I want to go and learn how to drive a car and I want to figure out is my experience with the bicycle relevant in any way to learning to drive a car?",
            "So what I'm trying is to embed.",
            "The parts of the bicycle to parts of the car in a way that's under this embedding, they will behave similarly.",
            "So if I have the steering whatever, how do you call it the bar?",
            "The handlebar of the bicycle instead of calling it a handlebar?",
            "I don't have a handle bar in the car, I'll call it a steering device and in the car I'll take the will, the steering wheel, and I also quality steering device.",
            "So under these embeddings the two tasks look more similar.",
            "But of course, if I just require that I want to embed the two task into a domain which they look similar, I can do it in a very trivial way.",
            "I can say, OK, I'll bet everything into one point.",
            "All of my first domain into one point.",
            "All of my other domain into one point.",
            "Now they would look very similar.",
            "They look like just one point, but I lost too much data.",
            "So how do I get a guarantee against?",
            "How do I guard against such a problem?",
            "What I require is that after this embedding, I still want to be able.",
            "In the source task to do reasonably good classification.",
            "So if I took all my bicycle and say just one point then.",
            "I will not be able to determine what should I do now when I'm riding my bicycle, but.",
            "If I just.",
            "We play say that my handlebar is a steering device, then it still flexible enough they still contain enough information.",
            "They can say now turn it right now.",
            "Turn it left.",
            "Now don't touch it, and so on.",
            "And the same, so I'm trying to find an embedding in which on one hand the two tasks look similar.",
            "On the other hand, I retained enough information so the original task can still be meaningfully predicted after the embedding, just with the representation of the embedding.",
            "So that's the idea.",
            "And then we treat the images of both tasks as if it's one task and we train on one and use it to predict."
        ],
        [
            "So the other so here this in the picture what we're doing, so we have one task is the purple Oval and the other one is the Brown one.",
            "We embed them both into a similar domain.",
            "We hope that they we want to find such a feature space in which after the embedding the two tasks look like a similar like the same distribution.",
            "But still that this is rich enough to allow me to do a good prediction over my training data where I have the labels.",
            "So that's the idea.",
            "And now we want to analyze this kind of approach.",
            "Now if I want to analyze.",
            "Quantitatively, how well this is going to succeed.",
            "The first hurdle that have overcome is I want to embed them into a feature space, such as in the feature space.",
            "The images of both distributions look the same, but how will I measure the look the same of two distributions?",
            "So the first problem first obstacle is I need a notion of saying how do I measure the similarity between two distributions?",
            "Because I want to say that after the embedding in the feature space, both distributions.",
            "Look the same, right?"
        ],
        [
            "So.",
            "Hey, there are some common measures of distribution similarity.",
            "One of them is the total variance total variance between two distributions.",
            "So now I'm just talking about completely pure mathematical task.",
            "No learning.",
            "How do we measure the similarity between two distributions over the same domain, so the total variance I have here 2 distributions.",
            "What is the total variance?",
            "Is the Super moon of all measurable sets of the weight of this set under D the difference with the weight of this set under the prime?",
            "That's a common measure of similarity between distributions or it's same as the L1 measure of similarity between distributions.",
            "Another very common one is the K Elder versions, so we have different.",
            "Several candidates for measuring the difference between two distributions, but it turns out that for our purposes, all of those measures are two sensitive.",
            "Nothing will ever look the same under such sensitive measures.",
            "And also there is another problem is that we cannot estimate how similar.",
            "Two distributions are just based on a sample, so if I'm trying to do the part of speech tagging and I take the words and embed every word is a vector of probabilities and I want to check.",
            "Did I get the same distribution of vectors of probabilities when I'm looking the biomedical data or looking at their legal documents?",
            "All I have is a sample.",
            "I want to be able to estimate just from the sample whether the two distributions are similar or not.",
            "Now there is a theorem from 2000 that shows that.",
            "Under these measures, you cannot rely abli.",
            "There's a negative strong negative result.",
            "You cannot reliably estimate the distance between the two distributions.",
            "If these are the measures of distributions of similarity that you are using.",
            "So again we want to embed our two tasks into a similar domain.",
            "We want to be able to measure this similarity on the feature space, and we need the measure of similarity that can be estimated from samples.",
            "So we had to come up."
        ],
        [
            "The new measure of similarity.",
            "And this is the measure of similarity that we introduced in a different work, and this is a similarity relative to a hypothesis class.",
            "And we say that.",
            "So I call it DH is airport is class and I'm trying to measure the similarity between two distributions.",
            "I call them U for unlabeled distributions.",
            "The recall we trying to make the enable distributions similar.",
            "So what is this similarity similarity?",
            "Is.",
            "1 minus twice the arrow of the best age.",
            "So what I'm trying to now do is the following.",
            "When will I say I have a class of predictors, say half spaces?",
            "When will I say the two distributions are similarly expect to have space if.",
            "I generate data form on distribution and I generate data from another distribution and I try to find a half space that will separate the data that was generated from one distribution from the data generated from the other distribution.",
            "If no half space manage is to separate them, then I said that there are similar.",
            "There are similar with respect to halfspaces.",
            "Halfspaces cannot distinguish between the two distributions, so I'm saying that two distributions are similar with respect to H if no element in H can.",
            "Distinguish between the two distributions I have here a pic."
        ],
        [
            "Show off.",
            "And here it is.",
            "So I have here say one distributions is the blue points, the other one is the red points.",
            "So if they look like this then.",
            "I do have a half space that does a pretty good job in separating the Reds from the Blues.",
            "The probability of error here is 1/4.",
            "So 1 -- 2 times the error will give me .5.",
            "So the distance between these two distributions respect to half spaces will be .5.",
            "On the other hand, if you two distributions look like this, then every half space that I try will have a big error.",
            "It will not be able to separate the Blues from the Reds and therefore I said that the two distributions are similar.",
            "So my distance is inversely proportional to the arrow that I can make the best minimal error that a predictor that trying to separate it to distributions can make.",
            "So I'm trying to use, say, halfspaces to.",
            "Predict part of speech tagging from one domain to another?",
            "I'm saying I say that the two domains became similar under my embedding in after the embedding.",
            "No halfspace can separate the two distributions, yes?",
            "With your post tagging example, I see that you have basically distribution over the vectors of of frequency of the right right preposition, adverb, etc.",
            "No, no, this is OK, so there's no.",
            "I have labels, blue, blue and red, right?",
            "But it's not the labels that the original.",
            "The labeling here is just saying this is what came from legal documents, and this is what came from biomedical documents.",
            "And this is a good situation where I cannot separate the two sources of documents.",
            "It's all unlabeled, but I colored it according to whether it is coming from one domain or it is coming from the other domain.",
            "OK. Space right over the unlabeled data, right?",
            "I'm measuring similarity between the unlabeled data.",
            "That's the only thing I can measure because on the target distributions I only have unlabeled data, so I cannot measure anything that involves labels.",
            "I mean our model.",
            "We assume that on the training distribution you have both points and labels, but on your target distribution you only have unlabeled points, so I can only measure things between them on the unlabeled points.",
            "So the blue and red here are not labels.",
            "The blue and red are the source of the points.",
            "It's an extension.",
            "Additional information which are available on the source and so you could have.",
            "Additional colors, yeah, but right, but I'm trying to say how different are the two distributions.",
            "So if I only have labels on one of them, it doesn't help me to tell if it's different or not from the other.",
            "The labels OK.",
            "So this is just over.",
            "Thanks for the question, because they're very good point and it's confusing.",
            "And I'm I'm try.",
            "I'm kind of doing this.",
            "You know, I'm using my class of classifiers for two purposes.",
            "Of course I'm going to use my classifiers to predict the labels, but now I'm suddenly using the classifiers for a different purpose to try to separate between the one domain another domain.",
            "And now in in this task I want to fail.",
            "I said that there are similar if I failed to separately."
        ],
        [
            "So we have a theorem from that paper in 2004 that shows that this type of measure the A between two probability distributions, can be estimated from samples, I mean.",
            "It's not important to go to the details, but I mentioned before that if you want to do the KL divergent or you want to do the total variance, then you cannot estimate it reliably from samples.",
            "But for this type of distance we proved that you can estimate it reliably from samples, so we have here a tool that can work just based on the available data.",
            "Your.",
            "Right, so the complexity of the classifier comes here is the.",
            "It has to do with the visit dimension of my class of classifiers, right?",
            "This is the shattering function.",
            "So the probability of making an error greater than epsilon depends on the shattering function of my class of classifiers.",
            "So the more complex my classifiers become, the more data I will need in order to reliably estimate how different the distributions, right?",
            "OK."
        ],
        [
            "OK, so now when we have these tools we can get.",
            "A concrete bounds that guarantees the success of this approach.",
            "The project Maps the two tasks into a common feature space and then tries to do the prediction on this common feature space so.",
            "Just to make this formula more visible, we can ignore this part.",
            "This part has goes to zero.",
            "M is the size of the labeled sample, and M prime is the size of my unlabeled sample from the targets distribution.",
            "So this part goes to zero as my sample sizes go to Infinity.",
            "So let us just ignore this part and look at what is happening here.",
            "So what is happening here is what I'm saying is that my for every H in my hypothesis space.",
            "My arrow on the target, which is what I'm trying to minimize, is bounded from my error on the source, which I can measure because that's my training data with samples +2 components and the two components is one of them.",
            "Is this distance between the two unlabeled distributions so the smaller the distance between the two unlabeled distributions, the more reliable?",
            "Is my estimate of the error based on the source when I'm trying to estimate the error on the target?",
            "So this is this new distance between the two unlabeled distributions.",
            "Plus I have this factor here and this factor here is inevitable and has to do with how the labels are related, because assuming I can have the two tasks that I have exactly the same distribution over X, But whenever one of them says 0, the other one says one right?",
            "Say that mean Francois getting exactly the same emails, but whenever there is a some advertisement for Viagra, I think it's great because I want to buy it and you think it's pump.",
            "So one we have to somehow take into account.",
            "How do we do the labeling and that's this Lambda?",
            "And what is precisely this Lambda?"
        ],
        [
            "The Lambda is.",
            "I'm looking at.",
            "I'm trying to find the.",
            "DICT with the same age on both the target and the source.",
            "So this is the infamous for all edges in my class of the error on the target, plus the error on the source.",
            "So Lambda is.",
            "How do I measure the label similarity between the two tasks?",
            "I'm trying to see what is the minimal error.",
            "Off a single hypothesis that is trying to predict on both domains.",
            "So I'm adding.",
            "I take the in from of all predictors.",
            "It's error on the target domain, plus it's error on the source domain.",
            "And with this.",
            "Relatedness between the labeling we can get our bound.",
            "So let me just go back to the bound.",
            "And.",
            "Come on.",
            "OK, so."
        ],
        [
            "You see, we say that.",
            "My error on the target for every age will be bounded by my error on the source plus these two components.",
            "So in order to make it small, what I need to do is to find a feature space in which.",
            "On one hand, the two distributions look similar.",
            "And on the other hand, I can in this feature space still have a small arrow over the source.",
            "These are the two requirements that are required from the feature space.",
            "Once I found such a physical space, the limiting factor is how related are the labels?",
            "Because I'm still I'm doing here very conservative adaptation.",
            "I'm using the same predictor that works best for the source.",
            "I'm using it from the target.",
            "The only thing I changed is each of them is using a different embedding.",
            "As in, as long as I'm doing this kind of prediction, I'm completely confined by how well are the labels related, but under this assumption I get a very provable bound that tells me.",
            "What should I try to minimize that you try to minimize?",
            "This the.",
            "Difference distance between the two distributions after the embedding in the feature space and the success after the embedding on the source data.",
            "Yes.",
            "Distance with Africa in bed.",
            "It's a tradeoff, right?",
            "You're trying to find because we know that.",
            "I mean, I could collapse everything to one point.",
            "In that case, the distance will be 0, but the Lambda will probably be big, so you're trying to find some kind of tradeoff between these two requirements.",
            "That's why I'm asking.",
            "It looks like two different term, but actually there is a balance there.",
            "Yeah, but an I'm also I'm not telling you how to find this feature space, I'm saying you see what happened here was that this is coming from domain knowledge.",
            "I mean the group of people that develop the algorithm that we saw before over those vectors.",
            "They had a lot of knowledge about language and they had an intuition that this will work.",
            "Now what I'm coming is to try to justify their intuition and saying.",
            "What did you actually do?",
            "What you actually did was you found some presentations under which the two unlabeled distributions look similar?",
            "You can still predict well, and Luckily the part of speech tagging behaves kind of similarly between the two domains, as reflected by this measure.",
            "And what I'm going to talk about next is right.",
            "How can we tighten up this bound?",
            "Can we get rid of any of those components, and then this is a very.",
            "Needed a issue.",
            "I mean, it's crucial issue.",
            "Can we tighten this bomb?"
        ],
        [
            "OK, so that was the here is.",
            "Here is some kind of demonstration of the two requirements so.",
            "When I'm saying here is I'm using my, I'm using a diamonds the diamonds to show nouns and triangles to show verbs, and we're comparing two kinds of embedding.",
            "A random embedding of the data and the embedding that was chosen by this part of speech tagger.",
            "So on a random embedding.",
            "On a random projection, I cannot distinguish nouns from verbs with a hyperspace.",
            "They look mixed.",
            "On embedding it was chosen, I can predict, distinguish them pretty well with halfspaces.",
            "But I."
        ],
        [
            "Also have to worry on can I distinguish the two distributions?",
            "So now what I have here is that the distribution of unlabeled data, the blue ones, came from financial documents, the.",
            "The blue ones came from biomedical documents.",
            "The red one came from financial documents and other random projection.",
            "I cannot distinguish them, which is very nice.",
            "I don't want to distinguish them and the projection that they chose.",
            "I can distinguish them, but what we concerned about is that some of the success of this and the success of distinguishing noun and verbs.",
            "And the sum is better on this column that in this column."
        ],
        [
            "So.",
            "The the conclusion the algorithmic conclusion is that.",
            "Try to find a feature space under which the enable distributions look the same and still there exists a good predictor for the domain distribution and then predict there.",
            "As if the two tasks."
        ],
        [
            "Are the same.",
            "OK, so.",
            "How good is this bound?",
            "So let us look at this bound now I'm.",
            "Here I raised the part that goes to 0 as a sample size, go to Infinity and just trying to look at this bound and ask how good is this bound.",
            "Now know that there is still lots of to be desired here.",
            "One thing is of course that I this is the Lambda.",
            "This is my Lambda.",
            "One thing is that what happens if the two distributions are exactly the same?",
            "If my source and tasks are exactly the same, what will happen then?",
            "What will happen then is that?",
            "What will be the distance between the two unlabeled distributions if it's the same task?",
            "It'll be 0 and.",
            "This will be the error of the best predictor.",
            "Over my target.",
            "But then what error can I guarantee?",
            "I can only guarantee three times the optimal error.",
            "Here is my error is if you have space, can do my task with Arrow .2.",
            "I'll get punched through here and point to here and point to here.",
            "So if I'm using this tool in order to just as a sanity check, I'm trying to use this tool just to measure how well can I predict from 1 task to itself I'm multiplying my arrow three times, so it's clearly not the best possible bound.",
            "It's the best that we could prove, but I pass it on to you young, bright minds.",
            "There is a lot to be improved here.",
            "It's a good direction, but it's not the end of the story.",
            "So."
        ],
        [
            "I want to talk a little bit about some other candidates of relatedness, so there is this covariate shift assumption and the question is can discover achieved assumption when they combine it with one of my other measures of similarity.",
            "So now I introduce to measure of similarity between distributions.",
            "The DH similarity between the unlabeled domains and the Lambda similarity between the labels.",
            "Can I get rid of one of them if I'm using the covariate shift right shift alone?",
            "We knew that useless, but maybe in combination with one of them it can save me the other one.",
            "And."
        ],
        [
            "The answer is that so we looked at all the pairs, so we know that.",
            "So I'm considering three types of similarity measures.",
            "One of them is good joint prediction for the two good joint predictor for the two distributions.",
            "This is the small Lambda.",
            "The other one is similarity of the unlabeled distributions.",
            "That's my DH distance.",
            "The bound that I showed you shows that if I have the combination of these two that I can get some guarantee on the behavior of the main adaptation.",
            "The question is if I consider also covariate shift, can it help me in combination with one of them to get rid of the other?",
            "And the answer is no very strong, no.",
            "So the covariate shift remains useless even if you add to it either the assumption that there is a good predictor for both distributions or the assumptions that both distributions look similar under the.",
            "Unlabeled data distributions.",
            "So I want to show you why this is the case.",
            "If you still have some cycles working."
        ],
        [
            "And these are very similar simple examples, so I have here.",
            "I mean, this is my my 8 fingers example right?",
            "I have here 2 distributions.",
            "That both of them have the same labeling.",
            "If you are in.",
            "Odd point, I label you one red if you aren't even point the label you zero.",
            "These are my two distributions, but in one distributions those odd points are heavy.",
            "Those are the points that get most of the weight and the other the blue points are heavy.",
            "So what will happen here?",
            "I do have the covariate shift assumptions in both of them.",
            "Labeling rule is the same if you are old, you are red.",
            "If you are blue.",
            "If you are even, you are blue nose 0.",
            "But if I try to learn a predictor for one and adapted to another, if I take a sample from here, I only see red points.",
            "So I can conclude that I'm going to predict everything is red and it will be a big disaster when I try to apply to.",
            "But not only this.",
            "Also we can notice that.",
            "The DH distribution here, the DH distance, is also small between them.",
            "Of course it depends.",
            "One is my age, so if my class H is the class of all intervals.",
            "If my class H or is the class of all.",
            "Initial segments of the classifier intervals.",
            "So on every interval the weight and the distribution and the weight on the distributions are very similar.",
            "So if my class ages the class of intervals is not only the have covariates such assumption, I also have small the age.",
            "And the combination still doesn't help me to predict because they might just training on this data.",
            "I see only red points.",
            "I'll choose the old one predictor and it will fail badly on the other distribution.",
            "So covariate shift plus small distance between the unlabeled distributions doesn't help me.",
            "See here, intervals cannot help me determine.",
            "They distinguish between the two distribution.",
            "Even if you don't see the target at that, you will have a very bad man.",
            "Anne.",
            "Our domain task I have a very good predictor.",
            "The old one predictor is very good in the domain because I'm unlikely to see these points, so its probability of error is very small.",
            "Set up OHSU the whole the whole one is an interval.",
            "Everything one is an interval.",
            "Yeah, it's a magma set H but certain age of our classifiers are all the possible intervals.",
            "The total set is an interval.",
            "So I have here a good, very good classifier.",
            "The all one classifier is very good here.",
            "OK. Now."
        ],
        [
            "Epsilon, which is small and it is distribution.",
            "This interval has a large weight.",
            "Everything happens here.",
            "So now now that we still have the covariate shift, both of them will give every point the same label points here will be ready in both of them and points here will be blue in both of them.",
            "And here I have small Lambda.",
            "I have a predictor that predicts simultaneously for both distributions.",
            "Very well what the predictor is, the predictor is.",
            "The predictor says everything is 1 up to this point and everything is 0 from here on.",
            "This predictor will make.",
            "0 error on the target.",
            "And it will make just two epsilon error on the domain.",
            "So I have a good joint predictor.",
            "Why did I put the two epsilon here?",
            "Because I want to show you that if I just look at what is my best predictor here.",
            "What is my best predictor on just the source domain?",
            "What would be my best predictor on just the source domain?",
            "What is the best predicting intervals on the source domain to minimize the error on the source?",
            "What is it going to be?",
            "Alright, and all red will make out of 1 here.",
            "So I have covariate shift.",
            "I have small Lambda.",
            "My best predictor, just seeing the labels here is the old Red one is going to make.",
            "Catastrophic error on the target.",
            "So having covariate shift plus small Lambda plus a good predictor for both of them still doesn't save me from disaster.",
            "OK, So what I showed you is just that these two sides of the triangle covariate shift with each of them alone is not helping us at all.",
            "Only the two, the combination that we."
        ],
        [
            "Ready now about.",
            "OK, now another has become kind of running out of time, but there's another aspect that I wanted just to discuss, just briefly is the distinction between.",
            "Conservative an adaptive domain adaptation what we did so far is conservative to meditation in the sense that.",
            "We just found the best predictor for the source domain and had some guarantee of when it will work well for the target domain, but that's not really adaptation is just saying you know I do my own thing and I hope that it will be good enough for you.",
            "But I didn't adapt, so this is what I call conservative algorithm.",
            "Other any good adaptive algorithms.",
            "Algorithms that really change the prediction when they see the unlabeled data from the target distribution.",
            "Right, so this is a very important question.",
            "Clearly there should be some algorithm like this, but what is it?",
            "So I could not come up with any but.",
            "Other guys came there, so there's a several papers by of Shiny Maureen Mansoor, both in cold so nine and you I own on that proposed the different predictor that is really adaptive.",
            "It says.",
            "OK, I should have hit this.",
            "This don't look at it right now.",
            "It says the foreign use the use the target unlabeled sample to reweigh your training data.",
            "We have training data which is labeled.",
            "We can we have unlabeled data from the target.",
            "We can try to re weigh our sample according to the target and then find the best prediction under this knew reweighing.",
            "And they claim that it works.",
            "In practice.",
            "We show that in theory it can fail very badly.",
            "So.",
            "Here is the example of."
        ],
        [
            "How it fails very badly.",
            "So look at the same example as before.",
            "I have this domain as before.",
            "Now I see the unlabeled points here.",
            "So if I'm trying to re weigh my sample according to what I saw from the target, it will tell me oh this is very important this interval.",
            "And because this interval is very important, I will not take the whole.",
            "A already assumption, because they already assumption is going to fail here and seeing a lot of unlabeled data here.",
            "I know that this part is important.",
            "So instead of taking the alright assumption, I'll take the assumption that says I'll be red and up to here and blue from hero, because I see here blue points and the unlabeled data tell me this is an important part.",
            "But he just steered me away from a good assumption.",
            "They already would have been very good for the target, the one they chose.",
            "Now after the waiting is very bad for the target so that we waiting can completely mislead you.",
            "So they just claimed that it works in practice they don't have any guarantee.",
            "This is a very short clear example why you cannot get currently like this.",
            "Of course, so the question is, under what assumptions can you get guarantees?",
            "That's what I'm asking.",
            "Hello.",
            "I mean it's a government example of very specific, and so yeah, it just tells you you have to watch out.",
            "You need.",
            "We need some extra parameters.",
            "What we have is not enough to guarantee success.",
            "We need to come up with more form, so let me jump to the my."
        ],
        [
            "Open questions.",
            "So.",
            "Wide open question is improve our basic generalization error, which is suspect should be improved a lot.",
            "And another one is to find relatedness parameters under which.",
            "Different paradigms work, so for example this reweighting, so we need new relatedness parameters clearly.",
            "And come up with new adaptive algorithms.",
            "Not just conservative domain adaptation algorithm, clearly we should be able to come up with algorithms that really adapt to the new data, but we don't have any good answers to any of those very, very basic questions.",
            "So there is a lot of open questions here.",
            "Do I have another one?",
            "Yeah, come up with more user friendly relatedness notions.",
            "Our Lambda, for example is very not user friendly because you cannot estimate it.",
            "So that many more.",
            "I mean, this is an area which is important.",
            "It's really practical and what we can do in theory so far is just scratching the surface of it.",
            "There's a lot of open questions that ought to be done, and what I'm trying to convince you is try to.",
            "Think about this problem and along these slides.",
            "So thank you very much.",
            "Other questions.",
            "Yes.",
            "Yeah, I'm sure you Friday.",
            "The missing assumption.",
            "So yeah, So what what it brings about is a different question.",
            "Note that there is a.",
            "Quite a lot of similarity between the problem was talking about here and the problem of semi supervised learning, because what I'm saying here is they have labeled data from the source and I have unlabeled data from the target.",
            "But maybe the assume that the source and target are the same.",
            "Then I have labeled data and unlabeled data and I have the same supervised learning problem and the semi supervised learning problem is a problem in which we are just as in dark as the domain adaptation.",
            "So in the in the semi supervised learning is.",
            "It's a good topic for four different lecture and there are other common assumptions like the cluster assumption, and we can construct other counterexamples that show that this may also fail, and it's not a satisfactory assumption in terms of theory.",
            "So yeah, the this problem is the manifestation problem is strictly harder than the semi supervised learning problem and semi supervised learning problem is already hard enough that we don't know much about except for counterexamples.",
            "Yes, well, you could assume that you have some label data and you're OK, so that.",
            "Look at the joint.",
            "Right, right?",
            "So that's a slightly different problem.",
            "I call it multi task learning when you have some labels from the other task and we have some work on this as well, right?",
            "It's a slightly different model, I was just.",
            "I was just focusing on this model we have.",
            "Lots of questions.",
            "It's slightly different.",
            "Some more answers, but also a lot of questions on this multi task.",
            "So the difference is that if you have few few examples from the new domain, for example, one of the questions that we would like to answer and we don't know is you know if you have a lot of examples from them.",
            "Target domain, you will ignore the source to me because you know that it's a different distribution.",
            "Where is the threshold, where?",
            "What ratio between the number of labeled points you have on the target and the number of Labor points you have from the source.",
            "Where is the critical point where you will start ignoring the the source with even we can't even give you this recipe, which is something that you might expect that theoretician will.",
            "Give the practitioners, tell them if you have only so few examples, use the the training data.",
            "If you have lots of examples, you can ignore the training data because you reduce the bias and the variance is not so bad.",
            "And we don't know yet how to give you this.",
            "So there are lots of questions in the multi task domain which is a very related problem.",
            "But I was concentrating on the domain adaptation.",
            "Yeah, so these are two very related issues.",
            "The semi supervised learning and the multi task learning.",
            "And we are have similar questions there as well."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so the emphasis of this talk is domain adaptation and theoretical analysis of that so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think that if we, I think this slide is going to be my first slide tomorrow also.",
                    "label": 0
                },
                {
                    "sent": "But we can see that the.",
                    "label": 0
                },
                {
                    "sent": "To ethical research in machine learning has contributed some very important tools to practical machine learning like boosting and support vector machines are a.",
                    "label": 1
                },
                {
                    "sent": "Very prominent examples of such contributions that grew out of theory and became important tools.",
                    "label": 0
                },
                {
                    "sent": "Essential tools in applications.",
                    "label": 1
                },
                {
                    "sent": "However, the there is a problem that the mainstream machine learning is based on simplifying assumptions that many potential applications fail to meet so that we always in order to do theory, we need to simplify the world and the mainstream theoretical machine learning makes very strong simplifying assumption about the world.",
                    "label": 0
                },
                {
                    "sent": "The problem is.",
                    "label": 0
                },
                {
                    "sent": "That in many potential applications you cannot apply the theory because it doesn't meet those assumptions, so it's a major challenge to extend our machine learning theory to cover a wider variety of realistic settings, and I think this is a big program.",
                    "label": 1
                },
                {
                    "sent": "This is direction that I think then theoretical machine learning should.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pushed out so.",
                    "label": 0
                },
                {
                    "sent": "One such limitation is that this is the central issue of this content of this workshop is the major common assumption is that the data.",
                    "label": 0
                },
                {
                    "sent": "Generating distribution is stationary that we make the assumptions that we trained on the same data that we're going to use in learning.",
                    "label": 0
                },
                {
                    "sent": "And of course we need to make some kind of such an assumption in order to make learning possible.",
                    "label": 0
                },
                {
                    "sent": "If all our experience in the past has no relevance to the future, then of course there's no point in trying to learn anything but the strong assumption here is that not only that our experience is relevant to our future, but also that.",
                    "label": 0
                },
                {
                    "sent": "It's exactly the same distribution that generated the data in the training set that we're going to be tested on, and that's a very strong assumption.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Formally, it is common to assume that both the training and the test examples are generated.",
                    "label": 1
                },
                {
                    "sent": "I I did, it is identically independently distributed by the same fixed probability distribution and.",
                    "label": 1
                },
                {
                    "sent": "This is unrealistic in many machine learning applications as they leave I mentioned and therefore I think it's great to have this workshop that focuses on this important.",
                    "label": 0
                },
                {
                    "sent": "Short full off the current theory of machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I'm going to do in this talk, I'm first going to talk the the central point of the talk is going to be domain adaptation and adaptation situation where we make relax the IID assumptions.",
                    "label": 0
                },
                {
                    "sent": "But we make another kind of simplifying assumption.",
                    "label": 0
                },
                {
                    "sent": "We assume that there are just two distributions, one on which we train and one on which we have to perform and test our performance.",
                    "label": 0
                },
                {
                    "sent": "And so I'll first talk about some general issues and some principles.",
                    "label": 1
                },
                {
                    "sent": "That we have to address, then I'll give an example of 1 success.",
                    "label": 0
                },
                {
                    "sent": "Analysis of domain adaptation where the theory could contribute to practical algorithms.",
                    "label": 1
                },
                {
                    "sent": "And then I'll discuss some inherent limitations in the approaches that we currently have told them in adaptations.",
                    "label": 0
                },
                {
                    "sent": "So there are some very.",
                    "label": 0
                },
                {
                    "sent": "Strong limits to what we can do unless we come up with.",
                    "label": 0
                },
                {
                    "sent": "Bright new innovations and then conclude with talking about open problems.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What can we do when we want to learn when the training and test distributions are different?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So I mean.",
                    "label": 0
                },
                {
                    "sent": "Of course, I mean my examples are more ways.",
                    "label": 0
                },
                {
                    "sent": "No toy real world examples in your practical examples, but one such example is driving lessons.",
                    "label": 0
                },
                {
                    "sent": "And if you, I mean, I've just been with this with my kids.",
                    "label": 0
                },
                {
                    "sent": "They learn to drive on your own car and then they go and do the test on a different car.",
                    "label": 1
                },
                {
                    "sent": "So you have to adapt what you learn to do on one car to what you have to do in on the other car.",
                    "label": 0
                },
                {
                    "sent": "We all do it intuitively, but we have to realize that it's there is a problem here of domain adaptation.",
                    "label": 0
                },
                {
                    "sent": "Either a mole computer cyantific problem is spam filters.",
                    "label": 0
                },
                {
                    "sent": "If I'm trying to train a spam filter then I have this tension between.",
                    "label": 0
                },
                {
                    "sent": "I can give it a lot of training, a lot of on my email and have a lot of data.",
                    "label": 0
                },
                {
                    "sent": "But then I want to apply the spam filter.",
                    "label": 0
                },
                {
                    "sent": "I want to sell it to you and this part filter will detect spam on your email and distribution of your emails is very different than distribution of my emails.",
                    "label": 0
                },
                {
                    "sent": "Still, we believe that there is some relationship.",
                    "label": 0
                },
                {
                    "sent": "That will make it relevant to you.",
                    "label": 0
                },
                {
                    "sent": "The lesson that I learned on training it on my email so spam filters are another very prominent.",
                    "label": 0
                },
                {
                    "sent": "Examples of it?",
                    "label": 0
                },
                {
                    "sent": "Difference between training and test distributions.",
                    "label": 1
                },
                {
                    "sent": "Another example that I'll talk about is in natural language processing task.",
                    "label": 0
                },
                {
                    "sent": "In natural language processing we usually train on one corpora of data and we have some type of documents that we train on, say biomedical documents.",
                    "label": 1
                },
                {
                    "sent": "But then we want that the lessons that we've learned will be relevant to other types of data.",
                    "label": 0
                },
                {
                    "sent": "So if I want to adapt some tool from.",
                    "label": 0
                },
                {
                    "sent": "Training it on legal documents and then I want to apply it to.",
                    "label": 0
                },
                {
                    "sent": "Say.",
                    "label": 0
                },
                {
                    "sent": "Well, Street Journal articles and so on.",
                    "label": 0
                },
                {
                    "sent": "Will the language, of course, is slightly different.",
                    "label": 0
                },
                {
                    "sent": "The domain is slightly different, but on the other hand we believe that there is relevance to the lessons that we learn from one domain and apply it to the other windows.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With our motivating examples.",
                    "label": 0
                },
                {
                    "sent": "And the goal is to figure out how and when will domain adaptation learning succeed.",
                    "label": 1
                },
                {
                    "sent": "I mean of course the main adaptation learning will not succeed always if the two tasks are not related or what we're trying to transfer is exactly the aspect of the task that is not.",
                    "label": 0
                },
                {
                    "sent": "The same then we may fail, so we have to.",
                    "label": 1
                },
                {
                    "sent": "Learn when and then how will the meditation succeed?",
                    "label": 0
                },
                {
                    "sent": "And the problem is one of those areas where there is a big gap between what is happening in practice and what is happening in theory.",
                    "label": 0
                },
                {
                    "sent": "So in practice people applied to manipulation all the time.",
                    "label": 0
                },
                {
                    "sent": "The problem is that we do not have satisfactory theoretical justification for that.",
                    "label": 1
                },
                {
                    "sent": "So practitioners, I don't know this audience how practician oriented value, but practitioners may say why should we care about theoretical justification.",
                    "label": 0
                },
                {
                    "sent": "If something works, why bother with philosophising?",
                    "label": 0
                },
                {
                    "sent": "About it, but so I give you some reasons.",
                    "label": 0
                },
                {
                    "sent": "If anybody needs those reasons.",
                    "label": 1
                },
                {
                    "sent": "So I think that what we gain from theory is that ability to provide success guarantees to common risztics.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you just see lots of.",
                    "label": 0
                },
                {
                    "sent": "I see Mail papers that say here is my new you Ristic, and here is this data that I tried it on and here is the graphs that show you my big success.",
                    "label": 0
                },
                {
                    "sent": "But of course the data that you want to try it on is a different one than what they did.",
                    "label": 0
                },
                {
                    "sent": "Do you have any guarantee that was succeeded on their experiments?",
                    "label": 1
                },
                {
                    "sent": "Will work for your data, and that's exactly where the theory comes in and can provide you with some kind of guarantees.",
                    "label": 0
                },
                {
                    "sent": "Another aspect is of course to understand under which circumstances will your algorithm work.",
                    "label": 0
                },
                {
                    "sent": "I mean, every algorithm works on the certain circumstances.",
                    "label": 0
                },
                {
                    "sent": "May fail in the others.",
                    "label": 0
                },
                {
                    "sent": "It's important for us to know.",
                    "label": 1
                },
                {
                    "sent": "Under which the contest will our algorithm work and this will help us choose the right learning paradigm for a given task.",
                    "label": 1
                },
                {
                    "sent": "If we have a long selection of different algorithms, what is the best one for my task?",
                    "label": 0
                },
                {
                    "sent": "If I have theoretical understanding of both the algorithms and posted tasks, then I can do this matching most successfully.",
                    "label": 0
                },
                {
                    "sent": "And of course to guide the development of new algorithms and so on and so forth so.",
                    "label": 0
                },
                {
                    "sent": "I hope that.",
                    "label": 0
                },
                {
                    "sent": "I managed to convince of that people are convinced already that we do need to, right?",
                    "label": 0
                },
                {
                    "sent": "It's not enough that something works we.",
                    "label": 0
                },
                {
                    "sent": "Do you have a benefits from having theoretical understanding of?",
                    "label": 0
                },
                {
                    "sent": "Why it works?",
                    "label": 0
                },
                {
                    "sent": "So this is what we're trying to do.",
                    "label": 0
                },
                {
                    "sent": "So domain adaptation works all the time and we try to understand theoretically why.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An under what circumstances it will work, so my I'll take a very simplistic model and the Convention will be that the learning task is a distribution of points and labels.",
                    "label": 0
                },
                {
                    "sent": "So we just take the simplistic classification tasks.",
                    "label": 0
                },
                {
                    "sent": "You have domain points.",
                    "label": 0
                },
                {
                    "sent": "Each of them is you have to predict the label and the label is binary is a 01, and we assume that there is some underlying probability distribution that generates the points and generates their labels.",
                    "label": 0
                },
                {
                    "sent": "So I don't assume here there is a fixed correct label for every point.",
                    "label": 0
                },
                {
                    "sent": "So it but that just the distribution that for every domain point there is some probability they'll get label zero and some probability will get label one.",
                    "label": 0
                },
                {
                    "sent": "So for me a learning task is such a probability distribution over domain and labels.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's just the convention and so now.",
                    "label": 0
                },
                {
                    "sent": "If we want to to worry about domain adaptation, the most important issue is how do we model task relatedness?",
                    "label": 0
                },
                {
                    "sent": "I mean domain adaptation will only work when tasks are related.",
                    "label": 0
                },
                {
                    "sent": "But how do we mathematically?",
                    "label": 0
                },
                {
                    "sent": "Measure this relatedness.",
                    "label": 0
                },
                {
                    "sent": "What parameters of relatedness should we use?",
                    "label": 0
                },
                {
                    "sent": "So if we view a task as probability distribution over domain and labels, then some very natural, very obvious notions of relatedness just suggest themselves, so one of them is saying.",
                    "label": 0
                },
                {
                    "sent": "Let us assume that in both tasks they may differ over the distribution over there points.",
                    "label": 1
                },
                {
                    "sent": "One of them gives some points, other give me another points, but the labels are the same.",
                    "label": 0
                },
                {
                    "sent": "That is that for every point, the probability that for every point on every label, the probability I'll get the label 1, four point X is the same in the training data and the same in the sector in the test data.",
                    "label": 0
                },
                {
                    "sent": "So you can think about, say, trying to learn spam filters.",
                    "label": 0
                },
                {
                    "sent": "I want to adapt my spam filters to your email.",
                    "label": 0
                },
                {
                    "sent": "We can make a strong assumption that what I will consider spam is exactly what you will consider spam, although this is not a very realistic assumption, this is definitely something that we may say.",
                    "label": 0
                },
                {
                    "sent": "I mean, if this is the situation, then the tasks are well related and maybe learning to detect spam on my email will help me detect spam on your email.",
                    "label": 0
                },
                {
                    "sent": "So this kind of assumption is called the covariate shift assumption and it was quite popular in research until quite recently.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The point to note about this is that, as is, this assumption is backwards.",
                    "label": 0
                },
                {
                    "sent": "I mean, if we just assume that.",
                    "label": 0
                },
                {
                    "sent": "The two tasks are related to the sense that for every point in every label, the probability of the label and the one task is same as probability of the label on the add task.",
                    "label": 0
                },
                {
                    "sent": "But we don't assume anything about the relatedness of how points are distributed then.",
                    "label": 0
                },
                {
                    "sent": "Obviously it may be that you know.",
                    "label": 0
                },
                {
                    "sent": "I say the points are like this.",
                    "label": 0
                },
                {
                    "sent": "I mean you have 10 points, my 10 fingers, but these are the points that come up on my data.",
                    "label": 0
                },
                {
                    "sent": "These are the points that come up on your data and these points are labeled one and disposal able 0 for both of us.",
                    "label": 0
                },
                {
                    "sent": "But when I train on my data I only see the points which are labeled one and then I'll come up with the predictor that states all one.",
                    "label": 0
                },
                {
                    "sent": "But on your points.",
                    "label": 0
                },
                {
                    "sent": "The table zero and my prediction predictor will fail on your points, so just assuming that both of us have the same, given this point, both of us will label one, and given that point, most of us will label it 0 does not yet give me sufficient grounds to guarantee the success of domain adaptation, and I will.",
                    "label": 1
                },
                {
                    "sent": "I will demonstrate it more formally later, so this is the covariate shift assumption and it's very tempting assumption to make.",
                    "label": 0
                },
                {
                    "sent": "But it suffers from 2 problems one of.",
                    "label": 0
                },
                {
                    "sent": "Is it's not realistic and the other problem is that it's useless.",
                    "label": 0
                },
                {
                    "sent": "So we have to be careful about this assumptions.",
                    "label": 0
                },
                {
                    "sent": "I was worried when I prepare the talk that I'll look at the program and I'll see that someone after me later talks about the Covenant shift, but I Luckily I don't think there's a situation.",
                    "label": 0
                },
                {
                    "sent": "So what do we need further so is this the problem with my covariate shift?",
                    "label": 0
                },
                {
                    "sent": "Was that you see my distribution?",
                    "label": 0
                },
                {
                    "sent": "Wasn't this eight fingers and one data set just gave me just those points and the other data sets that gave me those points and they had different labels?",
                    "label": 0
                },
                {
                    "sent": "But the covariate shift assumption held because both of them had the function which is 01010101.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "How should I strengthen the covariate shift assumption to make it useful to make a stronger assumption that will allow me to do the main adaptation?",
                    "label": 0
                },
                {
                    "sent": "I have to also assume that it could not be the case that I only see one set of points at the training and completely different set of points under the test.",
                    "label": 0
                },
                {
                    "sent": "So I have to assume the resulted resulting negative assumption.",
                    "label": 0
                },
                {
                    "sent": "Is we have to assume that there is relatedness not only in the labels but also relatedness on the unlabeled domains.",
                    "label": 0
                },
                {
                    "sent": "The points that will come up in the unlabeled domain on the 1 task related to the distribution, the unstable distribution in the other task.",
                    "label": 0
                },
                {
                    "sent": "So we have to make some kind of assumptions about labels and we have to make some kind of an assumptions about relatedness of the unlabeled distribution as well.",
                    "label": 0
                },
                {
                    "sent": "End.",
                    "label": 0
                },
                {
                    "sent": "We also consider relaxation of the covariate shift because the covariate shift.",
                    "label": 0
                },
                {
                    "sent": "In itself, as we said, is 2.",
                    "label": 0
                },
                {
                    "sent": "Strong in the sense of not being realistic, so we're going to make relaxations that are realistic and combine it with assumptions about the unlabeled distributions that will allow us to get some kind of guarantees.",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of assumptions that I will consider about task relatedness.",
                    "label": 0
                },
                {
                    "sent": "And as I said, task relatedness is the key to domain adaptation.",
                    "label": 0
                },
                {
                    "sent": "We have to understand how to measure the relatedness between tasks before we can do any analysis of domain adaptation.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is our success example that several we have.",
                    "label": 0
                },
                {
                    "sent": "Considered several models in which we could prove that under those models we have success of certain domain adaptations algorithms, and that's one of them.",
                    "label": 0
                },
                {
                    "sent": "If I have time, I'll also mention the different one.",
                    "label": 0
                },
                {
                    "sent": "So the example here is an example of.",
                    "label": 0
                },
                {
                    "sent": "Part of speech tagging.",
                    "label": 0
                },
                {
                    "sent": "And in part of speech tagging, doing inductive transfer between one domain to another domain.",
                    "label": 0
                },
                {
                    "sent": "So part of speech tagging is the common preprocessing step in natural language processing in part of speech tagging you take a sentence and you have to predict what's the subject.",
                    "label": 0
                },
                {
                    "sent": "What's the object?",
                    "label": 0
                },
                {
                    "sent": "What's the verb?",
                    "label": 0
                },
                {
                    "sent": "What's the noun and so on.",
                    "label": 0
                },
                {
                    "sent": "So you have to.",
                    "label": 0
                },
                {
                    "sent": "Figure out the grammatical roles of every word in your text, and we want to train.",
                    "label": 0
                },
                {
                    "sent": "So what will we do in training is?",
                    "label": 0
                },
                {
                    "sent": "We have students that sit down and tag every word in a big corpora of text and we pay them by the page and then we have this training data we fitted to our machine and we try to come up with some rules that will allow us to do automatic part of speech tagging.",
                    "label": 0
                },
                {
                    "sent": "The problem is that we are training it on one type of.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "Articles and we want to apply to different kind of text and we need to do the main adaptation or inductive transfer.",
                    "label": 0
                },
                {
                    "sent": "So this was this, a practical task and the question is can automatic part of its Tiger that was trained on one domain say legal documents.",
                    "label": 1
                },
                {
                    "sent": "Can it be used on a different domain, say biomedical abstracts?",
                    "label": 1
                },
                {
                    "sent": "And the issue is the discrepancy between the two.",
                    "label": 0
                },
                {
                    "sent": "Domains, both in terms of classification in terms of the distribution of the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "And for this problem we have the extra information we can get a lot of unlabeled target data.",
                    "label": 0
                },
                {
                    "sent": "So if I know that I want to apply my part of speech tagger, I trained it on legal documents.",
                    "label": 0
                },
                {
                    "sent": "I know that I want to apply it to biomedical abstract.",
                    "label": 0
                },
                {
                    "sent": "I can go and get lots of biomedical extra without labeling.",
                    "label": 0
                },
                {
                    "sent": "I want I don't want to sit my students again to label it and to pay them, but I can get a lot of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So the question is.",
                    "label": 0
                },
                {
                    "sent": "Is this ability to get a lot of unlabeled data of the target domain?",
                    "label": 0
                },
                {
                    "sent": "How much will it help me to adapt my part of speech tagger from one domain to the other domain?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Hey, this group in a Penn State tablets are McDonald and Pereira came up with an algorithm that they called structural correspondence learning and so they had a practical algorithm that worked.",
                    "label": 0
                },
                {
                    "sent": "And then we came and tried to provide.",
                    "label": 0
                },
                {
                    "sent": "A theoretical justification and understand why this algorithm works and how can we improve it.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to describe now the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what was the algorithm the algorithm was say work like this?",
                    "label": 0
                },
                {
                    "sent": "You choose a set of pre vote words, so you choose the kind of words which they called people words which are words that occur in both.",
                    "label": 1
                },
                {
                    "sent": "Types of articles with similar frequency, like the terminals proposed, prepositions like end, then our it if and so on.",
                    "label": 0
                },
                {
                    "sent": "All those words that are.",
                    "label": 0
                },
                {
                    "sent": "Content Lestin will probably occur similarly in both types of documents.",
                    "label": 0
                },
                {
                    "sent": "They choose those people words and then they represent every word in the text as a vector of this correlation with a small window around it with those pivot words.",
                    "label": 1
                },
                {
                    "sent": "So let me describe it.",
                    "label": 0
                },
                {
                    "sent": "Let me show you.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "So here is a an example.",
                    "label": 0
                },
                {
                    "sent": "So what I do is if I want to.",
                    "label": 0
                },
                {
                    "sent": "Look at this world I as well I don't view it as the word toughness, but just look at.",
                    "label": 0
                },
                {
                    "sent": "If I take a window of three words around it, how likely what's the probability I'll see in a what's the probability that see in you?",
                    "label": 0
                },
                {
                    "sent": "So I have a vector of probabilities over all those pivot words for every word I map it to the vector of probability of each word.",
                    "label": 0
                },
                {
                    "sent": "What is the probability that it will occur in a small window around my world?",
                    "label": 0
                },
                {
                    "sent": "So every word now is replaced by a vector of probabilities over the peoples words.",
                    "label": 0
                },
                {
                    "sent": "I do it in my first domain and I do the same in my second domain.",
                    "label": 0
                },
                {
                    "sent": "So again I mean I have here some kind of I take a window around everywhere.",
                    "label": 0
                },
                {
                    "sent": "Dan I say if I go over my data, what's the probability that my peoples words will occur in a small window around this world?",
                    "label": 0
                },
                {
                    "sent": "So every word isn't just now mapped to a vector of probabilities over pivot words and now I have a mapping of both domains.",
                    "label": 0
                },
                {
                    "sent": "Into a mutual domain initial representation.",
                    "label": 0
                },
                {
                    "sent": "The presentation of every word is a vector of probabilities and then I treat the two domains as if they were one domain.",
                    "label": 0
                },
                {
                    "sent": "So now that I kind of cleaned the differences between the two domains, I treat I just trained my tagger on the these vectors.",
                    "label": 0
                },
                {
                    "sent": "They trained the linear separator on this vectors in the training domain and applied it to the representation of words in the.",
                    "label": 0
                },
                {
                    "sent": "In the target domain and it turned out.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To work nicely.",
                    "label": 0
                },
                {
                    "sent": "So we are trying to understand to vertically why this.",
                    "label": 0
                },
                {
                    "sent": "A method worked.",
                    "label": 0
                },
                {
                    "sent": "So here I just introduced a little bit of formalism.",
                    "label": 0
                },
                {
                    "sent": "So the learner has access to a labeled training set, which I denote by South of randomly chosen a.",
                    "label": 1
                },
                {
                    "sent": "Points generated by the training distributions, so the trend distribution I call it P of SS for source and the target distribution.",
                    "label": 0
                },
                {
                    "sent": "I call it P of TT for target.",
                    "label": 0
                },
                {
                    "sent": "So we get labeled distributions or labeled sample according to the domain to the source distribution and we get an unlabeled sample which is generated by the test distribution.",
                    "label": 0
                },
                {
                    "sent": "As we said in this kind of applications you can get a lot of cheaply.",
                    "label": 0
                },
                {
                    "sent": "You can get a lot of unlabeled target data.",
                    "label": 0
                },
                {
                    "sent": "So we have these two types of samples and the learners task is to predict the labels and succeed according to the target distribution.",
                    "label": 1
                },
                {
                    "sent": "So we want to be.",
                    "label": 0
                },
                {
                    "sent": "Accurate according to the Target label.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's the formal setting.",
                    "label": 0
                },
                {
                    "sent": "And now what is the?",
                    "label": 0
                },
                {
                    "sent": "If I'm trying to abstract the algorithm and describe it in more general terms, what is doing is we are searching to embed?",
                    "label": 0
                },
                {
                    "sent": "We want to find an embedding of both domains into some fixed feature space and we want this embedding to satisfy two requirements.",
                    "label": 0
                },
                {
                    "sent": "One requirement is that under this embedding the two tags looks similar.",
                    "label": 0
                },
                {
                    "sent": "The other requirement is that we have not lost a lot of information in this embedding.",
                    "label": 0
                },
                {
                    "sent": "So let me let me explain it in a different example, so assume that I was.",
                    "label": 0
                },
                {
                    "sent": "I mean, I until I was 30 I was only riding my bicycle.",
                    "label": 0
                },
                {
                    "sent": "I never drove a car.",
                    "label": 0
                },
                {
                    "sent": "Then I want to go and learn how to drive a car and I want to figure out is my experience with the bicycle relevant in any way to learning to drive a car?",
                    "label": 0
                },
                {
                    "sent": "So what I'm trying is to embed.",
                    "label": 1
                },
                {
                    "sent": "The parts of the bicycle to parts of the car in a way that's under this embedding, they will behave similarly.",
                    "label": 0
                },
                {
                    "sent": "So if I have the steering whatever, how do you call it the bar?",
                    "label": 0
                },
                {
                    "sent": "The handlebar of the bicycle instead of calling it a handlebar?",
                    "label": 0
                },
                {
                    "sent": "I don't have a handle bar in the car, I'll call it a steering device and in the car I'll take the will, the steering wheel, and I also quality steering device.",
                    "label": 0
                },
                {
                    "sent": "So under these embeddings the two tasks look more similar.",
                    "label": 0
                },
                {
                    "sent": "But of course, if I just require that I want to embed the two task into a domain which they look similar, I can do it in a very trivial way.",
                    "label": 0
                },
                {
                    "sent": "I can say, OK, I'll bet everything into one point.",
                    "label": 0
                },
                {
                    "sent": "All of my first domain into one point.",
                    "label": 0
                },
                {
                    "sent": "All of my other domain into one point.",
                    "label": 0
                },
                {
                    "sent": "Now they would look very similar.",
                    "label": 0
                },
                {
                    "sent": "They look like just one point, but I lost too much data.",
                    "label": 0
                },
                {
                    "sent": "So how do I get a guarantee against?",
                    "label": 0
                },
                {
                    "sent": "How do I guard against such a problem?",
                    "label": 0
                },
                {
                    "sent": "What I require is that after this embedding, I still want to be able.",
                    "label": 0
                },
                {
                    "sent": "In the source task to do reasonably good classification.",
                    "label": 1
                },
                {
                    "sent": "So if I took all my bicycle and say just one point then.",
                    "label": 0
                },
                {
                    "sent": "I will not be able to determine what should I do now when I'm riding my bicycle, but.",
                    "label": 0
                },
                {
                    "sent": "If I just.",
                    "label": 0
                },
                {
                    "sent": "We play say that my handlebar is a steering device, then it still flexible enough they still contain enough information.",
                    "label": 0
                },
                {
                    "sent": "They can say now turn it right now.",
                    "label": 0
                },
                {
                    "sent": "Turn it left.",
                    "label": 0
                },
                {
                    "sent": "Now don't touch it, and so on.",
                    "label": 0
                },
                {
                    "sent": "And the same, so I'm trying to find an embedding in which on one hand the two tasks look similar.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, I retained enough information so the original task can still be meaningfully predicted after the embedding, just with the representation of the embedding.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea.",
                    "label": 1
                },
                {
                    "sent": "And then we treat the images of both tasks as if it's one task and we train on one and use it to predict.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the other so here this in the picture what we're doing, so we have one task is the purple Oval and the other one is the Brown one.",
                    "label": 0
                },
                {
                    "sent": "We embed them both into a similar domain.",
                    "label": 0
                },
                {
                    "sent": "We hope that they we want to find such a feature space in which after the embedding the two tasks look like a similar like the same distribution.",
                    "label": 0
                },
                {
                    "sent": "But still that this is rich enough to allow me to do a good prediction over my training data where I have the labels.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea.",
                    "label": 0
                },
                {
                    "sent": "And now we want to analyze this kind of approach.",
                    "label": 0
                },
                {
                    "sent": "Now if I want to analyze.",
                    "label": 0
                },
                {
                    "sent": "Quantitatively, how well this is going to succeed.",
                    "label": 0
                },
                {
                    "sent": "The first hurdle that have overcome is I want to embed them into a feature space, such as in the feature space.",
                    "label": 0
                },
                {
                    "sent": "The images of both distributions look the same, but how will I measure the look the same of two distributions?",
                    "label": 0
                },
                {
                    "sent": "So the first problem first obstacle is I need a notion of saying how do I measure the similarity between two distributions?",
                    "label": 0
                },
                {
                    "sent": "Because I want to say that after the embedding in the feature space, both distributions.",
                    "label": 1
                },
                {
                    "sent": "Look the same, right?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Hey, there are some common measures of distribution similarity.",
                    "label": 1
                },
                {
                    "sent": "One of them is the total variance total variance between two distributions.",
                    "label": 0
                },
                {
                    "sent": "So now I'm just talking about completely pure mathematical task.",
                    "label": 0
                },
                {
                    "sent": "No learning.",
                    "label": 0
                },
                {
                    "sent": "How do we measure the similarity between two distributions over the same domain, so the total variance I have here 2 distributions.",
                    "label": 1
                },
                {
                    "sent": "What is the total variance?",
                    "label": 0
                },
                {
                    "sent": "Is the Super moon of all measurable sets of the weight of this set under D the difference with the weight of this set under the prime?",
                    "label": 0
                },
                {
                    "sent": "That's a common measure of similarity between distributions or it's same as the L1 measure of similarity between distributions.",
                    "label": 1
                },
                {
                    "sent": "Another very common one is the K Elder versions, so we have different.",
                    "label": 0
                },
                {
                    "sent": "Several candidates for measuring the difference between two distributions, but it turns out that for our purposes, all of those measures are two sensitive.",
                    "label": 0
                },
                {
                    "sent": "Nothing will ever look the same under such sensitive measures.",
                    "label": 0
                },
                {
                    "sent": "And also there is another problem is that we cannot estimate how similar.",
                    "label": 0
                },
                {
                    "sent": "Two distributions are just based on a sample, so if I'm trying to do the part of speech tagging and I take the words and embed every word is a vector of probabilities and I want to check.",
                    "label": 0
                },
                {
                    "sent": "Did I get the same distribution of vectors of probabilities when I'm looking the biomedical data or looking at their legal documents?",
                    "label": 0
                },
                {
                    "sent": "All I have is a sample.",
                    "label": 0
                },
                {
                    "sent": "I want to be able to estimate just from the sample whether the two distributions are similar or not.",
                    "label": 1
                },
                {
                    "sent": "Now there is a theorem from 2000 that shows that.",
                    "label": 1
                },
                {
                    "sent": "Under these measures, you cannot rely abli.",
                    "label": 0
                },
                {
                    "sent": "There's a negative strong negative result.",
                    "label": 0
                },
                {
                    "sent": "You cannot reliably estimate the distance between the two distributions.",
                    "label": 0
                },
                {
                    "sent": "If these are the measures of distributions of similarity that you are using.",
                    "label": 0
                },
                {
                    "sent": "So again we want to embed our two tasks into a similar domain.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to measure this similarity on the feature space, and we need the measure of similarity that can be estimated from samples.",
                    "label": 0
                },
                {
                    "sent": "So we had to come up.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The new measure of similarity.",
                    "label": 0
                },
                {
                    "sent": "And this is the measure of similarity that we introduced in a different work, and this is a similarity relative to a hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "And we say that.",
                    "label": 0
                },
                {
                    "sent": "So I call it DH is airport is class and I'm trying to measure the similarity between two distributions.",
                    "label": 0
                },
                {
                    "sent": "I call them U for unlabeled distributions.",
                    "label": 0
                },
                {
                    "sent": "The recall we trying to make the enable distributions similar.",
                    "label": 0
                },
                {
                    "sent": "So what is this similarity similarity?",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "1 minus twice the arrow of the best age.",
                    "label": 0
                },
                {
                    "sent": "So what I'm trying to now do is the following.",
                    "label": 0
                },
                {
                    "sent": "When will I say I have a class of predictors, say half spaces?",
                    "label": 1
                },
                {
                    "sent": "When will I say the two distributions are similarly expect to have space if.",
                    "label": 0
                },
                {
                    "sent": "I generate data form on distribution and I generate data from another distribution and I try to find a half space that will separate the data that was generated from one distribution from the data generated from the other distribution.",
                    "label": 0
                },
                {
                    "sent": "If no half space manage is to separate them, then I said that there are similar.",
                    "label": 0
                },
                {
                    "sent": "There are similar with respect to halfspaces.",
                    "label": 0
                },
                {
                    "sent": "Halfspaces cannot distinguish between the two distributions, so I'm saying that two distributions are similar with respect to H if no element in H can.",
                    "label": 1
                },
                {
                    "sent": "Distinguish between the two distributions I have here a pic.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Show off.",
                    "label": 0
                },
                {
                    "sent": "And here it is.",
                    "label": 0
                },
                {
                    "sent": "So I have here say one distributions is the blue points, the other one is the red points.",
                    "label": 0
                },
                {
                    "sent": "So if they look like this then.",
                    "label": 0
                },
                {
                    "sent": "I do have a half space that does a pretty good job in separating the Reds from the Blues.",
                    "label": 0
                },
                {
                    "sent": "The probability of error here is 1/4.",
                    "label": 0
                },
                {
                    "sent": "So 1 -- 2 times the error will give me .5.",
                    "label": 0
                },
                {
                    "sent": "So the distance between these two distributions respect to half spaces will be .5.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you two distributions look like this, then every half space that I try will have a big error.",
                    "label": 0
                },
                {
                    "sent": "It will not be able to separate the Blues from the Reds and therefore I said that the two distributions are similar.",
                    "label": 0
                },
                {
                    "sent": "So my distance is inversely proportional to the arrow that I can make the best minimal error that a predictor that trying to separate it to distributions can make.",
                    "label": 0
                },
                {
                    "sent": "So I'm trying to use, say, halfspaces to.",
                    "label": 0
                },
                {
                    "sent": "Predict part of speech tagging from one domain to another?",
                    "label": 0
                },
                {
                    "sent": "I'm saying I say that the two domains became similar under my embedding in after the embedding.",
                    "label": 0
                },
                {
                    "sent": "No halfspace can separate the two distributions, yes?",
                    "label": 0
                },
                {
                    "sent": "With your post tagging example, I see that you have basically distribution over the vectors of of frequency of the right right preposition, adverb, etc.",
                    "label": 0
                },
                {
                    "sent": "No, no, this is OK, so there's no.",
                    "label": 0
                },
                {
                    "sent": "I have labels, blue, blue and red, right?",
                    "label": 0
                },
                {
                    "sent": "But it's not the labels that the original.",
                    "label": 0
                },
                {
                    "sent": "The labeling here is just saying this is what came from legal documents, and this is what came from biomedical documents.",
                    "label": 0
                },
                {
                    "sent": "And this is a good situation where I cannot separate the two sources of documents.",
                    "label": 0
                },
                {
                    "sent": "It's all unlabeled, but I colored it according to whether it is coming from one domain or it is coming from the other domain.",
                    "label": 0
                },
                {
                    "sent": "OK. Space right over the unlabeled data, right?",
                    "label": 0
                },
                {
                    "sent": "I'm measuring similarity between the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "That's the only thing I can measure because on the target distributions I only have unlabeled data, so I cannot measure anything that involves labels.",
                    "label": 0
                },
                {
                    "sent": "I mean our model.",
                    "label": 0
                },
                {
                    "sent": "We assume that on the training distribution you have both points and labels, but on your target distribution you only have unlabeled points, so I can only measure things between them on the unlabeled points.",
                    "label": 0
                },
                {
                    "sent": "So the blue and red here are not labels.",
                    "label": 0
                },
                {
                    "sent": "The blue and red are the source of the points.",
                    "label": 1
                },
                {
                    "sent": "It's an extension.",
                    "label": 0
                },
                {
                    "sent": "Additional information which are available on the source and so you could have.",
                    "label": 0
                },
                {
                    "sent": "Additional colors, yeah, but right, but I'm trying to say how different are the two distributions.",
                    "label": 0
                },
                {
                    "sent": "So if I only have labels on one of them, it doesn't help me to tell if it's different or not from the other.",
                    "label": 0
                },
                {
                    "sent": "The labels OK.",
                    "label": 0
                },
                {
                    "sent": "So this is just over.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the question, because they're very good point and it's confusing.",
                    "label": 0
                },
                {
                    "sent": "And I'm I'm try.",
                    "label": 0
                },
                {
                    "sent": "I'm kind of doing this.",
                    "label": 0
                },
                {
                    "sent": "You know, I'm using my class of classifiers for two purposes.",
                    "label": 0
                },
                {
                    "sent": "Of course I'm going to use my classifiers to predict the labels, but now I'm suddenly using the classifiers for a different purpose to try to separate between the one domain another domain.",
                    "label": 0
                },
                {
                    "sent": "And now in in this task I want to fail.",
                    "label": 0
                },
                {
                    "sent": "I said that there are similar if I failed to separately.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have a theorem from that paper in 2004 that shows that this type of measure the A between two probability distributions, can be estimated from samples, I mean.",
                    "label": 1
                },
                {
                    "sent": "It's not important to go to the details, but I mentioned before that if you want to do the KL divergent or you want to do the total variance, then you cannot estimate it reliably from samples.",
                    "label": 0
                },
                {
                    "sent": "But for this type of distance we proved that you can estimate it reliably from samples, so we have here a tool that can work just based on the available data.",
                    "label": 0
                },
                {
                    "sent": "Your.",
                    "label": 0
                },
                {
                    "sent": "Right, so the complexity of the classifier comes here is the.",
                    "label": 0
                },
                {
                    "sent": "It has to do with the visit dimension of my class of classifiers, right?",
                    "label": 0
                },
                {
                    "sent": "This is the shattering function.",
                    "label": 0
                },
                {
                    "sent": "So the probability of making an error greater than epsilon depends on the shattering function of my class of classifiers.",
                    "label": 0
                },
                {
                    "sent": "So the more complex my classifiers become, the more data I will need in order to reliably estimate how different the distributions, right?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now when we have these tools we can get.",
                    "label": 0
                },
                {
                    "sent": "A concrete bounds that guarantees the success of this approach.",
                    "label": 0
                },
                {
                    "sent": "The project Maps the two tasks into a common feature space and then tries to do the prediction on this common feature space so.",
                    "label": 0
                },
                {
                    "sent": "Just to make this formula more visible, we can ignore this part.",
                    "label": 0
                },
                {
                    "sent": "This part has goes to zero.",
                    "label": 0
                },
                {
                    "sent": "M is the size of the labeled sample, and M prime is the size of my unlabeled sample from the targets distribution.",
                    "label": 0
                },
                {
                    "sent": "So this part goes to zero as my sample sizes go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So let us just ignore this part and look at what is happening here.",
                    "label": 0
                },
                {
                    "sent": "So what is happening here is what I'm saying is that my for every H in my hypothesis space.",
                    "label": 0
                },
                {
                    "sent": "My arrow on the target, which is what I'm trying to minimize, is bounded from my error on the source, which I can measure because that's my training data with samples +2 components and the two components is one of them.",
                    "label": 0
                },
                {
                    "sent": "Is this distance between the two unlabeled distributions so the smaller the distance between the two unlabeled distributions, the more reliable?",
                    "label": 0
                },
                {
                    "sent": "Is my estimate of the error based on the source when I'm trying to estimate the error on the target?",
                    "label": 0
                },
                {
                    "sent": "So this is this new distance between the two unlabeled distributions.",
                    "label": 0
                },
                {
                    "sent": "Plus I have this factor here and this factor here is inevitable and has to do with how the labels are related, because assuming I can have the two tasks that I have exactly the same distribution over X, But whenever one of them says 0, the other one says one right?",
                    "label": 0
                },
                {
                    "sent": "Say that mean Francois getting exactly the same emails, but whenever there is a some advertisement for Viagra, I think it's great because I want to buy it and you think it's pump.",
                    "label": 0
                },
                {
                    "sent": "So one we have to somehow take into account.",
                    "label": 0
                },
                {
                    "sent": "How do we do the labeling and that's this Lambda?",
                    "label": 0
                },
                {
                    "sent": "And what is precisely this Lambda?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The Lambda is.",
                    "label": 0
                },
                {
                    "sent": "I'm looking at.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to find the.",
                    "label": 0
                },
                {
                    "sent": "DICT with the same age on both the target and the source.",
                    "label": 0
                },
                {
                    "sent": "So this is the infamous for all edges in my class of the error on the target, plus the error on the source.",
                    "label": 1
                },
                {
                    "sent": "So Lambda is.",
                    "label": 1
                },
                {
                    "sent": "How do I measure the label similarity between the two tasks?",
                    "label": 0
                },
                {
                    "sent": "I'm trying to see what is the minimal error.",
                    "label": 0
                },
                {
                    "sent": "Off a single hypothesis that is trying to predict on both domains.",
                    "label": 0
                },
                {
                    "sent": "So I'm adding.",
                    "label": 0
                },
                {
                    "sent": "I take the in from of all predictors.",
                    "label": 0
                },
                {
                    "sent": "It's error on the target domain, plus it's error on the source domain.",
                    "label": 0
                },
                {
                    "sent": "And with this.",
                    "label": 0
                },
                {
                    "sent": "Relatedness between the labeling we can get our bound.",
                    "label": 0
                },
                {
                    "sent": "So let me just go back to the bound.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Come on.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You see, we say that.",
                    "label": 0
                },
                {
                    "sent": "My error on the target for every age will be bounded by my error on the source plus these two components.",
                    "label": 0
                },
                {
                    "sent": "So in order to make it small, what I need to do is to find a feature space in which.",
                    "label": 0
                },
                {
                    "sent": "On one hand, the two distributions look similar.",
                    "label": 0
                },
                {
                    "sent": "And on the other hand, I can in this feature space still have a small arrow over the source.",
                    "label": 0
                },
                {
                    "sent": "These are the two requirements that are required from the feature space.",
                    "label": 0
                },
                {
                    "sent": "Once I found such a physical space, the limiting factor is how related are the labels?",
                    "label": 0
                },
                {
                    "sent": "Because I'm still I'm doing here very conservative adaptation.",
                    "label": 0
                },
                {
                    "sent": "I'm using the same predictor that works best for the source.",
                    "label": 0
                },
                {
                    "sent": "I'm using it from the target.",
                    "label": 0
                },
                {
                    "sent": "The only thing I changed is each of them is using a different embedding.",
                    "label": 0
                },
                {
                    "sent": "As in, as long as I'm doing this kind of prediction, I'm completely confined by how well are the labels related, but under this assumption I get a very provable bound that tells me.",
                    "label": 0
                },
                {
                    "sent": "What should I try to minimize that you try to minimize?",
                    "label": 0
                },
                {
                    "sent": "This the.",
                    "label": 0
                },
                {
                    "sent": "Difference distance between the two distributions after the embedding in the feature space and the success after the embedding on the source data.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Distance with Africa in bed.",
                    "label": 0
                },
                {
                    "sent": "It's a tradeoff, right?",
                    "label": 0
                },
                {
                    "sent": "You're trying to find because we know that.",
                    "label": 0
                },
                {
                    "sent": "I mean, I could collapse everything to one point.",
                    "label": 0
                },
                {
                    "sent": "In that case, the distance will be 0, but the Lambda will probably be big, so you're trying to find some kind of tradeoff between these two requirements.",
                    "label": 0
                },
                {
                    "sent": "That's why I'm asking.",
                    "label": 0
                },
                {
                    "sent": "It looks like two different term, but actually there is a balance there.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but an I'm also I'm not telling you how to find this feature space, I'm saying you see what happened here was that this is coming from domain knowledge.",
                    "label": 0
                },
                {
                    "sent": "I mean the group of people that develop the algorithm that we saw before over those vectors.",
                    "label": 0
                },
                {
                    "sent": "They had a lot of knowledge about language and they had an intuition that this will work.",
                    "label": 0
                },
                {
                    "sent": "Now what I'm coming is to try to justify their intuition and saying.",
                    "label": 0
                },
                {
                    "sent": "What did you actually do?",
                    "label": 0
                },
                {
                    "sent": "What you actually did was you found some presentations under which the two unlabeled distributions look similar?",
                    "label": 0
                },
                {
                    "sent": "You can still predict well, and Luckily the part of speech tagging behaves kind of similarly between the two domains, as reflected by this measure.",
                    "label": 0
                },
                {
                    "sent": "And what I'm going to talk about next is right.",
                    "label": 0
                },
                {
                    "sent": "How can we tighten up this bound?",
                    "label": 0
                },
                {
                    "sent": "Can we get rid of any of those components, and then this is a very.",
                    "label": 0
                },
                {
                    "sent": "Needed a issue.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's crucial issue.",
                    "label": 0
                },
                {
                    "sent": "Can we tighten this bomb?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that was the here is.",
                    "label": 0
                },
                {
                    "sent": "Here is some kind of demonstration of the two requirements so.",
                    "label": 0
                },
                {
                    "sent": "When I'm saying here is I'm using my, I'm using a diamonds the diamonds to show nouns and triangles to show verbs, and we're comparing two kinds of embedding.",
                    "label": 0
                },
                {
                    "sent": "A random embedding of the data and the embedding that was chosen by this part of speech tagger.",
                    "label": 0
                },
                {
                    "sent": "So on a random embedding.",
                    "label": 0
                },
                {
                    "sent": "On a random projection, I cannot distinguish nouns from verbs with a hyperspace.",
                    "label": 0
                },
                {
                    "sent": "They look mixed.",
                    "label": 0
                },
                {
                    "sent": "On embedding it was chosen, I can predict, distinguish them pretty well with halfspaces.",
                    "label": 0
                },
                {
                    "sent": "But I.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also have to worry on can I distinguish the two distributions?",
                    "label": 0
                },
                {
                    "sent": "So now what I have here is that the distribution of unlabeled data, the blue ones, came from financial documents, the.",
                    "label": 0
                },
                {
                    "sent": "The blue ones came from biomedical documents.",
                    "label": 0
                },
                {
                    "sent": "The red one came from financial documents and other random projection.",
                    "label": 0
                },
                {
                    "sent": "I cannot distinguish them, which is very nice.",
                    "label": 0
                },
                {
                    "sent": "I don't want to distinguish them and the projection that they chose.",
                    "label": 0
                },
                {
                    "sent": "I can distinguish them, but what we concerned about is that some of the success of this and the success of distinguishing noun and verbs.",
                    "label": 0
                },
                {
                    "sent": "And the sum is better on this column that in this column.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The the conclusion the algorithmic conclusion is that.",
                    "label": 1
                },
                {
                    "sent": "Try to find a feature space under which the enable distributions look the same and still there exists a good predictor for the domain distribution and then predict there.",
                    "label": 1
                },
                {
                    "sent": "As if the two tasks.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are the same.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "How good is this bound?",
                    "label": 1
                },
                {
                    "sent": "So let us look at this bound now I'm.",
                    "label": 0
                },
                {
                    "sent": "Here I raised the part that goes to 0 as a sample size, go to Infinity and just trying to look at this bound and ask how good is this bound.",
                    "label": 0
                },
                {
                    "sent": "Now know that there is still lots of to be desired here.",
                    "label": 0
                },
                {
                    "sent": "One thing is of course that I this is the Lambda.",
                    "label": 0
                },
                {
                    "sent": "This is my Lambda.",
                    "label": 0
                },
                {
                    "sent": "One thing is that what happens if the two distributions are exactly the same?",
                    "label": 0
                },
                {
                    "sent": "If my source and tasks are exactly the same, what will happen then?",
                    "label": 0
                },
                {
                    "sent": "What will happen then is that?",
                    "label": 0
                },
                {
                    "sent": "What will be the distance between the two unlabeled distributions if it's the same task?",
                    "label": 0
                },
                {
                    "sent": "It'll be 0 and.",
                    "label": 0
                },
                {
                    "sent": "This will be the error of the best predictor.",
                    "label": 0
                },
                {
                    "sent": "Over my target.",
                    "label": 0
                },
                {
                    "sent": "But then what error can I guarantee?",
                    "label": 0
                },
                {
                    "sent": "I can only guarantee three times the optimal error.",
                    "label": 0
                },
                {
                    "sent": "Here is my error is if you have space, can do my task with Arrow .2.",
                    "label": 0
                },
                {
                    "sent": "I'll get punched through here and point to here and point to here.",
                    "label": 0
                },
                {
                    "sent": "So if I'm using this tool in order to just as a sanity check, I'm trying to use this tool just to measure how well can I predict from 1 task to itself I'm multiplying my arrow three times, so it's clearly not the best possible bound.",
                    "label": 0
                },
                {
                    "sent": "It's the best that we could prove, but I pass it on to you young, bright minds.",
                    "label": 1
                },
                {
                    "sent": "There is a lot to be improved here.",
                    "label": 0
                },
                {
                    "sent": "It's a good direction, but it's not the end of the story.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to talk a little bit about some other candidates of relatedness, so there is this covariate shift assumption and the question is can discover achieved assumption when they combine it with one of my other measures of similarity.",
                    "label": 1
                },
                {
                    "sent": "So now I introduce to measure of similarity between distributions.",
                    "label": 0
                },
                {
                    "sent": "The DH similarity between the unlabeled domains and the Lambda similarity between the labels.",
                    "label": 1
                },
                {
                    "sent": "Can I get rid of one of them if I'm using the covariate shift right shift alone?",
                    "label": 0
                },
                {
                    "sent": "We knew that useless, but maybe in combination with one of them it can save me the other one.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The answer is that so we looked at all the pairs, so we know that.",
                    "label": 0
                },
                {
                    "sent": "So I'm considering three types of similarity measures.",
                    "label": 0
                },
                {
                    "sent": "One of them is good joint prediction for the two good joint predictor for the two distributions.",
                    "label": 1
                },
                {
                    "sent": "This is the small Lambda.",
                    "label": 0
                },
                {
                    "sent": "The other one is similarity of the unlabeled distributions.",
                    "label": 0
                },
                {
                    "sent": "That's my DH distance.",
                    "label": 0
                },
                {
                    "sent": "The bound that I showed you shows that if I have the combination of these two that I can get some guarantee on the behavior of the main adaptation.",
                    "label": 0
                },
                {
                    "sent": "The question is if I consider also covariate shift, can it help me in combination with one of them to get rid of the other?",
                    "label": 0
                },
                {
                    "sent": "And the answer is no very strong, no.",
                    "label": 0
                },
                {
                    "sent": "So the covariate shift remains useless even if you add to it either the assumption that there is a good predictor for both distributions or the assumptions that both distributions look similar under the.",
                    "label": 0
                },
                {
                    "sent": "Unlabeled data distributions.",
                    "label": 0
                },
                {
                    "sent": "So I want to show you why this is the case.",
                    "label": 0
                },
                {
                    "sent": "If you still have some cycles working.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these are very similar simple examples, so I have here.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is my my 8 fingers example right?",
                    "label": 0
                },
                {
                    "sent": "I have here 2 distributions.",
                    "label": 0
                },
                {
                    "sent": "That both of them have the same labeling.",
                    "label": 0
                },
                {
                    "sent": "If you are in.",
                    "label": 0
                },
                {
                    "sent": "Odd point, I label you one red if you aren't even point the label you zero.",
                    "label": 0
                },
                {
                    "sent": "These are my two distributions, but in one distributions those odd points are heavy.",
                    "label": 0
                },
                {
                    "sent": "Those are the points that get most of the weight and the other the blue points are heavy.",
                    "label": 0
                },
                {
                    "sent": "So what will happen here?",
                    "label": 0
                },
                {
                    "sent": "I do have the covariate shift assumptions in both of them.",
                    "label": 1
                },
                {
                    "sent": "Labeling rule is the same if you are old, you are red.",
                    "label": 0
                },
                {
                    "sent": "If you are blue.",
                    "label": 0
                },
                {
                    "sent": "If you are even, you are blue nose 0.",
                    "label": 0
                },
                {
                    "sent": "But if I try to learn a predictor for one and adapted to another, if I take a sample from here, I only see red points.",
                    "label": 0
                },
                {
                    "sent": "So I can conclude that I'm going to predict everything is red and it will be a big disaster when I try to apply to.",
                    "label": 0
                },
                {
                    "sent": "But not only this.",
                    "label": 0
                },
                {
                    "sent": "Also we can notice that.",
                    "label": 0
                },
                {
                    "sent": "The DH distribution here, the DH distance, is also small between them.",
                    "label": 0
                },
                {
                    "sent": "Of course it depends.",
                    "label": 0
                },
                {
                    "sent": "One is my age, so if my class H is the class of all intervals.",
                    "label": 0
                },
                {
                    "sent": "If my class H or is the class of all.",
                    "label": 0
                },
                {
                    "sent": "Initial segments of the classifier intervals.",
                    "label": 0
                },
                {
                    "sent": "So on every interval the weight and the distribution and the weight on the distributions are very similar.",
                    "label": 0
                },
                {
                    "sent": "So if my class ages the class of intervals is not only the have covariates such assumption, I also have small the age.",
                    "label": 0
                },
                {
                    "sent": "And the combination still doesn't help me to predict because they might just training on this data.",
                    "label": 0
                },
                {
                    "sent": "I see only red points.",
                    "label": 0
                },
                {
                    "sent": "I'll choose the old one predictor and it will fail badly on the other distribution.",
                    "label": 1
                },
                {
                    "sent": "So covariate shift plus small distance between the unlabeled distributions doesn't help me.",
                    "label": 0
                },
                {
                    "sent": "See here, intervals cannot help me determine.",
                    "label": 0
                },
                {
                    "sent": "They distinguish between the two distribution.",
                    "label": 0
                },
                {
                    "sent": "Even if you don't see the target at that, you will have a very bad man.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 1
                },
                {
                    "sent": "Our domain task I have a very good predictor.",
                    "label": 0
                },
                {
                    "sent": "The old one predictor is very good in the domain because I'm unlikely to see these points, so its probability of error is very small.",
                    "label": 0
                },
                {
                    "sent": "Set up OHSU the whole the whole one is an interval.",
                    "label": 0
                },
                {
                    "sent": "Everything one is an interval.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a magma set H but certain age of our classifiers are all the possible intervals.",
                    "label": 0
                },
                {
                    "sent": "The total set is an interval.",
                    "label": 0
                },
                {
                    "sent": "So I have here a good, very good classifier.",
                    "label": 0
                },
                {
                    "sent": "The all one classifier is very good here.",
                    "label": 0
                },
                {
                    "sent": "OK. Now.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Epsilon, which is small and it is distribution.",
                    "label": 0
                },
                {
                    "sent": "This interval has a large weight.",
                    "label": 0
                },
                {
                    "sent": "Everything happens here.",
                    "label": 0
                },
                {
                    "sent": "So now now that we still have the covariate shift, both of them will give every point the same label points here will be ready in both of them and points here will be blue in both of them.",
                    "label": 0
                },
                {
                    "sent": "And here I have small Lambda.",
                    "label": 0
                },
                {
                    "sent": "I have a predictor that predicts simultaneously for both distributions.",
                    "label": 0
                },
                {
                    "sent": "Very well what the predictor is, the predictor is.",
                    "label": 0
                },
                {
                    "sent": "The predictor says everything is 1 up to this point and everything is 0 from here on.",
                    "label": 0
                },
                {
                    "sent": "This predictor will make.",
                    "label": 0
                },
                {
                    "sent": "0 error on the target.",
                    "label": 1
                },
                {
                    "sent": "And it will make just two epsilon error on the domain.",
                    "label": 0
                },
                {
                    "sent": "So I have a good joint predictor.",
                    "label": 0
                },
                {
                    "sent": "Why did I put the two epsilon here?",
                    "label": 0
                },
                {
                    "sent": "Because I want to show you that if I just look at what is my best predictor here.",
                    "label": 0
                },
                {
                    "sent": "What is my best predictor on just the source domain?",
                    "label": 0
                },
                {
                    "sent": "What would be my best predictor on just the source domain?",
                    "label": 0
                },
                {
                    "sent": "What is the best predicting intervals on the source domain to minimize the error on the source?",
                    "label": 0
                },
                {
                    "sent": "What is it going to be?",
                    "label": 1
                },
                {
                    "sent": "Alright, and all red will make out of 1 here.",
                    "label": 0
                },
                {
                    "sent": "So I have covariate shift.",
                    "label": 0
                },
                {
                    "sent": "I have small Lambda.",
                    "label": 0
                },
                {
                    "sent": "My best predictor, just seeing the labels here is the old Red one is going to make.",
                    "label": 0
                },
                {
                    "sent": "Catastrophic error on the target.",
                    "label": 0
                },
                {
                    "sent": "So having covariate shift plus small Lambda plus a good predictor for both of them still doesn't save me from disaster.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I showed you is just that these two sides of the triangle covariate shift with each of them alone is not helping us at all.",
                    "label": 0
                },
                {
                    "sent": "Only the two, the combination that we.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ready now about.",
                    "label": 0
                },
                {
                    "sent": "OK, now another has become kind of running out of time, but there's another aspect that I wanted just to discuss, just briefly is the distinction between.",
                    "label": 0
                },
                {
                    "sent": "Conservative an adaptive domain adaptation what we did so far is conservative to meditation in the sense that.",
                    "label": 0
                },
                {
                    "sent": "We just found the best predictor for the source domain and had some guarantee of when it will work well for the target domain, but that's not really adaptation is just saying you know I do my own thing and I hope that it will be good enough for you.",
                    "label": 0
                },
                {
                    "sent": "But I didn't adapt, so this is what I call conservative algorithm.",
                    "label": 0
                },
                {
                    "sent": "Other any good adaptive algorithms.",
                    "label": 1
                },
                {
                    "sent": "Algorithms that really change the prediction when they see the unlabeled data from the target distribution.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is a very important question.",
                    "label": 0
                },
                {
                    "sent": "Clearly there should be some algorithm like this, but what is it?",
                    "label": 0
                },
                {
                    "sent": "So I could not come up with any but.",
                    "label": 0
                },
                {
                    "sent": "Other guys came there, so there's a several papers by of Shiny Maureen Mansoor, both in cold so nine and you I own on that proposed the different predictor that is really adaptive.",
                    "label": 0
                },
                {
                    "sent": "It says.",
                    "label": 0
                },
                {
                    "sent": "OK, I should have hit this.",
                    "label": 0
                },
                {
                    "sent": "This don't look at it right now.",
                    "label": 0
                },
                {
                    "sent": "It says the foreign use the use the target unlabeled sample to reweigh your training data.",
                    "label": 1
                },
                {
                    "sent": "We have training data which is labeled.",
                    "label": 1
                },
                {
                    "sent": "We can we have unlabeled data from the target.",
                    "label": 0
                },
                {
                    "sent": "We can try to re weigh our sample according to the target and then find the best prediction under this knew reweighing.",
                    "label": 0
                },
                {
                    "sent": "And they claim that it works.",
                    "label": 0
                },
                {
                    "sent": "In practice.",
                    "label": 0
                },
                {
                    "sent": "We show that in theory it can fail very badly.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here is the example of.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How it fails very badly.",
                    "label": 0
                },
                {
                    "sent": "So look at the same example as before.",
                    "label": 0
                },
                {
                    "sent": "I have this domain as before.",
                    "label": 0
                },
                {
                    "sent": "Now I see the unlabeled points here.",
                    "label": 0
                },
                {
                    "sent": "So if I'm trying to re weigh my sample according to what I saw from the target, it will tell me oh this is very important this interval.",
                    "label": 0
                },
                {
                    "sent": "And because this interval is very important, I will not take the whole.",
                    "label": 0
                },
                {
                    "sent": "A already assumption, because they already assumption is going to fail here and seeing a lot of unlabeled data here.",
                    "label": 0
                },
                {
                    "sent": "I know that this part is important.",
                    "label": 0
                },
                {
                    "sent": "So instead of taking the alright assumption, I'll take the assumption that says I'll be red and up to here and blue from hero, because I see here blue points and the unlabeled data tell me this is an important part.",
                    "label": 0
                },
                {
                    "sent": "But he just steered me away from a good assumption.",
                    "label": 0
                },
                {
                    "sent": "They already would have been very good for the target, the one they chose.",
                    "label": 0
                },
                {
                    "sent": "Now after the waiting is very bad for the target so that we waiting can completely mislead you.",
                    "label": 0
                },
                {
                    "sent": "So they just claimed that it works in practice they don't have any guarantee.",
                    "label": 0
                },
                {
                    "sent": "This is a very short clear example why you cannot get currently like this.",
                    "label": 0
                },
                {
                    "sent": "Of course, so the question is, under what assumptions can you get guarantees?",
                    "label": 0
                },
                {
                    "sent": "That's what I'm asking.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "I mean it's a government example of very specific, and so yeah, it just tells you you have to watch out.",
                    "label": 0
                },
                {
                    "sent": "You need.",
                    "label": 0
                },
                {
                    "sent": "We need some extra parameters.",
                    "label": 0
                },
                {
                    "sent": "What we have is not enough to guarantee success.",
                    "label": 0
                },
                {
                    "sent": "We need to come up with more form, so let me jump to the my.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Open questions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Wide open question is improve our basic generalization error, which is suspect should be improved a lot.",
                    "label": 0
                },
                {
                    "sent": "And another one is to find relatedness parameters under which.",
                    "label": 1
                },
                {
                    "sent": "Different paradigms work, so for example this reweighting, so we need new relatedness parameters clearly.",
                    "label": 1
                },
                {
                    "sent": "And come up with new adaptive algorithms.",
                    "label": 0
                },
                {
                    "sent": "Not just conservative domain adaptation algorithm, clearly we should be able to come up with algorithms that really adapt to the new data, but we don't have any good answers to any of those very, very basic questions.",
                    "label": 0
                },
                {
                    "sent": "So there is a lot of open questions here.",
                    "label": 0
                },
                {
                    "sent": "Do I have another one?",
                    "label": 0
                },
                {
                    "sent": "Yeah, come up with more user friendly relatedness notions.",
                    "label": 1
                },
                {
                    "sent": "Our Lambda, for example is very not user friendly because you cannot estimate it.",
                    "label": 0
                },
                {
                    "sent": "So that many more.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is an area which is important.",
                    "label": 0
                },
                {
                    "sent": "It's really practical and what we can do in theory so far is just scratching the surface of it.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of open questions that ought to be done, and what I'm trying to convince you is try to.",
                    "label": 0
                },
                {
                    "sent": "Think about this problem and along these slides.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm sure you Friday.",
                    "label": 0
                },
                {
                    "sent": "The missing assumption.",
                    "label": 0
                },
                {
                    "sent": "So yeah, So what what it brings about is a different question.",
                    "label": 0
                },
                {
                    "sent": "Note that there is a.",
                    "label": 0
                },
                {
                    "sent": "Quite a lot of similarity between the problem was talking about here and the problem of semi supervised learning, because what I'm saying here is they have labeled data from the source and I have unlabeled data from the target.",
                    "label": 0
                },
                {
                    "sent": "But maybe the assume that the source and target are the same.",
                    "label": 0
                },
                {
                    "sent": "Then I have labeled data and unlabeled data and I have the same supervised learning problem and the semi supervised learning problem is a problem in which we are just as in dark as the domain adaptation.",
                    "label": 0
                },
                {
                    "sent": "So in the in the semi supervised learning is.",
                    "label": 0
                },
                {
                    "sent": "It's a good topic for four different lecture and there are other common assumptions like the cluster assumption, and we can construct other counterexamples that show that this may also fail, and it's not a satisfactory assumption in terms of theory.",
                    "label": 0
                },
                {
                    "sent": "So yeah, the this problem is the manifestation problem is strictly harder than the semi supervised learning problem and semi supervised learning problem is already hard enough that we don't know much about except for counterexamples.",
                    "label": 0
                },
                {
                    "sent": "Yes, well, you could assume that you have some label data and you're OK, so that.",
                    "label": 0
                },
                {
                    "sent": "Look at the joint.",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "So that's a slightly different problem.",
                    "label": 0
                },
                {
                    "sent": "I call it multi task learning when you have some labels from the other task and we have some work on this as well, right?",
                    "label": 0
                },
                {
                    "sent": "It's a slightly different model, I was just.",
                    "label": 0
                },
                {
                    "sent": "I was just focusing on this model we have.",
                    "label": 0
                },
                {
                    "sent": "Lots of questions.",
                    "label": 0
                },
                {
                    "sent": "It's slightly different.",
                    "label": 0
                },
                {
                    "sent": "Some more answers, but also a lot of questions on this multi task.",
                    "label": 0
                },
                {
                    "sent": "So the difference is that if you have few few examples from the new domain, for example, one of the questions that we would like to answer and we don't know is you know if you have a lot of examples from them.",
                    "label": 0
                },
                {
                    "sent": "Target domain, you will ignore the source to me because you know that it's a different distribution.",
                    "label": 0
                },
                {
                    "sent": "Where is the threshold, where?",
                    "label": 0
                },
                {
                    "sent": "What ratio between the number of labeled points you have on the target and the number of Labor points you have from the source.",
                    "label": 0
                },
                {
                    "sent": "Where is the critical point where you will start ignoring the the source with even we can't even give you this recipe, which is something that you might expect that theoretician will.",
                    "label": 0
                },
                {
                    "sent": "Give the practitioners, tell them if you have only so few examples, use the the training data.",
                    "label": 0
                },
                {
                    "sent": "If you have lots of examples, you can ignore the training data because you reduce the bias and the variance is not so bad.",
                    "label": 0
                },
                {
                    "sent": "And we don't know yet how to give you this.",
                    "label": 0
                },
                {
                    "sent": "So there are lots of questions in the multi task domain which is a very related problem.",
                    "label": 0
                },
                {
                    "sent": "But I was concentrating on the domain adaptation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so these are two very related issues.",
                    "label": 0
                },
                {
                    "sent": "The semi supervised learning and the multi task learning.",
                    "label": 0
                },
                {
                    "sent": "And we are have similar questions there as well.",
                    "label": 0
                }
            ]
        }
    }
}