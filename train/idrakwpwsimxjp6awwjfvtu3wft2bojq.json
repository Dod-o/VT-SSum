{
    "id": "idrakwpwsimxjp6awwjfvtu3wft2bojq",
    "title": "An annotated hierarchies model of syntactic categories",
    "info": {
        "author": [
            "Virginia Savova, Computational Cognitive Science Group, Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, MIT"
        ],
        "published": "Oct. 31, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Natural Language Processing",
            "Top->Computer Science->Machine Learning->Human Language Technology",
            "Top->Computer Science->Semantic Web->Annotation"
        ]
    },
    "url": "http://videolectures.net/mlcs07_savova_ahm/",
    "segmentation": [
        [
            "So I'm going to talk today about a model of annotated hierarchies for syntactic categories, and this is joint work with Daniel Roy, Lauren Schmidt, and Josh Tenenbaum at MI."
        ],
        [
            "T. From the very beginning I want to clarify that I use the term syntactic categories as some generalized notion of part of speech tags, and the reason why you syntactic categories and not part of speech tags is because this talk is going to emphasize the hierarchical aspect of syntactic categories.",
            "I I'm going to start by reviewing what syntactic categories are supposed to do for linguistic theories, and try to convince you that it's better to think of syntactic categories as a. Nested inside a hierarchical structure rather than as a set of arbitrary size and generality, I'm going to also discuss what factors enter into determining the syntactic category of lexical item, and then I'm going to present the computational model for induction of hierarchical structure from relational feature data, which is applied to the problem of inferring the hierarchical structure of syntactic categories and present the results of the application of this model to.",
            "Relational and feature data, which is obtained from artificial and natural corpora.",
            "OK, So what is a syntactic categ?"
        ],
        [
            "You can think of a syntactic category as a sort of a shorthand for distributional, morphological, and semantic properties of a word that, in the Western tradition this notion goes at least as far back as Aristotle, and the idea is that if we were to write the grammar for language instead of writing for each word, all the properties or all the distributional properties with which it is associated, we can we can instead.",
            "Substitute these properties with a convenient label which is going to now stand for all of these properties.",
            "This is of course in the case when we have more than one word which shares these properties.",
            "For example, if we have the two words Bolan Cat, we see that they share a semantic property that both of them are entities.",
            "Both of them take the morphological suffix South and both of them frequently appear after the word the.",
            "Thus we can substitute all of those three."
        ],
        [
            "We would just one label now and have now stand for all of these.",
            "However, we soon will run into a problem when we think of.",
            "Syntactic categorisation like this because we can easily find the word which shares some properties with ball and cat, but not all properties with ball and cat and so now we're left with the choice to either make a separate category for this word, or maybe to expand and generalize the category which we which we constructed.",
            "So let's go the second route.",
            "Let's say that now now only stands for the two.",
            "These which Boland Caviar share, namely that it's an entity and goes after that.",
            "And if we do that and create a category and prime, now we have to annotate all the words which take the morphological suffix South separately in the lexicon as taking the suffix S."
        ],
        [
            "And if we go even further, we now find another word which shares some properties of both cat and caviar, but not all of them, namely a proper noun like John, which goes after, which is an entity and maybe contains shares other properties, but doesn't go after the.",
            "Neither does it take the suffix S. So now we can expand our non category even further and have it only stand for entity and we now have to annotate all words which we previously.",
            "Substituted with now with special properties, namely that they take the more females and go after that."
        ],
        [
            "So instead of doing, instead of choosing some arbitrary level of generalization.",
            "Linguistic descriptions in principle that make make use of sort of informal or semi formal notion, that those syntactic categories are nested in a hierarchical structure, such that we can refer with different rules to arbitrary levels of the syntactic category hierarchy.",
            "So this type of notion is actually formalized, as with the notion of annotated hierarchy, which.",
            "Is a hierarchy that contains with all the at all the nodes and annotation of the relevant features and relations which split the hierarchy at that particular node.",
            "So if we were to take our three words which we can see there so far John Caviar and ball, we would have a hierarchy like the one shown here, where all of those were shared.",
            "The feature entity, two of them share the distribution of property that they go after that.",
            "And only the one of them shares the has the morphological property of taking the suffix."
        ],
        [
            "This of course.",
            "Is equivalent to the grammarians notion of nouns splitting into proper nouns, common nouns, mass nouns, and count nouns.",
            "So if we are convinced that this is the right way to represent syntactic categories."
        ],
        [
            "The question is how can we induce those categories from data and whether or not it is possible to do that instead of justice taking an arbitrary horizontal cut through the hierarchy the way it's possible to do if we just do part of speech tagging?",
            "For example the Penn Treebank, part of speech tags take a horizontal cut through the hierarchy such that they will distinguish between proper nouns and.",
            "Common nouns, but they wouldn't distinguish between mass nouns and count nouns.",
            "Aristotle's eight categories maker."
        ],
        [
            "At the top.",
            "So if we were to try to induce the anesthetic it directly, we can apply a model which was developed for this purpose and proposed last year by Royale at NIPS, which, given the set of objects which are in have binary relations and features of binary relations among them, and features, finds an annotated hierarchy and associates the corresponding features in relations to an appropriate partition.",
            "So this model assumes that objects are essentially located at the leaves of a rooted tree such that objects nearby have similar feature values and relate to other objects in."
        ],
        [
            "Similar ways.",
            "It also assumes that each feature in relation you generated independently conditioned on the structure of the tree, which is an assumption which makes more sense for some features than others.",
            "For example, maybe the morphological suffix is as related to some semantic property of count nouns, but for other cases it might be arbitrary.",
            "So this is just an assumption that the model makes, which might or might not correspond to reality.",
            "Um?",
            "The model also associates each partition, each category in each partition with a particular probability that the feature and relation is positive inside this category.",
            "So if we were to partition the set of syntactic categories into verb and noun, then the verb category is going to be associated with a high probability with the ING suffix.",
            "Um?",
            "But not necessarily.",
            "So it doesn't necessarily mean that all the members of the category is going to take the entry suffix, but just with high probability.",
            "The model also places a prior over partitions, which encourages the use of the most general categories possible."
        ],
        [
            "Which is different from bottom up clustering.",
            "So the objective really is to optimize the tree.",
            "The probability of the tree given a set of features and relation.",
            "And we do this by.",
            "Oh, so defining the probability of the conditional probability of the tree as the product of the probabilities of each feature given the tree and the product's probability of each relation given the tree and the prior over the tree structure, which again creates which again places higher value on simpler tree structures.",
            "The conditional probability of the feature and relation with respect to the tree is computed as the sum of the probability of the feature on each partition, which this feature is relevant.",
            "And the prior probability over the partition given."
        ],
        [
            "Tree.",
            "So the search is quite complicated and not very efficient at this time, which is why we can only deal with small corpora.",
            "But we think that it's good to apply this model now to see how far it can get with small datasets.",
            "So the first experiment we did with applying the model to linguistic data is sort of.",
            "It's not directly inducing the annotated hierarchy from.",
            "Fully in a soup.",
            "Fully unsupervised way, but rather what we did is we try to find an annotated hierarchy internal to particular linguistic theory to see whether this notion of annotated hierarchy makes computational sense inside the assumptions of a particular set of linguistic descriptions which are proposed by this theory.",
            "So the series called X Bar theory and it's a theory of linguistic descriptions which uses restricted subclass of context free grammars and it.",
            "Assumes that all linguistic theories are generated by three types of productions specifier production, a compliment production, and agile project production.",
            "The specifier production splits non terminal category into two non terminal categories, one of which is labeled the specifier.",
            "The compliment productions splits a nonterminal category into a terminal or pre terminal category and non terminal category which is labeled a compliment.",
            "And the agent production splits there a non terminal category into two non terminal categories, one of which is said to be the agent of the other, it's better.",
            "To think of this theory is in the context of dependency grammars, perhaps where we consider a class of dependency grammars where we have two types of dependencies, two types of obligatory dependencies for each head, namely a specifier dependency and the complement dependencies, and we also have a number of any number of optional dependence for each head, which we will call."
        ],
        [
            "Urgent so.",
            "For the purposes of our algorithm, let's suppose that each dependent type is essentially a relation.",
            "So we've done is the specifier of the head read, then the relation specifier is true of John and Reed.",
            "Using these three relations from an artificial corpus of representative occurrences, we set out to infer an annotated hierarchy corresponding to this linguistic theory.",
            "So the utterances were chosen to contain representative subset of words such as transitive intransitive verbs, save type verbs, or verbs that take other verbs as complements, mass, and count nouns, as well as determiners, adverbs, adjectives.",
            "And so forth.",
            "And these were just for illustration purposes, chosen to be words that are likely to be present in early vocab."
        ],
        [
            "Larry so.",
            "The results here look quite reasonable.",
            "First, the algorithm finds annotated hierarchy, which contains all the major classes.",
            "So the verbs the announced the terminals, the adjectives and adverbs are well separated, and there is also internal structure.",
            "So inside the verb category, for example, there is a.",
            "The verbs which take other verbs complements, so, say think no type verbs are separated as well as intransitive verbs like break, go, sleep, and walk.",
            "There is a most of the transitive verbs are together with the exception of look and explode and teaser also sort of a type of I guess in transitive or weird verbs and for some reason they've clustered together the same.",
            "The same happens as internal structure happens in the noun class.",
            "There is the mass.",
            "Nouns are well separated together.",
            "Cheese and yogurt and a little bit separately, but still closer than Banana, which sometimes uses as a mass noun, then the.",
            "The count nouns Baldal car Bunny together and the animate nouns are separate because presumably because they are more often specifies of verbs which require.",
            "Animal agents and the determiners are form a separate category."
        ],
        [
            "So on the next slide, what you see is the relational structure, which is inferred over this tree.",
            "These are the two relations specifier of and complement of and what you can see here in green is the if."
        ],
        [
            "Remember, the terminal is in red, so if you were."
        ],
        [
            "Hold on a second.",
            "OK, so the determiners are the specifiers of the noun class.",
            "This is what this picture on the left shows and.",
            "So say verbs take all verbs as complements, including, say, say, verbs.",
            "This is what the picture on the right shows quite nicely.",
            "There is some other nice structure in those matrices which I'm not going to go over, so given this result, we kind of wanted to try more experiments on real data and we went to the British National Corpus and."
        ],
        [
            "We tried various experiments, but the real world is not as neat."
        ],
        [
            "Does the X bar theory relations, in particular the first problem, is the of course, the British National Corpus is a huge corpus.",
            "It contains 100 million words, which are way too many for this algorithm.",
            "At this stage it can handle about 100 or 200 words to create an annotated hierarchy and breaks down after that.",
            "So in order to try it with some real data, what we did is we extracted the representative subset of frequent.",
            "Nouns in this corpus and then as relational data for the algorithm we extracted semantic role relations from utility code sketch engine, which allows you to pull out all the subject of object of uses of verb in the British National Corpus.",
            "So we formed the.",
            "These are from the nouns and the frequently occurring verbs, adverbs, and adjectives output from the sketch engine in response to."
        ],
        [
            "Aries about these nouns.",
            "So in the first experiment, we collapsed all semantic relations which we found into a single dependency relation.",
            "So instead of having John subject of read, now we just would have John dependent of read book, also dependent of Reed, and we added some semantic features manually to our set, some of which pick subsets of categories, some of which pick pretty much.",
            "The whole categories, some of which do not pick actually span across categories.",
            "So the feature concrete, for example, is almost entirely associated with nouns and with most nouns.",
            "But the semantic feature intentional is associated only with the animate nouns and the semantic feature motion is.",
            "Associated with verbs as well as adverbs, we also added morphological features where the same thing is true.",
            "So the ING feature is associated exclusively with verbs, but the feature is split between nouns and verbs because now is take it as plural and verbs take it as third singular."
        ],
        [
            "Third person singular.",
            "So with this data, the annotated Clark is algorithm found all the main categories, but it didn't really find much internal structure within the classes.",
            "And one interesting thing which happened in this experiment, and we're not sure whether that's just an artifact of the experiment or something, or there is something interesting there.",
            "But there are the algorithms split.",
            "The nouns and verbs on one hand from the adverbs and adjectives on the other.",
            "And if you think that those are obligatory versus optional arguments and it's kind of interesting that maybe on the basis of the dependency relations.",
            "This split comes out."
        ],
        [
            "So here is the tree.",
            "We have the nouns on the left and then the verbs next to them and then on the right.",
            "The adverbs on the left, right and the furthest right.",
            "We have the adjectives.",
            "So in the final experiment, which I'm going to show today."
        ],
        [
            "Hey um.",
            "What we decided to do with this data is we decided to throw in as much information as possible and see whether we're going to recover any internal structure from the from the real data.",
            "So we actually threw in the original semantic role relations.",
            "The subject of an object of relations rather than the dependency relation process.",
            "We also kept the semantic features and the morphological features, but we also added the distribution of features.",
            "Which are used before by intervenes as work and others, namely the frequent frames.",
            "The most frequent frame.",
            "So things like the is are going to be the preceding.",
            "Now this is going to be now a distribution of fee."
        ],
        [
            "And with this type of data we did recover some internal structure.",
            "So in particular, we uncovered the structure in the adverb class where we found all the temporal adverbs clustering together, so actually never already.",
            "Also still, ever, usually now, and finally with some others inside.",
            "And we also found all the place."
        ],
        [
            "Adverbs of clustering together.",
            "We also uncovered some structure in the verbs, in particular, the verbs go pass around start clustered separately from the transitive verbs.",
            "In all of the experiments that we did, these which I presented here and others, the algorithm successfully recovered main categories.",
            "But the internal structure is a D."
        ],
        [
            "Frontmatter so, before I wrap up, I just want to say a few words about how this model compares with hierarchical clustering, which is another model of hierarchical hierarchical structure of syntactic categories.",
            "So in terms of performance issues, correct clustering is better, because first of all it's faster, and managers to actually handle large datasets pretty easily.",
            "The main reason why we want to push this model further is because.",
            "The character clustering has some representational drawbacks which annotated hierarchies is trying to deal with in particular, correct clustering doesn't really associate relations and features with particular nodes in the hierarchy, and relations are not explicitly treated as relations, but rather converted to features.",
            "And that's not a very theoretically elegant way of handling this."
        ],
        [
            "Issue.",
            "That said, of course our model of has a lot of improvement, requires a lot of improvement before it can actually compete on a large scale.",
            "First of all, it has to be improved on scalability, and this is just an algorithmic issue of the search over trees.",
            "We have done some improvements, but not enough to handle 100 million words we.",
            "Also, the model as it stands now does not really have an intrinsic capability of dealing with noise, and the way we deal with noises we threshold externally the data which we collect.",
            "So we try to find we put positive relation relation as true if it occurs and number of times and false if it occurs less than that.",
            "And that's an artificial way of doing it, and especially in natural corpora that probably really hurts us because.",
            "Of course they are noisy and sparse.",
            "Another thing to do with the annotate characters model is to think about automatic feature selection.",
            "So we have now given a lot to the model we have given the fact that we have assumed, for example, that can do morphological analysis of words, which is obviously not a solved problem as we heard in this in this workshop and many other places.",
            "And the main challenge is to reduce the abstract relations with which it works quite well in the expert case to some sort of surface observable relations.",
            "For example, the intuition is that their relation subject of should really be somehow observed from the precedence relations between the subject and the verb.",
            "But it's a tricky thing to actually do it, because the subject can be arbitrarily far from the verb and various."
        ],
        [
            "Issues like that.",
            "So in conclusion.",
            "I have presented the model of syntactic categories which represents the hierarchical structure of those categories and assosiates relations and features with treatment partitions inside the hierarchy and the annotated characters of syntactic categories can be in principle in Ferd if the linguistic structure is known or is easy to deduce from surface relations.",
            "But unsupervised learning from the data is really messy, even if.",
            "A lot of features are granted to the learner.",
            "That's it, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk today about a model of annotated hierarchies for syntactic categories, and this is joint work with Daniel Roy, Lauren Schmidt, and Josh Tenenbaum at MI.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "T. From the very beginning I want to clarify that I use the term syntactic categories as some generalized notion of part of speech tags, and the reason why you syntactic categories and not part of speech tags is because this talk is going to emphasize the hierarchical aspect of syntactic categories.",
                    "label": 0
                },
                {
                    "sent": "I I'm going to start by reviewing what syntactic categories are supposed to do for linguistic theories, and try to convince you that it's better to think of syntactic categories as a. Nested inside a hierarchical structure rather than as a set of arbitrary size and generality, I'm going to also discuss what factors enter into determining the syntactic category of lexical item, and then I'm going to present the computational model for induction of hierarchical structure from relational feature data, which is applied to the problem of inferring the hierarchical structure of syntactic categories and present the results of the application of this model to.",
                    "label": 1
                },
                {
                    "sent": "Relational and feature data, which is obtained from artificial and natural corpora.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is a syntactic categ?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can think of a syntactic category as a sort of a shorthand for distributional, morphological, and semantic properties of a word that, in the Western tradition this notion goes at least as far back as Aristotle, and the idea is that if we were to write the grammar for language instead of writing for each word, all the properties or all the distributional properties with which it is associated, we can we can instead.",
                    "label": 1
                },
                {
                    "sent": "Substitute these properties with a convenient label which is going to now stand for all of these properties.",
                    "label": 0
                },
                {
                    "sent": "This is of course in the case when we have more than one word which shares these properties.",
                    "label": 0
                },
                {
                    "sent": "For example, if we have the two words Bolan Cat, we see that they share a semantic property that both of them are entities.",
                    "label": 0
                },
                {
                    "sent": "Both of them take the morphological suffix South and both of them frequently appear after the word the.",
                    "label": 0
                },
                {
                    "sent": "Thus we can substitute all of those three.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We would just one label now and have now stand for all of these.",
                    "label": 0
                },
                {
                    "sent": "However, we soon will run into a problem when we think of.",
                    "label": 0
                },
                {
                    "sent": "Syntactic categorisation like this because we can easily find the word which shares some properties with ball and cat, but not all properties with ball and cat and so now we're left with the choice to either make a separate category for this word, or maybe to expand and generalize the category which we which we constructed.",
                    "label": 0
                },
                {
                    "sent": "So let's go the second route.",
                    "label": 0
                },
                {
                    "sent": "Let's say that now now only stands for the two.",
                    "label": 1
                },
                {
                    "sent": "These which Boland Caviar share, namely that it's an entity and goes after that.",
                    "label": 0
                },
                {
                    "sent": "And if we do that and create a category and prime, now we have to annotate all the words which take the morphological suffix South separately in the lexicon as taking the suffix S.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if we go even further, we now find another word which shares some properties of both cat and caviar, but not all of them, namely a proper noun like John, which goes after, which is an entity and maybe contains shares other properties, but doesn't go after the.",
                    "label": 1
                },
                {
                    "sent": "Neither does it take the suffix S. So now we can expand our non category even further and have it only stand for entity and we now have to annotate all words which we previously.",
                    "label": 0
                },
                {
                    "sent": "Substituted with now with special properties, namely that they take the more females and go after that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So instead of doing, instead of choosing some arbitrary level of generalization.",
                    "label": 0
                },
                {
                    "sent": "Linguistic descriptions in principle that make make use of sort of informal or semi formal notion, that those syntactic categories are nested in a hierarchical structure, such that we can refer with different rules to arbitrary levels of the syntactic category hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So this type of notion is actually formalized, as with the notion of annotated hierarchy, which.",
                    "label": 0
                },
                {
                    "sent": "Is a hierarchy that contains with all the at all the nodes and annotation of the relevant features and relations which split the hierarchy at that particular node.",
                    "label": 0
                },
                {
                    "sent": "So if we were to take our three words which we can see there so far John Caviar and ball, we would have a hierarchy like the one shown here, where all of those were shared.",
                    "label": 0
                },
                {
                    "sent": "The feature entity, two of them share the distribution of property that they go after that.",
                    "label": 0
                },
                {
                    "sent": "And only the one of them shares the has the morphological property of taking the suffix.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This of course.",
                    "label": 0
                },
                {
                    "sent": "Is equivalent to the grammarians notion of nouns splitting into proper nouns, common nouns, mass nouns, and count nouns.",
                    "label": 0
                },
                {
                    "sent": "So if we are convinced that this is the right way to represent syntactic categories.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The question is how can we induce those categories from data and whether or not it is possible to do that instead of justice taking an arbitrary horizontal cut through the hierarchy the way it's possible to do if we just do part of speech tagging?",
                    "label": 0
                },
                {
                    "sent": "For example the Penn Treebank, part of speech tags take a horizontal cut through the hierarchy such that they will distinguish between proper nouns and.",
                    "label": 0
                },
                {
                    "sent": "Common nouns, but they wouldn't distinguish between mass nouns and count nouns.",
                    "label": 0
                },
                {
                    "sent": "Aristotle's eight categories maker.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the top.",
                    "label": 0
                },
                {
                    "sent": "So if we were to try to induce the anesthetic it directly, we can apply a model which was developed for this purpose and proposed last year by Royale at NIPS, which, given the set of objects which are in have binary relations and features of binary relations among them, and features, finds an annotated hierarchy and associates the corresponding features in relations to an appropriate partition.",
                    "label": 0
                },
                {
                    "sent": "So this model assumes that objects are essentially located at the leaves of a rooted tree such that objects nearby have similar feature values and relate to other objects in.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Similar ways.",
                    "label": 0
                },
                {
                    "sent": "It also assumes that each feature in relation you generated independently conditioned on the structure of the tree, which is an assumption which makes more sense for some features than others.",
                    "label": 1
                },
                {
                    "sent": "For example, maybe the morphological suffix is as related to some semantic property of count nouns, but for other cases it might be arbitrary.",
                    "label": 0
                },
                {
                    "sent": "So this is just an assumption that the model makes, which might or might not correspond to reality.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "The model also associates each partition, each category in each partition with a particular probability that the feature and relation is positive inside this category.",
                    "label": 0
                },
                {
                    "sent": "So if we were to partition the set of syntactic categories into verb and noun, then the verb category is going to be associated with a high probability with the ING suffix.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But not necessarily.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't necessarily mean that all the members of the category is going to take the entry suffix, but just with high probability.",
                    "label": 0
                },
                {
                    "sent": "The model also places a prior over partitions, which encourages the use of the most general categories possible.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is different from bottom up clustering.",
                    "label": 0
                },
                {
                    "sent": "So the objective really is to optimize the tree.",
                    "label": 0
                },
                {
                    "sent": "The probability of the tree given a set of features and relation.",
                    "label": 1
                },
                {
                    "sent": "And we do this by.",
                    "label": 0
                },
                {
                    "sent": "Oh, so defining the probability of the conditional probability of the tree as the product of the probabilities of each feature given the tree and the product's probability of each relation given the tree and the prior over the tree structure, which again creates which again places higher value on simpler tree structures.",
                    "label": 0
                },
                {
                    "sent": "The conditional probability of the feature and relation with respect to the tree is computed as the sum of the probability of the feature on each partition, which this feature is relevant.",
                    "label": 0
                },
                {
                    "sent": "And the prior probability over the partition given.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tree.",
                    "label": 0
                },
                {
                    "sent": "So the search is quite complicated and not very efficient at this time, which is why we can only deal with small corpora.",
                    "label": 0
                },
                {
                    "sent": "But we think that it's good to apply this model now to see how far it can get with small datasets.",
                    "label": 0
                },
                {
                    "sent": "So the first experiment we did with applying the model to linguistic data is sort of.",
                    "label": 0
                },
                {
                    "sent": "It's not directly inducing the annotated hierarchy from.",
                    "label": 1
                },
                {
                    "sent": "Fully in a soup.",
                    "label": 0
                },
                {
                    "sent": "Fully unsupervised way, but rather what we did is we try to find an annotated hierarchy internal to particular linguistic theory to see whether this notion of annotated hierarchy makes computational sense inside the assumptions of a particular set of linguistic descriptions which are proposed by this theory.",
                    "label": 0
                },
                {
                    "sent": "So the series called X Bar theory and it's a theory of linguistic descriptions which uses restricted subclass of context free grammars and it.",
                    "label": 1
                },
                {
                    "sent": "Assumes that all linguistic theories are generated by three types of productions specifier production, a compliment production, and agile project production.",
                    "label": 0
                },
                {
                    "sent": "The specifier production splits non terminal category into two non terminal categories, one of which is labeled the specifier.",
                    "label": 0
                },
                {
                    "sent": "The compliment productions splits a nonterminal category into a terminal or pre terminal category and non terminal category which is labeled a compliment.",
                    "label": 0
                },
                {
                    "sent": "And the agent production splits there a non terminal category into two non terminal categories, one of which is said to be the agent of the other, it's better.",
                    "label": 0
                },
                {
                    "sent": "To think of this theory is in the context of dependency grammars, perhaps where we consider a class of dependency grammars where we have two types of dependencies, two types of obligatory dependencies for each head, namely a specifier dependency and the complement dependencies, and we also have a number of any number of optional dependence for each head, which we will call.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Urgent so.",
                    "label": 0
                },
                {
                    "sent": "For the purposes of our algorithm, let's suppose that each dependent type is essentially a relation.",
                    "label": 1
                },
                {
                    "sent": "So we've done is the specifier of the head read, then the relation specifier is true of John and Reed.",
                    "label": 0
                },
                {
                    "sent": "Using these three relations from an artificial corpus of representative occurrences, we set out to infer an annotated hierarchy corresponding to this linguistic theory.",
                    "label": 1
                },
                {
                    "sent": "So the utterances were chosen to contain representative subset of words such as transitive intransitive verbs, save type verbs, or verbs that take other verbs as complements, mass, and count nouns, as well as determiners, adverbs, adjectives.",
                    "label": 0
                },
                {
                    "sent": "And so forth.",
                    "label": 1
                },
                {
                    "sent": "And these were just for illustration purposes, chosen to be words that are likely to be present in early vocab.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Larry so.",
                    "label": 0
                },
                {
                    "sent": "The results here look quite reasonable.",
                    "label": 0
                },
                {
                    "sent": "First, the algorithm finds annotated hierarchy, which contains all the major classes.",
                    "label": 1
                },
                {
                    "sent": "So the verbs the announced the terminals, the adjectives and adverbs are well separated, and there is also internal structure.",
                    "label": 0
                },
                {
                    "sent": "So inside the verb category, for example, there is a.",
                    "label": 0
                },
                {
                    "sent": "The verbs which take other verbs complements, so, say think no type verbs are separated as well as intransitive verbs like break, go, sleep, and walk.",
                    "label": 0
                },
                {
                    "sent": "There is a most of the transitive verbs are together with the exception of look and explode and teaser also sort of a type of I guess in transitive or weird verbs and for some reason they've clustered together the same.",
                    "label": 0
                },
                {
                    "sent": "The same happens as internal structure happens in the noun class.",
                    "label": 0
                },
                {
                    "sent": "There is the mass.",
                    "label": 0
                },
                {
                    "sent": "Nouns are well separated together.",
                    "label": 0
                },
                {
                    "sent": "Cheese and yogurt and a little bit separately, but still closer than Banana, which sometimes uses as a mass noun, then the.",
                    "label": 0
                },
                {
                    "sent": "The count nouns Baldal car Bunny together and the animate nouns are separate because presumably because they are more often specifies of verbs which require.",
                    "label": 0
                },
                {
                    "sent": "Animal agents and the determiners are form a separate category.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So on the next slide, what you see is the relational structure, which is inferred over this tree.",
                    "label": 0
                },
                {
                    "sent": "These are the two relations specifier of and complement of and what you can see here in green is the if.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Remember, the terminal is in red, so if you were.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hold on a second.",
                    "label": 0
                },
                {
                    "sent": "OK, so the determiners are the specifiers of the noun class.",
                    "label": 0
                },
                {
                    "sent": "This is what this picture on the left shows and.",
                    "label": 0
                },
                {
                    "sent": "So say verbs take all verbs as complements, including, say, say, verbs.",
                    "label": 0
                },
                {
                    "sent": "This is what the picture on the right shows quite nicely.",
                    "label": 0
                },
                {
                    "sent": "There is some other nice structure in those matrices which I'm not going to go over, so given this result, we kind of wanted to try more experiments on real data and we went to the British National Corpus and.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We tried various experiments, but the real world is not as neat.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Does the X bar theory relations, in particular the first problem, is the of course, the British National Corpus is a huge corpus.",
                    "label": 0
                },
                {
                    "sent": "It contains 100 million words, which are way too many for this algorithm.",
                    "label": 0
                },
                {
                    "sent": "At this stage it can handle about 100 or 200 words to create an annotated hierarchy and breaks down after that.",
                    "label": 0
                },
                {
                    "sent": "So in order to try it with some real data, what we did is we extracted the representative subset of frequent.",
                    "label": 0
                },
                {
                    "sent": "Nouns in this corpus and then as relational data for the algorithm we extracted semantic role relations from utility code sketch engine, which allows you to pull out all the subject of object of uses of verb in the British National Corpus.",
                    "label": 1
                },
                {
                    "sent": "So we formed the.",
                    "label": 0
                },
                {
                    "sent": "These are from the nouns and the frequently occurring verbs, adverbs, and adjectives output from the sketch engine in response to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Aries about these nouns.",
                    "label": 0
                },
                {
                    "sent": "So in the first experiment, we collapsed all semantic relations which we found into a single dependency relation.",
                    "label": 1
                },
                {
                    "sent": "So instead of having John subject of read, now we just would have John dependent of read book, also dependent of Reed, and we added some semantic features manually to our set, some of which pick subsets of categories, some of which pick pretty much.",
                    "label": 0
                },
                {
                    "sent": "The whole categories, some of which do not pick actually span across categories.",
                    "label": 0
                },
                {
                    "sent": "So the feature concrete, for example, is almost entirely associated with nouns and with most nouns.",
                    "label": 0
                },
                {
                    "sent": "But the semantic feature intentional is associated only with the animate nouns and the semantic feature motion is.",
                    "label": 0
                },
                {
                    "sent": "Associated with verbs as well as adverbs, we also added morphological features where the same thing is true.",
                    "label": 0
                },
                {
                    "sent": "So the ING feature is associated exclusively with verbs, but the feature is split between nouns and verbs because now is take it as plural and verbs take it as third singular.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Third person singular.",
                    "label": 0
                },
                {
                    "sent": "So with this data, the annotated Clark is algorithm found all the main categories, but it didn't really find much internal structure within the classes.",
                    "label": 1
                },
                {
                    "sent": "And one interesting thing which happened in this experiment, and we're not sure whether that's just an artifact of the experiment or something, or there is something interesting there.",
                    "label": 0
                },
                {
                    "sent": "But there are the algorithms split.",
                    "label": 0
                },
                {
                    "sent": "The nouns and verbs on one hand from the adverbs and adjectives on the other.",
                    "label": 0
                },
                {
                    "sent": "And if you think that those are obligatory versus optional arguments and it's kind of interesting that maybe on the basis of the dependency relations.",
                    "label": 0
                },
                {
                    "sent": "This split comes out.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the tree.",
                    "label": 0
                },
                {
                    "sent": "We have the nouns on the left and then the verbs next to them and then on the right.",
                    "label": 0
                },
                {
                    "sent": "The adverbs on the left, right and the furthest right.",
                    "label": 0
                },
                {
                    "sent": "We have the adjectives.",
                    "label": 0
                },
                {
                    "sent": "So in the final experiment, which I'm going to show today.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hey um.",
                    "label": 0
                },
                {
                    "sent": "What we decided to do with this data is we decided to throw in as much information as possible and see whether we're going to recover any internal structure from the from the real data.",
                    "label": 0
                },
                {
                    "sent": "So we actually threw in the original semantic role relations.",
                    "label": 1
                },
                {
                    "sent": "The subject of an object of relations rather than the dependency relation process.",
                    "label": 1
                },
                {
                    "sent": "We also kept the semantic features and the morphological features, but we also added the distribution of features.",
                    "label": 0
                },
                {
                    "sent": "Which are used before by intervenes as work and others, namely the frequent frames.",
                    "label": 0
                },
                {
                    "sent": "The most frequent frame.",
                    "label": 0
                },
                {
                    "sent": "So things like the is are going to be the preceding.",
                    "label": 0
                },
                {
                    "sent": "Now this is going to be now a distribution of fee.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with this type of data we did recover some internal structure.",
                    "label": 0
                },
                {
                    "sent": "So in particular, we uncovered the structure in the adverb class where we found all the temporal adverbs clustering together, so actually never already.",
                    "label": 0
                },
                {
                    "sent": "Also still, ever, usually now, and finally with some others inside.",
                    "label": 0
                },
                {
                    "sent": "And we also found all the place.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Adverbs of clustering together.",
                    "label": 0
                },
                {
                    "sent": "We also uncovered some structure in the verbs, in particular, the verbs go pass around start clustered separately from the transitive verbs.",
                    "label": 1
                },
                {
                    "sent": "In all of the experiments that we did, these which I presented here and others, the algorithm successfully recovered main categories.",
                    "label": 1
                },
                {
                    "sent": "But the internal structure is a D.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Frontmatter so, before I wrap up, I just want to say a few words about how this model compares with hierarchical clustering, which is another model of hierarchical hierarchical structure of syntactic categories.",
                    "label": 0
                },
                {
                    "sent": "So in terms of performance issues, correct clustering is better, because first of all it's faster, and managers to actually handle large datasets pretty easily.",
                    "label": 0
                },
                {
                    "sent": "The main reason why we want to push this model further is because.",
                    "label": 0
                },
                {
                    "sent": "The character clustering has some representational drawbacks which annotated hierarchies is trying to deal with in particular, correct clustering doesn't really associate relations and features with particular nodes in the hierarchy, and relations are not explicitly treated as relations, but rather converted to features.",
                    "label": 1
                },
                {
                    "sent": "And that's not a very theoretically elegant way of handling this.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Issue.",
                    "label": 0
                },
                {
                    "sent": "That said, of course our model of has a lot of improvement, requires a lot of improvement before it can actually compete on a large scale.",
                    "label": 0
                },
                {
                    "sent": "First of all, it has to be improved on scalability, and this is just an algorithmic issue of the search over trees.",
                    "label": 0
                },
                {
                    "sent": "We have done some improvements, but not enough to handle 100 million words we.",
                    "label": 0
                },
                {
                    "sent": "Also, the model as it stands now does not really have an intrinsic capability of dealing with noise, and the way we deal with noises we threshold externally the data which we collect.",
                    "label": 0
                },
                {
                    "sent": "So we try to find we put positive relation relation as true if it occurs and number of times and false if it occurs less than that.",
                    "label": 0
                },
                {
                    "sent": "And that's an artificial way of doing it, and especially in natural corpora that probably really hurts us because.",
                    "label": 0
                },
                {
                    "sent": "Of course they are noisy and sparse.",
                    "label": 1
                },
                {
                    "sent": "Another thing to do with the annotate characters model is to think about automatic feature selection.",
                    "label": 0
                },
                {
                    "sent": "So we have now given a lot to the model we have given the fact that we have assumed, for example, that can do morphological analysis of words, which is obviously not a solved problem as we heard in this in this workshop and many other places.",
                    "label": 0
                },
                {
                    "sent": "And the main challenge is to reduce the abstract relations with which it works quite well in the expert case to some sort of surface observable relations.",
                    "label": 0
                },
                {
                    "sent": "For example, the intuition is that their relation subject of should really be somehow observed from the precedence relations between the subject and the verb.",
                    "label": 0
                },
                {
                    "sent": "But it's a tricky thing to actually do it, because the subject can be arbitrarily far from the verb and various.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Issues like that.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion.",
                    "label": 0
                },
                {
                    "sent": "I have presented the model of syntactic categories which represents the hierarchical structure of those categories and assosiates relations and features with treatment partitions inside the hierarchy and the annotated characters of syntactic categories can be in principle in Ferd if the linguistic structure is known or is easy to deduce from surface relations.",
                    "label": 1
                },
                {
                    "sent": "But unsupervised learning from the data is really messy, even if.",
                    "label": 0
                },
                {
                    "sent": "A lot of features are granted to the learner.",
                    "label": 0
                },
                {
                    "sent": "That's it, thank you.",
                    "label": 0
                }
            ]
        }
    }
}