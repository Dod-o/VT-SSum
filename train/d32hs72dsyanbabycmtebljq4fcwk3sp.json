{
    "id": "d32hs72dsyanbabycmtebljq4fcwk3sp",
    "title": "Spectral Clustering Based on the Graph p-Laplacian",
    "info": {
        "author": [
            "Thomas B\u00fchler, Department of Computer Science, Saarland University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/icml09_buhler_scb/",
    "segmentation": [
        [
            "OK so hello everybody.",
            "If she already said my name is Thomas Villa, I'm a PhD student from Saarland University and today I want to present some work I've done together with Matthias.",
            "I'm so the topic is generalization of spectral clustering using the eigenvectors of the graph P Laplacian.",
            "Too loud, yeah, it's better like this.",
            "OK.",
            "So let me just start with the description of the general setting.",
            "If I get this.",
            "Yeah, OK."
        ],
        [
            "So you have any data with some.",
            "With some so you have any any data in some notion of similarity between points and your goal is to divide your data into meaningful subsets.",
            "This can be any kind of data ranging from images to social networks.",
            "So what you do?"
        ],
        [
            "You can model this problem as a graph partitioning problem, so your data points so.",
            "OK, sorry, so you can model this problem as a graph partitioning problem so your data can be represented as a weighted undirected graph where the edges represent the similarities and the voters represents the data points and then you just partition.",
            "Drop your clustering program as this graph partitioning program OK so."
        ],
        [
            "Now we can ask what is the criteria we want to optimize.",
            "So how exactly do you make this partition so there are two goals you want to achieve.",
            "So first of all you want to minimize the the edge weight between your clusters.",
            "So you want to minimize this discut, but on the other hand you also want your clusters to be balanced in the sense of the size is doesn't differ too much.",
            "Different ways how to understand a sentence.",
            "Size of a cluster.",
            "So one way would be to favor solutions where the cardinality of the clusters is roughly equal, and this is done by the ratio cut.",
            "Which you see here, an closely related ratio, Cheeger cut, which is just a slightly different balancing behavior, and then on the other hand, you could also favor solutions where you have equal volumes and.",
            "By volume, I mean some of the degrees in the cluster and the degree is again the sum of the adjacent edge rates.",
            "So here again, you have to normalize cut.",
            "And closely related to, normally this trigger two year cut, so these problems can be motivated to be useful for clustering.",
            "But the problem is that unlike minimizing the cut, which is can be done polynomial time, these are NP hard problems.",
            "So we have to relax them somehow.",
            "And probably aware spectral clustering and I'm going to briefly briefly discuss the standard spectral relaxation."
        ],
        [
            "These problems, so first of all you can reformulate your problem as a labeling problem.",
            "So for any partitioning you can define a function F like this, which assigns a positive label to one cluster and negative label to the other cluster and then.",
            "The problem of finding of minimizing ratio cuts is equivalent to problem of minimizing this functional.",
            "Where you minimize overall functions of of this type.",
            "OK on here you see, this is the graph Laplacian which is defined as D -- W, where D is the matrix having the degree on the diagonal OK. OK, so now we've just reformulated or."
        ],
        [
            "Open now we will exit by emitting arbitrary functions but keeping one property of these types of functions, namely that orthogonal to the constant vector.",
            "And if you know minimize this expression but really risk principle, this is the smallest eigenvector.",
            "And as we know that the smallest eigenvectors the constant vector, this whole expression gives you the second eigenvector.",
            "OK, and now we from this second item."
        ],
        [
            "Services, which is a relaxation of the bands draft cut.",
            "We now transform this into a partitioning by thresholding and the threshold is just obtains such that we have to choose a threshold which gives us the best cut.",
            "OK now one."
        ],
        [
            "Ask how good is this partition?",
            "And when?"
        ],
        [
            "Unrevised from the spectrograph.",
            "Few theory is."
        ],
        [
            "Isoparametric inequality which relates the.",
            "The second eigenvalue, to the optimal Cheeger cut and not so well known."
        ],
        [
            "What is the following new quality which relates the obtained shirqat after thresholding to the optimal Cheeger cut and you see we have this lower bound, which is quite trivial and the upper bound you see it's actually it's quite loose since we have this square root here and one construct examples where we actually achieved this upper bound.",
            "So.",
            "I will let us show you something, so in this case, so this is a result from spec."
        ],
        [
            "Trusting standard spectral clustering and you see I mean the optimal trusting in this case is clear would be just dividing into two moons and spectral clustering will give you this results, which is clearly not optimal.",
            "So we want to find a different relaxation of this balance graph cuts which gives us waiting where you can give better quality guarantees.",
            "In effect, later on I will show you an equal inequality which looks similar to this one.",
            "In fact, isn't generalization of this inequality, and the interesting thing is that the upper bound becomes tight.",
            "In some limit OK, and the resulting clustering.",
            "Looks like this.",
            "OK, but before I am going to introduce you.",
            "Presenting my method, we have to dig a bit deeper into theory.",
            "So every failure seen by minimized by computing the Eigenvector V minimizing implicitly minimizing this quadratic form which is induced through the inner product by then induced by the Laplacian."
        ],
        [
            "Through this inner product and this can be now interpreted as a smoothness functional, where you penalize the discrete version of the great event quadratically.",
            "Now this is the quadratic panelization.",
            "No one can ask what is the operator which gives us the different finalization.",
            "Why?"
        ],
        [
            "Just replace 2 by P. And think about it, if you know decrease P from 2 to one.",
            "It's just basic response to an L1 regularization while we.",
            "And this should basically gives us peace and constant result if you penalize the gradient with everyone and not L2.",
            "And that's actually what we want, since this more corresponds to this indicator function, which I showed you earlier.",
            "So back to my question, what if what is this operator which induces this form and you might have seen the commonness appeal of Lawson?",
            "Um?",
            "So this definition, and if you just have a closer look at this expression here, with black and peaks two, you basically get back the standard graph Laplacian.",
            "And just written component wise.",
            "OK, so now we want to use the eigenvectors of the graph of the P Laplacian in a similar similar way as the actors in the standard case.",
            "Now one has of course the question how would you define eigenvectors of such a nonlinear operator?",
            "Another way is to do is like this, so this makes kind of complicated at first, but it can be clearly motivated as.",
            "As the eigenvectors are the critical points of this expression, so this is generalization of the Rayleigh Ritz principle.",
            "So if you take this and just take the derivative and then you basically get get back to this one.",
            "And just as they say."
        ],
        [
            "And to check if it just broke plug-in peak is 2."
        ],
        [
            "You will get back a standard eigenvalue."
        ],
        [
            "OK, but this problem is difficult to solve.",
            "And actually, we're interested."
        ],
        [
            "The second line values, So what we use is the following variation in characterization of the second time value.",
            "So you can modify this function are in such a way that you get the 2nd second line value.",
            "So what?",
            "What control is that for any P?",
            "The minimum of this expression is the 2nd eigenvalue of the graph below plus.",
            "And this minimum is achieved on the whole subspace spanned by the eigenvector on the constant vector.",
            "So you can easily, once you have one minimizer, you can easily compute the the the eigenvector from it."
        ],
        [
            "OK, so now we know how we how we can compute the eigenvector.",
            "And now I show you that it's actually useful to do this.",
            "So first of all, as in the case, peak peak is 2 you can see.",
            "The.",
            "Direct appeal of lesson as a relaxation of graph cuts so.",
            "For each partitioning, you can define a function F such that the function F FP2 evaluated at FPC is equal to this expression and this is a general for."
        ],
        [
            "Home of a balanced graph cut since for peak it's two.",
            "We get duration cut and for Pete was one."
        ],
        [
            "You get the racial trigger code.",
            "OK, now the argument is that the reason is the same as in the standard case minimizing ratio minimizing this cut.",
            "It's equivalent to minimizing this functional restricted to functions of this type.",
            "And when we omit this restriction, we get a second eigenvector."
        ],
        [
            "OK, and now we proceed as an extended case.",
            "We just threshold and the special just ruined such that we get the minimum ratio T cut."
        ],
        [
            "OK, now this is the inequality which I promised you earlier.",
            "This is basically the main motivation to do P spectral clustering at all so.",
            "Here again we have to take a cut which we obtained by this scheme.",
            "And this is the document she got cut, so lower bound is quite trivial in the upper bound.",
            "First of all, if you plug in pickets two, you got the info."
        ],
        [
            "Degree, which is which we see in earlier.",
            "And Interestingly, in the limit of P towards one, this upper bound becomes tight.",
            "So this means in the limit of P = 1, we.",
            "Basically getting the optimal cut, and this is actually this really amazing result I think.",
            "Um?",
            "So this motivated us to do this at all.",
            "Um?",
            "OK, now we've seen that we compute the eigenvector.",
            "Know that the activator is useful for crusting, and we seen that we basically computing by minimizing this function FP2.",
            "But the part I neglected so far this hour exactly minimize minimizing, do we?"
        ],
        [
            "Guys this.",
            "Since it's non convex so it's difficult to solve since you're likely to get stuck in the local minima.",
            "So what we do?",
            "We saw the sequence of problems."
        ],
        [
            "We start with the function for periods two, so here it's the solution can be directly computed since it's the eigenvector of the standard graph Laplacian, and then we gradually decrease P. Solve this sequence of problems and in each step used solution of the previous step as initialization.",
            "The motivation is FP 2 is.",
            "As is continuous in P. If you have a local minima minimum of FP1.",
            "Then the local minimum of FP 2.",
            "Then there should be close to each other.",
            "If P1 is close to P2.",
            "And Furthermore, if we minimize this subproblems, we are Newton like method, then convergence should also be fast.",
            "If we're close to initialization is good.",
            "So we we haven't proven this scheme to the conversion of the scheme, but the experiments it looks quite good, so and experiment.",
            "I'm going to show you I'm going to show you now.",
            "So."
        ],
        [
            "In this theoretical analyze analysis, I restricted myself not to racial Cheeger cut, but remember, I also introduced the normalized cut and basically the similar way you can do the same for the normalized cut, and this actually is no result from normalized cut.",
            "So you see this to Moon's data set, so we have two done 2 dimensions.",
            "So we have this to moon shape in two dimension and the whole thing is embedded in 100 dimensional space.",
            "So this is high dimensional and and also there's some noise edit and this makes this non adult trivia so it's actually quite complicated which can be seen in this K nearest neighbor graph.",
            "So it's quite complicated.",
            "And our first role shows you.",
            "Results for different values of P. So the eigenvectors which we which we computed.",
            "So each point corresponds to.",
            "One component of the eigenvector.",
            "And the color coding represents the values of the director.",
            "So you see, in uppercase P gets to.",
            "The transition is rather smoothly, whereas in case because 1.1 you have basically the same value here and the same way here and only here some smaller translation.",
            "Sorry, I'm shaking a little.",
            "And this is actually what we predicted.",
            "We predicted that it should become piecewise constant and the famous represented in this histogram of the values of the Eigenvector case.",
            "He gets two we value from minus 0.42, zero point 4 and basically everything in between also.",
            "And in case the peak is 1.1, you basically have only these two peaks.",
            "And this should more resemble this optimal indicator function.",
            "And actually, if you do know the thresholding like can't go anything wrong, so you basically get almost perfect clustering.",
            "In contrast to the crusting obtained by standard spectral clustering.",
            "OK, so now about the cuts you see here for decreasing values P for 2 from 2 to 1.1.",
            "Decreasing value of normalized cut and light and dark blue and no mestier cut in red and also you see what you can.",
            "You can also show that the second eigenvalue converges to the optimal Cheeger cut, which is depicted in this green line.",
            "And Furthermore, we also computed the arrow by by comparing this result to the optimal to the obvious ground truth in this case, and you see.",
            "What's quite impressive?",
            "Does the arrow is also really decreased, so this means that we are not only degrees in the tigr cut."
        ],
        [
            "It also means that it's useful, decreased, and she cannot.",
            "Yeah.",
            "OK, so we perform several other experiments and this is on two large datasets.",
            "Your first name list, so 10,000 and 77 points.",
            "And the modern two classes, obtained by recursive trusting screen where you use this.",
            "Generalization of Ratio Cat's mind partition criteria.",
            "So here again we."
        ],
        [
            "Results and again we see that ratio cut.",
            "Anne."
        ],
        [
            "And the ratio cut this degree Zing for UCS and both use person emnace.",
            "But this year we are approximating an NP hard problems, so it has to become difficult and more difficult in some way and this is reflected in.",
            "This is reflected in the runtime.",
            "So you see it's increasing.",
            "But on the other hand, I mean you put some more effort into this integrant computation, but I think it really pays off because you're not only degrees in ratio to cut, but also you see a really big difference in the error.",
            "So let me conclude."
        ],
        [
            "We proposed generalization of spectral clustering using the graph below plus in.",
            "To compute the directors of craft beer Fassinger proposed an American scheme which is based on the variation code violation of the second eigenvector of the Pylab lawsuit."
        ],
        [
            "This can be moated from so the theoretical background is quite strong, since we can also see this as a relaxation of spans.",
            "Graph cuts, as in the standard case.",
            "And this is the main result.",
            "The convergence of the cut converges, so the cut converges to the optimal Cheeger cut.",
            "When we decrease P from 2 to one."
        ],
        [
            "Or two towards one.",
            "Any experiments confirmed or results we saw in all cases decreasing Cheeger cut and Rachel cut values.",
            "And also the keys in decreasing errors which show that we actually really compute and better clustering.",
            "So thank you for your attention and yeah, feel free to ask questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so hello everybody.",
                    "label": 0
                },
                {
                    "sent": "If she already said my name is Thomas Villa, I'm a PhD student from Saarland University and today I want to present some work I've done together with Matthias.",
                    "label": 0
                },
                {
                    "sent": "I'm so the topic is generalization of spectral clustering using the eigenvectors of the graph P Laplacian.",
                    "label": 1
                },
                {
                    "sent": "Too loud, yeah, it's better like this.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let me just start with the description of the general setting.",
                    "label": 0
                },
                {
                    "sent": "If I get this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you have any data with some.",
                    "label": 1
                },
                {
                    "sent": "With some so you have any any data in some notion of similarity between points and your goal is to divide your data into meaningful subsets.",
                    "label": 0
                },
                {
                    "sent": "This can be any kind of data ranging from images to social networks.",
                    "label": 0
                },
                {
                    "sent": "So what you do?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can model this problem as a graph partitioning problem, so your data points so.",
                    "label": 0
                },
                {
                    "sent": "OK, sorry, so you can model this problem as a graph partitioning problem so your data can be represented as a weighted undirected graph where the edges represent the similarities and the voters represents the data points and then you just partition.",
                    "label": 1
                },
                {
                    "sent": "Drop your clustering program as this graph partitioning program OK so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can ask what is the criteria we want to optimize.",
                    "label": 0
                },
                {
                    "sent": "So how exactly do you make this partition so there are two goals you want to achieve.",
                    "label": 0
                },
                {
                    "sent": "So first of all you want to minimize the the edge weight between your clusters.",
                    "label": 0
                },
                {
                    "sent": "So you want to minimize this discut, but on the other hand you also want your clusters to be balanced in the sense of the size is doesn't differ too much.",
                    "label": 0
                },
                {
                    "sent": "Different ways how to understand a sentence.",
                    "label": 0
                },
                {
                    "sent": "Size of a cluster.",
                    "label": 0
                },
                {
                    "sent": "So one way would be to favor solutions where the cardinality of the clusters is roughly equal, and this is done by the ratio cut.",
                    "label": 0
                },
                {
                    "sent": "Which you see here, an closely related ratio, Cheeger cut, which is just a slightly different balancing behavior, and then on the other hand, you could also favor solutions where you have equal volumes and.",
                    "label": 0
                },
                {
                    "sent": "By volume, I mean some of the degrees in the cluster and the degree is again the sum of the adjacent edge rates.",
                    "label": 0
                },
                {
                    "sent": "So here again, you have to normalize cut.",
                    "label": 0
                },
                {
                    "sent": "And closely related to, normally this trigger two year cut, so these problems can be motivated to be useful for clustering.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that unlike minimizing the cut, which is can be done polynomial time, these are NP hard problems.",
                    "label": 0
                },
                {
                    "sent": "So we have to relax them somehow.",
                    "label": 0
                },
                {
                    "sent": "And probably aware spectral clustering and I'm going to briefly briefly discuss the standard spectral relaxation.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These problems, so first of all you can reformulate your problem as a labeling problem.",
                    "label": 0
                },
                {
                    "sent": "So for any partitioning you can define a function F like this, which assigns a positive label to one cluster and negative label to the other cluster and then.",
                    "label": 0
                },
                {
                    "sent": "The problem of finding of minimizing ratio cuts is equivalent to problem of minimizing this functional.",
                    "label": 0
                },
                {
                    "sent": "Where you minimize overall functions of of this type.",
                    "label": 0
                },
                {
                    "sent": "OK on here you see, this is the graph Laplacian which is defined as D -- W, where D is the matrix having the degree on the diagonal OK. OK, so now we've just reformulated or.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Open now we will exit by emitting arbitrary functions but keeping one property of these types of functions, namely that orthogonal to the constant vector.",
                    "label": 0
                },
                {
                    "sent": "And if you know minimize this expression but really risk principle, this is the smallest eigenvector.",
                    "label": 0
                },
                {
                    "sent": "And as we know that the smallest eigenvectors the constant vector, this whole expression gives you the second eigenvector.",
                    "label": 0
                },
                {
                    "sent": "OK, and now we from this second item.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Services, which is a relaxation of the bands draft cut.",
                    "label": 0
                },
                {
                    "sent": "We now transform this into a partitioning by thresholding and the threshold is just obtains such that we have to choose a threshold which gives us the best cut.",
                    "label": 0
                },
                {
                    "sent": "OK now one.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ask how good is this partition?",
                    "label": 0
                },
                {
                    "sent": "And when?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unrevised from the spectrograph.",
                    "label": 0
                },
                {
                    "sent": "Few theory is.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Isoparametric inequality which relates the.",
                    "label": 0
                },
                {
                    "sent": "The second eigenvalue, to the optimal Cheeger cut and not so well known.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is the following new quality which relates the obtained shirqat after thresholding to the optimal Cheeger cut and you see we have this lower bound, which is quite trivial and the upper bound you see it's actually it's quite loose since we have this square root here and one construct examples where we actually achieved this upper bound.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I will let us show you something, so in this case, so this is a result from spec.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trusting standard spectral clustering and you see I mean the optimal trusting in this case is clear would be just dividing into two moons and spectral clustering will give you this results, which is clearly not optimal.",
                    "label": 0
                },
                {
                    "sent": "So we want to find a different relaxation of this balance graph cuts which gives us waiting where you can give better quality guarantees.",
                    "label": 0
                },
                {
                    "sent": "In effect, later on I will show you an equal inequality which looks similar to this one.",
                    "label": 0
                },
                {
                    "sent": "In fact, isn't generalization of this inequality, and the interesting thing is that the upper bound becomes tight.",
                    "label": 0
                },
                {
                    "sent": "In some limit OK, and the resulting clustering.",
                    "label": 0
                },
                {
                    "sent": "Looks like this.",
                    "label": 0
                },
                {
                    "sent": "OK, but before I am going to introduce you.",
                    "label": 0
                },
                {
                    "sent": "Presenting my method, we have to dig a bit deeper into theory.",
                    "label": 0
                },
                {
                    "sent": "So every failure seen by minimized by computing the Eigenvector V minimizing implicitly minimizing this quadratic form which is induced through the inner product by then induced by the Laplacian.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through this inner product and this can be now interpreted as a smoothness functional, where you penalize the discrete version of the great event quadratically.",
                    "label": 0
                },
                {
                    "sent": "Now this is the quadratic panelization.",
                    "label": 0
                },
                {
                    "sent": "No one can ask what is the operator which gives us the different finalization.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just replace 2 by P. And think about it, if you know decrease P from 2 to one.",
                    "label": 0
                },
                {
                    "sent": "It's just basic response to an L1 regularization while we.",
                    "label": 0
                },
                {
                    "sent": "And this should basically gives us peace and constant result if you penalize the gradient with everyone and not L2.",
                    "label": 0
                },
                {
                    "sent": "And that's actually what we want, since this more corresponds to this indicator function, which I showed you earlier.",
                    "label": 0
                },
                {
                    "sent": "So back to my question, what if what is this operator which induces this form and you might have seen the commonness appeal of Lawson?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this definition, and if you just have a closer look at this expression here, with black and peaks two, you basically get back the standard graph Laplacian.",
                    "label": 1
                },
                {
                    "sent": "And just written component wise.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we want to use the eigenvectors of the graph of the P Laplacian in a similar similar way as the actors in the standard case.",
                    "label": 0
                },
                {
                    "sent": "Now one has of course the question how would you define eigenvectors of such a nonlinear operator?",
                    "label": 0
                },
                {
                    "sent": "Another way is to do is like this, so this makes kind of complicated at first, but it can be clearly motivated as.",
                    "label": 0
                },
                {
                    "sent": "As the eigenvectors are the critical points of this expression, so this is generalization of the Rayleigh Ritz principle.",
                    "label": 0
                },
                {
                    "sent": "So if you take this and just take the derivative and then you basically get get back to this one.",
                    "label": 0
                },
                {
                    "sent": "And just as they say.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to check if it just broke plug-in peak is 2.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You will get back a standard eigenvalue.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, but this problem is difficult to solve.",
                    "label": 0
                },
                {
                    "sent": "And actually, we're interested.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second line values, So what we use is the following variation in characterization of the second time value.",
                    "label": 0
                },
                {
                    "sent": "So you can modify this function are in such a way that you get the 2nd second line value.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "What control is that for any P?",
                    "label": 0
                },
                {
                    "sent": "The minimum of this expression is the 2nd eigenvalue of the graph below plus.",
                    "label": 0
                },
                {
                    "sent": "And this minimum is achieved on the whole subspace spanned by the eigenvector on the constant vector.",
                    "label": 0
                },
                {
                    "sent": "So you can easily, once you have one minimizer, you can easily compute the the the eigenvector from it.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we know how we how we can compute the eigenvector.",
                    "label": 0
                },
                {
                    "sent": "And now I show you that it's actually useful to do this.",
                    "label": 0
                },
                {
                    "sent": "So first of all, as in the case, peak peak is 2 you can see.",
                    "label": 1
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Direct appeal of lesson as a relaxation of graph cuts so.",
                    "label": 1
                },
                {
                    "sent": "For each partitioning, you can define a function F such that the function F FP2 evaluated at FPC is equal to this expression and this is a general for.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Home of a balanced graph cut since for peak it's two.",
                    "label": 0
                },
                {
                    "sent": "We get duration cut and for Pete was one.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You get the racial trigger code.",
                    "label": 0
                },
                {
                    "sent": "OK, now the argument is that the reason is the same as in the standard case minimizing ratio minimizing this cut.",
                    "label": 0
                },
                {
                    "sent": "It's equivalent to minimizing this functional restricted to functions of this type.",
                    "label": 0
                },
                {
                    "sent": "And when we omit this restriction, we get a second eigenvector.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and now we proceed as an extended case.",
                    "label": 0
                },
                {
                    "sent": "We just threshold and the special just ruined such that we get the minimum ratio T cut.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now this is the inequality which I promised you earlier.",
                    "label": 0
                },
                {
                    "sent": "This is basically the main motivation to do P spectral clustering at all so.",
                    "label": 0
                },
                {
                    "sent": "Here again we have to take a cut which we obtained by this scheme.",
                    "label": 0
                },
                {
                    "sent": "And this is the document she got cut, so lower bound is quite trivial in the upper bound.",
                    "label": 0
                },
                {
                    "sent": "First of all, if you plug in pickets two, you got the info.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Degree, which is which we see in earlier.",
                    "label": 0
                },
                {
                    "sent": "And Interestingly, in the limit of P towards one, this upper bound becomes tight.",
                    "label": 0
                },
                {
                    "sent": "So this means in the limit of P = 1, we.",
                    "label": 1
                },
                {
                    "sent": "Basically getting the optimal cut, and this is actually this really amazing result I think.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this motivated us to do this at all.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, now we've seen that we compute the eigenvector.",
                    "label": 0
                },
                {
                    "sent": "Know that the activator is useful for crusting, and we seen that we basically computing by minimizing this function FP2.",
                    "label": 0
                },
                {
                    "sent": "But the part I neglected so far this hour exactly minimize minimizing, do we?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Guys this.",
                    "label": 0
                },
                {
                    "sent": "Since it's non convex so it's difficult to solve since you're likely to get stuck in the local minima.",
                    "label": 0
                },
                {
                    "sent": "So what we do?",
                    "label": 0
                },
                {
                    "sent": "We saw the sequence of problems.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We start with the function for periods two, so here it's the solution can be directly computed since it's the eigenvector of the standard graph Laplacian, and then we gradually decrease P. Solve this sequence of problems and in each step used solution of the previous step as initialization.",
                    "label": 1
                },
                {
                    "sent": "The motivation is FP 2 is.",
                    "label": 1
                },
                {
                    "sent": "As is continuous in P. If you have a local minima minimum of FP1.",
                    "label": 0
                },
                {
                    "sent": "Then the local minimum of FP 2.",
                    "label": 1
                },
                {
                    "sent": "Then there should be close to each other.",
                    "label": 1
                },
                {
                    "sent": "If P1 is close to P2.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, if we minimize this subproblems, we are Newton like method, then convergence should also be fast.",
                    "label": 0
                },
                {
                    "sent": "If we're close to initialization is good.",
                    "label": 0
                },
                {
                    "sent": "So we we haven't proven this scheme to the conversion of the scheme, but the experiments it looks quite good, so and experiment.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you I'm going to show you now.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this theoretical analyze analysis, I restricted myself not to racial Cheeger cut, but remember, I also introduced the normalized cut and basically the similar way you can do the same for the normalized cut, and this actually is no result from normalized cut.",
                    "label": 0
                },
                {
                    "sent": "So you see this to Moon's data set, so we have two done 2 dimensions.",
                    "label": 0
                },
                {
                    "sent": "So we have this to moon shape in two dimension and the whole thing is embedded in 100 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So this is high dimensional and and also there's some noise edit and this makes this non adult trivia so it's actually quite complicated which can be seen in this K nearest neighbor graph.",
                    "label": 0
                },
                {
                    "sent": "So it's quite complicated.",
                    "label": 0
                },
                {
                    "sent": "And our first role shows you.",
                    "label": 0
                },
                {
                    "sent": "Results for different values of P. So the eigenvectors which we which we computed.",
                    "label": 0
                },
                {
                    "sent": "So each point corresponds to.",
                    "label": 0
                },
                {
                    "sent": "One component of the eigenvector.",
                    "label": 0
                },
                {
                    "sent": "And the color coding represents the values of the director.",
                    "label": 0
                },
                {
                    "sent": "So you see, in uppercase P gets to.",
                    "label": 0
                },
                {
                    "sent": "The transition is rather smoothly, whereas in case because 1.1 you have basically the same value here and the same way here and only here some smaller translation.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I'm shaking a little.",
                    "label": 0
                },
                {
                    "sent": "And this is actually what we predicted.",
                    "label": 0
                },
                {
                    "sent": "We predicted that it should become piecewise constant and the famous represented in this histogram of the values of the Eigenvector case.",
                    "label": 0
                },
                {
                    "sent": "He gets two we value from minus 0.42, zero point 4 and basically everything in between also.",
                    "label": 0
                },
                {
                    "sent": "And in case the peak is 1.1, you basically have only these two peaks.",
                    "label": 0
                },
                {
                    "sent": "And this should more resemble this optimal indicator function.",
                    "label": 0
                },
                {
                    "sent": "And actually, if you do know the thresholding like can't go anything wrong, so you basically get almost perfect clustering.",
                    "label": 0
                },
                {
                    "sent": "In contrast to the crusting obtained by standard spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "OK, so now about the cuts you see here for decreasing values P for 2 from 2 to 1.1.",
                    "label": 0
                },
                {
                    "sent": "Decreasing value of normalized cut and light and dark blue and no mestier cut in red and also you see what you can.",
                    "label": 0
                },
                {
                    "sent": "You can also show that the second eigenvalue converges to the optimal Cheeger cut, which is depicted in this green line.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, we also computed the arrow by by comparing this result to the optimal to the obvious ground truth in this case, and you see.",
                    "label": 0
                },
                {
                    "sent": "What's quite impressive?",
                    "label": 0
                },
                {
                    "sent": "Does the arrow is also really decreased, so this means that we are not only degrees in the tigr cut.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It also means that it's useful, decreased, and she cannot.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so we perform several other experiments and this is on two large datasets.",
                    "label": 0
                },
                {
                    "sent": "Your first name list, so 10,000 and 77 points.",
                    "label": 0
                },
                {
                    "sent": "And the modern two classes, obtained by recursive trusting screen where you use this.",
                    "label": 0
                },
                {
                    "sent": "Generalization of Ratio Cat's mind partition criteria.",
                    "label": 0
                },
                {
                    "sent": "So here again we.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results and again we see that ratio cut.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the ratio cut this degree Zing for UCS and both use person emnace.",
                    "label": 0
                },
                {
                    "sent": "But this year we are approximating an NP hard problems, so it has to become difficult and more difficult in some way and this is reflected in.",
                    "label": 0
                },
                {
                    "sent": "This is reflected in the runtime.",
                    "label": 0
                },
                {
                    "sent": "So you see it's increasing.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, I mean you put some more effort into this integrant computation, but I think it really pays off because you're not only degrees in ratio to cut, but also you see a really big difference in the error.",
                    "label": 0
                },
                {
                    "sent": "So let me conclude.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We proposed generalization of spectral clustering using the graph below plus in.",
                    "label": 0
                },
                {
                    "sent": "To compute the directors of craft beer Fassinger proposed an American scheme which is based on the variation code violation of the second eigenvector of the Pylab lawsuit.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This can be moated from so the theoretical background is quite strong, since we can also see this as a relaxation of spans.",
                    "label": 0
                },
                {
                    "sent": "Graph cuts, as in the standard case.",
                    "label": 0
                },
                {
                    "sent": "And this is the main result.",
                    "label": 0
                },
                {
                    "sent": "The convergence of the cut converges, so the cut converges to the optimal Cheeger cut.",
                    "label": 1
                },
                {
                    "sent": "When we decrease P from 2 to one.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or two towards one.",
                    "label": 0
                },
                {
                    "sent": "Any experiments confirmed or results we saw in all cases decreasing Cheeger cut and Rachel cut values.",
                    "label": 0
                },
                {
                    "sent": "And also the keys in decreasing errors which show that we actually really compute and better clustering.",
                    "label": 0
                },
                {
                    "sent": "So thank you for your attention and yeah, feel free to ask questions.",
                    "label": 0
                }
            ]
        }
    }
}