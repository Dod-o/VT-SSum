{
    "id": "eidflwpp24babq7qfdsssswdbnoungqv",
    "title": "Sparse Estimation with Structured Dictionaries",
    "info": {
        "author": [
            "David P Wipf, Microsoft Research Asia, Microsoft Research"
        ],
        "published": "Sept. 6, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Feature Selection"
        ]
    },
    "url": "http://videolectures.net/nips2011_wipf_estimation/",
    "segmentation": [
        [
            "Hello so this is basically a talk about the sparse estimation problem in the Canonical sense.",
            "Just minimizing the number of non zeros in some vector X subject to linear constraint y = 5 X The Phi is a overcomplete dictionary, which means there are more columns than rows.",
            "You could also add a noise term if you prefer for to model some uncertainty but and this is for simplicity we consider the.",
            "Fully constrained case, in general this is a non convex optimization problem and it's combinatorial so brute force techniques are not an option.",
            "So the most common are at least one of the most common things to do is make a convex relaxation of the L0 norm using the L1 norm and.",
            "Variety of situations that L1 norm solution will be exactly identical to the L0 norm solution, provided this dictionary Phi is sufficiently unstructured and I'll say what that means.",
            "But also I should mention that this file includes in compressive sensing it it includes the measurement matrix.",
            "It includes a sparsely transform, all sorts of things.",
            "For this talk, it's just a generic collection of basis vectors."
        ],
        [
            "So structured versus unstructured dictionaries is really about the correlation between the columns of the dictionary.",
            "So in the unstructured case basically there the off diagonal terms are almost 0.",
            "They're not exactly 0 because it's an Overcomplete dictionary, but there are generally assumed to be small and in the diagonal terms can be large, so some examples are Gaussian ID dictionary where each element is sampled from a Gaussian.",
            "Or you could take a discrete for your.",
            "Transform and then randomly sample rows of the discrete formula transforms, so these would be examples of an unstructured dictionary and then a structured dictionary is where you have lots of correlations off the diagonal, and so an example of how you could get a structured dictionary is taken unstructured one and right or left multiply it by something that adds correlations.",
            "So on the left you could have an arbitrary invertible matrix on the right.",
            "Perhaps you could have a block diagonal matrix which is group clustering.",
            "Groups of dictionary columns.",
            "So the problem is in the case on the left.",
            "I'm sorry on on the right, basically none of the equivalent results for L1L0 hold, so you can.",
            "There's no longer really any guarantees whether or not the convex relaxation will give you the maximally sparse solution."
        ],
        [
            "So here's one new strategy we thought about, which is basically to apply a dictionary dependent projection that somehow compensates for the structure.",
            "So you take your original vector X, you map it to a new Space Z, and then in the new space Z you apply the exact same sorts of sparsity penalties that you normally use.",
            "So this problem here that function G could be any sort of concave nondecreasing sparsity penalty, but you apply it on the.",
            "Coefficients of Z, not on X, which is traditionally done and some constraints on this projection operator.",
            "It should compensate for the dictionary structure somehow, and the main thing is that if the Z is maximally sparse, then the X should be maximally sparse as well, so the idea is that somehow in this new space.",
            "Solving this sparsity problem is somehow equivalent to doing the L1 norm with an original unstructured dictionary."
        ],
        [
            "So I don't have time to give much of the results, but it's very convenient to solve this with reweighted L1 minimization, so you can use the same tools you used to solve a one norm relaxations and there's some provable performance improvements with structured dictionaries.",
            "So there's provable cases where L1 will fail and this will succeed.",
            "And so I'll talk about some of those at the poster.",
            "Here's just a quick toy example with a 50 by 100 dictionary, so I generate.",
            "Two dictionaries, one is structured, one is unstructured, the unstructured one is just Gaussian ID.",
            "Their structured one.",
            "You right multiply by a block sparse matrix D with four by four blocks, and then you generate a sparse vector X an you compare the difficulty of estimating X from observations using the unstructured dictionary versus observations using the structure dictionary and basically the proposed approach is the green and the blue line, and there's no change.",
            "The performance is exactly the same basically, but when you use the best L1 standard, one algorithm, the performance completely degrades into being.",
            "Completely inadequate for this problem, and that's the pink in the red line, so you should come to the poster for more details.",
            "And there's also a workshop where I talk about later results of this genre that extend the NIPS paper.",
            "So thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello so this is basically a talk about the sparse estimation problem in the Canonical sense.",
                    "label": 1
                },
                {
                    "sent": "Just minimizing the number of non zeros in some vector X subject to linear constraint y = 5 X The Phi is a overcomplete dictionary, which means there are more columns than rows.",
                    "label": 0
                },
                {
                    "sent": "You could also add a noise term if you prefer for to model some uncertainty but and this is for simplicity we consider the.",
                    "label": 0
                },
                {
                    "sent": "Fully constrained case, in general this is a non convex optimization problem and it's combinatorial so brute force techniques are not an option.",
                    "label": 1
                },
                {
                    "sent": "So the most common are at least one of the most common things to do is make a convex relaxation of the L0 norm using the L1 norm and.",
                    "label": 0
                },
                {
                    "sent": "Variety of situations that L1 norm solution will be exactly identical to the L0 norm solution, provided this dictionary Phi is sufficiently unstructured and I'll say what that means.",
                    "label": 0
                },
                {
                    "sent": "But also I should mention that this file includes in compressive sensing it it includes the measurement matrix.",
                    "label": 0
                },
                {
                    "sent": "It includes a sparsely transform, all sorts of things.",
                    "label": 1
                },
                {
                    "sent": "For this talk, it's just a generic collection of basis vectors.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So structured versus unstructured dictionaries is really about the correlation between the columns of the dictionary.",
                    "label": 0
                },
                {
                    "sent": "So in the unstructured case basically there the off diagonal terms are almost 0.",
                    "label": 0
                },
                {
                    "sent": "They're not exactly 0 because it's an Overcomplete dictionary, but there are generally assumed to be small and in the diagonal terms can be large, so some examples are Gaussian ID dictionary where each element is sampled from a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Or you could take a discrete for your.",
                    "label": 0
                },
                {
                    "sent": "Transform and then randomly sample rows of the discrete formula transforms, so these would be examples of an unstructured dictionary and then a structured dictionary is where you have lots of correlations off the diagonal, and so an example of how you could get a structured dictionary is taken unstructured one and right or left multiply it by something that adds correlations.",
                    "label": 0
                },
                {
                    "sent": "So on the left you could have an arbitrary invertible matrix on the right.",
                    "label": 0
                },
                {
                    "sent": "Perhaps you could have a block diagonal matrix which is group clustering.",
                    "label": 0
                },
                {
                    "sent": "Groups of dictionary columns.",
                    "label": 0
                },
                {
                    "sent": "So the problem is in the case on the left.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry on on the right, basically none of the equivalent results for L1L0 hold, so you can.",
                    "label": 0
                },
                {
                    "sent": "There's no longer really any guarantees whether or not the convex relaxation will give you the maximally sparse solution.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's one new strategy we thought about, which is basically to apply a dictionary dependent projection that somehow compensates for the structure.",
                    "label": 1
                },
                {
                    "sent": "So you take your original vector X, you map it to a new Space Z, and then in the new space Z you apply the exact same sorts of sparsity penalties that you normally use.",
                    "label": 0
                },
                {
                    "sent": "So this problem here that function G could be any sort of concave nondecreasing sparsity penalty, but you apply it on the.",
                    "label": 0
                },
                {
                    "sent": "Coefficients of Z, not on X, which is traditionally done and some constraints on this projection operator.",
                    "label": 0
                },
                {
                    "sent": "It should compensate for the dictionary structure somehow, and the main thing is that if the Z is maximally sparse, then the X should be maximally sparse as well, so the idea is that somehow in this new space.",
                    "label": 1
                },
                {
                    "sent": "Solving this sparsity problem is somehow equivalent to doing the L1 norm with an original unstructured dictionary.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I don't have time to give much of the results, but it's very convenient to solve this with reweighted L1 minimization, so you can use the same tools you used to solve a one norm relaxations and there's some provable performance improvements with structured dictionaries.",
                    "label": 0
                },
                {
                    "sent": "So there's provable cases where L1 will fail and this will succeed.",
                    "label": 0
                },
                {
                    "sent": "And so I'll talk about some of those at the poster.",
                    "label": 0
                },
                {
                    "sent": "Here's just a quick toy example with a 50 by 100 dictionary, so I generate.",
                    "label": 0
                },
                {
                    "sent": "Two dictionaries, one is structured, one is unstructured, the unstructured one is just Gaussian ID.",
                    "label": 0
                },
                {
                    "sent": "Their structured one.",
                    "label": 0
                },
                {
                    "sent": "You right multiply by a block sparse matrix D with four by four blocks, and then you generate a sparse vector X an you compare the difficulty of estimating X from observations using the unstructured dictionary versus observations using the structure dictionary and basically the proposed approach is the green and the blue line, and there's no change.",
                    "label": 1
                },
                {
                    "sent": "The performance is exactly the same basically, but when you use the best L1 standard, one algorithm, the performance completely degrades into being.",
                    "label": 0
                },
                {
                    "sent": "Completely inadequate for this problem, and that's the pink in the red line, so you should come to the poster for more details.",
                    "label": 0
                },
                {
                    "sent": "And there's also a workshop where I talk about later results of this genre that extend the NIPS paper.",
                    "label": 0
                },
                {
                    "sent": "So thank you.",
                    "label": 0
                }
            ]
        }
    }
}