{
    "id": "tggf4x5i4ycupwfyzyhjhowwkzg7reqd",
    "title": "Opening remarks on the Workshop Learning on Cores, Clusters, and Clouds",
    "info": {
        "author": [
            "John Langford, Microsoft Research"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Events"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_langford_or/",
    "segmentation": [
        [
            "This is the workshop on learning encores clusters and clouds.",
            "So Alec John.",
            "Over and Lawrence are the organizers.",
            "We had quite a bit of support for the workshop from Google, Microsoft Pascal, and Yahoo.",
            "Although donated money, which which is great because now we can pay for video lectures.",
            "OK, so."
        ],
        [
            "Let me let me tell you my cartoon understanding of why we're here so.",
            "There's a lot more sensors in the world.",
            "We're recording a lot more information.",
            "And that means that you have startups that are getting like a billion or 10 billion even events per day now.",
            "Really common, right?",
            "And that would be great if the machines are keeping up with the right that we're getting information, but.",
            "But they're not so the computers I think are getting faster, and this is actually a little bit surprised by this.",
            "So I have a desktop that I bought."
        ],
        [
            "About four years ago and Matt Hoffman has a laptop that he bought just recently and it's about a factor of 10 faster on some code that we were working on.",
            "So in fact, even though the Giga Hertz are not going up, the computers are getting faster, but there's just not keeping up right, so the number of events for computation is going up.",
            "So that's what's really driving us to think about.",
            "About this workshop.",
            "OK, so."
        ],
        [
            "What are the sort of core issues SO1 core issue which I think is is not going to be really discussed here, but which is incredibly important?",
            "Is efficiency of your algorithm.",
            "So there are efficient ways to do things in their inefficient ways to do things and we try to make sure the presentations are about reasonably efficient ways to do things, but.",
            "But that's probably emphasized to greater extent."
        ],
        [
            "If you are pondering doing parallel things.",
            "And then what the workshop is mostly about is parallelism and distribution.",
            "So often you end up with more data than any particular machine can handle, and then parallelism becomes essential.",
            "Often you have maybe the best algorithm just is computationally intense, and then parallelism can be helpful.",
            "And then when you get to very large amounts of data, the paradigm for how you deal with it is that you don't move the data anymore.",
            "You keep the data wherever it is and you move the computation to the data.",
            "And that's kind of a senchal when you think about sort of bandwidth limits and things like that.",
            "It also.",
            "By the way, can become essential because of potential privacy reasons.",
            "OK, so those are sort of the core."
        ],
        [
            "Issues that we're going to be thinking about.",
            "And let me tell you how this kind of comes up in everyday life.",
            "So the great example of.",
            "Of machine learning is spam and in the beginning this wasn't a problem.",
            "I remember back when there wasn't spam and.",
            "And you know you do is just an open protocol.",
            "You could create your own email server and I did.",
            "But then things changed.",
            "People started spamming and."
        ],
        [
            "And got worse and worse and worse and started making money.",
            "And it got worse and worse.",
            "And then then it became kind of impractical to have an individual email server because it was pretty hard to actually filter out the spam quickly enough.",
            "So so then you started getting.",
            "Large centralized email providers.",
            "Partly because the spam is so severe and these large email providers could get enough data that they could actually start to filter the spam it with a reasonably high success rate.",
            "Also, they could because they were large, they could afford to have a group of people working on spam which is.",
            "Question very helpful.",
            "So now, if you're at one of these large centralized email providers, the question becomes, how do you efficiently learn to classify spam in a parallel distributed environment?",
            "Right, and that's it's a problem of obvious significance to everyone.",
            "OK, so.",
            "So now let me."
        ],
        [
            "Provide a little bit of context so it turns out that this is not the first workshop at NIPS, which is talking about parallel or distributed learning.",
            "There was 1 three years ago on efficient machine learning."
        ],
        [
            "It was 1 two years ago on."
        ],
        [
            "Parallel algorithms."
        ],
        [
            "And then there was one last year on large scale machine learning.",
            "And then there's this year.",
            "And what you notice here is that.",
            "The people organizing these workshops are completely different.",
            "Premier dear and they are all about very similar topics so I guess what we claim is that.",
            "Is it?",
            "This is a topic that is of essential interest to a lot of people right now."
        ],
        [
            "So next year.",
            "There's a book that Ron Beckerman and Misha Blincoe and I are are editing so.",
            "What I want to do is give you.",
            "A high level overview of this book.",
            "OK, so let's start with unsupervised learning methods.",
            "So there is an information theoretic Co clustering with MPI chapter.",
            "And there's a special clustering chapter which uses MapReduce.",
            "And there's a K means on a GPU chapter.",
            "And there's a latent dear Chalet analysis with MPI chapter.",
            "And."
        ],
        [
            "The thing which is.",
            "OK, so.",
            "Trying to create this summary was actually pretty difficult because these are completely different algorithms, different datasets and so forth, but nevertheless.",
            "It's interesting to take a look at."
        ],
        [
            "Results and see what they show you.",
            "So.",
            "The read here is the amount of OK.",
            "So how many measuring results?",
            "So we have some amount of time it takes to run the algorithm.",
            "And we have some number of features which are in the data set that we're going to apply the algorithm to.",
            "So we take the number of features we divided by the time.",
            "It's going to give us features per second and now what you see is this is a log rhythmic scale.",
            "So there's a vast difference in scales over which we're working.",
            "And all of these algorithms are things that people thought of as fast.",
            "Recently OK so.",
            "So the first one was the spectral clustering with map reduce.",
            "On 128 nodes, this is applied to the receive one data set.",
            "MCV one OK Anyways this is the information theoretic Co clustering.",
            "This is MPI on 400 nodes or 400 cores.",
            "This is LDA with MPI on 1024 nodes.",
            "This is K means on 400 processor GPU.",
            "It's only using a synthetic data set however.",
            "And then this is a restricted Boltzmann machine.",
            "Running on GPU.",
            "So.",
            "The green is.",
            "OK, So what is the green?",
            "So oftentimes there's.",
            "An algorithm there so explicitly speeding up which is running on a single CPU.",
            "In that case, it's a green.",
            "Sometimes the data is so large that you can't really run the algorithm on a single machine.",
            "In which case they often ran on on a smaller number of machines than the maximum, and so I just kind of interpreted down to one machine would mean.",
            "OK, So what do we see here?",
            "So we see GPU being very fast GPU.",
            "And then MPI, MPI and then MapReduce.",
            "OK, so."
        ],
        [
            "So now let me tell you a little bit more about the comparison.",
            "So one thing you have to understand is that I'm not comparing accuracy, so it may be that this is the method that you need to use."
        ],
        [
            "To get the kind of clustering that you want.",
            "In which case that's the method that you want to use, right?",
            "These other methods may not provide the right structure for what you want and may not provide the right solutions, and that's going to be.",
            "That's something you should really understand, because there's no way for me to.",
            "I couldn't figure out a way to talk about this.",
            "There's no way to compare things in terms of accuracy because they're not even apply the same datasets."
        ],
        [
            "OK, so that's a huge caveat when I'm looking at these things.",
            "I'm just looking at the input complexity so so typically in datasets or something you can think of as a feature which is within a constant factor of a byte in size.",
            "And I'm looking at the input complexity divided by time."
        ],
        [
            "OK, so this is.",
            "This means there's no credit for creating complexity, then reducing it.",
            "OK, so.",
            "For example, there are some people who scan over images a lot, in which case you know they create a lot of complexities, have scan over the image a lot, but but I'm only counting the size of the image right?",
            "And if that doesn't quite seem fair then then I appreciate that.",
            "In fact it reduced the numbers from my own algorithm using this kind of criteria.",
            "But nevertheless, I think it's actually if we go back to the beginning, we think about what is important.",
            "So efficiency is important.",
            "And then parallel and distribution.",
            "And this kind of metric, I think, encourage you to just think about efficiency, which I think is a good idea, yeah?"
        ],
        [
            "This life is the GPU ignition 'cause the single processor argument would be close to parallel.",
            "So it's important number.",
            "This is a log rhythmic scale.",
            "So the gap between between this and this is ginormous.",
            "Yeah so.",
            "I mean the flip flop difference between the GPU and CPU is 10 to hundreds.",
            "Yeah."
        ],
        [
            "OK, so."
        ],
        [
            "Are there other questions about about this?"
        ],
        [
            "OK, so."
        ],
        [
            "In many cases in these chapters there are different results reported.",
            "We made some effort to try to find the source of the most interesting result.",
            "As best we could.",
            "And of course there's some approximations here, so you should understand that.",
            "Depending on exactly how you.",
            "Decided to choose to report things there could be a variation of a factor of maybe as much as 10.",
            "Things.",
            "OK, so this is not the only chapters in the book, there's also."
        ],
        [
            "Chapters on supervised training.",
            "So when their supervised training is very solemn, people are concerned with.",
            "So one of them is is a.",
            "Radial basis functions support vector machines.",
            "And I'm actually not sure that these guys used MPI because they're at Google, but there's source code right now uses MPI.",
            "So this is on the RC one data set.",
            "And then there's an ensemble tree using MPI.",
            "Where they get some this is.",
            "This is sort of like a boosting tree, but a little bit different.",
            "So they're getting substantial speedups.",
            "And then there's another radial basis function, SVM.",
            "This is from the NEC people.",
            "And they're getting quite a bit more speedup, so they actually they didn't, didn't, didn't use MPI, then use MapReduce, just use TCP.",
            "It's pretty hardcore and they found a little bit small, so they made it bigger.",
            "So that's nice.",
            "And then.",
            "Is a decision tree implementation which is using map reduce?",
            "This is from Google again to predict on add bounce data set.",
            "So you want to predict.",
            "Which advertisements, if somebody clicks on well, they will come back because they're unhappy.",
            "So we're getting a pretty impressive speedup.",
            "And then there's a big decision tree.",
            "This one is from Microsoft, whether running MPI on 32 nodes and their baseline is much higher, but they're getting similar absolute speed, maybe slightly faster.",
            "One thing to notice, though, there's that boosted Decision Tree is about two orders of magnitude more difficult to learn then a decision tree.",
            "Because you boost, right?",
            "And then, and this is linear learning.",
            "This is with the vocal web at which we're going to do a tutorial on it too.",
            "And it's you get very little speedup.",
            "It's from there to there if you can see the difference.",
            "Because we're IO bound, right?",
            "So we're running out about 10 mega features per second in terms of raw input data.",
            "OK, so are there questions about this?",
            "Yeah.",
            "This graph is, but to what extent?",
            "Like in the first column is the difference close to 500 in the second column?",
            "Is it close to 128, etc.",
            "Yeah, so in terms of efficiency, the one which is very surprising is there is this one.",
            "This one, they have a super linear speedup.",
            "Do the caching effects essentially right, so you have a lot more cash when you have 48 nodes so you can go faster.",
            "Everything else the speedups are less than one is a factor of the number of things you're running on.",
            "It's a little bit hard to even think about those things when you're using a GPU.",
            "But when using a GPU becomes much more compelling, I guess to use to think about the energy usage, in which case GPU's are really dominant because they typically have a pretty low frequency in a whole lot of cores, which is which is great if you want to.",
            "To do some sort of computation intense thing."
        ],
        [
            "OK, so there are also several chapters on essentially testing but not training.",
            "So in general, you should imagine that all the training results can also be transferred to testing at least as well, because testing is typically much faster than training, but some people.",
            "Some people just concentrated on parallelizing the testing, which is very compelling in certain applications.",
            "So there's a pretty impressive chapter from speech people at Berkeley talking about how to speed up speech recognition.",
            "So they're using.",
            "Atomic assembly, which I wasn't aware of before reading this chapter, which is.",
            "They find the whole thread thing way too way too annoying, so they use atomic assembly instructions.",
            "And then there's a chapter on inference.",
            "So here, using MPI and 40 nodes, in trying to figure out protein protein interactions.",
            "So this one thing about features is a bit of a stretch, but basically it's the number of.",
            "The number of edges in a graph.",
            "OK, so then there's convolutional networks.",
            "So Jan has looked into speeding into applying them to those GPU's, but also Fpgas, which gave a little bit better results.",
            "And he has a simulation of how fast they would run on a if you burned it into the hardware.",
            "And of course, it's quite fast.",
            "Quite impressively fast.",
            "And then people have also looked at using these decision trees with GPU's on images.",
            "And then they get some very substantial speedups.",
            "So.",
            "Other questions here.",
            "One thing that I notice when looking at these.",
            "Is that?",
            "OK so we have GPU FPGA.",
            "And burn into the hardware.",
            "We have OK, so this is just this is IO bound.",
            "OK so we have MPI and the."
        ],
        [
            "Produce I'd like to compare different methods, GPU, GPU, so it seems like GPU's are relatively dominant as a strategy.",
            "And that's maybe important to keep in mind."
        ],
        [
            "Dominant is an effective strategy.",
            "OK."
        ],
        [
            "So there's other chapters in this in this book which are kind of hard to describe in this way.",
            "There's things on feature selection, and there's a frequent item systems which is much more of a database type setting.",
            "And then there's chapters on sort of new parallel computing frameworks which people are trying to or considering applying to machine learning.",
            "Alright, so.",
            "So that's the summary of the chapter, and now there's the workshop to worry about."
        ],
        [
            "So for the workshop in the morning, John Ceklis is going to speak about averaging algorithms.",
            "And then we're going to have coffee and posters so.",
            "Who has not set up a poster yet?",
            "OK, you should set it up soon.",
            "So during the breaks is when the posters are really available to look at, and I greatly encourage you to look at them.",
            "Because I only have a poster myself.",
            "OK, so then Joe is going to talk about.",
            "Optimal distributed online prediction using mini batches.",
            "And then so that Petrov is going to talk about map reduce for big table MapReduce, big table for distributed optimization, and we're going to have a sequence of Spotlight talks and then will break.",
            "And in the afternoon.",
            "So at 2:00 o'clock there's a unofficial tutorial on."
        ],
        [
            "On on that one.",
            "We have a new version of it out.",
            "And then the real workshop begins at 3:30.",
            "Carlos Guestrin will talk about machine learning with graph lab.",
            "And then we have this talk on distributed map inference and then.",
            "Jerry will talk about gradient boosted decision trees on Hadoop.",
            "And then we'll have a few more mini talks, and then we'll have a summary panel.",
            "So if you're a speaker, it would be very good if you can be around for the summary panel because.",
            "'cause I would like to discuss things.",
            "And then we will finish."
        ],
        [
            "Oh yes, so we're doing video lectures, which means that you should.",
            "You should provide your PDF.",
            "You can email John Ducci directly.",
            "Or and he'll put it up on the web page.",
            "We can also.",
            "We also want to stick it on the USB stick, and then there's forms here to fill out with John Duchi spin.",
            "Good."
        ],
        [
            "Flights.",
            "Sorry what.",
            "So, so all the people who are doing the mini talks, they should also give us insight so that we can put them on a single laptop first.",
            "These are switching.",
            "So if you haven't given us the slides yet, please do so soon.",
            "What's your email address?",
            "You can just come up front and give us something USB.",
            "OK. Let's begin then."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the workshop on learning encores clusters and clouds.",
                    "label": 1
                },
                {
                    "sent": "So Alec John.",
                    "label": 0
                },
                {
                    "sent": "Over and Lawrence are the organizers.",
                    "label": 0
                },
                {
                    "sent": "We had quite a bit of support for the workshop from Google, Microsoft Pascal, and Yahoo.",
                    "label": 0
                },
                {
                    "sent": "Although donated money, which which is great because now we can pay for video lectures.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me let me tell you my cartoon understanding of why we're here so.",
                    "label": 1
                },
                {
                    "sent": "There's a lot more sensors in the world.",
                    "label": 1
                },
                {
                    "sent": "We're recording a lot more information.",
                    "label": 0
                },
                {
                    "sent": "And that means that you have startups that are getting like a billion or 10 billion even events per day now.",
                    "label": 0
                },
                {
                    "sent": "Really common, right?",
                    "label": 0
                },
                {
                    "sent": "And that would be great if the machines are keeping up with the right that we're getting information, but.",
                    "label": 0
                },
                {
                    "sent": "But they're not so the computers I think are getting faster, and this is actually a little bit surprised by this.",
                    "label": 0
                },
                {
                    "sent": "So I have a desktop that I bought.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About four years ago and Matt Hoffman has a laptop that he bought just recently and it's about a factor of 10 faster on some code that we were working on.",
                    "label": 0
                },
                {
                    "sent": "So in fact, even though the Giga Hertz are not going up, the computers are getting faster, but there's just not keeping up right, so the number of events for computation is going up.",
                    "label": 0
                },
                {
                    "sent": "So that's what's really driving us to think about.",
                    "label": 0
                },
                {
                    "sent": "About this workshop.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What are the sort of core issues SO1 core issue which I think is is not going to be really discussed here, but which is incredibly important?",
                    "label": 1
                },
                {
                    "sent": "Is efficiency of your algorithm.",
                    "label": 0
                },
                {
                    "sent": "So there are efficient ways to do things in their inefficient ways to do things and we try to make sure the presentations are about reasonably efficient ways to do things, but.",
                    "label": 0
                },
                {
                    "sent": "But that's probably emphasized to greater extent.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you are pondering doing parallel things.",
                    "label": 0
                },
                {
                    "sent": "And then what the workshop is mostly about is parallelism and distribution.",
                    "label": 0
                },
                {
                    "sent": "So often you end up with more data than any particular machine can handle, and then parallelism becomes essential.",
                    "label": 1
                },
                {
                    "sent": "Often you have maybe the best algorithm just is computationally intense, and then parallelism can be helpful.",
                    "label": 0
                },
                {
                    "sent": "And then when you get to very large amounts of data, the paradigm for how you deal with it is that you don't move the data anymore.",
                    "label": 0
                },
                {
                    "sent": "You keep the data wherever it is and you move the computation to the data.",
                    "label": 0
                },
                {
                    "sent": "And that's kind of a senchal when you think about sort of bandwidth limits and things like that.",
                    "label": 0
                },
                {
                    "sent": "It also.",
                    "label": 0
                },
                {
                    "sent": "By the way, can become essential because of potential privacy reasons.",
                    "label": 0
                },
                {
                    "sent": "OK, so those are sort of the core.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Issues that we're going to be thinking about.",
                    "label": 0
                },
                {
                    "sent": "And let me tell you how this kind of comes up in everyday life.",
                    "label": 0
                },
                {
                    "sent": "So the great example of.",
                    "label": 0
                },
                {
                    "sent": "Of machine learning is spam and in the beginning this wasn't a problem.",
                    "label": 0
                },
                {
                    "sent": "I remember back when there wasn't spam and.",
                    "label": 0
                },
                {
                    "sent": "And you know you do is just an open protocol.",
                    "label": 0
                },
                {
                    "sent": "You could create your own email server and I did.",
                    "label": 0
                },
                {
                    "sent": "But then things changed.",
                    "label": 0
                },
                {
                    "sent": "People started spamming and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And got worse and worse and worse and started making money.",
                    "label": 0
                },
                {
                    "sent": "And it got worse and worse.",
                    "label": 0
                },
                {
                    "sent": "And then then it became kind of impractical to have an individual email server because it was pretty hard to actually filter out the spam quickly enough.",
                    "label": 0
                },
                {
                    "sent": "So so then you started getting.",
                    "label": 0
                },
                {
                    "sent": "Large centralized email providers.",
                    "label": 0
                },
                {
                    "sent": "Partly because the spam is so severe and these large email providers could get enough data that they could actually start to filter the spam it with a reasonably high success rate.",
                    "label": 0
                },
                {
                    "sent": "Also, they could because they were large, they could afford to have a group of people working on spam which is.",
                    "label": 0
                },
                {
                    "sent": "Question very helpful.",
                    "label": 0
                },
                {
                    "sent": "So now, if you're at one of these large centralized email providers, the question becomes, how do you efficiently learn to classify spam in a parallel distributed environment?",
                    "label": 1
                },
                {
                    "sent": "Right, and that's it's a problem of obvious significance to everyone.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So now let me.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Provide a little bit of context so it turns out that this is not the first workshop at NIPS, which is talking about parallel or distributed learning.",
                    "label": 0
                },
                {
                    "sent": "There was 1 three years ago on efficient machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It was 1 two years ago on.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parallel algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there was one last year on large scale machine learning.",
                    "label": 0
                },
                {
                    "sent": "And then there's this year.",
                    "label": 0
                },
                {
                    "sent": "And what you notice here is that.",
                    "label": 0
                },
                {
                    "sent": "The people organizing these workshops are completely different.",
                    "label": 0
                },
                {
                    "sent": "Premier dear and they are all about very similar topics so I guess what we claim is that.",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "This is a topic that is of essential interest to a lot of people right now.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So next year.",
                    "label": 0
                },
                {
                    "sent": "There's a book that Ron Beckerman and Misha Blincoe and I are are editing so.",
                    "label": 0
                },
                {
                    "sent": "What I want to do is give you.",
                    "label": 0
                },
                {
                    "sent": "A high level overview of this book.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's start with unsupervised learning methods.",
                    "label": 0
                },
                {
                    "sent": "So there is an information theoretic Co clustering with MPI chapter.",
                    "label": 0
                },
                {
                    "sent": "And there's a special clustering chapter which uses MapReduce.",
                    "label": 0
                },
                {
                    "sent": "And there's a K means on a GPU chapter.",
                    "label": 0
                },
                {
                    "sent": "And there's a latent dear Chalet analysis with MPI chapter.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The thing which is.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Trying to create this summary was actually pretty difficult because these are completely different algorithms, different datasets and so forth, but nevertheless.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to take a look at.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results and see what they show you.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The read here is the amount of OK.",
                    "label": 0
                },
                {
                    "sent": "So how many measuring results?",
                    "label": 0
                },
                {
                    "sent": "So we have some amount of time it takes to run the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And we have some number of features which are in the data set that we're going to apply the algorithm to.",
                    "label": 0
                },
                {
                    "sent": "So we take the number of features we divided by the time.",
                    "label": 0
                },
                {
                    "sent": "It's going to give us features per second and now what you see is this is a log rhythmic scale.",
                    "label": 0
                },
                {
                    "sent": "So there's a vast difference in scales over which we're working.",
                    "label": 0
                },
                {
                    "sent": "And all of these algorithms are things that people thought of as fast.",
                    "label": 0
                },
                {
                    "sent": "Recently OK so.",
                    "label": 0
                },
                {
                    "sent": "So the first one was the spectral clustering with map reduce.",
                    "label": 0
                },
                {
                    "sent": "On 128 nodes, this is applied to the receive one data set.",
                    "label": 0
                },
                {
                    "sent": "MCV one OK Anyways this is the information theoretic Co clustering.",
                    "label": 0
                },
                {
                    "sent": "This is MPI on 400 nodes or 400 cores.",
                    "label": 0
                },
                {
                    "sent": "This is LDA with MPI on 1024 nodes.",
                    "label": 0
                },
                {
                    "sent": "This is K means on 400 processor GPU.",
                    "label": 0
                },
                {
                    "sent": "It's only using a synthetic data set however.",
                    "label": 0
                },
                {
                    "sent": "And then this is a restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "Running on GPU.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The green is.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is the green?",
                    "label": 0
                },
                {
                    "sent": "So oftentimes there's.",
                    "label": 0
                },
                {
                    "sent": "An algorithm there so explicitly speeding up which is running on a single CPU.",
                    "label": 0
                },
                {
                    "sent": "In that case, it's a green.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the data is so large that you can't really run the algorithm on a single machine.",
                    "label": 0
                },
                {
                    "sent": "In which case they often ran on on a smaller number of machines than the maximum, and so I just kind of interpreted down to one machine would mean.",
                    "label": 0
                },
                {
                    "sent": "OK, So what do we see here?",
                    "label": 0
                },
                {
                    "sent": "So we see GPU being very fast GPU.",
                    "label": 0
                },
                {
                    "sent": "And then MPI, MPI and then MapReduce.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let me tell you a little bit more about the comparison.",
                    "label": 0
                },
                {
                    "sent": "So one thing you have to understand is that I'm not comparing accuracy, so it may be that this is the method that you need to use.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To get the kind of clustering that you want.",
                    "label": 0
                },
                {
                    "sent": "In which case that's the method that you want to use, right?",
                    "label": 0
                },
                {
                    "sent": "These other methods may not provide the right structure for what you want and may not provide the right solutions, and that's going to be.",
                    "label": 0
                },
                {
                    "sent": "That's something you should really understand, because there's no way for me to.",
                    "label": 0
                },
                {
                    "sent": "I couldn't figure out a way to talk about this.",
                    "label": 0
                },
                {
                    "sent": "There's no way to compare things in terms of accuracy because they're not even apply the same datasets.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's a huge caveat when I'm looking at these things.",
                    "label": 0
                },
                {
                    "sent": "I'm just looking at the input complexity so so typically in datasets or something you can think of as a feature which is within a constant factor of a byte in size.",
                    "label": 0
                },
                {
                    "sent": "And I'm looking at the input complexity divided by time.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "This means there's no credit for creating complexity, then reducing it.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "For example, there are some people who scan over images a lot, in which case you know they create a lot of complexities, have scan over the image a lot, but but I'm only counting the size of the image right?",
                    "label": 0
                },
                {
                    "sent": "And if that doesn't quite seem fair then then I appreciate that.",
                    "label": 0
                },
                {
                    "sent": "In fact it reduced the numbers from my own algorithm using this kind of criteria.",
                    "label": 0
                },
                {
                    "sent": "But nevertheless, I think it's actually if we go back to the beginning, we think about what is important.",
                    "label": 0
                },
                {
                    "sent": "So efficiency is important.",
                    "label": 0
                },
                {
                    "sent": "And then parallel and distribution.",
                    "label": 0
                },
                {
                    "sent": "And this kind of metric, I think, encourage you to just think about efficiency, which I think is a good idea, yeah?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This life is the GPU ignition 'cause the single processor argument would be close to parallel.",
                    "label": 0
                },
                {
                    "sent": "So it's important number.",
                    "label": 0
                },
                {
                    "sent": "This is a log rhythmic scale.",
                    "label": 0
                },
                {
                    "sent": "So the gap between between this and this is ginormous.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "I mean the flip flop difference between the GPU and CPU is 10 to hundreds.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are there other questions about about this?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In many cases in these chapters there are different results reported.",
                    "label": 0
                },
                {
                    "sent": "We made some effort to try to find the source of the most interesting result.",
                    "label": 0
                },
                {
                    "sent": "As best we could.",
                    "label": 0
                },
                {
                    "sent": "And of course there's some approximations here, so you should understand that.",
                    "label": 0
                },
                {
                    "sent": "Depending on exactly how you.",
                    "label": 1
                },
                {
                    "sent": "Decided to choose to report things there could be a variation of a factor of maybe as much as 10.",
                    "label": 0
                },
                {
                    "sent": "Things.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is not the only chapters in the book, there's also.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Chapters on supervised training.",
                    "label": 0
                },
                {
                    "sent": "So when their supervised training is very solemn, people are concerned with.",
                    "label": 1
                },
                {
                    "sent": "So one of them is is a.",
                    "label": 0
                },
                {
                    "sent": "Radial basis functions support vector machines.",
                    "label": 0
                },
                {
                    "sent": "And I'm actually not sure that these guys used MPI because they're at Google, but there's source code right now uses MPI.",
                    "label": 0
                },
                {
                    "sent": "So this is on the RC one data set.",
                    "label": 0
                },
                {
                    "sent": "And then there's an ensemble tree using MPI.",
                    "label": 1
                },
                {
                    "sent": "Where they get some this is.",
                    "label": 0
                },
                {
                    "sent": "This is sort of like a boosting tree, but a little bit different.",
                    "label": 0
                },
                {
                    "sent": "So they're getting substantial speedups.",
                    "label": 0
                },
                {
                    "sent": "And then there's another radial basis function, SVM.",
                    "label": 0
                },
                {
                    "sent": "This is from the NEC people.",
                    "label": 0
                },
                {
                    "sent": "And they're getting quite a bit more speedup, so they actually they didn't, didn't, didn't use MPI, then use MapReduce, just use TCP.",
                    "label": 0
                },
                {
                    "sent": "It's pretty hardcore and they found a little bit small, so they made it bigger.",
                    "label": 0
                },
                {
                    "sent": "So that's nice.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Is a decision tree implementation which is using map reduce?",
                    "label": 0
                },
                {
                    "sent": "This is from Google again to predict on add bounce data set.",
                    "label": 0
                },
                {
                    "sent": "So you want to predict.",
                    "label": 0
                },
                {
                    "sent": "Which advertisements, if somebody clicks on well, they will come back because they're unhappy.",
                    "label": 0
                },
                {
                    "sent": "So we're getting a pretty impressive speedup.",
                    "label": 0
                },
                {
                    "sent": "And then there's a big decision tree.",
                    "label": 1
                },
                {
                    "sent": "This one is from Microsoft, whether running MPI on 32 nodes and their baseline is much higher, but they're getting similar absolute speed, maybe slightly faster.",
                    "label": 0
                },
                {
                    "sent": "One thing to notice, though, there's that boosted Decision Tree is about two orders of magnitude more difficult to learn then a decision tree.",
                    "label": 0
                },
                {
                    "sent": "Because you boost, right?",
                    "label": 0
                },
                {
                    "sent": "And then, and this is linear learning.",
                    "label": 0
                },
                {
                    "sent": "This is with the vocal web at which we're going to do a tutorial on it too.",
                    "label": 0
                },
                {
                    "sent": "And it's you get very little speedup.",
                    "label": 0
                },
                {
                    "sent": "It's from there to there if you can see the difference.",
                    "label": 0
                },
                {
                    "sent": "Because we're IO bound, right?",
                    "label": 0
                },
                {
                    "sent": "So we're running out about 10 mega features per second in terms of raw input data.",
                    "label": 0
                },
                {
                    "sent": "OK, so are there questions about this?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "This graph is, but to what extent?",
                    "label": 0
                },
                {
                    "sent": "Like in the first column is the difference close to 500 in the second column?",
                    "label": 0
                },
                {
                    "sent": "Is it close to 128, etc.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in terms of efficiency, the one which is very surprising is there is this one.",
                    "label": 0
                },
                {
                    "sent": "This one, they have a super linear speedup.",
                    "label": 0
                },
                {
                    "sent": "Do the caching effects essentially right, so you have a lot more cash when you have 48 nodes so you can go faster.",
                    "label": 0
                },
                {
                    "sent": "Everything else the speedups are less than one is a factor of the number of things you're running on.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit hard to even think about those things when you're using a GPU.",
                    "label": 0
                },
                {
                    "sent": "But when using a GPU becomes much more compelling, I guess to use to think about the energy usage, in which case GPU's are really dominant because they typically have a pretty low frequency in a whole lot of cores, which is which is great if you want to.",
                    "label": 0
                },
                {
                    "sent": "To do some sort of computation intense thing.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there are also several chapters on essentially testing but not training.",
                    "label": 1
                },
                {
                    "sent": "So in general, you should imagine that all the training results can also be transferred to testing at least as well, because testing is typically much faster than training, but some people.",
                    "label": 0
                },
                {
                    "sent": "Some people just concentrated on parallelizing the testing, which is very compelling in certain applications.",
                    "label": 0
                },
                {
                    "sent": "So there's a pretty impressive chapter from speech people at Berkeley talking about how to speed up speech recognition.",
                    "label": 0
                },
                {
                    "sent": "So they're using.",
                    "label": 0
                },
                {
                    "sent": "Atomic assembly, which I wasn't aware of before reading this chapter, which is.",
                    "label": 0
                },
                {
                    "sent": "They find the whole thread thing way too way too annoying, so they use atomic assembly instructions.",
                    "label": 0
                },
                {
                    "sent": "And then there's a chapter on inference.",
                    "label": 0
                },
                {
                    "sent": "So here, using MPI and 40 nodes, in trying to figure out protein protein interactions.",
                    "label": 0
                },
                {
                    "sent": "So this one thing about features is a bit of a stretch, but basically it's the number of.",
                    "label": 0
                },
                {
                    "sent": "The number of edges in a graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so then there's convolutional networks.",
                    "label": 0
                },
                {
                    "sent": "So Jan has looked into speeding into applying them to those GPU's, but also Fpgas, which gave a little bit better results.",
                    "label": 0
                },
                {
                    "sent": "And he has a simulation of how fast they would run on a if you burned it into the hardware.",
                    "label": 0
                },
                {
                    "sent": "And of course, it's quite fast.",
                    "label": 0
                },
                {
                    "sent": "Quite impressively fast.",
                    "label": 0
                },
                {
                    "sent": "And then people have also looked at using these decision trees with GPU's on images.",
                    "label": 0
                },
                {
                    "sent": "And then they get some very substantial speedups.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Other questions here.",
                    "label": 0
                },
                {
                    "sent": "One thing that I notice when looking at these.",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "OK so we have GPU FPGA.",
                    "label": 0
                },
                {
                    "sent": "And burn into the hardware.",
                    "label": 0
                },
                {
                    "sent": "We have OK, so this is just this is IO bound.",
                    "label": 0
                },
                {
                    "sent": "OK so we have MPI and the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Produce I'd like to compare different methods, GPU, GPU, so it seems like GPU's are relatively dominant as a strategy.",
                    "label": 0
                },
                {
                    "sent": "And that's maybe important to keep in mind.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dominant is an effective strategy.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's other chapters in this in this book which are kind of hard to describe in this way.",
                    "label": 0
                },
                {
                    "sent": "There's things on feature selection, and there's a frequent item systems which is much more of a database type setting.",
                    "label": 1
                },
                {
                    "sent": "And then there's chapters on sort of new parallel computing frameworks which people are trying to or considering applying to machine learning.",
                    "label": 1
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "So that's the summary of the chapter, and now there's the workshop to worry about.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the workshop in the morning, John Ceklis is going to speak about averaging algorithms.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to have coffee and posters so.",
                    "label": 0
                },
                {
                    "sent": "Who has not set up a poster yet?",
                    "label": 0
                },
                {
                    "sent": "OK, you should set it up soon.",
                    "label": 0
                },
                {
                    "sent": "So during the breaks is when the posters are really available to look at, and I greatly encourage you to look at them.",
                    "label": 0
                },
                {
                    "sent": "Because I only have a poster myself.",
                    "label": 0
                },
                {
                    "sent": "OK, so then Joe is going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Optimal distributed online prediction using mini batches.",
                    "label": 1
                },
                {
                    "sent": "And then so that Petrov is going to talk about map reduce for big table MapReduce, big table for distributed optimization, and we're going to have a sequence of Spotlight talks and then will break.",
                    "label": 0
                },
                {
                    "sent": "And in the afternoon.",
                    "label": 0
                },
                {
                    "sent": "So at 2:00 o'clock there's a unofficial tutorial on.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On on that one.",
                    "label": 0
                },
                {
                    "sent": "We have a new version of it out.",
                    "label": 0
                },
                {
                    "sent": "And then the real workshop begins at 3:30.",
                    "label": 0
                },
                {
                    "sent": "Carlos Guestrin will talk about machine learning with graph lab.",
                    "label": 0
                },
                {
                    "sent": "And then we have this talk on distributed map inference and then.",
                    "label": 0
                },
                {
                    "sent": "Jerry will talk about gradient boosted decision trees on Hadoop.",
                    "label": 1
                },
                {
                    "sent": "And then we'll have a few more mini talks, and then we'll have a summary panel.",
                    "label": 0
                },
                {
                    "sent": "So if you're a speaker, it would be very good if you can be around for the summary panel because.",
                    "label": 0
                },
                {
                    "sent": "'cause I would like to discuss things.",
                    "label": 0
                },
                {
                    "sent": "And then we will finish.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh yes, so we're doing video lectures, which means that you should.",
                    "label": 0
                },
                {
                    "sent": "You should provide your PDF.",
                    "label": 0
                },
                {
                    "sent": "You can email John Ducci directly.",
                    "label": 0
                },
                {
                    "sent": "Or and he'll put it up on the web page.",
                    "label": 0
                },
                {
                    "sent": "We can also.",
                    "label": 0
                },
                {
                    "sent": "We also want to stick it on the USB stick, and then there's forms here to fill out with John Duchi spin.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Flights.",
                    "label": 0
                },
                {
                    "sent": "Sorry what.",
                    "label": 0
                },
                {
                    "sent": "So, so all the people who are doing the mini talks, they should also give us insight so that we can put them on a single laptop first.",
                    "label": 0
                },
                {
                    "sent": "These are switching.",
                    "label": 0
                },
                {
                    "sent": "So if you haven't given us the slides yet, please do so soon.",
                    "label": 0
                },
                {
                    "sent": "What's your email address?",
                    "label": 0
                },
                {
                    "sent": "You can just come up front and give us something USB.",
                    "label": 0
                },
                {
                    "sent": "OK. Let's begin then.",
                    "label": 0
                }
            ]
        }
    }
}