{
    "id": "lm2yltzoboenz7ukglzodq63g3a7mhnm",
    "title": "Deep Learning from Temporal Coherence in Video",
    "info": {
        "author": [
            "Ronan Collobert, NEC Laboratories America, Inc."
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_collobert_dlt/",
    "segmentation": [
        [
            "So I'm going to talk about today about deep learning technique using top or lurker erence in video and this is joint work with Jose in mobile.",
            "He and Jason Weston at the initial."
        ],
        [
            "America.",
            "So the goal here is pretty simple.",
            "We are interested in the object recognition and well actually object classification using images.",
            "And for that we are going to use a deep architecture.",
            "In other words, in our network.",
            "And there is a very good architecture which works for for object recognition actually which is called a convolutional neural network.",
            "That's what we are going to use here.",
            "And it looks like that it's basically something which takes an input image with no particulare features.",
            "An apply successfully.",
            "Several convolutions or subsampling's, followed by some nonlinearity to finally output score per class of the problem you have you tried that with like a stochastic gradient descent?",
            "There is no particular trick and that's it.",
            "The only problem to my opinion, is that there are a lot of power metals.",
            "And of course if you have a lot of parameters we need a lot of examples to train them properly.",
            "So in this talk we are interested in finding ways to like leverage, unlabeled data and in particular structure labeled data.",
            "To try to train this, not this kind of networks would be better."
        ],
        [
            "So to deal with the unlabeled data, we are actually interested in.",
            "Quite funny embedding algorithm which has been used in many cases actually.",
            "So suppose that you have.",
            "Suppose you have some.",
            "Unlabeled data well, you don't really.",
            "I mean, it's unlabeled, so you don't have any labels, but you have like some kind of measure of like if 2 examples are similar or not.",
            "So the measure can be whatever you want you choose before hand and you are going.",
            "You are going to take like 2 two examples which are similar according to your measure.",
            "Optionally you can map them according to some feature.",
            "I mean feature functions that you are going to train into some.",
            "Embedding space if you like and then in this like feature space you are going to apply.",
            "I mean compute the distance.",
            "I mean whatever distance you want again, and try to pull together your two examples which are similar according to your distance.",
            "Then for what we call a negative pair for like 2 examples which are dissimilar according to your distance, you are going to try to push them away.",
            "Still according to your distance.",
            "So if you take this algorithm like.",
            "Out of the box like that, it's basically something which is called Doctor Lim which is coming out from Young's lab.",
            "And it works pretty well.",
            "It gives you very nice embeddings.",
            "He's are in this feature space here or in the original space depending if you are training those or not.",
            "And you can relate it to Siamese networks like Russian anger map.",
            "I mean whatever kind of embedding algorithm if you like.",
            "In our case, we actually applied it in many, many problems."
        ],
        [
            "So we had like structured data, for example in text.",
            "So in this case positive player.",
            "So apparently I like to example match.",
            "Would be like the beginning of a sentence like the cat sat on the and then the other part of the pair would be 1 one which which is correct.",
            "In this sentence, the cat on the mat and a negative pair would be like something where the the beginning of the sentence is the same, but the end of the sentence basically doesn't match.",
            "So some random world.",
            "So here we use the ranking loss.",
            "We train dissembling algorithm.",
            "We obtain very nice and buildings for for each word.",
            "In our dictionary we upgraded to also like on a rich River problem where they are the structure is like trying to match queries on document.",
            "Again, using a ranking list we applied it also in some supervised learning setup.",
            "So usually in supervised learning you have some kind of structure because you assume in general the cluster assumption.",
            "So basically you would like 2 examples which are neighbors to have the same label.",
            "So you can again apply this kind of embedding algorithm and it works pretty well.",
            "So if you want to know details about that.",
            "You can see like Jason's Western stock tomorrow in the learning feature Yankee Workshop, but here today I'm going to show you another application of this algorithm.",
            "But for object recognition and objective cognition, a good source of flag, unlabeled, structured data.",
            "It's of course video.",
            "So that's what I want to talk about now."
        ],
        [
            "If it works, but it doesn't.",
            "Yeah it works.",
            "So sure it looks like an example of a video.",
            "The left one on the right one exactly the same, but the right one as it's like it's friends shuffle.",
            "So obviously the temporal coherence that is present in the left one helps us to understand what's going on so.",
            "And obviously as well, I mean two consecutive frames in like the original one are likely to contain the same objects, maybe in different.",
            "Like you know, lighting conditions with different pools with different deformations with different background.",
            "But we assume that two consecutive frames have a good chance to contain the same object.",
            "So we want to use this top one coherence to try to induce a good measure in some space which would be invariant to all this condition."
        ],
        [
            "Thanks.",
            "So for that we just like take a mix between operational network, an unbending algorithm.",
            "So we take our convolution network an in some quite abstract representation of like one of the networks, one layer which is far enough from the input layer.",
            "We are going to force the representation of two consecutive frames of video.",
            "To be similar while for two random frames of video would like them to be dissimilar.",
            "So."
        ],
        [
            "The algorithm in the end is very simple, you still, I mean to train your network.",
            "You still have to train.",
            "I mean to minimize like the cost associated with the label examples, while at the same time so you minimize the cost associated to this occurrence that I just presented.",
            "And you can train train it with stochastic gradient descent, which in the end like corresponds to like picking one random examples in the label.",
            "Do a gradient step, then the consecutive images in the video.",
            "Do a gradient step to to decrease the coherence and so on.",
            "So it's."
        ],
        [
            "This is important.",
            "But you, you might wonder why we didn't try, you know, like classical submits provides learning algorithm, so actually they are very well known as once like transaction.",
            "So graph based learning.",
            "But in both cases we have like some problems, I mean some actually annoying constraints like the transcription usually assumes that unlabeled examples must come from the same distribution than the label ones.",
            "Also, as I said before, we need to consider the cluster assumptions.",
            "Which might not between our case.",
            "And finally, if you use something like a kernel.",
            "The metric associated to it, like for example, if it's so obvious kernel, you might be actually using the occlusion distance, so this occasion distance may be completely wrong for the for the image recognition task.",
            "And it's about the same story in story and for based learning.",
            "Semi supervised algorithm.",
            "Well, it's even slower because usually you have like some you know neighbors computer before hand, so still using like symmetrically OK, distance, which might be wrong.",
            "Again for a task like imager or condition.",
            "So I mean same story."
        ],
        [
            "So I would like to insist on the fact that the Euclidean distance is really a bad metric file for an image processing task.",
            "I mean, at least an object recognition task.",
            "So in these two images I mean it's exactly the same object, right?",
            "But on there like different lighting conditions, and if you just apply a stupid clean and distance on that, well, you are going to.",
            "I mean sorry."
        ],
        [
            "So it's it's basically the same.",
            "I mean if you have like you're different, you know.",
            "It was on."
        ],
        [
            "Backgrounds or like some bad occlusion."
        ],
        [
            "In in all cases, you might have a problem, so I want to insist on the fact that in our case we really induce in the layer.",
            "I mean in the representation layer or hidden.",
            "For our phone network, we really induce a good metric for image recognition becausw using top or coherence.",
            "We enforce that the distance in this representation layer.",
            "Is closed for like 2 friends which are consecutive and of course which represent the same object but in different conditions.",
            "Lighting conditions pose background effects or whatever."
        ],
        [
            "And also about the console assumption.",
            "We don't need to assume assume the Christmas luncheon in the in the original space.",
            "I mean something like here.",
            "Where we would like separate these two label examples and we would have all this green label examples.",
            "So this setup would not work in a classical prospective setup because there is no cluster assumption valid here, but in the representation space induced by your metric.",
            "Bioparque oils metric.",
            "We can hope that this the embedding algorithm which is pushing and pulling things apart we are going to have this cluster assumption."
        ],
        [
            "Valid.",
            "So Indian I mean algorithm has a bunch of advantages compared to the other ones.",
            "We do not need to assume the cursor assumption in the original space.",
            "We have a very natural metric induced with the top of our clients, so four pairs of examples.",
            "There is no cost to collect pairs because you can just take whatever videos you have on the web and you have a quite weak assumptions.",
            "On the."
        ],
        [
            "Open label distribution.",
            "So there have been some work as well done on top or occurrence before, and I'm not going to talk more.",
            "Talk much about it.",
            "I'm just going to cite like three of them.",
            "SO11, which is really close to walk, is like a slow feature analysis which is about to learn transformation ships which are invariant with time under some constraints.",
            "To enforce, like I mean for not obtaining like trivial solution like 0 viable so.",
            "This is quite similar to what we do, but these constraints are actually very tough to enforce in online setup, and in fact it's very tough to scale this kind of method, so same for like Baker 99.",
            "They proposed like a very complex network handcrafted on everything, while our method is very simple, an also like the IMAX method, which again from the same as which.",
            "Is unfortunately very slow according to the also and which is like trying to maximize them between formations according to the currency.",
            "So really again, in contrast, our method is really simple.",
            "It's highly scalable, and we actually observed like improve generalization whenever we try."
        ],
        [
            "Give so we did some experiments using like some quite classical benchmarker data for object recognition.",
            "So we use like coil 100 and and we followed the setup of like Versing 2003.",
            "Which to my knowledge is like the state of the audiences database.",
            "So they are using a very complex network which has like a future dektec cells which are biologically inspired.",
            "They say so."
        ],
        [
            "So well, I mean we want is like.",
            "It's like a bunch of images which look like that there there are like 100 objects each objects as like 70 two different poses which have been obtained by rotating the object on a turntable.",
            "And in those setups they are using like 4 views of this object to train and like the rest basically to test and they have looked set with sort here."
        ],
        [
            "100 objects to train on test.",
            "So as a video we like collected some object.",
            "We are like in offices and we like make them term like comment on a turntable and so this objective objects were actually quite related to the object we can find in coil.",
            "So from these videos we extracted like the 772 views as they had and we just input that to our network's."
        ],
        [
            "Video streaming.",
            "We also like made some little video which have nothing to do with with oil objects like we took some animal, you know objects and made them term as well and."
        ],
        [
            "Made a video out of it.",
            "An we compare that's in the in the two setups that we had with like another bunch of algorithm which are quite classical.",
            "And so the baseline is basically like this Stoner convolutional neural network, which is quite far from the state of the art.",
            "Then you have like this video convolutional net, which is actually.",
            "Kind of a transitive set up because we took all the object we had.",
            "So all the images of the object we like, concatenate them as a big video.",
            "Basically on fed that as a video too.",
            "To the to the training algorithm.",
            "So in that respect the frames, even if they were not labeled where seen during the training.",
            "And I mean the frame for testing were seen during the training.",
            "So it's really attractive set up.",
            "So we also did like a kind of summit supervise set up in the case of like 30 objects.",
            "We took all the other 70 objects.",
            "Again, like Mount all the images and use that as a videos and once again we obtain very good performance compared to all the other benchmarks.",
            "Then we have like De Video coming from similar objects which is I would say more interesting because here we have like different lighting conditions that what they had basically.",
            "And even more interesting, if we take something which has nothing to do with oil is still works pretty well and is compatible to the state of the art."
        ],
        [
            "We also did like some little experiment on the AT&T or FACE data set, which is the data set where there is like 40 subjects, 10 images per subject, where they like change, pose lighting.",
            "Both lightings, I mean they are smiling."
        ],
        [
            "But and all experiments was trying to train.",
            "Using like different number of label examples.",
            "And using the rest of the images as basically a big video game, only bowl of course.",
            "Fed to the network, so using this transitive set up, it's against transductive we obtain very good results on all the datasets.",
            "So Even so, it's like small experiments.",
            "I think we showed him that Tom Parker Erence can be really useful for improving the training of such."
        ],
        [
            "Networks.",
            "So in the end, to summarize, I showed you like quite general embedding algorithm which can be used to leverage any structural data it has been applied actually on text in the past.",
            "Document retrieval Service supervised learning.",
            "We applied it on on object recognition using like video.",
            "So we showed like we can use video clearance to induce a good basically representation of images and try to improve like that.",
            "So performance of the network, it outperforms baselines without any engineering features.",
            "And on the plus side, there is also like a very weak assumptions compared to classical algorithm like some.",
            "It's for learning and you can take you know huge collection of data from the web like videos without any human annotations on hopes that it will improve your object recognition problem."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about today about deep learning technique using top or lurker erence in video and this is joint work with Jose in mobile.",
                    "label": 0
                },
                {
                    "sent": "He and Jason Weston at the initial.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "America.",
                    "label": 0
                },
                {
                    "sent": "So the goal here is pretty simple.",
                    "label": 1
                },
                {
                    "sent": "We are interested in the object recognition and well actually object classification using images.",
                    "label": 1
                },
                {
                    "sent": "And for that we are going to use a deep architecture.",
                    "label": 0
                },
                {
                    "sent": "In other words, in our network.",
                    "label": 0
                },
                {
                    "sent": "And there is a very good architecture which works for for object recognition actually which is called a convolutional neural network.",
                    "label": 1
                },
                {
                    "sent": "That's what we are going to use here.",
                    "label": 0
                },
                {
                    "sent": "And it looks like that it's basically something which takes an input image with no particulare features.",
                    "label": 0
                },
                {
                    "sent": "An apply successfully.",
                    "label": 0
                },
                {
                    "sent": "Several convolutions or subsampling's, followed by some nonlinearity to finally output score per class of the problem you have you tried that with like a stochastic gradient descent?",
                    "label": 0
                },
                {
                    "sent": "There is no particular trick and that's it.",
                    "label": 0
                },
                {
                    "sent": "The only problem to my opinion, is that there are a lot of power metals.",
                    "label": 1
                },
                {
                    "sent": "And of course if you have a lot of parameters we need a lot of examples to train them properly.",
                    "label": 0
                },
                {
                    "sent": "So in this talk we are interested in finding ways to like leverage, unlabeled data and in particular structure labeled data.",
                    "label": 0
                },
                {
                    "sent": "To try to train this, not this kind of networks would be better.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to deal with the unlabeled data, we are actually interested in.",
                    "label": 0
                },
                {
                    "sent": "Quite funny embedding algorithm which has been used in many cases actually.",
                    "label": 0
                },
                {
                    "sent": "So suppose that you have.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have some.",
                    "label": 0
                },
                {
                    "sent": "Unlabeled data well, you don't really.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's unlabeled, so you don't have any labels, but you have like some kind of measure of like if 2 examples are similar or not.",
                    "label": 0
                },
                {
                    "sent": "So the measure can be whatever you want you choose before hand and you are going.",
                    "label": 0
                },
                {
                    "sent": "You are going to take like 2 two examples which are similar according to your measure.",
                    "label": 0
                },
                {
                    "sent": "Optionally you can map them according to some feature.",
                    "label": 0
                },
                {
                    "sent": "I mean feature functions that you are going to train into some.",
                    "label": 0
                },
                {
                    "sent": "Embedding space if you like and then in this like feature space you are going to apply.",
                    "label": 0
                },
                {
                    "sent": "I mean compute the distance.",
                    "label": 0
                },
                {
                    "sent": "I mean whatever distance you want again, and try to pull together your two examples which are similar according to your distance.",
                    "label": 0
                },
                {
                    "sent": "Then for what we call a negative pair for like 2 examples which are dissimilar according to your distance, you are going to try to push them away.",
                    "label": 0
                },
                {
                    "sent": "Still according to your distance.",
                    "label": 0
                },
                {
                    "sent": "So if you take this algorithm like.",
                    "label": 0
                },
                {
                    "sent": "Out of the box like that, it's basically something which is called Doctor Lim which is coming out from Young's lab.",
                    "label": 0
                },
                {
                    "sent": "And it works pretty well.",
                    "label": 0
                },
                {
                    "sent": "It gives you very nice embeddings.",
                    "label": 0
                },
                {
                    "sent": "He's are in this feature space here or in the original space depending if you are training those or not.",
                    "label": 0
                },
                {
                    "sent": "And you can relate it to Siamese networks like Russian anger map.",
                    "label": 1
                },
                {
                    "sent": "I mean whatever kind of embedding algorithm if you like.",
                    "label": 0
                },
                {
                    "sent": "In our case, we actually applied it in many, many problems.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we had like structured data, for example in text.",
                    "label": 0
                },
                {
                    "sent": "So in this case positive player.",
                    "label": 0
                },
                {
                    "sent": "So apparently I like to example match.",
                    "label": 0
                },
                {
                    "sent": "Would be like the beginning of a sentence like the cat sat on the and then the other part of the pair would be 1 one which which is correct.",
                    "label": 1
                },
                {
                    "sent": "In this sentence, the cat on the mat and a negative pair would be like something where the the beginning of the sentence is the same, but the end of the sentence basically doesn't match.",
                    "label": 1
                },
                {
                    "sent": "So some random world.",
                    "label": 0
                },
                {
                    "sent": "So here we use the ranking loss.",
                    "label": 0
                },
                {
                    "sent": "We train dissembling algorithm.",
                    "label": 0
                },
                {
                    "sent": "We obtain very nice and buildings for for each word.",
                    "label": 0
                },
                {
                    "sent": "In our dictionary we upgraded to also like on a rich River problem where they are the structure is like trying to match queries on document.",
                    "label": 0
                },
                {
                    "sent": "Again, using a ranking list we applied it also in some supervised learning setup.",
                    "label": 0
                },
                {
                    "sent": "So usually in supervised learning you have some kind of structure because you assume in general the cluster assumption.",
                    "label": 0
                },
                {
                    "sent": "So basically you would like 2 examples which are neighbors to have the same label.",
                    "label": 0
                },
                {
                    "sent": "So you can again apply this kind of embedding algorithm and it works pretty well.",
                    "label": 1
                },
                {
                    "sent": "So if you want to know details about that.",
                    "label": 0
                },
                {
                    "sent": "You can see like Jason's Western stock tomorrow in the learning feature Yankee Workshop, but here today I'm going to show you another application of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "But for object recognition and objective cognition, a good source of flag, unlabeled, structured data.",
                    "label": 0
                },
                {
                    "sent": "It's of course video.",
                    "label": 0
                },
                {
                    "sent": "So that's what I want to talk about now.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If it works, but it doesn't.",
                    "label": 0
                },
                {
                    "sent": "Yeah it works.",
                    "label": 0
                },
                {
                    "sent": "So sure it looks like an example of a video.",
                    "label": 0
                },
                {
                    "sent": "The left one on the right one exactly the same, but the right one as it's like it's friends shuffle.",
                    "label": 0
                },
                {
                    "sent": "So obviously the temporal coherence that is present in the left one helps us to understand what's going on so.",
                    "label": 0
                },
                {
                    "sent": "And obviously as well, I mean two consecutive frames in like the original one are likely to contain the same objects, maybe in different.",
                    "label": 1
                },
                {
                    "sent": "Like you know, lighting conditions with different pools with different deformations with different background.",
                    "label": 0
                },
                {
                    "sent": "But we assume that two consecutive frames have a good chance to contain the same object.",
                    "label": 0
                },
                {
                    "sent": "So we want to use this top one coherence to try to induce a good measure in some space which would be invariant to all this condition.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "So for that we just like take a mix between operational network, an unbending algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we take our convolution network an in some quite abstract representation of like one of the networks, one layer which is far enough from the input layer.",
                    "label": 0
                },
                {
                    "sent": "We are going to force the representation of two consecutive frames of video.",
                    "label": 1
                },
                {
                    "sent": "To be similar while for two random frames of video would like them to be dissimilar.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The algorithm in the end is very simple, you still, I mean to train your network.",
                    "label": 0
                },
                {
                    "sent": "You still have to train.",
                    "label": 0
                },
                {
                    "sent": "I mean to minimize like the cost associated with the label examples, while at the same time so you minimize the cost associated to this occurrence that I just presented.",
                    "label": 0
                },
                {
                    "sent": "And you can train train it with stochastic gradient descent, which in the end like corresponds to like picking one random examples in the label.",
                    "label": 1
                },
                {
                    "sent": "Do a gradient step, then the consecutive images in the video.",
                    "label": 1
                },
                {
                    "sent": "Do a gradient step to to decrease the coherence and so on.",
                    "label": 0
                },
                {
                    "sent": "So it's.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is important.",
                    "label": 0
                },
                {
                    "sent": "But you, you might wonder why we didn't try, you know, like classical submits provides learning algorithm, so actually they are very well known as once like transaction.",
                    "label": 0
                },
                {
                    "sent": "So graph based learning.",
                    "label": 0
                },
                {
                    "sent": "But in both cases we have like some problems, I mean some actually annoying constraints like the transcription usually assumes that unlabeled examples must come from the same distribution than the label ones.",
                    "label": 0
                },
                {
                    "sent": "Also, as I said before, we need to consider the cluster assumptions.",
                    "label": 0
                },
                {
                    "sent": "Which might not between our case.",
                    "label": 0
                },
                {
                    "sent": "And finally, if you use something like a kernel.",
                    "label": 0
                },
                {
                    "sent": "The metric associated to it, like for example, if it's so obvious kernel, you might be actually using the occlusion distance, so this occasion distance may be completely wrong for the for the image recognition task.",
                    "label": 0
                },
                {
                    "sent": "And it's about the same story in story and for based learning.",
                    "label": 0
                },
                {
                    "sent": "Semi supervised algorithm.",
                    "label": 0
                },
                {
                    "sent": "Well, it's even slower because usually you have like some you know neighbors computer before hand, so still using like symmetrically OK, distance, which might be wrong.",
                    "label": 0
                },
                {
                    "sent": "Again for a task like imager or condition.",
                    "label": 0
                },
                {
                    "sent": "So I mean same story.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I would like to insist on the fact that the Euclidean distance is really a bad metric file for an image processing task.",
                    "label": 1
                },
                {
                    "sent": "I mean, at least an object recognition task.",
                    "label": 0
                },
                {
                    "sent": "So in these two images I mean it's exactly the same object, right?",
                    "label": 0
                },
                {
                    "sent": "But on there like different lighting conditions, and if you just apply a stupid clean and distance on that, well, you are going to.",
                    "label": 0
                },
                {
                    "sent": "I mean sorry.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's it's basically the same.",
                    "label": 0
                },
                {
                    "sent": "I mean if you have like you're different, you know.",
                    "label": 0
                },
                {
                    "sent": "It was on.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Backgrounds or like some bad occlusion.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In in all cases, you might have a problem, so I want to insist on the fact that in our case we really induce in the layer.",
                    "label": 0
                },
                {
                    "sent": "I mean in the representation layer or hidden.",
                    "label": 1
                },
                {
                    "sent": "For our phone network, we really induce a good metric for image recognition becausw using top or coherence.",
                    "label": 0
                },
                {
                    "sent": "We enforce that the distance in this representation layer.",
                    "label": 0
                },
                {
                    "sent": "Is closed for like 2 friends which are consecutive and of course which represent the same object but in different conditions.",
                    "label": 0
                },
                {
                    "sent": "Lighting conditions pose background effects or whatever.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also about the console assumption.",
                    "label": 0
                },
                {
                    "sent": "We don't need to assume assume the Christmas luncheon in the in the original space.",
                    "label": 1
                },
                {
                    "sent": "I mean something like here.",
                    "label": 0
                },
                {
                    "sent": "Where we would like separate these two label examples and we would have all this green label examples.",
                    "label": 1
                },
                {
                    "sent": "So this setup would not work in a classical prospective setup because there is no cluster assumption valid here, but in the representation space induced by your metric.",
                    "label": 0
                },
                {
                    "sent": "Bioparque oils metric.",
                    "label": 0
                },
                {
                    "sent": "We can hope that this the embedding algorithm which is pushing and pulling things apart we are going to have this cluster assumption.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Valid.",
                    "label": 0
                },
                {
                    "sent": "So Indian I mean algorithm has a bunch of advantages compared to the other ones.",
                    "label": 0
                },
                {
                    "sent": "We do not need to assume the cursor assumption in the original space.",
                    "label": 1
                },
                {
                    "sent": "We have a very natural metric induced with the top of our clients, so four pairs of examples.",
                    "label": 0
                },
                {
                    "sent": "There is no cost to collect pairs because you can just take whatever videos you have on the web and you have a quite weak assumptions.",
                    "label": 1
                },
                {
                    "sent": "On the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Open label distribution.",
                    "label": 0
                },
                {
                    "sent": "So there have been some work as well done on top or occurrence before, and I'm not going to talk more.",
                    "label": 0
                },
                {
                    "sent": "Talk much about it.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to cite like three of them.",
                    "label": 0
                },
                {
                    "sent": "SO11, which is really close to walk, is like a slow feature analysis which is about to learn transformation ships which are invariant with time under some constraints.",
                    "label": 1
                },
                {
                    "sent": "To enforce, like I mean for not obtaining like trivial solution like 0 viable so.",
                    "label": 0
                },
                {
                    "sent": "This is quite similar to what we do, but these constraints are actually very tough to enforce in online setup, and in fact it's very tough to scale this kind of method, so same for like Baker 99.",
                    "label": 1
                },
                {
                    "sent": "They proposed like a very complex network handcrafted on everything, while our method is very simple, an also like the IMAX method, which again from the same as which.",
                    "label": 0
                },
                {
                    "sent": "Is unfortunately very slow according to the also and which is like trying to maximize them between formations according to the currency.",
                    "label": 0
                },
                {
                    "sent": "So really again, in contrast, our method is really simple.",
                    "label": 1
                },
                {
                    "sent": "It's highly scalable, and we actually observed like improve generalization whenever we try.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give so we did some experiments using like some quite classical benchmarker data for object recognition.",
                    "label": 0
                },
                {
                    "sent": "So we use like coil 100 and and we followed the setup of like Versing 2003.",
                    "label": 1
                },
                {
                    "sent": "Which to my knowledge is like the state of the audiences database.",
                    "label": 0
                },
                {
                    "sent": "So they are using a very complex network which has like a future dektec cells which are biologically inspired.",
                    "label": 1
                },
                {
                    "sent": "They say so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So well, I mean we want is like.",
                    "label": 0
                },
                {
                    "sent": "It's like a bunch of images which look like that there there are like 100 objects each objects as like 70 two different poses which have been obtained by rotating the object on a turntable.",
                    "label": 1
                },
                {
                    "sent": "And in those setups they are using like 4 views of this object to train and like the rest basically to test and they have looked set with sort here.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "100 objects to train on test.",
                    "label": 0
                },
                {
                    "sent": "So as a video we like collected some object.",
                    "label": 1
                },
                {
                    "sent": "We are like in offices and we like make them term like comment on a turntable and so this objective objects were actually quite related to the object we can find in coil.",
                    "label": 1
                },
                {
                    "sent": "So from these videos we extracted like the 772 views as they had and we just input that to our network's.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Video streaming.",
                    "label": 0
                },
                {
                    "sent": "We also like made some little video which have nothing to do with with oil objects like we took some animal, you know objects and made them term as well and.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Made a video out of it.",
                    "label": 0
                },
                {
                    "sent": "An we compare that's in the in the two setups that we had with like another bunch of algorithm which are quite classical.",
                    "label": 0
                },
                {
                    "sent": "And so the baseline is basically like this Stoner convolutional neural network, which is quite far from the state of the art.",
                    "label": 0
                },
                {
                    "sent": "Then you have like this video convolutional net, which is actually.",
                    "label": 0
                },
                {
                    "sent": "Kind of a transitive set up because we took all the object we had.",
                    "label": 0
                },
                {
                    "sent": "So all the images of the object we like, concatenate them as a big video.",
                    "label": 0
                },
                {
                    "sent": "Basically on fed that as a video too.",
                    "label": 0
                },
                {
                    "sent": "To the to the training algorithm.",
                    "label": 0
                },
                {
                    "sent": "So in that respect the frames, even if they were not labeled where seen during the training.",
                    "label": 0
                },
                {
                    "sent": "And I mean the frame for testing were seen during the training.",
                    "label": 0
                },
                {
                    "sent": "So it's really attractive set up.",
                    "label": 0
                },
                {
                    "sent": "So we also did like a kind of summit supervise set up in the case of like 30 objects.",
                    "label": 1
                },
                {
                    "sent": "We took all the other 70 objects.",
                    "label": 1
                },
                {
                    "sent": "Again, like Mount all the images and use that as a videos and once again we obtain very good performance compared to all the other benchmarks.",
                    "label": 0
                },
                {
                    "sent": "Then we have like De Video coming from similar objects which is I would say more interesting because here we have like different lighting conditions that what they had basically.",
                    "label": 0
                },
                {
                    "sent": "And even more interesting, if we take something which has nothing to do with oil is still works pretty well and is compatible to the state of the art.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also did like some little experiment on the AT&T or FACE data set, which is the data set where there is like 40 subjects, 10 images per subject, where they like change, pose lighting.",
                    "label": 0
                },
                {
                    "sent": "Both lightings, I mean they are smiling.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But and all experiments was trying to train.",
                    "label": 0
                },
                {
                    "sent": "Using like different number of label examples.",
                    "label": 0
                },
                {
                    "sent": "And using the rest of the images as basically a big video game, only bowl of course.",
                    "label": 0
                },
                {
                    "sent": "Fed to the network, so using this transitive set up, it's against transductive we obtain very good results on all the datasets.",
                    "label": 0
                },
                {
                    "sent": "So Even so, it's like small experiments.",
                    "label": 0
                },
                {
                    "sent": "I think we showed him that Tom Parker Erence can be really useful for improving the training of such.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Networks.",
                    "label": 0
                },
                {
                    "sent": "So in the end, to summarize, I showed you like quite general embedding algorithm which can be used to leverage any structural data it has been applied actually on text in the past.",
                    "label": 1
                },
                {
                    "sent": "Document retrieval Service supervised learning.",
                    "label": 1
                },
                {
                    "sent": "We applied it on on object recognition using like video.",
                    "label": 1
                },
                {
                    "sent": "So we showed like we can use video clearance to induce a good basically representation of images and try to improve like that.",
                    "label": 0
                },
                {
                    "sent": "So performance of the network, it outperforms baselines without any engineering features.",
                    "label": 0
                },
                {
                    "sent": "And on the plus side, there is also like a very weak assumptions compared to classical algorithm like some.",
                    "label": 0
                },
                {
                    "sent": "It's for learning and you can take you know huge collection of data from the web like videos without any human annotations on hopes that it will improve your object recognition problem.",
                    "label": 0
                }
            ]
        }
    }
}