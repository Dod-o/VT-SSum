{
    "id": "dhjjp77nk7bqjru5exa56lwfutrei6q7",
    "title": "Locality-Sensitive Binary Codes from Shift-Invariant Kernels",
    "info": {
        "author": [
            "Maxim Raginsky, Department of Electrical and Computer Engineering, Duke University"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nips09_raginsky_lsbc/",
    "segmentation": [
        [
            "Alright, well thank you very much.",
            "So I am Maxine Roginski from Duke University this this is joint work with Svetlana Lazebnik from UNC Chapel Hill.",
            "You hear a lot about the rivalry between these two institutions, but.",
            "We're trying to."
        ],
        [
            "Get over that.",
            "So the problem is that we want to.",
            "In many applications, map high dimensional vectors to binary strings for the purposes of search or retrieval.",
            "The Hamming distance between the binary strings has to correspond to some similarity measure between the original data points.",
            "So if two data points are similar, we want the Hamming distance to be small of the two points are dissimilar will want the Hamming distance to be to be large and the similarity is measured by a kernel.",
            "There's quite a bit of work on this, some of it has exploited random projections and locality sensitive hashing.",
            "Work is a prime example of that recent work by.",
            "Brian Kulis and Kristen Grauman tries to combine it with kernels.",
            "And there's work that instead of using random projections, learns representations using restricted Boltzmann machines or deep belief networks or similar things from from training data.",
            "So that's semantic hashing and small codes, large databases, and more recent work by value ice at all on spectral hashing so well."
        ],
        [
            "What we're going to do is.",
            "Again, use random projections to come up with a mapping from a set of high dimensional data points endowed with the kernel to measure similarity to binary strings, and we're going to.",
            "Develop a distribution free approach, meaning that our embedding will not depend on any distribution of the data.",
            "In fact, for all we know, the data may not even have a reasonable probabilistic model associated to it, or it may be hard to obtain or learn, and so it may not be worth it.",
            "We're going to give uniform guarantees on locality sensitive behavior for any two data points in a reasonable domain, so it's simple and data independent, and we're going to give theoretical convergence guarantees.",
            "And present experimental results.",
            "So the approach in a nutshell is.",
            "It's a it's kind of a two stage approach.",
            "From the work of Ali Rahimi and Ben Recht that was presented at NIPS, I believe two years ago.",
            "We know that it's possible to map data to a higher dimensional Euclidean space in such a way that the inner product of the dot product between embedded points concentrates around the kernel value between these two points in the original space.",
            "So we're going to build on that and binarize that mapping in a proper way to preserve this behavior so."
        ],
        [
            "Let me kind of introduce the random Fourier features of right Hemi and wrecked.",
            "So the idea is that we have a Mercer kernel.",
            "Which is also translation invariant and normalized.",
            "So in other words, the kernel value between two points only depends on their vector difference.",
            "I'm overloading notation here that KX, Y is the same thing as K X -- Y, but I hope I'll be forgiven for this.",
            "Didn't want to use another notation for for the for the difference."
        ],
        [
            "So there's a fundamental result in the harmonic analysis called Buckners theorem, which basically says that any such kernel is a Fourier transform of a unique probability measure.",
            "So in other words.",
            "The kernel value is an expectation of a complex exponential.",
            "The well known example is if we have a Gaussian kernel with a certain bandwidth, then this distribution effect is a spherical Gaussian and the size of the covariance ellipsoid is controlled by the kernel."
        ],
        [
            "So from both new theorem we can actually write.",
            "Like I said that the kernel value is an expectation of a complex exponential, and Moreover because the kernel itself is real valued, it's half the expectation of cosine when based on this."
        ],
        [
            "Fundamental result, right?",
            "Hemi and rap showed using trigonometric identities and properties of of distributions that you can actually map point in and in some space into.",
            "Into another point into a scalar given here by this square root, 2 cosine Omega is drawn from this distribution PK that correspond to the kernel via Brokeness theorem.",
            "Then we had a random phase and the interesting."
        ],
        [
            "Thing is that first of all, if you take 2 points and compute these random features, take the product of these two points, on average, you get the kernel value.",
            "And therefore by standard variance reducing tricks, concatenating these guys and then normalizing them, standardizing them when you look at the inner product, you get back kernel value in.",
            "In fact, the normalized inner product concentrates or on the kernel value.",
            "For any two pairs of points in the compact set.",
            "In fact, we get uniform concentration, so that's very nice.",
            "And what we said, well, what happens if we then take this, take this random Fourier feature?"
        ],
        [
            "And binarize it, so here's how we're going to binarize it.",
            "So let's suppose that somebody gives us this kernel which is Mercer, and look shift invariant, and then we have a desired code length in, so we're going to map the point, which is a D dimensional point into end bits where the NTH bit is produced as follows.",
            "First we draw at random the parameters for a random Fourier feature, compute the Fourier feature then.",
            "We shift it randomly by a threshold that's uniformly distributed on the range of the random features which is between minus root two and two, and then we take the sign of that.",
            "So now each bit is actually determined by three random parameters.",
            "One is Omega that's drawn from this kernel induced distribution.",
            "The other one is the random phase that's needed to get the Buckners theorem to come through, and the third parameter is this random threshold.",
            "So well, So what we're doing here was taking is we're taking one non linearity, which is the cosine and composing it with another really really harsh non non linearity which is the which is the sign.",
            "But what happens is, well, ideally would like the Hamming distance to console."
        ],
        [
            "Great around the kernel value, but that's not going to happen because of the two really, really weird nonlinearities acting and acting on one another, but instead will instead of what we found.",
            "And this is where the random thresholding is crucial.",
            "Is that the expected?",
            "Hamming distance.",
            "Divided by the number of bits is actually given by a certain well defined function of the kernel of the two data points, so this function is shown in blue and it's given by an infinite series, which you can compute any desired precision precision by power.",
            "Many terms you want, But the interesting thing is that the form of this function is independent of the kernel.",
            "And that like I said, has to do with the random thresholding.",
            "I can show you some bounds, so here in red we showed the."
        ],
        [
            "Scatter points scatter plot for points drawn uniformly in a 2 dimensional rectangle, and this shows the normalized Hamming distance and then the green scatter plot shows you the.",
            "Dot product for the random features.",
            "I think they actually should be normalized by M and then the black curve is the kernel value and you can see that."
        ],
        [
            "There is a monotonic relationship between the kernel value and the normalized Hamming distance, but normalized Hamming distance has fatter tails.",
            "Well, let's OK.",
            "Here is.",
            "Here is the Hamming distance."
        ],
        [
            "And a comparison with random features around them 48 features again, we observe a monotonic relationship.",
            "It's not as steep as well would want, but nonetheless it's there, and in fact, if you don't want to compute the infinite series, it's easy to bound that infinite series from above and from below by two functions.",
            "One of them is linear, the lower bound is linear in the kernel value, and the upper bound is piecewise smooth.",
            "It has a linear part which.",
            "Eventually comes to dominate at large Euclidian distances, and it has a square root concave part, so again, you can actually see that the bounds are pretty tight, so if we look at the black curve, is the infinite series around which are Hamming distance concentrates, and then the colored curves correspond to the bounds, and then the minimum of the blue and the red curves is the upper bound.",
            "You can see as the Euclidean distance increases actually hugs the curves.",
            "Curve pretty closely and the same goes for for the green lower bound.",
            "So armed with this week and then start proving theoretical results, the 1st result is a kind of a Johnson Lynn."
        ],
        [
            "Strauss result, where I say that well, if we have a finite set of points then we can embed them in a hypercube binary hypercube of dimension about log in, so their endpoints demand embed them."
        ],
        [
            "Hypercube of dimension about login section will preserve all pair pair wise functions of the kernel values H case in terms of Hamming distance.",
            "So Hamming distance actually concentrates around this monotone function of the kernel value.",
            "So this is the Johnson Lindenstrauss part.",
            "So for any endpoints we can essentially embed, embed them into a hypercube of dimension about log in, and I put Johnson Lindenstrauss in quotes because we don't really preserve the actual kernel value.",
            "We preserve some function of it, but that's good enough for applications as we'll see.",
            "But this is for embedding a finite data set."
        ],
        [
            "Here's this much stronger result that says that, well, if my kernel has a finite second moment.",
            "Which is also condition actually imposed by rahimian rect.",
            "And if I have a compact domain and this compact domains has some intrinsic dimension, then I can essentially preserve.",
            "The function of kernel values between any two points in a compact domain and the and the worst case deviation between normalized Hamming distance and this function can be controlled and can be small with high probability, provided that I draw enough points.",
            "In particular, they draw enough bits, in particular note.",
            "So I showed in blue here that this is a guarantee that uniform right here over the data points.",
            "It does not assume any distribution.",
            "The only randomness here is from.",
            "The code selection and so if it so happens that we choose the kernel band."
        ],
        [
            "With in the right way, for example, in the Gaussian case such that the second moment constant Anna diameter of my data set give us a constant.",
            "Then we can basically embed our data into hypercube whose dimension is equal to the intrinsic dimension of the data domain.",
            "So that's a nice interesting result.",
            "And so that's much stronger than Johnson Lindenstrauss, so this is on par with the original right?",
            "He direct result 4.",
            "Random for you features.",
            "The proof is proof of the Johnson Lindenstrauss type result is easy.",
            "Bunch of popping inequality and union bounds.",
            "This is a bit more intricate.",
            "Uses some empirical process bounds from learning theory, but it gets the job done.",
            "So let me show you some experimental comparisons, so we ran a comparison with this."
        ],
        [
            "Hashing method of why so dull, which in fact was kind of an inspiration for us to do this work in the 1st place.",
            "So spectral hashing is a deterministic encoding, but it makes some distributional assumptions on the data, so we generated.",
            "A bunch of points randomly drawn from from a 2D rectangle.",
            "And here you show the scatter plots that show you the correspondence between Euclidean distance."
        ],
        [
            "Is an hamming distance.",
            "So we define neighbors to be within Euclidean distance of 1.",
            "The green points are the ones that turn out to be within the Euclidean distance of 1 and you can see that for low code links.",
            "So this is 16 bits.",
            "Random signs don't do so great, but spectral hashing does wonderful, but we start increasing the code length and what happens is that our method actually starts concentrating quite nicely and you can see."
        ],
        [
            "That this behavior that as we increase the code length, the distance, the Hamming distance, normalized Hamming distance between faraway points, converges to about .4.",
            "So 40% of the bits will disagree, whereas if the points are close.",
            "The bits will agree and so you can see this concentration, whereas spectral hashing actually does not have this concentration property at at high code links, so even the intermediate code links we do significantly better, so I attribute this to."
        ],
        [
            "Randomness random projections actually doing their thing, so this actually kind of tends to VND."
        ],
        [
            "Locate the use of random projections.",
            "In learning theory.",
            "People were kind of buying them with suspicion, but but I think that if used right, they can actually do pretty amazing."
        ],
        [
            "Thanks, so here's a recall.",
            "Precision analysis for this, so you can actually."
        ],
        [
            "We see that as we increase the code length, our equal precision curves actually become really, really steep and nicely concave, whereas if you increase the.",
            "Code length for spectral hashing.",
            "The recall precision curves actually deteriorate, so again this has to do with the fact that it's a deterministic approach, so it will tend to overfit.",
            "Here's some comparison on real data, so this this is a."
        ],
        [
            "Subset of the label ME database.",
            "We used about 14,800 or so training images of 1000.",
            "Test images and the data we described using the gist descriptors which Rob Fergus had had defined earlier.",
            "These guys are 320 dimensional and again we can show you recall precision curves for our method versus spectral hashing.",
            "Again, we do quite a bit better and we have theoretical guarantees on this on this stuff and our method is also.",
            "In fact, it doesn't matter how independent the dimensions are, they can be as dependent as as they want.",
            "Here are some example queries so."
        ],
        [
            "We basically define a nominal radius for a.",
            "For Euclidean neighborhoods retrieve so, so the image shown in magenta there, that's the that's the center.",
            "Then we retrieve neighbors and then we use a 32 bit code to retrieve.",
            "Neighbors using these binary keys.",
            "So with 32 bits we actually get .81 precision.",
            "The ones shown in red are incorrectly retrieved.",
            "Points that are outside the radius.",
            "Here's a 512 bit code, actually perfect precision."
        ],
        [
            "Here's another set of queries.",
            "32 bit code actually has."
        ],
        [
            "Less than 50% precision but increased up to 512 again, perfect precision."
        ],
        [
            "And here's a final example of query so."
        ],
        [
            "So these guys are quite a bit harder with the 32 bit code.",
            "The precision is .38.",
            "And with the 512 bit code, the precision is .9."
        ],
        [
            "This is pretty good so so this, like I said, vindicates random projections for use of learning, search and retrieval applications.",
            "We're looking at right now, lower bounds on similar schemes and try to find connections to information theory there.",
            "'cause I'm an information theorist, so I'm interested in that, and for more details and for more explanations please come see our poster T."
        ],
        [
            "Tonight, thank you.",
            "Yeah hi thanks.",
            "It was very interesting talk.",
            "I was wondering at this May or may not even make sense but if I just took the Rahimi and Recht kind of idea and quantized their Fourier descriptors or whatever they were called to say 8 bits, could you compare that sort of bit encoding to this and?",
            "And work.",
            "Well, so there are kind of count quantizing them randomly to just one bit, right?",
            "Well, since so since since the I could analyze it to to uniform quantization, doing number of bins because the range of random features is bounded.",
            "So as long as you add a dither signal, as long as you randomize the random feature before quantizing, I can show you more or less the same results were interested in binary encoding encodings here, but as long as you use dithering in a principled way, this.",
            "Random thresholding before random shifting.",
            "Before quantization I can show you that more or less the same result, so you're going to see unbiased witness and.",
            "In the in the Hamming distance there, so that's going to work.",
            "We just didn't pursue it because we're interested in binary mappings.",
            "OK, thank you.",
            "I had a question about the experimental results.",
            "Just the images.",
            "I couldn't see what was visually similar between them.",
            "They all seem to be considered true positives, but they look very well.",
            "These are so right.",
            "You have to be a bit careful with these.",
            "Are these are actually Euclidean neighbors in the between the gist descriptors so we didn't go for any perceptual kind of similarity there.",
            "We just wanted to see how it retrieves the Euclidean neighbors."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, well thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So I am Maxine Roginski from Duke University this this is joint work with Svetlana Lazebnik from UNC Chapel Hill.",
                    "label": 1
                },
                {
                    "sent": "You hear a lot about the rivalry between these two institutions, but.",
                    "label": 0
                },
                {
                    "sent": "We're trying to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get over that.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that we want to.",
                    "label": 1
                },
                {
                    "sent": "In many applications, map high dimensional vectors to binary strings for the purposes of search or retrieval.",
                    "label": 1
                },
                {
                    "sent": "The Hamming distance between the binary strings has to correspond to some similarity measure between the original data points.",
                    "label": 0
                },
                {
                    "sent": "So if two data points are similar, we want the Hamming distance to be small of the two points are dissimilar will want the Hamming distance to be to be large and the similarity is measured by a kernel.",
                    "label": 1
                },
                {
                    "sent": "There's quite a bit of work on this, some of it has exploited random projections and locality sensitive hashing.",
                    "label": 0
                },
                {
                    "sent": "Work is a prime example of that recent work by.",
                    "label": 0
                },
                {
                    "sent": "Brian Kulis and Kristen Grauman tries to combine it with kernels.",
                    "label": 0
                },
                {
                    "sent": "And there's work that instead of using random projections, learns representations using restricted Boltzmann machines or deep belief networks or similar things from from training data.",
                    "label": 1
                },
                {
                    "sent": "So that's semantic hashing and small codes, large databases, and more recent work by value ice at all on spectral hashing so well.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we're going to do is.",
                    "label": 0
                },
                {
                    "sent": "Again, use random projections to come up with a mapping from a set of high dimensional data points endowed with the kernel to measure similarity to binary strings, and we're going to.",
                    "label": 0
                },
                {
                    "sent": "Develop a distribution free approach, meaning that our embedding will not depend on any distribution of the data.",
                    "label": 0
                },
                {
                    "sent": "In fact, for all we know, the data may not even have a reasonable probabilistic model associated to it, or it may be hard to obtain or learn, and so it may not be worth it.",
                    "label": 0
                },
                {
                    "sent": "We're going to give uniform guarantees on locality sensitive behavior for any two data points in a reasonable domain, so it's simple and data independent, and we're going to give theoretical convergence guarantees.",
                    "label": 0
                },
                {
                    "sent": "And present experimental results.",
                    "label": 0
                },
                {
                    "sent": "So the approach in a nutshell is.",
                    "label": 0
                },
                {
                    "sent": "It's a it's kind of a two stage approach.",
                    "label": 0
                },
                {
                    "sent": "From the work of Ali Rahimi and Ben Recht that was presented at NIPS, I believe two years ago.",
                    "label": 0
                },
                {
                    "sent": "We know that it's possible to map data to a higher dimensional Euclidean space in such a way that the inner product of the dot product between embedded points concentrates around the kernel value between these two points in the original space.",
                    "label": 0
                },
                {
                    "sent": "So we're going to build on that and binarize that mapping in a proper way to preserve this behavior so.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me kind of introduce the random Fourier features of right Hemi and wrecked.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we have a Mercer kernel.",
                    "label": 1
                },
                {
                    "sent": "Which is also translation invariant and normalized.",
                    "label": 1
                },
                {
                    "sent": "So in other words, the kernel value between two points only depends on their vector difference.",
                    "label": 0
                },
                {
                    "sent": "I'm overloading notation here that KX, Y is the same thing as K X -- Y, but I hope I'll be forgiven for this.",
                    "label": 0
                },
                {
                    "sent": "Didn't want to use another notation for for the for the difference.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's a fundamental result in the harmonic analysis called Buckners theorem, which basically says that any such kernel is a Fourier transform of a unique probability measure.",
                    "label": 1
                },
                {
                    "sent": "So in other words.",
                    "label": 0
                },
                {
                    "sent": "The kernel value is an expectation of a complex exponential.",
                    "label": 0
                },
                {
                    "sent": "The well known example is if we have a Gaussian kernel with a certain bandwidth, then this distribution effect is a spherical Gaussian and the size of the covariance ellipsoid is controlled by the kernel.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So from both new theorem we can actually write.",
                    "label": 0
                },
                {
                    "sent": "Like I said that the kernel value is an expectation of a complex exponential, and Moreover because the kernel itself is real valued, it's half the expectation of cosine when based on this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fundamental result, right?",
                    "label": 0
                },
                {
                    "sent": "Hemi and rap showed using trigonometric identities and properties of of distributions that you can actually map point in and in some space into.",
                    "label": 0
                },
                {
                    "sent": "Into another point into a scalar given here by this square root, 2 cosine Omega is drawn from this distribution PK that correspond to the kernel via Brokeness theorem.",
                    "label": 0
                },
                {
                    "sent": "Then we had a random phase and the interesting.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing is that first of all, if you take 2 points and compute these random features, take the product of these two points, on average, you get the kernel value.",
                    "label": 0
                },
                {
                    "sent": "And therefore by standard variance reducing tricks, concatenating these guys and then normalizing them, standardizing them when you look at the inner product, you get back kernel value in.",
                    "label": 0
                },
                {
                    "sent": "In fact, the normalized inner product concentrates or on the kernel value.",
                    "label": 1
                },
                {
                    "sent": "For any two pairs of points in the compact set.",
                    "label": 0
                },
                {
                    "sent": "In fact, we get uniform concentration, so that's very nice.",
                    "label": 1
                },
                {
                    "sent": "And what we said, well, what happens if we then take this, take this random Fourier feature?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And binarize it, so here's how we're going to binarize it.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose that somebody gives us this kernel which is Mercer, and look shift invariant, and then we have a desired code length in, so we're going to map the point, which is a D dimensional point into end bits where the NTH bit is produced as follows.",
                    "label": 0
                },
                {
                    "sent": "First we draw at random the parameters for a random Fourier feature, compute the Fourier feature then.",
                    "label": 1
                },
                {
                    "sent": "We shift it randomly by a threshold that's uniformly distributed on the range of the random features which is between minus root two and two, and then we take the sign of that.",
                    "label": 0
                },
                {
                    "sent": "So now each bit is actually determined by three random parameters.",
                    "label": 0
                },
                {
                    "sent": "One is Omega that's drawn from this kernel induced distribution.",
                    "label": 0
                },
                {
                    "sent": "The other one is the random phase that's needed to get the Buckners theorem to come through, and the third parameter is this random threshold.",
                    "label": 0
                },
                {
                    "sent": "So well, So what we're doing here was taking is we're taking one non linearity, which is the cosine and composing it with another really really harsh non non linearity which is the which is the sign.",
                    "label": 0
                },
                {
                    "sent": "But what happens is, well, ideally would like the Hamming distance to console.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Great around the kernel value, but that's not going to happen because of the two really, really weird nonlinearities acting and acting on one another, but instead will instead of what we found.",
                    "label": 0
                },
                {
                    "sent": "And this is where the random thresholding is crucial.",
                    "label": 0
                },
                {
                    "sent": "Is that the expected?",
                    "label": 0
                },
                {
                    "sent": "Hamming distance.",
                    "label": 0
                },
                {
                    "sent": "Divided by the number of bits is actually given by a certain well defined function of the kernel of the two data points, so this function is shown in blue and it's given by an infinite series, which you can compute any desired precision precision by power.",
                    "label": 0
                },
                {
                    "sent": "Many terms you want, But the interesting thing is that the form of this function is independent of the kernel.",
                    "label": 0
                },
                {
                    "sent": "And that like I said, has to do with the random thresholding.",
                    "label": 0
                },
                {
                    "sent": "I can show you some bounds, so here in red we showed the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scatter points scatter plot for points drawn uniformly in a 2 dimensional rectangle, and this shows the normalized Hamming distance and then the green scatter plot shows you the.",
                    "label": 0
                },
                {
                    "sent": "Dot product for the random features.",
                    "label": 0
                },
                {
                    "sent": "I think they actually should be normalized by M and then the black curve is the kernel value and you can see that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a monotonic relationship between the kernel value and the normalized Hamming distance, but normalized Hamming distance has fatter tails.",
                    "label": 0
                },
                {
                    "sent": "Well, let's OK.",
                    "label": 0
                },
                {
                    "sent": "Here is.",
                    "label": 0
                },
                {
                    "sent": "Here is the Hamming distance.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And a comparison with random features around them 48 features again, we observe a monotonic relationship.",
                    "label": 0
                },
                {
                    "sent": "It's not as steep as well would want, but nonetheless it's there, and in fact, if you don't want to compute the infinite series, it's easy to bound that infinite series from above and from below by two functions.",
                    "label": 0
                },
                {
                    "sent": "One of them is linear, the lower bound is linear in the kernel value, and the upper bound is piecewise smooth.",
                    "label": 0
                },
                {
                    "sent": "It has a linear part which.",
                    "label": 0
                },
                {
                    "sent": "Eventually comes to dominate at large Euclidian distances, and it has a square root concave part, so again, you can actually see that the bounds are pretty tight, so if we look at the black curve, is the infinite series around which are Hamming distance concentrates, and then the colored curves correspond to the bounds, and then the minimum of the blue and the red curves is the upper bound.",
                    "label": 0
                },
                {
                    "sent": "You can see as the Euclidean distance increases actually hugs the curves.",
                    "label": 0
                },
                {
                    "sent": "Curve pretty closely and the same goes for for the green lower bound.",
                    "label": 0
                },
                {
                    "sent": "So armed with this week and then start proving theoretical results, the 1st result is a kind of a Johnson Lynn.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Strauss result, where I say that well, if we have a finite set of points then we can embed them in a hypercube binary hypercube of dimension about log in, so their endpoints demand embed them.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hypercube of dimension about login section will preserve all pair pair wise functions of the kernel values H case in terms of Hamming distance.",
                    "label": 0
                },
                {
                    "sent": "So Hamming distance actually concentrates around this monotone function of the kernel value.",
                    "label": 0
                },
                {
                    "sent": "So this is the Johnson Lindenstrauss part.",
                    "label": 0
                },
                {
                    "sent": "So for any endpoints we can essentially embed, embed them into a hypercube of dimension about log in, and I put Johnson Lindenstrauss in quotes because we don't really preserve the actual kernel value.",
                    "label": 0
                },
                {
                    "sent": "We preserve some function of it, but that's good enough for applications as we'll see.",
                    "label": 0
                },
                {
                    "sent": "But this is for embedding a finite data set.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's this much stronger result that says that, well, if my kernel has a finite second moment.",
                    "label": 1
                },
                {
                    "sent": "Which is also condition actually imposed by rahimian rect.",
                    "label": 1
                },
                {
                    "sent": "And if I have a compact domain and this compact domains has some intrinsic dimension, then I can essentially preserve.",
                    "label": 0
                },
                {
                    "sent": "The function of kernel values between any two points in a compact domain and the and the worst case deviation between normalized Hamming distance and this function can be controlled and can be small with high probability, provided that I draw enough points.",
                    "label": 0
                },
                {
                    "sent": "In particular, they draw enough bits, in particular note.",
                    "label": 0
                },
                {
                    "sent": "So I showed in blue here that this is a guarantee that uniform right here over the data points.",
                    "label": 0
                },
                {
                    "sent": "It does not assume any distribution.",
                    "label": 0
                },
                {
                    "sent": "The only randomness here is from.",
                    "label": 0
                },
                {
                    "sent": "The code selection and so if it so happens that we choose the kernel band.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With in the right way, for example, in the Gaussian case such that the second moment constant Anna diameter of my data set give us a constant.",
                    "label": 1
                },
                {
                    "sent": "Then we can basically embed our data into hypercube whose dimension is equal to the intrinsic dimension of the data domain.",
                    "label": 0
                },
                {
                    "sent": "So that's a nice interesting result.",
                    "label": 0
                },
                {
                    "sent": "And so that's much stronger than Johnson Lindenstrauss, so this is on par with the original right?",
                    "label": 0
                },
                {
                    "sent": "He direct result 4.",
                    "label": 0
                },
                {
                    "sent": "Random for you features.",
                    "label": 0
                },
                {
                    "sent": "The proof is proof of the Johnson Lindenstrauss type result is easy.",
                    "label": 0
                },
                {
                    "sent": "Bunch of popping inequality and union bounds.",
                    "label": 0
                },
                {
                    "sent": "This is a bit more intricate.",
                    "label": 0
                },
                {
                    "sent": "Uses some empirical process bounds from learning theory, but it gets the job done.",
                    "label": 0
                },
                {
                    "sent": "So let me show you some experimental comparisons, so we ran a comparison with this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hashing method of why so dull, which in fact was kind of an inspiration for us to do this work in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "So spectral hashing is a deterministic encoding, but it makes some distributional assumptions on the data, so we generated.",
                    "label": 0
                },
                {
                    "sent": "A bunch of points randomly drawn from from a 2D rectangle.",
                    "label": 1
                },
                {
                    "sent": "And here you show the scatter plots that show you the correspondence between Euclidean distance.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is an hamming distance.",
                    "label": 0
                },
                {
                    "sent": "So we define neighbors to be within Euclidean distance of 1.",
                    "label": 0
                },
                {
                    "sent": "The green points are the ones that turn out to be within the Euclidean distance of 1 and you can see that for low code links.",
                    "label": 0
                },
                {
                    "sent": "So this is 16 bits.",
                    "label": 0
                },
                {
                    "sent": "Random signs don't do so great, but spectral hashing does wonderful, but we start increasing the code length and what happens is that our method actually starts concentrating quite nicely and you can see.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That this behavior that as we increase the code length, the distance, the Hamming distance, normalized Hamming distance between faraway points, converges to about .4.",
                    "label": 0
                },
                {
                    "sent": "So 40% of the bits will disagree, whereas if the points are close.",
                    "label": 0
                },
                {
                    "sent": "The bits will agree and so you can see this concentration, whereas spectral hashing actually does not have this concentration property at at high code links, so even the intermediate code links we do significantly better, so I attribute this to.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Randomness random projections actually doing their thing, so this actually kind of tends to VND.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Locate the use of random projections.",
                    "label": 0
                },
                {
                    "sent": "In learning theory.",
                    "label": 0
                },
                {
                    "sent": "People were kind of buying them with suspicion, but but I think that if used right, they can actually do pretty amazing.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks, so here's a recall.",
                    "label": 0
                },
                {
                    "sent": "Precision analysis for this, so you can actually.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We see that as we increase the code length, our equal precision curves actually become really, really steep and nicely concave, whereas if you increase the.",
                    "label": 0
                },
                {
                    "sent": "Code length for spectral hashing.",
                    "label": 1
                },
                {
                    "sent": "The recall precision curves actually deteriorate, so again this has to do with the fact that it's a deterministic approach, so it will tend to overfit.",
                    "label": 0
                },
                {
                    "sent": "Here's some comparison on real data, so this this is a.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Subset of the label ME database.",
                    "label": 0
                },
                {
                    "sent": "We used about 14,800 or so training images of 1000.",
                    "label": 0
                },
                {
                    "sent": "Test images and the data we described using the gist descriptors which Rob Fergus had had defined earlier.",
                    "label": 1
                },
                {
                    "sent": "These guys are 320 dimensional and again we can show you recall precision curves for our method versus spectral hashing.",
                    "label": 0
                },
                {
                    "sent": "Again, we do quite a bit better and we have theoretical guarantees on this on this stuff and our method is also.",
                    "label": 0
                },
                {
                    "sent": "In fact, it doesn't matter how independent the dimensions are, they can be as dependent as as they want.",
                    "label": 0
                },
                {
                    "sent": "Here are some example queries so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We basically define a nominal radius for a.",
                    "label": 0
                },
                {
                    "sent": "For Euclidean neighborhoods retrieve so, so the image shown in magenta there, that's the that's the center.",
                    "label": 0
                },
                {
                    "sent": "Then we retrieve neighbors and then we use a 32 bit code to retrieve.",
                    "label": 1
                },
                {
                    "sent": "Neighbors using these binary keys.",
                    "label": 0
                },
                {
                    "sent": "So with 32 bits we actually get .81 precision.",
                    "label": 0
                },
                {
                    "sent": "The ones shown in red are incorrectly retrieved.",
                    "label": 0
                },
                {
                    "sent": "Points that are outside the radius.",
                    "label": 0
                },
                {
                    "sent": "Here's a 512 bit code, actually perfect precision.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another set of queries.",
                    "label": 0
                },
                {
                    "sent": "32 bit code actually has.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Less than 50% precision but increased up to 512 again, perfect precision.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's a final example of query so.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these guys are quite a bit harder with the 32 bit code.",
                    "label": 1
                },
                {
                    "sent": "The precision is .38.",
                    "label": 0
                },
                {
                    "sent": "And with the 512 bit code, the precision is .9.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is pretty good so so this, like I said, vindicates random projections for use of learning, search and retrieval applications.",
                    "label": 0
                },
                {
                    "sent": "We're looking at right now, lower bounds on similar schemes and try to find connections to information theory there.",
                    "label": 0
                },
                {
                    "sent": "'cause I'm an information theorist, so I'm interested in that, and for more details and for more explanations please come see our poster T.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tonight, thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah hi thanks.",
                    "label": 0
                },
                {
                    "sent": "It was very interesting talk.",
                    "label": 0
                },
                {
                    "sent": "I was wondering at this May or may not even make sense but if I just took the Rahimi and Recht kind of idea and quantized their Fourier descriptors or whatever they were called to say 8 bits, could you compare that sort of bit encoding to this and?",
                    "label": 0
                },
                {
                    "sent": "And work.",
                    "label": 0
                },
                {
                    "sent": "Well, so there are kind of count quantizing them randomly to just one bit, right?",
                    "label": 0
                },
                {
                    "sent": "Well, since so since since the I could analyze it to to uniform quantization, doing number of bins because the range of random features is bounded.",
                    "label": 0
                },
                {
                    "sent": "So as long as you add a dither signal, as long as you randomize the random feature before quantizing, I can show you more or less the same results were interested in binary encoding encodings here, but as long as you use dithering in a principled way, this.",
                    "label": 0
                },
                {
                    "sent": "Random thresholding before random shifting.",
                    "label": 0
                },
                {
                    "sent": "Before quantization I can show you that more or less the same result, so you're going to see unbiased witness and.",
                    "label": 0
                },
                {
                    "sent": "In the in the Hamming distance there, so that's going to work.",
                    "label": 0
                },
                {
                    "sent": "We just didn't pursue it because we're interested in binary mappings.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "I had a question about the experimental results.",
                    "label": 0
                },
                {
                    "sent": "Just the images.",
                    "label": 0
                },
                {
                    "sent": "I couldn't see what was visually similar between them.",
                    "label": 0
                },
                {
                    "sent": "They all seem to be considered true positives, but they look very well.",
                    "label": 0
                },
                {
                    "sent": "These are so right.",
                    "label": 0
                },
                {
                    "sent": "You have to be a bit careful with these.",
                    "label": 0
                },
                {
                    "sent": "Are these are actually Euclidean neighbors in the between the gist descriptors so we didn't go for any perceptual kind of similarity there.",
                    "label": 0
                },
                {
                    "sent": "We just wanted to see how it retrieves the Euclidean neighbors.",
                    "label": 0
                }
            ]
        }
    }
}