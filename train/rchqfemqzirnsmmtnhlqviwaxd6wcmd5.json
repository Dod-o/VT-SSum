{
    "id": "rchqfemqzirnsmmtnhlqviwaxd6wcmd5",
    "title": "Large-Scale Clustering through Functional Embedding",
    "info": {
        "author": [
            "Frederic Ratle, University of Lausanne",
            "Jason Weston, Facebook",
            "Mathew L. Miller, NEC Laboratories America, Inc."
        ],
        "published": "Oct. 10, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd08_ratle_lsct/",
    "segmentation": [
        [
            "So hello, thank you for attending my talk entitled large scale clustering through a functional embedding and this is a work between Jason Weston, Matthew Miller and myself.",
            "So basically"
        ],
        [
            "What I'm going to present is a new way at looking at performing data clustering by using.",
            "By using a dimensionality reduction method.",
            "With a direct optimization over the discrete labels, this way we wish to.",
            "Try to build a more efficient way of doing clustering.",
            "We have found that the joint optimization of embedding N clustering algorithms can sometimes lead to improve results.",
            "The implementation within a functional model, in our case a neural network.",
            "Avoids the classical out of sample problem that people usually have in feature extraction or or clustering problem.",
            "And the training of this functional model by stochastic gradient descent allows to have at hand really fast and scalable method.",
            "So that's basically the big picture."
        ],
        [
            "No.",
            "The usual way of clustering.",
            "How does it work?",
            "Most algorithms such As for instance spectral clustering which have grown in popularity in the recent years.",
            "Are based on a two stage approach like you really have two disjoint steps that you perform.",
            "First you find a good embedding, whatever that means.",
            "Some people doing good to preserve locality or a neighborhood relations, or the topology of the data set.",
            "It depends on the problem at hand.",
            "Following this step, people usually perform K means or any sophisticated variant of of of this.",
            "This algorithm.",
            "Also, there's been some work on the K means in feature space like kernel K means or all algorithms related to a margin based clustering such as support vector clustering.",
            "These approaches are all based on the somehow a 2 steps approach and we wish to go around this this way of.",
            "Of doing of doing things.",
            "Try this."
        ],
        [
            "But with.",
            "So when I talk about embedding, I'm going to first try to describe what are the principles that are usually used behind embedding algorithms.",
            "And most of the embedding algorithms, which are explicit algorithms, that is, we have points in the input space we wish to find a point to point mapping from the input space to the embedding space, and this is done by minimizing some sort of pairwise loss function.",
            "This is a loss function that deals with pairs of example.",
            "So you have example XI XJ there mapping through a function F or a look up table that we can name F. And their weight, WIJ, which encodes the similarity or dissimilarity between those data points.",
            "So among the popular algorithms, you have multi dimensional scaling which which minimizes the loss function of this form you have.",
            "ISL map which minimizes the same type of function.",
            "But by using the shortest path under the neighborhood graph.",
            "And in my opinion, a method that it's a bit more general is the Laplacian Eigen Maps which has been thoroughly discussed in this.",
            "This conference which minimizes some sort of energy loss function.",
            "So basically if you have data points that are close in input space, you wish them to be mapped closely and output space, and this is subject to some balancing constraints because obviously the trivial solution with minimize this loss function.",
            "Now, spectral clustering is basically apart from some little technical details.",
            "You basically apply AK means algorithm on Laplacian Eigen Maps using some normalized Laplacian.",
            "So you really have a first optimization problem that's finding a good embedding and then you had like a K means.",
            "On top of that."
        ],
        [
            "There have been some attempts to learn embeddings for clustering using functional models.",
            "You have, for instance, autoencoder networks or more recently.",
            "Dimensionality reduction using an invariant mapping.",
            "The Doctor Lim approach from Hatzel show kinda car, which is basically Laplacian Eigen Maps implemented in a neural network.",
            "So what this loss function does is basically well here you have the mappings of Point XI an XJ.",
            "So if there are the neighbors, we want to minimize this, and if they're not the neighbors we want to minimize this.",
            "Anne.",
            "Did this.",
            "This means that if we want our neighbors from input space to be closed and the points that are in neighbors, we want them to be pushed away one from another if they're too close, that is that if they are closer than the margin M. So this second term avoids the use of balancing constraints, because because you cannot collapse the embedding through a single point by minimizing those two terms, which makes the optimization with stochastic gradient descent much more easy than with Laplacian Eigen Maps.",
            "And by using a functional model, we avoid some problems we can actually control the capacity of the classifier.",
            "We can add some prior knowledge in the set of functions and we don't have an out of sample problem as we have a nice mapping to use with new examples."
        ],
        [
            "Now what we did is basically using the Doctor Lim ideas, but rather than optimizing and embedding, we've tried to learn directly the clustering task.",
            "So the main idea has been to train our classifier to basically push points that are deemed neighbors in the same class an pulling apart in different classes.",
            "Points that are not dim neighbors.",
            "So basically, if you have an edge between two data points.",
            "And they're not in the same class you train the classifier to get them in the same class, and vice versa.",
            "So you could see this as some form of a. Unsupervised transduction maybe?",
            "I don't know.",
            "Anne."
        ],
        [
            "The last function is a little bit similar to the Doctor Lim approach, but rather than using the Laplacian term we use here a hinge loss H. You could use any other classification based loss function.",
            "We've used this one because it's nice because it has some sort of margin concept.",
            "So we have a loss between F of X, which is the mapping of XI and C, which is the class towards which we think that we must push F of X.",
            "And why is actually?"
        ],
        [
            "Sorry.",
            "Huawei is actually the weight.",
            "It encodes the weights to assigned to our point XI.",
            "Being in cluster C. So for instance, if I&J are neighbors.",
            "We take the sign of the sum of the two points.",
            "So for instance, with a neural network you have an output between minus one and one.",
            "So by taking the sign of those predictions you'll actually get the maximum output and you push the points towards class Class C with some positive learning rates.",
            "If the points are neighbors you do you do the similar thing, but user, you basically train the classifier to push them apart using using some other learning rate.",
            "And we optimize this by stochastic gradient descent, so it's it's a function that's easy to optimize this way and and it gives a really nice update rule."
        ],
        [
            "So basically the algorithm which I call here, the end cut embedding algorithm, basically looks like that.",
            "So you have some unlabeled data.",
            "You pick a random pair of neighbors from from your graph, you select the class to push towards doors.",
            "Those points, which is the sign of the some of the predictions an if some balancing constraints are verified, I'll just talk about them on the next slide.",
            "You do a gradient step to push them together.",
            "Then you pick just some random pair an you do a gradient step to push them a, pull them apart.",
            "The nice thing about this algorithm is that you can really easily also get an extension to semi supervised learning, because if you have some labeled data you can also optimize a supervised hinge loss as a bonus, for instance at each iteration, so you get a really general framework for clustering or semi supervised learning tasks."
        ],
        [
            "Now when you have two class problem, it's really important to use some balancing constraints, because you could really easily get all your points into one class which you want to avoid.",
            "So a really simple way to do that is keep the last predictions and in memory and if you've seen too much of one class saying the last 100 examples, you just ignore examples of that class for awhile.",
            "This is called a cold.",
            "Is the hard constraint, and you can also use a soft constraint.",
            "That is, you just divide the learning rate by number of examples that you've seen of that class in the last 100 example, for instance.",
            "So, so it's easier.",
            "It's really easy to implement some constraints."
        ],
        [
            "The framework is actually quite easy to to generalize to multi class settings or multi cluster settings, sorry.",
            "For instance, you could.",
            "Sorry.",
            "You could simply if you had.",
            "If you have a classifier with multiple outputs, you can simply select the maximum outputs and push neighbors towards that maximum output.",
            "Alternatively, you can also choose to use the mean label.",
            "Another approach that we've tried is the all approach, that is, you have one learning rate per cluster, which is related to the value of that prediction for that cluster.",
            "So in that case you can use for instance a softmax function of the ID output of the of the neural net, and you can obtain nice weights between between zero and one.",
            "And you can use the same type of balancing constraints to two class problems."
        ],
        [
            "Well, first we've tested this approach on some small scale datasets that are used in clustering and semi supervised learning literature.",
            "Both two class in multiclass datasets, USPS text and G50C are widely used in semi supervised learning.",
            "BCW and Glass serve UCI datasets and the Ellipse one is simply for 450 dimensional... connected through.",
            "For one point."
        ],
        [
            "So here I give two measures.",
            "Actually there's the clustering error, which is, well, the the incorrect assignation of labels and the out of sample error which you could see as as a classification error.",
            "It's here.",
            "I've trained the neural network on some train training set and this is somehow the test error.",
            "In order to assess if the out of sample clustering is as good as the in simple clustering.",
            "And actually, well, it's compared to K means and spectral clustering using both are full RBF graph and a K nearest neighbor graph.",
            "So actually we find that the method is quite competitive to spectral clustering, and one good surprise that we had is that in some cases it's even a little bit better, which in my opinion could be an effect of doing the optimization of clustering and embedding in one single algorithm rather than having this.",
            "This two stage approach.",
            "Anne.",
            "So these are the two class results."
        ],
        [
            "And in the multiclass case the same, the same can be observed.",
            "We have results that are compareable to the best results that can be obtained with spectral clustering.",
            "So you have the income embedding with the Max and all variants.",
            "And.",
            "So it's the same here.",
            "I've computed the clustering error in the out of sample error and there also have the same order of magnitude, which is, which is a good thing.",
            "Next"
        ],
        [
            "Just to get an idea of how fast it wasn't databases that are a bit larger.",
            "Ava did some amnesty experiments, so here I've just plotted a subset of EM list with the passthrough Laplacian Eigen Maps.",
            "Because people seldom do that, but it's quite instructive to see that it's it has a really nice cluster structure actually."
        ],
        [
            "So I've decided some amnist experiments with a varying number of clusters.",
            "So here you have the 10 digits and I've fragmented them in 20 and 50 clusters.",
            "In the case of the 10 cluster problem, this is done with a neural net with one hidden layer.",
            "The results are really much better than K means.",
            "Well, for scalability issues I didn't try it on spectral clustering, but.",
            "With faster approaches for spectral clustering, doing a comparison would be an interesting.",
            "Of course, the more you divide the problems into more clusters, the closer you get to K means, because cluster are easier to their closer to round structures, but still in all cases we get we get an error that's slightly better or much better than K means.",
            "In the case of 10 clusters.",
            "But the best thing about about this is that it's actually really fast.",
            "It's been done on the normal normal computer and depending on it it can take from half an hour to 40 minutes, maybe for the endless database, which is, which is quite good and using stochastic gradient descent allows the method to scale linearly with the size of the data set.",
            "So, so that's basically it.",
            "Maybe some concluding remarks.",
            "Anne."
        ],
        [
            "The op sorry, I just have a little remark to do about the training on pairs, which is usually considered as very slow, but something that can be done in case that you have some specific prior knowledge about your problem is that you can assign.",
            "And adds a positive edge 2, two neighbor, two data points based on prior knowledge like for video and audio mining, consecutive frames can be labeled as neighbors directly, or in the case of text to words that are close in, your text can be assigned the same topic.",
            "The same.",
            "For web mining you can use link information, so of course the computation of neighbors is always the bottleneck, but in most cases you have, you have little tricks that you can apply to speed that up."
        ],
        [
            "So in summary.",
            "The concluding remarks would be that the functional embedding for clustering allows a faster clustering and avoids the out of sample problem.",
            "We found that to some extent the joint optimization of clustering and embedding provides good or at least better or at least similar results to existing methods and that we have a scalable and flexible framework with this type of algorithm that can be.",
            "Easily generalized for instance to semi supervised learning, so that's it.",
            "Thank you for your attention.",
            "To."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So hello, thank you for attending my talk entitled large scale clustering through a functional embedding and this is a work between Jason Weston, Matthew Miller and myself.",
                    "label": 0
                },
                {
                    "sent": "So basically",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I'm going to present is a new way at looking at performing data clustering by using.",
                    "label": 1
                },
                {
                    "sent": "By using a dimensionality reduction method.",
                    "label": 1
                },
                {
                    "sent": "With a direct optimization over the discrete labels, this way we wish to.",
                    "label": 1
                },
                {
                    "sent": "Try to build a more efficient way of doing clustering.",
                    "label": 0
                },
                {
                    "sent": "We have found that the joint optimization of embedding N clustering algorithms can sometimes lead to improve results.",
                    "label": 0
                },
                {
                    "sent": "The implementation within a functional model, in our case a neural network.",
                    "label": 1
                },
                {
                    "sent": "Avoids the classical out of sample problem that people usually have in feature extraction or or clustering problem.",
                    "label": 0
                },
                {
                    "sent": "And the training of this functional model by stochastic gradient descent allows to have at hand really fast and scalable method.",
                    "label": 1
                },
                {
                    "sent": "So that's basically the big picture.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "The usual way of clustering.",
                    "label": 1
                },
                {
                    "sent": "How does it work?",
                    "label": 0
                },
                {
                    "sent": "Most algorithms such As for instance spectral clustering which have grown in popularity in the recent years.",
                    "label": 1
                },
                {
                    "sent": "Are based on a two stage approach like you really have two disjoint steps that you perform.",
                    "label": 0
                },
                {
                    "sent": "First you find a good embedding, whatever that means.",
                    "label": 1
                },
                {
                    "sent": "Some people doing good to preserve locality or a neighborhood relations, or the topology of the data set.",
                    "label": 0
                },
                {
                    "sent": "It depends on the problem at hand.",
                    "label": 0
                },
                {
                    "sent": "Following this step, people usually perform K means or any sophisticated variant of of of this.",
                    "label": 0
                },
                {
                    "sent": "This algorithm.",
                    "label": 0
                },
                {
                    "sent": "Also, there's been some work on the K means in feature space like kernel K means or all algorithms related to a margin based clustering such as support vector clustering.",
                    "label": 0
                },
                {
                    "sent": "These approaches are all based on the somehow a 2 steps approach and we wish to go around this this way of.",
                    "label": 0
                },
                {
                    "sent": "Of doing of doing things.",
                    "label": 0
                },
                {
                    "sent": "Try this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But with.",
                    "label": 0
                },
                {
                    "sent": "So when I talk about embedding, I'm going to first try to describe what are the principles that are usually used behind embedding algorithms.",
                    "label": 1
                },
                {
                    "sent": "And most of the embedding algorithms, which are explicit algorithms, that is, we have points in the input space we wish to find a point to point mapping from the input space to the embedding space, and this is done by minimizing some sort of pairwise loss function.",
                    "label": 0
                },
                {
                    "sent": "This is a loss function that deals with pairs of example.",
                    "label": 0
                },
                {
                    "sent": "So you have example XI XJ there mapping through a function F or a look up table that we can name F. And their weight, WIJ, which encodes the similarity or dissimilarity between those data points.",
                    "label": 0
                },
                {
                    "sent": "So among the popular algorithms, you have multi dimensional scaling which which minimizes the loss function of this form you have.",
                    "label": 0
                },
                {
                    "sent": "ISL map which minimizes the same type of function.",
                    "label": 0
                },
                {
                    "sent": "But by using the shortest path under the neighborhood graph.",
                    "label": 1
                },
                {
                    "sent": "And in my opinion, a method that it's a bit more general is the Laplacian Eigen Maps which has been thoroughly discussed in this.",
                    "label": 0
                },
                {
                    "sent": "This conference which minimizes some sort of energy loss function.",
                    "label": 0
                },
                {
                    "sent": "So basically if you have data points that are close in input space, you wish them to be mapped closely and output space, and this is subject to some balancing constraints because obviously the trivial solution with minimize this loss function.",
                    "label": 1
                },
                {
                    "sent": "Now, spectral clustering is basically apart from some little technical details.",
                    "label": 0
                },
                {
                    "sent": "You basically apply AK means algorithm on Laplacian Eigen Maps using some normalized Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So you really have a first optimization problem that's finding a good embedding and then you had like a K means.",
                    "label": 0
                },
                {
                    "sent": "On top of that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There have been some attempts to learn embeddings for clustering using functional models.",
                    "label": 0
                },
                {
                    "sent": "You have, for instance, autoencoder networks or more recently.",
                    "label": 0
                },
                {
                    "sent": "Dimensionality reduction using an invariant mapping.",
                    "label": 0
                },
                {
                    "sent": "The Doctor Lim approach from Hatzel show kinda car, which is basically Laplacian Eigen Maps implemented in a neural network.",
                    "label": 0
                },
                {
                    "sent": "So what this loss function does is basically well here you have the mappings of Point XI an XJ.",
                    "label": 0
                },
                {
                    "sent": "So if there are the neighbors, we want to minimize this, and if they're not the neighbors we want to minimize this.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Did this.",
                    "label": 0
                },
                {
                    "sent": "This means that if we want our neighbors from input space to be closed and the points that are in neighbors, we want them to be pushed away one from another if they're too close, that is that if they are closer than the margin M. So this second term avoids the use of balancing constraints, because because you cannot collapse the embedding through a single point by minimizing those two terms, which makes the optimization with stochastic gradient descent much more easy than with Laplacian Eigen Maps.",
                    "label": 0
                },
                {
                    "sent": "And by using a functional model, we avoid some problems we can actually control the capacity of the classifier.",
                    "label": 0
                },
                {
                    "sent": "We can add some prior knowledge in the set of functions and we don't have an out of sample problem as we have a nice mapping to use with new examples.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now what we did is basically using the Doctor Lim ideas, but rather than optimizing and embedding, we've tried to learn directly the clustering task.",
                    "label": 1
                },
                {
                    "sent": "So the main idea has been to train our classifier to basically push points that are deemed neighbors in the same class an pulling apart in different classes.",
                    "label": 0
                },
                {
                    "sent": "Points that are not dim neighbors.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you have an edge between two data points.",
                    "label": 0
                },
                {
                    "sent": "And they're not in the same class you train the classifier to get them in the same class, and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So you could see this as some form of a. Unsupervised transduction maybe?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The last function is a little bit similar to the Doctor Lim approach, but rather than using the Laplacian term we use here a hinge loss H. You could use any other classification based loss function.",
                    "label": 1
                },
                {
                    "sent": "We've used this one because it's nice because it has some sort of margin concept.",
                    "label": 0
                },
                {
                    "sent": "So we have a loss between F of X, which is the mapping of XI and C, which is the class towards which we think that we must push F of X.",
                    "label": 0
                },
                {
                    "sent": "And why is actually?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Huawei is actually the weight.",
                    "label": 0
                },
                {
                    "sent": "It encodes the weights to assigned to our point XI.",
                    "label": 0
                },
                {
                    "sent": "Being in cluster C. So for instance, if I&J are neighbors.",
                    "label": 1
                },
                {
                    "sent": "We take the sign of the sum of the two points.",
                    "label": 0
                },
                {
                    "sent": "So for instance, with a neural network you have an output between minus one and one.",
                    "label": 0
                },
                {
                    "sent": "So by taking the sign of those predictions you'll actually get the maximum output and you push the points towards class Class C with some positive learning rates.",
                    "label": 0
                },
                {
                    "sent": "If the points are neighbors you do you do the similar thing, but user, you basically train the classifier to push them apart using using some other learning rate.",
                    "label": 0
                },
                {
                    "sent": "And we optimize this by stochastic gradient descent, so it's it's a function that's easy to optimize this way and and it gives a really nice update rule.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically the algorithm which I call here, the end cut embedding algorithm, basically looks like that.",
                    "label": 0
                },
                {
                    "sent": "So you have some unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "You pick a random pair of neighbors from from your graph, you select the class to push towards doors.",
                    "label": 1
                },
                {
                    "sent": "Those points, which is the sign of the some of the predictions an if some balancing constraints are verified, I'll just talk about them on the next slide.",
                    "label": 1
                },
                {
                    "sent": "You do a gradient step to push them together.",
                    "label": 0
                },
                {
                    "sent": "Then you pick just some random pair an you do a gradient step to push them a, pull them apart.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about this algorithm is that you can really easily also get an extension to semi supervised learning, because if you have some labeled data you can also optimize a supervised hinge loss as a bonus, for instance at each iteration, so you get a really general framework for clustering or semi supervised learning tasks.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now when you have two class problem, it's really important to use some balancing constraints, because you could really easily get all your points into one class which you want to avoid.",
                    "label": 0
                },
                {
                    "sent": "So a really simple way to do that is keep the last predictions and in memory and if you've seen too much of one class saying the last 100 examples, you just ignore examples of that class for awhile.",
                    "label": 1
                },
                {
                    "sent": "This is called a cold.",
                    "label": 1
                },
                {
                    "sent": "Is the hard constraint, and you can also use a soft constraint.",
                    "label": 0
                },
                {
                    "sent": "That is, you just divide the learning rate by number of examples that you've seen of that class in the last 100 example, for instance.",
                    "label": 0
                },
                {
                    "sent": "So, so it's easier.",
                    "label": 0
                },
                {
                    "sent": "It's really easy to implement some constraints.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The framework is actually quite easy to to generalize to multi class settings or multi cluster settings, sorry.",
                    "label": 0
                },
                {
                    "sent": "For instance, you could.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "You could simply if you had.",
                    "label": 0
                },
                {
                    "sent": "If you have a classifier with multiple outputs, you can simply select the maximum outputs and push neighbors towards that maximum output.",
                    "label": 0
                },
                {
                    "sent": "Alternatively, you can also choose to use the mean label.",
                    "label": 0
                },
                {
                    "sent": "Another approach that we've tried is the all approach, that is, you have one learning rate per cluster, which is related to the value of that prediction for that cluster.",
                    "label": 1
                },
                {
                    "sent": "So in that case you can use for instance a softmax function of the ID output of the of the neural net, and you can obtain nice weights between between zero and one.",
                    "label": 1
                },
                {
                    "sent": "And you can use the same type of balancing constraints to two class problems.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, first we've tested this approach on some small scale datasets that are used in clustering and semi supervised learning literature.",
                    "label": 0
                },
                {
                    "sent": "Both two class in multiclass datasets, USPS text and G50C are widely used in semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "BCW and Glass serve UCI datasets and the Ellipse one is simply for 450 dimensional... connected through.",
                    "label": 0
                },
                {
                    "sent": "For one point.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I give two measures.",
                    "label": 0
                },
                {
                    "sent": "Actually there's the clustering error, which is, well, the the incorrect assignation of labels and the out of sample error which you could see as as a classification error.",
                    "label": 0
                },
                {
                    "sent": "It's here.",
                    "label": 0
                },
                {
                    "sent": "I've trained the neural network on some train training set and this is somehow the test error.",
                    "label": 0
                },
                {
                    "sent": "In order to assess if the out of sample clustering is as good as the in simple clustering.",
                    "label": 0
                },
                {
                    "sent": "And actually, well, it's compared to K means and spectral clustering using both are full RBF graph and a K nearest neighbor graph.",
                    "label": 0
                },
                {
                    "sent": "So actually we find that the method is quite competitive to spectral clustering, and one good surprise that we had is that in some cases it's even a little bit better, which in my opinion could be an effect of doing the optimization of clustering and embedding in one single algorithm rather than having this.",
                    "label": 0
                },
                {
                    "sent": "This two stage approach.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So these are the two class results.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the multiclass case the same, the same can be observed.",
                    "label": 0
                },
                {
                    "sent": "We have results that are compareable to the best results that can be obtained with spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "So you have the income embedding with the Max and all variants.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So it's the same here.",
                    "label": 0
                },
                {
                    "sent": "I've computed the clustering error in the out of sample error and there also have the same order of magnitude, which is, which is a good thing.",
                    "label": 0
                },
                {
                    "sent": "Next",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to get an idea of how fast it wasn't databases that are a bit larger.",
                    "label": 0
                },
                {
                    "sent": "Ava did some amnesty experiments, so here I've just plotted a subset of EM list with the passthrough Laplacian Eigen Maps.",
                    "label": 0
                },
                {
                    "sent": "Because people seldom do that, but it's quite instructive to see that it's it has a really nice cluster structure actually.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I've decided some amnist experiments with a varying number of clusters.",
                    "label": 0
                },
                {
                    "sent": "So here you have the 10 digits and I've fragmented them in 20 and 50 clusters.",
                    "label": 0
                },
                {
                    "sent": "In the case of the 10 cluster problem, this is done with a neural net with one hidden layer.",
                    "label": 0
                },
                {
                    "sent": "The results are really much better than K means.",
                    "label": 0
                },
                {
                    "sent": "Well, for scalability issues I didn't try it on spectral clustering, but.",
                    "label": 0
                },
                {
                    "sent": "With faster approaches for spectral clustering, doing a comparison would be an interesting.",
                    "label": 0
                },
                {
                    "sent": "Of course, the more you divide the problems into more clusters, the closer you get to K means, because cluster are easier to their closer to round structures, but still in all cases we get we get an error that's slightly better or much better than K means.",
                    "label": 0
                },
                {
                    "sent": "In the case of 10 clusters.",
                    "label": 0
                },
                {
                    "sent": "But the best thing about about this is that it's actually really fast.",
                    "label": 0
                },
                {
                    "sent": "It's been done on the normal normal computer and depending on it it can take from half an hour to 40 minutes, maybe for the endless database, which is, which is quite good and using stochastic gradient descent allows the method to scale linearly with the size of the data set.",
                    "label": 0
                },
                {
                    "sent": "So, so that's basically it.",
                    "label": 0
                },
                {
                    "sent": "Maybe some concluding remarks.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The op sorry, I just have a little remark to do about the training on pairs, which is usually considered as very slow, but something that can be done in case that you have some specific prior knowledge about your problem is that you can assign.",
                    "label": 1
                },
                {
                    "sent": "And adds a positive edge 2, two neighbor, two data points based on prior knowledge like for video and audio mining, consecutive frames can be labeled as neighbors directly, or in the case of text to words that are close in, your text can be assigned the same topic.",
                    "label": 0
                },
                {
                    "sent": "The same.",
                    "label": 1
                },
                {
                    "sent": "For web mining you can use link information, so of course the computation of neighbors is always the bottleneck, but in most cases you have, you have little tricks that you can apply to speed that up.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary.",
                    "label": 0
                },
                {
                    "sent": "The concluding remarks would be that the functional embedding for clustering allows a faster clustering and avoids the out of sample problem.",
                    "label": 0
                },
                {
                    "sent": "We found that to some extent the joint optimization of clustering and embedding provides good or at least better or at least similar results to existing methods and that we have a scalable and flexible framework with this type of algorithm that can be.",
                    "label": 1
                },
                {
                    "sent": "Easily generalized for instance to semi supervised learning, so that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "To.",
                    "label": 0
                }
            ]
        }
    }
}