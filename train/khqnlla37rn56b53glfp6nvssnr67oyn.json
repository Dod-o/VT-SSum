{
    "id": "khqnlla37rn56b53glfp6nvssnr67oyn",
    "title": "Deep Learning for Distribution Estimation",
    "info": {
        "author": [
            "Hugo Larochelle, D\u00e9partement d'informatique, Universit\u00e9 de Sherbrooke"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_larochelle_deep_learning/",
    "segmentation": [
        [
            "Welcome back.",
            "So I guess now at this point you've seen pretty much like the basic that you'll need to read 12 into some of the more recent advances in in deep learning, and I'm going to touch upon this subject, which is related unsupervised learning.",
            "It essentially concerns developments are fairly recent.",
            "But yeah, so I think now you have a really nice place where you known about the basic things which will probably pop up during the rest of the summer school.",
            "You've got a good basis, unlike what the first 2 days might suggest.",
            "French speaking is not requirement for deep learning based on the speaker list it's turned.",
            "You can do deeper.",
            "Don't worry, you can do deep learning with speak French.",
            "So now we gotta talk about instead of talking about supervised learning, which has been the main topic so far, I'm going to talk more about unsupervised learning and I'm going to talk also about how we can leverage unsupervised learning methods.",
            "Context of a supervised learning problem.",
            "So this is I'm going to touch a lot of work in collaborations with people that you see here in Murray Ben urea engine and stanislaw slowly Matthew John may are some of my students and Carol Gregor as well."
        ],
        [
            "Alright, so specifically the problem that I'm going to discuss talk about is it probably distribution estimation that is, given a training set of examples are not labeled so a set of X.",
            "So I set of inputs.",
            "The goal is to try to produce a nestum it of the true distribution P of X from which the samples come from.",
            "OK, so this is a very complicated.",
            "This very hard problem and also quite fundamental.",
            "There are a lot of different tasks that.",
            "If you have a good distribution estimator, you can use it to solve their classification problem.",
            "It's a very hard problem and particularly becomes particularly hard when the dimensionality of the observation increases.",
            "So what you have to do here is you have to output essentially function P of X that gives you what's the probability that I would see this sample from from that population for that distribution, and So what it means is that you have to.",
            "Produce the correct output for any potential value of X, and that's different from classification, because usually if you're say, classifying digits well, you already expect that the input is going to be a digit.",
            "You just want to know is it a zero or one or two or three.",
            "Here for any value including a random vector presented as input, you need to produce a proper value for it, so that I think is one of the fundamental reason why this is a hard problem.",
            "As I said, it's very general, so if X was.",
            "Partly whimpered, but also a target.",
            "Then if I have the joint distribution over the two I could essentially use.",
            "I could just condition get the conditional of Y given X, and then I'd get a classifier.",
            "OK, so in that respect it's a very general problem from which you can solve other specific problems.",
            "It can be a good proxy for feature learning and can provide regularization cues for another model.",
            "So when I was talking about for instance, unsupervised pretraining well, something like this distribution estimation.",
            "Can deal yield algorithms that might give good neutralization for a supervised classifier so as initialization before fine tuning.",
            "Actually, we'll see how I use the specific algorithm that we that I developed in the past years to do exactly that.",
            "It can be used also if you have observations where you have missing inputs, you could use a model like this to fill in the missing inputs.",
            "If you want to feed that to a classifier that doesn't deal with missing inputs, you can simulate new examples.",
            "There a lot of application for a model that's a distribution estimator."
        ],
        [
            "Alright, So what I'll talk about is 2 models on the last one.",
            "If I have time, I'll talk about it.",
            "Maybe I'll skip it, depending on time.",
            "Again, you know, don't hesitate to ask questions the whole time.",
            "There's really no problem.",
            "So first of all, I'm going to mostly talk about is the note or aggressive distribution estimator ornate, and I'm going to start by talking about this model in the context of modeling binary vectors, and then I'll show how you can generalize to continue real valued observations, multinomial.",
            "And then talk about the diversion of Nate and if I have time I'll talk about another variant that uses the same idea of autoregression, but that's closer in spirit to regular autoencoders, which we've seen today."
        ],
        [
            "OK, so just to make it clear what I'm talking about when I'm talking about distribution estimation, I'll just give you 2 examples of models that are distribution estimators that you might know about.",
            "A mixture model is an example of distribution estimator, so in a mixture model you assume that your data point.",
            "A priori belongs to a certain number of C classes, but that you don't observe, and for each class you have a generative model that says, well, given that a sample is from a given class, here's I was generated, or here is the distribution P of X given to which class the input belongs.",
            "That could give you, you know, the likelihood of different inputs belonging to being generated by that class, and so to get a P of X.",
            "Since we don't observe the class which.",
            "The input belongs because we're in an unsupervised learning setting.",
            "We just sum each of these components, weighted by the probability of a point belonging to that class.",
            "So that's a mixture model.",
            "Gaussian mixture model is an example which you might know when people use quite a bit.",
            "So it's a simple model, well understood, usually training with the EM algorithm button context.",
            "The performance can actually be disappointing that specifically truthful binary data, as we'll probably see later on.",
            "So it belongs to a larger class of graphical models.",
            "I think you'll learn more about this, probably tomorrow that."
        ],
        [
            "Type of model.",
            "I'm going to focus more on.",
            "And I'm going to extend and use neural Nets in that framework.",
            "Is what I call fully visible Bayesian networks.",
            "So the difference here is that in."
        ],
        [
            "Say a mixture model.",
            "Other types of graphical models.",
            "There's usually the notion of a stochastic latent variable, and if we want to compute the probability of some input, we have to integrate it out.",
            "And when we do inference, we have to infer posterior over that latent space.",
            "So in this case the little space is very simple, it's just a partition is essentially a choice between one up to C classes."
        ],
        [
            "A free visible Bayesian network doesn't explicitly have this notion of a latent variable.",
            "All it does is that it assumes 1st that the distribution over my input space P of X is the product of the probability of the distribution over the KF dimension.",
            "Given the value taken by each of the previous dimensions up to K -- 1.",
            "So that symbol here, that's the sub vector from dimension one up two K -- 1.",
            "That's what I mean by their smaller than K as an index here.",
            "So actually this is not an assumption.",
            "This is true for any distribution.",
            "Any distribution I can decompose in this way, but these conditionals for certain models will have a simple form and others will have a complicated form that perhaps we can't even compute OK, but that's the starting point for models that are based on the idea of visible Bayesian networks, and so if we were to illustrate that as a graphical model, it would correspond to this.",
            "So these arrows sort of illustrate the dependencies structure.",
            "Between the inputs and, it's essentially also illustrating what is what we call the generative story behind this model.",
            "So mixture model the generative story is that first before you generated an ex, you chose OK from what class am I going to generate an input four and so you picked out according to that probability, probability over whoops probability over classes, and then once you pick that where you're going to generate from the conditional corresponding to that class, that's the generative story for mixture model.",
            "For fully visible Bayesian network, the assumption as to how data was generated is that from left to right.",
            "Let's say for left to right.",
            "For now I'm first going to sample X from P of X1, the first the first dimension of my vector.",
            "I'm going to sample it from P of X1 and I'm given that simple and going to generate the value for the second dimension conditioning on X1 because the arrow the directed arrow signifies that.",
            "I've characterized X2 given X one and then the third dimension is going to be generated based on the value of both the second dimension and the first.",
            "Because I have directed connections from the first and second dimension to the third and so on.",
            "So essentially all dimensions in the input space are generated sequentially in some order, let's say left to right.",
            "To make things simple.",
            "So an example of a model that follows in this more general family, fully visible Bayesian networks, is the fully visible sigmoid belief network in this case.",
            "It's simply corresponds to choosing that these conditional.",
            "So again, we're assuming that everything is binary.",
            "So when I want to specify the distribution of the KF dimension, given the previous dimensions, all I need to provide is what's the probability of being equal to 1 and the probability of being equal to 0 is going to be 1 minus that, so once I give this P value, I've specified the full conditional distribution an in the fully visible segment belief net.",
            "The alleged a logistic progressor model is used to model this conditional.",
            "So in other words, that probability is I take the vector, the sub vector from dimension 1 to K -- 1 and multiplied that by a weight vector plus a bias pass this through sigmoid.",
            "That's my probability that XK is equal to 1 in that model, and so each conditional has its own magic regression vector, an that's all joint distribution is being modeled.",
            "It's going to be the product of each of these conditionals from left to right.",
            "So as a shorthand values X hat K for what is P of XK give equal to 1 given the previous subvector subvector?",
            "Because you can sort of think of it as a reconstruction, so from the previous dimensions you're trying to reproduce forgiven.",
            "Example, the would like to assign high probability anyways to the actual observed value of X.",
            "So it's kind of like a reconstruction, but it's sort of a predictive reconstruction because you're not conditioning on SK only conditioning on the previous dimensions.",
            "And if you want to view it as a flow graph, that's kind of like your neural network.",
            "What it would look like something like this.",
            "You would have an output layer directly connected with the input layer an you have this left or right connectivity here where this input is not connected to itself at the output, but only to the.",
            "Left the sorry the all the components at the right in the output layer and same thing with this one and so on.",
            "OK, so that's just another visualization which here exposes more directly how we para tries this fully visible simulate belief net.",
            "Other questions about this.",
            "OK so.",
            "Just to summarize, in this setting we assume the data is generated one dimension after another in some given order, and for fully visible similar belief net, we're using logistic regression models for each of these conditionals we have a different logistic regressor for each dimension, and we can train that by gradient descent, optimizing the log likelihood on your training data.",
            "So this is this should be a fairly weak model because these conditional is log linear and linear is about the weakest model.",
            "Can be turns out for a lot of data.",
            "It's actually surprisingly good in some cases.",
            "On the binarized version of Emnace, which I'm going to talk about later, is actually better than the mixture model an this sort of inspired me to say, OK, well can we take this model?",
            "Just make it more powerful and we stick unknown net in there because that's what I do as a PhD student.",
            "I.",
            "Stick neural Nets where they had been been neural Nets before so.",
            "Let's try to do that.",
            "So he."
        ],
        [
            "Here's what I came up with.",
            "Essentially, we give up this model we call Neid.",
            "Neid is a model where, which is sort of like a note on quarter.",
            "Maybe talk a bit more about why we can think of it this way.",
            "And in words, the idea is that each of these conditionals in that productive conditional alpera tries it as a neural net, but it's always going to be essentially the same neural network, and in particular the connections with the hidden layers are going to be shared across conditionals.",
            "So I have this inference machine if you want this neural net that takes a subvector an outputs, what's the probability for the next dimension in that sequence of inputs?",
            "OK, so this means that the value of the hidden layer.",
            "For predicting the Keith Dimension dimension, sorry.",
            "So this is not an index for the depth.",
            "It's more an index for this is the hidden layer to predict which dimension, which conditional.",
            "That's going to be so.",
            "In this case we pick the sigmoid of a linear transformation of this sub vector containing all the dimensions from 1 to K -- 1 and I'm going to take that vector.",
            "An use a linear transformation specific to the conditional.",
            "I'm trying to train, so this part here is essentially logistic regressor, but that takes as input this hidden layer.",
            "Pass this through sigmoid and that's going to give me my probability that XK is equal to 1.",
            "OK, so I'll just go through the generative story by sort of going through the flow graph here.",
            "So if I want to generate a vector X using this model, I'd start by computing H1, which is the hidden layer for the.",
            "First conditional, which usually the marginal, it's P of X1, so this is not dependent on any input.",
            "So essentially this part here is 0, so it's just the simulator bias.",
            "It's kind of over parameterized really.",
            "And then I take that I multiply it by its by the output weights for the first dimension V1.",
            "So that's essentially the 1st.",
            "So for the first conditional, this would be a one, so it must be the first row of my matrix V plus my scalar bias for the first dimension by the sigmoid.",
            "That gives me the X at one, so the probability that X one is equal to 1.",
            "Then, so that's a probability between zero and one.",
            "I flip a coin with that probability.",
            "It that becomes my new input to the name model.",
            "The 01 stochastic choice, and then from that I compute the hidden layer for the second conditional, which now depends on what I filled in, filled in here OK, and so it's going to be essentially just the column vector of W corresponding to the first dimension time 001 plus the bias vector pass through non linearity.",
            "Say the sigmoid.",
            "And then again I applied the output weights for the second dimension, so this would be so you'd have a 2 here now and this would be a 2.",
            "Not very good drawing with my mouse, sorry.",
            "And so you get that probability you sample from it.",
            "And that becomes the second dimension that you put in input, and then you compute the third hidden layer for the third conditional and you continue like this and notice that and this is what I've indicated with this little drawing here with the blue dots.",
            "What this means is that this set of connections and this set of connections here are the same vectors, the same set of parameters it's reused across the hidden layer, and that's what I mean by it's the same neural net for each conditional.",
            "There's this weight sharing across the conditionals in the level of the hidden layers.",
            "And actually, we can leverage this to make the computation of all these hidden layers fairly efficient.",
            "So normally if I was to compute say that hidden layer, I would need to take all of this sub vector, multiply it by the submatrix which is almost the whole matrix, and then apply my sigmoid.",
            "But since I since I've already computed once, I'm here assuming computer activation here.",
            "Well, I've had most of the job that's done, I just need to add the contribution that comes from this new dimension so I can exploit the fact essentially that the.",
            "Pre activation at adjacent positions, adjacent conditionals is just the contribution from the new dimension, so I just have a vector to add which is the column vector of matrix W times.",
            "What is the actual sample and I can pre essentially reuse their pre activation so that makes it kind of like a recurrent neural net for those that know about it.",
            "But it's it's linear before the activation function and it has this property that computations are fairly efficient.",
            "So anyways, that's the generative model.",
            "I would continue like this.",
            "Sampling all all the dimensions by getting each of my outputs and you can see it's sort of a little tone color because the way I'm going to train it is that for a given example X.",
            "So if I observe a sample X and I want to increase its likelihood, well, what I'll be doing is that I will increase the likelihood of each dimension given the previous one.",
            "So if you don't really know what's going on here, it's like you have an input which is X.",
            "And you have an output with a vector of probability and trying to make that output match the input.",
            "So it's sort of like an annoying quarter, but it's been structured.",
            "It's been tortured in a way that you can interpret the output as a series of conditionals, from which you can sample.",
            "So we talked about weather, like denoising autoencoders were turn colors, were generative models, note encoder, not really.",
            "But this definitely is.",
            "It has a generative story behind it, and it's a proper distribution.",
            "OK, so that was kind of the interesting thing about this.",
            "Try to see.",
            "We can get with this neural net generative model that's just feedforward no latent variables.",
            "It's based fully fully visible graphical model on air right now is a good time to ask questions about this, because this is like most of the rest of the talk really.",
            "So yes.",
            "Sure.",
            "I so I think you get something in generalization an I'll try to talk about this when I talk about another model later on which so that you explored in the 2000s and then we re explored recently and what we found is that this sometimes tend to overfit less because of the weight sharing and the weight sharing sort of comes from an RBM, and since we haven't seen respectable machines, I didn't dig into that.",
            "It's described in the paper, so once you.",
            "Become experts in our BMS.",
            "You can go look at the innate paper and you'll see what's the relationship between the two.",
            "I'm going to yes.",
            "Yeah, I'll talk about that.",
            "So the question, I would decide the order.",
            "I'll talk about it and next slide.",
            "I think you're not going to like the answer.",
            "So there are none would be like.",
            "So the question is what's the difference with an RN ends?",
            "As I said, it's similar and all of them would have connections directly between this nonlinear hidden layer and the next nonlinear hidden there, whereas here you have this sort of trailing.",
            "The recursion is before the pre activation, before the activation.",
            "I mean, so it's really linear recursion, so that's the main difference I guess.",
            "Other questions yes.",
            "Yeah, so I guess you can think of it as a just in general a state space model to some extent.",
            "But here, unlike an HMM generalized, this actual notion of the stochastic latent state here there's no interpretation of these hidden units as being stochastic.",
            "There're feedforward hidden units that are deterministic, but to some extent, like most sort of sequential models.",
            "Have shared something in that their sequential.",
            "There's a latent state and you progress, and as you see as you get observations, you update your latent state, broadly speaking.",
            "Other questions.",
            "Alright, so."
        ],
        [
            "How do you say it well?",
            "Since it's a generative Model 1 natural thing to do is to maximize the likelihood of the training example.",
            "So optimize the, let's say, optimize the average of the log probability, or minimize so that we can maximize.",
            "This will minimize minus that.",
            "Because P of X is the product of emotional and then you have, so you would have a log of a product so that becomes that can become a sum of the log of each conditional and so here you really see this sort of predictive autoencoder nature, where each dimension K is predicted from the input.",
            "But we made sure that we're predicting a dimension we are predicting it from all the inputs before case, so there's no direct connection.",
            "There's no sort of information flowing from SK to its probability.",
            "And that's essentially why it's a proper generative model, so we can compute this loss function exactly.",
            "There's no approximations, unlike things like.",
            "PBM's, for instance, as you'll see, computations are linear in both the number of hidden units and the number of input dimensions.",
            "Because you get a deterministic loss function.",
            "Then you could use or it's it's definitely easier to use 2nd order optimisers, though.",
            "I mean there's not.",
            "They're not used a whole lot.",
            "I guess it seems like stochastic, especially on large data set, but that would be an option if we ever made even more progress in 2nd order optimizers.",
            "As we see there is an extendable to a bunch of different observations.",
            "So so yeah, so the question that we had is what order should we use for the inputs you have to specify some sort of ordering and it turns out that in unfortunately in many, as in many things random ordering works fine.",
            "So I have some results about this.",
            "But essentially we tried a bunch of different random orderings and the variation in the performance was very small, small compared to the uncertainty we had about the test error.",
            "So it doesn't mean that there wouldn't be benefits to optimizing it, but it definitely seems like if there is a benefit, it might not be.",
            "You know that substantial.",
            "So I have found that on images doing left to right and then top down traversal of the pixels did on some image dataset give better results than doing random ordering.",
            "But it's not like.",
            "If you don't do this, it doesn't work, or sorry.",
            "Anyways, if you use a random ordering, it still works.",
            "I guess what I mean to say.",
            "I saw a hand in the yeah.",
            "Well, so that's a good question that I think one reason it is fairly robust.",
            "And by the way, the fully visible Sigma belief net which is using logistic regressors, they've also found that optimizing the ordering wasn't that useful.",
            "So that seems to be a fairly general statement in the case of using neural Nets, because they essentially like there, there is an ordering which might yield conditionals which are more complicated functions than they could be.",
            "But if you haven't ruled that that has enough capacity, theoretically you could still model that fairly complicated function.",
            "Maybe so I think maybe that's partly the reason why it doesn't matter so much the ordering.",
            "Otherwise I don't have, unless, like you knew what was the order in which things were generated.",
            "Then I'd exploit that, but I think this is, yeah, I don't have really strong understanding otherwise.",
            "Yes.",
            "You mean so that I explained just now or?",
            "So I made a statement about distribution estimation being really hard in high dimensions.",
            "Is that what you're referring to?",
            "So so yeah, so the idea here.",
            "So the idea is that when you are learning a distribution estimator, your P of X needs to be good everywhere in this space.",
            "So if you assign, that's because it's a distribution that's normalized.",
            "So if you assign high probability, so say you're learning the distribution of images of digits, but you're assigning high probability to a bunch of random images while you losing probability there because it needs to all sum to one.",
            "So it means that if you had actually pushed down the probability of random images.",
            "You would do better for the probability of real digits, so that's in that respect that it needs to essentially have a good output everywhere, unlike classification.",
            "So you definitely have to model.",
            "So the question is whether in high dimension, isn't this a problem?",
            "Because you'll have essentially have that a lot of hidden layers, in particular, the computations are linear in a number in the dimensionality D, so I think being linear in the dimensionality is not so bad if it was quadratic in the dimensionality, then this would not really scale.",
            "As well.",
            "So I mean, it's in that context being linear in dimensionality.",
            "That's pretty much as good as it gets.",
            "Another question.",
            "I think that's part of it.",
            "Yeah, I mean, that was definitely appealing thing.",
            "Like you, you get a grain that's exact.",
            "Yeah, the question is whether the main appeal really for fully visible Bayesian network is the fact that there is no Lenten variable, and that's my conception of it.",
            "Yeah, because ultimately otherwise it does provide.",
            "It does correspond to a generative model, which is a bit awkward in the sense that you don't truly believe that this image was generated one pixel at a time, so you know the generative story doesn't really work, but still the basic assumption, which is that P of X is decomposable as a product conditional.",
            "Is true and if you put enough capacity to each conditional, then perhaps you're not making so assumptions are that strong, but so in you know, with the benefit of not having latent variables, which in terms of the algorithms, means that we have deterministic gradient and efficiency.",
            "That's great, but the generative model itself is a bit awkward.",
            "Alright, so."
        ],
        [
            "And so here what I'm showing is results on the benchmark from datasets I took from UCI.",
            "They're all binary datasets, so binary observations.",
            "I'm not going to go through each row.",
            "Essentially what we did is that, so I guess the first row is the mixture of Bernoulli distribution productive Burnley, so it's the mixture model and what we did is that we're showing here the lock probability of the test set and then we subtracted.",
            "The log probability of the mixture bernali to each.",
            "So for this data set we took its actual log probability and we subtracted that to all of them.",
            "That's why it's always zero for the mixture of Bernoulli.",
            "This ought to have like a sort of basis of comparison, since lock probabilities in absolute terms can vary a lot depending on the nationality of the input, and so on.",
            "An I mean, the main message here.",
            "So here we have fully visible segment belief net, and here you have name and the main takeaway is that most of the bold entries are in this role.",
            "Like it in general, it does really well.",
            "It tends to outperform the other alternatives.",
            "This these here are essentially undirected graphical models based on the PBM and variations of it.",
            "But there are small PBM such that this is not going to make sense for some of you.",
            "I'll see it you this is smaller games where you can still compute the partition function you'll.",
            "This will make more sense if you remember it tomorrow when Aaron talks about undirected graphical models.",
            "So and the other experiment, the other experiment here is what we did for these three datasets we computed again this performance, but using a different random ordering.",
            "So just to be clear, the random ordering before you do any training is determined.",
            "A priority and it's not changed during training.",
            "It's always the same fixed ordering.",
            "Now what we've done here is we, I think, took five different random ordering train made under these five different orderings.",
            "Optimizing the hyperparameters for each.",
            "And we look at the performance on the test set.",
            "And as you can see, the variation is very small, so that's what I meant by random ordering.",
            "Works fine.",
            "It works fine because it's doing better than most other models.",
            "And also as you change the ordering still randomly, you will not really gain by optimizing that.",
            "Not a whole lot anyways.",
            "I saw a hand, yes.",
            "So they are, well, we treated that as like each dimension is a boundary observation.",
            "I think if you actually some of these datasets, actually there are like 1 hot vectors which we ignored entirely, which means you can do even better than that by taking that into account, their model, any of these models really don't take that into account.",
            "OK, so they would sort implicitly have to learn this one hot encoding constraint.",
            "Alright."
        ],
        [
            "And this is results on the binarized.",
            "In this data set.",
            "So this is the end.",
            "This data set of handwritten digits where we've taken each pixel.",
            "We normalize the intensity between zero and one, and we obtain the binary value by sampling from that intensity normalized intensity as if it was a probability of being equal to 1 OK, and so that gives digits that are bit noisy.",
            "This is samples from a train name.",
            "You can see that for a lot of cases you do recognize a particular digit.",
            "But it's not a perfect model an I think during the rest of this week will probably see models that are better than that.",
            "But what was really interesting when we publish this is that if we compare it to the best result which came from an restricted Boltzmann machine, this in approximation because of this partition function I talked about which you have to approximate, which is not the case for Nate.",
            "This is the true log likelihood.",
            "There are no approximations here.",
            "It was really close and I think at the time we were sort of this.",
            "I guess common belief that in order to achieve really good generative models, it could be that this.",
            "Intractable normalization constant was kind of unavoidable.",
            "You kinda needed it to get good luck like you were so good generative models and that sort of went against.",
            "I guess this this belief because this was very, very close to that result here.",
            "So we're really happy with this and that sort of picked my interest to pursue this even more later."
        ],
        [
            "Alright, and this is, well, that's fine.",
            "I guess this is the instead of the samples.",
            "I'm showing the probability, so now they are Gray level values where you can see even more what digit might have been."
        ],
        [
            "Alright.",
            "So that's the basic model and I will extend in a few different ways in a few different directions.",
            "So first I'll talk about how to extend it to real valued observations, because in most cases really your data is not binary, so it would be nice to have a real valued version of Nate."
        ],
        [
            "This was with Benny Area and Murray.",
            "And what I like.",
            "So by the way is it?",
            "Is it easier to see my slides now?",
            "I used a white background based on the suggestion by someone.",
            "Here.",
            "Is that better now or seeing some yeses but not a whole lot.",
            "Hopefully it's better.",
            "So essentially, if you want to model other types of inputs well, all we need to do is change the nature of these conditional distributions.",
            "So if XK is real valued, will have P of X KB, A distribution for real valued observations.",
            "For instance, it could be a Gaussian mixture.",
            "Each conditional could themselves be Gaussian mixture, which means that we would want the neural net given a subvector to output the.",
            "Mean value of each Gaussian, the standard deviation, or the variance and the mixing weights between the different Gaussians.",
            "You might think I'm wouldn't be simpler to just have a single Gaussian and just predict it's mean and its standard deviation.",
            "It would be simpler, but it probably wouldn't be appropriate.",
            "In particular, if you think about the distribution for the first few dimensions, those very big, fairly important reasons to think that these are probably going to be multimodal distributions where you have multiple modes around different values in terms of pixels for the first dimension, you'll probably have a big Gaussian like bump around an activation of 0 or.",
            "Intensity at zero and intensity of 1.",
            "If you normalized everything.",
            "If we used, you know more or less binary images, and in general that's just through the for the first conditionals, you just observe that they tend to be multi models.",
            "Also using a mixture means that we can approximate a fair number amount of different distribution of different shapes by placing the Gaussians in different positions and playing with how what's the mixing weights of each and what's the variance of each.",
            "So in this case I will see illustration where there was essentially be 3 Gaussians, and so you would predict each of these means and then you would also predict the variance of each and also the mixing weight.",
            "So now for each conditional you have to output.",
            "If you have mixture of 10 Gaussians you have to output 30 numbers, third 10 means 10 standard deviations and then mixing weights to make sure the standard deviations are positive.",
            "We just passed this through a next potential.",
            "Non linearity.",
            "That means we're guaranteed that as standard deviations should be it's positive and to get the mixing weights that sum to one we pass the pre activation through a softmax.",
            "And for the means, we just use a linear transformation from the hidden layer."
        ],
        [
            "So I'll just skip."
        ],
        [
            "Over that an so he are results.",
            "This is again a UCI datasets were comparing with a mixture of Gaussian and make sure factor analyzers which are essentially Gaussians that are mixture of Gaussians are better regularised and we were able to get.",
            "So what you're seeing here is the average log probabilities, so you get higher numbers here for the real value Nate with mixture of Gaussians."
        ],
        [
            "We looked at modeling natural image patches, so we take a large photograph of a natural scene and then we extract from its small patches from anywhere and it becomes our training set and maybe we take two different images and have a training, validation and also another image for the test set.",
            "And here we are comparing with a mixture of Gaussian.",
            "This comes from a paper where they did a really, really good job at carefully training a mixture of Gaussian and we were fairly competitive.",
            "Though we didn't outperform them, were fairly close."
        ],
        [
            "And this is for spectrograms of speech signal, where again we got good test log compared to a mixture of Gaussian."
        ],
        [
            "Alright, let's move to another type of observation.",
            "Multinomial observations.",
            "So, example of multinomial observation would be in natural language processing a sequence of words.",
            "So normally the way we represent text data, the vast majority of ways that models will manipulate text data in machine learning is that you would first agree on a vocabulary that is a sequence of words.",
            "Often fairly large and 10s of thousands or hundreds of thousands or even more, and your series of word.",
            "What you'll do is that you essentially replace the word by what is the position of that word in the vocabulary.",
            "So you replace each you sensually have a mapping from his string to an integer from one to the number of words in the vocabulary.",
            "Often we add a word which is the out of vocabulary word that is for any other word which does not belong to my original set of words, I'll map them to the same integer.",
            "OK, so now what we have really for modeling a stream of text is a series of these integers, and multinomial distribution would be appropriate for modeling such integers."
        ],
        [
            "So again, what do we do?",
            "Well, we just change how we deal with taking the value of a hidden layer for conditional and transforming that into an output distribution for that dimension.",
            "And so one thing we could do is just use a softmax.",
            "We could take the hidden layer, multiply this by matrix passes through softmax.",
            "This gives us the probability of assigning of sampling each of the different words in the vocabulary.",
            "If you have a very large vocabulary that computation, so essentially computing, say 100,000 different probabilities for.",
            "Each position could be very expensive, so here we use a different approach which has been used in for language modeling.",
            "For instance, you'll probably.",
            "This will probably come back when you get lectures on natural language, but I'll describe it here since I sort of need it here and what we did instead is to get more efficient computation of the probability of the next word given the previous ones we've essentially used."
        ],
        [
            "What's known as a tree structured multinomial.",
            "So here it works.",
            "So imagine I have a conditional where I've observed the previous words in some stream, so I'll explain later why we take it in random order.",
            "But we have N dog the the end cap, and for some reason all my examples involve cats.",
            "Not quite sure why, but I don't even like cats.",
            "I don't know what that is about, so.",
            "Here you would have a hidden layer which essentially takes that, as in putting computer hidden layer and now what I want.",
            "Imagine the next word is cat.",
            "OK now I want to compute this probability under my model, but I don't want to compute the full softmax.",
            "So one approach is to decompose these probabilities under a tree and as we'll see this is a trick that will allow us to compute probabilities under that tree in log arhythmic times where it's log rhythmic in the number.",
            "Of words.",
            "So what I do is I take all my words in my vocabulary.",
            "I've conveniently chosen a vocabulary of size 8 which fits comfortably my slides, and now what I want is from my hidden layer.",
            "I want my output matrix to essentially predict what's the probability of, say, turning left or turning right.",
            "So you pick one option.",
            "It's either turning left or turning right.",
            "Don't quite remember which one I chose in the slides, will discover that together in a few minutes.",
            "And then in this case, the probability of a given word is going to be what's the probability of starting here and reaching cat.",
            "For our example here.",
            "So that's the probability of going left, right and then right.",
            "OK, so specifically.",
            "The probability of cap under this model would only involve using the connections with these three internal nodes, and none of the other connections in my output layer, so they involve only three as opposed to 8 OK, so that's how it becomes like a rhythmic because I've used a tree which is balanced and binary.",
            "If I'm sorry.",
            "Yes, the question is yes.",
            "This corresponds to a sort of binary encoding.",
            "So OK, so this is related to, I guess the general question of how do you get this tree?",
            "Essentially in our experiments random was being so friendly with us that we use a random tree random balance tree where words are just assigned randomly to leaves in that binary tree.",
            "You could do something better and computationally by using a tree that's based on Huffman tree, which is based on the marginal probability of each word, so that you have a smaller path forwards that are more frequent.",
            "And longer paths for words are less frequent, and that would be much more efficient.",
            "But we here we used, which essentially the worst case, which is a binary assignment of words to treat two leaves, yes.",
            "Yeah yes, so would it be better to have some sort of semantic structure that direct how we construct the tree?",
            "Yes, and there is definitely experiments that showed, at least in the context of language modeling, that you get better results, better reflect cities we play with that a little bit with this model, which I haven't finished explaining, but which is more topic model and we didn't get much success.",
            "But I think it's because we're not doing language modeling here, but we're actually doing topic modeling, but I haven't addressed that yet.",
            "That's coming up.",
            "So finishing my example, I want these probabilities, but I'm parrot rising the probability of turning right.",
            "As it turns out, and so the probably turning left is just 1 -- 2.",
            "Probably turning right and I'll just model those as just the sigmoid on the linear transformation associated with that internal node.",
            "That's how I get a distribution over an items an words, but where I can compute the probability of a given word in log arhythmic time?",
            "OK, so that makes this algorithm since I have a lot of conditionals to evaluate, it's going to make it much more efficient."
        ],
        [
            "So yes, so we're going to use it in the context of essentially topic modeling.",
            "That is, we have a bag of words where we don't have the ordering of the words, mainly because we are only interested in the general semantics behind this document.",
            "We're not interested in modeling the syntax, so which mainly dominates the order of the words.",
            "So essentially we have a sequence of words that we've shuffled in random ordering, and then we're trying to predict them sequentially based on the previous word in that random stream.",
            "And what it means is that the best thing you can do is just figure out after seeing a few words.",
            "Oh, this document seems to be about politics, so it seems to be about support.",
            "So it seems to be about so essentially the best you can do really with a model like this is extract hidden units that are topical that tend to reflect the general topic of discourse.",
            "And because it's also a variable number of words, unlike before, I'm going to have more parameter sharing.",
            "So now the connections between a word and the hidden units is not going to depend on the position where it is because anyways, there's a variable number of words, so I couldn't have one vector for each position because the position is unbounded and also the order doesn't matter.",
            "The position in this ordering doesn't matter 'cause it was random, so we share all of these weights.",
            "And also we always use the same output weights, no matter which conditional we're using."
        ],
        [
            "Yeah, exactly yes.",
            "And then we're converting it into a random stream of words by picking without replacement from that bag of word.",
            "OK, we actually shuffled the word order at every update and in terms."
        ],
        [
            "Evaluation we looked at things like the perplexity.",
            "The perplexity is essentially the exponential of the negative average luck probability, and it's actually here normalized by the size of documents, and I'm comparing with a few models or LD is a very popular topic.",
            "Model is inspired by the PBM and sort of a topic model version of the PBM, and this is document made our new model or getting so lower is better here, and we got, frankly quite surprising low perplexity results."
        ],
        [
            "We can use.",
            "We can essentially construct a new representation for a document by passing it all the words and looking at what is the value of the hidden layer given all of these words and we can use that to do some retrieval in it, assuming that this representation is going to correlate with semantic similarity and we compare with another model which is again the replicated softmax with state of the art at the time we did a bit better."
        ],
        [
            "One thing we can do also is that so in this model I essentially have a matrix that is the number of hidden units times the number of words in my vocabulary, and when I'm observing a word in my sequence, what I'm taking.",
            "What I'm doing essentially is that I'm taking one of the corresponding columns of that matrix, and I'm adding it into the pre activation vector and also what I have is that for each hidden unit I have a notion of connectivity with all of the words.",
            "OK, so for one roll of that matrix can essentially be used to identify which are the words, which tend to increase the activation of that units.",
            "Those will be the words with the hyest positive connectivity.",
            "The highest weight, and that's essentially what I'm used showing here.",
            "So for three hidden units.",
            "So before we looked at what are the words that have the so we sorted the words based on in decreasing order of their connection weight with a hidden unit and we're showing you the top 10 here.",
            "And you see that we have a hidden it, which seems to be more sensible and seems to be detecting realigion topic here.",
            "This more science sports security, so it has really picked up.",
            "These semantic topics from these documents, even though you know there was no labeled data for that.",
            "So that's when we look at the yes.",
            "I don't know.",
            "That's a good I. Yeah, that's maybe there's a back story to this.",
            "Probably not Clipper though.",
            "So they're not perfect.",
            "Oh, is that right?",
            "Maybe there's a connection with that's The funny thing.",
            "Sometimes with neural Nets they actually discover things you don't know about, so I don't know.",
            "That's a good question, maybe.",
            "Maybe there is a connection."
        ],
        [
            "Say what?",
            "OK.",
            "So OK, so this is observing.",
            "This is inspecting the rows of that matrix.",
            "Now what I can do also is for one word I can look at the vector column vector associated with it, which is the vector of its weights with all the hidden units.",
            "And I could think OK this is going to be my vector representation for that word.",
            "And now what I could do is I take that vector representation for word like say weapons.",
            "I'm going to look at where are the other words whose representation is closest.",
            "And here I'm showing the five nearest neighbors and for a few words again.",
            "There's some high level semantic similarity that seems to have been learned.",
            "Weapons, weapon shooting, firearms and so on.",
            "Windows, Dos, Microsoft, and so on.",
            "I'll skip over that."
        ],
        [
            "So another experiment we did is, and now we're going to start about how we can use in unsupervised learning method to regularize and improve the generalization performance of a supervised model.",
            "So in this case here, what will do is that we'll assume we have documents, but we also have labels with them that we can use.",
            "And so now we're going to have a joint model over both X&Y and Argentum model.",
            "What it does is that it generates all of X and then once it has all of X it generates why which is going to be a classification label, and we do this just by adding a final hidden layer which uses again all the same connections that we've been using for these hidden units and then from that I'm going to feed this into a linear transformation and a softmax that's going to be output distribution for the class and because now there's this weight sharing between these connections.",
            "And all the connections use here.",
            "If I train on both, predicting X&YI will effectively be doing some sort of unsupervised regularization.",
            "So."
        ],
        [
            "More explicitly, what you'll have as an objective is that where this shouldn't be an equal should be approximation, but essentially will be optimizing minus the log of the probability of the label given X plus.",
            "A certain fraction of minus the love, the probability of each word given the previous words in that sequence.",
            "So we can see that this does not depend on why it does depend on the data set, but.",
            "So you can think of it as this, this being a sort of regularizer, but that is data dependent.",
            "It's dependent on the specific data distribution and essentially saying that the kind of features that should be useful at predicting Y should probably also be useful at predicting other words.",
            "If I showed you a subset of that document, yes.",
            "Oh yeah, so I'm assuming yes, I'm using a notation which assumes a particular ordering.",
            "Yeah, I think in this case in our experiment, every time we would sort of change the ordering as a trend.",
            "Yes.",
            "Let's see so in terms of like so the question is how you can compute a proper perplexity without the word ordering so.",
            "So actually what we did in the paper when we computer perplexity is that we use just one random ordering, computed the probability for that what we could have done also, and actually, I'll talk about this.",
            "'cause this is going to come up later.",
            "We could actually create an assemble where we're going to consider a set of orderings an actually get probabilities for each of them, and then assemble them.",
            "That's something else we could do, but given an ordering it does give you an given the number where you generate it does give you a proper.",
            "Probability distribution, but you're you're right.",
            "You're right, there is this sort of random variable which I'm more or less ignoring just sampling one order stochastically.",
            "Yes.",
            "So yeah, so I don't think this is anywhere as good as a neural net language model, for instance, because this you know syntax is a lot of what explains the structure of words in sentences, and this does not model this at all.",
            "The only models, the semantics.",
            "It would, but then because of the weight sharing, actually you could shuffle.",
            "The context it would give you the same hidden layer because the weights are shared so far because of that.",
            "So we are currently looking at mixing this with the language model to have a sort of topic hidden layer combined with more syntactic.",
            "You know with a window or something hidden layer.",
            "That's something we have been looking at but but the main point is that this is not a good language model, it's really that's why I'm sending it more as a topic model.",
            "Now, the experience we're going to do here is that we're going to take an image.",
            "An well, actually going to classify images.",
            "So we're going to turn them into a bag of visual words.",
            "And the way this is done.",
            "Usually this is not something we invented that's fairly common practice.",
            "We take the image a splice it into into patches.",
            "We do.",
            "Some K means clustering of all these patches, and are each Patch is going to replace be replaced by the ID of the cluster to which it belongs.",
            "And so each cluster is essentially a visual word, an now, once you've replaced the patches by this cluster IDs, you have a sequence of multinomial zan of so called visual words.",
            "And that's actually the input will be dealing with and not just that.",
            "But this is a data set where we have both images, an annotation, so people have tagged words to each image and will add that as well into the stream of words.",
            "So you have a sequence of visual words, an actual words.",
            "Into a the same joint vocabulary.",
            "And what we'll do is we'll try to model like this."
        ],
        [
            "I think one of the."
        ],
        [
            "Interesting experiment is this one here, which essentially confirms that using this data dependent regularization is useful.",
            "So unfortunately probably don't see a whole lot, but essentially these two curves use a value of Lambda that is non 0, but it's tuned so the other property of this is that.",
            "Instead of assuming."
        ],
        [
            "At Lambda, is equals to one, which would be the case if if this this would be equal to that if Lambda was equal to 1.",
            "So this is not so why this is not an = so it's it's only.",
            "From this we derive this, but we decided to wait than unsupervised part.",
            "So if you tune this, there is a benefit at."
        ],
        [
            "Using the unsupervised regularization term, you see that these two curves here, which corresponds to those that vary.",
            "And same thing here tends to yield higher accuracy, so this is the accuracy of classification accuracy of the model.",
            "So this is an example where having an unsupervised learning signal actually helped in terms of improving the generalizations overfitting less."
        ],
        [
            "We can do the same kind of inspection of the hidden units where I can and I can actually look at All in all the hidden units.",
            "I'll pick a few which have the stronger connection.",
            "Output connection with the class highway.",
            "So because were unsupervised learning, I have connections with between classes and the units, and then I'll take each of these and I'll look at the sum of the weights of the different words and look at those that are most strongly connected and same thing with the visual words.",
            "Where to visualize the visual words.",
            "I've taken the cluster associated with the visual word an I've shown here a subset of 16 patches and you can see for Hwy you can patches that kind of look like roads.",
            "With words that are similar to like car and road and so on."
        ],
        [
            "For sailing it looks a bit more like water.",
            "There are some areas where you have more like a brick wall here."
        ],
        [
            "Or street where you have things that look like buildings and when you see the word building that seems to correlate with with street and be useful to predict, St would learn again something that's a bit more higher level in terms of feature."
        ],
        [
            "OK. Any questions at this point, yes.",
            "Yes."
        ],
        [
            "So what I yeah.",
            "So what I did is that I took these hidden units and then I have connections between a hidden unit and all the cluster IDs.",
            "I took the four top ones an for each to illustrate them I just pick some random patches from that.",
            "Right?"
        ],
        [
            "Right, OK, so now I'm going to talk about how we can take this.",
            "Name model which so far as really only been 1 hidden layer, so between the prediction and the input there was always 1 hidden there.",
            "There was a sequence of hidden layers as I went through the predictions that through the conditionals but from the input to the output there was only one hidden there.",
            "So how can we make this model deep?",
            "This is work with Benyan even more."
        ],
        [
            "Again, alright, so how could what does it mean to make this model deep model?",
            "So this is nail.",
            "I've put a bit more space between the layers.",
            "So what I would do is."
        ],
        [
            "Is that I would essentially add a hidden."
        ],
        [
            "They're like this and I'm showing you this illustration to show that I'm really good at Keynote, and also to show you that what I mean by this is that the connections between adjacent hidden layers are actually the same for.",
            "Again, because I'm using the same neural net for each conditional and we have again the same weight sharing here.",
            "Now this graph is stuck."
        ],
        [
            "To be a bit complicated, so instead I'll use a color coding.",
            "So essentially within one layer the connections are the same color are the same weights, the same set of parameters, so these are shared.",
            "These all black, so these are shared, and these are actually different for each dimension.",
            "So the problem with doing it this way is that.",
            "The amount of computations we have to do to get all of our outputs has now become.",
            "Squared in the number of hidden units.",
            "The reason is that, well, to get each of these D outputs, I need to compute each of the hidden layers of the topmost hidden layers.",
            "And computing each of those requires.",
            "So assuming all hidden layers have the same size as well, that would be 8 squared and I have D of them, so I have D * 8 squared complexity an.",
            "Since essentially the complexity, so you know we are used with moral Nets to deal with things that are more less quadratic in the input size in the sense that the hidden layer tends to have a dimensionality that's compatible with the input layer, not always, but it's often a fraction of.",
            "But now having something that would essentially be D to the three is going to be impractical, so that's why the reason why this basic and direct application of just extending into a deep network is not super appealing.",
            "To solve this problem, we're going to make it even worse, and we're actually also going to change the training criteria so that instead of training for just a single fixed ordering, as it is usually foreign aid, where going to train it for any ordering.",
            "So that is I'm going to, you know, sometimes I'm going to have this particular ordering of the word of the dimensions, so think of the binary case for now going back to the binary, Nate.",
            "But sometimes I'm going to produce."
        ],
        [
            "The 4th input and then the third.",
            "That's a different ordering, OK?",
            "What's convenient with this?",
            "If we achieve this, is that I could do this kind of conditional inference if I had missing inputs with arbitrary patterns, because if I have a model that's been trained on any arbitrary missing pattern, that could be a useful inference machine and say, filling in missing inputs.",
            "But there is, you know, there's a whole lot of ordering, so that seems like there was.",
            "That would complicate things a little bit.",
            "How?"
        ],
        [
            "Never imagined, let's consider now just a single conditional.",
            "So let's convince."
        ],
        [
            "Consider this conditional here the conditional of the third dimension given the first 2.",
            "Now imagine that I've performed this forward propagation already, while the only thing that's missing from getting the conditional for."
        ],
        [
            "The fourth dimension is to just take that and multiply by different set of weights for the fourth dimension, and very cheaply.",
            "I would get the output.",
            "For the ordering where the fourth dimension actually comes after the first and the second OK, so now because I'm also in this setting where I want to train all orderings, what I could do instead instead of being stochastic, why pick pick one ordering and I commit to it?",
            "And I trained all of its conditionals instead.",
            "I'll pick an example and I'll split the input vector into random.",
            "These actually I'll shuffle the examples as split in random position, and then what I'll do is that.",
            "Our condition and what is left to the right lower left left of this link position and I'll predict all of the other inputs at the output.",
            "Everything that's right of this splitting position and what I'm effectively doing here is that I'm not training all the conditionals for one ordering, but I'm training several conditionals for several orderings at the same time, and the kind of computations are required here are very similar to a regular feedforward network, it's just essentially have my input.",
            "But some of them are.",
            "Essentially, it's as if there were zero hidden layer hidden there, and I predict an output.",
            "Kinda like an auto encoder.",
            "But now I actually don't care about these outputs, only care about those.",
            "OK, so the computational complexity of a single update is now going to be similar to a regular autoencoder.",
            "OK, now there's just one little thing about this, which is that.",
            "As I said, when I'm conditioning on only these two things, it's as if for this hidden layer that X3 isn't there.",
            "Sorry X3 is actually equal to zero and four.",
            "X4 is equal to 0, but this might not be the case.",
            "I just, you know, by the way I constructed model it so it turns out that being equal to 0 and being missing is the same thing for the neural net."
        ],
        [
            "So the way to fix this is that we're going to add for each dimension a bit, which is going to indicate whether the input is present or not.",
            "So this would be one.",
            "This would be one, and there would be a zero and a zero here.",
            "OK, so in effect this hidden layer has two vectors, the vectors X where I've set to 0 all the dimensions on which I'm not conditioning, and this binary mask essentially.",
            "OK.",
            "So we call those conditioning weights and this is so that the hidden layer can distinguish between an input being absent, but zero and input being present but being zero.",
            "Sorry being absent and having any value and being present but being 0."
        ],
        [
            "Alright, so unfortunately that doesn't solve the complexity of forgiven ordering computing P of X if on computer of XI need all the conditionals.",
            "So I will have a complexity which is squared in the in the size of the hidden layers times the number of inputs.",
            "But that's still somewhat tractable, so it's not so bad if I just want to check the performance as a generative model.",
            "I can I can still do this, it's not exponential, for instance.",
            "Now at Test time, what is interesting here is that because I'm training on all orderings, there isn't one specific ordering I'm supposed to choose when I'm computing P of X, because it's been trained on all of them, and now there's something that will seem like a bug, which is that if I choose one ordering and I compute P of X and I pick another ordering to compute P of X based on that ordering, these two things will probably not match because the model isn't constructed in a way to make sure that all of the conditionals are consistent from one another.",
            "So that seems like a bug, but it actually can be used as a feature in the sense that as I'm using the model, I can just generate all of these likelihoods from a bunch of different orderings, make the compute the average of those, and essentially get and assemble from scratch as I need it.",
            "OK, so if you have changed just one model, I can construct an example of arbitrary size, really, because there's you know an exponential number of orderings.",
            "And as we'll see, this actually gives better log probabilities when we measure measure performance.",
            "All."
        ],
        [
            "So let's look at results.",
            "So this is on the binarized MNIST data set here.",
            "These are all of the name models that were trained in the orderly fashion, where we essentially for each update we pick an ordering.",
            "And then we take that is, we pick the input, we sample in ordering.",
            "We split the input into two at a random position, and then we predict what is the right based on what is on the left.",
            "And this is when we evaluate the test log probability.",
            "And so we're getting, you know, results are OK, but they're not actually worse than what we had with regular made.",
            "However, when we start constructing these assembles, we get a significant boost would actually exploit the fact that we've trained on multiple orderings, and we get log probabilities that are now much better than the name model we had.",
            "It's better than the PBM model that I showed before, and it's compatible to a deep belief net.",
            "Which is one of these neural net models with stochastic latent variables."
        ],
        [
            "One thing we can so these just showing samples which you know for most cases we can distinguish fairly easily a digit from it."
        ],
        [
            "One thing, one interesting thing we can do is then we can hide part of the input and then because we've trained on all the orderings, what we can do is can generate sorry condition and all the dimensions that are not hidden, and then we sample what is in the little Patch here by going in some arbitrary order in that Patch and just see what it generates, how it fills in these missing inputs and you get things that are interesting.",
            "For instance when there is ambiguity.",
            "So for instance here.",
            "B5, but this could also be a decent looking 6 if you just filled it like that or you have like a four versus something that looks more like a 9 here when you hide that part.",
            "So it's doing a decent job."
        ],
        [
            "And this is again results.",
            "But now for natural image patches where we can see that with depth, we're getting better and better lock probability higher and higher.",
            "And now we actually able to beat the mixture of Gaussian."
        ],
        [
            "Any questions on this, yes."
        ],
        [
            "So when you questions about how do we pick the ordering within the red area, right?",
            "And.",
            "That's a good question.",
            "I'm not sure well it's OK so.",
            "The question is essentially to what extent does the order rate actually determine whether I'm going to sample this versus that, and that's a good question.",
            "I don't know there shouldn't be a reason for this.",
            "It could be that for some reason there it does introduce a bias, but that's a good question.",
            "I don't know whether it was always the same ordering here.",
            "Question and I'm not sure.",
            "Other questions."
        ],
        [
            "And then the final experiment with this deep name model I'm going to go back to this problem of learning from pairs of images and text.",
            "But now we're going to use a deep version of Doc Nate, so we do the same trick where we take the words there in random.",
            "We were already putting them in random order.",
            "What we do is we split the stream of words at a random position and we're going to predict all of the words after that spinning position condition on all the words before that's playing position.",
            "In this context, because we, because we do the weight sharing at the output, we don't use that restructuring multinomial anymore.",
            "We use a single softmax because there's only one softmax.",
            "We can we have to compute since there's weight sharing for all position, then I really only have to compute one softmax and that's the probability I'll use to evaluate the probability for all of the upcoming words.",
            "So now the softmax is a decent choice.",
            "Here we use a GPU implementation.",
            "This is on a much larger data set than what I showed before.",
            "The Flickr MRM IR data set where there is what's interesting about it?",
            "There's a million images that are not labeled.",
            "They have annotation words, but they don't have classes and there's a much smaller subset.",
            "I think 30K or 25K of label images.",
            "So what we did in these experiments that we pre trained deep made on the unlabeled data.",
            "So not knowing what the label is trying to me as a generative model and then after we've done that for some time will use these weights to initialize a supervised neural net.",
            "That will do the fine tuning and that gives us a representation for which we can do."
        ],
        [
            "And.",
            "These are the results, so this is results that will they will come up in the upcoming teepi ME Journal Paper which we learn today was accepted, so that's that's good.",
            "So here you have a few different baselines, some of which are neural network based.",
            "With this one is DBM and this is another kind of neural net which you might hear about.",
            "This is from unlikely will be presenting during the summer school and hear what I'm showing is for instance that.",
            "Death seems to play a role that is 1 hidden.",
            "Layer seems to be worse than two or three for this amount of training.",
            "Then if we did more pretraining, the results were improving even more, and eventually the third hidden layer would start to be significantly better with even more pre training."
        ],
        [
            "And actually, you know we could have continued and do more pre training and it seems the performance would continue to improve.",
            "So this is really showing this idea that training a generative model on a lot of unlabeled data can actually be useful in generalizing better.",
            "It took a long time to generate these points.",
            "I don't know to what extent the implementation is optimal, but it definitely needed a GPU."
        ],
        [
            "And this is just a quantitative evaluation where what we did is that we presented this to the model.",
            "So only visual words and what I'm showing here is what is the ground truth, annotation words or showing.",
            "Here is the words that had the highest probability for that model, and those are cases where, frankly I think the ground truth was not particularly good and the model was actually doing a better job, so it's actually sort of denoising essentially the.",
            "The annotation and I should say that now this is not as impressive as the most recent work which I've been generating actual sentences for.",
            "Describing images where I think you'll probably hear about during the this week."
        ],
        [
            "Alright, so I'm not going to talk about a different model.",
            "I'll end with this, which is again a note on quarter was actually it's going to be much more.",
            "Close to the definition of another time quarter, he is so."
        ],
        [
            "Home.",
            "So the reason that made is a distribution estimator is essentially because it respects a property which I called the autoregressive property.",
            "What I mean by that is that each output in the model is such that it depends only on previous inputs in some ordering.",
            "A regular autoencoder isn't a distribution estimator or a generative model, exactly because it doesn't satisfy this so."
        ],
        [
            "I have a regular autoencoder.",
            "There is nothing forbidding say that can output to have to get information directly from itself, for instance, and there's no ordering this notion of ordering that's being imposed here.",
            "So one question that we had is, can we just take a regular autoencoder and try to modify it as simply as possible, as little as possible and still get something that we can interpret as a generative model that we can compute log probabilities for?",
            "And just see what kind of performance we can get.",
            "And essentially, to do this, the simple simple we found is that we are going to impose this auto regressive property by changing the connectivity of the network using essentially some masking.",
            "So I'll show you a very simple recipe."
        ],
        [
            "Doing this.",
            "I use a little picture just to illustrate that.",
            "So what we want to do so this model that we came up with we called it mastering quarter distribution estimator or made.",
            "So what we want to do essentially is taken autoencoder like this where we have inputs of dimensionality three an at the output we produce a reconstruction and would like to have something like this which can be used for the output.",
            "Can be used as conditional distributions according to some ordering or would just take the product and that would define a valid joint distribution.",
            "So the first thing that we need to do is we need to pick an ordering for the inputs, which is going to determine the sequence of conditionals.",
            "So here I randomly decided that the first element of this ordering is going to be X2, the second is X3 and the last one is X one that was picked randomly.",
            "So what that means is that at the output.",
            "I need this output to correspond to P of X2, which means it needs to depend on none of the inputs.",
            "As you can see.",
            "Indeed it has no connections.",
            "X3 is the second one, so the output for the third output needs to be P of X3.",
            "Given X2.",
            "That is, it needs to be connected to hidden units, which themselves are connected to hidden units that are only connected to X2 and not other inputs.",
            "And then the last one which is X1 needs to correspond to P of X1 given X2 and X3, which means that it needs to be connected to hidden units that are either connected to X2X3 or both, but not X1.",
            "If I have this then I can use these as conditionals.",
            "I could take the product of those that would define a valid joint distribution.",
            "So.",
            "So what this means, as I said, is that this needs to be connected to zero inputs.",
            "This to only the first input, and this one only to the second input.",
            "Well, I could achieve this by just randomly deciding for each of the hidden units how many inputs in this ordering it's going to be connected to.",
            "So for each unit what I've done here is that I've generated a number between one and the number of inputs minus one, and that's going to be this number.",
            "How many inputs?",
            "In that ordering, am I allowed to be connected to OK, so it's never simple 0 because that would be a useless hidden unit.",
            "Would compute nothing and never sample 3 because then it's connected to everything, so I can't use it to produce any conditional at the output.",
            "And once I've decided this one, I've you know uniformly and sorry, yeah, uniformly independently sample each of these integers where I can construct a mask which will respect this constraint.",
            "This auto regressive constraint.",
            "So all I do is that I construct a Max, wear the mask where the value is whether the hidden unit the corresponding hidden unit has a number which is greater or equal than the corresponding unit, and the layer below.",
            "OK, so for instance this.",
            "Role here that corresponds to the mask for that hidden for that hidden unit, and well as we can see, it's two is not as I'm sure you are learning right now too is not greater or equal than three.",
            "So so here we have a zero.",
            "That's what black means here, but it is greater or equal than one and two, so I have value in the mask of 1 here and one here.",
            "And I do this for all connectivity's.",
            "And actually, something you can trivially implement them parallelized by just in Theano.",
            "It's trivial, for instance.",
            "And you do this for all layers just using these numbers.",
            "So coming up with these mask, once you've done this sampling assignment is super trivial.",
            "And then when you the only they are changes.",
            "When you're training this new masked autoencoder is that before you use the weight, you multiply by the mask.",
            "And then you can use it to compute the pre activation at the next layer and when you back propagate you take into account the mask.",
            "It's actually exactly like it was very similar to dropout, but instead of dropping out units, you're dropping connections, and for those you might know about Drop Connect, which is another version of drop out or an extension I guess of dropout which was proposed.",
            "Well, that's essentially drop connect, but with the constraint that we're trying to do here is getting autoencoder whose output can be interpreted as a distribution.",
            "OK, so it has more constraint in the way we generate these masks."
        ],
        [
            "So a few more details we could generate masks for each input.",
            "OK, so that would more or less correspond to joining the multiple orderings, for instance.",
            "As I said from training similar to dropout, and it's almost exactly like Drop Connect with some additional constraint.",
            "At Test time again we can use and assemble by generating multiple masks and averaging the probabilities."
        ],
        [
            "And we can generalize to any connections you could have connections that skip from the input to the output, or to other hidden units.",
            "Generating the mask would just correspond to comparing the numbers.",
            "The integers for the corresponding layers.",
            "So it's actually very trivial to get arbitrary connections.",
            "There's a few connected or related word I want to mention, so it turns out that the fully visible simulate belief net is made, but without the hidden units and a fixed mask.",
            "That's essentially where it corresponds to.",
            "So I mentioned the work that Joshua and Sammy Benjo as well did in 2000.",
            "On exploring the use of neural Nets for essentially replacing these conditionals in a fully visible belief network.",
            "While this is exactly like 1 hidden layer version of this, but without the mass sampling, which is something that we explored for performing the average, if you're going to do research in deep learning, something you have to get used to some of your ideas you actually have done already.",
            "It's just something that you have to get used to.",
            "I take it as a compliment now.",
            "If you're if none of your ideas and unbiased revenga probably doing something wrong, that's why I'm saying.",
            "You did not pay me to say that, so I guess you're paying for the hotel right here, but anyways, so.",
            "There might be a little bit conflict of interest, so OK.",
            "They wanted to say is that made is actually when you use it when you code it and you compare it with an implementation of Nate, it's much more.",
            "It's much faster.",
            "The main reason is that matrix matrix multiplies are much faster than doing this sort of cumulative thing that you have to do to get the pre activation vectors.",
            "And also there are much fewer nonlinearities.",
            "So because they are effectively in aid as many hidden layers as they are input dimensions, you have number of nonlinearities scales with.",
            "The number of inputs times the number of hidden units, whereas here it's just linear in the number of units total, so it's linear and number of units and not quadratic.",
            "So that means that in some of the experiments were actually able to train a much bigger made model and it turns out that this was beneficial to make it competitive for the Binarized missed experiment that show and made it all so much faster to evaluate them deep made.",
            "That is, in one forward pass I get all my conditionals, which I can multiply, get a probability, or get the log, and then some to get the log probability for deep made.",
            "I do have to generate, sorry.",
            "I do have to do as many passes in this deep neural net as they are conditionals.",
            "As I mentioned before, but to actually generate samples they are equally.",
            "They are both as or not, not enough efficient."
        ],
        [
            "So OK, so there's some results on this UCI benchmark.",
            "The main point of which is that results are fairly comperable to named an other models have been generated that have been proposed."
        ],
        [
            "Here this is experiencing binarized hymnists, where you have samples here which are somewhat similar and lock probabilities for made which are compareable to the other results.",
            "I've been out there."
        ],
        [
            "An that's it, so I guess I wanted to also present this just to give you an idea of, you know in that space fully visible basean Nets, that's already a space you can explore and try different things and just generally speaking.",
            "With that right now, Jonathan modeling or distribution estimation is a fairly active and hot topic.",
            "Days are essentially all of the different models that were proposed.",
            "Almost most of them in the last two years.",
            "Actually, I think you're going to see the majority of them be presented during the deep Learning Summer School, so.",
            "Part of the reason for that, even though it's not helping to get better results on image net right now or speech doing better speech recognition, I think they will touch on the topic a little bit.",
            "So for other problems where we don't have a whole lot of labeled data, I think this is where we are in need of better solutions.",
            "And I think that part of solution is, I suspect is going to come from having better unsupervised learning methods which are going to be able to explore the large amount of unlabeled data that we often have.",
            "And could exploit and so that's ultimately the direction that I think some of this work.",
            "Could you know?",
            "Help us get towards?",
            "But I think we're still not exactly there in terms of reaching practical applications, hopefully with some of your work.",
            "Maybe in the future we'll get we'll get there."
        ],
        [
            "Alright, so that's it for me, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Welcome back.",
                    "label": 0
                },
                {
                    "sent": "So I guess now at this point you've seen pretty much like the basic that you'll need to read 12 into some of the more recent advances in in deep learning, and I'm going to touch upon this subject, which is related unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "It essentially concerns developments are fairly recent.",
                    "label": 0
                },
                {
                    "sent": "But yeah, so I think now you have a really nice place where you known about the basic things which will probably pop up during the rest of the summer school.",
                    "label": 0
                },
                {
                    "sent": "You've got a good basis, unlike what the first 2 days might suggest.",
                    "label": 0
                },
                {
                    "sent": "French speaking is not requirement for deep learning based on the speaker list it's turned.",
                    "label": 0
                },
                {
                    "sent": "You can do deeper.",
                    "label": 0
                },
                {
                    "sent": "Don't worry, you can do deep learning with speak French.",
                    "label": 1
                },
                {
                    "sent": "So now we gotta talk about instead of talking about supervised learning, which has been the main topic so far, I'm going to talk more about unsupervised learning and I'm going to talk also about how we can leverage unsupervised learning methods.",
                    "label": 0
                },
                {
                    "sent": "Context of a supervised learning problem.",
                    "label": 0
                },
                {
                    "sent": "So this is I'm going to touch a lot of work in collaborations with people that you see here in Murray Ben urea engine and stanislaw slowly Matthew John may are some of my students and Carol Gregor as well.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so specifically the problem that I'm going to discuss talk about is it probably distribution estimation that is, given a training set of examples are not labeled so a set of X.",
                    "label": 0
                },
                {
                    "sent": "So I set of inputs.",
                    "label": 0
                },
                {
                    "sent": "The goal is to try to produce a nestum it of the true distribution P of X from which the samples come from.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a very complicated.",
                    "label": 0
                },
                {
                    "sent": "This very hard problem and also quite fundamental.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of different tasks that.",
                    "label": 0
                },
                {
                    "sent": "If you have a good distribution estimator, you can use it to solve their classification problem.",
                    "label": 0
                },
                {
                    "sent": "It's a very hard problem and particularly becomes particularly hard when the dimensionality of the observation increases.",
                    "label": 0
                },
                {
                    "sent": "So what you have to do here is you have to output essentially function P of X that gives you what's the probability that I would see this sample from from that population for that distribution, and So what it means is that you have to.",
                    "label": 0
                },
                {
                    "sent": "Produce the correct output for any potential value of X, and that's different from classification, because usually if you're say, classifying digits well, you already expect that the input is going to be a digit.",
                    "label": 0
                },
                {
                    "sent": "You just want to know is it a zero or one or two or three.",
                    "label": 0
                },
                {
                    "sent": "Here for any value including a random vector presented as input, you need to produce a proper value for it, so that I think is one of the fundamental reason why this is a hard problem.",
                    "label": 0
                },
                {
                    "sent": "As I said, it's very general, so if X was.",
                    "label": 0
                },
                {
                    "sent": "Partly whimpered, but also a target.",
                    "label": 0
                },
                {
                    "sent": "Then if I have the joint distribution over the two I could essentially use.",
                    "label": 0
                },
                {
                    "sent": "I could just condition get the conditional of Y given X, and then I'd get a classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, so in that respect it's a very general problem from which you can solve other specific problems.",
                    "label": 0
                },
                {
                    "sent": "It can be a good proxy for feature learning and can provide regularization cues for another model.",
                    "label": 1
                },
                {
                    "sent": "So when I was talking about for instance, unsupervised pretraining well, something like this distribution estimation.",
                    "label": 0
                },
                {
                    "sent": "Can deal yield algorithms that might give good neutralization for a supervised classifier so as initialization before fine tuning.",
                    "label": 0
                },
                {
                    "sent": "Actually, we'll see how I use the specific algorithm that we that I developed in the past years to do exactly that.",
                    "label": 0
                },
                {
                    "sent": "It can be used also if you have observations where you have missing inputs, you could use a model like this to fill in the missing inputs.",
                    "label": 0
                },
                {
                    "sent": "If you want to feed that to a classifier that doesn't deal with missing inputs, you can simulate new examples.",
                    "label": 0
                },
                {
                    "sent": "There a lot of application for a model that's a distribution estimator.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, So what I'll talk about is 2 models on the last one.",
                    "label": 0
                },
                {
                    "sent": "If I have time, I'll talk about it.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll skip it, depending on time.",
                    "label": 0
                },
                {
                    "sent": "Again, you know, don't hesitate to ask questions the whole time.",
                    "label": 0
                },
                {
                    "sent": "There's really no problem.",
                    "label": 0
                },
                {
                    "sent": "So first of all, I'm going to mostly talk about is the note or aggressive distribution estimator ornate, and I'm going to start by talking about this model in the context of modeling binary vectors, and then I'll show how you can generalize to continue real valued observations, multinomial.",
                    "label": 0
                },
                {
                    "sent": "And then talk about the diversion of Nate and if I have time I'll talk about another variant that uses the same idea of autoregression, but that's closer in spirit to regular autoencoders, which we've seen today.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to make it clear what I'm talking about when I'm talking about distribution estimation, I'll just give you 2 examples of models that are distribution estimators that you might know about.",
                    "label": 0
                },
                {
                    "sent": "A mixture model is an example of distribution estimator, so in a mixture model you assume that your data point.",
                    "label": 0
                },
                {
                    "sent": "A priori belongs to a certain number of C classes, but that you don't observe, and for each class you have a generative model that says, well, given that a sample is from a given class, here's I was generated, or here is the distribution P of X given to which class the input belongs.",
                    "label": 0
                },
                {
                    "sent": "That could give you, you know, the likelihood of different inputs belonging to being generated by that class, and so to get a P of X.",
                    "label": 0
                },
                {
                    "sent": "Since we don't observe the class which.",
                    "label": 0
                },
                {
                    "sent": "The input belongs because we're in an unsupervised learning setting.",
                    "label": 0
                },
                {
                    "sent": "We just sum each of these components, weighted by the probability of a point belonging to that class.",
                    "label": 0
                },
                {
                    "sent": "So that's a mixture model.",
                    "label": 1
                },
                {
                    "sent": "Gaussian mixture model is an example which you might know when people use quite a bit.",
                    "label": 0
                },
                {
                    "sent": "So it's a simple model, well understood, usually training with the EM algorithm button context.",
                    "label": 0
                },
                {
                    "sent": "The performance can actually be disappointing that specifically truthful binary data, as we'll probably see later on.",
                    "label": 1
                },
                {
                    "sent": "So it belongs to a larger class of graphical models.",
                    "label": 0
                },
                {
                    "sent": "I think you'll learn more about this, probably tomorrow that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Type of model.",
                    "label": 0
                },
                {
                    "sent": "I'm going to focus more on.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to extend and use neural Nets in that framework.",
                    "label": 0
                },
                {
                    "sent": "Is what I call fully visible Bayesian networks.",
                    "label": 1
                },
                {
                    "sent": "So the difference here is that in.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say a mixture model.",
                    "label": 0
                },
                {
                    "sent": "Other types of graphical models.",
                    "label": 0
                },
                {
                    "sent": "There's usually the notion of a stochastic latent variable, and if we want to compute the probability of some input, we have to integrate it out.",
                    "label": 0
                },
                {
                    "sent": "And when we do inference, we have to infer posterior over that latent space.",
                    "label": 0
                },
                {
                    "sent": "So in this case the little space is very simple, it's just a partition is essentially a choice between one up to C classes.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A free visible Bayesian network doesn't explicitly have this notion of a latent variable.",
                    "label": 0
                },
                {
                    "sent": "All it does is that it assumes 1st that the distribution over my input space P of X is the product of the probability of the distribution over the KF dimension.",
                    "label": 0
                },
                {
                    "sent": "Given the value taken by each of the previous dimensions up to K -- 1.",
                    "label": 0
                },
                {
                    "sent": "So that symbol here, that's the sub vector from dimension one up two K -- 1.",
                    "label": 0
                },
                {
                    "sent": "That's what I mean by their smaller than K as an index here.",
                    "label": 0
                },
                {
                    "sent": "So actually this is not an assumption.",
                    "label": 0
                },
                {
                    "sent": "This is true for any distribution.",
                    "label": 0
                },
                {
                    "sent": "Any distribution I can decompose in this way, but these conditionals for certain models will have a simple form and others will have a complicated form that perhaps we can't even compute OK, but that's the starting point for models that are based on the idea of visible Bayesian networks, and so if we were to illustrate that as a graphical model, it would correspond to this.",
                    "label": 0
                },
                {
                    "sent": "So these arrows sort of illustrate the dependencies structure.",
                    "label": 0
                },
                {
                    "sent": "Between the inputs and, it's essentially also illustrating what is what we call the generative story behind this model.",
                    "label": 0
                },
                {
                    "sent": "So mixture model the generative story is that first before you generated an ex, you chose OK from what class am I going to generate an input four and so you picked out according to that probability, probability over whoops probability over classes, and then once you pick that where you're going to generate from the conditional corresponding to that class, that's the generative story for mixture model.",
                    "label": 0
                },
                {
                    "sent": "For fully visible Bayesian network, the assumption as to how data was generated is that from left to right.",
                    "label": 0
                },
                {
                    "sent": "Let's say for left to right.",
                    "label": 0
                },
                {
                    "sent": "For now I'm first going to sample X from P of X1, the first the first dimension of my vector.",
                    "label": 0
                },
                {
                    "sent": "I'm going to sample it from P of X1 and I'm given that simple and going to generate the value for the second dimension conditioning on X1 because the arrow the directed arrow signifies that.",
                    "label": 0
                },
                {
                    "sent": "I've characterized X2 given X one and then the third dimension is going to be generated based on the value of both the second dimension and the first.",
                    "label": 0
                },
                {
                    "sent": "Because I have directed connections from the first and second dimension to the third and so on.",
                    "label": 0
                },
                {
                    "sent": "So essentially all dimensions in the input space are generated sequentially in some order, let's say left to right.",
                    "label": 0
                },
                {
                    "sent": "To make things simple.",
                    "label": 0
                },
                {
                    "sent": "So an example of a model that follows in this more general family, fully visible Bayesian networks, is the fully visible sigmoid belief network in this case.",
                    "label": 1
                },
                {
                    "sent": "It's simply corresponds to choosing that these conditional.",
                    "label": 0
                },
                {
                    "sent": "So again, we're assuming that everything is binary.",
                    "label": 0
                },
                {
                    "sent": "So when I want to specify the distribution of the KF dimension, given the previous dimensions, all I need to provide is what's the probability of being equal to 1 and the probability of being equal to 0 is going to be 1 minus that, so once I give this P value, I've specified the full conditional distribution an in the fully visible segment belief net.",
                    "label": 0
                },
                {
                    "sent": "The alleged a logistic progressor model is used to model this conditional.",
                    "label": 0
                },
                {
                    "sent": "So in other words, that probability is I take the vector, the sub vector from dimension 1 to K -- 1 and multiplied that by a weight vector plus a bias pass this through sigmoid.",
                    "label": 0
                },
                {
                    "sent": "That's my probability that XK is equal to 1 in that model, and so each conditional has its own magic regression vector, an that's all joint distribution is being modeled.",
                    "label": 0
                },
                {
                    "sent": "It's going to be the product of each of these conditionals from left to right.",
                    "label": 0
                },
                {
                    "sent": "So as a shorthand values X hat K for what is P of XK give equal to 1 given the previous subvector subvector?",
                    "label": 0
                },
                {
                    "sent": "Because you can sort of think of it as a reconstruction, so from the previous dimensions you're trying to reproduce forgiven.",
                    "label": 0
                },
                {
                    "sent": "Example, the would like to assign high probability anyways to the actual observed value of X.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of like a reconstruction, but it's sort of a predictive reconstruction because you're not conditioning on SK only conditioning on the previous dimensions.",
                    "label": 0
                },
                {
                    "sent": "And if you want to view it as a flow graph, that's kind of like your neural network.",
                    "label": 0
                },
                {
                    "sent": "What it would look like something like this.",
                    "label": 0
                },
                {
                    "sent": "You would have an output layer directly connected with the input layer an you have this left or right connectivity here where this input is not connected to itself at the output, but only to the.",
                    "label": 0
                },
                {
                    "sent": "Left the sorry the all the components at the right in the output layer and same thing with this one and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just another visualization which here exposes more directly how we para tries this fully visible simulate belief net.",
                    "label": 0
                },
                {
                    "sent": "Other questions about this.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Just to summarize, in this setting we assume the data is generated one dimension after another in some given order, and for fully visible similar belief net, we're using logistic regression models for each of these conditionals we have a different logistic regressor for each dimension, and we can train that by gradient descent, optimizing the log likelihood on your training data.",
                    "label": 0
                },
                {
                    "sent": "So this is this should be a fairly weak model because these conditional is log linear and linear is about the weakest model.",
                    "label": 0
                },
                {
                    "sent": "Can be turns out for a lot of data.",
                    "label": 0
                },
                {
                    "sent": "It's actually surprisingly good in some cases.",
                    "label": 0
                },
                {
                    "sent": "On the binarized version of Emnace, which I'm going to talk about later, is actually better than the mixture model an this sort of inspired me to say, OK, well can we take this model?",
                    "label": 0
                },
                {
                    "sent": "Just make it more powerful and we stick unknown net in there because that's what I do as a PhD student.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Stick neural Nets where they had been been neural Nets before so.",
                    "label": 0
                },
                {
                    "sent": "Let's try to do that.",
                    "label": 0
                },
                {
                    "sent": "So he.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's what I came up with.",
                    "label": 0
                },
                {
                    "sent": "Essentially, we give up this model we call Neid.",
                    "label": 0
                },
                {
                    "sent": "Neid is a model where, which is sort of like a note on quarter.",
                    "label": 0
                },
                {
                    "sent": "Maybe talk a bit more about why we can think of it this way.",
                    "label": 0
                },
                {
                    "sent": "And in words, the idea is that each of these conditionals in that productive conditional alpera tries it as a neural net, but it's always going to be essentially the same neural network, and in particular the connections with the hidden layers are going to be shared across conditionals.",
                    "label": 1
                },
                {
                    "sent": "So I have this inference machine if you want this neural net that takes a subvector an outputs, what's the probability for the next dimension in that sequence of inputs?",
                    "label": 0
                },
                {
                    "sent": "OK, so this means that the value of the hidden layer.",
                    "label": 0
                },
                {
                    "sent": "For predicting the Keith Dimension dimension, sorry.",
                    "label": 0
                },
                {
                    "sent": "So this is not an index for the depth.",
                    "label": 0
                },
                {
                    "sent": "It's more an index for this is the hidden layer to predict which dimension, which conditional.",
                    "label": 0
                },
                {
                    "sent": "That's going to be so.",
                    "label": 0
                },
                {
                    "sent": "In this case we pick the sigmoid of a linear transformation of this sub vector containing all the dimensions from 1 to K -- 1 and I'm going to take that vector.",
                    "label": 0
                },
                {
                    "sent": "An use a linear transformation specific to the conditional.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to train, so this part here is essentially logistic regressor, but that takes as input this hidden layer.",
                    "label": 0
                },
                {
                    "sent": "Pass this through sigmoid and that's going to give me my probability that XK is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll just go through the generative story by sort of going through the flow graph here.",
                    "label": 0
                },
                {
                    "sent": "So if I want to generate a vector X using this model, I'd start by computing H1, which is the hidden layer for the.",
                    "label": 0
                },
                {
                    "sent": "First conditional, which usually the marginal, it's P of X1, so this is not dependent on any input.",
                    "label": 0
                },
                {
                    "sent": "So essentially this part here is 0, so it's just the simulator bias.",
                    "label": 0
                },
                {
                    "sent": "It's kind of over parameterized really.",
                    "label": 1
                },
                {
                    "sent": "And then I take that I multiply it by its by the output weights for the first dimension V1.",
                    "label": 0
                },
                {
                    "sent": "So that's essentially the 1st.",
                    "label": 0
                },
                {
                    "sent": "So for the first conditional, this would be a one, so it must be the first row of my matrix V plus my scalar bias for the first dimension by the sigmoid.",
                    "label": 0
                },
                {
                    "sent": "That gives me the X at one, so the probability that X one is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Then, so that's a probability between zero and one.",
                    "label": 0
                },
                {
                    "sent": "I flip a coin with that probability.",
                    "label": 0
                },
                {
                    "sent": "It that becomes my new input to the name model.",
                    "label": 0
                },
                {
                    "sent": "The 01 stochastic choice, and then from that I compute the hidden layer for the second conditional, which now depends on what I filled in, filled in here OK, and so it's going to be essentially just the column vector of W corresponding to the first dimension time 001 plus the bias vector pass through non linearity.",
                    "label": 0
                },
                {
                    "sent": "Say the sigmoid.",
                    "label": 0
                },
                {
                    "sent": "And then again I applied the output weights for the second dimension, so this would be so you'd have a 2 here now and this would be a 2.",
                    "label": 0
                },
                {
                    "sent": "Not very good drawing with my mouse, sorry.",
                    "label": 0
                },
                {
                    "sent": "And so you get that probability you sample from it.",
                    "label": 0
                },
                {
                    "sent": "And that becomes the second dimension that you put in input, and then you compute the third hidden layer for the third conditional and you continue like this and notice that and this is what I've indicated with this little drawing here with the blue dots.",
                    "label": 0
                },
                {
                    "sent": "What this means is that this set of connections and this set of connections here are the same vectors, the same set of parameters it's reused across the hidden layer, and that's what I mean by it's the same neural net for each conditional.",
                    "label": 0
                },
                {
                    "sent": "There's this weight sharing across the conditionals in the level of the hidden layers.",
                    "label": 1
                },
                {
                    "sent": "And actually, we can leverage this to make the computation of all these hidden layers fairly efficient.",
                    "label": 0
                },
                {
                    "sent": "So normally if I was to compute say that hidden layer, I would need to take all of this sub vector, multiply it by the submatrix which is almost the whole matrix, and then apply my sigmoid.",
                    "label": 0
                },
                {
                    "sent": "But since I since I've already computed once, I'm here assuming computer activation here.",
                    "label": 0
                },
                {
                    "sent": "Well, I've had most of the job that's done, I just need to add the contribution that comes from this new dimension so I can exploit the fact essentially that the.",
                    "label": 0
                },
                {
                    "sent": "Pre activation at adjacent positions, adjacent conditionals is just the contribution from the new dimension, so I just have a vector to add which is the column vector of matrix W times.",
                    "label": 0
                },
                {
                    "sent": "What is the actual sample and I can pre essentially reuse their pre activation so that makes it kind of like a recurrent neural net for those that know about it.",
                    "label": 0
                },
                {
                    "sent": "But it's it's linear before the activation function and it has this property that computations are fairly efficient.",
                    "label": 0
                },
                {
                    "sent": "So anyways, that's the generative model.",
                    "label": 0
                },
                {
                    "sent": "I would continue like this.",
                    "label": 0
                },
                {
                    "sent": "Sampling all all the dimensions by getting each of my outputs and you can see it's sort of a little tone color because the way I'm going to train it is that for a given example X.",
                    "label": 0
                },
                {
                    "sent": "So if I observe a sample X and I want to increase its likelihood, well, what I'll be doing is that I will increase the likelihood of each dimension given the previous one.",
                    "label": 0
                },
                {
                    "sent": "So if you don't really know what's going on here, it's like you have an input which is X.",
                    "label": 0
                },
                {
                    "sent": "And you have an output with a vector of probability and trying to make that output match the input.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of like an annoying quarter, but it's been structured.",
                    "label": 0
                },
                {
                    "sent": "It's been tortured in a way that you can interpret the output as a series of conditionals, from which you can sample.",
                    "label": 0
                },
                {
                    "sent": "So we talked about weather, like denoising autoencoders were turn colors, were generative models, note encoder, not really.",
                    "label": 0
                },
                {
                    "sent": "But this definitely is.",
                    "label": 0
                },
                {
                    "sent": "It has a generative story behind it, and it's a proper distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was kind of the interesting thing about this.",
                    "label": 0
                },
                {
                    "sent": "Try to see.",
                    "label": 0
                },
                {
                    "sent": "We can get with this neural net generative model that's just feedforward no latent variables.",
                    "label": 0
                },
                {
                    "sent": "It's based fully fully visible graphical model on air right now is a good time to ask questions about this, because this is like most of the rest of the talk really.",
                    "label": 0
                },
                {
                    "sent": "So yes.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "I so I think you get something in generalization an I'll try to talk about this when I talk about another model later on which so that you explored in the 2000s and then we re explored recently and what we found is that this sometimes tend to overfit less because of the weight sharing and the weight sharing sort of comes from an RBM, and since we haven't seen respectable machines, I didn't dig into that.",
                    "label": 0
                },
                {
                    "sent": "It's described in the paper, so once you.",
                    "label": 0
                },
                {
                    "sent": "Become experts in our BMS.",
                    "label": 0
                },
                {
                    "sent": "You can go look at the innate paper and you'll see what's the relationship between the two.",
                    "label": 0
                },
                {
                    "sent": "I'm going to yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll talk about that.",
                    "label": 0
                },
                {
                    "sent": "So the question, I would decide the order.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about it and next slide.",
                    "label": 0
                },
                {
                    "sent": "I think you're not going to like the answer.",
                    "label": 0
                },
                {
                    "sent": "So there are none would be like.",
                    "label": 0
                },
                {
                    "sent": "So the question is what's the difference with an RN ends?",
                    "label": 0
                },
                {
                    "sent": "As I said, it's similar and all of them would have connections directly between this nonlinear hidden layer and the next nonlinear hidden there, whereas here you have this sort of trailing.",
                    "label": 0
                },
                {
                    "sent": "The recursion is before the pre activation, before the activation.",
                    "label": 0
                },
                {
                    "sent": "I mean, so it's really linear recursion, so that's the main difference I guess.",
                    "label": 0
                },
                {
                    "sent": "Other questions yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I guess you can think of it as a just in general a state space model to some extent.",
                    "label": 0
                },
                {
                    "sent": "But here, unlike an HMM generalized, this actual notion of the stochastic latent state here there's no interpretation of these hidden units as being stochastic.",
                    "label": 0
                },
                {
                    "sent": "There're feedforward hidden units that are deterministic, but to some extent, like most sort of sequential models.",
                    "label": 0
                },
                {
                    "sent": "Have shared something in that their sequential.",
                    "label": 0
                },
                {
                    "sent": "There's a latent state and you progress, and as you see as you get observations, you update your latent state, broadly speaking.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How do you say it well?",
                    "label": 0
                },
                {
                    "sent": "Since it's a generative Model 1 natural thing to do is to maximize the likelihood of the training example.",
                    "label": 0
                },
                {
                    "sent": "So optimize the, let's say, optimize the average of the log probability, or minimize so that we can maximize.",
                    "label": 1
                },
                {
                    "sent": "This will minimize minus that.",
                    "label": 0
                },
                {
                    "sent": "Because P of X is the product of emotional and then you have, so you would have a log of a product so that becomes that can become a sum of the log of each conditional and so here you really see this sort of predictive autoencoder nature, where each dimension K is predicted from the input.",
                    "label": 0
                },
                {
                    "sent": "But we made sure that we're predicting a dimension we are predicting it from all the inputs before case, so there's no direct connection.",
                    "label": 0
                },
                {
                    "sent": "There's no sort of information flowing from SK to its probability.",
                    "label": 0
                },
                {
                    "sent": "And that's essentially why it's a proper generative model, so we can compute this loss function exactly.",
                    "label": 0
                },
                {
                    "sent": "There's no approximations, unlike things like.",
                    "label": 1
                },
                {
                    "sent": "PBM's, for instance, as you'll see, computations are linear in both the number of hidden units and the number of input dimensions.",
                    "label": 0
                },
                {
                    "sent": "Because you get a deterministic loss function.",
                    "label": 0
                },
                {
                    "sent": "Then you could use or it's it's definitely easier to use 2nd order optimisers, though.",
                    "label": 0
                },
                {
                    "sent": "I mean there's not.",
                    "label": 0
                },
                {
                    "sent": "They're not used a whole lot.",
                    "label": 0
                },
                {
                    "sent": "I guess it seems like stochastic, especially on large data set, but that would be an option if we ever made even more progress in 2nd order optimizers.",
                    "label": 0
                },
                {
                    "sent": "As we see there is an extendable to a bunch of different observations.",
                    "label": 1
                },
                {
                    "sent": "So so yeah, so the question that we had is what order should we use for the inputs you have to specify some sort of ordering and it turns out that in unfortunately in many, as in many things random ordering works fine.",
                    "label": 1
                },
                {
                    "sent": "So I have some results about this.",
                    "label": 0
                },
                {
                    "sent": "But essentially we tried a bunch of different random orderings and the variation in the performance was very small, small compared to the uncertainty we had about the test error.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't mean that there wouldn't be benefits to optimizing it, but it definitely seems like if there is a benefit, it might not be.",
                    "label": 0
                },
                {
                    "sent": "You know that substantial.",
                    "label": 0
                },
                {
                    "sent": "So I have found that on images doing left to right and then top down traversal of the pixels did on some image dataset give better results than doing random ordering.",
                    "label": 0
                },
                {
                    "sent": "But it's not like.",
                    "label": 0
                },
                {
                    "sent": "If you don't do this, it doesn't work, or sorry.",
                    "label": 0
                },
                {
                    "sent": "Anyways, if you use a random ordering, it still works.",
                    "label": 0
                },
                {
                    "sent": "I guess what I mean to say.",
                    "label": 0
                },
                {
                    "sent": "I saw a hand in the yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, so that's a good question that I think one reason it is fairly robust.",
                    "label": 0
                },
                {
                    "sent": "And by the way, the fully visible Sigma belief net which is using logistic regressors, they've also found that optimizing the ordering wasn't that useful.",
                    "label": 0
                },
                {
                    "sent": "So that seems to be a fairly general statement in the case of using neural Nets, because they essentially like there, there is an ordering which might yield conditionals which are more complicated functions than they could be.",
                    "label": 0
                },
                {
                    "sent": "But if you haven't ruled that that has enough capacity, theoretically you could still model that fairly complicated function.",
                    "label": 0
                },
                {
                    "sent": "Maybe so I think maybe that's partly the reason why it doesn't matter so much the ordering.",
                    "label": 0
                },
                {
                    "sent": "Otherwise I don't have, unless, like you knew what was the order in which things were generated.",
                    "label": 0
                },
                {
                    "sent": "Then I'd exploit that, but I think this is, yeah, I don't have really strong understanding otherwise.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "You mean so that I explained just now or?",
                    "label": 0
                },
                {
                    "sent": "So I made a statement about distribution estimation being really hard in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "Is that what you're referring to?",
                    "label": 0
                },
                {
                    "sent": "So so yeah, so the idea here.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that when you are learning a distribution estimator, your P of X needs to be good everywhere in this space.",
                    "label": 0
                },
                {
                    "sent": "So if you assign, that's because it's a distribution that's normalized.",
                    "label": 0
                },
                {
                    "sent": "So if you assign high probability, so say you're learning the distribution of images of digits, but you're assigning high probability to a bunch of random images while you losing probability there because it needs to all sum to one.",
                    "label": 0
                },
                {
                    "sent": "So it means that if you had actually pushed down the probability of random images.",
                    "label": 0
                },
                {
                    "sent": "You would do better for the probability of real digits, so that's in that respect that it needs to essentially have a good output everywhere, unlike classification.",
                    "label": 0
                },
                {
                    "sent": "So you definitely have to model.",
                    "label": 0
                },
                {
                    "sent": "So the question is whether in high dimension, isn't this a problem?",
                    "label": 0
                },
                {
                    "sent": "Because you'll have essentially have that a lot of hidden layers, in particular, the computations are linear in a number in the dimensionality D, so I think being linear in the dimensionality is not so bad if it was quadratic in the dimensionality, then this would not really scale.",
                    "label": 0
                },
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "So I mean, it's in that context being linear in dimensionality.",
                    "label": 0
                },
                {
                    "sent": "That's pretty much as good as it gets.",
                    "label": 0
                },
                {
                    "sent": "Another question.",
                    "label": 0
                },
                {
                    "sent": "I think that's part of it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, that was definitely appealing thing.",
                    "label": 0
                },
                {
                    "sent": "Like you, you get a grain that's exact.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the question is whether the main appeal really for fully visible Bayesian network is the fact that there is no Lenten variable, and that's my conception of it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because ultimately otherwise it does provide.",
                    "label": 0
                },
                {
                    "sent": "It does correspond to a generative model, which is a bit awkward in the sense that you don't truly believe that this image was generated one pixel at a time, so you know the generative story doesn't really work, but still the basic assumption, which is that P of X is decomposable as a product conditional.",
                    "label": 0
                },
                {
                    "sent": "Is true and if you put enough capacity to each conditional, then perhaps you're not making so assumptions are that strong, but so in you know, with the benefit of not having latent variables, which in terms of the algorithms, means that we have deterministic gradient and efficiency.",
                    "label": 0
                },
                {
                    "sent": "That's great, but the generative model itself is a bit awkward.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so here what I'm showing is results on the benchmark from datasets I took from UCI.",
                    "label": 0
                },
                {
                    "sent": "They're all binary datasets, so binary observations.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go through each row.",
                    "label": 0
                },
                {
                    "sent": "Essentially what we did is that, so I guess the first row is the mixture of Bernoulli distribution productive Burnley, so it's the mixture model and what we did is that we're showing here the lock probability of the test set and then we subtracted.",
                    "label": 0
                },
                {
                    "sent": "The log probability of the mixture bernali to each.",
                    "label": 0
                },
                {
                    "sent": "So for this data set we took its actual log probability and we subtracted that to all of them.",
                    "label": 0
                },
                {
                    "sent": "That's why it's always zero for the mixture of Bernoulli.",
                    "label": 0
                },
                {
                    "sent": "This ought to have like a sort of basis of comparison, since lock probabilities in absolute terms can vary a lot depending on the nationality of the input, and so on.",
                    "label": 0
                },
                {
                    "sent": "An I mean, the main message here.",
                    "label": 0
                },
                {
                    "sent": "So here we have fully visible segment belief net, and here you have name and the main takeaway is that most of the bold entries are in this role.",
                    "label": 0
                },
                {
                    "sent": "Like it in general, it does really well.",
                    "label": 0
                },
                {
                    "sent": "It tends to outperform the other alternatives.",
                    "label": 0
                },
                {
                    "sent": "This these here are essentially undirected graphical models based on the PBM and variations of it.",
                    "label": 0
                },
                {
                    "sent": "But there are small PBM such that this is not going to make sense for some of you.",
                    "label": 0
                },
                {
                    "sent": "I'll see it you this is smaller games where you can still compute the partition function you'll.",
                    "label": 0
                },
                {
                    "sent": "This will make more sense if you remember it tomorrow when Aaron talks about undirected graphical models.",
                    "label": 0
                },
                {
                    "sent": "So and the other experiment, the other experiment here is what we did for these three datasets we computed again this performance, but using a different random ordering.",
                    "label": 0
                },
                {
                    "sent": "So just to be clear, the random ordering before you do any training is determined.",
                    "label": 0
                },
                {
                    "sent": "A priority and it's not changed during training.",
                    "label": 0
                },
                {
                    "sent": "It's always the same fixed ordering.",
                    "label": 0
                },
                {
                    "sent": "Now what we've done here is we, I think, took five different random ordering train made under these five different orderings.",
                    "label": 0
                },
                {
                    "sent": "Optimizing the hyperparameters for each.",
                    "label": 0
                },
                {
                    "sent": "And we look at the performance on the test set.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, the variation is very small, so that's what I meant by random ordering.",
                    "label": 0
                },
                {
                    "sent": "Works fine.",
                    "label": 0
                },
                {
                    "sent": "It works fine because it's doing better than most other models.",
                    "label": 0
                },
                {
                    "sent": "And also as you change the ordering still randomly, you will not really gain by optimizing that.",
                    "label": 0
                },
                {
                    "sent": "Not a whole lot anyways.",
                    "label": 0
                },
                {
                    "sent": "I saw a hand, yes.",
                    "label": 0
                },
                {
                    "sent": "So they are, well, we treated that as like each dimension is a boundary observation.",
                    "label": 0
                },
                {
                    "sent": "I think if you actually some of these datasets, actually there are like 1 hot vectors which we ignored entirely, which means you can do even better than that by taking that into account, their model, any of these models really don't take that into account.",
                    "label": 0
                },
                {
                    "sent": "OK, so they would sort implicitly have to learn this one hot encoding constraint.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is results on the binarized.",
                    "label": 0
                },
                {
                    "sent": "In this data set.",
                    "label": 0
                },
                {
                    "sent": "So this is the end.",
                    "label": 0
                },
                {
                    "sent": "This data set of handwritten digits where we've taken each pixel.",
                    "label": 0
                },
                {
                    "sent": "We normalize the intensity between zero and one, and we obtain the binary value by sampling from that intensity normalized intensity as if it was a probability of being equal to 1 OK, and so that gives digits that are bit noisy.",
                    "label": 0
                },
                {
                    "sent": "This is samples from a train name.",
                    "label": 0
                },
                {
                    "sent": "You can see that for a lot of cases you do recognize a particular digit.",
                    "label": 0
                },
                {
                    "sent": "But it's not a perfect model an I think during the rest of this week will probably see models that are better than that.",
                    "label": 0
                },
                {
                    "sent": "But what was really interesting when we publish this is that if we compare it to the best result which came from an restricted Boltzmann machine, this in approximation because of this partition function I talked about which you have to approximate, which is not the case for Nate.",
                    "label": 0
                },
                {
                    "sent": "This is the true log likelihood.",
                    "label": 0
                },
                {
                    "sent": "There are no approximations here.",
                    "label": 0
                },
                {
                    "sent": "It was really close and I think at the time we were sort of this.",
                    "label": 0
                },
                {
                    "sent": "I guess common belief that in order to achieve really good generative models, it could be that this.",
                    "label": 0
                },
                {
                    "sent": "Intractable normalization constant was kind of unavoidable.",
                    "label": 0
                },
                {
                    "sent": "You kinda needed it to get good luck like you were so good generative models and that sort of went against.",
                    "label": 0
                },
                {
                    "sent": "I guess this this belief because this was very, very close to that result here.",
                    "label": 0
                },
                {
                    "sent": "So we're really happy with this and that sort of picked my interest to pursue this even more later.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, and this is, well, that's fine.",
                    "label": 0
                },
                {
                    "sent": "I guess this is the instead of the samples.",
                    "label": 0
                },
                {
                    "sent": "I'm showing the probability, so now they are Gray level values where you can see even more what digit might have been.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So that's the basic model and I will extend in a few different ways in a few different directions.",
                    "label": 0
                },
                {
                    "sent": "So first I'll talk about how to extend it to real valued observations, because in most cases really your data is not binary, so it would be nice to have a real valued version of Nate.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This was with Benny Area and Murray.",
                    "label": 0
                },
                {
                    "sent": "And what I like.",
                    "label": 0
                },
                {
                    "sent": "So by the way is it?",
                    "label": 0
                },
                {
                    "sent": "Is it easier to see my slides now?",
                    "label": 0
                },
                {
                    "sent": "I used a white background based on the suggestion by someone.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Is that better now or seeing some yeses but not a whole lot.",
                    "label": 0
                },
                {
                    "sent": "Hopefully it's better.",
                    "label": 0
                },
                {
                    "sent": "So essentially, if you want to model other types of inputs well, all we need to do is change the nature of these conditional distributions.",
                    "label": 0
                },
                {
                    "sent": "So if XK is real valued, will have P of X KB, A distribution for real valued observations.",
                    "label": 0
                },
                {
                    "sent": "For instance, it could be a Gaussian mixture.",
                    "label": 0
                },
                {
                    "sent": "Each conditional could themselves be Gaussian mixture, which means that we would want the neural net given a subvector to output the.",
                    "label": 0
                },
                {
                    "sent": "Mean value of each Gaussian, the standard deviation, or the variance and the mixing weights between the different Gaussians.",
                    "label": 0
                },
                {
                    "sent": "You might think I'm wouldn't be simpler to just have a single Gaussian and just predict it's mean and its standard deviation.",
                    "label": 0
                },
                {
                    "sent": "It would be simpler, but it probably wouldn't be appropriate.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you think about the distribution for the first few dimensions, those very big, fairly important reasons to think that these are probably going to be multimodal distributions where you have multiple modes around different values in terms of pixels for the first dimension, you'll probably have a big Gaussian like bump around an activation of 0 or.",
                    "label": 0
                },
                {
                    "sent": "Intensity at zero and intensity of 1.",
                    "label": 0
                },
                {
                    "sent": "If you normalized everything.",
                    "label": 0
                },
                {
                    "sent": "If we used, you know more or less binary images, and in general that's just through the for the first conditionals, you just observe that they tend to be multi models.",
                    "label": 0
                },
                {
                    "sent": "Also using a mixture means that we can approximate a fair number amount of different distribution of different shapes by placing the Gaussians in different positions and playing with how what's the mixing weights of each and what's the variance of each.",
                    "label": 0
                },
                {
                    "sent": "So in this case I will see illustration where there was essentially be 3 Gaussians, and so you would predict each of these means and then you would also predict the variance of each and also the mixing weight.",
                    "label": 0
                },
                {
                    "sent": "So now for each conditional you have to output.",
                    "label": 0
                },
                {
                    "sent": "If you have mixture of 10 Gaussians you have to output 30 numbers, third 10 means 10 standard deviations and then mixing weights to make sure the standard deviations are positive.",
                    "label": 0
                },
                {
                    "sent": "We just passed this through a next potential.",
                    "label": 0
                },
                {
                    "sent": "Non linearity.",
                    "label": 0
                },
                {
                    "sent": "That means we're guaranteed that as standard deviations should be it's positive and to get the mixing weights that sum to one we pass the pre activation through a softmax.",
                    "label": 0
                },
                {
                    "sent": "And for the means, we just use a linear transformation from the hidden layer.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll just skip.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Over that an so he are results.",
                    "label": 0
                },
                {
                    "sent": "This is again a UCI datasets were comparing with a mixture of Gaussian and make sure factor analyzers which are essentially Gaussians that are mixture of Gaussians are better regularised and we were able to get.",
                    "label": 1
                },
                {
                    "sent": "So what you're seeing here is the average log probabilities, so you get higher numbers here for the real value Nate with mixture of Gaussians.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We looked at modeling natural image patches, so we take a large photograph of a natural scene and then we extract from its small patches from anywhere and it becomes our training set and maybe we take two different images and have a training, validation and also another image for the test set.",
                    "label": 0
                },
                {
                    "sent": "And here we are comparing with a mixture of Gaussian.",
                    "label": 0
                },
                {
                    "sent": "This comes from a paper where they did a really, really good job at carefully training a mixture of Gaussian and we were fairly competitive.",
                    "label": 0
                },
                {
                    "sent": "Though we didn't outperform them, were fairly close.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is for spectrograms of speech signal, where again we got good test log compared to a mixture of Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, let's move to another type of observation.",
                    "label": 0
                },
                {
                    "sent": "Multinomial observations.",
                    "label": 0
                },
                {
                    "sent": "So, example of multinomial observation would be in natural language processing a sequence of words.",
                    "label": 0
                },
                {
                    "sent": "So normally the way we represent text data, the vast majority of ways that models will manipulate text data in machine learning is that you would first agree on a vocabulary that is a sequence of words.",
                    "label": 0
                },
                {
                    "sent": "Often fairly large and 10s of thousands or hundreds of thousands or even more, and your series of word.",
                    "label": 0
                },
                {
                    "sent": "What you'll do is that you essentially replace the word by what is the position of that word in the vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So you replace each you sensually have a mapping from his string to an integer from one to the number of words in the vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Often we add a word which is the out of vocabulary word that is for any other word which does not belong to my original set of words, I'll map them to the same integer.",
                    "label": 0
                },
                {
                    "sent": "OK, so now what we have really for modeling a stream of text is a series of these integers, and multinomial distribution would be appropriate for modeling such integers.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, what do we do?",
                    "label": 0
                },
                {
                    "sent": "Well, we just change how we deal with taking the value of a hidden layer for conditional and transforming that into an output distribution for that dimension.",
                    "label": 1
                },
                {
                    "sent": "And so one thing we could do is just use a softmax.",
                    "label": 0
                },
                {
                    "sent": "We could take the hidden layer, multiply this by matrix passes through softmax.",
                    "label": 0
                },
                {
                    "sent": "This gives us the probability of assigning of sampling each of the different words in the vocabulary.",
                    "label": 0
                },
                {
                    "sent": "If you have a very large vocabulary that computation, so essentially computing, say 100,000 different probabilities for.",
                    "label": 0
                },
                {
                    "sent": "Each position could be very expensive, so here we use a different approach which has been used in for language modeling.",
                    "label": 1
                },
                {
                    "sent": "For instance, you'll probably.",
                    "label": 0
                },
                {
                    "sent": "This will probably come back when you get lectures on natural language, but I'll describe it here since I sort of need it here and what we did instead is to get more efficient computation of the probability of the next word given the previous ones we've essentially used.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's known as a tree structured multinomial.",
                    "label": 1
                },
                {
                    "sent": "So here it works.",
                    "label": 0
                },
                {
                    "sent": "So imagine I have a conditional where I've observed the previous words in some stream, so I'll explain later why we take it in random order.",
                    "label": 1
                },
                {
                    "sent": "But we have N dog the the end cap, and for some reason all my examples involve cats.",
                    "label": 0
                },
                {
                    "sent": "Not quite sure why, but I don't even like cats.",
                    "label": 0
                },
                {
                    "sent": "I don't know what that is about, so.",
                    "label": 0
                },
                {
                    "sent": "Here you would have a hidden layer which essentially takes that, as in putting computer hidden layer and now what I want.",
                    "label": 0
                },
                {
                    "sent": "Imagine the next word is cat.",
                    "label": 0
                },
                {
                    "sent": "OK now I want to compute this probability under my model, but I don't want to compute the full softmax.",
                    "label": 0
                },
                {
                    "sent": "So one approach is to decompose these probabilities under a tree and as we'll see this is a trick that will allow us to compute probabilities under that tree in log arhythmic times where it's log rhythmic in the number.",
                    "label": 0
                },
                {
                    "sent": "Of words.",
                    "label": 0
                },
                {
                    "sent": "So what I do is I take all my words in my vocabulary.",
                    "label": 0
                },
                {
                    "sent": "I've conveniently chosen a vocabulary of size 8 which fits comfortably my slides, and now what I want is from my hidden layer.",
                    "label": 0
                },
                {
                    "sent": "I want my output matrix to essentially predict what's the probability of, say, turning left or turning right.",
                    "label": 0
                },
                {
                    "sent": "So you pick one option.",
                    "label": 0
                },
                {
                    "sent": "It's either turning left or turning right.",
                    "label": 0
                },
                {
                    "sent": "Don't quite remember which one I chose in the slides, will discover that together in a few minutes.",
                    "label": 0
                },
                {
                    "sent": "And then in this case, the probability of a given word is going to be what's the probability of starting here and reaching cat.",
                    "label": 0
                },
                {
                    "sent": "For our example here.",
                    "label": 0
                },
                {
                    "sent": "So that's the probability of going left, right and then right.",
                    "label": 0
                },
                {
                    "sent": "OK, so specifically.",
                    "label": 0
                },
                {
                    "sent": "The probability of cap under this model would only involve using the connections with these three internal nodes, and none of the other connections in my output layer, so they involve only three as opposed to 8 OK, so that's how it becomes like a rhythmic because I've used a tree which is balanced and binary.",
                    "label": 0
                },
                {
                    "sent": "If I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes, the question is yes.",
                    "label": 0
                },
                {
                    "sent": "This corresponds to a sort of binary encoding.",
                    "label": 0
                },
                {
                    "sent": "So OK, so this is related to, I guess the general question of how do you get this tree?",
                    "label": 0
                },
                {
                    "sent": "Essentially in our experiments random was being so friendly with us that we use a random tree random balance tree where words are just assigned randomly to leaves in that binary tree.",
                    "label": 0
                },
                {
                    "sent": "You could do something better and computationally by using a tree that's based on Huffman tree, which is based on the marginal probability of each word, so that you have a smaller path forwards that are more frequent.",
                    "label": 0
                },
                {
                    "sent": "And longer paths for words are less frequent, and that would be much more efficient.",
                    "label": 0
                },
                {
                    "sent": "But we here we used, which essentially the worst case, which is a binary assignment of words to treat two leaves, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah yes, so would it be better to have some sort of semantic structure that direct how we construct the tree?",
                    "label": 0
                },
                {
                    "sent": "Yes, and there is definitely experiments that showed, at least in the context of language modeling, that you get better results, better reflect cities we play with that a little bit with this model, which I haven't finished explaining, but which is more topic model and we didn't get much success.",
                    "label": 0
                },
                {
                    "sent": "But I think it's because we're not doing language modeling here, but we're actually doing topic modeling, but I haven't addressed that yet.",
                    "label": 0
                },
                {
                    "sent": "That's coming up.",
                    "label": 1
                },
                {
                    "sent": "So finishing my example, I want these probabilities, but I'm parrot rising the probability of turning right.",
                    "label": 0
                },
                {
                    "sent": "As it turns out, and so the probably turning left is just 1 -- 2.",
                    "label": 0
                },
                {
                    "sent": "Probably turning right and I'll just model those as just the sigmoid on the linear transformation associated with that internal node.",
                    "label": 0
                },
                {
                    "sent": "That's how I get a distribution over an items an words, but where I can compute the probability of a given word in log arhythmic time?",
                    "label": 0
                },
                {
                    "sent": "OK, so that makes this algorithm since I have a lot of conditionals to evaluate, it's going to make it much more efficient.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yes, so we're going to use it in the context of essentially topic modeling.",
                    "label": 0
                },
                {
                    "sent": "That is, we have a bag of words where we don't have the ordering of the words, mainly because we are only interested in the general semantics behind this document.",
                    "label": 1
                },
                {
                    "sent": "We're not interested in modeling the syntax, so which mainly dominates the order of the words.",
                    "label": 0
                },
                {
                    "sent": "So essentially we have a sequence of words that we've shuffled in random ordering, and then we're trying to predict them sequentially based on the previous word in that random stream.",
                    "label": 0
                },
                {
                    "sent": "And what it means is that the best thing you can do is just figure out after seeing a few words.",
                    "label": 0
                },
                {
                    "sent": "Oh, this document seems to be about politics, so it seems to be about support.",
                    "label": 0
                },
                {
                    "sent": "So it seems to be about so essentially the best you can do really with a model like this is extract hidden units that are topical that tend to reflect the general topic of discourse.",
                    "label": 0
                },
                {
                    "sent": "And because it's also a variable number of words, unlike before, I'm going to have more parameter sharing.",
                    "label": 1
                },
                {
                    "sent": "So now the connections between a word and the hidden units is not going to depend on the position where it is because anyways, there's a variable number of words, so I couldn't have one vector for each position because the position is unbounded and also the order doesn't matter.",
                    "label": 1
                },
                {
                    "sent": "The position in this ordering doesn't matter 'cause it was random, so we share all of these weights.",
                    "label": 0
                },
                {
                    "sent": "And also we always use the same output weights, no matter which conditional we're using.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, exactly yes.",
                    "label": 0
                },
                {
                    "sent": "And then we're converting it into a random stream of words by picking without replacement from that bag of word.",
                    "label": 0
                },
                {
                    "sent": "OK, we actually shuffled the word order at every update and in terms.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Evaluation we looked at things like the perplexity.",
                    "label": 0
                },
                {
                    "sent": "The perplexity is essentially the exponential of the negative average luck probability, and it's actually here normalized by the size of documents, and I'm comparing with a few models or LD is a very popular topic.",
                    "label": 0
                },
                {
                    "sent": "Model is inspired by the PBM and sort of a topic model version of the PBM, and this is document made our new model or getting so lower is better here, and we got, frankly quite surprising low perplexity results.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can use.",
                    "label": 0
                },
                {
                    "sent": "We can essentially construct a new representation for a document by passing it all the words and looking at what is the value of the hidden layer given all of these words and we can use that to do some retrieval in it, assuming that this representation is going to correlate with semantic similarity and we compare with another model which is again the replicated softmax with state of the art at the time we did a bit better.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One thing we can do also is that so in this model I essentially have a matrix that is the number of hidden units times the number of words in my vocabulary, and when I'm observing a word in my sequence, what I'm taking.",
                    "label": 0
                },
                {
                    "sent": "What I'm doing essentially is that I'm taking one of the corresponding columns of that matrix, and I'm adding it into the pre activation vector and also what I have is that for each hidden unit I have a notion of connectivity with all of the words.",
                    "label": 0
                },
                {
                    "sent": "OK, so for one roll of that matrix can essentially be used to identify which are the words, which tend to increase the activation of that units.",
                    "label": 0
                },
                {
                    "sent": "Those will be the words with the hyest positive connectivity.",
                    "label": 0
                },
                {
                    "sent": "The highest weight, and that's essentially what I'm used showing here.",
                    "label": 0
                },
                {
                    "sent": "So for three hidden units.",
                    "label": 0
                },
                {
                    "sent": "So before we looked at what are the words that have the so we sorted the words based on in decreasing order of their connection weight with a hidden unit and we're showing you the top 10 here.",
                    "label": 0
                },
                {
                    "sent": "And you see that we have a hidden it, which seems to be more sensible and seems to be detecting realigion topic here.",
                    "label": 0
                },
                {
                    "sent": "This more science sports security, so it has really picked up.",
                    "label": 0
                },
                {
                    "sent": "These semantic topics from these documents, even though you know there was no labeled data for that.",
                    "label": 0
                },
                {
                    "sent": "So that's when we look at the yes.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "That's a good I. Yeah, that's maybe there's a back story to this.",
                    "label": 0
                },
                {
                    "sent": "Probably not Clipper though.",
                    "label": 0
                },
                {
                    "sent": "So they're not perfect.",
                    "label": 0
                },
                {
                    "sent": "Oh, is that right?",
                    "label": 0
                },
                {
                    "sent": "Maybe there's a connection with that's The funny thing.",
                    "label": 0
                },
                {
                    "sent": "Sometimes with neural Nets they actually discover things you don't know about, so I don't know.",
                    "label": 0
                },
                {
                    "sent": "That's a good question, maybe.",
                    "label": 0
                },
                {
                    "sent": "Maybe there is a connection.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say what?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So OK, so this is observing.",
                    "label": 0
                },
                {
                    "sent": "This is inspecting the rows of that matrix.",
                    "label": 0
                },
                {
                    "sent": "Now what I can do also is for one word I can look at the vector column vector associated with it, which is the vector of its weights with all the hidden units.",
                    "label": 0
                },
                {
                    "sent": "And I could think OK this is going to be my vector representation for that word.",
                    "label": 0
                },
                {
                    "sent": "And now what I could do is I take that vector representation for word like say weapons.",
                    "label": 0
                },
                {
                    "sent": "I'm going to look at where are the other words whose representation is closest.",
                    "label": 0
                },
                {
                    "sent": "And here I'm showing the five nearest neighbors and for a few words again.",
                    "label": 0
                },
                {
                    "sent": "There's some high level semantic similarity that seems to have been learned.",
                    "label": 0
                },
                {
                    "sent": "Weapons, weapon shooting, firearms and so on.",
                    "label": 0
                },
                {
                    "sent": "Windows, Dos, Microsoft, and so on.",
                    "label": 0
                },
                {
                    "sent": "I'll skip over that.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So another experiment we did is, and now we're going to start about how we can use in unsupervised learning method to regularize and improve the generalization performance of a supervised model.",
                    "label": 0
                },
                {
                    "sent": "So in this case here, what will do is that we'll assume we have documents, but we also have labels with them that we can use.",
                    "label": 0
                },
                {
                    "sent": "And so now we're going to have a joint model over both X&Y and Argentum model.",
                    "label": 0
                },
                {
                    "sent": "What it does is that it generates all of X and then once it has all of X it generates why which is going to be a classification label, and we do this just by adding a final hidden layer which uses again all the same connections that we've been using for these hidden units and then from that I'm going to feed this into a linear transformation and a softmax that's going to be output distribution for the class and because now there's this weight sharing between these connections.",
                    "label": 0
                },
                {
                    "sent": "And all the connections use here.",
                    "label": 0
                },
                {
                    "sent": "If I train on both, predicting X&YI will effectively be doing some sort of unsupervised regularization.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More explicitly, what you'll have as an objective is that where this shouldn't be an equal should be approximation, but essentially will be optimizing minus the log of the probability of the label given X plus.",
                    "label": 0
                },
                {
                    "sent": "A certain fraction of minus the love, the probability of each word given the previous words in that sequence.",
                    "label": 1
                },
                {
                    "sent": "So we can see that this does not depend on why it does depend on the data set, but.",
                    "label": 0
                },
                {
                    "sent": "So you can think of it as this, this being a sort of regularizer, but that is data dependent.",
                    "label": 0
                },
                {
                    "sent": "It's dependent on the specific data distribution and essentially saying that the kind of features that should be useful at predicting Y should probably also be useful at predicting other words.",
                    "label": 0
                },
                {
                    "sent": "If I showed you a subset of that document, yes.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, so I'm assuming yes, I'm using a notation which assumes a particular ordering.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think in this case in our experiment, every time we would sort of change the ordering as a trend.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Let's see so in terms of like so the question is how you can compute a proper perplexity without the word ordering so.",
                    "label": 0
                },
                {
                    "sent": "So actually what we did in the paper when we computer perplexity is that we use just one random ordering, computed the probability for that what we could have done also, and actually, I'll talk about this.",
                    "label": 0
                },
                {
                    "sent": "'cause this is going to come up later.",
                    "label": 0
                },
                {
                    "sent": "We could actually create an assemble where we're going to consider a set of orderings an actually get probabilities for each of them, and then assemble them.",
                    "label": 0
                },
                {
                    "sent": "That's something else we could do, but given an ordering it does give you an given the number where you generate it does give you a proper.",
                    "label": 0
                },
                {
                    "sent": "Probability distribution, but you're you're right.",
                    "label": 0
                },
                {
                    "sent": "You're right, there is this sort of random variable which I'm more or less ignoring just sampling one order stochastically.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so I don't think this is anywhere as good as a neural net language model, for instance, because this you know syntax is a lot of what explains the structure of words in sentences, and this does not model this at all.",
                    "label": 0
                },
                {
                    "sent": "The only models, the semantics.",
                    "label": 0
                },
                {
                    "sent": "It would, but then because of the weight sharing, actually you could shuffle.",
                    "label": 0
                },
                {
                    "sent": "The context it would give you the same hidden layer because the weights are shared so far because of that.",
                    "label": 0
                },
                {
                    "sent": "So we are currently looking at mixing this with the language model to have a sort of topic hidden layer combined with more syntactic.",
                    "label": 0
                },
                {
                    "sent": "You know with a window or something hidden layer.",
                    "label": 0
                },
                {
                    "sent": "That's something we have been looking at but but the main point is that this is not a good language model, it's really that's why I'm sending it more as a topic model.",
                    "label": 0
                },
                {
                    "sent": "Now, the experience we're going to do here is that we're going to take an image.",
                    "label": 0
                },
                {
                    "sent": "An well, actually going to classify images.",
                    "label": 0
                },
                {
                    "sent": "So we're going to turn them into a bag of visual words.",
                    "label": 1
                },
                {
                    "sent": "And the way this is done.",
                    "label": 0
                },
                {
                    "sent": "Usually this is not something we invented that's fairly common practice.",
                    "label": 0
                },
                {
                    "sent": "We take the image a splice it into into patches.",
                    "label": 0
                },
                {
                    "sent": "We do.",
                    "label": 0
                },
                {
                    "sent": "Some K means clustering of all these patches, and are each Patch is going to replace be replaced by the ID of the cluster to which it belongs.",
                    "label": 0
                },
                {
                    "sent": "And so each cluster is essentially a visual word, an now, once you've replaced the patches by this cluster IDs, you have a sequence of multinomial zan of so called visual words.",
                    "label": 0
                },
                {
                    "sent": "And that's actually the input will be dealing with and not just that.",
                    "label": 0
                },
                {
                    "sent": "But this is a data set where we have both images, an annotation, so people have tagged words to each image and will add that as well into the stream of words.",
                    "label": 1
                },
                {
                    "sent": "So you have a sequence of visual words, an actual words.",
                    "label": 0
                },
                {
                    "sent": "Into a the same joint vocabulary.",
                    "label": 0
                },
                {
                    "sent": "And what we'll do is we'll try to model like this.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think one of the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interesting experiment is this one here, which essentially confirms that using this data dependent regularization is useful.",
                    "label": 0
                },
                {
                    "sent": "So unfortunately probably don't see a whole lot, but essentially these two curves use a value of Lambda that is non 0, but it's tuned so the other property of this is that.",
                    "label": 0
                },
                {
                    "sent": "Instead of assuming.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At Lambda, is equals to one, which would be the case if if this this would be equal to that if Lambda was equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So this is not so why this is not an = so it's it's only.",
                    "label": 0
                },
                {
                    "sent": "From this we derive this, but we decided to wait than unsupervised part.",
                    "label": 0
                },
                {
                    "sent": "So if you tune this, there is a benefit at.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using the unsupervised regularization term, you see that these two curves here, which corresponds to those that vary.",
                    "label": 0
                },
                {
                    "sent": "And same thing here tends to yield higher accuracy, so this is the accuracy of classification accuracy of the model.",
                    "label": 0
                },
                {
                    "sent": "So this is an example where having an unsupervised learning signal actually helped in terms of improving the generalizations overfitting less.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can do the same kind of inspection of the hidden units where I can and I can actually look at All in all the hidden units.",
                    "label": 0
                },
                {
                    "sent": "I'll pick a few which have the stronger connection.",
                    "label": 0
                },
                {
                    "sent": "Output connection with the class highway.",
                    "label": 1
                },
                {
                    "sent": "So because were unsupervised learning, I have connections with between classes and the units, and then I'll take each of these and I'll look at the sum of the weights of the different words and look at those that are most strongly connected and same thing with the visual words.",
                    "label": 1
                },
                {
                    "sent": "Where to visualize the visual words.",
                    "label": 0
                },
                {
                    "sent": "I've taken the cluster associated with the visual word an I've shown here a subset of 16 patches and you can see for Hwy you can patches that kind of look like roads.",
                    "label": 0
                },
                {
                    "sent": "With words that are similar to like car and road and so on.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For sailing it looks a bit more like water.",
                    "label": 0
                },
                {
                    "sent": "There are some areas where you have more like a brick wall here.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or street where you have things that look like buildings and when you see the word building that seems to correlate with with street and be useful to predict, St would learn again something that's a bit more higher level in terms of feature.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Any questions at this point, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I yeah.",
                    "label": 0
                },
                {
                    "sent": "So what I did is that I took these hidden units and then I have connections between a hidden unit and all the cluster IDs.",
                    "label": 0
                },
                {
                    "sent": "I took the four top ones an for each to illustrate them I just pick some random patches from that.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, OK, so now I'm going to talk about how we can take this.",
                    "label": 0
                },
                {
                    "sent": "Name model which so far as really only been 1 hidden layer, so between the prediction and the input there was always 1 hidden there.",
                    "label": 0
                },
                {
                    "sent": "There was a sequence of hidden layers as I went through the predictions that through the conditionals but from the input to the output there was only one hidden there.",
                    "label": 0
                },
                {
                    "sent": "So how can we make this model deep?",
                    "label": 0
                },
                {
                    "sent": "This is work with Benyan even more.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, alright, so how could what does it mean to make this model deep model?",
                    "label": 0
                },
                {
                    "sent": "So this is nail.",
                    "label": 0
                },
                {
                    "sent": "I've put a bit more space between the layers.",
                    "label": 0
                },
                {
                    "sent": "So what I would do is.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that I would essentially add a hidden.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They're like this and I'm showing you this illustration to show that I'm really good at Keynote, and also to show you that what I mean by this is that the connections between adjacent hidden layers are actually the same for.",
                    "label": 0
                },
                {
                    "sent": "Again, because I'm using the same neural net for each conditional and we have again the same weight sharing here.",
                    "label": 0
                },
                {
                    "sent": "Now this graph is stuck.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To be a bit complicated, so instead I'll use a color coding.",
                    "label": 1
                },
                {
                    "sent": "So essentially within one layer the connections are the same color are the same weights, the same set of parameters, so these are shared.",
                    "label": 0
                },
                {
                    "sent": "These all black, so these are shared, and these are actually different for each dimension.",
                    "label": 0
                },
                {
                    "sent": "So the problem with doing it this way is that.",
                    "label": 0
                },
                {
                    "sent": "The amount of computations we have to do to get all of our outputs has now become.",
                    "label": 0
                },
                {
                    "sent": "Squared in the number of hidden units.",
                    "label": 0
                },
                {
                    "sent": "The reason is that, well, to get each of these D outputs, I need to compute each of the hidden layers of the topmost hidden layers.",
                    "label": 1
                },
                {
                    "sent": "And computing each of those requires.",
                    "label": 1
                },
                {
                    "sent": "So assuming all hidden layers have the same size as well, that would be 8 squared and I have D of them, so I have D * 8 squared complexity an.",
                    "label": 0
                },
                {
                    "sent": "Since essentially the complexity, so you know we are used with moral Nets to deal with things that are more less quadratic in the input size in the sense that the hidden layer tends to have a dimensionality that's compatible with the input layer, not always, but it's often a fraction of.",
                    "label": 0
                },
                {
                    "sent": "But now having something that would essentially be D to the three is going to be impractical, so that's why the reason why this basic and direct application of just extending into a deep network is not super appealing.",
                    "label": 0
                },
                {
                    "sent": "To solve this problem, we're going to make it even worse, and we're actually also going to change the training criteria so that instead of training for just a single fixed ordering, as it is usually foreign aid, where going to train it for any ordering.",
                    "label": 0
                },
                {
                    "sent": "So that is I'm going to, you know, sometimes I'm going to have this particular ordering of the word of the dimensions, so think of the binary case for now going back to the binary, Nate.",
                    "label": 0
                },
                {
                    "sent": "But sometimes I'm going to produce.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The 4th input and then the third.",
                    "label": 0
                },
                {
                    "sent": "That's a different ordering, OK?",
                    "label": 0
                },
                {
                    "sent": "What's convenient with this?",
                    "label": 0
                },
                {
                    "sent": "If we achieve this, is that I could do this kind of conditional inference if I had missing inputs with arbitrary patterns, because if I have a model that's been trained on any arbitrary missing pattern, that could be a useful inference machine and say, filling in missing inputs.",
                    "label": 0
                },
                {
                    "sent": "But there is, you know, there's a whole lot of ordering, so that seems like there was.",
                    "label": 0
                },
                {
                    "sent": "That would complicate things a little bit.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Never imagined, let's consider now just a single conditional.",
                    "label": 0
                },
                {
                    "sent": "So let's convince.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consider this conditional here the conditional of the third dimension given the first 2.",
                    "label": 0
                },
                {
                    "sent": "Now imagine that I've performed this forward propagation already, while the only thing that's missing from getting the conditional for.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The fourth dimension is to just take that and multiply by different set of weights for the fourth dimension, and very cheaply.",
                    "label": 0
                },
                {
                    "sent": "I would get the output.",
                    "label": 0
                },
                {
                    "sent": "For the ordering where the fourth dimension actually comes after the first and the second OK, so now because I'm also in this setting where I want to train all orderings, what I could do instead instead of being stochastic, why pick pick one ordering and I commit to it?",
                    "label": 1
                },
                {
                    "sent": "And I trained all of its conditionals instead.",
                    "label": 0
                },
                {
                    "sent": "I'll pick an example and I'll split the input vector into random.",
                    "label": 1
                },
                {
                    "sent": "These actually I'll shuffle the examples as split in random position, and then what I'll do is that.",
                    "label": 0
                },
                {
                    "sent": "Our condition and what is left to the right lower left left of this link position and I'll predict all of the other inputs at the output.",
                    "label": 1
                },
                {
                    "sent": "Everything that's right of this splitting position and what I'm effectively doing here is that I'm not training all the conditionals for one ordering, but I'm training several conditionals for several orderings at the same time, and the kind of computations are required here are very similar to a regular feedforward network, it's just essentially have my input.",
                    "label": 1
                },
                {
                    "sent": "But some of them are.",
                    "label": 0
                },
                {
                    "sent": "Essentially, it's as if there were zero hidden layer hidden there, and I predict an output.",
                    "label": 0
                },
                {
                    "sent": "Kinda like an auto encoder.",
                    "label": 0
                },
                {
                    "sent": "But now I actually don't care about these outputs, only care about those.",
                    "label": 0
                },
                {
                    "sent": "OK, so the computational complexity of a single update is now going to be similar to a regular autoencoder.",
                    "label": 0
                },
                {
                    "sent": "OK, now there's just one little thing about this, which is that.",
                    "label": 0
                },
                {
                    "sent": "As I said, when I'm conditioning on only these two things, it's as if for this hidden layer that X3 isn't there.",
                    "label": 0
                },
                {
                    "sent": "Sorry X3 is actually equal to zero and four.",
                    "label": 0
                },
                {
                    "sent": "X4 is equal to 0, but this might not be the case.",
                    "label": 0
                },
                {
                    "sent": "I just, you know, by the way I constructed model it so it turns out that being equal to 0 and being missing is the same thing for the neural net.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the way to fix this is that we're going to add for each dimension a bit, which is going to indicate whether the input is present or not.",
                    "label": 1
                },
                {
                    "sent": "So this would be one.",
                    "label": 0
                },
                {
                    "sent": "This would be one, and there would be a zero and a zero here.",
                    "label": 0
                },
                {
                    "sent": "OK, so in effect this hidden layer has two vectors, the vectors X where I've set to 0 all the dimensions on which I'm not conditioning, and this binary mask essentially.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "So we call those conditioning weights and this is so that the hidden layer can distinguish between an input being absent, but zero and input being present but being zero.",
                    "label": 0
                },
                {
                    "sent": "Sorry being absent and having any value and being present but being 0.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so unfortunately that doesn't solve the complexity of forgiven ordering computing P of X if on computer of XI need all the conditionals.",
                    "label": 0
                },
                {
                    "sent": "So I will have a complexity which is squared in the in the size of the hidden layers times the number of inputs.",
                    "label": 0
                },
                {
                    "sent": "But that's still somewhat tractable, so it's not so bad if I just want to check the performance as a generative model.",
                    "label": 0
                },
                {
                    "sent": "I can I can still do this, it's not exponential, for instance.",
                    "label": 0
                },
                {
                    "sent": "Now at Test time, what is interesting here is that because I'm training on all orderings, there isn't one specific ordering I'm supposed to choose when I'm computing P of X, because it's been trained on all of them, and now there's something that will seem like a bug, which is that if I choose one ordering and I compute P of X and I pick another ordering to compute P of X based on that ordering, these two things will probably not match because the model isn't constructed in a way to make sure that all of the conditionals are consistent from one another.",
                    "label": 0
                },
                {
                    "sent": "So that seems like a bug, but it actually can be used as a feature in the sense that as I'm using the model, I can just generate all of these likelihoods from a bunch of different orderings, make the compute the average of those, and essentially get and assemble from scratch as I need it.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have changed just one model, I can construct an example of arbitrary size, really, because there's you know an exponential number of orderings.",
                    "label": 0
                },
                {
                    "sent": "And as we'll see, this actually gives better log probabilities when we measure measure performance.",
                    "label": 0
                },
                {
                    "sent": "All.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's look at results.",
                    "label": 0
                },
                {
                    "sent": "So this is on the binarized MNIST data set here.",
                    "label": 0
                },
                {
                    "sent": "These are all of the name models that were trained in the orderly fashion, where we essentially for each update we pick an ordering.",
                    "label": 0
                },
                {
                    "sent": "And then we take that is, we pick the input, we sample in ordering.",
                    "label": 0
                },
                {
                    "sent": "We split the input into two at a random position, and then we predict what is the right based on what is on the left.",
                    "label": 0
                },
                {
                    "sent": "And this is when we evaluate the test log probability.",
                    "label": 0
                },
                {
                    "sent": "And so we're getting, you know, results are OK, but they're not actually worse than what we had with regular made.",
                    "label": 0
                },
                {
                    "sent": "However, when we start constructing these assembles, we get a significant boost would actually exploit the fact that we've trained on multiple orderings, and we get log probabilities that are now much better than the name model we had.",
                    "label": 0
                },
                {
                    "sent": "It's better than the PBM model that I showed before, and it's compatible to a deep belief net.",
                    "label": 0
                },
                {
                    "sent": "Which is one of these neural net models with stochastic latent variables.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One thing we can so these just showing samples which you know for most cases we can distinguish fairly easily a digit from it.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One thing, one interesting thing we can do is then we can hide part of the input and then because we've trained on all the orderings, what we can do is can generate sorry condition and all the dimensions that are not hidden, and then we sample what is in the little Patch here by going in some arbitrary order in that Patch and just see what it generates, how it fills in these missing inputs and you get things that are interesting.",
                    "label": 0
                },
                {
                    "sent": "For instance when there is ambiguity.",
                    "label": 0
                },
                {
                    "sent": "So for instance here.",
                    "label": 0
                },
                {
                    "sent": "B5, but this could also be a decent looking 6 if you just filled it like that or you have like a four versus something that looks more like a 9 here when you hide that part.",
                    "label": 0
                },
                {
                    "sent": "So it's doing a decent job.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is again results.",
                    "label": 0
                },
                {
                    "sent": "But now for natural image patches where we can see that with depth, we're getting better and better lock probability higher and higher.",
                    "label": 0
                },
                {
                    "sent": "And now we actually able to beat the mixture of Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any questions on this, yes.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when you questions about how do we pick the ordering within the red area, right?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That's a good question.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure well it's OK so.",
                    "label": 0
                },
                {
                    "sent": "The question is essentially to what extent does the order rate actually determine whether I'm going to sample this versus that, and that's a good question.",
                    "label": 0
                },
                {
                    "sent": "I don't know there shouldn't be a reason for this.",
                    "label": 0
                },
                {
                    "sent": "It could be that for some reason there it does introduce a bias, but that's a good question.",
                    "label": 0
                },
                {
                    "sent": "I don't know whether it was always the same ordering here.",
                    "label": 0
                },
                {
                    "sent": "Question and I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then the final experiment with this deep name model I'm going to go back to this problem of learning from pairs of images and text.",
                    "label": 1
                },
                {
                    "sent": "But now we're going to use a deep version of Doc Nate, so we do the same trick where we take the words there in random.",
                    "label": 0
                },
                {
                    "sent": "We were already putting them in random order.",
                    "label": 0
                },
                {
                    "sent": "What we do is we split the stream of words at a random position and we're going to predict all of the words after that spinning position condition on all the words before that's playing position.",
                    "label": 0
                },
                {
                    "sent": "In this context, because we, because we do the weight sharing at the output, we don't use that restructuring multinomial anymore.",
                    "label": 0
                },
                {
                    "sent": "We use a single softmax because there's only one softmax.",
                    "label": 0
                },
                {
                    "sent": "We can we have to compute since there's weight sharing for all position, then I really only have to compute one softmax and that's the probability I'll use to evaluate the probability for all of the upcoming words.",
                    "label": 0
                },
                {
                    "sent": "So now the softmax is a decent choice.",
                    "label": 0
                },
                {
                    "sent": "Here we use a GPU implementation.",
                    "label": 1
                },
                {
                    "sent": "This is on a much larger data set than what I showed before.",
                    "label": 0
                },
                {
                    "sent": "The Flickr MRM IR data set where there is what's interesting about it?",
                    "label": 1
                },
                {
                    "sent": "There's a million images that are not labeled.",
                    "label": 1
                },
                {
                    "sent": "They have annotation words, but they don't have classes and there's a much smaller subset.",
                    "label": 1
                },
                {
                    "sent": "I think 30K or 25K of label images.",
                    "label": 0
                },
                {
                    "sent": "So what we did in these experiments that we pre trained deep made on the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So not knowing what the label is trying to me as a generative model and then after we've done that for some time will use these weights to initialize a supervised neural net.",
                    "label": 0
                },
                {
                    "sent": "That will do the fine tuning and that gives us a representation for which we can do.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "These are the results, so this is results that will they will come up in the upcoming teepi ME Journal Paper which we learn today was accepted, so that's that's good.",
                    "label": 0
                },
                {
                    "sent": "So here you have a few different baselines, some of which are neural network based.",
                    "label": 0
                },
                {
                    "sent": "With this one is DBM and this is another kind of neural net which you might hear about.",
                    "label": 0
                },
                {
                    "sent": "This is from unlikely will be presenting during the summer school and hear what I'm showing is for instance that.",
                    "label": 0
                },
                {
                    "sent": "Death seems to play a role that is 1 hidden.",
                    "label": 0
                },
                {
                    "sent": "Layer seems to be worse than two or three for this amount of training.",
                    "label": 0
                },
                {
                    "sent": "Then if we did more pretraining, the results were improving even more, and eventually the third hidden layer would start to be significantly better with even more pre training.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And actually, you know we could have continued and do more pre training and it seems the performance would continue to improve.",
                    "label": 0
                },
                {
                    "sent": "So this is really showing this idea that training a generative model on a lot of unlabeled data can actually be useful in generalizing better.",
                    "label": 0
                },
                {
                    "sent": "It took a long time to generate these points.",
                    "label": 0
                },
                {
                    "sent": "I don't know to what extent the implementation is optimal, but it definitely needed a GPU.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is just a quantitative evaluation where what we did is that we presented this to the model.",
                    "label": 0
                },
                {
                    "sent": "So only visual words and what I'm showing here is what is the ground truth, annotation words or showing.",
                    "label": 0
                },
                {
                    "sent": "Here is the words that had the highest probability for that model, and those are cases where, frankly I think the ground truth was not particularly good and the model was actually doing a better job, so it's actually sort of denoising essentially the.",
                    "label": 0
                },
                {
                    "sent": "The annotation and I should say that now this is not as impressive as the most recent work which I've been generating actual sentences for.",
                    "label": 0
                },
                {
                    "sent": "Describing images where I think you'll probably hear about during the this week.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I'm not going to talk about a different model.",
                    "label": 0
                },
                {
                    "sent": "I'll end with this, which is again a note on quarter was actually it's going to be much more.",
                    "label": 0
                },
                {
                    "sent": "Close to the definition of another time quarter, he is so.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Home.",
                    "label": 0
                },
                {
                    "sent": "So the reason that made is a distribution estimator is essentially because it respects a property which I called the autoregressive property.",
                    "label": 0
                },
                {
                    "sent": "What I mean by that is that each output in the model is such that it depends only on previous inputs in some ordering.",
                    "label": 0
                },
                {
                    "sent": "A regular autoencoder isn't a distribution estimator or a generative model, exactly because it doesn't satisfy this so.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have a regular autoencoder.",
                    "label": 0
                },
                {
                    "sent": "There is nothing forbidding say that can output to have to get information directly from itself, for instance, and there's no ordering this notion of ordering that's being imposed here.",
                    "label": 0
                },
                {
                    "sent": "So one question that we had is, can we just take a regular autoencoder and try to modify it as simply as possible, as little as possible and still get something that we can interpret as a generative model that we can compute log probabilities for?",
                    "label": 0
                },
                {
                    "sent": "And just see what kind of performance we can get.",
                    "label": 0
                },
                {
                    "sent": "And essentially, to do this, the simple simple we found is that we are going to impose this auto regressive property by changing the connectivity of the network using essentially some masking.",
                    "label": 0
                },
                {
                    "sent": "So I'll show you a very simple recipe.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doing this.",
                    "label": 0
                },
                {
                    "sent": "I use a little picture just to illustrate that.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do so this model that we came up with we called it mastering quarter distribution estimator or made.",
                    "label": 1
                },
                {
                    "sent": "So what we want to do essentially is taken autoencoder like this where we have inputs of dimensionality three an at the output we produce a reconstruction and would like to have something like this which can be used for the output.",
                    "label": 0
                },
                {
                    "sent": "Can be used as conditional distributions according to some ordering or would just take the product and that would define a valid joint distribution.",
                    "label": 0
                },
                {
                    "sent": "So the first thing that we need to do is we need to pick an ordering for the inputs, which is going to determine the sequence of conditionals.",
                    "label": 0
                },
                {
                    "sent": "So here I randomly decided that the first element of this ordering is going to be X2, the second is X3 and the last one is X one that was picked randomly.",
                    "label": 0
                },
                {
                    "sent": "So what that means is that at the output.",
                    "label": 0
                },
                {
                    "sent": "I need this output to correspond to P of X2, which means it needs to depend on none of the inputs.",
                    "label": 0
                },
                {
                    "sent": "As you can see.",
                    "label": 0
                },
                {
                    "sent": "Indeed it has no connections.",
                    "label": 0
                },
                {
                    "sent": "X3 is the second one, so the output for the third output needs to be P of X3.",
                    "label": 0
                },
                {
                    "sent": "Given X2.",
                    "label": 0
                },
                {
                    "sent": "That is, it needs to be connected to hidden units, which themselves are connected to hidden units that are only connected to X2 and not other inputs.",
                    "label": 0
                },
                {
                    "sent": "And then the last one which is X1 needs to correspond to P of X1 given X2 and X3, which means that it needs to be connected to hidden units that are either connected to X2X3 or both, but not X1.",
                    "label": 0
                },
                {
                    "sent": "If I have this then I can use these as conditionals.",
                    "label": 0
                },
                {
                    "sent": "I could take the product of those that would define a valid joint distribution.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So what this means, as I said, is that this needs to be connected to zero inputs.",
                    "label": 1
                },
                {
                    "sent": "This to only the first input, and this one only to the second input.",
                    "label": 0
                },
                {
                    "sent": "Well, I could achieve this by just randomly deciding for each of the hidden units how many inputs in this ordering it's going to be connected to.",
                    "label": 1
                },
                {
                    "sent": "So for each unit what I've done here is that I've generated a number between one and the number of inputs minus one, and that's going to be this number.",
                    "label": 0
                },
                {
                    "sent": "How many inputs?",
                    "label": 0
                },
                {
                    "sent": "In that ordering, am I allowed to be connected to OK, so it's never simple 0 because that would be a useless hidden unit.",
                    "label": 0
                },
                {
                    "sent": "Would compute nothing and never sample 3 because then it's connected to everything, so I can't use it to produce any conditional at the output.",
                    "label": 0
                },
                {
                    "sent": "And once I've decided this one, I've you know uniformly and sorry, yeah, uniformly independently sample each of these integers where I can construct a mask which will respect this constraint.",
                    "label": 0
                },
                {
                    "sent": "This auto regressive constraint.",
                    "label": 0
                },
                {
                    "sent": "So all I do is that I construct a Max, wear the mask where the value is whether the hidden unit the corresponding hidden unit has a number which is greater or equal than the corresponding unit, and the layer below.",
                    "label": 0
                },
                {
                    "sent": "OK, so for instance this.",
                    "label": 0
                },
                {
                    "sent": "Role here that corresponds to the mask for that hidden for that hidden unit, and well as we can see, it's two is not as I'm sure you are learning right now too is not greater or equal than three.",
                    "label": 0
                },
                {
                    "sent": "So so here we have a zero.",
                    "label": 0
                },
                {
                    "sent": "That's what black means here, but it is greater or equal than one and two, so I have value in the mask of 1 here and one here.",
                    "label": 0
                },
                {
                    "sent": "And I do this for all connectivity's.",
                    "label": 0
                },
                {
                    "sent": "And actually, something you can trivially implement them parallelized by just in Theano.",
                    "label": 0
                },
                {
                    "sent": "It's trivial, for instance.",
                    "label": 0
                },
                {
                    "sent": "And you do this for all layers just using these numbers.",
                    "label": 0
                },
                {
                    "sent": "So coming up with these mask, once you've done this sampling assignment is super trivial.",
                    "label": 1
                },
                {
                    "sent": "And then when you the only they are changes.",
                    "label": 0
                },
                {
                    "sent": "When you're training this new masked autoencoder is that before you use the weight, you multiply by the mask.",
                    "label": 0
                },
                {
                    "sent": "And then you can use it to compute the pre activation at the next layer and when you back propagate you take into account the mask.",
                    "label": 0
                },
                {
                    "sent": "It's actually exactly like it was very similar to dropout, but instead of dropping out units, you're dropping connections, and for those you might know about Drop Connect, which is another version of drop out or an extension I guess of dropout which was proposed.",
                    "label": 0
                },
                {
                    "sent": "Well, that's essentially drop connect, but with the constraint that we're trying to do here is getting autoencoder whose output can be interpreted as a distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so it has more constraint in the way we generate these masks.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a few more details we could generate masks for each input.",
                    "label": 0
                },
                {
                    "sent": "OK, so that would more or less correspond to joining the multiple orderings, for instance.",
                    "label": 0
                },
                {
                    "sent": "As I said from training similar to dropout, and it's almost exactly like Drop Connect with some additional constraint.",
                    "label": 0
                },
                {
                    "sent": "At Test time again we can use and assemble by generating multiple masks and averaging the probabilities.",
                    "label": 1
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can generalize to any connections you could have connections that skip from the input to the output, or to other hidden units.",
                    "label": 0
                },
                {
                    "sent": "Generating the mask would just correspond to comparing the numbers.",
                    "label": 0
                },
                {
                    "sent": "The integers for the corresponding layers.",
                    "label": 0
                },
                {
                    "sent": "So it's actually very trivial to get arbitrary connections.",
                    "label": 0
                },
                {
                    "sent": "There's a few connected or related word I want to mention, so it turns out that the fully visible simulate belief net is made, but without the hidden units and a fixed mask.",
                    "label": 0
                },
                {
                    "sent": "That's essentially where it corresponds to.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned the work that Joshua and Sammy Benjo as well did in 2000.",
                    "label": 0
                },
                {
                    "sent": "On exploring the use of neural Nets for essentially replacing these conditionals in a fully visible belief network.",
                    "label": 0
                },
                {
                    "sent": "While this is exactly like 1 hidden layer version of this, but without the mass sampling, which is something that we explored for performing the average, if you're going to do research in deep learning, something you have to get used to some of your ideas you actually have done already.",
                    "label": 0
                },
                {
                    "sent": "It's just something that you have to get used to.",
                    "label": 0
                },
                {
                    "sent": "I take it as a compliment now.",
                    "label": 0
                },
                {
                    "sent": "If you're if none of your ideas and unbiased revenga probably doing something wrong, that's why I'm saying.",
                    "label": 0
                },
                {
                    "sent": "You did not pay me to say that, so I guess you're paying for the hotel right here, but anyways, so.",
                    "label": 0
                },
                {
                    "sent": "There might be a little bit conflict of interest, so OK.",
                    "label": 0
                },
                {
                    "sent": "They wanted to say is that made is actually when you use it when you code it and you compare it with an implementation of Nate, it's much more.",
                    "label": 0
                },
                {
                    "sent": "It's much faster.",
                    "label": 0
                },
                {
                    "sent": "The main reason is that matrix matrix multiplies are much faster than doing this sort of cumulative thing that you have to do to get the pre activation vectors.",
                    "label": 1
                },
                {
                    "sent": "And also there are much fewer nonlinearities.",
                    "label": 0
                },
                {
                    "sent": "So because they are effectively in aid as many hidden layers as they are input dimensions, you have number of nonlinearities scales with.",
                    "label": 1
                },
                {
                    "sent": "The number of inputs times the number of hidden units, whereas here it's just linear in the number of units total, so it's linear and number of units and not quadratic.",
                    "label": 0
                },
                {
                    "sent": "So that means that in some of the experiments were actually able to train a much bigger made model and it turns out that this was beneficial to make it competitive for the Binarized missed experiment that show and made it all so much faster to evaluate them deep made.",
                    "label": 1
                },
                {
                    "sent": "That is, in one forward pass I get all my conditionals, which I can multiply, get a probability, or get the log, and then some to get the log probability for deep made.",
                    "label": 0
                },
                {
                    "sent": "I do have to generate, sorry.",
                    "label": 0
                },
                {
                    "sent": "I do have to do as many passes in this deep neural net as they are conditionals.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned before, but to actually generate samples they are equally.",
                    "label": 0
                },
                {
                    "sent": "They are both as or not, not enough efficient.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, so there's some results on this UCI benchmark.",
                    "label": 0
                },
                {
                    "sent": "The main point of which is that results are fairly comperable to named an other models have been generated that have been proposed.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here this is experiencing binarized hymnists, where you have samples here which are somewhat similar and lock probabilities for made which are compareable to the other results.",
                    "label": 0
                },
                {
                    "sent": "I've been out there.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An that's it, so I guess I wanted to also present this just to give you an idea of, you know in that space fully visible basean Nets, that's already a space you can explore and try different things and just generally speaking.",
                    "label": 0
                },
                {
                    "sent": "With that right now, Jonathan modeling or distribution estimation is a fairly active and hot topic.",
                    "label": 1
                },
                {
                    "sent": "Days are essentially all of the different models that were proposed.",
                    "label": 0
                },
                {
                    "sent": "Almost most of them in the last two years.",
                    "label": 0
                },
                {
                    "sent": "Actually, I think you're going to see the majority of them be presented during the deep Learning Summer School, so.",
                    "label": 0
                },
                {
                    "sent": "Part of the reason for that, even though it's not helping to get better results on image net right now or speech doing better speech recognition, I think they will touch on the topic a little bit.",
                    "label": 0
                },
                {
                    "sent": "So for other problems where we don't have a whole lot of labeled data, I think this is where we are in need of better solutions.",
                    "label": 0
                },
                {
                    "sent": "And I think that part of solution is, I suspect is going to come from having better unsupervised learning methods which are going to be able to explore the large amount of unlabeled data that we often have.",
                    "label": 0
                },
                {
                    "sent": "And could exploit and so that's ultimately the direction that I think some of this work.",
                    "label": 0
                },
                {
                    "sent": "Could you know?",
                    "label": 0
                },
                {
                    "sent": "Help us get towards?",
                    "label": 1
                },
                {
                    "sent": "But I think we're still not exactly there in terms of reaching practical applications, hopefully with some of your work.",
                    "label": 0
                },
                {
                    "sent": "Maybe in the future we'll get we'll get there.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so that's it for me, thank you.",
                    "label": 0
                }
            ]
        }
    }
}