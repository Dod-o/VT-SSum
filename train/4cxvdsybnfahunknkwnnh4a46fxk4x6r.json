{
    "id": "4cxvdsybnfahunknkwnnh4a46fxk4x6r",
    "title": "An Empirical Comparison of Abstraction in Models of Markov Decision Processes",
    "info": {
        "author": [
            "Todd Hester, Department of Computer Science, University of Texas at Austin"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/icml09_hester_ecammdp/",
    "segmentation": [
        [
            "OK, thank you, so I'll just."
        ],
        [
            "Get started right away.",
            "So the goal of my research is looking at applying IRL to real world problems problems where actions are expensive.",
            "The state space is large, and we can't really explore exhaustively, so this is 1 sort of example robot soccer, where in particular actions are very expensive.",
            "'cause if you take too many actions, the robot breaks eventually."
        ],
        [
            "So I'm looking at this in sort of a typical reinforcement learning framework.",
            "The sort of 1 particular thing to take note of is the factorization here.",
            "So the state could be set up by some vector of state features."
        ],
        [
            "So I have a morning motivating example here.",
            "This map, which I guess is a little tough to see.",
            "The map of Boston with little red dots are all the restaurants in Austin an.",
            "Our goal is to find our favorite restaurant in the city.",
            "But to explore every single restaurant it's going to take us many, many years.",
            "So we want to be able to do is figure out which ones we can avoid exploring.",
            "So perhaps after going to McDonald's and Burger King a few times, we can realize you know enough about fast food.",
            "I know it's fast food is not going to be the final answer.",
            "Starting to go to anymore fast food restaurants."
        ],
        [
            "Um?",
            "So I sort of have this abstract version of this.",
            "This is our agent here, Bender.",
            "Some restaurants he can go to."
        ],
        [
            "He's at his house when he goes, he has a value function over his."
        ],
        [
            "Actions and we go to a particular restaurant.",
            "He gets hurt."
        ],
        [
            "Go back and you can update his value function."
        ],
        [
            "I'm going to take a look at a few different algorithms in that set up.",
            "The first one is the Model 3M Q.",
            "Learning to think most of us are familiar with, so because colorings model free, so the agents actions directly impact the value function.",
            "In particular, when you take an action, you update the Q value of that action towards the value of the next state and the reward you received an we're looking at Q learning with epsilon greedy exploration where it takes the greedy action most of the time, and some personal time will take a random."
        ],
        [
            "So using Q learning Bender here will go to."
        ],
        [
            "Random restaurant.",
            "He gets a reward of 20.",
            "His value function gets updated slightly towards that value."
        ],
        [
            "You and who wander around."
        ],
        [
            "Randomly, some what other ways doing greedy exploration, so it's going to be sort of driven to go towards the ones."
        ],
        [
            "Things are better suited."
        ],
        [
            "Over here a few times."
        ],
        [
            "But it isn't."
        ],
        [
            "Efficient, he has to go to each action multiple times to get values backed up all the way.",
            "He's not really exploring very intelligent."
        ],
        [
            "He hasn't explored any of them."
        ],
        [
            "Strantz up here."
        ],
        [
            "There's no way to sort of.",
            "Guarantee is going to find the optimal restaurant in this case."
        ],
        [
            "So typical approach to improve on that is to use a model based method to model based method.",
            "You learn a transition and reward function and then we're going to form some sort of planning.",
            "So for example valuation on the model within the Q values.",
            "To look at a particular model based method are Max which tracks the number of its detection pair and learns a tabular model.",
            "So it's learning a separate model for each state action in the domain.",
            "And in particular, it uses their visits each state to drive exploration, so states with fewer than N visits the agent will go explore."
        ],
        [
            "So you can see here with the same setup now Agent Bender here is learning this tabular model of the traditional."
        ],
        [
            "Functions for state.",
            "Once he goes to a restaurant, he can update the reward in his model, and then when he does, valuation the value function for that action will be up."
        ],
        [
            "All the way to that reward.",
            "And so he asked."
        ],
        [
            "The only visitor."
        ],
        [
            "Restaurant once and he can use this sort of number of visits that he."
        ],
        [
            "As each one to know."
        ],
        [
            "To explore."
        ],
        [
            "All of them."
        ],
        [
            "But he still has to go and explore every single restaurant.",
            "So here you know, there's 12 restaurants going to take him 12 nights to go eat it.",
            "All of them, hundreds of restaurants.",
            "It's still going to take."
        ],
        [
            "Far too long.",
            "Another example of why we don't want to sort of a tabular model like that.",
            "You have to learn a separate model for each state action.",
            "You know the agent has to learn from this state that the right action goes.",
            "Here, it has to relearn that the right action goes to the state from here and it goes to that state from there.",
            "So it seems like there should be some way to add generalization into the model learning so we don't have to relearn these transitions across every single state."
        ],
        [
            "And so we're looking for something that is more efficient.",
            "Exploration circulating, sort of heavily extra actions to update the values.",
            "Will our Max or other tabular model based methods still have to visit?"
        ],
        [
            "Single state, so we're looking at approaches to incorporate function approximation into the model learning.",
            "So in particular, we want to be able to generalize the transition or defects in the model, and it's not the same as doing sort of function approximation in the value function as you sort of normalcy, But actually going to the function approximation on the model learning.",
            "So function approximation in the learning of the transition and reward model sort of.",
            "The example that here is Bender is learning this model using.",
            "In this case decision trees, and that's pretty updated value function."
        ],
        [
            "So we give sort of an example of how that might work here.",
            "You can imagine each of these restaurants maybe has some feature representation, maybe with bits telling you if it's fast food or burgers Italian."
        ],
        [
            "So when it goes to a particular restaurant, it's reward back in his model, he generalizes that reward and can update the orders model for multiple restaurants.",
            "Maybe all the fast food restaurants he sort of lowers his reward for his model, 'cause he doesn't."
        ],
        [
            "McDonald's how much?",
            "Similarly?",
            "Maybe the same thing happened to talk about like an update?",
            "All the."
        ],
        [
            "Chicken restaurants as well as the fast food."
        ],
        [
            "And for Italian."
        ],
        [
            "And so as he goes around, he eventually finds.",
            "No restaurant, at least close to his favorite restaurant, this Mexican restaurant down here."
        ],
        [
            "And so we really want in our model learning technique as we want to be able to generalize traditions awards well across states.",
            "So we want to visit every state action.",
            "And we also want to model that knows what it knows.",
            "So there's this quick framework basically says that the models either accurately predict the transition or defects, or it should tell you I don't know, you can use that to drive the agents exploration, so the agent will go explore states that it doesn't.",
            "The model doesn't know about otherwise it can use the accurate model that has.",
            "And the question we're looking at in this work is whether we can use existing supervised learning techniques to do this, both to generalize these transition or defects and also to get some sort of confidence measure out of our model.",
            "To say we know these states really well, we don't do explore the more these other states, we don't know too well and we should expect."
        ],
        [
            "Those.",
            "So we're making this into a supervised learning problem.",
            "So in this case, the input is the state the whole state vector and the action and our output.",
            "We're trying to predict the change in state and the reward, so we're predicting the change in the state because that relative changes actually commonly generalizes better than predicting absolute transition, which will demonstrate the second.",
            "We're also learning a separate model for each state feature in the Ward, and sort of another key thing we're trying to go to.",
            "This conference measures, so we want to be able to know which things are modeling well versus which things are modeling poorly.",
            "And use that to drive the agents exploration.",
            "So hopefully if there are some parts of the domain that we think we know well, we think they're not coming apart of the final optimal policy.",
            "We don't have to explore those.",
            "We can hopefully avoid exploring all the states in the."
        ],
        [
            "Green so these are the seven different methods we looked at in this paper.",
            "Tabular method for comparison.",
            "We have three methods that use decision trees.",
            "We have support vector machine neural networks in K nearest neighbor and in parentheses.",
            "Here is the different confidence measure.",
            "Work stretching out of there.",
            "So for example from the decision tree we're looking at how many instances we have in each leaf.",
            "Is the confidence in our prediction.",
            "To go into too much detail on these, but you can look at the paper if you want to see."
        ],
        [
            "Exactly how it works.",
            "There's some related work CFR Max from Alex Trail and others which tries to make predictions based on subsets of features, but it's computationally very expensive 'cause it looks at every possible subset of features that could be used, and there's also a method by Thoma degree, which uses decision trees, but it's modeling using absolute transitions, so the generalization across states."
        ],
        [
            "Does not work as well.",
            "This is an example of how this works with decision trees, which actually turned out to be the best approach in our experiments.",
            "So for example here.",
            "What we did is we train the model on randomly sampled transitions from the domain we built into.",
            "This isn't remodel of the change in the X variable.",
            "So for example here this state the agent is at X of one and Y of two and when it takes a section it becomes goes to X of two.",
            "So it's a change in X + 1.",
            "And so you'll see that it can learn that and that sort of general is really well.",
            "So that same thing appearing takes the right action is a plus one.",
            "So for example, when it takes action here, there's a wall, and so the change in X is 0.",
            "We're looking at the conference measure.",
            "In this case, is how many instances we have in each leaf.",
            "So since we have 6 examples of the up and down actions, we say we have confidence, higher confidence that we have sort of this wall case where we only had one instance.",
            "This could be used to drive exploration so they would go and explore the.",
            "Instances with lower confidence and for example, go explore more along this wall and hopefully find these doorways that the model doesn't predict yet."
        ],
        [
            "So experiments I'm not going to go into detail on the experimental setup and results, but the experiments are in three different domains trying to compare how well the model generalizes these transition or defects across states as well as how well the model really knows what it knows.",
            "So whether the transitions that it thinks that it's confident in whether those are really accurate or not.",
            "And what we found was that the tree based methods perform extremely well on a small number of training samples.",
            "So after even 50 samples, very accurate model of the."
        ],
        [
            "And so you can kind of see that here, even after just giving that remodel one right and one left action, it's going to build a model that says if the action was right, absolutely committed one.",
            "If the action was left actually documented by one, and that will already be true in sort of all the open space states just to learn about the walls and.",
            "It is a good sign."
        ],
        [
            "An accurate model.",
            "After you give it more training samples.",
            "So for example, after maybe 10,000 training examples, support vector machines actually perform very well, and then if you have enough data we don't have to generalize the tabular models conform the best and you're not doing any generalization you don't worry about, sort of generalizing incorrectly."
        ],
        [
            "So in conclusion, we can use supervised learning techniques to for model learning in our setting, and it looks like we should be able to use these techniques to drive exploration.",
            "So create a confidence measure on our model learning and use that to decide which ones we can explore and which ones we can safely not explore.",
            "But the real test is to use these in reinforcement learning agent and actually use those conference measures to drive the agents exploration."
        ],
        [
            "So in conclusion, we can use supervised learning techniques to build models and drive exploration in large domains.",
            "Thanks.",
            "The real nice thing about there quicker things are that you can easily convert them to and our mix like address.",
            "At this rate case and some other cases.",
            "So how do you see a way to is I don't know easily take super vector machine and convert this to an Army slide or put it into numbers like other.",
            "Yeah so with all these techniques I'm getting this conference measure out of how confident that I'm in the predictions.",
            "And so we wanted our Max.",
            "Like you put a threshold on that and say once with confidence below threshold we say are unknown or sort of.",
            "I don't know in the quick framework and just like in our Max you say states with fewer than visits are unknown.",
            "So we drive agent to explore all the transitions with sort of conference below threshold and assume it knows with confidence above threshold value able to solve model like this.",
            "So whatever the model is given by the SBM so probably OK, it gives an arm extra work.",
            "But you have somehow you have to get there.",
            "Right, so once we write, so once we're using this model, we're putting it back into the full state space for the planning step.",
            "So we're planning over the entire state space and still complain multiple step explorations to states where we give the exploration bonus too.",
            "All right questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you, so I'll just.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get started right away.",
                    "label": 0
                },
                {
                    "sent": "So the goal of my research is looking at applying IRL to real world problems problems where actions are expensive.",
                    "label": 1
                },
                {
                    "sent": "The state space is large, and we can't really explore exhaustively, so this is 1 sort of example robot soccer, where in particular actions are very expensive.",
                    "label": 0
                },
                {
                    "sent": "'cause if you take too many actions, the robot breaks eventually.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm looking at this in sort of a typical reinforcement learning framework.",
                    "label": 0
                },
                {
                    "sent": "The sort of 1 particular thing to take note of is the factorization here.",
                    "label": 0
                },
                {
                    "sent": "So the state could be set up by some vector of state features.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I have a morning motivating example here.",
                    "label": 0
                },
                {
                    "sent": "This map, which I guess is a little tough to see.",
                    "label": 0
                },
                {
                    "sent": "The map of Boston with little red dots are all the restaurants in Austin an.",
                    "label": 1
                },
                {
                    "sent": "Our goal is to find our favorite restaurant in the city.",
                    "label": 1
                },
                {
                    "sent": "But to explore every single restaurant it's going to take us many, many years.",
                    "label": 0
                },
                {
                    "sent": "So we want to be able to do is figure out which ones we can avoid exploring.",
                    "label": 0
                },
                {
                    "sent": "So perhaps after going to McDonald's and Burger King a few times, we can realize you know enough about fast food.",
                    "label": 0
                },
                {
                    "sent": "I know it's fast food is not going to be the final answer.",
                    "label": 0
                },
                {
                    "sent": "Starting to go to anymore fast food restaurants.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So I sort of have this abstract version of this.",
                    "label": 0
                },
                {
                    "sent": "This is our agent here, Bender.",
                    "label": 0
                },
                {
                    "sent": "Some restaurants he can go to.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He's at his house when he goes, he has a value function over his.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actions and we go to a particular restaurant.",
                    "label": 0
                },
                {
                    "sent": "He gets hurt.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go back and you can update his value function.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to take a look at a few different algorithms in that set up.",
                    "label": 0
                },
                {
                    "sent": "The first one is the Model 3M Q.",
                    "label": 0
                },
                {
                    "sent": "Learning to think most of us are familiar with, so because colorings model free, so the agents actions directly impact the value function.",
                    "label": 0
                },
                {
                    "sent": "In particular, when you take an action, you update the Q value of that action towards the value of the next state and the reward you received an we're looking at Q learning with epsilon greedy exploration where it takes the greedy action most of the time, and some personal time will take a random.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So using Q learning Bender here will go to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Random restaurant.",
                    "label": 0
                },
                {
                    "sent": "He gets a reward of 20.",
                    "label": 0
                },
                {
                    "sent": "His value function gets updated slightly towards that value.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You and who wander around.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Randomly, some what other ways doing greedy exploration, so it's going to be sort of driven to go towards the ones.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things are better suited.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over here a few times.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it isn't.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Efficient, he has to go to each action multiple times to get values backed up all the way.",
                    "label": 0
                },
                {
                    "sent": "He's not really exploring very intelligent.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He hasn't explored any of them.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Strantz up here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's no way to sort of.",
                    "label": 0
                },
                {
                    "sent": "Guarantee is going to find the optimal restaurant in this case.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So typical approach to improve on that is to use a model based method to model based method.",
                    "label": 0
                },
                {
                    "sent": "You learn a transition and reward function and then we're going to form some sort of planning.",
                    "label": 0
                },
                {
                    "sent": "So for example valuation on the model within the Q values.",
                    "label": 0
                },
                {
                    "sent": "To look at a particular model based method are Max which tracks the number of its detection pair and learns a tabular model.",
                    "label": 0
                },
                {
                    "sent": "So it's learning a separate model for each state action in the domain.",
                    "label": 0
                },
                {
                    "sent": "And in particular, it uses their visits each state to drive exploration, so states with fewer than N visits the agent will go explore.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can see here with the same setup now Agent Bender here is learning this tabular model of the traditional.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functions for state.",
                    "label": 0
                },
                {
                    "sent": "Once he goes to a restaurant, he can update the reward in his model, and then when he does, valuation the value function for that action will be up.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the way to that reward.",
                    "label": 0
                },
                {
                    "sent": "And so he asked.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The only visitor.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Restaurant once and he can use this sort of number of visits that he.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As each one to know.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To explore.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All of them.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But he still has to go and explore every single restaurant.",
                    "label": 0
                },
                {
                    "sent": "So here you know, there's 12 restaurants going to take him 12 nights to go eat it.",
                    "label": 0
                },
                {
                    "sent": "All of them, hundreds of restaurants.",
                    "label": 0
                },
                {
                    "sent": "It's still going to take.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Far too long.",
                    "label": 0
                },
                {
                    "sent": "Another example of why we don't want to sort of a tabular model like that.",
                    "label": 0
                },
                {
                    "sent": "You have to learn a separate model for each state action.",
                    "label": 0
                },
                {
                    "sent": "You know the agent has to learn from this state that the right action goes.",
                    "label": 0
                },
                {
                    "sent": "Here, it has to relearn that the right action goes to the state from here and it goes to that state from there.",
                    "label": 0
                },
                {
                    "sent": "So it seems like there should be some way to add generalization into the model learning so we don't have to relearn these transitions across every single state.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we're looking for something that is more efficient.",
                    "label": 0
                },
                {
                    "sent": "Exploration circulating, sort of heavily extra actions to update the values.",
                    "label": 0
                },
                {
                    "sent": "Will our Max or other tabular model based methods still have to visit?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Single state, so we're looking at approaches to incorporate function approximation into the model learning.",
                    "label": 0
                },
                {
                    "sent": "So in particular, we want to be able to generalize the transition or defects in the model, and it's not the same as doing sort of function approximation in the value function as you sort of normalcy, But actually going to the function approximation on the model learning.",
                    "label": 0
                },
                {
                    "sent": "So function approximation in the learning of the transition and reward model sort of.",
                    "label": 0
                },
                {
                    "sent": "The example that here is Bender is learning this model using.",
                    "label": 0
                },
                {
                    "sent": "In this case decision trees, and that's pretty updated value function.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we give sort of an example of how that might work here.",
                    "label": 0
                },
                {
                    "sent": "You can imagine each of these restaurants maybe has some feature representation, maybe with bits telling you if it's fast food or burgers Italian.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when it goes to a particular restaurant, it's reward back in his model, he generalizes that reward and can update the orders model for multiple restaurants.",
                    "label": 0
                },
                {
                    "sent": "Maybe all the fast food restaurants he sort of lowers his reward for his model, 'cause he doesn't.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "McDonald's how much?",
                    "label": 0
                },
                {
                    "sent": "Similarly?",
                    "label": 0
                },
                {
                    "sent": "Maybe the same thing happened to talk about like an update?",
                    "label": 0
                },
                {
                    "sent": "All the.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chicken restaurants as well as the fast food.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for Italian.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so as he goes around, he eventually finds.",
                    "label": 0
                },
                {
                    "sent": "No restaurant, at least close to his favorite restaurant, this Mexican restaurant down here.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we really want in our model learning technique as we want to be able to generalize traditions awards well across states.",
                    "label": 0
                },
                {
                    "sent": "So we want to visit every state action.",
                    "label": 0
                },
                {
                    "sent": "And we also want to model that knows what it knows.",
                    "label": 0
                },
                {
                    "sent": "So there's this quick framework basically says that the models either accurately predict the transition or defects, or it should tell you I don't know, you can use that to drive the agents exploration, so the agent will go explore states that it doesn't.",
                    "label": 0
                },
                {
                    "sent": "The model doesn't know about otherwise it can use the accurate model that has.",
                    "label": 0
                },
                {
                    "sent": "And the question we're looking at in this work is whether we can use existing supervised learning techniques to do this, both to generalize these transition or defects and also to get some sort of confidence measure out of our model.",
                    "label": 0
                },
                {
                    "sent": "To say we know these states really well, we don't do explore the more these other states, we don't know too well and we should expect.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those.",
                    "label": 0
                },
                {
                    "sent": "So we're making this into a supervised learning problem.",
                    "label": 0
                },
                {
                    "sent": "So in this case, the input is the state the whole state vector and the action and our output.",
                    "label": 0
                },
                {
                    "sent": "We're trying to predict the change in state and the reward, so we're predicting the change in the state because that relative changes actually commonly generalizes better than predicting absolute transition, which will demonstrate the second.",
                    "label": 0
                },
                {
                    "sent": "We're also learning a separate model for each state feature in the Ward, and sort of another key thing we're trying to go to.",
                    "label": 0
                },
                {
                    "sent": "This conference measures, so we want to be able to know which things are modeling well versus which things are modeling poorly.",
                    "label": 0
                },
                {
                    "sent": "And use that to drive the agents exploration.",
                    "label": 0
                },
                {
                    "sent": "So hopefully if there are some parts of the domain that we think we know well, we think they're not coming apart of the final optimal policy.",
                    "label": 0
                },
                {
                    "sent": "We don't have to explore those.",
                    "label": 0
                },
                {
                    "sent": "We can hopefully avoid exploring all the states in the.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Green so these are the seven different methods we looked at in this paper.",
                    "label": 0
                },
                {
                    "sent": "Tabular method for comparison.",
                    "label": 0
                },
                {
                    "sent": "We have three methods that use decision trees.",
                    "label": 0
                },
                {
                    "sent": "We have support vector machine neural networks in K nearest neighbor and in parentheses.",
                    "label": 0
                },
                {
                    "sent": "Here is the different confidence measure.",
                    "label": 0
                },
                {
                    "sent": "Work stretching out of there.",
                    "label": 0
                },
                {
                    "sent": "So for example from the decision tree we're looking at how many instances we have in each leaf.",
                    "label": 0
                },
                {
                    "sent": "Is the confidence in our prediction.",
                    "label": 0
                },
                {
                    "sent": "To go into too much detail on these, but you can look at the paper if you want to see.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exactly how it works.",
                    "label": 0
                },
                {
                    "sent": "There's some related work CFR Max from Alex Trail and others which tries to make predictions based on subsets of features, but it's computationally very expensive 'cause it looks at every possible subset of features that could be used, and there's also a method by Thoma degree, which uses decision trees, but it's modeling using absolute transitions, so the generalization across states.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does not work as well.",
                    "label": 0
                },
                {
                    "sent": "This is an example of how this works with decision trees, which actually turned out to be the best approach in our experiments.",
                    "label": 0
                },
                {
                    "sent": "So for example here.",
                    "label": 0
                },
                {
                    "sent": "What we did is we train the model on randomly sampled transitions from the domain we built into.",
                    "label": 0
                },
                {
                    "sent": "This isn't remodel of the change in the X variable.",
                    "label": 0
                },
                {
                    "sent": "So for example here this state the agent is at X of one and Y of two and when it takes a section it becomes goes to X of two.",
                    "label": 0
                },
                {
                    "sent": "So it's a change in X + 1.",
                    "label": 0
                },
                {
                    "sent": "And so you'll see that it can learn that and that sort of general is really well.",
                    "label": 0
                },
                {
                    "sent": "So that same thing appearing takes the right action is a plus one.",
                    "label": 0
                },
                {
                    "sent": "So for example, when it takes action here, there's a wall, and so the change in X is 0.",
                    "label": 0
                },
                {
                    "sent": "We're looking at the conference measure.",
                    "label": 0
                },
                {
                    "sent": "In this case, is how many instances we have in each leaf.",
                    "label": 0
                },
                {
                    "sent": "So since we have 6 examples of the up and down actions, we say we have confidence, higher confidence that we have sort of this wall case where we only had one instance.",
                    "label": 0
                },
                {
                    "sent": "This could be used to drive exploration so they would go and explore the.",
                    "label": 0
                },
                {
                    "sent": "Instances with lower confidence and for example, go explore more along this wall and hopefully find these doorways that the model doesn't predict yet.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So experiments I'm not going to go into detail on the experimental setup and results, but the experiments are in three different domains trying to compare how well the model generalizes these transition or defects across states as well as how well the model really knows what it knows.",
                    "label": 0
                },
                {
                    "sent": "So whether the transitions that it thinks that it's confident in whether those are really accurate or not.",
                    "label": 0
                },
                {
                    "sent": "And what we found was that the tree based methods perform extremely well on a small number of training samples.",
                    "label": 0
                },
                {
                    "sent": "So after even 50 samples, very accurate model of the.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so you can kind of see that here, even after just giving that remodel one right and one left action, it's going to build a model that says if the action was right, absolutely committed one.",
                    "label": 0
                },
                {
                    "sent": "If the action was left actually documented by one, and that will already be true in sort of all the open space states just to learn about the walls and.",
                    "label": 0
                },
                {
                    "sent": "It is a good sign.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An accurate model.",
                    "label": 0
                },
                {
                    "sent": "After you give it more training samples.",
                    "label": 0
                },
                {
                    "sent": "So for example, after maybe 10,000 training examples, support vector machines actually perform very well, and then if you have enough data we don't have to generalize the tabular models conform the best and you're not doing any generalization you don't worry about, sort of generalizing incorrectly.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, we can use supervised learning techniques to for model learning in our setting, and it looks like we should be able to use these techniques to drive exploration.",
                    "label": 1
                },
                {
                    "sent": "So create a confidence measure on our model learning and use that to decide which ones we can explore and which ones we can safely not explore.",
                    "label": 0
                },
                {
                    "sent": "But the real test is to use these in reinforcement learning agent and actually use those conference measures to drive the agents exploration.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in conclusion, we can use supervised learning techniques to build models and drive exploration in large domains.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "The real nice thing about there quicker things are that you can easily convert them to and our mix like address.",
                    "label": 0
                },
                {
                    "sent": "At this rate case and some other cases.",
                    "label": 0
                },
                {
                    "sent": "So how do you see a way to is I don't know easily take super vector machine and convert this to an Army slide or put it into numbers like other.",
                    "label": 0
                },
                {
                    "sent": "Yeah so with all these techniques I'm getting this conference measure out of how confident that I'm in the predictions.",
                    "label": 0
                },
                {
                    "sent": "And so we wanted our Max.",
                    "label": 0
                },
                {
                    "sent": "Like you put a threshold on that and say once with confidence below threshold we say are unknown or sort of.",
                    "label": 0
                },
                {
                    "sent": "I don't know in the quick framework and just like in our Max you say states with fewer than visits are unknown.",
                    "label": 0
                },
                {
                    "sent": "So we drive agent to explore all the transitions with sort of conference below threshold and assume it knows with confidence above threshold value able to solve model like this.",
                    "label": 0
                },
                {
                    "sent": "So whatever the model is given by the SBM so probably OK, it gives an arm extra work.",
                    "label": 0
                },
                {
                    "sent": "But you have somehow you have to get there.",
                    "label": 0
                },
                {
                    "sent": "Right, so once we write, so once we're using this model, we're putting it back into the full state space for the planning step.",
                    "label": 0
                },
                {
                    "sent": "So we're planning over the entire state space and still complain multiple step explorations to states where we give the exploration bonus too.",
                    "label": 0
                },
                {
                    "sent": "All right questions.",
                    "label": 0
                }
            ]
        }
    }
}