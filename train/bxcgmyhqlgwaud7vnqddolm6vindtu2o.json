{
    "id": "bxcgmyhqlgwaud7vnqddolm6vindtu2o",
    "title": "Link Discovery with Guaranteed Reduction Ratio in A\u000ene Spaces with Minkowski Measures",
    "info": {
        "author": [
            "Axel-Cyrille Ngonga Ngomo, University of Leipzig"
        ],
        "published": "Dec. 3, 2012",
        "recorded": "November 2012",
        "category": [
            "Top->Computer Science->Semantic Web->Linked Data"
        ]
    },
    "url": "http://videolectures.net/iswc2012_ngonga_ngomo_minkowski_measures/",
    "segmentation": [
        [
            "I realize that I am the guy between you and your lunch, so I'll try to be as precise and as concise as I can for those who do not know me.",
            "My name is Axel.",
            "I'm from the University of Leipzig in Germany and as introduced our talk about link discovery with guaranteed reduction ratio in affine spaces with Minkowski measures and what all is are explained in the."
        ],
        [
            "Talk, what was the motivation for work?",
            "Well, it was.",
            "It was this.",
            "Selling open data cloud.",
            "You've seen this picture thousands of times already, but as you know we have 31 plus billions of triples in there, but only 500 million of these triples actually links that connect knowledge basis.",
            "So basically what it means is that less than 2% of the triples in the link open data cloud links between knowledge basis.",
            "But as we know, links are seriously important if you want to do data integration.",
            "If you want to do Federated querying.",
            "If you want to do cross ontology question answering, you need links across the knowledge base is to be able to do all that.",
            "So this lack of law."
        ],
        [
            "Things has actually led to.",
            "Quite a development respect to link discovery tools.",
            "What is the link discovery problem from a formal point of view, we assume that we're giving a set S of source instances is said T of target instances and some relation R and what we want to find is the set M of pairs that are such that our holds for St.",
            "So we're looking for a pair, a set of pairs St that has taught that RST actually holds in most cases is quite tricky to find so."
        ],
        [
            "Link Discovery tools actually use an approximation of the set in the computer set M prime.",
            "That is such that for all the elements in the set.",
            "So for repairs St.",
            "The following.",
            "Inequation holds Delta, SC is less or equal to a threshold Tetteh."
        ],
        [
            "Now that several problems when we try to solve that problem, the problem of link discovery by using this form of approximation number one is obviously that is it becomes quadratic.",
            "So we technically if you wanted to go for a brute force algorithm, you have to compare all the elements of South with all elements of T. And if you imagine that you just want to link places from Pedia, meet places from Geonames, I need one millisecond for each similarity.",
            "Computation will take you approximately 2 months.",
            "If you go further and you try to link the whole of the PDF with the whole of Geo names, it would take you decades if not centuries, so there's no way that we can do.",
            "We can solve this problem by using quadratic algorithms.",
            "They also have the same problem with respect to resource management if you try to solve the same problem that is, linking DB pedia with geonames an you only need one bite to solve to basically store such a pair, you need approximately 125 terabytes of RAM to basically.",
            "Run your algorithm.",
            "If you go for a brute force approach, so that's not going to work either and."
        ],
        [
            "The third problem we faced with is actually the complexity of link specifications themselves, so this is a specification for a very simple problem actually, which is linking directors from the picture with the linked movie database.",
            "And as you can see we have to use quite a few different similarity functions here to cover all the corner cases and get.",
            "A high precision, high recall researcher specification.",
            "We're not going to deal with the complexity of in specifications today, we're much more going to be concerned with the time complexity problem."
        ],
        [
            "Over the years, several approaches have been developed to solve or address at least the time complexity problem and what they do in most cases is that try to reduce the number CA of comparisons that they carry out.",
            "In other terms, where they tried to do is maximized the so called reduction ratio which is 1 -- C divided by the complexity of the problem, which is size of S times size of T."
        ],
        [
            "The main question then behind our work was country device.",
            "Loopless approaches with a guaranteed reduction ratio, and I'm going to explain in a second what we mean by guaranteed reduction ratio.",
            "But if we had such approaches, we actually tremendous because we could run proper space management.",
            "We could approximate the runtime of the algorithms in a way more efficient manner and we could do resource scheduling so we could know this algorithm is going to take this much time in this much space.",
            "So in my cloud I can give it only two nodes.",
            "Instead of five when I don't know how much resources algorithm is actually going to need."
        ],
        [
            "No.",
            "Before we continue to.",
            "Present one variable the best achievable reduction ratio is actually our Max, which is 1 -- M. Prime was the size of all the pairs that are such that Delta is less or equal to teta.",
            "And the best achievable reduction ratio are Max is 1 minus the size of M prime divided by the complexity of the problem at hand."
        ],
        [
            "Now when we say that we want to approach an operator, has a guaranteed reduction ratio.",
            "What we mean is that for all ours that are less than our Max, we want to have an approach that is such that there is a parameter of or a set of parameters are for that little approach having a reduction ratio larger or equal to R. That's basically what we aim at."
        ],
        [
            "So we want to have and throughout the talk, or simply use the.",
            "Relative reduction ratio, which is our marks divided by our of a.",
            "It's important to notice here that what we want to have is a relative reduction ratio that goes to one and most approaches or all approaches.",
            "Actually we always have reduction ratio that's larger than one."
        ],
        [
            "So, formally, what's the goal here?",
            "The goal is to device an approach.",
            "Each of our four that is such that for all ours larger than one there is an Alpha that is a set of parameters that is such that the reduction ratio of our approach is less or equal to R. That's basically formally what we're trying to do."
        ],
        [
            "There's obviously a restriction here.",
            "We can.",
            "So far we only deal with affine spaces that contain Minkowski distances and incomplete distances.",
            "That's a fancy word for a generalization of the Euclidean distance, so you have squares in the Euclidean distance, Ann from its course, Minkowski distance.",
            "We simply have a value P that is somewhere between zero and Infinity Ann.",
            "These little shapes here actually show you how a circle looks like in spaces with Minkowski distances for different values of P. So this is the circle as we know it.",
            "That's the circle for sqrt 2, and so on and so forth."
        ],
        [
            "Good.",
            "So the basis for the algorithm that I'm going to present that actually has these nice properties that we wanted to have is the ideal space tiling which was implemented in the heap algorithm which was presented last year at the ontology matching Workshop.",
            "The basic intuition here is that Delta SC less or equal to to actually describes a hypersphere.",
            "Technically, it's a hyper bowl, but also hyper fear for the rest of the talk, just for the sake of simplicity, and we can approximate a hypersphere by using a hypercube.",
            "Main advantages, obviously that hypercubes are way easier to compute directly, and if we can have a hypercube that always contains all the points that are in the hyper sphere, we also have no loss of recall, which is a property that we definitely want to have.",
            "That is, in contrast to some blocking approaches that do lose recall.",
            "So graphically what we've got here is we've got the point S, and that is basically the set of points that we are actually interested in.",
            "I want to approximate this by using a hypercube."
        ],
        [
            "How do we do that?",
            "But we begin by setting the width of a single hypercube.",
            "2D equals Delta divided by Alpha.",
            "This equation is going to be spread around the whole talk, so it's quite important that you remember that Delta is to divided by Alpha."
        ],
        [
            "And then retired.",
            "The space simply retired the whole space into adjacent cube C. We give them a whole number coordinates and say that each cube C contains the points Omega data such that CI times Delta is less or equal to Omega I, which is less than CI plus one times Delta.",
            "So all we do basically."
        ],
        [
            "This is pretty straightforward.",
            "We take the space, we chop it into hypercubes, and then we take the smallest set of hypercube that contain the whole of the points that are supposed to be in our hyper sphere.",
            "This basically shows what happens when we set Alpha to one, and if P = 2, that is.",
            "Basically, we're dealing with a Euclidean space here.",
            "Then we need 9 squares to approximate.",
            "Excuse me to approximate this circle."
        ],
        [
            "We can actually go forward further and compute.",
            "What happens when we change the value of Alpha and we can prove that we need exactly 2A plus one to the power N hypercubes to approximate such a sphere?",
            "An interesting property of the algorithm is obviously that if we increase the value of Alpha, we actually approximation gets better and better.",
            "Now the question is, does the fact that the quality of the approximation converges that sufficient?",
            "We already have an algorithm that has the nice properties that we want to have.",
            "And the bad news is no, we did not.",
            "We can actually see exactly what they are of the hip algorithm would be if you had a uniform distribution of points across the space, it would be 2A plus one to the power of N divided by Alpha to the both end times the volume of the unit sphere in North dimensions.",
            "And if you let half ago towards Infinity is 2 to the power of 10 divided by the volume of the sphere.",
            "Dimensions are."
        ],
        [
            "Just plotted to make it easier.",
            "So this is basically a kind of plot that you get as you see, we converge towards the value, but that value is not one, so we don't have what we wanted to have."
        ],
        [
            "We're going to compute the exact values that we get, and you can see that for N = 2, we get a limit that is around 1.2 seven.",
            "That's 4 / \u03c0, and for N = 4 you get 32 / \u03c0 square.",
            "So we do converge, but not towards one."
        ],
        [
            "Now the basic idea behind the H R3 algorithm which has these properties is to use an indexing scheme.",
            "So basically to add an indexing scheme to the Hippo algorithm as explained already.",
            "Basic idea here is that we will assume that this cube, which is acutely interested in that it has the coordinates 00000, and so on.",
            "No, all the cubes that are adjacent to the X. I'm sorry the X and the Y axis.",
            "So basically this set of cubes over here and that and this sort of Q over here.",
            "They basically get the index zero.",
            "Very simple and all the other cubes they get this index which is.",
            "The coordinate of the cube minus the coordinator of this, which is zero in our case, minus 1 to the power of P, and we sum it up overall dimensions just to tell you how it works.",
            "So this cube over here for example has the coordinates 2, two, which basically means that it gets the index 2 -- 0 -- 1 across this dimension.",
            "Squared plus 2 minus or minus one across this dimension squared and that gives it an index of two and we can do that for all the other cubes."
        ],
        [
            "Alright, and the basic insight behind HR 3 is that.",
            "All we need to do is only compare the elements of this cubes with cubes that are such that the index is less or equal to offer to the power of P. That is basically what happens here.",
            "In this example.",
            "This is basically what people would run, but H R3 can actually discards this four cubes simply because they have an index of 18.",
            "An 18 is greater than 4 squared, which is 16, so we can actually discuss those cubes and we do not need to run the computation there.",
            "Now."
        ],
        [
            "What I stated before translates to this.",
            "We want an algorithm that does that has no loss of recall.",
            "That's the first thing that we need to prove, and then we need to prove that the limit of the error of this algorithm when Alpha goes towards Infinity is one.",
            "That's basically what we need to do here.",
            "R."
        ],
        [
            "Again, with the first thing.",
            "Basic idea here is very simple.",
            "What we need to prove before we can actually prove that.",
            "Yeah, that we do not lose any recall is that if the index of civil respect to X is active, respect to X is X, then the distance between S&T for cheese that I see to power P is must be larger than X times Delta to the power of P."
        ],
        [
            "And then actually simply comes out of the construction of the index, which is kind of the secret source here.",
            "So if you look at this.",
            "Basically what this value tells us the value of five is simply tells us that all points here the distance between our points here and there will always be larger than five and five is nothing else that this length squared for P = 2.",
            "Thus, we can actually assume we actually know that for all points here and opens there, the distance will always be superior to five and."
        ],
        [
            "You can prove that formally basic idea here is that we take the definition of the index as shown before and.",
            "Out of the definition of the cube, we basically said every segment this piece.",
            "We know that they across the ice dimension that as I'm honesty, I must be larger than this value, and if you Simply put a power of pure everywhere and sum it up, overall dimensions you actually get dilemma that we were talking about now that was very interesting about this lemma is."
        ],
        [
            "But what it means is that for all S in S, if the index of Siri respect to S is larger than Alpha to the power of P, then all the TS in C must be non matches or we need to do is replace X with Alpha to both be in the equation I showed before and we remember that Delta is Delta divided by Alpha.",
            "So we have Delta report P times are for above pendek users data report piece.",
            "So we know the distances will always be larger so we do not discard anything.",
            "That should not be discard."
        ],
        [
            "That's step one.",
            "We know that we do not lose recall.",
            "Step 2 is now.",
            "Do we actually have an algorithm that is such that there is always a set of parameters that leads to whatever reduction ratio we want?"
        ],
        [
            "The proof of this third lemma is pretty involved, so I'm not going to go too much into detail there or just going to give you the basic idea behind the proof, and it is the following.",
            "Thank you.",
            "If we double the value of Alpha, we can be sure that we actually augment the reduction ratio of the approach.",
            "So this is basically the example that we had before where we discarded only these four corners.",
            "Now if we double the value of our family set off to 8, whatever."
        ],
        [
            "Happens is that we do not only discard the corners we have here 16 order smaller cubes that we can discard, which basically means that our reduction ratio of men's and we can do that for any."
        ],
        [
            "Value of offer.",
            "If offer is 25 and you double the value."
        ],
        [
            "Let me just.",
            "You can see that actually deserve.",
            "It gets we have to do less computations, basically."
        ],
        [
            "Now the interesting thing about what I just showed is that the hip algorithm also has that property.",
            "As I showed before.",
            "So if you double the value of actually you get an even better approximation.",
            "So we know that we converge towards the value.",
            "But we're not sure they would converge towards one.",
            "So how do we show that we converge towards one again?",
            "Again, just the proof behind the idea or the idea behind the proof?",
            "That way around is that if Alpha goes towards Infinity then Delta goes towards zero.",
            "It basically means that if we look at the index function and multiply all terms by Delta to the power of P. What happens is that we have data report P times.",
            "This term is less or equal to Delta to the power of three times after the power P. Now if you put Delta in here, you basically get Delta time CI, which is actually the coordinate of South and you have here Delta Times Delta T series of S is according to South Delta Times.",
            "CI is the coordinate of TB cause the cube has reduced to just one point which is T itself and you have here minus Delta but Delta goes towards 0.",
            "So we can actually get rid of that.",
            "And if you then sum it up, you get exactly the equation that we had at the beginning.",
            "Basic idea is we do not compute anything that we should not be computing, which means that the reduction ratio is exactly 1."
        ],
        [
            "OK, so we there we have an approach that does not lose recall.",
            "An actually has the nice property that the reduction ratio can be set to any value we want.",
            "Now the question is.",
            "Does it perform well?",
            "I mean, it's nice to have this theoretically, but can we actually outperformed the state of the art respect to anything we actually run?"
        ],
        [
            "Experiments.",
            "We compared our approach with the Hippogriff himself and with algorithm implemented in the Silk Framework version 251, and we did sell for experiments.",
            "One where we did Applicated deeply job places by using the minimum elevation, elevation, the maximum elevation, and we also try to link cities in geonames, unlinked geodata by longitude and latitude and all experiments were carried out on the laptop over there."
        ],
        [
            "So we can show is that even for small experiments where we only had eight 9 million computations, we can already get rid of 640,000 comparisons for energy value of 32."
        ],
        [
            "And if we if the experiment is larger, obviously we get rid of even more computations here.",
            "4.3 million computations for Alpha equals 32."
        ],
        [
            "Oh, this looks different anyway.",
            "It also shows that the reduction ratio really converges towards one here.",
            "This is the line for HR3 and there as well.",
            "So if we actually augment the value of our, we also get practically what we expected.",
            "Theoretically, a reduction ratio that goes towards."
        ],
        [
            "One an even respecter runtime, especially here we could show that H R3 actually outperforms the Hippo algorithm."
        ],
        [
            "And also outperforms the algorithm that is implemented in this silk framework."
        ],
        [
            "OK, so the mission behind our work was actually 2.",
            "Create a new category of algorithms for link discovery.",
            "So far we've had algorithms that promise that they are faster than put forth, and there are.",
            "Indeed there's no discussion there, but we don't know how fast they are.",
            "We can't really tune the algorithm to fit whatever resources we have, and the advantage of the algorithm we developed is that we can actually tune it to do and to behave the way you want it to behave within.",
            "Obviously the reasonable boundaries of of the minimum or maximum RR, and that can be achieved theoretically."
        ],
        [
            "Um?",
            "Yes, and in future work the first thing we want to do is combine this approach with multi indexing approaches.",
            "The idea being simply that we deal with N dimensional data.",
            "So if you use multi indexing you might be actually be even faster right now and now that we know that we can actually tune the reduction ratio of the algorithm we wanted device resource management approaches that make use of that and.",
            "On the linked open Data Cloud, we have plenty of strings and plenty of numbers, so the third thing that we want to do is develop algorithms for strings that have similar, if not the same theoretical guarantees."
        ],
        [
            "That was it from my side.",
            "Thank you very much for listening and if you have any questions please bring them on."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I realize that I am the guy between you and your lunch, so I'll try to be as precise and as concise as I can for those who do not know me.",
                    "label": 0
                },
                {
                    "sent": "My name is Axel.",
                    "label": 0
                },
                {
                    "sent": "I'm from the University of Leipzig in Germany and as introduced our talk about link discovery with guaranteed reduction ratio in affine spaces with Minkowski measures and what all is are explained in the.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk, what was the motivation for work?",
                    "label": 0
                },
                {
                    "sent": "Well, it was.",
                    "label": 0
                },
                {
                    "sent": "It was this.",
                    "label": 0
                },
                {
                    "sent": "Selling open data cloud.",
                    "label": 0
                },
                {
                    "sent": "You've seen this picture thousands of times already, but as you know we have 31 plus billions of triples in there, but only 500 million of these triples actually links that connect knowledge basis.",
                    "label": 0
                },
                {
                    "sent": "So basically what it means is that less than 2% of the triples in the link open data cloud links between knowledge basis.",
                    "label": 0
                },
                {
                    "sent": "But as we know, links are seriously important if you want to do data integration.",
                    "label": 0
                },
                {
                    "sent": "If you want to do Federated querying.",
                    "label": 0
                },
                {
                    "sent": "If you want to do cross ontology question answering, you need links across the knowledge base is to be able to do all that.",
                    "label": 0
                },
                {
                    "sent": "So this lack of law.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things has actually led to.",
                    "label": 0
                },
                {
                    "sent": "Quite a development respect to link discovery tools.",
                    "label": 0
                },
                {
                    "sent": "What is the link discovery problem from a formal point of view, we assume that we're giving a set S of source instances is said T of target instances and some relation R and what we want to find is the set M of pairs that are such that our holds for St.",
                    "label": 1
                },
                {
                    "sent": "So we're looking for a pair, a set of pairs St that has taught that RST actually holds in most cases is quite tricky to find so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Link Discovery tools actually use an approximation of the set in the computer set M prime.",
                    "label": 0
                },
                {
                    "sent": "That is such that for all the elements in the set.",
                    "label": 0
                },
                {
                    "sent": "So for repairs St.",
                    "label": 0
                },
                {
                    "sent": "The following.",
                    "label": 0
                },
                {
                    "sent": "Inequation holds Delta, SC is less or equal to a threshold Tetteh.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now that several problems when we try to solve that problem, the problem of link discovery by using this form of approximation number one is obviously that is it becomes quadratic.",
                    "label": 0
                },
                {
                    "sent": "So we technically if you wanted to go for a brute force algorithm, you have to compare all the elements of South with all elements of T. And if you imagine that you just want to link places from Pedia, meet places from Geonames, I need one millisecond for each similarity.",
                    "label": 0
                },
                {
                    "sent": "Computation will take you approximately 2 months.",
                    "label": 0
                },
                {
                    "sent": "If you go further and you try to link the whole of the PDF with the whole of Geo names, it would take you decades if not centuries, so there's no way that we can do.",
                    "label": 0
                },
                {
                    "sent": "We can solve this problem by using quadratic algorithms.",
                    "label": 0
                },
                {
                    "sent": "They also have the same problem with respect to resource management if you try to solve the same problem that is, linking DB pedia with geonames an you only need one bite to solve to basically store such a pair, you need approximately 125 terabytes of RAM to basically.",
                    "label": 0
                },
                {
                    "sent": "Run your algorithm.",
                    "label": 0
                },
                {
                    "sent": "If you go for a brute force approach, so that's not going to work either and.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The third problem we faced with is actually the complexity of link specifications themselves, so this is a specification for a very simple problem actually, which is linking directors from the picture with the linked movie database.",
                    "label": 1
                },
                {
                    "sent": "And as you can see we have to use quite a few different similarity functions here to cover all the corner cases and get.",
                    "label": 0
                },
                {
                    "sent": "A high precision, high recall researcher specification.",
                    "label": 0
                },
                {
                    "sent": "We're not going to deal with the complexity of in specifications today, we're much more going to be concerned with the time complexity problem.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over the years, several approaches have been developed to solve or address at least the time complexity problem and what they do in most cases is that try to reduce the number CA of comparisons that they carry out.",
                    "label": 0
                },
                {
                    "sent": "In other terms, where they tried to do is maximized the so called reduction ratio which is 1 -- C divided by the complexity of the problem, which is size of S times size of T.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main question then behind our work was country device.",
                    "label": 0
                },
                {
                    "sent": "Loopless approaches with a guaranteed reduction ratio, and I'm going to explain in a second what we mean by guaranteed reduction ratio.",
                    "label": 1
                },
                {
                    "sent": "But if we had such approaches, we actually tremendous because we could run proper space management.",
                    "label": 0
                },
                {
                    "sent": "We could approximate the runtime of the algorithms in a way more efficient manner and we could do resource scheduling so we could know this algorithm is going to take this much time in this much space.",
                    "label": 0
                },
                {
                    "sent": "So in my cloud I can give it only two nodes.",
                    "label": 0
                },
                {
                    "sent": "Instead of five when I don't know how much resources algorithm is actually going to need.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Before we continue to.",
                    "label": 0
                },
                {
                    "sent": "Present one variable the best achievable reduction ratio is actually our Max, which is 1 -- M. Prime was the size of all the pairs that are such that Delta is less or equal to teta.",
                    "label": 0
                },
                {
                    "sent": "And the best achievable reduction ratio are Max is 1 minus the size of M prime divided by the complexity of the problem at hand.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now when we say that we want to approach an operator, has a guaranteed reduction ratio.",
                    "label": 0
                },
                {
                    "sent": "What we mean is that for all ours that are less than our Max, we want to have an approach that is such that there is a parameter of or a set of parameters are for that little approach having a reduction ratio larger or equal to R. That's basically what we aim at.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we want to have and throughout the talk, or simply use the.",
                    "label": 0
                },
                {
                    "sent": "Relative reduction ratio, which is our marks divided by our of a.",
                    "label": 1
                },
                {
                    "sent": "It's important to notice here that what we want to have is a relative reduction ratio that goes to one and most approaches or all approaches.",
                    "label": 0
                },
                {
                    "sent": "Actually we always have reduction ratio that's larger than one.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, formally, what's the goal here?",
                    "label": 0
                },
                {
                    "sent": "The goal is to device an approach.",
                    "label": 0
                },
                {
                    "sent": "Each of our four that is such that for all ours larger than one there is an Alpha that is a set of parameters that is such that the reduction ratio of our approach is less or equal to R. That's basically formally what we're trying to do.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's obviously a restriction here.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "So far we only deal with affine spaces that contain Minkowski distances and incomplete distances.",
                    "label": 0
                },
                {
                    "sent": "That's a fancy word for a generalization of the Euclidean distance, so you have squares in the Euclidean distance, Ann from its course, Minkowski distance.",
                    "label": 1
                },
                {
                    "sent": "We simply have a value P that is somewhere between zero and Infinity Ann.",
                    "label": 0
                },
                {
                    "sent": "These little shapes here actually show you how a circle looks like in spaces with Minkowski distances for different values of P. So this is the circle as we know it.",
                    "label": 0
                },
                {
                    "sent": "That's the circle for sqrt 2, and so on and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "So the basis for the algorithm that I'm going to present that actually has these nice properties that we wanted to have is the ideal space tiling which was implemented in the heap algorithm which was presented last year at the ontology matching Workshop.",
                    "label": 0
                },
                {
                    "sent": "The basic intuition here is that Delta SC less or equal to to actually describes a hypersphere.",
                    "label": 1
                },
                {
                    "sent": "Technically, it's a hyper bowl, but also hyper fear for the rest of the talk, just for the sake of simplicity, and we can approximate a hypersphere by using a hypercube.",
                    "label": 0
                },
                {
                    "sent": "Main advantages, obviously that hypercubes are way easier to compute directly, and if we can have a hypercube that always contains all the points that are in the hyper sphere, we also have no loss of recall, which is a property that we definitely want to have.",
                    "label": 0
                },
                {
                    "sent": "That is, in contrast to some blocking approaches that do lose recall.",
                    "label": 0
                },
                {
                    "sent": "So graphically what we've got here is we've got the point S, and that is basically the set of points that we are actually interested in.",
                    "label": 0
                },
                {
                    "sent": "I want to approximate this by using a hypercube.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do we do that?",
                    "label": 0
                },
                {
                    "sent": "But we begin by setting the width of a single hypercube.",
                    "label": 0
                },
                {
                    "sent": "2D equals Delta divided by Alpha.",
                    "label": 0
                },
                {
                    "sent": "This equation is going to be spread around the whole talk, so it's quite important that you remember that Delta is to divided by Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then retired.",
                    "label": 0
                },
                {
                    "sent": "The space simply retired the whole space into adjacent cube C. We give them a whole number coordinates and say that each cube C contains the points Omega data such that CI times Delta is less or equal to Omega I, which is less than CI plus one times Delta.",
                    "label": 0
                },
                {
                    "sent": "So all we do basically.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "We take the space, we chop it into hypercubes, and then we take the smallest set of hypercube that contain the whole of the points that are supposed to be in our hyper sphere.",
                    "label": 0
                },
                {
                    "sent": "This basically shows what happens when we set Alpha to one, and if P = 2, that is.",
                    "label": 0
                },
                {
                    "sent": "Basically, we're dealing with a Euclidean space here.",
                    "label": 0
                },
                {
                    "sent": "Then we need 9 squares to approximate.",
                    "label": 0
                },
                {
                    "sent": "Excuse me to approximate this circle.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can actually go forward further and compute.",
                    "label": 0
                },
                {
                    "sent": "What happens when we change the value of Alpha and we can prove that we need exactly 2A plus one to the power N hypercubes to approximate such a sphere?",
                    "label": 0
                },
                {
                    "sent": "An interesting property of the algorithm is obviously that if we increase the value of Alpha, we actually approximation gets better and better.",
                    "label": 0
                },
                {
                    "sent": "Now the question is, does the fact that the quality of the approximation converges that sufficient?",
                    "label": 0
                },
                {
                    "sent": "We already have an algorithm that has the nice properties that we want to have.",
                    "label": 0
                },
                {
                    "sent": "And the bad news is no, we did not.",
                    "label": 0
                },
                {
                    "sent": "We can actually see exactly what they are of the hip algorithm would be if you had a uniform distribution of points across the space, it would be 2A plus one to the power of N divided by Alpha to the both end times the volume of the unit sphere in North dimensions.",
                    "label": 0
                },
                {
                    "sent": "And if you let half ago towards Infinity is 2 to the power of 10 divided by the volume of the sphere.",
                    "label": 0
                },
                {
                    "sent": "Dimensions are.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just plotted to make it easier.",
                    "label": 0
                },
                {
                    "sent": "So this is basically a kind of plot that you get as you see, we converge towards the value, but that value is not one, so we don't have what we wanted to have.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to compute the exact values that we get, and you can see that for N = 2, we get a limit that is around 1.2 seven.",
                    "label": 0
                },
                {
                    "sent": "That's 4 / \u03c0, and for N = 4 you get 32 / \u03c0 square.",
                    "label": 1
                },
                {
                    "sent": "So we do converge, but not towards one.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the basic idea behind the H R3 algorithm which has these properties is to use an indexing scheme.",
                    "label": 0
                },
                {
                    "sent": "So basically to add an indexing scheme to the Hippo algorithm as explained already.",
                    "label": 0
                },
                {
                    "sent": "Basic idea here is that we will assume that this cube, which is acutely interested in that it has the coordinates 00000, and so on.",
                    "label": 0
                },
                {
                    "sent": "No, all the cubes that are adjacent to the X. I'm sorry the X and the Y axis.",
                    "label": 0
                },
                {
                    "sent": "So basically this set of cubes over here and that and this sort of Q over here.",
                    "label": 0
                },
                {
                    "sent": "They basically get the index zero.",
                    "label": 0
                },
                {
                    "sent": "Very simple and all the other cubes they get this index which is.",
                    "label": 0
                },
                {
                    "sent": "The coordinate of the cube minus the coordinator of this, which is zero in our case, minus 1 to the power of P, and we sum it up overall dimensions just to tell you how it works.",
                    "label": 0
                },
                {
                    "sent": "So this cube over here for example has the coordinates 2, two, which basically means that it gets the index 2 -- 0 -- 1 across this dimension.",
                    "label": 0
                },
                {
                    "sent": "Squared plus 2 minus or minus one across this dimension squared and that gives it an index of two and we can do that for all the other cubes.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, and the basic insight behind HR 3 is that.",
                    "label": 0
                },
                {
                    "sent": "All we need to do is only compare the elements of this cubes with cubes that are such that the index is less or equal to offer to the power of P. That is basically what happens here.",
                    "label": 0
                },
                {
                    "sent": "In this example.",
                    "label": 0
                },
                {
                    "sent": "This is basically what people would run, but H R3 can actually discards this four cubes simply because they have an index of 18.",
                    "label": 0
                },
                {
                    "sent": "An 18 is greater than 4 squared, which is 16, so we can actually discuss those cubes and we do not need to run the computation there.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I stated before translates to this.",
                    "label": 0
                },
                {
                    "sent": "We want an algorithm that does that has no loss of recall.",
                    "label": 1
                },
                {
                    "sent": "That's the first thing that we need to prove, and then we need to prove that the limit of the error of this algorithm when Alpha goes towards Infinity is one.",
                    "label": 0
                },
                {
                    "sent": "That's basically what we need to do here.",
                    "label": 0
                },
                {
                    "sent": "R.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, with the first thing.",
                    "label": 0
                },
                {
                    "sent": "Basic idea here is very simple.",
                    "label": 0
                },
                {
                    "sent": "What we need to prove before we can actually prove that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that we do not lose any recall is that if the index of civil respect to X is active, respect to X is X, then the distance between S&T for cheese that I see to power P is must be larger than X times Delta to the power of P.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then actually simply comes out of the construction of the index, which is kind of the secret source here.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this.",
                    "label": 0
                },
                {
                    "sent": "Basically what this value tells us the value of five is simply tells us that all points here the distance between our points here and there will always be larger than five and five is nothing else that this length squared for P = 2.",
                    "label": 0
                },
                {
                    "sent": "Thus, we can actually assume we actually know that for all points here and opens there, the distance will always be superior to five and.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can prove that formally basic idea here is that we take the definition of the index as shown before and.",
                    "label": 0
                },
                {
                    "sent": "Out of the definition of the cube, we basically said every segment this piece.",
                    "label": 1
                },
                {
                    "sent": "We know that they across the ice dimension that as I'm honesty, I must be larger than this value, and if you Simply put a power of pure everywhere and sum it up, overall dimensions you actually get dilemma that we were talking about now that was very interesting about this lemma is.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But what it means is that for all S in S, if the index of Siri respect to S is larger than Alpha to the power of P, then all the TS in C must be non matches or we need to do is replace X with Alpha to both be in the equation I showed before and we remember that Delta is Delta divided by Alpha.",
                    "label": 0
                },
                {
                    "sent": "So we have Delta report P times are for above pendek users data report piece.",
                    "label": 0
                },
                {
                    "sent": "So we know the distances will always be larger so we do not discard anything.",
                    "label": 0
                },
                {
                    "sent": "That should not be discard.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's step one.",
                    "label": 0
                },
                {
                    "sent": "We know that we do not lose recall.",
                    "label": 0
                },
                {
                    "sent": "Step 2 is now.",
                    "label": 0
                },
                {
                    "sent": "Do we actually have an algorithm that is such that there is always a set of parameters that leads to whatever reduction ratio we want?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The proof of this third lemma is pretty involved, so I'm not going to go too much into detail there or just going to give you the basic idea behind the proof, and it is the following.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "If we double the value of Alpha, we can be sure that we actually augment the reduction ratio of the approach.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the example that we had before where we discarded only these four corners.",
                    "label": 0
                },
                {
                    "sent": "Now if we double the value of our family set off to 8, whatever.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Happens is that we do not only discard the corners we have here 16 order smaller cubes that we can discard, which basically means that our reduction ratio of men's and we can do that for any.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Value of offer.",
                    "label": 0
                },
                {
                    "sent": "If offer is 25 and you double the value.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me just.",
                    "label": 0
                },
                {
                    "sent": "You can see that actually deserve.",
                    "label": 0
                },
                {
                    "sent": "It gets we have to do less computations, basically.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the interesting thing about what I just showed is that the hip algorithm also has that property.",
                    "label": 0
                },
                {
                    "sent": "As I showed before.",
                    "label": 0
                },
                {
                    "sent": "So if you double the value of actually you get an even better approximation.",
                    "label": 0
                },
                {
                    "sent": "So we know that we converge towards the value.",
                    "label": 0
                },
                {
                    "sent": "But we're not sure they would converge towards one.",
                    "label": 0
                },
                {
                    "sent": "So how do we show that we converge towards one again?",
                    "label": 0
                },
                {
                    "sent": "Again, just the proof behind the idea or the idea behind the proof?",
                    "label": 0
                },
                {
                    "sent": "That way around is that if Alpha goes towards Infinity then Delta goes towards zero.",
                    "label": 0
                },
                {
                    "sent": "It basically means that if we look at the index function and multiply all terms by Delta to the power of P. What happens is that we have data report P times.",
                    "label": 0
                },
                {
                    "sent": "This term is less or equal to Delta to the power of three times after the power P. Now if you put Delta in here, you basically get Delta time CI, which is actually the coordinate of South and you have here Delta Times Delta T series of S is according to South Delta Times.",
                    "label": 0
                },
                {
                    "sent": "CI is the coordinate of TB cause the cube has reduced to just one point which is T itself and you have here minus Delta but Delta goes towards 0.",
                    "label": 0
                },
                {
                    "sent": "So we can actually get rid of that.",
                    "label": 0
                },
                {
                    "sent": "And if you then sum it up, you get exactly the equation that we had at the beginning.",
                    "label": 0
                },
                {
                    "sent": "Basic idea is we do not compute anything that we should not be computing, which means that the reduction ratio is exactly 1.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we there we have an approach that does not lose recall.",
                    "label": 0
                },
                {
                    "sent": "An actually has the nice property that the reduction ratio can be set to any value we want.",
                    "label": 0
                },
                {
                    "sent": "Now the question is.",
                    "label": 0
                },
                {
                    "sent": "Does it perform well?",
                    "label": 0
                },
                {
                    "sent": "I mean, it's nice to have this theoretically, but can we actually outperformed the state of the art respect to anything we actually run?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiments.",
                    "label": 0
                },
                {
                    "sent": "We compared our approach with the Hippogriff himself and with algorithm implemented in the Silk Framework version 251, and we did sell for experiments.",
                    "label": 0
                },
                {
                    "sent": "One where we did Applicated deeply job places by using the minimum elevation, elevation, the maximum elevation, and we also try to link cities in geonames, unlinked geodata by longitude and latitude and all experiments were carried out on the laptop over there.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can show is that even for small experiments where we only had eight 9 million computations, we can already get rid of 640,000 comparisons for energy value of 32.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we if the experiment is larger, obviously we get rid of even more computations here.",
                    "label": 0
                },
                {
                    "sent": "4.3 million computations for Alpha equals 32.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, this looks different anyway.",
                    "label": 0
                },
                {
                    "sent": "It also shows that the reduction ratio really converges towards one here.",
                    "label": 0
                },
                {
                    "sent": "This is the line for HR3 and there as well.",
                    "label": 0
                },
                {
                    "sent": "So if we actually augment the value of our, we also get practically what we expected.",
                    "label": 0
                },
                {
                    "sent": "Theoretically, a reduction ratio that goes towards.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One an even respecter runtime, especially here we could show that H R3 actually outperforms the Hippo algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also outperforms the algorithm that is implemented in this silk framework.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the mission behind our work was actually 2.",
                    "label": 0
                },
                {
                    "sent": "Create a new category of algorithms for link discovery.",
                    "label": 1
                },
                {
                    "sent": "So far we've had algorithms that promise that they are faster than put forth, and there are.",
                    "label": 0
                },
                {
                    "sent": "Indeed there's no discussion there, but we don't know how fast they are.",
                    "label": 0
                },
                {
                    "sent": "We can't really tune the algorithm to fit whatever resources we have, and the advantage of the algorithm we developed is that we can actually tune it to do and to behave the way you want it to behave within.",
                    "label": 0
                },
                {
                    "sent": "Obviously the reasonable boundaries of of the minimum or maximum RR, and that can be achieved theoretically.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yes, and in future work the first thing we want to do is combine this approach with multi indexing approaches.",
                    "label": 1
                },
                {
                    "sent": "The idea being simply that we deal with N dimensional data.",
                    "label": 0
                },
                {
                    "sent": "So if you use multi indexing you might be actually be even faster right now and now that we know that we can actually tune the reduction ratio of the algorithm we wanted device resource management approaches that make use of that and.",
                    "label": 1
                },
                {
                    "sent": "On the linked open Data Cloud, we have plenty of strings and plenty of numbers, so the third thing that we want to do is develop algorithms for strings that have similar, if not the same theoretical guarantees.",
                    "label": 1
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That was it from my side.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for listening and if you have any questions please bring them on.",
                    "label": 0
                }
            ]
        }
    }
}