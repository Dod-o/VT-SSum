{
    "id": "24kaqbkrkxgoajgiwd5foequozvtdyzm",
    "title": "LOD Lab: Experiments at LOD Scale",
    "info": {
        "author": [
            "Laurens Rietveld, Faculty of Sciences, Vrije Universiteit Amsterdam (VU)"
        ],
        "published": "Nov. 10, 2015",
        "recorded": "October 2015",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2015_rietveld_lod_scale/",
    "segmentation": [
        [
            "So yes, my name is Gladys Knight 4th and I'm interested lab work I did together without the bacon.",
            "Steven sloba.",
            "For those who can't read the slides properly.",
            "Check out presentations.",
            "Those loud and sheets of betrayal less.",
            "I SWC.",
            "You can get the online version.",
            "Anne."
        ],
        [
            "Let me start off with applauds this this chart part of this bar chart shows the research paper of loss as just the research papers of plus size WC and the datasets that they use.",
            "We see on the left side that's the pedia is by far the most popular.",
            "One is used to have times.",
            "Let me check this.",
            "It's used 12 times and the other datasets are used four times or less.",
            "So.",
            "And on average, the number of datasets that are used in research papers is about two, I think in last year.",
            "Um, so this shows us that.",
            "We don't.",
            "We do not take the truth, variety of link data into accounts in the rest of this presentation, we will show you that there are hundreds and hundreds of thousands of linked data documents around, and yet we only optimize for these few.",
            "Anne.",
            "So our results are less generalizable than they should be.",
            "So why is this?",
            "Why is it difficult to evaluate on a much larger scale?"
        ],
        [
            "And we think there are three reasons for this.",
            "The first one is messy datasets.",
            "I think we've all encountered problems with parsing several datasets, fixing syntax errors, which everybody probably doesn't have different way parsing it again or problems with linked data files that are incorrect.",
            "Compressed archives.",
            "Or failure to access datasets via HTTP?",
            "That's a problem, of course, but that's also a problem to find the datasets we have the data set catalogs, but they only cover a smaller part of selling data clouds.",
            "And if you have a particular interest in datasets with certain structural properties, it is even more hard to find these datasets.",
            "We have voids, but again uptake of for it isn't 100%.",
            "Am I finally, there's no uniform means for accessing link data.",
            "We have sparkle, but again, Sparkle is only a subset of all the link data that is out there.",
            "And we need.",
            "Uniform means for accessing both files.",
            "All over the web."
        ],
        [
            "So we think all these three problems using three components and these three components from the look lab two kits.",
            "The first components targets the problem of messy datasets in this little mouth, presented at last year's ISO bucy.",
            "The problem will find ability.",
            "We approach that by providing metadata to the laundromat documents and the problem of Accessibility is.",
            "Our solution to that is providing a single means for accessing both the documents and their metadata.",
            "I'll talk about these three components a bit in the next few slides."
        ],
        [
            "Am first finished a lot longer amounts that tries to solve the problem of messy data."
        ],
        [
            "So the loan amounts since last year it has crawled over 650,000 link data documents.",
            "Anne and Republishes, these as gzipped quotes or in triples.",
            "And since you said you see this year.",
            "We also provide triple pattern API's for each of these documents, so documents that are otherwise often static files on the web are now queryable, fired a longer amount."
        ],
        [
            "But ironically, even having all these link data files in one place, it's still very difficult to find datasets based on certain characteristics.",
            "Our approach to this problem is twofold."
        ],
        [
            "First, we describe these documents structurally.",
            "This is iaccessible fighter laundromats sparkle endpoints.",
            "So we provide aggregate descriptions.",
            "Must like the voids, does so that's the number of triples, the number of resources, etc.",
            "We also provide syntactic descriptions.",
            "Think about the average number.",
            "The average literal lengths in documents.",
            "And we also provide network properties.",
            "Think the median in degree in a certain datasets.",
            "And next to these structures."
        ],
        [
            "Scriptions we also provide contents, descriptions, Iaccessible fire index last longer amount of work.",
            "Anne and this index.",
            "It's a simple index, but it gives you a mapping between the resources and the documents they occur in in.",
            "The same goes for the namespaces."
        ],
        [
            "So we have these APIs, the sparkle endpoints and all these documents in one place.",
            "But still to access these and to integrate this in your evaluation pipeline is difficult.",
            "You need to use lots of API requests.",
            "You need to merge the request.",
            "You need to handle pagination, stuff like that, it's work.",
            "So we developed Frank."
        ],
        [
            "Anne Frank is the glue between the laundromat surface, the sparkle endpoints and the index.",
            "Anne Frank is a commandline tool but.",
            "So that means that it can use best pipes to talk to the actual experiments.",
            "The experiments might be written in Python in C or Java it doesn't matter, but you can pass on.",
            "Or the output of Frank to your experiments.",
            "So something so some examples of what Frank actually does."
        ],
        [
            "Um, this example I try to minimize the best in this presentation, but this example shows you how you can fetch statements from the lunar month with the predicate FOAF name, and in this case I only want the top five.",
            "And those are the results.",
            "But other than fetching statements, you can also fetch document references.",
            "So download location for instance of a longer more documents."
        ],
        [
            "That happens like this.",
            "I run Frank documents.",
            "I say I want documents with namespace where the namespace void occurs in.",
            "And I want at least a document with 1000 triples.",
            "And the nice thing with the best pipes as we can, we can combine these commands.",
            "So in this case we ask for datasets with at least 1000 triples with the Ford namespace, and then for those documents we want to get the statements where folk name occurs in the predicate position.",
            "So what happens in the background?"
        ],
        [
            "I'll show you.",
            "First we have to Frank documents commands.",
            "It talks to the to the namespace index, gets all the documents that.",
            "That have to avoid namespace.",
            "Then I filter this list of documents using this park or endpoints by only selecting the documents will have at least 1000 triples.",
            "An then I pipe this to the Frank statements commands.",
            "The stream of documents is then filtered using the resource index.",
            "Because I only want documents which have both name somewhere.",
            "And for that set of documents I execute.",
            "Ripple pattern requests to the triple pattern API an output the statements.",
            "Ann, this is all done using IX requests.",
            "Frank bundles these requests together does all the work for you and this is fast is so it's a stream you get and the 1st results are often within a second."
        ],
        [
            "So now I've showed you a better look, lab two kits, the infrastructure.",
            "So we're going to showcase local lab on three previous semantic Web applications.",
            "So this is not a reproducibility paper.",
            "I'm not going to try to reproduce this paper.",
            "I won't do that justice.",
            "But I'm simply cherry picking a small part of their evaluation and showcase how using Lot lab you can scale these experiments.",
            "Much larger scale.",
            "The first papers, RDF foods.",
            "This is a paper published at Easter.",
            "We see this year by Ham.",
            "It's possible Andy and others.",
            "And this is a in memory RDF dictionary.",
            "Anne.",
            "The 2nd paper is is from the HTTP people.",
            "Men author is half anonymous.",
            "Published at Web sites 2013.",
            "And this is compressed queryable.",
            "Binary file formats.",
            "In the last paper, is linked at the best practices.",
            "Published by smacking Berg and others at last years as Debussy.",
            "And this is a more exploitive paper, where they measure the use of particular namespaces, vocabularies interlinks between datasets, and much more.",
            "So I'll start off with the RDF old paper."
        ],
        [
            "This is the in memory RDF dictionary.",
            "And on the left side we have the audio false results on the right side we have the lab results.",
            "We see that.",
            "So what this shows you is the compression time in NS per entity.",
            "And the colors are so blue is for YRI rettich for literals, and yellow is for both and the vertical axis is log scale.",
            "An an RDF lock lab was evaluated against the Building Triple Challenge.",
            "The PDF free base.",
            "And Spire portal.",
            "And on the right side we see lot map.",
            "We do the same.",
            "We do the same experiments but we group the documents by buckets of documents where size up to 1000 triples 1000 to 100,000 etc etc.",
            "The first interesting thing we see is that very small documents they have.",
            "Much larger encoding time per entity than the larger documents.",
            "That's one thing we can derive from this.",
            "But again, this is not a reproducibility study, so I won't drive any conclusions from this.",
            "This is just a showcase.",
            "So how did we do this?"
        ],
        [
            "We simply ran Frank documents.",
            "We asked for the download to arrive.",
            "And we say I want at least 1000 triples and at maximum 100,000 triples in this document, and we stream this to the actual experiment."
        ],
        [
            "The 2nd paper is already FAC team and this part of their evaluation.",
            "They, if elevated HCT against unit Broads in different sizes.",
            "In Lot lab, so that's on the left parts and they measure the compression ratio of the HTT files.",
            "On the right side we have looked lab again.",
            "We select documents which have almost the same size as the unit sizes.",
            "An interesting thing here is that mobile app has.",
            "Quite a big variety in the results.",
            "Somehow it seems that the data set dimension of size isn't a really good predictor of how the compression ratio will.",
            "Compression ratio will be.",
            "So we thought, let's try some different dimensions."
        ],
        [
            "So we try to group documents by the average degree.",
            "So we grouped them in small average degree, medium and large.",
            "And then we see a clear pattern here that datasets with small average degree have a very high compression ratio and datasets with a high average degree have a very small compression ratio.",
            "Anne.",
            "And to show you how this works again."
        ],
        [
            "Small best commands.",
            "I run Frank documents.",
            "I once only datasets with the minimum average degree of five and a maximum of 10, and again I pipe and I stream these results to the extra experiments."
        ],
        [
            "In the final datasets, this is the more exploitative paper on Link data best practices.",
            "In this example, they count the number the popularity of namespaces in datasets, normal left hand side.",
            "We have the.",
            "We have to original results right inside the longer amounts and you can see these differ quite a bit and a probable reason for this is.",
            "Calling methods where in the original paper they use the reference for your eyes.",
            "And as a consequence, the scale is.",
            "Large as the month.",
            "Anne.",
            "And again, how we do this experiment?"
        ],
        [
            "Is now simply by fetching all the Frank documents, so not with any filter.",
            "Can we stream this to the experiments to count all the namespaces?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yes, my name is Gladys Knight 4th and I'm interested lab work I did together without the bacon.",
                    "label": 0
                },
                {
                    "sent": "Steven sloba.",
                    "label": 0
                },
                {
                    "sent": "For those who can't read the slides properly.",
                    "label": 0
                },
                {
                    "sent": "Check out presentations.",
                    "label": 0
                },
                {
                    "sent": "Those loud and sheets of betrayal less.",
                    "label": 0
                },
                {
                    "sent": "I SWC.",
                    "label": 0
                },
                {
                    "sent": "You can get the online version.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me start off with applauds this this chart part of this bar chart shows the research paper of loss as just the research papers of plus size WC and the datasets that they use.",
                    "label": 0
                },
                {
                    "sent": "We see on the left side that's the pedia is by far the most popular.",
                    "label": 0
                },
                {
                    "sent": "One is used to have times.",
                    "label": 0
                },
                {
                    "sent": "Let me check this.",
                    "label": 0
                },
                {
                    "sent": "It's used 12 times and the other datasets are used four times or less.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And on average, the number of datasets that are used in research papers is about two, I think in last year.",
                    "label": 0
                },
                {
                    "sent": "Um, so this shows us that.",
                    "label": 0
                },
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "We do not take the truth, variety of link data into accounts in the rest of this presentation, we will show you that there are hundreds and hundreds of thousands of linked data documents around, and yet we only optimize for these few.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So our results are less generalizable than they should be.",
                    "label": 0
                },
                {
                    "sent": "So why is this?",
                    "label": 0
                },
                {
                    "sent": "Why is it difficult to evaluate on a much larger scale?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we think there are three reasons for this.",
                    "label": 0
                },
                {
                    "sent": "The first one is messy datasets.",
                    "label": 1
                },
                {
                    "sent": "I think we've all encountered problems with parsing several datasets, fixing syntax errors, which everybody probably doesn't have different way parsing it again or problems with linked data files that are incorrect.",
                    "label": 0
                },
                {
                    "sent": "Compressed archives.",
                    "label": 0
                },
                {
                    "sent": "Or failure to access datasets via HTTP?",
                    "label": 0
                },
                {
                    "sent": "That's a problem, of course, but that's also a problem to find the datasets we have the data set catalogs, but they only cover a smaller part of selling data clouds.",
                    "label": 0
                },
                {
                    "sent": "And if you have a particular interest in datasets with certain structural properties, it is even more hard to find these datasets.",
                    "label": 1
                },
                {
                    "sent": "We have voids, but again uptake of for it isn't 100%.",
                    "label": 0
                },
                {
                    "sent": "Am I finally, there's no uniform means for accessing link data.",
                    "label": 0
                },
                {
                    "sent": "We have sparkle, but again, Sparkle is only a subset of all the link data that is out there.",
                    "label": 0
                },
                {
                    "sent": "And we need.",
                    "label": 0
                },
                {
                    "sent": "Uniform means for accessing both files.",
                    "label": 0
                },
                {
                    "sent": "All over the web.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we think all these three problems using three components and these three components from the look lab two kits.",
                    "label": 0
                },
                {
                    "sent": "The first components targets the problem of messy datasets in this little mouth, presented at last year's ISO bucy.",
                    "label": 0
                },
                {
                    "sent": "The problem will find ability.",
                    "label": 0
                },
                {
                    "sent": "We approach that by providing metadata to the laundromat documents and the problem of Accessibility is.",
                    "label": 0
                },
                {
                    "sent": "Our solution to that is providing a single means for accessing both the documents and their metadata.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about these three components a bit in the next few slides.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Am first finished a lot longer amounts that tries to solve the problem of messy data.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the loan amounts since last year it has crawled over 650,000 link data documents.",
                    "label": 0
                },
                {
                    "sent": "Anne and Republishes, these as gzipped quotes or in triples.",
                    "label": 0
                },
                {
                    "sent": "And since you said you see this year.",
                    "label": 0
                },
                {
                    "sent": "We also provide triple pattern API's for each of these documents, so documents that are otherwise often static files on the web are now queryable, fired a longer amount.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But ironically, even having all these link data files in one place, it's still very difficult to find datasets based on certain characteristics.",
                    "label": 0
                },
                {
                    "sent": "Our approach to this problem is twofold.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, we describe these documents structurally.",
                    "label": 0
                },
                {
                    "sent": "This is iaccessible fighter laundromats sparkle endpoints.",
                    "label": 0
                },
                {
                    "sent": "So we provide aggregate descriptions.",
                    "label": 1
                },
                {
                    "sent": "Must like the voids, does so that's the number of triples, the number of resources, etc.",
                    "label": 1
                },
                {
                    "sent": "We also provide syntactic descriptions.",
                    "label": 0
                },
                {
                    "sent": "Think about the average number.",
                    "label": 1
                },
                {
                    "sent": "The average literal lengths in documents.",
                    "label": 0
                },
                {
                    "sent": "And we also provide network properties.",
                    "label": 0
                },
                {
                    "sent": "Think the median in degree in a certain datasets.",
                    "label": 0
                },
                {
                    "sent": "And next to these structures.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scriptions we also provide contents, descriptions, Iaccessible fire index last longer amount of work.",
                    "label": 0
                },
                {
                    "sent": "Anne and this index.",
                    "label": 0
                },
                {
                    "sent": "It's a simple index, but it gives you a mapping between the resources and the documents they occur in in.",
                    "label": 0
                },
                {
                    "sent": "The same goes for the namespaces.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have these APIs, the sparkle endpoints and all these documents in one place.",
                    "label": 0
                },
                {
                    "sent": "But still to access these and to integrate this in your evaluation pipeline is difficult.",
                    "label": 0
                },
                {
                    "sent": "You need to use lots of API requests.",
                    "label": 0
                },
                {
                    "sent": "You need to merge the request.",
                    "label": 0
                },
                {
                    "sent": "You need to handle pagination, stuff like that, it's work.",
                    "label": 0
                },
                {
                    "sent": "So we developed Frank.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne Frank is the glue between the laundromat surface, the sparkle endpoints and the index.",
                    "label": 1
                },
                {
                    "sent": "Anne Frank is a commandline tool but.",
                    "label": 0
                },
                {
                    "sent": "So that means that it can use best pipes to talk to the actual experiments.",
                    "label": 0
                },
                {
                    "sent": "The experiments might be written in Python in C or Java it doesn't matter, but you can pass on.",
                    "label": 0
                },
                {
                    "sent": "Or the output of Frank to your experiments.",
                    "label": 0
                },
                {
                    "sent": "So something so some examples of what Frank actually does.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, this example I try to minimize the best in this presentation, but this example shows you how you can fetch statements from the lunar month with the predicate FOAF name, and in this case I only want the top five.",
                    "label": 0
                },
                {
                    "sent": "And those are the results.",
                    "label": 0
                },
                {
                    "sent": "But other than fetching statements, you can also fetch document references.",
                    "label": 0
                },
                {
                    "sent": "So download location for instance of a longer more documents.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That happens like this.",
                    "label": 0
                },
                {
                    "sent": "I run Frank documents.",
                    "label": 0
                },
                {
                    "sent": "I say I want documents with namespace where the namespace void occurs in.",
                    "label": 0
                },
                {
                    "sent": "And I want at least a document with 1000 triples.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing with the best pipes as we can, we can combine these commands.",
                    "label": 0
                },
                {
                    "sent": "So in this case we ask for datasets with at least 1000 triples with the Ford namespace, and then for those documents we want to get the statements where folk name occurs in the predicate position.",
                    "label": 0
                },
                {
                    "sent": "So what happens in the background?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll show you.",
                    "label": 0
                },
                {
                    "sent": "First we have to Frank documents commands.",
                    "label": 0
                },
                {
                    "sent": "It talks to the to the namespace index, gets all the documents that.",
                    "label": 0
                },
                {
                    "sent": "That have to avoid namespace.",
                    "label": 0
                },
                {
                    "sent": "Then I filter this list of documents using this park or endpoints by only selecting the documents will have at least 1000 triples.",
                    "label": 0
                },
                {
                    "sent": "An then I pipe this to the Frank statements commands.",
                    "label": 0
                },
                {
                    "sent": "The stream of documents is then filtered using the resource index.",
                    "label": 0
                },
                {
                    "sent": "Because I only want documents which have both name somewhere.",
                    "label": 0
                },
                {
                    "sent": "And for that set of documents I execute.",
                    "label": 0
                },
                {
                    "sent": "Ripple pattern requests to the triple pattern API an output the statements.",
                    "label": 0
                },
                {
                    "sent": "Ann, this is all done using IX requests.",
                    "label": 0
                },
                {
                    "sent": "Frank bundles these requests together does all the work for you and this is fast is so it's a stream you get and the 1st results are often within a second.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I've showed you a better look, lab two kits, the infrastructure.",
                    "label": 0
                },
                {
                    "sent": "So we're going to showcase local lab on three previous semantic Web applications.",
                    "label": 0
                },
                {
                    "sent": "So this is not a reproducibility paper.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to try to reproduce this paper.",
                    "label": 0
                },
                {
                    "sent": "I won't do that justice.",
                    "label": 0
                },
                {
                    "sent": "But I'm simply cherry picking a small part of their evaluation and showcase how using Lot lab you can scale these experiments.",
                    "label": 0
                },
                {
                    "sent": "Much larger scale.",
                    "label": 0
                },
                {
                    "sent": "The first papers, RDF foods.",
                    "label": 0
                },
                {
                    "sent": "This is a paper published at Easter.",
                    "label": 0
                },
                {
                    "sent": "We see this year by Ham.",
                    "label": 0
                },
                {
                    "sent": "It's possible Andy and others.",
                    "label": 0
                },
                {
                    "sent": "And this is a in memory RDF dictionary.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The 2nd paper is is from the HTTP people.",
                    "label": 0
                },
                {
                    "sent": "Men author is half anonymous.",
                    "label": 0
                },
                {
                    "sent": "Published at Web sites 2013.",
                    "label": 0
                },
                {
                    "sent": "And this is compressed queryable.",
                    "label": 0
                },
                {
                    "sent": "Binary file formats.",
                    "label": 0
                },
                {
                    "sent": "In the last paper, is linked at the best practices.",
                    "label": 0
                },
                {
                    "sent": "Published by smacking Berg and others at last years as Debussy.",
                    "label": 0
                },
                {
                    "sent": "And this is a more exploitive paper, where they measure the use of particular namespaces, vocabularies interlinks between datasets, and much more.",
                    "label": 0
                },
                {
                    "sent": "So I'll start off with the RDF old paper.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the in memory RDF dictionary.",
                    "label": 0
                },
                {
                    "sent": "And on the left side we have the audio false results on the right side we have the lab results.",
                    "label": 0
                },
                {
                    "sent": "We see that.",
                    "label": 0
                },
                {
                    "sent": "So what this shows you is the compression time in NS per entity.",
                    "label": 0
                },
                {
                    "sent": "And the colors are so blue is for YRI rettich for literals, and yellow is for both and the vertical axis is log scale.",
                    "label": 0
                },
                {
                    "sent": "An an RDF lock lab was evaluated against the Building Triple Challenge.",
                    "label": 0
                },
                {
                    "sent": "The PDF free base.",
                    "label": 0
                },
                {
                    "sent": "And Spire portal.",
                    "label": 0
                },
                {
                    "sent": "And on the right side we see lot map.",
                    "label": 0
                },
                {
                    "sent": "We do the same.",
                    "label": 0
                },
                {
                    "sent": "We do the same experiments but we group the documents by buckets of documents where size up to 1000 triples 1000 to 100,000 etc etc.",
                    "label": 0
                },
                {
                    "sent": "The first interesting thing we see is that very small documents they have.",
                    "label": 0
                },
                {
                    "sent": "Much larger encoding time per entity than the larger documents.",
                    "label": 0
                },
                {
                    "sent": "That's one thing we can derive from this.",
                    "label": 0
                },
                {
                    "sent": "But again, this is not a reproducibility study, so I won't drive any conclusions from this.",
                    "label": 0
                },
                {
                    "sent": "This is just a showcase.",
                    "label": 0
                },
                {
                    "sent": "So how did we do this?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We simply ran Frank documents.",
                    "label": 1
                },
                {
                    "sent": "We asked for the download to arrive.",
                    "label": 0
                },
                {
                    "sent": "And we say I want at least 1000 triples and at maximum 100,000 triples in this document, and we stream this to the actual experiment.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The 2nd paper is already FAC team and this part of their evaluation.",
                    "label": 0
                },
                {
                    "sent": "They, if elevated HCT against unit Broads in different sizes.",
                    "label": 0
                },
                {
                    "sent": "In Lot lab, so that's on the left parts and they measure the compression ratio of the HTT files.",
                    "label": 0
                },
                {
                    "sent": "On the right side we have looked lab again.",
                    "label": 0
                },
                {
                    "sent": "We select documents which have almost the same size as the unit sizes.",
                    "label": 0
                },
                {
                    "sent": "An interesting thing here is that mobile app has.",
                    "label": 0
                },
                {
                    "sent": "Quite a big variety in the results.",
                    "label": 0
                },
                {
                    "sent": "Somehow it seems that the data set dimension of size isn't a really good predictor of how the compression ratio will.",
                    "label": 0
                },
                {
                    "sent": "Compression ratio will be.",
                    "label": 0
                },
                {
                    "sent": "So we thought, let's try some different dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we try to group documents by the average degree.",
                    "label": 0
                },
                {
                    "sent": "So we grouped them in small average degree, medium and large.",
                    "label": 0
                },
                {
                    "sent": "And then we see a clear pattern here that datasets with small average degree have a very high compression ratio and datasets with a high average degree have a very small compression ratio.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And to show you how this works again.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Small best commands.",
                    "label": 0
                },
                {
                    "sent": "I run Frank documents.",
                    "label": 0
                },
                {
                    "sent": "I once only datasets with the minimum average degree of five and a maximum of 10, and again I pipe and I stream these results to the extra experiments.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the final datasets, this is the more exploitative paper on Link data best practices.",
                    "label": 1
                },
                {
                    "sent": "In this example, they count the number the popularity of namespaces in datasets, normal left hand side.",
                    "label": 0
                },
                {
                    "sent": "We have the.",
                    "label": 0
                },
                {
                    "sent": "We have to original results right inside the longer amounts and you can see these differ quite a bit and a probable reason for this is.",
                    "label": 0
                },
                {
                    "sent": "Calling methods where in the original paper they use the reference for your eyes.",
                    "label": 0
                },
                {
                    "sent": "And as a consequence, the scale is.",
                    "label": 0
                },
                {
                    "sent": "Large as the month.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And again, how we do this experiment?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is now simply by fetching all the Frank documents, so not with any filter.",
                    "label": 0
                },
                {
                    "sent": "Can we stream this to the experiments to count all the namespaces?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}