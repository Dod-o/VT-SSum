{
    "id": "ueikua6vo7oaoyz726ydixfhjvo3bklc",
    "title": "Lower bounds on the performance of polynomial-time algorithms for sparse linear regression",
    "info": {
        "author": [
            "Yuchen Zhang, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_zhang_algorithms/",
    "segmentation": [
        [
            "It's a great pleasure to present here.",
            "This is John Volk with Martin Wright and Michael Jordan, so in this presentation going to talk about some performance gap between the polynomial time algorithms, exponential time algorithm for sparse linear regression."
        ],
        [
            "In recent years, statisticians are more interested in the relation between computation and statistical accuracy, and there are active research differentiating the statistical optimal rate and concept optimal optimal rate computable in polynomial time, and there's two kinds of rates are in sometimes probably different and the performance gap between the polynomial time algorithm an exponential time algorithm have been established for several problems, including sparse PCA detection, submatrix detection, and learning halfspaces.",
            "So in this talk I'm going to focus on sparse linear regression, which is another classical problem in machine learning and statistics.",
            "By the way, there are two direct consequences of discovering this computation barriers in statistical estimation on my hand when the data scale is fixed, then having a limited computation power means that we have to expect a lower statistical statistical accuracy and on the other hand, when the desired accuracy is fixed, then having more data means faster computation.",
            "So this represents a very interesting tradeoff between data in time.",
            "First, I want to formalize the problem to solve."
        ],
        [
            "It is possible regression.",
            "We have observed a design matrix X which has N rows and columns.",
            "Because we are in the regime of high dimensionality, we assume that these greater than ND represented dimensionality and represent the sample size.",
            "We also observe a response vector Y which is equal to X times response factor.",
            "See the Star plus a random Gaussian noise W. To make the problem solver, we have to make some sparsity assumptions on the design matrix on the regression coefficient.",
            "So here we assume that key Cedar storage case bars, meaning at most K entries of Silver Star on 0.",
            "And will go off the goal of this parsing regression is to find the estimator seat ahead for Superstar.",
            "And here we consider the proper learning scenario where we require see that had to be case bars as well and we want to minimize the distance between seat ahead and see the star in terms of the prediction error which is defined here.",
            "On the slide.",
            "OK."
        ],
        [
            "For this problem there is a well known upper bound.",
            "If you consider the L0 based estimator which searches overall the K sparse vectors in the D dimensional space and find one that minimizes the empirical risk, then we know that the prediction error associated with this estimator is upper bounded by K times log D divided by it.",
            "This is a very general upper bound.",
            "Becausw imposes no constraint on the design matrix, so it is true for all design matrices.",
            "But what is the computation complexity of searching over order case prospector's in a dimensional space it is at least the truth is K. So it is not computationally intractable unless case very small.",
            "To resolve the computational tractability problem, people have invented many convex relaxation method for sparse linear regression.",
            "Here, for example, we consider the truncated lasso estimator, which is running less so and then truncate the resulting vector to be a case prospector.",
            "There's an upper bound associated with this truncated also estimator, which is similar to the upper bound to for the L0 based estimator.",
            "Except this there's a dependence on the restricted eigenvalues of the design matrix, so you can see that despite this red dependence on Gamma Square, we're gonna miss the restricted eigenvalues of the design matrix.",
            "This upper bound is the same as the upper bound for L0 based estimator.",
            "Now you may be curious about what is the definition of the restricted ragamala order restricting mental condition to the finest concept that consider a coin in the dimensional space such that any vector belongs to this coin if and only if."
        ],
        [
            "Is L1 norm with respect to a small subset of coordinates, it roughly dominates the overall L1 norm, or in other words, this coin defines a convex relaxation for a set of K sparse vectors.",
            "And matrix ax is set to satisfy their equivalent condition.",
            "If always singular values in this case bars corn is upper bounded by is lower bounded by gamma.",
            "Because Karma is a function of the design matrix X, it is also a function of the dimensionality D sample size N and sparsity level K. So in this upper bound, gamma cannot be treated as a constant."
        ],
        [
            "Now we know that there is a one over Gamma Square performance gap between the truncated also estimator and the L0 based estimator, and we also know that gamma could be arbitrarily close to 0.",
            "So this guy could be arbitrarily large.",
            "A natural question is, is there a way to close this gap?",
            "There are several possibilities.",
            "It is possible that the existing analysis on the solar overly conservative but less so can actually achieve the same convergence rate as the L0 based estimator.",
            "Another possibility is that also is not good enough, but there exists a more clever polynomial time algorithms that can achieve the same convergence rate as the LO.",
            "Better L0 based estimator.",
            "At the most pessimistic possibility is that getting rid of this dependence on one over Gamma Square is impossible for all polynomial time algorithms, which is the statement of our main result.",
            "More specifically, we have shown that first we assume that MP is not a sub."
        ],
        [
            "Of people which is widely believed in the complexity theory and we show that for any and the NK which are assembled size dimensionalities spots to level an for any constant gamma which is greater than zero, we can always find the design matrix X which satisfies the gammas restricted mental condition and social prediction error is lower bounded by K to the one minus Delta times log divided by Gamma Square.",
            "So comparing to the upper bound for the truncated also estimator, we see that this lower bound is almost tight, despite this small Delta dependence.",
            "But Delta could be arbitrarily close to 0, which is a constant.",
            "So it means that the dependence on one over Gamma Square is essential dependence for all polynomial time algorithms.",
            "But it is also important to notice that this lower bound doesn't give a necessary and sufficient conditions necessary and sufficient condition for all design matrices to for the performance gap to appear.",
            "There are design matrices which satisfied which doesn't satisfy any restrictive mental condition for any gamma, but appears that it is easy to be solved by some polynomial time algorithm.",
            "Like for example, you can consider a random Gaussian design matrix X, but we forced the first 2 columns of the Gaussian matrix to be equal to each other, and for this design matrix.",
            "It doesn't satisfy trigonometric condition for any gamma, but it appears that a single variant of the lasso estimator can solve it to the optimal statistical accuracy.",
            "So this is the point that I think it's worth noting.",
            "It is also well."
        ],
        [
            "Knowing that getting a sparse solution for an arbitrary linear equation is NP hard problem, and I think it's interesting to compare this lower bound with this well known hardness result.",
            "So there are several differences between solving linear system in solving sparse linear regression.",
            "1st for Design Matrix for solving linear equations, the design matrix is arbitrary.",
            "It could be selected by adversary who knows your algorithm.",
            "And for solving sparse linear regression betrayed the design matrix is a fixed matrix, so the algorithm can be adaptive to the matrix.",
            "And we're solving linear equations.",
            "The response factor is a deterministic vector.",
            "It is a deterministic function of the design matrix and the regression coefficient, and for sparse linear regression it is a random vector corrected by the Gaussian noise, and more importantly, for solving linear systems.",
            "The design matrix doesn't necessarily satisfy the restricted eigenvalues condition, so it is probable that both the upper bound and lower bound Infinity.",
            "And for something sparse linear regression.",
            "For our problem, we know that there is a finite upper bound for the truncated so estimator we want to establish lower bounding matches their problem."
        ],
        [
            "Case for the rest of the presentation, I want to sketch a proof for the main result.",
            "And before delving into the technical details, I think it would be helpful to understand why this is nontrivial."
        ],
        [
            "This such kind of result.",
            "The proof is challenging because we're trying to establish the harness not for deterministic problem before.",
            "A stochastic problem to understand this.",
            "Recall that the response factor is equal to a linear transform of the design matrix plus a Gaussian noise.",
            "And if we make the variance of the Gaussian noise goes to 0, then the response vector becomes a deterministic vector, and plugging this into the upper bound for the truncated so estimator, we see that the prediction error associated with this design matrix is going to zero as well.",
            "Accordingly, that means that the problem gets easy if the design if the response vector is deterministic.",
            "Any other words the hardness of the problem comes from the randomness.",
            "So in order to prove the result, it is important to explore the property of the.",
            "The Gaussian noise that corrupted the response vector, so the proof consists of three steps.",
            "In the first step."
        ],
        [
            "We are trying to establish the hardness of the following problem.",
            "The problem is to finding a case bars solution to the linear system in the form of V equal to M times you start where V is unknown vector, an emison known matrix and you store is unknown.",
            "Our first alarm assessed that if M is some fixed design matrix, an you story sample from some distribution than any polynomial time estimator or solver must have ever lower bounded by one harvest high probability.",
            "And the proof is by reducing the exact set cover problem to this problem.",
            "The difference between this llama and the well well, no harness result on solving sparse linear system is that here we are using a fixed design matrix, so the algorithm can be adaptive to the matrix.",
            "But still this is not a lower bound for sparse linear regression because of two reasons.",
            "First, the response vector is not random, it is fixed and 2nd design matrix M is not satisfying the restricted mental condition.",
            "But still we're going to use as an important building block to prove the main result."
        ],
        [
            "The second step we reduce the previous problem, which we call P12 intermediate problem called Peter Prime.",
            "To make the reduction, we construct the instance of Peter Prime for every instance of P1, and the construction is going as follow.",
            "So we first construct design matrix X, whose upper part is equal to a rescaled version of the matrix M on the last slide, and the lower part is equal to gamma times random Gaussian matrix G. And by the property off the random Gaussian matrix, we can show that this matrix Act satisfied restricted eigenvalues condition with parameter, We also construct a response vector Y prime according to the information contained in the instance of people in a P1.",
            "The goal of P2 prime is to recover the regression coefficient.",
            "Rho Times used are, according to the observation of X&Y prime, but this is not a regression problem.",
            "This is not a linear regression problem because there's no linear relation between Y prime and X.",
            "Still, this is closely related to a sparsity regression problem, and according to the hardness of P1 we have just approved.",
            "We can see that for any polynomial time algorithm that gives a solution to Peter Prime, we have the following inequality, because this is just the equivalent statement of Llama one.",
            "And by multiplying bro on both sides of the inequality we can get this inequality which says that the prediction error on P2 prime is lower bounded by roll squared divided by 4 with high probability and we call this as the hardness of Pete."
        ],
        [
            "Prime the final step of the reduction reduces Peter Prime to the original sparse linear regression problem.",
            "Which we call P2 from P2.",
            "We use the same design matrix, but we use a standard response vector Y which is equal to a linear transform of X + a Gaussian noise.",
            "The key observation is that the difference between white and white prime is proportional to roll times, So if you can make sure that role times, is small, then we can make sure that the difference between the two response vectors are small, which suggests that the difficulty of solving P2 and P2 prime similar to each other and this intuition is formalized by the following lemma, which says that if we can control the role times, to be upper bounded by some expression on Sigma K&N.",
            "Then the prediction error on P2 can be lower bounded by the prediction around Peter Prime and the proof of this lemma explicitly uses the fact that the noises are Gaussian noise.",
            "So putting all pieces together, we can lower bound the prediction around P2 by the by the prediction around."
        ],
        [
            "Problem is he to prime and by the hardness of Peter applying this is lower bounded by gamma squared divided by 4 and now using the constraint down unroll that we have made an Lama two.",
            "This is equal to Sigma squared divided by gamma squared times can.",
            "So this is a lower bound for the sparse linear regression problem and comparing this to the upper bound we know that dependence on Sigma gamma an N or type.",
            "But the dependence on KM Rowan outside.",
            "And to accomplish the title lower bound, which is our main result, you're encouraged to read our paper to find out more details.",
            "So under summarize, to talk in this talk we have described some performance gap between the pole."
        ],
        [
            "Normal time algorithm in exponential time algorithms for sparse linear regression.",
            "We have shown that the gap is depending on one of our gamma square where gamma is the restricted eigenvalues of the design matrix may also show an upper bound and matches the lower bound.",
            "To demonstrate that both the upper bound and lower bound almost tight.",
            "There are indeed some open problems in this."
        ],
        [
            "Issue first, I've mentioned that the lower bound doesn't doesn't give a necessary sufficient condition for the design for all design matrices to have the gap, so it was very nice to characterize this necessary and sufficient condition for any design matrix.",
            "And of course the harness comes from the assumption of the proper learning, which is that the estimator is case sparse, but in practice against estimator may also be able to achieve a small prediction error.",
            "So in the in the improper learning scenario, proving such a gap to exist developer problem.",
            "And finally, I believe that such technical result may be possible to be generalized to other problem, including ranking in graphical model detection, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a great pleasure to present here.",
                    "label": 0
                },
                {
                    "sent": "This is John Volk with Martin Wright and Michael Jordan, so in this presentation going to talk about some performance gap between the polynomial time algorithms, exponential time algorithm for sparse linear regression.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In recent years, statisticians are more interested in the relation between computation and statistical accuracy, and there are active research differentiating the statistical optimal rate and concept optimal optimal rate computable in polynomial time, and there's two kinds of rates are in sometimes probably different and the performance gap between the polynomial time algorithm an exponential time algorithm have been established for several problems, including sparse PCA detection, submatrix detection, and learning halfspaces.",
                    "label": 1
                },
                {
                    "sent": "So in this talk I'm going to focus on sparse linear regression, which is another classical problem in machine learning and statistics.",
                    "label": 0
                },
                {
                    "sent": "By the way, there are two direct consequences of discovering this computation barriers in statistical estimation on my hand when the data scale is fixed, then having a limited computation power means that we have to expect a lower statistical statistical accuracy and on the other hand, when the desired accuracy is fixed, then having more data means faster computation.",
                    "label": 0
                },
                {
                    "sent": "So this represents a very interesting tradeoff between data in time.",
                    "label": 0
                },
                {
                    "sent": "First, I want to formalize the problem to solve.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is possible regression.",
                    "label": 0
                },
                {
                    "sent": "We have observed a design matrix X which has N rows and columns.",
                    "label": 1
                },
                {
                    "sent": "Because we are in the regime of high dimensionality, we assume that these greater than ND represented dimensionality and represent the sample size.",
                    "label": 0
                },
                {
                    "sent": "We also observe a response vector Y which is equal to X times response factor.",
                    "label": 1
                },
                {
                    "sent": "See the Star plus a random Gaussian noise W. To make the problem solver, we have to make some sparsity assumptions on the design matrix on the regression coefficient.",
                    "label": 0
                },
                {
                    "sent": "So here we assume that key Cedar storage case bars, meaning at most K entries of Silver Star on 0.",
                    "label": 0
                },
                {
                    "sent": "And will go off the goal of this parsing regression is to find the estimator seat ahead for Superstar.",
                    "label": 0
                },
                {
                    "sent": "And here we consider the proper learning scenario where we require see that had to be case bars as well and we want to minimize the distance between seat ahead and see the star in terms of the prediction error which is defined here.",
                    "label": 0
                },
                {
                    "sent": "On the slide.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For this problem there is a well known upper bound.",
                    "label": 0
                },
                {
                    "sent": "If you consider the L0 based estimator which searches overall the K sparse vectors in the D dimensional space and find one that minimizes the empirical risk, then we know that the prediction error associated with this estimator is upper bounded by K times log D divided by it.",
                    "label": 0
                },
                {
                    "sent": "This is a very general upper bound.",
                    "label": 1
                },
                {
                    "sent": "Becausw imposes no constraint on the design matrix, so it is true for all design matrices.",
                    "label": 1
                },
                {
                    "sent": "But what is the computation complexity of searching over order case prospector's in a dimensional space it is at least the truth is K. So it is not computationally intractable unless case very small.",
                    "label": 1
                },
                {
                    "sent": "To resolve the computational tractability problem, people have invented many convex relaxation method for sparse linear regression.",
                    "label": 0
                },
                {
                    "sent": "Here, for example, we consider the truncated lasso estimator, which is running less so and then truncate the resulting vector to be a case prospector.",
                    "label": 0
                },
                {
                    "sent": "There's an upper bound associated with this truncated also estimator, which is similar to the upper bound to for the L0 based estimator.",
                    "label": 0
                },
                {
                    "sent": "Except this there's a dependence on the restricted eigenvalues of the design matrix, so you can see that despite this red dependence on Gamma Square, we're gonna miss the restricted eigenvalues of the design matrix.",
                    "label": 0
                },
                {
                    "sent": "This upper bound is the same as the upper bound for L0 based estimator.",
                    "label": 0
                },
                {
                    "sent": "Now you may be curious about what is the definition of the restricted ragamala order restricting mental condition to the finest concept that consider a coin in the dimensional space such that any vector belongs to this coin if and only if.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is L1 norm with respect to a small subset of coordinates, it roughly dominates the overall L1 norm, or in other words, this coin defines a convex relaxation for a set of K sparse vectors.",
                    "label": 0
                },
                {
                    "sent": "And matrix ax is set to satisfy their equivalent condition.",
                    "label": 0
                },
                {
                    "sent": "If always singular values in this case bars corn is upper bounded by is lower bounded by gamma.",
                    "label": 1
                },
                {
                    "sent": "Because Karma is a function of the design matrix X, it is also a function of the dimensionality D sample size N and sparsity level K. So in this upper bound, gamma cannot be treated as a constant.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we know that there is a one over Gamma Square performance gap between the truncated also estimator and the L0 based estimator, and we also know that gamma could be arbitrarily close to 0.",
                    "label": 1
                },
                {
                    "sent": "So this guy could be arbitrarily large.",
                    "label": 1
                },
                {
                    "sent": "A natural question is, is there a way to close this gap?",
                    "label": 0
                },
                {
                    "sent": "There are several possibilities.",
                    "label": 0
                },
                {
                    "sent": "It is possible that the existing analysis on the solar overly conservative but less so can actually achieve the same convergence rate as the L0 based estimator.",
                    "label": 0
                },
                {
                    "sent": "Another possibility is that also is not good enough, but there exists a more clever polynomial time algorithms that can achieve the same convergence rate as the LO.",
                    "label": 1
                },
                {
                    "sent": "Better L0 based estimator.",
                    "label": 0
                },
                {
                    "sent": "At the most pessimistic possibility is that getting rid of this dependence on one over Gamma Square is impossible for all polynomial time algorithms, which is the statement of our main result.",
                    "label": 0
                },
                {
                    "sent": "More specifically, we have shown that first we assume that MP is not a sub.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of people which is widely believed in the complexity theory and we show that for any and the NK which are assembled size dimensionalities spots to level an for any constant gamma which is greater than zero, we can always find the design matrix X which satisfies the gammas restricted mental condition and social prediction error is lower bounded by K to the one minus Delta times log divided by Gamma Square.",
                    "label": 0
                },
                {
                    "sent": "So comparing to the upper bound for the truncated also estimator, we see that this lower bound is almost tight, despite this small Delta dependence.",
                    "label": 1
                },
                {
                    "sent": "But Delta could be arbitrarily close to 0, which is a constant.",
                    "label": 0
                },
                {
                    "sent": "So it means that the dependence on one over Gamma Square is essential dependence for all polynomial time algorithms.",
                    "label": 0
                },
                {
                    "sent": "But it is also important to notice that this lower bound doesn't give a necessary and sufficient conditions necessary and sufficient condition for all design matrices to for the performance gap to appear.",
                    "label": 1
                },
                {
                    "sent": "There are design matrices which satisfied which doesn't satisfy any restrictive mental condition for any gamma, but appears that it is easy to be solved by some polynomial time algorithm.",
                    "label": 0
                },
                {
                    "sent": "Like for example, you can consider a random Gaussian design matrix X, but we forced the first 2 columns of the Gaussian matrix to be equal to each other, and for this design matrix.",
                    "label": 1
                },
                {
                    "sent": "It doesn't satisfy trigonometric condition for any gamma, but it appears that a single variant of the lasso estimator can solve it to the optimal statistical accuracy.",
                    "label": 0
                },
                {
                    "sent": "So this is the point that I think it's worth noting.",
                    "label": 0
                },
                {
                    "sent": "It is also well.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Knowing that getting a sparse solution for an arbitrary linear equation is NP hard problem, and I think it's interesting to compare this lower bound with this well known hardness result.",
                    "label": 0
                },
                {
                    "sent": "So there are several differences between solving linear system in solving sparse linear regression.",
                    "label": 1
                },
                {
                    "sent": "1st for Design Matrix for solving linear equations, the design matrix is arbitrary.",
                    "label": 0
                },
                {
                    "sent": "It could be selected by adversary who knows your algorithm.",
                    "label": 0
                },
                {
                    "sent": "And for solving sparse linear regression betrayed the design matrix is a fixed matrix, so the algorithm can be adaptive to the matrix.",
                    "label": 1
                },
                {
                    "sent": "And we're solving linear equations.",
                    "label": 1
                },
                {
                    "sent": "The response factor is a deterministic vector.",
                    "label": 0
                },
                {
                    "sent": "It is a deterministic function of the design matrix and the regression coefficient, and for sparse linear regression it is a random vector corrected by the Gaussian noise, and more importantly, for solving linear systems.",
                    "label": 1
                },
                {
                    "sent": "The design matrix doesn't necessarily satisfy the restricted eigenvalues condition, so it is probable that both the upper bound and lower bound Infinity.",
                    "label": 0
                },
                {
                    "sent": "And for something sparse linear regression.",
                    "label": 0
                },
                {
                    "sent": "For our problem, we know that there is a finite upper bound for the truncated so estimator we want to establish lower bounding matches their problem.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case for the rest of the presentation, I want to sketch a proof for the main result.",
                    "label": 0
                },
                {
                    "sent": "And before delving into the technical details, I think it would be helpful to understand why this is nontrivial.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This such kind of result.",
                    "label": 0
                },
                {
                    "sent": "The proof is challenging because we're trying to establish the harness not for deterministic problem before.",
                    "label": 1
                },
                {
                    "sent": "A stochastic problem to understand this.",
                    "label": 1
                },
                {
                    "sent": "Recall that the response factor is equal to a linear transform of the design matrix plus a Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "And if we make the variance of the Gaussian noise goes to 0, then the response vector becomes a deterministic vector, and plugging this into the upper bound for the truncated so estimator, we see that the prediction error associated with this design matrix is going to zero as well.",
                    "label": 0
                },
                {
                    "sent": "Accordingly, that means that the problem gets easy if the design if the response vector is deterministic.",
                    "label": 1
                },
                {
                    "sent": "Any other words the hardness of the problem comes from the randomness.",
                    "label": 1
                },
                {
                    "sent": "So in order to prove the result, it is important to explore the property of the.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian noise that corrupted the response vector, so the proof consists of three steps.",
                    "label": 0
                },
                {
                    "sent": "In the first step.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are trying to establish the hardness of the following problem.",
                    "label": 1
                },
                {
                    "sent": "The problem is to finding a case bars solution to the linear system in the form of V equal to M times you start where V is unknown vector, an emison known matrix and you store is unknown.",
                    "label": 0
                },
                {
                    "sent": "Our first alarm assessed that if M is some fixed design matrix, an you story sample from some distribution than any polynomial time estimator or solver must have ever lower bounded by one harvest high probability.",
                    "label": 0
                },
                {
                    "sent": "And the proof is by reducing the exact set cover problem to this problem.",
                    "label": 1
                },
                {
                    "sent": "The difference between this llama and the well well, no harness result on solving sparse linear system is that here we are using a fixed design matrix, so the algorithm can be adaptive to the matrix.",
                    "label": 1
                },
                {
                    "sent": "But still this is not a lower bound for sparse linear regression because of two reasons.",
                    "label": 0
                },
                {
                    "sent": "First, the response vector is not random, it is fixed and 2nd design matrix M is not satisfying the restricted mental condition.",
                    "label": 0
                },
                {
                    "sent": "But still we're going to use as an important building block to prove the main result.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second step we reduce the previous problem, which we call P12 intermediate problem called Peter Prime.",
                    "label": 0
                },
                {
                    "sent": "To make the reduction, we construct the instance of Peter Prime for every instance of P1, and the construction is going as follow.",
                    "label": 0
                },
                {
                    "sent": "So we first construct design matrix X, whose upper part is equal to a rescaled version of the matrix M on the last slide, and the lower part is equal to gamma times random Gaussian matrix G. And by the property off the random Gaussian matrix, we can show that this matrix Act satisfied restricted eigenvalues condition with parameter, We also construct a response vector Y prime according to the information contained in the instance of people in a P1.",
                    "label": 0
                },
                {
                    "sent": "The goal of P2 prime is to recover the regression coefficient.",
                    "label": 0
                },
                {
                    "sent": "Rho Times used are, according to the observation of X&Y prime, but this is not a regression problem.",
                    "label": 0
                },
                {
                    "sent": "This is not a linear regression problem because there's no linear relation between Y prime and X.",
                    "label": 1
                },
                {
                    "sent": "Still, this is closely related to a sparsity regression problem, and according to the hardness of P1 we have just approved.",
                    "label": 0
                },
                {
                    "sent": "We can see that for any polynomial time algorithm that gives a solution to Peter Prime, we have the following inequality, because this is just the equivalent statement of Llama one.",
                    "label": 0
                },
                {
                    "sent": "And by multiplying bro on both sides of the inequality we can get this inequality which says that the prediction error on P2 prime is lower bounded by roll squared divided by 4 with high probability and we call this as the hardness of Pete.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prime the final step of the reduction reduces Peter Prime to the original sparse linear regression problem.",
                    "label": 1
                },
                {
                    "sent": "Which we call P2 from P2.",
                    "label": 1
                },
                {
                    "sent": "We use the same design matrix, but we use a standard response vector Y which is equal to a linear transform of X + a Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "The key observation is that the difference between white and white prime is proportional to roll times, So if you can make sure that role times, is small, then we can make sure that the difference between the two response vectors are small, which suggests that the difficulty of solving P2 and P2 prime similar to each other and this intuition is formalized by the following lemma, which says that if we can control the role times, to be upper bounded by some expression on Sigma K&N.",
                    "label": 0
                },
                {
                    "sent": "Then the prediction error on P2 can be lower bounded by the prediction around Peter Prime and the proof of this lemma explicitly uses the fact that the noises are Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "So putting all pieces together, we can lower bound the prediction around P2 by the by the prediction around.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem is he to prime and by the hardness of Peter applying this is lower bounded by gamma squared divided by 4 and now using the constraint down unroll that we have made an Lama two.",
                    "label": 0
                },
                {
                    "sent": "This is equal to Sigma squared divided by gamma squared times can.",
                    "label": 0
                },
                {
                    "sent": "So this is a lower bound for the sparse linear regression problem and comparing this to the upper bound we know that dependence on Sigma gamma an N or type.",
                    "label": 1
                },
                {
                    "sent": "But the dependence on KM Rowan outside.",
                    "label": 0
                },
                {
                    "sent": "And to accomplish the title lower bound, which is our main result, you're encouraged to read our paper to find out more details.",
                    "label": 0
                },
                {
                    "sent": "So under summarize, to talk in this talk we have described some performance gap between the pole.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Normal time algorithm in exponential time algorithms for sparse linear regression.",
                    "label": 1
                },
                {
                    "sent": "We have shown that the gap is depending on one of our gamma square where gamma is the restricted eigenvalues of the design matrix may also show an upper bound and matches the lower bound.",
                    "label": 1
                },
                {
                    "sent": "To demonstrate that both the upper bound and lower bound almost tight.",
                    "label": 0
                },
                {
                    "sent": "There are indeed some open problems in this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Issue first, I've mentioned that the lower bound doesn't doesn't give a necessary sufficient condition for the design for all design matrices to have the gap, so it was very nice to characterize this necessary and sufficient condition for any design matrix.",
                    "label": 1
                },
                {
                    "sent": "And of course the harness comes from the assumption of the proper learning, which is that the estimator is case sparse, but in practice against estimator may also be able to achieve a small prediction error.",
                    "label": 1
                },
                {
                    "sent": "So in the in the improper learning scenario, proving such a gap to exist developer problem.",
                    "label": 1
                },
                {
                    "sent": "And finally, I believe that such technical result may be possible to be generalized to other problem, including ranking in graphical model detection, thank you.",
                    "label": 0
                }
            ]
        }
    }
}