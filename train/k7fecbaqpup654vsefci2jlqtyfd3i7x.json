{
    "id": "k7fecbaqpup654vsefci2jlqtyfd3i7x",
    "title": "Markov Decision Processes with Ordinal Rewards: Reference Point-Based Preferences",
    "info": {
        "author": [
            "Paul Weng, LIP6, Universit\u00e9 Pierre et Marie Curie - Paris 6"
        ],
        "published": "July 21, 2011",
        "recorded": "June 2011",
        "category": [
            "Top->Computer Science->Artificial Intelligence->Planning and Scheduling"
        ]
    },
    "url": "http://videolectures.net/icaps2011_weng_preferences/",
    "segmentation": [
        [
            "OK, I'm going to start with some areas of my talk.",
            "It's not very good for the suspense of the talk, but I hopefully you stayed until the end for the explanation of the idea.",
            "So what I'm going to present it's.",
            "A way to define a.",
            "In a meaningful way where we are function.",
            "Yeah, in cases when the rewards it's very difficult to value.",
            "And the idea is to use a reference point.",
            "So when you compare two solutions, you will say that the first solution is better than the other one.",
            "When the extent the first ocean beats our fans point is greater than the extent to which the second certain bit servant.",
            "So that's the basic idea.",
            "I would see how we can find an unbuilt justify this fence parts."
        ],
        [
            "So what I'm interested in is the problems with this kind of structure, so you know all about that about that.",
            "It's a planning under uncertainty problem, so you are in a certain state.",
            "So we start in a state S. And then we have a different problem.",
            "We need to choose one action.",
            "We can, when you choose one action, we get feedback.",
            "It could be a positive feedback and negative feedback or no feedback at all an we have.",
            "When you choose an action, we have some priority to which the future state and choice polymer start over, so it's."
        ],
        [
            "Very classical and we can model this problem with the MDP so.",
            "The good thing about being the last speaker is you can go very fast on the framework, so I just give you the my notation.",
            "So as the state aid, the set up action.",
            "We have a public distribution of artificial States and we want function.",
            "His story for me will be just a sequence of state, an action.",
            "So here for example in blue it's a history and what we want is to compare policies.",
            "So we have a.",
            "Our preference relation of our policies."
        ],
        [
            "And then send out NDPS.",
            "It's how we."
        ],
        [
            "Define this proof."
        ],
        [
            "So relation.",
            "So I'm going to go very fast on that.",
            "Yeah, policy is valued by the Interstate is valued by the expected total rewards, discounted rewards going to yield.",
            "And so we can compute iteratively this."
        ],
        [
            "The value function and the three main families of solution method.",
            "So I'm I don't.",
            "I'm not going to."
        ],
        [
            "And that.",
            "So to the main point of my work or the motivation of my work.",
            "It's not very clear.",
            "Fun opening on a small example you have here.",
            "2 states, one and 2, two actions and state one and only deterministic action is BI.",
            "Didn't put the probability, but it's we have a public policy of 0.5 to stay in state one or poverty of open 5 to go on Stage 2.",
            "And we have the rewards for each action as well.",
            "Assume that you we only know that our is better than our prime and better than our second.",
            "If you want to use this under MVP's."
        ],
        [
            "You need to value those rewards, so assume that we use those values 210.",
            "The best decision in a state one will be be here.",
            "But but had we?"
        ],
        [
            "It wasn't another local representation for our rewards.",
            "We will obtain completely different choices.",
            "So the choice of the value for the choice of the rewards are very important, and when we don't know those those values, it's not very good to arbitrarily chose truth those values.",
            "So is it always the case that the optimal solution optimal policies depend on the?"
        ],
        [
            "He wants, in fact, there's one one exception.",
            "It's a very simple case.",
            "The case is when the reward function can only take two values, zero an unusual value.",
            "We can change this this value to any other value as long as the ordinal information about rewards are.",
            "Change and so we can model in this situation.",
            "Choose any value here.",
            "And doesn't change the optimal policies."
        ],
        [
            "So when is it difficult to choose to define this reward function, and when it is easy, so it's easy when this one has physical meaning properly when it's money, mandatory cost or gain length, duration, and so in.",
            "Problems like in stochastic shortest path problem, it's probably not too difficult to define the reward function."
        ],
        [
            "But even in those cases, sometimes we don't know the precise value of of the rewards.",
            "So when we don't know those those values precisely, it may be difficult to just pick one Honda only.",
            "Because in my previous example it was.",
            "I changed Watts quite drastically.",
            "But we can build instance of problem where a slight change of 1 value of one can change completely the optimal policies.",
            "And it's difficult as well when the rewards are really of qualitative nature, so we don't want in this case to present those rewards with a numerical value.",
            "An example of problems where it is difficult to define a reward function is in video games.",
            "For example, for the Games like Warcraft, if you want to program about that, going to plan the production of units, it's quite difficult to say this unit is valued then this we need this value 100 or whatever, so it's very difficult to value the.",
            "To choose a numerical value of the reward in such case."
        ],
        [
            "So when we don't have any medical information about the rewards, I think a better way would be to use ordinary Ward MVP.",
            "In that case though, what is valued in the scale E and the only thing we know about the value in this scale is that we can order those keywords.",
            "So here are one is the best one or two with the second one, and so on until RN.",
            "And, uh, we're going to try to build a perfect system for MVP's.",
            "So of course we need to add some information because here we don't have much information about the profound system."
        ],
        [
            "So we need to be able to compare history Zinno MD piece.",
            "So if you look at the history exactly you will yield.",
            "A sequence of ordinary words.",
            "So the basic idea is just to count the rewards and we can value history by a vector of numbers here and I beta will be the discounted number of what I I obtained by the history."
        ],
        [
            "And we're going to make this assumption that comparing histories is equivalent to compare to the vectors."
        ],
        [
            "So if you evaluate a history by vectors, we can also value policies by vectors.",
            "Yeah indeed, a policy provided solution over his stories so we can just take the expectation of those vectors.",
            "To value a policy, no state."
        ],
        [
            "So now our problem is to compare vectors.",
            "And what we did is not.",
            "Completely specific to OMB piece.",
            "It can be done in standard MDP's, so we can ask ourselves how installed at MPs can we aggregate those vectors to compare policies or his stories?",
            "So in this region, so we like to use the axiomatic approach.",
            "Is it's.",
            "The idea is to list a set of actions and to characterize the decision criterion that is used.",
            "So here for Standard MVP's we need the three axioms we need to be to assume A1.",
            "It means that we are able to compare any pair of vectors and this performance is a pre order."
        ],
        [
            "We need the action A2 as well, so if we compare two vectors we can add the same element.",
            "It won't change their preference.",
            "Here AI is the new vector vector newer everywhere except in component I where it's equal to 1.",
            "So in fact I will present the ordinary we want our I."
        ],
        [
            "And the last axiom it's a Archimedean action.",
            "State is says that here you have N preferred strictly to end prime, so there is a kind of profound difference between those two vectors.",
            "So actually A3 says that if we duplicate those vectors a certain certain number of times, we can make this profound difference as big as rounds.",
            "So.",
            "An capital N is.",
            "N Capital N added to itself and times."
        ],
        [
            "So if you assume those three axioms we have this representation theorem, so we have equivalence between those two propositions.",
            "Anne.",
            "So if you assume those three actions, we know that we can give a value to each ordinary one, you will give the numerical value to each ordinary words, and that's the way you aggregate the vector in standout NDPS.",
            "Anne.",
            "And you can read the history on both ways.",
            "So it means that if you stand out MVP's and use numerical values for your reward function, it means that you implicitly assume assume a 1283.",
            "So in order to build.",
            "11 point based preferences.",
            "I'm going to start from those reactions.",
            "It seems to me that they are quite natural, so I'm going to add a few actions an tried then to to make appear."
        ],
        [
            "Point, so I'm going to add a two actions.",
            "The first one is that that says that one so E one who presents the ordinary world are one.",
            "And so, here are a form states that the order we specified on the ordinary words.",
            "Tips on the vectors?"
        ],
        [
            "And I'm going to.",
            "Add another assumption.",
            "This assumption is that EKO is a newer one.",
            "So yeah, just say that there's a new one and it's KO.",
            "So if you."
        ],
        [
            "Add those two actions to the previous one.",
            "We have this representation showing.",
            "It means that we can find our fans point and we can compare two vectors through this reference point.",
            "So the formula is here.",
            "I'm going to explain this formula.",
            "So to make it easier to understand, we first assume that all the feedback are positive at the beginning.",
            "So if all the feedback or positive, it means that KKO is equal to N. Because the RN is the lowest we want, and so this part was going to disappear.",
            "We are."
        ],
        [
            "I only have the first part."
        ],
        [
            "Let's let's see on a small example how you compute that so forgiven.",
            "For a given K. We are going to multiply NK by the number of rewards that are lower than what are K. So for example here and is 102.",
            "It means that we have one times he what I want the best one and two times we want our three, the lowest one and prime is zero to 1.",
            "So we have two rewards are two and one he was asked.",
            "Three, etc.",
            "So if you want to compute this criterion for N. It means we're going to look at the first.",
            "We want one, and we multiply this to all the rewards that are.",
            "That are.",
            "Lower than the first one, so here we have 1, two and so we have three here, and the value for this quarter we stream.",
            "We can do the same for any prime and we are open for and for this.",
            "This point we can say that N prime is better than three.",
            "And so the interpretation is as follows.",
            "This criterion says that evaluates number of times or what selected in N is better than one selected in the reference point."
        ],
        [
            "And we can give a probabilistic interpretation.",
            "If we normalize this value, interpretation would be the property that we were drawn from.",
            "N is better than one one in anytime."
        ],
        [
            "So quickly the negative value negative if you only have negative feedback, we only have the last part.",
            "So here we compute the number of times the reward.",
            "In the reference point, is better than the.",
            "The word in the in the vector N and we add the minus and so if we normalize another one, it's a property that's the vector.",
            "N gives better work done and so forth.",
            "When Journal case is just the sum of the soup."
        ],
        [
            "So, just to summarize what we did, we saw we justified.",
            "Reference point based profound system.",
            "So if you want to use that you need just need to define our MVP picture, reference points and then compute the reference vector and this reference vector in fact define the value of of rewards.",
            "So I don't go on details here.",
            "But then when you have a new worker here, what you can use it to solve this MDP with standard method.",
            "And quickly.",
            "If we have some ideas to how to pick the standpoint, we can just use a scale of the of the ordinary words, or we can use a probability distribution of the case, or can even use history phone number if you have.",
            "Navigation problem you can always sometimes define a path that's preferred.",
            "That's quite good, so it could be.",
            "It could be used as a reference point an you can also provide a policy and this policy will.",
            "Will induce a reference vector and we can use that too.",
            "To serve their own."
        ],
        [
            "Also just quickly.",
            "Even though even when you.",
            "You start with the NDP and you have a new more color he wants.",
            "You can use reference point based preferences to pick.",
            "It's a bit controversial, but I would say better policy.",
            "The idea is as follows.",
            "You have MVP, you serve it on you.",
            "Obtain optimal policy star."
        ],
        [
            "So this pistol can induce a reference vector and we can define a new function on this server."
        ],
        [
            "And can be used to define a notable policy in this new MVP.",
            "And the idea of this, the interpretation of this new optimal policy, is the policy that.",
            "Maximize the probability of beating the the optimal policy that we found before and when it is useful.",
            "It is useful when we can only apply when you are going to apply this policy only one time or a few times because the sound system in Soundout MVP's is in expectation, you're going to achieve the value function only if you repeat the process many times."
        ],
        [
            "So I think I do."
        ],
        [
            "Not much time, so I'm."
        ],
        [
            "I'm going to skip."
        ],
        [
            "Example and to finish.",
            "So I presented a way to define semantically justified the word function an just a few words for future work.",
            "If some of you interested, it was just theoretical ideas, but I would like to test those ideas in the real problems.",
            "So if you have some problems that where you want to apply reference point based preferences, please come and have a chat with me.",
            "Anne.",
            "For building, for justifying the profound system, I use some axioms and we are, I think some work could be done to relax some of those actions.",
            "OK, thank you.",
            "OK so I was saying that I've read some of his work quite a long time ago and it was I think also about ordinal based rewards and MVP's.",
            "And given that the motivation seems to be quite legitimate, I mean the solutions are very sensitive to rewards.",
            "I was I was wondering, you know, as as this stuff actually been, all these ideas actually been used massively by.",
            "I don't know by people and if not, why not?",
            "And you know, in in kind of practical yeah, settings so far they wanted to just theoretical, so I haven't tried.",
            "Tried it on real problems.",
            "Not necessarily that particular.",
            "You know semantics, but the idea of ordinal rewards in in MVP's.",
            "I mean I've seen before.",
            "Yeah, what I did before it was just way even more surgical and said that in a very general framework.",
            "So I think you can't do in a real problems you you just want to have an optimal policy and have just one.",
            "Ann, you're you're very happy with it and you apply it.",
            "No, right now I don't know.",
            "I mean, if it tells you just anything, because when you are defining the ordinary rewards, the points you are going to come with your going to get a lot of policies that are equivalent.",
            "So you had to add some preference information, yeah?",
            "Alright, any questions?",
            "Any other question?",
            "Alright, well thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm going to start with some areas of my talk.",
                    "label": 0
                },
                {
                    "sent": "It's not very good for the suspense of the talk, but I hopefully you stayed until the end for the explanation of the idea.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to present it's.",
                    "label": 0
                },
                {
                    "sent": "A way to define a.",
                    "label": 0
                },
                {
                    "sent": "In a meaningful way where we are function.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in cases when the rewards it's very difficult to value.",
                    "label": 0
                },
                {
                    "sent": "And the idea is to use a reference point.",
                    "label": 0
                },
                {
                    "sent": "So when you compare two solutions, you will say that the first solution is better than the other one.",
                    "label": 0
                },
                {
                    "sent": "When the extent the first ocean beats our fans point is greater than the extent to which the second certain bit servant.",
                    "label": 0
                },
                {
                    "sent": "So that's the basic idea.",
                    "label": 0
                },
                {
                    "sent": "I would see how we can find an unbuilt justify this fence parts.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I'm interested in is the problems with this kind of structure, so you know all about that about that.",
                    "label": 0
                },
                {
                    "sent": "It's a planning under uncertainty problem, so you are in a certain state.",
                    "label": 0
                },
                {
                    "sent": "So we start in a state S. And then we have a different problem.",
                    "label": 0
                },
                {
                    "sent": "We need to choose one action.",
                    "label": 0
                },
                {
                    "sent": "We can, when you choose one action, we get feedback.",
                    "label": 0
                },
                {
                    "sent": "It could be a positive feedback and negative feedback or no feedback at all an we have.",
                    "label": 0
                },
                {
                    "sent": "When you choose an action, we have some priority to which the future state and choice polymer start over, so it's.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very classical and we can model this problem with the MDP so.",
                    "label": 0
                },
                {
                    "sent": "The good thing about being the last speaker is you can go very fast on the framework, so I just give you the my notation.",
                    "label": 0
                },
                {
                    "sent": "So as the state aid, the set up action.",
                    "label": 0
                },
                {
                    "sent": "We have a public distribution of artificial States and we want function.",
                    "label": 0
                },
                {
                    "sent": "His story for me will be just a sequence of state, an action.",
                    "label": 0
                },
                {
                    "sent": "So here for example in blue it's a history and what we want is to compare policies.",
                    "label": 0
                },
                {
                    "sent": "So we have a.",
                    "label": 0
                },
                {
                    "sent": "Our preference relation of our policies.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then send out NDPS.",
                    "label": 0
                },
                {
                    "sent": "It's how we.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Define this proof.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So relation.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to go very fast on that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, policy is valued by the Interstate is valued by the expected total rewards, discounted rewards going to yield.",
                    "label": 0
                },
                {
                    "sent": "And so we can compute iteratively this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The value function and the three main families of solution method.",
                    "label": 0
                },
                {
                    "sent": "So I'm I don't.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that.",
                    "label": 0
                },
                {
                    "sent": "So to the main point of my work or the motivation of my work.",
                    "label": 0
                },
                {
                    "sent": "It's not very clear.",
                    "label": 0
                },
                {
                    "sent": "Fun opening on a small example you have here.",
                    "label": 0
                },
                {
                    "sent": "2 states, one and 2, two actions and state one and only deterministic action is BI.",
                    "label": 0
                },
                {
                    "sent": "Didn't put the probability, but it's we have a public policy of 0.5 to stay in state one or poverty of open 5 to go on Stage 2.",
                    "label": 0
                },
                {
                    "sent": "And we have the rewards for each action as well.",
                    "label": 0
                },
                {
                    "sent": "Assume that you we only know that our is better than our prime and better than our second.",
                    "label": 0
                },
                {
                    "sent": "If you want to use this under MVP's.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You need to value those rewards, so assume that we use those values 210.",
                    "label": 0
                },
                {
                    "sent": "The best decision in a state one will be be here.",
                    "label": 0
                },
                {
                    "sent": "But but had we?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It wasn't another local representation for our rewards.",
                    "label": 0
                },
                {
                    "sent": "We will obtain completely different choices.",
                    "label": 0
                },
                {
                    "sent": "So the choice of the value for the choice of the rewards are very important, and when we don't know those those values, it's not very good to arbitrarily chose truth those values.",
                    "label": 0
                },
                {
                    "sent": "So is it always the case that the optimal solution optimal policies depend on the?",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He wants, in fact, there's one one exception.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple case.",
                    "label": 1
                },
                {
                    "sent": "The case is when the reward function can only take two values, zero an unusual value.",
                    "label": 1
                },
                {
                    "sent": "We can change this this value to any other value as long as the ordinal information about rewards are.",
                    "label": 0
                },
                {
                    "sent": "Change and so we can model in this situation.",
                    "label": 1
                },
                {
                    "sent": "Choose any value here.",
                    "label": 0
                },
                {
                    "sent": "And doesn't change the optimal policies.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when is it difficult to choose to define this reward function, and when it is easy, so it's easy when this one has physical meaning properly when it's money, mandatory cost or gain length, duration, and so in.",
                    "label": 0
                },
                {
                    "sent": "Problems like in stochastic shortest path problem, it's probably not too difficult to define the reward function.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But even in those cases, sometimes we don't know the precise value of of the rewards.",
                    "label": 0
                },
                {
                    "sent": "So when we don't know those those values precisely, it may be difficult to just pick one Honda only.",
                    "label": 0
                },
                {
                    "sent": "Because in my previous example it was.",
                    "label": 0
                },
                {
                    "sent": "I changed Watts quite drastically.",
                    "label": 0
                },
                {
                    "sent": "But we can build instance of problem where a slight change of 1 value of one can change completely the optimal policies.",
                    "label": 0
                },
                {
                    "sent": "And it's difficult as well when the rewards are really of qualitative nature, so we don't want in this case to present those rewards with a numerical value.",
                    "label": 1
                },
                {
                    "sent": "An example of problems where it is difficult to define a reward function is in video games.",
                    "label": 1
                },
                {
                    "sent": "For example, for the Games like Warcraft, if you want to program about that, going to plan the production of units, it's quite difficult to say this unit is valued then this we need this value 100 or whatever, so it's very difficult to value the.",
                    "label": 0
                },
                {
                    "sent": "To choose a numerical value of the reward in such case.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when we don't have any medical information about the rewards, I think a better way would be to use ordinary Ward MVP.",
                    "label": 0
                },
                {
                    "sent": "In that case though, what is valued in the scale E and the only thing we know about the value in this scale is that we can order those keywords.",
                    "label": 0
                },
                {
                    "sent": "So here are one is the best one or two with the second one, and so on until RN.",
                    "label": 0
                },
                {
                    "sent": "And, uh, we're going to try to build a perfect system for MVP's.",
                    "label": 0
                },
                {
                    "sent": "So of course we need to add some information because here we don't have much information about the profound system.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we need to be able to compare history Zinno MD piece.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the history exactly you will yield.",
                    "label": 0
                },
                {
                    "sent": "A sequence of ordinary words.",
                    "label": 1
                },
                {
                    "sent": "So the basic idea is just to count the rewards and we can value history by a vector of numbers here and I beta will be the discounted number of what I I obtained by the history.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we're going to make this assumption that comparing histories is equivalent to compare to the vectors.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you evaluate a history by vectors, we can also value policies by vectors.",
                    "label": 0
                },
                {
                    "sent": "Yeah indeed, a policy provided solution over his stories so we can just take the expectation of those vectors.",
                    "label": 0
                },
                {
                    "sent": "To value a policy, no state.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now our problem is to compare vectors.",
                    "label": 0
                },
                {
                    "sent": "And what we did is not.",
                    "label": 0
                },
                {
                    "sent": "Completely specific to OMB piece.",
                    "label": 0
                },
                {
                    "sent": "It can be done in standard MDP's, so we can ask ourselves how installed at MPs can we aggregate those vectors to compare policies or his stories?",
                    "label": 1
                },
                {
                    "sent": "So in this region, so we like to use the axiomatic approach.",
                    "label": 0
                },
                {
                    "sent": "Is it's.",
                    "label": 0
                },
                {
                    "sent": "The idea is to list a set of actions and to characterize the decision criterion that is used.",
                    "label": 0
                },
                {
                    "sent": "So here for Standard MVP's we need the three axioms we need to be to assume A1.",
                    "label": 0
                },
                {
                    "sent": "It means that we are able to compare any pair of vectors and this performance is a pre order.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We need the action A2 as well, so if we compare two vectors we can add the same element.",
                    "label": 0
                },
                {
                    "sent": "It won't change their preference.",
                    "label": 0
                },
                {
                    "sent": "Here AI is the new vector vector newer everywhere except in component I where it's equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So in fact I will present the ordinary we want our I.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the last axiom it's a Archimedean action.",
                    "label": 0
                },
                {
                    "sent": "State is says that here you have N preferred strictly to end prime, so there is a kind of profound difference between those two vectors.",
                    "label": 0
                },
                {
                    "sent": "So actually A3 says that if we duplicate those vectors a certain certain number of times, we can make this profound difference as big as rounds.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "An capital N is.",
                    "label": 0
                },
                {
                    "sent": "N Capital N added to itself and times.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you assume those three axioms we have this representation theorem, so we have equivalence between those two propositions.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So if you assume those three actions, we know that we can give a value to each ordinary one, you will give the numerical value to each ordinary words, and that's the way you aggregate the vector in standout NDPS.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And you can read the history on both ways.",
                    "label": 0
                },
                {
                    "sent": "So it means that if you stand out MVP's and use numerical values for your reward function, it means that you implicitly assume assume a 1283.",
                    "label": 0
                },
                {
                    "sent": "So in order to build.",
                    "label": 0
                },
                {
                    "sent": "11 point based preferences.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start from those reactions.",
                    "label": 0
                },
                {
                    "sent": "It seems to me that they are quite natural, so I'm going to add a few actions an tried then to to make appear.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point, so I'm going to add a two actions.",
                    "label": 0
                },
                {
                    "sent": "The first one is that that says that one so E one who presents the ordinary world are one.",
                    "label": 0
                },
                {
                    "sent": "And so, here are a form states that the order we specified on the ordinary words.",
                    "label": 0
                },
                {
                    "sent": "Tips on the vectors?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Add another assumption.",
                    "label": 0
                },
                {
                    "sent": "This assumption is that EKO is a newer one.",
                    "label": 0
                },
                {
                    "sent": "So yeah, just say that there's a new one and it's KO.",
                    "label": 0
                },
                {
                    "sent": "So if you.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Add those two actions to the previous one.",
                    "label": 0
                },
                {
                    "sent": "We have this representation showing.",
                    "label": 0
                },
                {
                    "sent": "It means that we can find our fans point and we can compare two vectors through this reference point.",
                    "label": 0
                },
                {
                    "sent": "So the formula is here.",
                    "label": 0
                },
                {
                    "sent": "I'm going to explain this formula.",
                    "label": 0
                },
                {
                    "sent": "So to make it easier to understand, we first assume that all the feedback are positive at the beginning.",
                    "label": 0
                },
                {
                    "sent": "So if all the feedback or positive, it means that KKO is equal to N. Because the RN is the lowest we want, and so this part was going to disappear.",
                    "label": 0
                },
                {
                    "sent": "We are.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I only have the first part.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's let's see on a small example how you compute that so forgiven.",
                    "label": 0
                },
                {
                    "sent": "For a given K. We are going to multiply NK by the number of rewards that are lower than what are K. So for example here and is 102.",
                    "label": 0
                },
                {
                    "sent": "It means that we have one times he what I want the best one and two times we want our three, the lowest one and prime is zero to 1.",
                    "label": 0
                },
                {
                    "sent": "So we have two rewards are two and one he was asked.",
                    "label": 0
                },
                {
                    "sent": "Three, etc.",
                    "label": 0
                },
                {
                    "sent": "So if you want to compute this criterion for N. It means we're going to look at the first.",
                    "label": 0
                },
                {
                    "sent": "We want one, and we multiply this to all the rewards that are.",
                    "label": 0
                },
                {
                    "sent": "That are.",
                    "label": 0
                },
                {
                    "sent": "Lower than the first one, so here we have 1, two and so we have three here, and the value for this quarter we stream.",
                    "label": 0
                },
                {
                    "sent": "We can do the same for any prime and we are open for and for this.",
                    "label": 0
                },
                {
                    "sent": "This point we can say that N prime is better than three.",
                    "label": 0
                },
                {
                    "sent": "And so the interpretation is as follows.",
                    "label": 0
                },
                {
                    "sent": "This criterion says that evaluates number of times or what selected in N is better than one selected in the reference point.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can give a probabilistic interpretation.",
                    "label": 0
                },
                {
                    "sent": "If we normalize this value, interpretation would be the property that we were drawn from.",
                    "label": 0
                },
                {
                    "sent": "N is better than one one in anytime.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So quickly the negative value negative if you only have negative feedback, we only have the last part.",
                    "label": 0
                },
                {
                    "sent": "So here we compute the number of times the reward.",
                    "label": 0
                },
                {
                    "sent": "In the reference point, is better than the.",
                    "label": 0
                },
                {
                    "sent": "The word in the in the vector N and we add the minus and so if we normalize another one, it's a property that's the vector.",
                    "label": 0
                },
                {
                    "sent": "N gives better work done and so forth.",
                    "label": 0
                },
                {
                    "sent": "When Journal case is just the sum of the soup.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, just to summarize what we did, we saw we justified.",
                    "label": 0
                },
                {
                    "sent": "Reference point based profound system.",
                    "label": 1
                },
                {
                    "sent": "So if you want to use that you need just need to define our MVP picture, reference points and then compute the reference vector and this reference vector in fact define the value of of rewards.",
                    "label": 0
                },
                {
                    "sent": "So I don't go on details here.",
                    "label": 0
                },
                {
                    "sent": "But then when you have a new worker here, what you can use it to solve this MDP with standard method.",
                    "label": 0
                },
                {
                    "sent": "And quickly.",
                    "label": 0
                },
                {
                    "sent": "If we have some ideas to how to pick the standpoint, we can just use a scale of the of the ordinary words, or we can use a probability distribution of the case, or can even use history phone number if you have.",
                    "label": 1
                },
                {
                    "sent": "Navigation problem you can always sometimes define a path that's preferred.",
                    "label": 0
                },
                {
                    "sent": "That's quite good, so it could be.",
                    "label": 1
                },
                {
                    "sent": "It could be used as a reference point an you can also provide a policy and this policy will.",
                    "label": 0
                },
                {
                    "sent": "Will induce a reference vector and we can use that too.",
                    "label": 0
                },
                {
                    "sent": "To serve their own.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also just quickly.",
                    "label": 0
                },
                {
                    "sent": "Even though even when you.",
                    "label": 0
                },
                {
                    "sent": "You start with the NDP and you have a new more color he wants.",
                    "label": 0
                },
                {
                    "sent": "You can use reference point based preferences to pick.",
                    "label": 0
                },
                {
                    "sent": "It's a bit controversial, but I would say better policy.",
                    "label": 0
                },
                {
                    "sent": "The idea is as follows.",
                    "label": 0
                },
                {
                    "sent": "You have MVP, you serve it on you.",
                    "label": 0
                },
                {
                    "sent": "Obtain optimal policy star.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this pistol can induce a reference vector and we can define a new function on this server.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And can be used to define a notable policy in this new MVP.",
                    "label": 0
                },
                {
                    "sent": "And the idea of this, the interpretation of this new optimal policy, is the policy that.",
                    "label": 0
                },
                {
                    "sent": "Maximize the probability of beating the the optimal policy that we found before and when it is useful.",
                    "label": 0
                },
                {
                    "sent": "It is useful when we can only apply when you are going to apply this policy only one time or a few times because the sound system in Soundout MVP's is in expectation, you're going to achieve the value function only if you repeat the process many times.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think I do.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not much time, so I'm.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to skip.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example and to finish.",
                    "label": 0
                },
                {
                    "sent": "So I presented a way to define semantically justified the word function an just a few words for future work.",
                    "label": 0
                },
                {
                    "sent": "If some of you interested, it was just theoretical ideas, but I would like to test those ideas in the real problems.",
                    "label": 0
                },
                {
                    "sent": "So if you have some problems that where you want to apply reference point based preferences, please come and have a chat with me.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "For building, for justifying the profound system, I use some axioms and we are, I think some work could be done to relax some of those actions.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "OK so I was saying that I've read some of his work quite a long time ago and it was I think also about ordinal based rewards and MVP's.",
                    "label": 0
                },
                {
                    "sent": "And given that the motivation seems to be quite legitimate, I mean the solutions are very sensitive to rewards.",
                    "label": 0
                },
                {
                    "sent": "I was I was wondering, you know, as as this stuff actually been, all these ideas actually been used massively by.",
                    "label": 0
                },
                {
                    "sent": "I don't know by people and if not, why not?",
                    "label": 0
                },
                {
                    "sent": "And you know, in in kind of practical yeah, settings so far they wanted to just theoretical, so I haven't tried.",
                    "label": 0
                },
                {
                    "sent": "Tried it on real problems.",
                    "label": 0
                },
                {
                    "sent": "Not necessarily that particular.",
                    "label": 0
                },
                {
                    "sent": "You know semantics, but the idea of ordinal rewards in in MVP's.",
                    "label": 0
                },
                {
                    "sent": "I mean I've seen before.",
                    "label": 0
                },
                {
                    "sent": "Yeah, what I did before it was just way even more surgical and said that in a very general framework.",
                    "label": 0
                },
                {
                    "sent": "So I think you can't do in a real problems you you just want to have an optimal policy and have just one.",
                    "label": 0
                },
                {
                    "sent": "Ann, you're you're very happy with it and you apply it.",
                    "label": 0
                },
                {
                    "sent": "No, right now I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean, if it tells you just anything, because when you are defining the ordinary rewards, the points you are going to come with your going to get a lot of policies that are equivalent.",
                    "label": 0
                },
                {
                    "sent": "So you had to add some preference information, yeah?",
                    "label": 0
                },
                {
                    "sent": "Alright, any questions?",
                    "label": 0
                },
                {
                    "sent": "Any other question?",
                    "label": 0
                },
                {
                    "sent": "Alright, well thank you.",
                    "label": 0
                }
            ]
        }
    }
}