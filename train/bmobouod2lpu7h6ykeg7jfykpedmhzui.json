{
    "id": "bmobouod2lpu7h6ykeg7jfykpedmhzui",
    "title": "Mining the Web for Meaning",
    "info": {
        "author": [
            "Peter D. Turney, National Research Council of Canada"
        ],
        "published": "Oct. 3, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Web Mining"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_turney_10years/",
    "segmentation": [
        [
            "Hi and thanks to Easymail PKD for giving me this award.",
            "Great honor.",
            "Thank you very much.",
            "I.",
            "Attended a number of talks today and they were all very good and.",
            "It seems to me that the conference Committee should be thanked for doing organizing a great conference.",
            "Thank you.",
            "And thanks to everyone here for coming to hear me talk.",
            "So I'm going to talk about mining the web for synonyms."
        ],
        [
            "My paper in 2001 was mining the web for synonyms, and in the last 10 years I've.",
            "Essentially expanded that to mining the web for meaning so beyond synonymy too.",
            "Analogy hypergamy miranna me composition, semantic composition beyond words to phrases sentences.",
            "So this 2001 paper was kind of a turning point in my career.",
            "I was before this paper I was.",
            "Writing mainly papers in machine learning and I've focused very much on computational linguistics and specifically on semantics.",
            "So.",
            "That's why you may not have seen me in many recent machine learning conferences, not mainly going to natural language processing conferences, which Interestingly, is a field that becoming domino."
        ],
        [
            "Did by machine learning.",
            "So.",
            "What I did in my 2001 paper was a very simple idea that the.",
            "It's based on pointwise mutual information, which is something that's been around for a long time.",
            "Long before my paper.",
            "So simple measure of.",
            "Co occurrence probability.",
            "The basic idea is you take the probability that 2 words appear together in attacks, the observed probability, and you compare that to the expected probability under the assumption that the Co occurrence of the words is just pure random chance and the ratio of the observed to the expected is a measure of how surprising it is that the two words occur together.",
            "It's basically a clue that there's an important semantic relationship between those two words.",
            "And for example, it gives you an indicator that the words may be synonyms.",
            "So the normal thing that my paper did was a very simple idea, it's just.",
            "Use hit counts for a web search engine to estimate the probability values in this equation.",
            "So if there's one lesson that you could take away from me getting this award, it's be the first one to write a paper on a very simple idea.",
            "Seems to be the trick."
        ],
        [
            "Now what can you do with the pointwise mutual information between 2 words or one thing you can do and one thing that I have done and many others have done now is sentiment analysis.",
            "Again, it's a very simple idea.",
            "If you have a set of reference positive words in a set of reference, negative words, so the positive words like good, nice, excellent, negative words like bad, nasty, poor and you have an unknown word like formal.",
            "Is formal a positive word?",
            "Is it a negative word you don't know?",
            "Well, you can estimate whether it's positive or negative just by taking the sum of its PMI with your positive reference words and subtracting the sum of the PMI with your negative reference words and.",
            "One thing you can do with that is decide whether a review, for example, a review of a car as a positive review or a negative review just by averaging.",
            "All the.",
            "Positive negative ratings that the words in the review have.",
            "It's a very simple idea, but it seems to work."
        ],
        [
            "What it doesn't tell you, though, is how much pointwise mutual information is.",
            "Is revealing about synonymy.",
            "What I did in my 2001 paper was evaluate pointwise mutual information using the test of English as a foreign language and in this test there's a target word or a stem word.",
            "Here, in this example, it's levied and there are four choice words and the problem is to select the choice where that's most similar in meaning to the target word, and here the solution is imposed.",
            "Levied is more or less synonymous with imposed, and this ends up getting.",
            "74% correct on the TOEFL test.",
            "The total synonym test.",
            "And the lesson here is it a very simple idea can work if you have enough data and the really made this work is using a search engine that at the time I use the AltaVista search engine search engine, that index is a huge amount of data and you can leverage that data to make a simple algorithm.",
            "Yield interesting results.",
            "And.",
            "My subsequent research is used corpora from the web, almost exclusive."
        ],
        [
            "Play.",
            "This is a table from the wiki of the Association for Computational Linguistics.",
            "It shows a whole bunch of different algorithms that have been evaluated using the TOEFL test and I just I'm just going to pick out three from this table.",
            "First is the human performance, which is.",
            "These are non English US college applicants so.",
            "Taking that into account, it's perhaps not too surprising that the only get about 64 1/2% correct.",
            "My 2001 pointwise mutual information algorithm gets 73.75% correct, but the I think the most interesting result is reinart rap.",
            "It's 92.5% correct.",
            "It's the highest score for a pure approach.",
            "You can get higher with a hybrid approach.",
            "Hybridizing several different algorithms.",
            "But I think a pure approach is more interesting 'cause it gives you more insight into what's going on, and I'll say a little bit about more about."
        ],
        [
            "At the moment.",
            "So One Direction you can take this research is beyond synonyms to analogies.",
            "US has analogy questions that are similar in structure to the total synonym questions except instead of individual words, it's looking at word pairs and instead of properties of individual words, it's looking at relationships between the words in a word pair.",
            "So here there's a target word pair, Mason stone, and then there are five choices, and the task is to find the choice that has the same semantic relationship as the target in solution here is Carpenter would.",
            "Mason is to stone as carpenters Tuita Mason cut Stone, a Mason builds with stone.",
            "A Carpenter cuts would a Carpenter builds with wood."
        ],
        [
            "And here's a table from the ACL wiki again.",
            "Showing various algorithms that have been applied to the SAT analogy questions and.",
            "The human score on this, which is.",
            "Grade 12 students in US.",
            "High schools graduating and applying to universities.",
            "The human score is 57%.",
            "The best algorithm gets 56.1%."
        ],
        [
            "Now the interesting thing for both, the total synonyms and the SSAT analogies is the best algorithm.",
            "Is doing something quite similar.",
            "Reinhardt wrap on the synonyms used, Ascentia Leah variation of latent semantic analysis.",
            "He used a big matrix.",
            "And.",
            "2 words were compared by calculating row vectors for them where the vectors were based on frequencies of occurrence in various contexts and the similarity of two words is calculated by the cosine of the angle between the vectors.",
            "I did the same thing essentially for the SAT analogies, except instead of looking at single words, I looked at a pair of words and the context in which 2 words appear together and again the similarity measure is the cosine of the angle."
        ],
        [
            "And this is this table here is just to give you a little bit of intuition about what's going on.",
            "So here's an analogy.",
            "Traffic is to street as water is to riverbed.",
            "Traffic flows down the street, water flows down a River bed.",
            "If you search on the web for phrases X in the Y where X is traffic and why is street or X is water and why is riverbed the pattern of hits you get is the same general pattern for both traffic St and water riverbed.",
            "The columns there for traffic St and Water River better essentially vectors, and if you compare those vectors by looking at their cosine, they'll have a high cosine because they point in the same general direction in a high dimensional space.",
            "Well, in this case it's only a 5 dimensional space, but in the actual paper it's several 1000 dimensional space, so that's the common idea essentially for the synonyms and the.",
            "Analogies is this."
        ],
        [
            "Vector space idea.",
            "And here you can see if you look at the cosines, the highest cosine is with between traffic St and Water Ridge."
        ],
        [
            "Or bad?",
            "So lately I've been focusing very much on vector space models of semantics, and if this interests you at all.",
            "Patrick Pentel and I wrote a survey paper that's in the Journal of Artificial Intelligence Research, where we try to survey the field from.",
            "From our own perspective, and there's a lot of pointers into the literature if you want to learn more about it.",
            "One thing that's interesting is the vector space model is closely tide to the distributional hypothesis, which is a hypothesis.",
            "The came out of linguistics, standard traditional linguistics.",
            "It's a hypothesis words that occur in similar context and have similar meanings.",
            "And there's a more general form of this which.",
            "Which formulated by well, perhaps by Warren Weaver in the 40s, or certainly by George Furnas and colleagues in 1983.",
            "Statistical patterns of human word usage can be used to figure out what people mean and that's.",
            "The statistical semantics hypothesis is what underlies most of my work for the last 10 years."
        ],
        [
            "So in this paper analogy, perception applied to 7 tests of word comprehension.",
            "I show that vector space model can handle quite a few things beyond synonymy.",
            "An analogy can handle synonym versus antonym distinction.",
            "It can distinguish words that are similar from words that are associated merely associated and words that are both similar and associated.",
            "It can classify noun modifier relations."
        ],
        [
            "It can also do beyond simple proportional analogies.",
            "Analogies that I showed you earlier for the SAT tests are called proportional analogies or just four terminologies.",
            "There are more complicated analogies, for example the mapping of the solar system to the Rutherford Bohr Atom.",
            "This analogy was used.",
            "In physics at one point as a way of understanding how the Atom works.",
            "And it's.",
            "It's not a simple proportional analogy, it's a system."
        ],
        [
            "Sonic analogy.",
            "So there's a vector based algorithm that can solve this kind of analogy.",
            "You feed in a list of words from your source domain, list of words from your target domain."
        ],
        [
            "And.",
            "The output of the algorithm is a mapping.",
            "And essentially what the algorithm does is it breaks this complex systematic analogy into a whole bunch of little proportional analogies."
        ],
        [
            "No, the vector approach works well for individual words, but what about phrases, sentences, paragraphs?",
            "This is a very hot topic in this field these days for the last.",
            "Couple of years there's been a lot of interest in this, and there's been some progress made, so the idea is if you have a vector for dog and you have a vector for house, can you calculate a vector that represents doghouse and it looks like you can.",
            "I mean, it's not a solved problem, but there's some very promising results.",
            "So this kind of vector based approach to semantics.",
            "It looks like it will scale up to phrases and sentences or papers that apply it to sentence is ultimately.",
            "It seems to me that there's no reason it can't apply to much more complicated paragraphs documents."
        ],
        [
            "I think there's a lot of potential here.",
            "What about logic?",
            "Perhaps the when?",
            "If you know anything about semantics, you probably thinking what about logic?",
            "Where does formal logic coming here?",
            "Isn't that the way that you're supposed to understand the semantics of text?",
            "In my own opinion.",
            "No, I don't think we're very logical.",
            "Humans are not very logical if you try to take sentences in English and translate them into logic, you'll find it's a very difficult task, but.",
            "Another answer is that.",
            "Vectors and logic are not exclusive, and there's some interesting work on bringing the two together.",
            "And Dominic Widdows has a book on this topic 24.",
            "Geometry and meaning.",
            "Which I think Sketch is a very interesting direction.",
            "The basic idea is that operations in vector space correspond to operations and logic, so you can if you want to, say, do a search on the word bass but or base, but not bass you can.",
            "You can formulate that as a vector operation.",
            "Look for the projection of the base vector into the Earth orthogonal complement of the fish vector."
        ],
        [
            "Analog versus digital.",
            "So vectors are analog revalued, real valued, continuous distributed, diffuse, spatial geometrical words are digital at either.",
            "Is that word or it isn't that word, it's true or false or discrete, their symbolic, they're logical.",
            "It seems kind of counter and to be using vectors as a way of representing the meaning of words.",
            "So I think that raises the question of how you can reconcile vectors in words."
        ],
        [
            "And.",
            "This is kind of a sketch of an answer.",
            "I don't have a worked out detailed answer, but I think this is the way to approach this kind of thing.",
            "The vectors give you prior probabilities and.",
            "Once, once you observe an event.",
            "You update your probabilities.",
            "You have a posterior probability and the prior probabilities are distributed.",
            "They're close to being uniform, but once you've made an observation.",
            "The probability is the posterior probabilities become much more sharp and crisp.",
            "So for example, if I'm looking at something like this class here, I might have a cloud of prior probabilities of what label I might apply to this and.",
            "Then I see the glass I identify it is a glass, so I converge on glass having a high probability and other things having a very low probability.",
            "I see a feedback loop here too.",
            "Once you've made an observation and classified something, you go back and update your model for your prior probabilities."
        ],
        [
            "So.",
            "What I've been working on for the last 10 years since my 2001 paper is the statistical semantics hypothesis, statistical patterns of human word usage can be used to figure out what people mean, not hypothesis was first put forth.",
            "Clearly, I think in a paper by Furnace Al in 1983 an if you're interested in learning more about this work.",
            "There's actually a lot.",
            "I'm certainly not the only person doing it.",
            "There's many, many people doing it.",
            "And this survey paper I did with Patrick Pentel is a good pointer into that."
        ],
        [
            "So very briefly, what's happened in the last 10 years?",
            "I've gone from mining the web for synonyms to mining the web for meaning.",
            "Thanks again for giving me this award.",
            "Thank you very much Peter for this very nice talk.",
            "Very intuitive.",
            "And we have some meetings for a few questions, so it's ready.",
            "It's going to be hard one.",
            "Don't worry.",
            "So, so I think there's a lot of semantics you can get out of just looking at language and it text at Co occurrences.",
            "All these things, but there's clearly a lot you cannot get.",
            "You know if I want to robot that can say, you know, pick up the red block on top of the blue block.",
            "We're never going to be able to get a robot to do that without understanding the connection between language and perception.",
            "And no matter how much text we mind, we're not going to get there.",
            "So I assume you agree.",
            "I mean, they get you a lot, but there's much more to semantics than just what you can mine from text.",
            "I'm not sure what where the limit is my.",
            "Research program is to push it as far as it'll go.",
            "One thing I like to think of is Helen Keller is an example like she was deaf and blind.",
            "All she had was touch.",
            "Now if you asked her to pick up the red block or the blue box blocks, she'd have a problem.",
            "Now.",
            "Touch of course is a very important sense and it tells you a lot about the world.",
            "So.",
            "Maybe what if, maybe if if you exclude the input to text, what you're limited to is what a computer with no manipulators and no sense organs.",
            "We do it.",
            "You'd be limited, perhaps to what a human being trapped in a body unable, paralyzed and unable to see in here maybe would be limited to what they would be capable of, but that I think is an awful lot.",
            "It's quite an impressive amount, really.",
            "Hi, thank you for this and you're the 10 years ago talk, you're saying that we can keep it simple when we have huge amounts of data.",
            "But right now you you see people and they're trying to train models of language was millions of parameters and they do so with rather huge collection of documents.",
            "Although not as huge as web.",
            "So I was wondering, would you have recommendations or position or comments?",
            "You probably.",
            "Heard the expression of picking the low hanging fruit.",
            "In a way I kind of made a career, I think out of picking low hanging fruit, getting to it before anyone else can grab it.",
            "So eventually I'm going to run out of low hanging fruit I guess, and I think that's maybe what you're alluding to.",
            "How far can you get just throwing simple algorithms that huge quantities of data?",
            "Eventually those algorithms are going to have to be become more complicated or eventually well.",
            "Humans seem to do pretty well with smaller amounts of data, so.",
            "It looks like that strategy is eventually going to run out, and maybe that'll be time for me to retire.",
            "I don't know.",
            "One last question.",
            "This is sort of a follow up to both the previous questions about in a concrete case.",
            "How can you use distribution to tell the difference between antonyms?",
            "So the context of the word black is almost identical to the context of the word white.",
            "Read my 2008 koelling paper.",
            "That's the short answer.",
            "The longer answer is you look for Co occurrences of the two words and the types of phrases they appear in, and it turns out you can tell that way and actually decay in Lynn I think did this first in a 2003 paper.",
            "Black and white tend to occur in phrases that mark them as being antonyms black, not white.",
            "Not black, but white, so it's certainly feasible.",
            "OK, so let's think Peter Darnick once more."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi and thanks to Easymail PKD for giving me this award.",
                    "label": 0
                },
                {
                    "sent": "Great honor.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Attended a number of talks today and they were all very good and.",
                    "label": 0
                },
                {
                    "sent": "It seems to me that the conference Committee should be thanked for doing organizing a great conference.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "And thanks to everyone here for coming to hear me talk.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about mining the web for synonyms.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My paper in 2001 was mining the web for synonyms, and in the last 10 years I've.",
                    "label": 1
                },
                {
                    "sent": "Essentially expanded that to mining the web for meaning so beyond synonymy too.",
                    "label": 1
                },
                {
                    "sent": "Analogy hypergamy miranna me composition, semantic composition beyond words to phrases sentences.",
                    "label": 0
                },
                {
                    "sent": "So this 2001 paper was kind of a turning point in my career.",
                    "label": 0
                },
                {
                    "sent": "I was before this paper I was.",
                    "label": 0
                },
                {
                    "sent": "Writing mainly papers in machine learning and I've focused very much on computational linguistics and specifically on semantics.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's why you may not have seen me in many recent machine learning conferences, not mainly going to natural language processing conferences, which Interestingly, is a field that becoming domino.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Did by machine learning.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What I did in my 2001 paper was a very simple idea that the.",
                    "label": 0
                },
                {
                    "sent": "It's based on pointwise mutual information, which is something that's been around for a long time.",
                    "label": 1
                },
                {
                    "sent": "Long before my paper.",
                    "label": 0
                },
                {
                    "sent": "So simple measure of.",
                    "label": 0
                },
                {
                    "sent": "Co occurrence probability.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is you take the probability that 2 words appear together in attacks, the observed probability, and you compare that to the expected probability under the assumption that the Co occurrence of the words is just pure random chance and the ratio of the observed to the expected is a measure of how surprising it is that the two words occur together.",
                    "label": 0
                },
                {
                    "sent": "It's basically a clue that there's an important semantic relationship between those two words.",
                    "label": 0
                },
                {
                    "sent": "And for example, it gives you an indicator that the words may be synonyms.",
                    "label": 0
                },
                {
                    "sent": "So the normal thing that my paper did was a very simple idea, it's just.",
                    "label": 0
                },
                {
                    "sent": "Use hit counts for a web search engine to estimate the probability values in this equation.",
                    "label": 1
                },
                {
                    "sent": "So if there's one lesson that you could take away from me getting this award, it's be the first one to write a paper on a very simple idea.",
                    "label": 0
                },
                {
                    "sent": "Seems to be the trick.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now what can you do with the pointwise mutual information between 2 words or one thing you can do and one thing that I have done and many others have done now is sentiment analysis.",
                    "label": 0
                },
                {
                    "sent": "Again, it's a very simple idea.",
                    "label": 0
                },
                {
                    "sent": "If you have a set of reference positive words in a set of reference, negative words, so the positive words like good, nice, excellent, negative words like bad, nasty, poor and you have an unknown word like formal.",
                    "label": 1
                },
                {
                    "sent": "Is formal a positive word?",
                    "label": 0
                },
                {
                    "sent": "Is it a negative word you don't know?",
                    "label": 1
                },
                {
                    "sent": "Well, you can estimate whether it's positive or negative just by taking the sum of its PMI with your positive reference words and subtracting the sum of the PMI with your negative reference words and.",
                    "label": 0
                },
                {
                    "sent": "One thing you can do with that is decide whether a review, for example, a review of a car as a positive review or a negative review just by averaging.",
                    "label": 0
                },
                {
                    "sent": "All the.",
                    "label": 0
                },
                {
                    "sent": "Positive negative ratings that the words in the review have.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple idea, but it seems to work.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What it doesn't tell you, though, is how much pointwise mutual information is.",
                    "label": 0
                },
                {
                    "sent": "Is revealing about synonymy.",
                    "label": 0
                },
                {
                    "sent": "What I did in my 2001 paper was evaluate pointwise mutual information using the test of English as a foreign language and in this test there's a target word or a stem word.",
                    "label": 1
                },
                {
                    "sent": "Here, in this example, it's levied and there are four choice words and the problem is to select the choice where that's most similar in meaning to the target word, and here the solution is imposed.",
                    "label": 0
                },
                {
                    "sent": "Levied is more or less synonymous with imposed, and this ends up getting.",
                    "label": 0
                },
                {
                    "sent": "74% correct on the TOEFL test.",
                    "label": 0
                },
                {
                    "sent": "The total synonym test.",
                    "label": 0
                },
                {
                    "sent": "And the lesson here is it a very simple idea can work if you have enough data and the really made this work is using a search engine that at the time I use the AltaVista search engine search engine, that index is a huge amount of data and you can leverage that data to make a simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "Yield interesting results.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "My subsequent research is used corpora from the web, almost exclusive.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Play.",
                    "label": 0
                },
                {
                    "sent": "This is a table from the wiki of the Association for Computational Linguistics.",
                    "label": 0
                },
                {
                    "sent": "It shows a whole bunch of different algorithms that have been evaluated using the TOEFL test and I just I'm just going to pick out three from this table.",
                    "label": 0
                },
                {
                    "sent": "First is the human performance, which is.",
                    "label": 0
                },
                {
                    "sent": "These are non English US college applicants so.",
                    "label": 0
                },
                {
                    "sent": "Taking that into account, it's perhaps not too surprising that the only get about 64 1/2% correct.",
                    "label": 0
                },
                {
                    "sent": "My 2001 pointwise mutual information algorithm gets 73.75% correct, but the I think the most interesting result is reinart rap.",
                    "label": 0
                },
                {
                    "sent": "It's 92.5% correct.",
                    "label": 0
                },
                {
                    "sent": "It's the highest score for a pure approach.",
                    "label": 0
                },
                {
                    "sent": "You can get higher with a hybrid approach.",
                    "label": 0
                },
                {
                    "sent": "Hybridizing several different algorithms.",
                    "label": 0
                },
                {
                    "sent": "But I think a pure approach is more interesting 'cause it gives you more insight into what's going on, and I'll say a little bit about more about.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the moment.",
                    "label": 0
                },
                {
                    "sent": "So One Direction you can take this research is beyond synonyms to analogies.",
                    "label": 1
                },
                {
                    "sent": "US has analogy questions that are similar in structure to the total synonym questions except instead of individual words, it's looking at word pairs and instead of properties of individual words, it's looking at relationships between the words in a word pair.",
                    "label": 1
                },
                {
                    "sent": "So here there's a target word pair, Mason stone, and then there are five choices, and the task is to find the choice that has the same semantic relationship as the target in solution here is Carpenter would.",
                    "label": 0
                },
                {
                    "sent": "Mason is to stone as carpenters Tuita Mason cut Stone, a Mason builds with stone.",
                    "label": 0
                },
                {
                    "sent": "A Carpenter cuts would a Carpenter builds with wood.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's a table from the ACL wiki again.",
                    "label": 0
                },
                {
                    "sent": "Showing various algorithms that have been applied to the SAT analogy questions and.",
                    "label": 0
                },
                {
                    "sent": "The human score on this, which is.",
                    "label": 0
                },
                {
                    "sent": "Grade 12 students in US.",
                    "label": 0
                },
                {
                    "sent": "High schools graduating and applying to universities.",
                    "label": 0
                },
                {
                    "sent": "The human score is 57%.",
                    "label": 0
                },
                {
                    "sent": "The best algorithm gets 56.1%.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the interesting thing for both, the total synonyms and the SSAT analogies is the best algorithm.",
                    "label": 0
                },
                {
                    "sent": "Is doing something quite similar.",
                    "label": 0
                },
                {
                    "sent": "Reinhardt wrap on the synonyms used, Ascentia Leah variation of latent semantic analysis.",
                    "label": 0
                },
                {
                    "sent": "He used a big matrix.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "2 words were compared by calculating row vectors for them where the vectors were based on frequencies of occurrence in various contexts and the similarity of two words is calculated by the cosine of the angle between the vectors.",
                    "label": 1
                },
                {
                    "sent": "I did the same thing essentially for the SAT analogies, except instead of looking at single words, I looked at a pair of words and the context in which 2 words appear together and again the similarity measure is the cosine of the angle.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is this table here is just to give you a little bit of intuition about what's going on.",
                    "label": 0
                },
                {
                    "sent": "So here's an analogy.",
                    "label": 0
                },
                {
                    "sent": "Traffic is to street as water is to riverbed.",
                    "label": 1
                },
                {
                    "sent": "Traffic flows down the street, water flows down a River bed.",
                    "label": 0
                },
                {
                    "sent": "If you search on the web for phrases X in the Y where X is traffic and why is street or X is water and why is riverbed the pattern of hits you get is the same general pattern for both traffic St and water riverbed.",
                    "label": 0
                },
                {
                    "sent": "The columns there for traffic St and Water River better essentially vectors, and if you compare those vectors by looking at their cosine, they'll have a high cosine because they point in the same general direction in a high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Well, in this case it's only a 5 dimensional space, but in the actual paper it's several 1000 dimensional space, so that's the common idea essentially for the synonyms and the.",
                    "label": 0
                },
                {
                    "sent": "Analogies is this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vector space idea.",
                    "label": 0
                },
                {
                    "sent": "And here you can see if you look at the cosines, the highest cosine is with between traffic St and Water Ridge.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or bad?",
                    "label": 0
                },
                {
                    "sent": "So lately I've been focusing very much on vector space models of semantics, and if this interests you at all.",
                    "label": 0
                },
                {
                    "sent": "Patrick Pentel and I wrote a survey paper that's in the Journal of Artificial Intelligence Research, where we try to survey the field from.",
                    "label": 1
                },
                {
                    "sent": "From our own perspective, and there's a lot of pointers into the literature if you want to learn more about it.",
                    "label": 0
                },
                {
                    "sent": "One thing that's interesting is the vector space model is closely tide to the distributional hypothesis, which is a hypothesis.",
                    "label": 1
                },
                {
                    "sent": "The came out of linguistics, standard traditional linguistics.",
                    "label": 0
                },
                {
                    "sent": "It's a hypothesis words that occur in similar context and have similar meanings.",
                    "label": 1
                },
                {
                    "sent": "And there's a more general form of this which.",
                    "label": 0
                },
                {
                    "sent": "Which formulated by well, perhaps by Warren Weaver in the 40s, or certainly by George Furnas and colleagues in 1983.",
                    "label": 0
                },
                {
                    "sent": "Statistical patterns of human word usage can be used to figure out what people mean and that's.",
                    "label": 1
                },
                {
                    "sent": "The statistical semantics hypothesis is what underlies most of my work for the last 10 years.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this paper analogy, perception applied to 7 tests of word comprehension.",
                    "label": 1
                },
                {
                    "sent": "I show that vector space model can handle quite a few things beyond synonymy.",
                    "label": 0
                },
                {
                    "sent": "An analogy can handle synonym versus antonym distinction.",
                    "label": 0
                },
                {
                    "sent": "It can distinguish words that are similar from words that are associated merely associated and words that are both similar and associated.",
                    "label": 0
                },
                {
                    "sent": "It can classify noun modifier relations.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It can also do beyond simple proportional analogies.",
                    "label": 0
                },
                {
                    "sent": "Analogies that I showed you earlier for the SAT tests are called proportional analogies or just four terminologies.",
                    "label": 0
                },
                {
                    "sent": "There are more complicated analogies, for example the mapping of the solar system to the Rutherford Bohr Atom.",
                    "label": 1
                },
                {
                    "sent": "This analogy was used.",
                    "label": 0
                },
                {
                    "sent": "In physics at one point as a way of understanding how the Atom works.",
                    "label": 0
                },
                {
                    "sent": "And it's.",
                    "label": 0
                },
                {
                    "sent": "It's not a simple proportional analogy, it's a system.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sonic analogy.",
                    "label": 0
                },
                {
                    "sent": "So there's a vector based algorithm that can solve this kind of analogy.",
                    "label": 0
                },
                {
                    "sent": "You feed in a list of words from your source domain, list of words from your target domain.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The output of the algorithm is a mapping.",
                    "label": 1
                },
                {
                    "sent": "And essentially what the algorithm does is it breaks this complex systematic analogy into a whole bunch of little proportional analogies.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, the vector approach works well for individual words, but what about phrases, sentences, paragraphs?",
                    "label": 1
                },
                {
                    "sent": "This is a very hot topic in this field these days for the last.",
                    "label": 1
                },
                {
                    "sent": "Couple of years there's been a lot of interest in this, and there's been some progress made, so the idea is if you have a vector for dog and you have a vector for house, can you calculate a vector that represents doghouse and it looks like you can.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not a solved problem, but there's some very promising results.",
                    "label": 0
                },
                {
                    "sent": "So this kind of vector based approach to semantics.",
                    "label": 0
                },
                {
                    "sent": "It looks like it will scale up to phrases and sentences or papers that apply it to sentence is ultimately.",
                    "label": 0
                },
                {
                    "sent": "It seems to me that there's no reason it can't apply to much more complicated paragraphs documents.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think there's a lot of potential here.",
                    "label": 0
                },
                {
                    "sent": "What about logic?",
                    "label": 0
                },
                {
                    "sent": "Perhaps the when?",
                    "label": 0
                },
                {
                    "sent": "If you know anything about semantics, you probably thinking what about logic?",
                    "label": 0
                },
                {
                    "sent": "Where does formal logic coming here?",
                    "label": 0
                },
                {
                    "sent": "Isn't that the way that you're supposed to understand the semantics of text?",
                    "label": 0
                },
                {
                    "sent": "In my own opinion.",
                    "label": 0
                },
                {
                    "sent": "No, I don't think we're very logical.",
                    "label": 0
                },
                {
                    "sent": "Humans are not very logical if you try to take sentences in English and translate them into logic, you'll find it's a very difficult task, but.",
                    "label": 0
                },
                {
                    "sent": "Another answer is that.",
                    "label": 0
                },
                {
                    "sent": "Vectors and logic are not exclusive, and there's some interesting work on bringing the two together.",
                    "label": 1
                },
                {
                    "sent": "And Dominic Widdows has a book on this topic 24.",
                    "label": 0
                },
                {
                    "sent": "Geometry and meaning.",
                    "label": 0
                },
                {
                    "sent": "Which I think Sketch is a very interesting direction.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is that operations in vector space correspond to operations and logic, so you can if you want to, say, do a search on the word bass but or base, but not bass you can.",
                    "label": 0
                },
                {
                    "sent": "You can formulate that as a vector operation.",
                    "label": 0
                },
                {
                    "sent": "Look for the projection of the base vector into the Earth orthogonal complement of the fish vector.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Analog versus digital.",
                    "label": 0
                },
                {
                    "sent": "So vectors are analog revalued, real valued, continuous distributed, diffuse, spatial geometrical words are digital at either.",
                    "label": 1
                },
                {
                    "sent": "Is that word or it isn't that word, it's true or false or discrete, their symbolic, they're logical.",
                    "label": 0
                },
                {
                    "sent": "It seems kind of counter and to be using vectors as a way of representing the meaning of words.",
                    "label": 0
                },
                {
                    "sent": "So I think that raises the question of how you can reconcile vectors in words.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This is kind of a sketch of an answer.",
                    "label": 0
                },
                {
                    "sent": "I don't have a worked out detailed answer, but I think this is the way to approach this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "The vectors give you prior probabilities and.",
                    "label": 0
                },
                {
                    "sent": "Once, once you observe an event.",
                    "label": 0
                },
                {
                    "sent": "You update your probabilities.",
                    "label": 0
                },
                {
                    "sent": "You have a posterior probability and the prior probabilities are distributed.",
                    "label": 0
                },
                {
                    "sent": "They're close to being uniform, but once you've made an observation.",
                    "label": 0
                },
                {
                    "sent": "The probability is the posterior probabilities become much more sharp and crisp.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I'm looking at something like this class here, I might have a cloud of prior probabilities of what label I might apply to this and.",
                    "label": 0
                },
                {
                    "sent": "Then I see the glass I identify it is a glass, so I converge on glass having a high probability and other things having a very low probability.",
                    "label": 0
                },
                {
                    "sent": "I see a feedback loop here too.",
                    "label": 0
                },
                {
                    "sent": "Once you've made an observation and classified something, you go back and update your model for your prior probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What I've been working on for the last 10 years since my 2001 paper is the statistical semantics hypothesis, statistical patterns of human word usage can be used to figure out what people mean, not hypothesis was first put forth.",
                    "label": 1
                },
                {
                    "sent": "Clearly, I think in a paper by Furnace Al in 1983 an if you're interested in learning more about this work.",
                    "label": 0
                },
                {
                    "sent": "There's actually a lot.",
                    "label": 0
                },
                {
                    "sent": "I'm certainly not the only person doing it.",
                    "label": 0
                },
                {
                    "sent": "There's many, many people doing it.",
                    "label": 0
                },
                {
                    "sent": "And this survey paper I did with Patrick Pentel is a good pointer into that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So very briefly, what's happened in the last 10 years?",
                    "label": 0
                },
                {
                    "sent": "I've gone from mining the web for synonyms to mining the web for meaning.",
                    "label": 1
                },
                {
                    "sent": "Thanks again for giving me this award.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much Peter for this very nice talk.",
                    "label": 0
                },
                {
                    "sent": "Very intuitive.",
                    "label": 0
                },
                {
                    "sent": "And we have some meetings for a few questions, so it's ready.",
                    "label": 0
                },
                {
                    "sent": "It's going to be hard one.",
                    "label": 0
                },
                {
                    "sent": "Don't worry.",
                    "label": 0
                },
                {
                    "sent": "So, so I think there's a lot of semantics you can get out of just looking at language and it text at Co occurrences.",
                    "label": 0
                },
                {
                    "sent": "All these things, but there's clearly a lot you cannot get.",
                    "label": 0
                },
                {
                    "sent": "You know if I want to robot that can say, you know, pick up the red block on top of the blue block.",
                    "label": 0
                },
                {
                    "sent": "We're never going to be able to get a robot to do that without understanding the connection between language and perception.",
                    "label": 0
                },
                {
                    "sent": "And no matter how much text we mind, we're not going to get there.",
                    "label": 0
                },
                {
                    "sent": "So I assume you agree.",
                    "label": 0
                },
                {
                    "sent": "I mean, they get you a lot, but there's much more to semantics than just what you can mine from text.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure what where the limit is my.",
                    "label": 0
                },
                {
                    "sent": "Research program is to push it as far as it'll go.",
                    "label": 0
                },
                {
                    "sent": "One thing I like to think of is Helen Keller is an example like she was deaf and blind.",
                    "label": 0
                },
                {
                    "sent": "All she had was touch.",
                    "label": 0
                },
                {
                    "sent": "Now if you asked her to pick up the red block or the blue box blocks, she'd have a problem.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Touch of course is a very important sense and it tells you a lot about the world.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Maybe what if, maybe if if you exclude the input to text, what you're limited to is what a computer with no manipulators and no sense organs.",
                    "label": 0
                },
                {
                    "sent": "We do it.",
                    "label": 0
                },
                {
                    "sent": "You'd be limited, perhaps to what a human being trapped in a body unable, paralyzed and unable to see in here maybe would be limited to what they would be capable of, but that I think is an awful lot.",
                    "label": 0
                },
                {
                    "sent": "It's quite an impressive amount, really.",
                    "label": 0
                },
                {
                    "sent": "Hi, thank you for this and you're the 10 years ago talk, you're saying that we can keep it simple when we have huge amounts of data.",
                    "label": 0
                },
                {
                    "sent": "But right now you you see people and they're trying to train models of language was millions of parameters and they do so with rather huge collection of documents.",
                    "label": 0
                },
                {
                    "sent": "Although not as huge as web.",
                    "label": 0
                },
                {
                    "sent": "So I was wondering, would you have recommendations or position or comments?",
                    "label": 0
                },
                {
                    "sent": "You probably.",
                    "label": 0
                },
                {
                    "sent": "Heard the expression of picking the low hanging fruit.",
                    "label": 0
                },
                {
                    "sent": "In a way I kind of made a career, I think out of picking low hanging fruit, getting to it before anyone else can grab it.",
                    "label": 0
                },
                {
                    "sent": "So eventually I'm going to run out of low hanging fruit I guess, and I think that's maybe what you're alluding to.",
                    "label": 0
                },
                {
                    "sent": "How far can you get just throwing simple algorithms that huge quantities of data?",
                    "label": 0
                },
                {
                    "sent": "Eventually those algorithms are going to have to be become more complicated or eventually well.",
                    "label": 0
                },
                {
                    "sent": "Humans seem to do pretty well with smaller amounts of data, so.",
                    "label": 0
                },
                {
                    "sent": "It looks like that strategy is eventually going to run out, and maybe that'll be time for me to retire.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "One last question.",
                    "label": 0
                },
                {
                    "sent": "This is sort of a follow up to both the previous questions about in a concrete case.",
                    "label": 0
                },
                {
                    "sent": "How can you use distribution to tell the difference between antonyms?",
                    "label": 0
                },
                {
                    "sent": "So the context of the word black is almost identical to the context of the word white.",
                    "label": 0
                },
                {
                    "sent": "Read my 2008 koelling paper.",
                    "label": 0
                },
                {
                    "sent": "That's the short answer.",
                    "label": 0
                },
                {
                    "sent": "The longer answer is you look for Co occurrences of the two words and the types of phrases they appear in, and it turns out you can tell that way and actually decay in Lynn I think did this first in a 2003 paper.",
                    "label": 0
                },
                {
                    "sent": "Black and white tend to occur in phrases that mark them as being antonyms black, not white.",
                    "label": 0
                },
                {
                    "sent": "Not black, but white, so it's certainly feasible.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's think Peter Darnick once more.",
                    "label": 0
                }
            ]
        }
    }
}