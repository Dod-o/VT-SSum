{
    "id": "7vow5dp3b5q5vbb37l4vx2fubhkmrick",
    "title": "Combining Word and Entity Embeddings for Entity Linking",
    "info": {
        "author": [
            "Jos\u00e9 Moreno, IRIT (Toulouse Institute of Computer Science Research)"
        ],
        "published": "July 10, 2017",
        "recorded": "May 2017",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2017_moreno_entity_linking/",
    "segmentation": [
        [
            "Today I will present this Old World combining work and entity embeddings for entity linking.",
            "This is a collaborative work.",
            "Between a three different labs in France, the I did Sarah and Lindsay.",
            "I'm going to say Moreno in the future."
        ],
        [
            "So first I will start with the definition of and the task for us and we want to do some entity linking here in this work.",
            "So we define it like the task line consists in connecting an entity.",
            "Mention two are already known entity in a knowledge base and one important thing is then we assume then somebody else have already identified the.",
            "Stream then we need to connect to the knowledge base.",
            "Don't please consider this sample in which the words in bold are being already identified and serve the world and we need to connect to knowledge base.",
            "So the solution for this example will be the given entities for each of these mentions.",
            "So I'm using here the info box of the Wikipedia page.",
            "If we consider them Wikipedia.",
            "Is all knowledge base and to indicate the solution."
        ],
        [
            "So this task is also called entity disambiguation, because we can.",
            "Yeah, apply a simple keyword search engine.",
            "Over these words, then we need to link and then we will retrieve some candidates and then when we need to do is disambiguate.",
            "One or decide which of these candidates is the correct one.",
            "So this is a challenging task because as more stream camera to many different entities.",
            "For example here we read.",
            "We found a 1700 different entities for the World Bank."
        ],
        [
            "So the outline of this presentation I will just introduce a little bit more work then I will explain how people move from front to entity embeddings an then will I will introduce or model.",
            "And I will show you how our model had been integrated in entity linking system.",
            "That results and."
        ],
        [
            "Conclusion.",
            "Don't edit the preliminaries.",
            "We want to say that the Entity link task than the sun is int array.",
            "We want to address in this work is the entity linking task in which we have annotated corpus such as Wikipedia.",
            "We have, we know one knowledge based and could be Wikipedia itself or DB pedia or any other knowledge base.",
            "But we need to know the mapping between the knowledge base and the Wikipedia and we have annotated documents.",
            "These documents could be.",
            "A patent.",
            "A scientific article, but we need to have this annotated documents and it's are the target documents.",
            "Then we went to annotate, and in this case, or a document is a web page or for our work is only a web page and when I try it when I.",
            "Use them, they work wedding, I mean mentioned the the scene with the opposite in a document."
        ],
        [
            "I would bet."
        ],
        [
            "So what we can use to solve this problem?",
            "We have two types of the information.",
            "When is the information that came from the input.",
            "Or the quitting?",
            "And then the information that is in the dotted, the information that is in the knowledge base.",
            "So many words has already tried to solve this problem using this information in a separated features.",
            "So what we want to do is combine these two features.",
            "Combine these two seem formation in just one and to do or or objective is or on.",
            "Alright, but this is then the the combination of these both sources.",
            "We deal with a with a better description to disambiguate the entity in the entities.",
            "Then we trip.",
            "So at the end we want to do is calculate similarities between information from the input and information from the output."
        ],
        [
            "So how people move from word to entity embeddings?",
            "And this is a.",
            "The better beginning the disease stories with this simple idea you shall know award by the company that it keeps and I will give you this one example on how Skip Gram model works.",
            "You take a window and when you want to.",
            "Apply this algorithm and you will take the middle word like a input and we will try to predict all the context, word the water underline with green here.",
            "So Judah, this key brand model will learn some weights to do this task, and these weights are used for represent the information and then you have.",
            "This is a completely unsupervised algorithm and the idea is that you will optimize.",
            "You will find these two vectors.",
            "For the input and the output words and, you just define the conditional probability using this formula.",
            "So people have already thinking then you shall know an entity by the company that he keeps, because you just translate the idea from word 20s and there are I will just focus in three words, previous work, in which they use this idea.",
            "And they say, OK, we can use our knowledge base an represent each of the elements in the knowledge base like betters.",
            "So this is a transition work and they managed to get good results, but they only represent.",
            "Entities or relations.",
            "Another word that I have syncing then how to combine this detects and the entities and they use for example the idea of escape gram.",
            "They have an optimization function in which they include.",
            "The screen is keep gramedia the trans E idea and they happen and alignment.",
            "And alignment is step in which they try to just put together a.",
            "Word an entity 10 correlate.",
            "There are also other words in which they just use escape gram for example, and based on Wikipedia they use the full page of Wikipedia and then they duplicated this information using only the entities.",
            "So you have a double size web Wikipedia page and with you apply directly the Skip gram model and you manage to learn these two.",
            "These two different representations.",
            "So these two works allow you to calculate similarities between words.",
            "On the entities."
        ],
        [
            "So order order idea is inspired in this.",
            "In this previous work of course, but what we wanted to do is to directly learn the representations based on the.",
            "The contents of the entity because the previous word doesn't doesn't have used directly the contest, they just.",
            "Use the context of entities for entities, for example, and what we do is we just define a new unit and we call it the IT unit.",
            "This unit it could be represented for an entity an award.",
            "And each time then we need to process a sentence in which we find it unit.",
            "We just apply this formula instantly.",
            "The original skip gram formula.",
            "So what we are doing here is only we are expanding.",
            "So when we find out it's unit, we just expand the window, keeping the same contest for the entity and for the world.",
            "So we will do all this process for the all the heat units.",
            "Then we can find in a in a in a document."
        ],
        [
            "So I will just give a graphical explanation of this idea.",
            "So for example, here we are processing this window and escape grand model.",
            "We use these words like input and the green ones like output.",
            "So like we have here it.",
            "Unit we need to expand this this window so the 1st and we are.",
            "We will expand this window using only the world and the second time we will expand it using the entity.",
            "We use an identifier for the entity.",
            "So this.",
            "Simple idea law us to represent this warning pal and this.",
            "Identifier for the the entity nipple in the same space using the same context, but the intuition is that maybe you will get.",
            "Duplicated better design, better for new balances and better for the Wikipedia.",
            "They the entity name part.",
            "But it's not true because the the Warner Pal is associated in Wikipedia.",
            "Two different entities and also the the entity Nepal is associated.",
            "We different words to use different words are used to associate to this entity."
        ],
        [
            "So the main advantage of our model is then you can apply the Skip gram model, but also the continued back before model.",
            "The other advantage is that we use directly the anchor text information that is available in Wikipedia and.",
            "And then in and then this this representation for the entities are learning using directly the contest and not we're not doing some catenation or alignment.",
            "Additional steps."
        ],
        [
            "So now that we have a model that allows us to calculate these similarities, we want to integrate it in an entity linking and system.",
            "So this is this figure at the right show you the different steps and we have to implemented entity linking system and we concentrate in these two steps.",
            "The query expansion and the candidate generation and also the candidate ranking where we are using our it features."
        ],
        [
            "So fierce to achieve a good I strong baseline we define.",
            "And some groups of features to represent our entities that feeds group is the generation group in which we also use each of these strategies to generate over candidates.",
            "Tom, for example, we check if the lexical form of the entity is equal to the Entity label.",
            "So this is a generation.",
            "This will allow us to generate entities candidates, but also we will use it to indicate if the candidate will use it like a feature.",
            "We need to indicate if the candidate was generated with disk strategy or not, so this this fields are only binary features.",
            "We also used 11 sun distance between dimension and the entity for example.",
            "Then we have global.",
            "The global context features in which we calculate the similarity between the document.",
            "Then we need to annotate an each of the entities.",
            "So for us, in this case we will calculate the similarity between the Wikipedia content, the text and we have found in the in the Wikipedia page and the.",
            "The document that we need to annotate or then we use for for training and we have some features related to the popularity of the entity and so how many uncommon links they have in Wikipedia and the number of visits."
        ],
        [
            "So regarding the eat features, uh, we define it, four of them.",
            "And once we have these vectors, we can calculate different similarities they feel when we call it the average local content similarity.",
            "So we just calculate the cosine similarity between the entity and each of the local context, the paragraph or the sentence.",
            "Then we went to annotate and with the device I just calculate the average of this value.",
            "The other one is.",
            "The similarity with the average local Contacts.",
            "So first we calculate the average content and then we.",
            "Calculated similarity using the cosines.",
            "And similarly so.",
            "Also we use a feature, then try to identify the more salient words for this paragraph.",
            "So we calculate the most similar key K words to this contest.",
            "For the old work we define.",
            "T = 2 two 3.",
            "Oh can you mention similarity?",
            "We just calculate the similarity between dimension and entity, so the stream then we need to annotate and entity."
        ],
        [
            "So they do represent each candidate, so I will give you a really splendid idea with this sample.",
            "So again we need to annotate the words in in involved.",
            "So first we need you need eight candidates for Nepal.",
            "For example, we have these three candidates and now.",
            "Then we have the candidates we will for each of the candidate we will build better than represent the candidate.",
            "The first part is the generation features, the second one, the global development popularity and at the end we will Add all its features.",
            "For example here we are trying to calculate the theater features for the world quake and each of the candidates.",
            "So once we have all these vectors, we apply a binary classifier to build a model.",
            "Then will allow us to annotate the entities."
        ],
        [
            "So this this part I will explain it now like the what we do in training time and when we are doing in testing time.",
            "So in training time with this example Nepal we generate the candidates again we represent it and like this is training.",
            "We know the entity, the correct entity.",
            "So we'll put one.",
            "For the like the label one for this entity's zeros for all the other entities.",
            "And but we can have many, many of them.",
            "So thousands of candidates.",
            "So we will choose only 10 randomly.",
            "Selected and candidates from the negative examples and we will use these 11 elements to build our own model.",
            "So we will do exactly the same procedure for all the difference.",
            "Jim mentions to annotate.",
            "So at the end where we have been every classifier than is learning with this with big matrix than half.",
            "All represented all the training elements."
        ],
        [
            "Now we are in the in the test time on prediction, so for risk query will generate again all the candidates don't.",
            "For example we have Bush here with generate all the candidates we represent all them and this time we will not discuss the candidates.",
            "We will use all them too.",
            "Will credit the label for all them using our previous learning model.",
            "So now that many of these entities could be no.",
            "Label it lie positive examples.",
            "So what we will do is is there are many of them will choose the one that have the highest prediction.",
            "Is core like the correct entity?",
            "In the case in the particular case in which none of the candidates is markedly positive, will say this is an entity, but we don't know.",
            "We don't know this entity in the knowledge base, so this is considered like Anil solution like Kneeler entity.",
            "So for this case, if we have all these candidates, we generate all the values, the prediction scores and we take the Huggies one like the correct one."
        ],
        [
            "Donald Evolution is set up for this work.",
            "We use it at the tag EDL 2015 datasets.",
            "This dentist is composed for.",
            "168 documents, 12,000 queries and most of the queries have known entity in the knowledge base, so this is an example of the.",
            "Equities and then we have the information here at the stream.",
            "Then we went to annotate the document and it refers and this is the document that is our page.",
            "OK, so training and test is more or less similar.",
            "The documents are the size of the collection as well as similar and here we will try to predict the right entity for all these questions.",
            "So the evaluation metrics we have four to compare to compare with the previous.",
            "The participants.",
            "For this challenge we use this this metric.",
            "This is the one of the official metrics using for this challenge, so they will wait the precision and recall for the Neil entities.",
            "They've already precision and recall for the link entities, the one that we know then exists in the knowledge base and they calculate their precision for all them."
        ],
        [
            "So these are or results, so not here.",
            "Then we have our baseline.",
            "The baseline is considering a strong baseline because fairly approximate the top performance in this in this challenge.",
            "So if we use the only eat one feature, we improve the the baseline, but.",
            "Just a little bit, and then we compare.",
            "Also what happens if we include only the second IT feature and we improve it again and the same situation is for the running for the four one.",
            "So all they eat an individual.",
            "It features allow us to improve the baseline.",
            "And what happens if we combine all them an use it like like just one better so we improve also the baseline and all the individual it features.",
            "So how?",
            "OK, so we check it an area every metric was.",
            "We were improving the baseline so.",
            "We managed to get systems and improves all baseline and their uses by line and also we managed to improve the state of the art of these for this challenge."
        ],
        [
            "So here we try to analyze the.",
            "The impact of our classifier.",
            "So for all four classification, we use Adaboost like the binary classifier, Adaboost, San.",
            "We just perform a. Oh cross validation to find the better, the best parameters in the in the training set.",
            "So did they measure and count the Neil and non nil entities?",
            "This was the result and this is the result for the best participant on this this challenge and what happen if we use other classifier we we don't get so high resolves but we still keep in the top on the top of the list.",
            "So this is the second participant for this challenge.",
            "So remember or idea is we just built a better for this.",
            "Entity we use the classifier, so it means that we can just look on an put another classifier and we made the prediction.",
            "This is important because then we can just look and use state of the art algorithm for the classification and we will.",
            "We can maybe improve the results.",
            "So the best result we managed to get it with Adaboost."
        ],
        [
            "So now I'm just trying to to make the conclusion of this work done.",
            "The first one is they eat, model was capable to jointly represent the words and the entities in a unique space, and this help us to calculate similarities easily between the candidates and the other words.",
            "And we make it without loss of accuracy.",
            "We verified already then.",
            "The accuracy than you remind us to get in with the state of the art algorithms.",
            "Scheduling, for example, is not lose it because we introduce this IT units.",
            "We only use the contest to learn or or representation of each entity.",
            "So that means that if we did that people this very active people from deep learning, they developed a new technique.",
            "Will you just can easily plug or or or strategy?",
            "Indeed we implement this algorithm 20 is well known tools for do a two to calculate the word embeddings as GNC man.",
            "Hyper words and in both cases we have no problem to to do the implementation.",
            "We define it 4 different features to use or eat model an in which we consider the contents an they candidate information and the results show done in this data set and is our recent data set or individual features outperform the baseline and the combination of all the IT features.",
            "An hypothetic achieve the top, the top position."
        ],
        [
            "So I went to finish through with.",
            "This is like then aggregate all the work that we did in in this in this paper fears we propose a way to learn.",
            "It's from a Wikipedia to represent entities.",
            "Then we define similarities, then could be calculated with this and with these vectors.",
            "And we use it in a.",
            "We show how to train an how to predict?",
            "So.",
            "Pretty today, the links, an Indentity link task don't thank you for your attention.",
            "One quick questions, did you do you distribute to it model?",
            "For that you have confused I guess on the English Wikipedia and perhaps on different localization of Wikipedia.",
            "And did you compare what each model will provide depending on the localization of the of the Wikipedia?",
            "For their folder for the Come on empties.",
            "OK, I have no problem with the distribution of the model.",
            "OK, just write me an email.",
            "I will send you a link to download it, but this is where is quite big, so it's the only problem an even I want to provide the code so only then I'm just checking then they're not big mistakes in the programming Part 2 to be sure.",
            "Then it will be useful for everybody.",
            "And also we want to provide the candidate generation system because I firmly believe.",
            "Then using binary features for each entity that helps a lot for the classification is not only is, of course they need model because we showed and it's better than the baseline.",
            "But I firmly believe that these features in the generation are an important key for for these results.",
            "And the second question about the location.",
            "I just did it for English.",
            "Do you mean for French or the other language?",
            "I just did it for English, but of course we wanted to do it for all language but.",
            "Yeah.",
            "Activate the problem for us was we already did this model and was showing that was useful in a in an application problem and we choose entity linking because it look normal for us to apply in this.",
            "In this task the big problem was then implementing an entity linking system that works with a binary classifier.",
            "Is it to class sometime?",
            "But yes, I will try to do it from different language.",
            "Of course.",
            "Thank you very much.",
            "Impressive talk.",
            "I have a short question about how you apply the Skip gram model.",
            "So you did show that basically whenever you see an answer in a sense it's processed two ways.",
            "Either is a simple Warren or as an entity, and so the integer part is clear me.",
            "What happens if an entity is Multiword expression?",
            "Do you consider so you keep them as separate words?",
            "Or you could take him as a single talking.",
            "We use it like a single word.",
            "Indeed, this is what make the original implementation.",
            "If you put underscores and your words mean, then it's just one token in the original algorithm, so.",
            "OK, we put the underscores for indicate.",
            "Then it's just one entity and we are in considering.",
            "Any other question we have time for a couple of questions if we want no.",
            "OK. Let's go on with the the other speaker.",
            "Let's think again, you say."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today I will present this Old World combining work and entity embeddings for entity linking.",
                    "label": 0
                },
                {
                    "sent": "This is a collaborative work.",
                    "label": 0
                },
                {
                    "sent": "Between a three different labs in France, the I did Sarah and Lindsay.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say Moreno in the future.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first I will start with the definition of and the task for us and we want to do some entity linking here in this work.",
                    "label": 0
                },
                {
                    "sent": "So we define it like the task line consists in connecting an entity.",
                    "label": 1
                },
                {
                    "sent": "Mention two are already known entity in a knowledge base and one important thing is then we assume then somebody else have already identified the.",
                    "label": 1
                },
                {
                    "sent": "Stream then we need to connect to the knowledge base.",
                    "label": 0
                },
                {
                    "sent": "Don't please consider this sample in which the words in bold are being already identified and serve the world and we need to connect to knowledge base.",
                    "label": 1
                },
                {
                    "sent": "So the solution for this example will be the given entities for each of these mentions.",
                    "label": 0
                },
                {
                    "sent": "So I'm using here the info box of the Wikipedia page.",
                    "label": 0
                },
                {
                    "sent": "If we consider them Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "Is all knowledge base and to indicate the solution.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this task is also called entity disambiguation, because we can.",
                    "label": 0
                },
                {
                    "sent": "Yeah, apply a simple keyword search engine.",
                    "label": 1
                },
                {
                    "sent": "Over these words, then we need to link and then we will retrieve some candidates and then when we need to do is disambiguate.",
                    "label": 0
                },
                {
                    "sent": "One or decide which of these candidates is the correct one.",
                    "label": 1
                },
                {
                    "sent": "So this is a challenging task because as more stream camera to many different entities.",
                    "label": 0
                },
                {
                    "sent": "For example here we read.",
                    "label": 0
                },
                {
                    "sent": "We found a 1700 different entities for the World Bank.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the outline of this presentation I will just introduce a little bit more work then I will explain how people move from front to entity embeddings an then will I will introduce or model.",
                    "label": 1
                },
                {
                    "sent": "And I will show you how our model had been integrated in entity linking system.",
                    "label": 0
                },
                {
                    "sent": "That results and.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclusion.",
                    "label": 0
                },
                {
                    "sent": "Don't edit the preliminaries.",
                    "label": 0
                },
                {
                    "sent": "We want to say that the Entity link task than the sun is int array.",
                    "label": 0
                },
                {
                    "sent": "We want to address in this work is the entity linking task in which we have annotated corpus such as Wikipedia.",
                    "label": 1
                },
                {
                    "sent": "We have, we know one knowledge based and could be Wikipedia itself or DB pedia or any other knowledge base.",
                    "label": 0
                },
                {
                    "sent": "But we need to know the mapping between the knowledge base and the Wikipedia and we have annotated documents.",
                    "label": 0
                },
                {
                    "sent": "These documents could be.",
                    "label": 0
                },
                {
                    "sent": "A patent.",
                    "label": 0
                },
                {
                    "sent": "A scientific article, but we need to have this annotated documents and it's are the target documents.",
                    "label": 0
                },
                {
                    "sent": "Then we went to annotate, and in this case, or a document is a web page or for our work is only a web page and when I try it when I.",
                    "label": 0
                },
                {
                    "sent": "Use them, they work wedding, I mean mentioned the the scene with the opposite in a document.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I would bet.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we can use to solve this problem?",
                    "label": 0
                },
                {
                    "sent": "We have two types of the information.",
                    "label": 0
                },
                {
                    "sent": "When is the information that came from the input.",
                    "label": 0
                },
                {
                    "sent": "Or the quitting?",
                    "label": 0
                },
                {
                    "sent": "And then the information that is in the dotted, the information that is in the knowledge base.",
                    "label": 1
                },
                {
                    "sent": "So many words has already tried to solve this problem using this information in a separated features.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is combine these two features.",
                    "label": 0
                },
                {
                    "sent": "Combine these two seem formation in just one and to do or or objective is or on.",
                    "label": 0
                },
                {
                    "sent": "Alright, but this is then the the combination of these both sources.",
                    "label": 0
                },
                {
                    "sent": "We deal with a with a better description to disambiguate the entity in the entities.",
                    "label": 0
                },
                {
                    "sent": "Then we trip.",
                    "label": 0
                },
                {
                    "sent": "So at the end we want to do is calculate similarities between information from the input and information from the output.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how people move from word to entity embeddings?",
                    "label": 0
                },
                {
                    "sent": "And this is a.",
                    "label": 0
                },
                {
                    "sent": "The better beginning the disease stories with this simple idea you shall know award by the company that it keeps and I will give you this one example on how Skip Gram model works.",
                    "label": 0
                },
                {
                    "sent": "You take a window and when you want to.",
                    "label": 0
                },
                {
                    "sent": "Apply this algorithm and you will take the middle word like a input and we will try to predict all the context, word the water underline with green here.",
                    "label": 0
                },
                {
                    "sent": "So Judah, this key brand model will learn some weights to do this task, and these weights are used for represent the information and then you have.",
                    "label": 0
                },
                {
                    "sent": "This is a completely unsupervised algorithm and the idea is that you will optimize.",
                    "label": 1
                },
                {
                    "sent": "You will find these two vectors.",
                    "label": 1
                },
                {
                    "sent": "For the input and the output words and, you just define the conditional probability using this formula.",
                    "label": 0
                },
                {
                    "sent": "So people have already thinking then you shall know an entity by the company that he keeps, because you just translate the idea from word 20s and there are I will just focus in three words, previous work, in which they use this idea.",
                    "label": 0
                },
                {
                    "sent": "And they say, OK, we can use our knowledge base an represent each of the elements in the knowledge base like betters.",
                    "label": 1
                },
                {
                    "sent": "So this is a transition work and they managed to get good results, but they only represent.",
                    "label": 0
                },
                {
                    "sent": "Entities or relations.",
                    "label": 0
                },
                {
                    "sent": "Another word that I have syncing then how to combine this detects and the entities and they use for example the idea of escape gram.",
                    "label": 0
                },
                {
                    "sent": "They have an optimization function in which they include.",
                    "label": 0
                },
                {
                    "sent": "The screen is keep gramedia the trans E idea and they happen and alignment.",
                    "label": 0
                },
                {
                    "sent": "And alignment is step in which they try to just put together a.",
                    "label": 0
                },
                {
                    "sent": "Word an entity 10 correlate.",
                    "label": 0
                },
                {
                    "sent": "There are also other words in which they just use escape gram for example, and based on Wikipedia they use the full page of Wikipedia and then they duplicated this information using only the entities.",
                    "label": 0
                },
                {
                    "sent": "So you have a double size web Wikipedia page and with you apply directly the Skip gram model and you manage to learn these two.",
                    "label": 0
                },
                {
                    "sent": "These two different representations.",
                    "label": 0
                },
                {
                    "sent": "So these two works allow you to calculate similarities between words.",
                    "label": 0
                },
                {
                    "sent": "On the entities.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So order order idea is inspired in this.",
                    "label": 0
                },
                {
                    "sent": "In this previous work of course, but what we wanted to do is to directly learn the representations based on the.",
                    "label": 1
                },
                {
                    "sent": "The contents of the entity because the previous word doesn't doesn't have used directly the contest, they just.",
                    "label": 0
                },
                {
                    "sent": "Use the context of entities for entities, for example, and what we do is we just define a new unit and we call it the IT unit.",
                    "label": 0
                },
                {
                    "sent": "This unit it could be represented for an entity an award.",
                    "label": 0
                },
                {
                    "sent": "And each time then we need to process a sentence in which we find it unit.",
                    "label": 0
                },
                {
                    "sent": "We just apply this formula instantly.",
                    "label": 0
                },
                {
                    "sent": "The original skip gram formula.",
                    "label": 0
                },
                {
                    "sent": "So what we are doing here is only we are expanding.",
                    "label": 0
                },
                {
                    "sent": "So when we find out it's unit, we just expand the window, keeping the same contest for the entity and for the world.",
                    "label": 0
                },
                {
                    "sent": "So we will do all this process for the all the heat units.",
                    "label": 0
                },
                {
                    "sent": "Then we can find in a in a in a document.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will just give a graphical explanation of this idea.",
                    "label": 0
                },
                {
                    "sent": "So for example, here we are processing this window and escape grand model.",
                    "label": 0
                },
                {
                    "sent": "We use these words like input and the green ones like output.",
                    "label": 0
                },
                {
                    "sent": "So like we have here it.",
                    "label": 0
                },
                {
                    "sent": "Unit we need to expand this this window so the 1st and we are.",
                    "label": 0
                },
                {
                    "sent": "We will expand this window using only the world and the second time we will expand it using the entity.",
                    "label": 0
                },
                {
                    "sent": "We use an identifier for the entity.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Simple idea law us to represent this warning pal and this.",
                    "label": 0
                },
                {
                    "sent": "Identifier for the the entity nipple in the same space using the same context, but the intuition is that maybe you will get.",
                    "label": 0
                },
                {
                    "sent": "Duplicated better design, better for new balances and better for the Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "They the entity name part.",
                    "label": 0
                },
                {
                    "sent": "But it's not true because the the Warner Pal is associated in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "Two different entities and also the the entity Nepal is associated.",
                    "label": 0
                },
                {
                    "sent": "We different words to use different words are used to associate to this entity.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the main advantage of our model is then you can apply the Skip gram model, but also the continued back before model.",
                    "label": 0
                },
                {
                    "sent": "The other advantage is that we use directly the anchor text information that is available in Wikipedia and.",
                    "label": 0
                },
                {
                    "sent": "And then in and then this this representation for the entities are learning using directly the contest and not we're not doing some catenation or alignment.",
                    "label": 0
                },
                {
                    "sent": "Additional steps.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now that we have a model that allows us to calculate these similarities, we want to integrate it in an entity linking and system.",
                    "label": 0
                },
                {
                    "sent": "So this is this figure at the right show you the different steps and we have to implemented entity linking system and we concentrate in these two steps.",
                    "label": 0
                },
                {
                    "sent": "The query expansion and the candidate generation and also the candidate ranking where we are using our it features.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So fierce to achieve a good I strong baseline we define.",
                    "label": 0
                },
                {
                    "sent": "And some groups of features to represent our entities that feeds group is the generation group in which we also use each of these strategies to generate over candidates.",
                    "label": 0
                },
                {
                    "sent": "Tom, for example, we check if the lexical form of the entity is equal to the Entity label.",
                    "label": 0
                },
                {
                    "sent": "So this is a generation.",
                    "label": 0
                },
                {
                    "sent": "This will allow us to generate entities candidates, but also we will use it to indicate if the candidate will use it like a feature.",
                    "label": 0
                },
                {
                    "sent": "We need to indicate if the candidate was generated with disk strategy or not, so this this fields are only binary features.",
                    "label": 0
                },
                {
                    "sent": "We also used 11 sun distance between dimension and the entity for example.",
                    "label": 0
                },
                {
                    "sent": "Then we have global.",
                    "label": 0
                },
                {
                    "sent": "The global context features in which we calculate the similarity between the document.",
                    "label": 0
                },
                {
                    "sent": "Then we need to annotate an each of the entities.",
                    "label": 0
                },
                {
                    "sent": "So for us, in this case we will calculate the similarity between the Wikipedia content, the text and we have found in the in the Wikipedia page and the.",
                    "label": 0
                },
                {
                    "sent": "The document that we need to annotate or then we use for for training and we have some features related to the popularity of the entity and so how many uncommon links they have in Wikipedia and the number of visits.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So regarding the eat features, uh, we define it, four of them.",
                    "label": 1
                },
                {
                    "sent": "And once we have these vectors, we can calculate different similarities they feel when we call it the average local content similarity.",
                    "label": 0
                },
                {
                    "sent": "So we just calculate the cosine similarity between the entity and each of the local context, the paragraph or the sentence.",
                    "label": 0
                },
                {
                    "sent": "Then we went to annotate and with the device I just calculate the average of this value.",
                    "label": 0
                },
                {
                    "sent": "The other one is.",
                    "label": 0
                },
                {
                    "sent": "The similarity with the average local Contacts.",
                    "label": 0
                },
                {
                    "sent": "So first we calculate the average content and then we.",
                    "label": 0
                },
                {
                    "sent": "Calculated similarity using the cosines.",
                    "label": 0
                },
                {
                    "sent": "And similarly so.",
                    "label": 0
                },
                {
                    "sent": "Also we use a feature, then try to identify the more salient words for this paragraph.",
                    "label": 0
                },
                {
                    "sent": "So we calculate the most similar key K words to this contest.",
                    "label": 0
                },
                {
                    "sent": "For the old work we define.",
                    "label": 0
                },
                {
                    "sent": "T = 2 two 3.",
                    "label": 0
                },
                {
                    "sent": "Oh can you mention similarity?",
                    "label": 1
                },
                {
                    "sent": "We just calculate the similarity between dimension and entity, so the stream then we need to annotate and entity.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So they do represent each candidate, so I will give you a really splendid idea with this sample.",
                    "label": 0
                },
                {
                    "sent": "So again we need to annotate the words in in involved.",
                    "label": 0
                },
                {
                    "sent": "So first we need you need eight candidates for Nepal.",
                    "label": 0
                },
                {
                    "sent": "For example, we have these three candidates and now.",
                    "label": 0
                },
                {
                    "sent": "Then we have the candidates we will for each of the candidate we will build better than represent the candidate.",
                    "label": 0
                },
                {
                    "sent": "The first part is the generation features, the second one, the global development popularity and at the end we will Add all its features.",
                    "label": 0
                },
                {
                    "sent": "For example here we are trying to calculate the theater features for the world quake and each of the candidates.",
                    "label": 0
                },
                {
                    "sent": "So once we have all these vectors, we apply a binary classifier to build a model.",
                    "label": 0
                },
                {
                    "sent": "Then will allow us to annotate the entities.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this this part I will explain it now like the what we do in training time and when we are doing in testing time.",
                    "label": 0
                },
                {
                    "sent": "So in training time with this example Nepal we generate the candidates again we represent it and like this is training.",
                    "label": 0
                },
                {
                    "sent": "We know the entity, the correct entity.",
                    "label": 0
                },
                {
                    "sent": "So we'll put one.",
                    "label": 0
                },
                {
                    "sent": "For the like the label one for this entity's zeros for all the other entities.",
                    "label": 0
                },
                {
                    "sent": "And but we can have many, many of them.",
                    "label": 0
                },
                {
                    "sent": "So thousands of candidates.",
                    "label": 0
                },
                {
                    "sent": "So we will choose only 10 randomly.",
                    "label": 0
                },
                {
                    "sent": "Selected and candidates from the negative examples and we will use these 11 elements to build our own model.",
                    "label": 0
                },
                {
                    "sent": "So we will do exactly the same procedure for all the difference.",
                    "label": 0
                },
                {
                    "sent": "Jim mentions to annotate.",
                    "label": 0
                },
                {
                    "sent": "So at the end where we have been every classifier than is learning with this with big matrix than half.",
                    "label": 0
                },
                {
                    "sent": "All represented all the training elements.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we are in the in the test time on prediction, so for risk query will generate again all the candidates don't.",
                    "label": 0
                },
                {
                    "sent": "For example we have Bush here with generate all the candidates we represent all them and this time we will not discuss the candidates.",
                    "label": 0
                },
                {
                    "sent": "We will use all them too.",
                    "label": 0
                },
                {
                    "sent": "Will credit the label for all them using our previous learning model.",
                    "label": 0
                },
                {
                    "sent": "So now that many of these entities could be no.",
                    "label": 0
                },
                {
                    "sent": "Label it lie positive examples.",
                    "label": 0
                },
                {
                    "sent": "So what we will do is is there are many of them will choose the one that have the highest prediction.",
                    "label": 0
                },
                {
                    "sent": "Is core like the correct entity?",
                    "label": 0
                },
                {
                    "sent": "In the case in the particular case in which none of the candidates is markedly positive, will say this is an entity, but we don't know.",
                    "label": 0
                },
                {
                    "sent": "We don't know this entity in the knowledge base, so this is considered like Anil solution like Kneeler entity.",
                    "label": 0
                },
                {
                    "sent": "So for this case, if we have all these candidates, we generate all the values, the prediction scores and we take the Huggies one like the correct one.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Donald Evolution is set up for this work.",
                    "label": 0
                },
                {
                    "sent": "We use it at the tag EDL 2015 datasets.",
                    "label": 0
                },
                {
                    "sent": "This dentist is composed for.",
                    "label": 0
                },
                {
                    "sent": "168 documents, 12,000 queries and most of the queries have known entity in the knowledge base, so this is an example of the.",
                    "label": 0
                },
                {
                    "sent": "Equities and then we have the information here at the stream.",
                    "label": 0
                },
                {
                    "sent": "Then we went to annotate the document and it refers and this is the document that is our page.",
                    "label": 0
                },
                {
                    "sent": "OK, so training and test is more or less similar.",
                    "label": 0
                },
                {
                    "sent": "The documents are the size of the collection as well as similar and here we will try to predict the right entity for all these questions.",
                    "label": 0
                },
                {
                    "sent": "So the evaluation metrics we have four to compare to compare with the previous.",
                    "label": 0
                },
                {
                    "sent": "The participants.",
                    "label": 0
                },
                {
                    "sent": "For this challenge we use this this metric.",
                    "label": 0
                },
                {
                    "sent": "This is the one of the official metrics using for this challenge, so they will wait the precision and recall for the Neil entities.",
                    "label": 0
                },
                {
                    "sent": "They've already precision and recall for the link entities, the one that we know then exists in the knowledge base and they calculate their precision for all them.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are or results, so not here.",
                    "label": 0
                },
                {
                    "sent": "Then we have our baseline.",
                    "label": 0
                },
                {
                    "sent": "The baseline is considering a strong baseline because fairly approximate the top performance in this in this challenge.",
                    "label": 0
                },
                {
                    "sent": "So if we use the only eat one feature, we improve the the baseline, but.",
                    "label": 0
                },
                {
                    "sent": "Just a little bit, and then we compare.",
                    "label": 0
                },
                {
                    "sent": "Also what happens if we include only the second IT feature and we improve it again and the same situation is for the running for the four one.",
                    "label": 0
                },
                {
                    "sent": "So all they eat an individual.",
                    "label": 0
                },
                {
                    "sent": "It features allow us to improve the baseline.",
                    "label": 0
                },
                {
                    "sent": "And what happens if we combine all them an use it like like just one better so we improve also the baseline and all the individual it features.",
                    "label": 0
                },
                {
                    "sent": "So how?",
                    "label": 0
                },
                {
                    "sent": "OK, so we check it an area every metric was.",
                    "label": 0
                },
                {
                    "sent": "We were improving the baseline so.",
                    "label": 0
                },
                {
                    "sent": "We managed to get systems and improves all baseline and their uses by line and also we managed to improve the state of the art of these for this challenge.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we try to analyze the.",
                    "label": 0
                },
                {
                    "sent": "The impact of our classifier.",
                    "label": 0
                },
                {
                    "sent": "So for all four classification, we use Adaboost like the binary classifier, Adaboost, San.",
                    "label": 0
                },
                {
                    "sent": "We just perform a. Oh cross validation to find the better, the best parameters in the in the training set.",
                    "label": 0
                },
                {
                    "sent": "So did they measure and count the Neil and non nil entities?",
                    "label": 0
                },
                {
                    "sent": "This was the result and this is the result for the best participant on this this challenge and what happen if we use other classifier we we don't get so high resolves but we still keep in the top on the top of the list.",
                    "label": 0
                },
                {
                    "sent": "So this is the second participant for this challenge.",
                    "label": 0
                },
                {
                    "sent": "So remember or idea is we just built a better for this.",
                    "label": 0
                },
                {
                    "sent": "Entity we use the classifier, so it means that we can just look on an put another classifier and we made the prediction.",
                    "label": 0
                },
                {
                    "sent": "This is important because then we can just look and use state of the art algorithm for the classification and we will.",
                    "label": 0
                },
                {
                    "sent": "We can maybe improve the results.",
                    "label": 0
                },
                {
                    "sent": "So the best result we managed to get it with Adaboost.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm just trying to to make the conclusion of this work done.",
                    "label": 0
                },
                {
                    "sent": "The first one is they eat, model was capable to jointly represent the words and the entities in a unique space, and this help us to calculate similarities easily between the candidates and the other words.",
                    "label": 0
                },
                {
                    "sent": "And we make it without loss of accuracy.",
                    "label": 0
                },
                {
                    "sent": "We verified already then.",
                    "label": 0
                },
                {
                    "sent": "The accuracy than you remind us to get in with the state of the art algorithms.",
                    "label": 0
                },
                {
                    "sent": "Scheduling, for example, is not lose it because we introduce this IT units.",
                    "label": 0
                },
                {
                    "sent": "We only use the contest to learn or or representation of each entity.",
                    "label": 0
                },
                {
                    "sent": "So that means that if we did that people this very active people from deep learning, they developed a new technique.",
                    "label": 0
                },
                {
                    "sent": "Will you just can easily plug or or or strategy?",
                    "label": 0
                },
                {
                    "sent": "Indeed we implement this algorithm 20 is well known tools for do a two to calculate the word embeddings as GNC man.",
                    "label": 0
                },
                {
                    "sent": "Hyper words and in both cases we have no problem to to do the implementation.",
                    "label": 0
                },
                {
                    "sent": "We define it 4 different features to use or eat model an in which we consider the contents an they candidate information and the results show done in this data set and is our recent data set or individual features outperform the baseline and the combination of all the IT features.",
                    "label": 0
                },
                {
                    "sent": "An hypothetic achieve the top, the top position.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I went to finish through with.",
                    "label": 0
                },
                {
                    "sent": "This is like then aggregate all the work that we did in in this in this paper fears we propose a way to learn.",
                    "label": 0
                },
                {
                    "sent": "It's from a Wikipedia to represent entities.",
                    "label": 0
                },
                {
                    "sent": "Then we define similarities, then could be calculated with this and with these vectors.",
                    "label": 0
                },
                {
                    "sent": "And we use it in a.",
                    "label": 0
                },
                {
                    "sent": "We show how to train an how to predict?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Pretty today, the links, an Indentity link task don't thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "One quick questions, did you do you distribute to it model?",
                    "label": 0
                },
                {
                    "sent": "For that you have confused I guess on the English Wikipedia and perhaps on different localization of Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "And did you compare what each model will provide depending on the localization of the of the Wikipedia?",
                    "label": 0
                },
                {
                    "sent": "For their folder for the Come on empties.",
                    "label": 0
                },
                {
                    "sent": "OK, I have no problem with the distribution of the model.",
                    "label": 0
                },
                {
                    "sent": "OK, just write me an email.",
                    "label": 0
                },
                {
                    "sent": "I will send you a link to download it, but this is where is quite big, so it's the only problem an even I want to provide the code so only then I'm just checking then they're not big mistakes in the programming Part 2 to be sure.",
                    "label": 0
                },
                {
                    "sent": "Then it will be useful for everybody.",
                    "label": 0
                },
                {
                    "sent": "And also we want to provide the candidate generation system because I firmly believe.",
                    "label": 0
                },
                {
                    "sent": "Then using binary features for each entity that helps a lot for the classification is not only is, of course they need model because we showed and it's better than the baseline.",
                    "label": 0
                },
                {
                    "sent": "But I firmly believe that these features in the generation are an important key for for these results.",
                    "label": 0
                },
                {
                    "sent": "And the second question about the location.",
                    "label": 0
                },
                {
                    "sent": "I just did it for English.",
                    "label": 0
                },
                {
                    "sent": "Do you mean for French or the other language?",
                    "label": 0
                },
                {
                    "sent": "I just did it for English, but of course we wanted to do it for all language but.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Activate the problem for us was we already did this model and was showing that was useful in a in an application problem and we choose entity linking because it look normal for us to apply in this.",
                    "label": 0
                },
                {
                    "sent": "In this task the big problem was then implementing an entity linking system that works with a binary classifier.",
                    "label": 1
                },
                {
                    "sent": "Is it to class sometime?",
                    "label": 0
                },
                {
                    "sent": "But yes, I will try to do it from different language.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Impressive talk.",
                    "label": 0
                },
                {
                    "sent": "I have a short question about how you apply the Skip gram model.",
                    "label": 0
                },
                {
                    "sent": "So you did show that basically whenever you see an answer in a sense it's processed two ways.",
                    "label": 0
                },
                {
                    "sent": "Either is a simple Warren or as an entity, and so the integer part is clear me.",
                    "label": 0
                },
                {
                    "sent": "What happens if an entity is Multiword expression?",
                    "label": 0
                },
                {
                    "sent": "Do you consider so you keep them as separate words?",
                    "label": 0
                },
                {
                    "sent": "Or you could take him as a single talking.",
                    "label": 0
                },
                {
                    "sent": "We use it like a single word.",
                    "label": 0
                },
                {
                    "sent": "Indeed, this is what make the original implementation.",
                    "label": 0
                },
                {
                    "sent": "If you put underscores and your words mean, then it's just one token in the original algorithm, so.",
                    "label": 0
                },
                {
                    "sent": "OK, we put the underscores for indicate.",
                    "label": 0
                },
                {
                    "sent": "Then it's just one entity and we are in considering.",
                    "label": 0
                },
                {
                    "sent": "Any other question we have time for a couple of questions if we want no.",
                    "label": 0
                },
                {
                    "sent": "OK. Let's go on with the the other speaker.",
                    "label": 1
                },
                {
                    "sent": "Let's think again, you say.",
                    "label": 0
                }
            ]
        }
    }
}