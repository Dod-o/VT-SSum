{
    "id": "5tjl55oapcjvrrp5wefs2oj6f7p3472y",
    "title": "On-line learning algorithms: theory and practice",
    "info": {
        "author": [
            "Nicol\u00f2 Cesa-Bianchi, University of Milano - Bicocca"
        ],
        "published": "Dec. 14, 2007",
        "recorded": "October 2007",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/aop07_cesa_bianchi_onl/",
    "segmentation": [
        [
            "OK, so.",
            "Let's start with this.",
            "Mini tutorial on online learning.",
            "This is, I think, the photo school I teach this year.",
            "Sometimes it happens, you know there's a large deviations in the number of courses you give every year, and there was a peak this year, so some of you may have seen these things.",
            "Maybe in Tubingen are some of the places I so there's going to be some overlapping with what I said into being in a couple of months ago.",
            "Anyway."
        ],
        [
            "The plan this is the plan, which is maybe a little bit too ambitious, so I might skip some something and this is meant to be a very simple and basic introduction to themes.",
            "Certain teams in online learning.",
            "I will particularly will look at linear binary classification.",
            "Most of essentially all of online learning has to deal with deals with linear classification, and the the simplest instance of it is the binary problem.",
            "So that's why we will focus on it.",
            "Then I will say.",
            "A few a certain number of things on the perception algorithm.",
            "Which is the most basic example of online algorithm in math or binary classification?",
            "And then then I will move on to analyze a little bit in detail the case of.",
            "Linearly separable data sets and all the possible mistake bounds that you can prove for this specific sequence of sequences of data.",
            "And the further I will move on to discuss a bit relationship within online learning and convex optimization.",
            "And this is a theme that has been booming in the last two or three years.",
            "And I will say very few very simple things, so very basic.",
            "And that shows how few concepts few basic concepts from convex optimization can help in the analysis of online algorithms.",
            "Then I will time permitting, I will talk a bit about kernel based online learning, especially memory bounded online kernel based learning and possibly online SVM and active learning.",
            "Maybe not.",
            "I'll try to cover the last part which is the relation the way you can convert an analysis for for online algorithms to a classical statistical risk bound.",
            "For the.",
            "Statistical learning model, which is maybe more familiar."
        ],
        [
            "Too many of you.",
            "OK."
        ],
        [
            "Hey, let's start with linear classification.",
            "So the basic scenario I have in mind when I talk about online classification is this one you can think of a system for providing online classification of text documents, so you have a stream of unlabeled.",
            "The text that documents that are fed into classification system, and every time a new document comes in, the system guess is a label which will think of as a binary label.",
            "So for instance, is this document about?",
            "Politics.",
            "And the label is.",
            "Given to the user that possibly provides a feedback and the feedback is supposed to be the true, the ground truth for the semantics of the category of that document, and in the most general case, the label is actually something that the system asks explicitly to the user.",
            "So the most the most general scenario is an active learning scenario in which labels might be requested or not, but for the.",
            "Most part of this talk, we will think that the label is always provided by the user, so after each classification of a document that the user is providing the label to the true label, the ground truth to the system.",
            "So this might be plausible or implausible depending on a specific application that you have in mind."
        ],
        [
            "But I argue a bit about it.",
            "So when I talk about linear classifiers, you should all know that.",
            "I mean these things so we have for the online model we have a stream of data instances that is actually.",
            "This is our data.",
            "And these data instances are encoded as vectors in a real dimensional space.",
            "As usual for many applications.",
            "For most learning algorithms.",
            "And actually, for linear learning algorithms, let's say and there's a with each data instance, there's a binary label associated with it which tells the true classification of the document or that data item.",
            "So YT is going to be the true label of XT and.",
            "We will assume there is a learner maintains a linear classifier WT minus one which predicts the label of the current distance with by taking the sign on the sign of the inner product between the weight parameter and the current distance.",
            "OK, so that's the picture.",
            "And of course you know the you might know this notion of margin, which is can be associated with certain with confidence.",
            "Of classification of a certain X by given with parameter W and so the margin is just the distance between the distance of.",
            "Let the tip of X, the point X to the hyperplane that defines that separates the positive classification of W from the negative classification of W. So everything that lies in this half space where the determinant by the direction of W is going to be classified positive as positive by the binary classifiers associated with W, and everything that goes in the other half space is going to be classified as negative just by looking at.",
            "The sign of that inner product.",
            "So this is what we mean by by linear classifiers.",
            "So you see it's very simple because labels are binary, so there is a very simple decision, the decision space."
        ],
        [
            "It's just a hyperplane.",
            "And in the if we.",
            "If we look at online learning with linear classifier that now we look at this kind of."
        ],
        [
            "Protocol in which you have at each time step you have a data instance which is given to the online classifier.",
            "The online classifier predicts with the current weight vector.",
            "I'm using this indexing at T -- 1 for the current with parameter of the classifier to stress the fact that the classifier.",
            "Has a before the theater round starts.",
            "Comes up with some hypothesis with some linear hypothesis which does not depend on the on the on the next instance to be observed.",
            "So that's why I'm using this indexing and then after the prediction the true label is observed and then the learner learner as the chance to update the.",
            "The wait to come up with a new weight vector WT which will be used in the next."
        ],
        [
            "Next iteration.",
            "And so you can think of as in response.",
            "Of the sequence of examples being observed, the learner produces a corresponding sequence of classifiers.",
            "That will generate that will grow as more data from the stream are fed into the classifier, and when we we will look at conversions between online learning results to statistical learning results.",
            "We will specifically concentrate on this ensemble of classifiers generated by the online learning as iterating over the data.",
            "The in this case, but if you look simply at the the online problem, we don't want to make any specific and some assumption on the way this stream is generated so.",
            "The strength of the beauty of the results in online learning, that is that they do not depend on stochasticity of data generation system, so the source of data is completely arbitrary an we are facing every time we are facing a prediction problem on an individual stream of data.",
            "So without making making any assumption on the way this stream is generated and the results.",
            "The that we prove will depend on the specific properties of the of the individual stream of data that the learner is observing.",
            "OK, so that's why we cannot define a notion of risk, because there is no.",
            "There's no probability here, and the only quantity we can look at our the empirical quantities.",
            "That are the in case of binary classification.",
            "This is going to be the number of classification mistakes that the learner makes on any specific stream.",
            "So this is going to be the main measure of performance for the learner here.",
            "So we will look at the evolution of the number of mistakes made as the stream becomes longer and longer.",
            "OK."
        ],
        [
            "So a few remarks, so this models of online learning are natural on certain tasks.",
            "Especially, I mean if you think of a problem in which you always get the true label after every prediction, then you should be thinking of application like market or weather forecasting.",
            "In which you know after you know you predict tomorrow's weather and the day after you actually observe the what the weather is like.",
            "So this is our application in which you have a natural.",
            "And a natural nature is giving you the label after each prediction.",
            "In otherwise, you might also think of.",
            "In cases in which you want to actually solve a batch problem, but for instance you have a huge amount of data, and so to avoid.",
            "Time and space complexity issues that other learning algorithms might incur.",
            "You might use an online learning algorithm that is usually very efficient and very scalable on the size of data and you can.",
            "You can just cycle the online online learning algorithm on the data set and then to use some aggregation criteria not to.",
            "To combine the sequence of hypothesis that has been generated by the algorithm as cycling over this data set.",
            "So the problem itself doesn't have to be strictly online for to have a good motivation to use this algorithm.",
            "And so I said that this usually these are very easy, really easy to code, so there's just few lines of code and they and they have good scaling properties.",
            "Most of them they can be implemented with kernels, so you can run them in a reproducing kernel Hilbert space.",
            "We would maybe mention a little bit about it and they have a strong performance guarantees as I said because they do not need the stochastic.",
            "Data generation model to be analyzed in the.",
            "On the other hand, you can derive a stochastic risk bounds using those online to batch conversions.",
            "I'm going to mention at the end of the talk and you have active natural active learning variants whenever the true label comes at the cost, you don't have it for free, but you have to explicitly ask for it.",
            "And last but not least, you can actually easily turn.",
            "In most cases you can turn.",
            "Usual mistake bounds.",
            "So the analysis for an online algorithms into an analysis that takes into account the fact that the data might not be well explained by single linear classifier.",
            "So I won't mention this, but this is interesting, so this is the case in which, for instance there is again the stream of data is such that there is very very good linear fit for the four of the first part, then some other.",
            "Good linear feet for the central part and then as yet another different linear feet for the third part.",
            "An online algorithms can be.",
            "Modified in such a way that they are able to detect this change points in the best linear feet of the stream that is facing without actually knowing how many change points are there and where they are situated into the stream.",
            "And of course, the bound that you are able to prove in this case is will depend on the degree of nonstationarity of the data, so this is an interesting, interesting path, but I don't have time to."
        ],
        [
            "Cover it."
        ],
        [
            "OK, so if you let's start with the perception of the perception is the simplest online learning algorithm.",
            "It comes from the 50s was introduced by Rosenblatt to study the visual abilities that visual recognition abilities of the brain.",
            "But it's a very simple linear model, so you have an initial empty weight, which is the zero vector and the app to the update is just like that.",
            "So you remember any linear online linear learn."
        ],
        [
            "The algorithm is just characterized by the by this step here, everything else is fixed that this is the only place in which you can.",
            "We can say something we can.",
            "Show how the algorithm what is the action of the algorithm on the stream."
        ],
        [
            "OK, and the update rule for the perception is a is that easy whenever and no mistake is made on the current instance, then the weights remain.",
            "The vector remains the same, otherwise we add the.",
            "Current distance multiplied by the label, so that's the very simple."
        ],
        [
            "Update rules, so now a little bit of intuition because this is should be beneficial for learning.",
            "So the idea is that.",
            "Clearly you're going to make a mistake if you use a linear classifier whenever your margin, which is that quantity over there is negative.",
            "So whenever the inner product between the current weight and distance doesn't have the same sign as the true label, of course in that case you're going to make a mistake.",
            "So if you made a mistake on the current example, then you want to imagine was negative, and you want to increase it to make it positive so that you won't incur any mistake if the same.",
            "Element comes up again in the stream, so now you can look at the effect of the perception update on the margin and you can easily see that the if the the margin of the updated weight is measured on the same example that caused the mistake on the previous with vector, then you might observe that the margin.",
            "Is the old margin plus the squared norm of distance so the margin has increased by the squared norm of distance?",
            "Of the current distance, this might be enough or not to fix the mistake, so maybe enough or not for the this guy to be corrected on the same instance that caused the mistake on this guy, but.",
            "This might not be sufficient because imagine was so small that even if we increase it by that it remains negative.",
            "So for this reason that perceptual update is called non corrective because it increases the margin by a fixed amount.",
            "But we are not guaranteed that is going to correct the mistake.",
            "And even if it corrects the mistake, then it's also unclear that correcting a mistake is important because you don't know whether this example will come up again in the stream, or whether the stream the next part of the stream is related to the previous part.",
            "So in principle it's not clear whether these things is going to are going."
        ],
        [
            "Work.",
            "Sorry OK.",
            "So it's very special case of data sequences or streams.",
            "The leader is a parable sequences.",
            "So for instance, suppose that you have a finite sequence of data with my collected data set, and this is clearly separable, so there is some U that provides the correct classification in the sense that the margin is always strictly positive on any example of this data set.",
            "And notice that.",
            "And then in this case, the normal the length of this vector doesn't really matter, so you might just think that the zoo is unit norm.",
            "OK."
        ],
        [
            "So you might wonder first whether if we cycle, if we have the perception perception cycling over this little separable data set, then by this using this very simple and non corrective learning rule anything will happen.",
            "So that will find any such linear separator.",
            "If the perception whenever it's cycle my stream OK and what happens?",
            "Can you prove anything if you have actually?",
            "An infinite stream of data that is not linearly separable.",
            "So what can you prove about the behavior of the perception when it goes on into the stream so OK, can you say anything about learning in this case when there is no clear notion of convergence?",
            "OK, so these are the questions that."
        ],
        [
            "We're going to answer in very quickly, so the results in online early learning have usually this form.",
            "So this is called the relative loss bound and is a notion that is.",
            "It's a non statistical notion of risk bound when you compare the risk of a classifier with the risk of the best classifier in a given class.",
            "For instance, in the class of all linear classifiers this is the non statistical version of that bounds.",
            "An reminds of analogous bounds in competitive analysis of online online algorithms in computer science.",
            "So I'm using this notation M sub T of WT minus one to indicate the event that the W T -- 1 made made a mistake on the teeth.",
            "Example of a stream.",
            "So this is the indicator indicator function of a mistake for the for the element of the stream.",
            "That example of the stream.",
            "So in general we might want to compare.",
            "The number of mistakes made by a online learner, such the perceptron with the number of mistakes on the same stream made by the best linear classifier for that specific stream.",
            "So I'm taking the infamous overall linear classifiers you OK?",
            "So I'm comparing the these guys that change at every step with the best fixed linear fit for the same stream and I'm doing it on any finite prefix of the stream, so I'm stopping this some at some obitury point OK."
        ],
        [
            "So this this is an interesting goal, but unfortunately it's easy to see that.",
            "I mean, if you think about it and.",
            "There are a number of very nice papers that prove this informal way that computing and solving that batch optimization problem.",
            "So even if you're given the whole sequence of data in advance, computing the hyperplane computed the linear classifier that minimizes the number of misclassified examples on arbitrary sequence of examples is an NP hard problem.",
            "So even if you're given the whole sequence of data in advance.",
            "You cannot compute the optimal guy, so it's unlikely that it in.",
            "If you can efficiently.",
            "Compete with that best linear fit in an online way.",
            "When the data is revealed to you.",
            "1 by 1 one element after the other so."
        ],
        [
            "Make it feasible that this is this goal is relaxed, that by replacing the mistake indicator function in that in the right hand side with the different loss which is this one, this is called the hinge loss.",
            "An is a convex upper bound on the mistake indicator function.",
            "So in this graph here you have the margin.",
            "So this is the mistake indicator function so that you have a mistake.",
            "When the margin is negative, you have no mistake.",
            "When the margin is positive and this is the hinge loss.",
            "Which is zero whenever the margin is bigger than one, an grows linearly with the margin, as the margin is grows into the negative from 1.",
            "OK, and so the hinge loss wants to not only to have a positive margin, but once we have a margin bigger than one on each example.",
            "OK, so of course we are making the our competitor to pay more than us because we are using upper bound.",
            "But the fact that the upper bound is convex mixed the problem feasible so makes the problem of competing in an online fashion again against the best linear feet feasible problem for us.",
            "So now all bounds that I'm going to show will have this specific form here with maybe constants here and there.",
            "Any questions?"
        ],
        [
            "K. So now it's immediate to see that the norm of U of the best linear classifier that we're competing against the matters becausw the hinge loss cares about the scale of the managing.",
            "The classification error doesn't care about the scale at the length of you, because we're just looking for just looking at the sign plus or minus.",
            "But the hinge loss which make us pay whenever the margin is more than one.",
            "Looks at this game, so essentially.",
            "So this is for those of you who know supports vector support vector machines, so this is the.",
            "Exactly the same kind of constraint that we want to enforce on every example in a data set on which the support vector machine is applied.",
            "So if you look at for instance, in this case, we have linearly separable data set of examples.",
            "All of them have unit norm.",
            "Then for a linear classifier a linear classifier U that uses this hyperplane and to have zero hinge loss, you want to have that the projection of you on every example has length at least one.",
            "OK, so you have to make it you long enough so that this length will be will so that the zero loss will be minimized.",
            "The hinge loss will be minimized, so now.",
            "The normal view for hinge loss becomes a complexity measure of the hypothesis and natural complexity measure for the apophysis.",
            "So now you might think that there must be some kind of tradeoff between the length of you and the hinge.",
            "Cumulative hinge loss achieved by that you on the on a given data set, and this tradeoff will arise will be observed in the bounces.",
            "We're going to prove in a very short one."
        ],
        [
            "OK, so let's now look at the Knights of Perceptron, which is just a couple of slides very simple on any non separable stream.",
            "So we do the general case and then we look at the specific case for the separable streams.",
            "So we now looking at any sequence, any sequence of examples.",
            "And we compete against an arbitrary linear classifier, fixed U, and for simplicity we assume that all examples have unit norm, so they lie on the atmosphere.",
            "And then we want to prove something about the number of some we want to bound.",
            "Somehow the number of mistakes made by the perceptron on this stream as a function of the hinge loss of you where you is completely arbitrary, OK?",
            "And so there are two proof techniques to do that, and they look at the evolution of two quantities.",
            "One is the is this quantity here, which is the square that difference the squared distance between you and W. The current hypothesis of the perception and the other way is devolution of the inner product.",
            "You is just using analysis.",
            "So the perception is simultaneously competing against all linear classifiers, all fixed linear classifiers for that stream.",
            "So in particularly.",
            "Is competing with the best fixed linear classifier for this particular stream that is working on.",
            "So even the algorithm, even though the algorithm is very sick."
        ],
        [
            "So it has some unexpectedly strong properties.",
            "So here comes the proof.",
            "It's very, very easy, so we look at the evolution of this inner product between you and a beauty on a mistaken round.",
            "So we assume that WT minus one has made a mistake on YTXT and has been updated so we can expand the WT using the update rule.",
            "That's the.",
            "Let's actually quality, sorry, and then we can just.",
            "Go there, go to the second line and observe that we can just add and subtract 1 to that quantity and now we get this quantity over here in parenthesis and this is the definition.",
            "If we take the positive part of this quantity, we get the hinge loss of you.",
            "That's the positive part of this quantity.",
            "Here is by definition the hinge loss of you.",
            "So if you could take the positive path because we have a - we're going to make this quantity is smaller.",
            "So we have proven this line here.",
            "OK, so this is the the indicator of the fact that there was a mistake by the perception for that time step.",
            "And here we have the recurrence.",
            "So we get the the inner product before the updated inner product after the update.",
            "And here we have the hinge loss of the competitor."
        ],
        [
            "So we can expand this recurrence.",
            "And what we get when we iterate it, we get the inner product between the final weight a final, let's say after T after any number T of predictions is related to this, which is the initial weight which is 0 because the perception starts with O.",
            "This ones sum up to the number of mistakes and hear this quantity sum up to the cumulative hinge loss of the fixed the linear classifier.",
            "OK, so this is half of the story.",
            "Now you should notice that we have this quantity coming up here, which is the our quantity of interest, because this is just the number of mistakes made by the perception on the First Capital T element."
        ],
        [
            "So examples of the stream so we can do the other part so the other half we look at this.",
            "First we look at the squared norm of T on a mistake it round, so again we expand the WT using the update rule.",
            "Then we expand the square square over there and we just observe that cause WT made a mistake on that example.",
            "Then this must be negative.",
            "This is the margin, and because it was a mistake, the margin as to be negative.",
            "So we can drop these, get an upper bound and because we are assuming that these guys are have all unit norm, we get this relationship here.",
            "So again, we have a recurrence we can work."
        ],
        [
            "The recurrence an we get this OK where again these things sum up to the number of mistakes."
        ],
        [
            "And so we can combine using cash Schwarz inequality to recover.",
            "To relate to this with the quantity that we used to prove the other part.",
            "We can just solve the for this, which we happen to have in both side in both inequalities.",
            "This one in the previous one."
        ],
        [
            "We can solve for it and we can get this which is you can think of it as generalized perception convergence theorem.",
            "So this test tells you that the number of mistakes made by the perception on any arbitrary stream is bounded by the.",
            "Best tradeoff in from of the hinge loss of fixed the linear classifier U.",
            "Plus squared norm plus across term that involves both both things.",
            "So this is you see this is a."
        ],
        [
            "Has the interesting properties.",
            "It's an Oracle inequality because it basically without the knowing anything about this you is able.",
            "You're able to compare the number of mistakes with the best tradeoff between hinge loss and norm.",
            "OK, so for those who knows of you who know support vector machines, you know that if we drop this path here, this is actually the functional which is minimized by the support vector machine solution.",
            "So it shows that there is some kind of relationship even though you have this additional cross term here.",
            "And now you can get, of course the special case of separable streams.",
            "So if you cycle the perception of the linearly separable stream then that you have that.",
            "You then this term disappear.",
            "This term disappear because the hinge loss of the best guy you on a linear separable stream is going to be 0.",
            "And then you're left with the infimum U, the infamous overview of normal.",
            "You squared where you squared is any separator achieving 0 inch loss, so in particular you can take the support vector machine solution, which is the shortest linear separator achieving zero hinge loss.",
            "So basically shows that the number of mistakes made the best section on an English separable stream as is bounded by the squared norm of the support.",
            "Vector machine solution for the same stream.",
            "And another consequence is that.",
            "If you have a separable stream, there's always exist linear separator expressible as a linear combination of.",
            "At most these many supports this big cause the update."
        ],
        [
            "Rule of the perceptron.",
            "You can view it as adding a support, which is a mistake.",
            "An example every time a mistake is made.",
            "So you see you add this guy in the summer.",
            "So after mistakes this guy here will be a linear combination of M mistaken instances which you can view as a support."
        ],
        [
            "So that's why you can.",
            "Since the.",
            "Most this many mistakes you're going to make, you're going to have at most this many supports in your soul in your solution.",
            "So, so there's a number of an interesting property, so this is a very basic example of online mistake bound for the simplest banner classification algorithm.",
            "OK, so let me move on."
        ],
        [
            "Owner.",
            "Two, the special case of separable stream.",
            "I want to say."
        ],
        [
            "Something more about here and the first thing is OK, what?",
            "One thing that you could, you can be asking now is OK. We saw that the perception doesn't doesn't have a correcting app update, so the margin after every update the margin increases by a constant additive term which is the square.",
            "The norm of distance.",
            "So now what you might want to do is that you might want to modify the perception rules so that so to enforce a large margin, so you want to come up with a corrective update, and now you can wonder whether corrective update might be might work better.",
            "Then the perception update."
        ],
        [
            "So.",
            "So in particularly what you might define a corrective update as follows as an update that enforces a zero hinge loss.",
            "Zero hinge loss.",
            "Wait after the update.",
            "OK, so we want to be.",
            "We want to modify the perception to sort of make it more aggressive in responding to mistakes and see what happens in terms of the solution.",
            "So now you again, you made it, you made.",
            "You made a.",
            "Now you're looking at the hinge loss of the of the online learner.",
            "So whenever the online learner incurs some huge loss, then you make an update so that the updated weight will have no no hinge loss.",
            "On the same example."
        ],
        [
            "So this is a.",
            "More definitely, more aggressive update.",
            "So we now can try to analyze the properties of this different update here and this.",
            "These things this what I'm going to say in a moment is strongly related to an algorithm proposed by Hildreth in convex optimization in the 50s.",
            "That has.",
            "I mean, you can view this as a very special case of that general general algorithm by hildred."
        ],
        [
            "I'm not going to tell more about this relationships.",
            "That you start to see in learning and convex optimization.",
            "So now let's guess that the solution of this solution of the weight will have this form, which is just the perception update with the learning rate which are going to determine.",
            "So now we want to choose 80 such that the updated weight as margin big enough so we can just solve and find this solution here."
        ],
        [
            "So since we made an update this is this is positive, so we can just write the hinge loss here.",
            "This is actually equivalent, so the hinge loss again is the positive part of this.",
            "But since we made an update, since we make an update whenever this is positive then we are fine.",
            "So this is the learning rate that is used to enforce that condition of."
        ],
        [
            "Over there.",
            "And you can also view.",
            "This update here with this additive chosen in this way so that you enforce large hinge loss as a sort of a first order SVM.",
            "So basically this is the solution.",
            "The update this update here with this choice is the solution of this constrained optimization problem, in which you want to stay close to which you choose W, so to be close to the previous weight WT minus one.",
            "But satisfying this constraint also to have the.",
            "Aging large margin on the."
        ],
        [
            "Current example, so in other words, geometrically, you're just projecting the previous weight onto the hyperplane defined by this condition here.",
            "So onto the hyperplane on of weights that have margin exactly equal to 1 on that fixed example, white TXTYT.",
            "OK.",
            "So this is a very very easy modification.",
            "So just by introducing a learning rate you can enforce a large margin at each update, so it's a sort of again online SVM or first order SVM, because we're just looking at a single example."
        ],
        [
            "At the time.",
            "OK, now it's actually very easy to analyze this one.",
            "OK, and this is a nice proof.",
            "So now we have to recall that.",
            "The hinge loss on the theater example of WT0.",
            "By definition, because you've made a corrective update a corrective in the sense of the hinge loss."
        ],
        [
            "OK, so now you can write the hinge loss of of an example T of the previous guy as this.",
            "Since this is zero, you can write it like that.",
            "Then you can expand this by removing the positive part, so you're making it bigger and this is again 0, so you can keep writing it.",
            "Then you collect Whitey and then you get this expression here and then you use Koshy Schwartz to get these two.",
            "OK."
        ],
        [
            "Now.",
            "You can square both sides again.",
            "This relation here between the squared hinge loss.",
            "And this this product between the squared norms.",
            "OK."
        ],
        [
            "Now this is half of the story again.",
            "Now you get you again.",
            "You fix some linear separator you."
        ],
        [
            "You remember this is analysis for separable streams, be 'cause it's very simple in this case, so I'm not covering the analysis of this algorithm for no super, I'll do it later.",
            "Actually, I'll do it later, but for now I'm going to show a very simple analysis for this."
        ],
        [
            "Parable streams.",
            "OK, so there's going to be some.",
            "A separator linear separator U for the stream which has a zero hinge loss.",
            "And actually I can choose the short test and not the shorter, but the shortest such linear separator with zero hinge loss, which is the SVM solution so I can pick that one for instance.",
            "And by the law of cosines, since this since WT minus one and U leave on the opposite sides of the half space of this half space here."
        ],
        [
            "OK.",
            "So this is how space W T -- 1 and you live on opposite sides of this half past space.",
            "Becaused abate minus one makes a mistake on this example, whereas you doesn't because US."
        ],
        [
            "Separator then you can apply the law of cosine and get that inequality over there.",
            "That relates the squared difference between TT TT's minus minus one with the debt difference that involves you.",
            "OK, so now you can you have that."
        ],
        [
            "And you had the previous one.",
            "Which had the same term and you had the disk."
        ],
        [
            "Red hinge loss here.",
            "So you can sum up and use the telescoping property over there and what you get at the end there is this expression here.",
            "OK, now this thing.",
            "Here is the squared hinge loss, which is this red curve over here.",
            "This is clearly an upper bound on the number of mistakes, so this is squared.",
            "That doesn't make any sense, so there's no square here.",
            "This is just some.",
            "You can square indicator function will remain an indicator function.",
            "So this is just a sum of number of mistakes.",
            "This curve here and this is the upper bound that you get, which is exactly the what you had in the perception case for the separable stream analysis squared normal view squared norm of the largest XD.",
            "When I saw it when I showed you the analysis purpose for perception, this thing was one because the XD had norm one.",
            "But if XDR obitury then you have this term here.",
            "So this is the perception bound.",
            "But now we can prove a tighter relationship because this squared hinge loss is an upper bound on a mistake on the mistake indicator function.",
            "So for perception we had this.",
            "Smaller equipment that and now we have this thing here, which is an upper bound smaller liquid date, so it's a it's a stronger result.",
            "Is then perception convergence theorem.",
            "However, it doesn't really imply.",
            "Any convergence to the SVM solution?",
            "So again, we cannot, as in the perception case that we cannot guarantee that this algorithm or the perceptron algorithm will find a solution.",
            "Linear separator with large margin.",
            "OK, if you think about it, it is.",
            "It might seem that actually this shows that OK, you're minimizing the.",
            "So this is a constant, so this discriminative squared hinge loss is going to be bounded by a constant.",
            "OK, so it is this is going to converge to something.",
            "Yes, it's going to converge to something, but it's not going to converge to linear separated with with a large margin.",
            "Again, as in the perception case will find some linear separator OK. Good so.",
            "Now these these algorithms are.",
            "Ann, so this more aggressive updates are actually.",
            "Not some somehow disappointing because you cannot prove the candle analysis that you can make is almost is not much stronger than what you can prove for the for the perception case.",
            "But in practice in many practical examples aggressive updates will work well.",
            "We work better than a perception updates for for in terms of practical performance.",
            "OK, so this is anyway this is still a very special case because we are looking at separable streams and of as in the."
        ],
        [
            "In the support vector machine analysis.",
            "Just that, enforcing strictly enforcing a positive zero hinge loss on each example by bit too much if the stream is not.",
            "Actually linearly separable.",
            "It might in a real case it might be useful.",
            "It might be more beneficial to introduce some kind of slack, just like in the case of support vector machines too.",
            "Be partially corrected.",
            "So to introduce a sort of a tuning.",
            "Bing parameter to measure the aggressiveness of the update so that because you're not actually confident that the stream that you are trying to predict is linearly separable one.",
            "So we will look at this tunable updates.",
            "Tunable aggressive updates in a moment."
        ],
        [
            "And now we make a little detour.",
            "How much time do I have?",
            "Half an hour.",
            "Yes, I see someone on the."
        ],
        [
            "OK. OK, so now let me."
        ],
        [
            "Now we have seen bounce of, you know.",
            "In both perception and this, more aggressive updates in terms of number of mistakes made on a separable stream of the same bound.",
            "Now you can wonder, is this the best possible boundaries can prove in the case of a separable stream.",
            "So this is a very very basic and Chris question.",
            "You have a data set which is linearly separable.",
            "You want to find in an incremental way you want to find the linear separator.",
            "How many mistakes?",
            "How many updates are is going to take you to find the any linear separator with no specific.",
            "Condition on the margin."
        ],
        [
            "OK.",
            "So this is this is a question that it's you know perception as a solution.",
            "It takes you if all the examples in the data set have unit norm is going to take you squared normal view where you is the SVM solution for that data set.",
            "Is this the best possible bound in terms of number of mistakes?",
            "OK, so you can.",
            "You can relate this problem again at 2.",
            "Convex optimization constrained convex optimization, so this questions has been studied in convex optimization in the past 30 years.",
            "So you can use the solutions you can express.",
            "You can view optimization algorithm for constrained convex optimization problems as online linear online learning algorithms.",
            "And now I'm going to make this relationship relationship to show you how this more sophisticated learners from from convex optimization.",
            "What kind of properties have, if you just look at the number of mistakes they make in this problem of finding a consistent hyperplane?",
            "OK, so you have again an arbitrary.",
            "Legally separable data."
        ],
        [
            "At the unit norm."
        ],
        [
            "So you can symmetrize it, so you can basically multiply each point by its label so the negative."
        ],
        [
            "Points will be flipped to the positive side.",
            "The problem hasn't changed, but just have just all the points.",
            "Have positive you can."
        ],
        [
            "Of all the points of."
        ],
        [
            "In positive labels and now you can look at.",
            "In this case you can look at the two extreme points.",
            "These two extremes."
        ],
        [
            "Points.",
            "We'll define two hyperplanes.",
            "This one particular to this one and this one particular to that one.",
            "And then these two hyperplanes define a cone into dimension.",
            "Is just this in many dimension will be a political cone and any your linear separator, any vector that is a linear separator for this stream will have to lie within this cone, because in this case it will correctly classifier all the points that were that were limited by these two more most extreme ones.",
            "OK, so this is the cone of the consistent solutions.",
            "The cone of the solutions for this very simple, nearly separable data set.",
            "So now you're looking at this."
        ],
        [
            "Is an online problem, so you think you have some solutions on current solution.",
            "For the example you've seen."
        ],
        [
            "So far and now you'll see a new example.",
            "An example, for instance over here.",
            "So this is the hyperplane that is cutting through the cone and giving you a new constraint that the solution has to satisfy.",
            "So for instance, now this is the new consistent cone and you have to update the solution over here."
        ],
        [
            "OK, So what perception does is is basically it observes that the current solution as a negative margin with respect to the example so increases the margin by fixed amount.",
            "So it might correct in this case it corrects the solution and so it finds actually new solution in the new consistent code.",
            "But it's really it's really.",
            "It might be, it might be non corrective or may overshoot over there.",
            "I mean it's just an update.",
            "That increases the margin by a fixed amount."
        ],
        [
            "OK, the Hildred algorithm that we saw.",
            "Makes an update such that the margin.",
            "The margin on the with respect to the new example that was mistaken is 1, so this this is the hyperplane of the solutions that have margin at least one with respect to this guy here.",
            "And so the new solution of the hidden algorithm is just the projection of this onto this hyperplane here.",
            "And so in this case you see this guy overshoots."
        ],
        [
            "End.",
            "So now you can say OK, so let's assume that you have a sequence of examples that literally separable and Now suppose that capital U is the length of the norm of the support vectors."
        ],
        [
            "Lucian and now you can look at a bunch of algorithms that solve this problem and somehow have been developed.",
            "I mean just a perception and 2nd perception come from learning theory and these are algorithms that can be used to solve the same problem from convex optimization.",
            "So now you see that.",
            "These are the bounds that we proved for the perception and Hilbert algorithm.",
            "OK, and the number of mistakes they make before converging to the linear separator is.",
            "Squared normal of the support vector machine solution and the update time is linear in the number of dimensions because we have to make an update that is.",
            "The update of the perception and the passive aggressive is just you just add the current distance into the weight.",
            "So you just linear in the number of dimensions.",
            "So modification of perception which is called 2nd order perception, has a more complicated update which is quadratic and gets you a bound which now depends on D, which is the dimension of the space where the problem leave where all the data leave and it's only.",
            "It replaces factor of you with the log.",
            "More sophisticated algorithms are the ellipsoid, the volumetric center, and the geometric center.",
            "So the ellipsoid is a classical algorithm for linear programming an this is the number of updates it takes.",
            "And you have an increase in the number or in the complexity of the update each time and the the two best algorithms for with respect to the number of mistakes at the volumetric center and the geometric center of the.",
            "Of.",
            "Well, I'm not going to explain a lot about these ones because it's going to take too much time, so these are going to need actually to know the length of the solution so that this scalar U an have a higher complexity but achieve this.",
            "Bound here on the number of mistakes and.",
            "And this is.",
            "And this is optimal.",
            "Whenever you and D are fixed, so it's actually very easy to prove that you squared is an optimal bound for fixed U whenever D is is, a is free is a free parameter.",
            "But if you fix both U&D then DLNU is the optimal is the smallest number of miss of updates that you can make.",
            "We're finding a solution, so these are.",
            "Best possible algorithms for solving this problem, and of course I. I mean, I didn't tell you I didn't tell explicitly.",
            "This is a.",
            "This is just a linear programming problem because you have you have you have a bunch of a bunch of sorry.",
            "You have a bunch of linear constraints that you want to want to satisfy, so it's just a feasibility problem for the linear programming, and this is all the whole range of algorithms that you can use, so you can.",
            "You can see you can situate perception and this other corrective update in this whole range of Al."
        ],
        [
            "Rims and you have difference different costs in terms of update time that you're going to pay.",
            "OK. Now.",
            "How much time difference?"
        ],
        [
            "So.",
            "Now let me let me talk a bit more about relationships between online learning and convex optimization so."
        ],
        [
            "Uh.",
            "In in this form in this sense, so we want to see now whether we can extend this concept of aggressive update to the case on non separable streams.",
            "So basically we are taking the algorithm we saw before this Hildreth algorithm that makes an update that is completely corrective and we're going to introduce azzlack parameter that makes.",
            "That let us decide that the amount of aggressiveness that we're going to use to fix to.",
            "To enforce the constraint of large margin on the next example.",
            "So this is an algorithm that is called passive aggressive that in with respect to perceptible, has a tuneable parameter, which is always a nuisance.",
            "But in general it is if it has better performance than perception on practical problems, and it is easy to implement as perception and doesn't have any additional complexity.",
            "So this is interesting because the analysis of this algorithm will bring interesting connections with."
        ],
        [
            "An optimization.",
            "So the.",
            "OK, so let me explain this so now you remember this Hidrate algorithm was just the perception with this learning rate here and the learning rate was chosen in such a way that you had.",
            "Was chosen like this like this term here.",
            "So now you have a variant of this algorithm that has a tuneable parameter to measure the degree of aggressiveness.",
            "In the update is called passive passive aggressive.",
            "I agree and the learning rate is chosen in this way as a minimum between C, which is a parameter and the same quantity as before."
        ],
        [
            "Now, why what is the meaning of that?",
            "OK, you see now.",
            "This is this.",
            "You can view this as the solution of this.",
            "Constrained optimization problem in which, as before, this is again then one one step SVM online SVM.",
            "In which again, you have a tradeoff between the distance between the solution to the previous weight and the hinge loss of the of the update.",
            "But now you have introduced.",
            "You have introduced this this.",
            "Trade off term CK2.",
            "So basically you want to you want to decide how important is to be corrective to reduce this with respect to B.",
            "Close to the previous solution, so this is the bias term and this is the variance term.",
            "If you if you want to."
        ],
        [
            "Good them way.",
            "And you have a similar update."
        ],
        [
            "Is this if you have?",
            "If you put the squared hinge loss here, so this is like this is the analogon of the SVM with squared hinge loss.",
            "One step again.",
            "OK, so now you have do 2 updates here.",
            "You have a way of trading off aggressiveness, so now you can think of using this parameter here to be able to analyze the performance of this and that on arbitrary streams that are not necessarily separable.",
            "OK, so the presence of this parameter should help you to deal with the fact that you cannot strictly enforce a large margin on each example because the whole stream might not be clearly separable.",
            "So, so there's no point in enforcing this this merging.",
            "Insisting on having such a large margin on a no separable on non separable stream, so that's why you are introducing this term here.",
            "OK."
        ],
        [
            "Good so.",
            "Now the interest in doing this is that you can.",
            "Again you can related related to the analysis of support vector machines in the following ways so.",
            "This is a primal, objective function of support vector machines.",
            "So again in general, not necessarily the separable data set.",
            "The support vector linear classifier is the minimizer of the squared norm plus the sum of the slacks city, where this lacks the amount by which the solutions fail to achieve, fails to achieve the margin of one of each example.",
            "OK, so the SVM is in general is minimizing this functional here subject to these constraints?",
            "OK, so we call this."
        ],
        [
            "SVM, APTA, and you can.",
            "You can define the dual LaGrange function of this primal objective, which is something that gives rise to the well known quadratic optimization problem solved by any SVM package.",
            "So now you introduce a LaGrange multiplier for each linear constraint and now you have an expression function D that depends on this on these LaGrange multipliers and has this form.",
            "Ann, you have a constant box constraint on the LaGrange multipliers of this thing here.",
            "Now this is a function of of any choice of the T LaGrange multipliers for the constraints."
        ],
        [
            "K. And by duality, actually you can prove that.",
            "By weak duality, any choice of the LaGrange multipliers will be a lower bound on the value of the optimal value of the primal objective.",
            "And by strong duality, the best optimal choice of the LaGrange multipliers will.",
            "We let give a value of the dual LaGrange function which is equal to the value of the primal optimal.",
            "So solving the dual an optimal solution and the dual provides an optimal solution for the primer OK, but in general any solution for the dual is the lower bound for the solution of the primary, so we're going to use this dual here to analyze the passive aggressive."
        ],
        [
            "So let's look.",
            "It's actually very easy.",
            "So this is was into this kind of a nicer introduced by Schwartzenbur Amsinger last year.",
            "So now let's look at the SVM dual."
        ],
        [
            "Was that function over there?",
            "OK, if you set all of them to 0 then this is cool."
        ],
        [
            "0.",
            "Which is what I wrote there, and since that is zero, you can write it.",
            "You can write this as a telescoping difference.",
            "Trivially.",
            "Just look at that and you simplify and then you have that initial term is 0.",
            "OK, so that's equality."
        ],
        [
            "Is of that.",
            "So now you can look at these differences here.",
            "OK, you can just go there.",
            "You have the function.",
            "The dual the dual you have it, you have it.",
            "So you compute out this difference and you get this expression here.",
            "OK, where you use this notation to denote this thing here?",
            "OK, you just have to believe me, it's you.",
            "It's very easy to do it.",
            "OK so you have an expression for these differences of duals which."
        ],
        [
            "Why are you interested in it?",
            "It's not clear yet, but now let's look at this passive aggressive online algorithm that does this tunable aggressive update.",
            "So again, I told you that this update corresponds to solving this primal objective function subject to that constraint here.",
            "So I just rewrote the same thing introducing this select variables instead of the hinge loss.",
            "This is as this is equal and equivalent to writer wrote before using the hinge loss.",
            "But now I replace the hinge loss by select variables, same thing."
        ],
        [
            "So this is the online algorithm, so you can compute the do LaGrange function for this specific.",
            "Primal objective defined on a single example now, and you have this expression here but."
        ],
        [
            "This is what exactly what we wrote before, so now it turns out that the update, not surprisingly, that the learning rate we chose is the maximizer of this dual LaGrange function for the primal objective of passive aggressive.",
            "And so we can call it Alpha T because we recognize that this entity is the optimal choice for the LaGrange multiplier of this guy.",
            "Gay."
        ],
        [
            "However, another thing is that."
        ],
        [
            "This quantity for.",
            "Any choice of Alpha, so in particular also for Alpha T. This quantity here is equal to what we."
        ],
        [
            "Before for the.",
            "For the SVN so."
        ],
        [
            "If we take if you take any sequence of LaGrange multipliers A1 through Alpha T, you can write these qualities here for any sequence.",
            "So in particular also for the sequence chosen by the passive aggressive.",
            "Note that these are not the optimal LaGrange multipliers for the SVM solution, just obitury LaGrange multipliers.",
            "But again, I'm using the fact that this thing here for any choice of the LaGrange multipliers is a lower bound on the VMS solution on the value of the CL."
        ],
        [
            "Primal objective OK, so now you know that the sum of these things because of telescoping.",
            "The sum of these things?",
            "Is at most the value of the SVM objective primal objective that one so now you see this is something that reminds you of the upper bound.",
            "The perception upper bound.",
            "You have the same quantities squared normal view and the sum of the hinge losses of you.",
            "OK, and here you have this match."
        ],
        [
            "Is this some over there?",
            "So now you can use this to prove mistakes bound for passive aggressive.",
            "How do you do it?",
            "You basically you prove a lower bound improve that.",
            "This quantity.",
            "Has to increase by at most a certain amount every every time the algorithm makes a mistake.",
            "Since it cannot discount, it is upper bounded by this, which is constant.",
            "For any, any sequence of any given sequence of examples, you can use this to prove.",
            "An upper bound on the number of mistakes made by passive aggressive in terms of this.",
            "Quantity here and now.",
            "See is the same see parameter used by the passive aggressive update.",
            "OK."
        ],
        [
            "So in particular, you can again assume."
        ],
        [
            "For simplicity that instances of unit norm.",
            "You can prove that every time passive aggressive makes an update, the dual is going to increase by at least this match.",
            "OK."
        ],
        [
            "So now what you do now, you can prove that the indicator function of a mistake for passive aggressive at 20 multiplied by this.",
            "Is at most this thing over here 'cause?",
            "Anne.",
            "Because this is going to be at least one.",
            "OK, and this is going to be at least this is an upper bound on the mistake indicator function.",
            "And the claim over there showed that this is upper bounded.",
            "You see it here by twice 'cause we have the true do it twice the dual.",
            "OK, so now you have a relationship between this number of mistakes and the dual, but you know that they do the sum of the duals is upper bounded by the SVM at primal objective function."
        ],
        [
            "So what you ever the end is this bound over here, which is abandoned.",
            "The sum of the number of mistakes made by the passive aggressive in terms of the value of the.",
            "SVM function with.",
            "You have some additional factors here and you see now the role played by the trade off parameter C. OK so.",
            "Of course you don't know the value of this for any any sequence of examples, so it's not clear how you should tune C. But it I mean, and also this bound is not.",
            "Particularly, is not more always better than the perception bound, which is another disappointing fact, so it's not a very strong proof for the behavior of the passive aggressive, even though in practice for reasonable choices of C, this algorithm, passive aggressive performs better than perceptron in terms of the analysis, this doesn't reveal that this is this bound is actually not.",
            "It's many times it might be worse than the perception bound, but is interesting to show the fact that you used this.",
            "Duality in optimization relating the SVM solution with the with the dual dual function of the passive aggressive to prove it, and this is essentially a way this is a kind of route that you can follow to prove mistake bounds for many variants of algorithms, and this is also an easy way to prove mistake bounds for versions of.",
            "Algorithms such that the perception and the passive aggressive that are modified to predict structured outputs.",
            "So for the case in which you don't have a binary classification problems, but your labels are discrete, combinatorial objects like sequences or trees or more complicated combinatorial objects.",
            "So it's a versatile framework that can be used to prove.",
            "And related mistake bounds for online algorithms.",
            "And it is nice because it is suggestive suggestive in terms of relationships within the local optimization problems solved by the online algorithm that looks at one example of the time and the global optimization problem that is solved by batch algorithms like the support vector machine that looks at the whole set of examples at once.",
            "OK, so."
        ],
        [
            "This is to close this part OK and now in the remaining time, let's see.",
            "Maybe I will, I will talk about bit about this one, which is some some work I've done recently and then I will skip this last two parts, OK?"
        ],
        [
            "I still do like this.",
            "OK, I don't know whether you've seen kernels so far.",
            "Everything kernels in learning.",
            "Anything?",
            "Did anybody talk about cognition learning no."
        ],
        [
            "OK, so."
        ],
        [
            "This is not."
        ],
        [
            "So maybe it was not a good choice.",
            "This one if I had to introduce kernels in 10 minutes.",
            "So maybe let me let me move to the."
        ],
        [
            "Who knows kernels?",
            "It was discussed a bit, so you know a little bit of it.",
            "OK, so I."
        ],
        [
            "I can I can, maybe this is not so depending on knowing intimately what the kernel is, but it certainly helps.",
            "So let's say that the kernel is is a feature map that Maps the original space in which inputs are expressed in a complicated space.",
            "Complex, high dimensional space which is reproducing."
        ],
        [
            "Kernel Hilbert space and you know that the kernel is an object that implements the inner product between the images of distances in this.",
            "An complex feature space."
        ],
        [
            "K so so kernels are a way to represent complicated decision surfaces for, for instance, for binary classification problem.",
            "So decision services that are not necessarily hyperplane in the case of linear classification, but because of the kernel machinery, you can use a linear learning algorithm to learn this complicated surface because they can represent it as hyperplanes in the reproducing in the.",
            "Features based induced by the kernel in this reproducing kernel Hilbert space.",
            "So now we know that all these algorithms, the weights of all these algorithms are expressed as linear combination of supports just like support vector machines.",
            "But now the supports are just the elements of the stream where the algorithm is take."
        ],
        [
            "OK, so whenever you have a, you have your classifiers expressing in such way.",
            "Then you can learn the classifier in the reproducing kernel Hilbert space by using the kernel to compute predictions.",
            "So all you have to remember is the set of supports that make up your weight.",
            "So whenever you have to make, you have to make a prediction of of data X.",
            "You map implicitly map the data into the feature space with five.",
            "But the computing this prediction just amounts because of the kernel property, and because the way the way the weight is defined just amounts to compute the sum of labels times the kernels.",
            "OK, so all you have to remember is just the set of I that make up your weight W and you have to have an Oracle to compute your kernel function, which is a piece of code in practice.",
            "So this is a way in which.",
            "All these algorithms I've showed you can be implemented."
        ],
        [
            "In a kernel space, for instance, this is a perception.",
            "The."
        ],
        [
            "Etc you start with."
        ],
        [
            "The empty set to support examples you read next systems and then you predict just by taking all the supports you have in your list.",
            "Computing the inner product, the kernel using the kernel, you compute the inner product of these guys in the current guy that you have to predict."
        ],
        [
            "And you then have turn obtain a true label, and then if you make a mistake then you can store then you support which is the instance multiplied by the label in the list L and that's it.",
            "So basically this is implementation of the kernel of the perceptron.",
            "In the run in the inner reproducing kernel Hilbert space, reproducing kernel space generated by the kernel key, so you're now running.",
            "You're running the same algorithm, but in this very complex high dimensional feature space.",
            "So the only thing that changes and is that the algorithm Now the complexity of the algorithm, is now not expressed the in terms of the dimension of the original space.",
            "But this is expressed in terms of the size of this list.",
            "So you see now and update any evaluation, any prediction is going to cost you.",
            "The length of this list so and the length of the list will grow as you make more mistakes.",
            "So now you're the number of mistakes is ruling your the time complexity of your prediction and the space.",
            "Also, because we have to keep all these, you have to keep this list stored in memory."
        ],
        [
            "On the other hand, the mistakes bound mistake bounds will hold.",
            "We're not competing now with the best linear classifier, but you're now competing with the best function in the reproducing kernel Hilbert space.",
            "So all the all of your bounds are lifted.",
            "Two elements that live in the reproducing kernel Hilbert space computed by the kernel.",
            "So it's a huge power that you gain by this very simple change in the code OK."
        ],
        [
            "So now one interesting thing that you run in practical issue that you run into when you use kernel based learning with Prince with perceptions is that you know you run the algorithm over stream.",
            "You keep on making mistakes and your list is going to grow, grow, grow, grow, grow and especially if the data is very noisy even for the kernel that are used using then this list grows.",
            "And to grow an unlimited Lee.",
            "OK, so in practice what you do is that you're keeping a cache with fixed memory cache of size B for instance, so you are willing to spend at most time linear in B on each day update, so that's why you're keeping at most B supports, and whenever you're making more mistakes, then you have to decide to throw away some support from the list.",
            "Of support that you're keeping to make room for the new mistake that the new supports that you have gained by making a new mistake."
        ],
        [
            "So now interested.",
            "The interesting question is that what?",
            "How is this going to affect the performance of your algorithm?",
            "So if you don't remember all of your mistakes?",
            "How can you prove something about the performance of the algorithm so it's it's easy to prove a lower bound on a very simple lower bound, so if you.",
            "Now again your.",
            "Suppose let's look at let's for."
        ],
        [
            "Implicitly suppose that.",
            "Colonel here is an inner product.",
            "OK, so it's just this kernel is implementing the inner product between the vectors in the original space, so you are not gaining anything here, but for the sake of simplicity you're still running the algorithm in the in with in the kernel version, even though the kernel implements.",
            "Linear just implements the inner product between the.",
            "To Easter, since in the original space.",
            "So now this algorithm is.",
            "This is the inner product between VXT is just the perception algorithm in the original input."
        ],
        [
            "Face no difference.",
            "So now you're competing against again as before.",
            "You're competing against linear classifiers, but still you're keeping a bounded number of supports in your list.",
            "OK, so.",
            "Before you could compete with any U and you had a trade off in the bound within the hinge loss of yuan, it's squared norm.",
            "You remember this trade off.",
            "Now it's easy to show that you cannot compete with the huge U whose norm is larger than the roughly the square root of the number of supports.",
            "So basically.",
            "If you're you is.",
            "If you have data set which is nearly separable by some you which is longer than that, then there is a sequence of examples that is perfectly classified by the zoo on which your algorithm that only remembers be supports is going to make a mistake, at which time the proof is is very, very easy."
        ],
        [
            "Not going to make it, but it's easy.",
            "So.",
            "You have this, you have this necessity condition, so you need at least this many supports to compete against any you have length you.",
            "Otherwise there's no lower bounds prevents you."
        ],
        [
            "Proving anything interesting.",
            "So now the question is, can you compete against any you which is ever so slightly shorter than this bound here?",
            "So basically is.",
            "Just one plus epsilon, longer than the limit you squared.",
            "OK."
        ],
        [
            "And there is a very simple algorithm that does that, and we call it randomized budget perception.",
            "And this this algorithm is again.",
            "Now as a size B of."
        ],
        [
            "Cash for the support systems with an empty cache an.",
            "Just."
        ],
        [
            "Like before and loops over examples and makes predictions just like before using the examples from the list."
        ],
        [
            "But whenever."
        ],
        [
            "Makes a mistake then, if.",
            "The list is already full so that you have already be supports in your list.",
            "Then it throws away random support from the list to make room."
        ],
        [
            "Room for the new one and then add.",
            "The new one to the list.",
            "So this is the.",
            "Very simple, deceivingly simple policy for a victim.",
            "All the supports to make room for new ones so, but indeed, because your analysis is adverse aerial so you're bound is going to be you.",
            "One you're bound to hold for any individual sequence of examples OK?",
            "A simple random policy for managing your cash turns out to be opt in."
        ],
        [
            "Optimal in the sense that.",
            "Basically.",
            "You can achieve this.",
            "Bound here so you can compete with use that are arbitrarily close to the limit to EU squared limit, beyond which you cannot go, and the bound on a number of mistakes is going to that you're going to incur into is going to scale by this quantity here, so as your epsilon goes to 0, meaning that you're approaching this threshold that after which you are going to make in the worst case, a mistake at every step.",
            "Then you're bound.",
            "You're going to pay this in your bound.",
            "The bond is going to blow up according to the inverse power of that.",
            "OK, so this gives you a precise relationship.",
            "I mean it's we don't know whether this is optimal, so you can whether you can get any better relationship between the parameter epsilon and the actual number of mistakes.",
            "But it's definitely give you an idea of that.",
            "You're bound is not going to crash as an effect of using a bounded number of supports.",
            "So I'm going to close."
        ],
        [
            "This talk by showing a couple of of of experimental results.",
            "So here you have.",
            "A document classification problem.",
            "This is actually the average of, I think, 50 classification binary classification problems on document, categorized according to different news categories.",
            "So for each category you have a binary classification problem.",
            "You're running a 50 perceptrons in parallel, and you're making the average, so this is the.",
            "An average number of prediction mistakes on I think were 40,000 examples.",
            "A stream of 40,000 examples.",
            "This is the performance.",
            "This is the error of the perceptron which.",
            "Uses an unlimited budget size, so this is the budget size and this is a completely stationary problem in the sense that.",
            "Every stream, every binary problem has the same.",
            "Is is so basically every perception is facing the same binary classification problem for the whole length of the stream.",
            "So this data, this data set is stationary in the sense that you are not changing the classification problem as the perception runs.",
            "Then your as your budget to decrease decreases then your mistake is going to rate mistakes is going to increase correspondingly and these are different versions of this randomized budget perception algorithm that you saw is around here.",
            "It's over here.",
            "OK, so this is OK.",
            "This is what you expect.",
            "OK, there is some degradation in performance.",
            "As you decrease the budget side, but this is now, let's, let's suppose that the data is highly nonstationary.",
            "So basically you are changing the binary classification problem.",
            "As you run each person as you run a perception, so every perception is fit is facing a classification problem where the meaning of the positive and negative examples is being changed after awhile.",
            "So for instance, for awhile positive was a sport news item and then after a while becomes a politics news item and then a finance news item."
        ],
        [
            "OK.",
            "So now you see that again this is the over here.",
            "The performance of the perception is here.",
            "Which is higher is worse than before because it's highly non stationary problem.",
            "But now you see that for the randomized budget budget perception here and some other versions of them of it actually have there's no degradation in performance, because remember fewer examples from the past helps.",
            "If the data is highly non stationary.",
            "So this memory boundedness feature helps in the case the.",
            "Problem is highly non stationary so you are trying to track an the best linear classifier which is changing from time to time.",
            "So actually you see that in certain cases some versions of this budgeted perceptual actually perform better than the perception itself.",
            "Because of this nationality.",
            "But then if you."
        ],
        [
            "Use a second order, a second order perception version of the budget algorithm.",
            "So better perception algorithms run with the budget.",
            "Then it's actually you can actually see that this memory boundedness helps in the sense helps in a concretely significantly so miss that.",
            "If the budget goes beyond a certain amount, then that the number of mistakes is.",
            "Number of mistakes is is really really small, are significantly smaller than the number of mistakes made by the perception that hasn't a limited budget.",
            "And again because these algorithms are able to exploit the combination of a non stationary target and the bounded memory more effectively.",
            "So this tells you that this budget this memory boundedness.",
            "Feature is beneficial in terms of keeping the space complexity bounded, so the time complexity of the algorithm bounded by constant.",
            "But it's also very can be very useful in the case in which you are.",
            "Prediction problem is is it hard.",
            "The nonstationary prediction problem.",
            "So in this case the two things Dennis stationarity and the memory boundedness go together in a in a in a profitable way.",
            "So you can actually save on the number of mistakes.",
            "OK I think I will stop here, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Let's start with this.",
                    "label": 0
                },
                {
                    "sent": "Mini tutorial on online learning.",
                    "label": 1
                },
                {
                    "sent": "This is, I think, the photo school I teach this year.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it happens, you know there's a large deviations in the number of courses you give every year, and there was a peak this year, so some of you may have seen these things.",
                    "label": 0
                },
                {
                    "sent": "Maybe in Tubingen are some of the places I so there's going to be some overlapping with what I said into being in a couple of months ago.",
                    "label": 0
                },
                {
                    "sent": "Anyway.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The plan this is the plan, which is maybe a little bit too ambitious, so I might skip some something and this is meant to be a very simple and basic introduction to themes.",
                    "label": 0
                },
                {
                    "sent": "Certain teams in online learning.",
                    "label": 1
                },
                {
                    "sent": "I will particularly will look at linear binary classification.",
                    "label": 0
                },
                {
                    "sent": "Most of essentially all of online learning has to deal with deals with linear classification, and the the simplest instance of it is the binary problem.",
                    "label": 0
                },
                {
                    "sent": "So that's why we will focus on it.",
                    "label": 0
                },
                {
                    "sent": "Then I will say.",
                    "label": 0
                },
                {
                    "sent": "A few a certain number of things on the perception algorithm.",
                    "label": 1
                },
                {
                    "sent": "Which is the most basic example of online algorithm in math or binary classification?",
                    "label": 0
                },
                {
                    "sent": "And then then I will move on to analyze a little bit in detail the case of.",
                    "label": 0
                },
                {
                    "sent": "Linearly separable data sets and all the possible mistake bounds that you can prove for this specific sequence of sequences of data.",
                    "label": 1
                },
                {
                    "sent": "And the further I will move on to discuss a bit relationship within online learning and convex optimization.",
                    "label": 0
                },
                {
                    "sent": "And this is a theme that has been booming in the last two or three years.",
                    "label": 0
                },
                {
                    "sent": "And I will say very few very simple things, so very basic.",
                    "label": 0
                },
                {
                    "sent": "And that shows how few concepts few basic concepts from convex optimization can help in the analysis of online algorithms.",
                    "label": 0
                },
                {
                    "sent": "Then I will time permitting, I will talk a bit about kernel based online learning, especially memory bounded online kernel based learning and possibly online SVM and active learning.",
                    "label": 0
                },
                {
                    "sent": "Maybe not.",
                    "label": 0
                },
                {
                    "sent": "I'll try to cover the last part which is the relation the way you can convert an analysis for for online algorithms to a classical statistical risk bound.",
                    "label": 0
                },
                {
                    "sent": "For the.",
                    "label": 0
                },
                {
                    "sent": "Statistical learning model, which is maybe more familiar.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Too many of you.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hey, let's start with linear classification.",
                    "label": 1
                },
                {
                    "sent": "So the basic scenario I have in mind when I talk about online classification is this one you can think of a system for providing online classification of text documents, so you have a stream of unlabeled.",
                    "label": 0
                },
                {
                    "sent": "The text that documents that are fed into classification system, and every time a new document comes in, the system guess is a label which will think of as a binary label.",
                    "label": 0
                },
                {
                    "sent": "So for instance, is this document about?",
                    "label": 0
                },
                {
                    "sent": "Politics.",
                    "label": 0
                },
                {
                    "sent": "And the label is.",
                    "label": 0
                },
                {
                    "sent": "Given to the user that possibly provides a feedback and the feedback is supposed to be the true, the ground truth for the semantics of the category of that document, and in the most general case, the label is actually something that the system asks explicitly to the user.",
                    "label": 0
                },
                {
                    "sent": "So the most the most general scenario is an active learning scenario in which labels might be requested or not, but for the.",
                    "label": 0
                },
                {
                    "sent": "Most part of this talk, we will think that the label is always provided by the user, so after each classification of a document that the user is providing the label to the true label, the ground truth to the system.",
                    "label": 0
                },
                {
                    "sent": "So this might be plausible or implausible depending on a specific application that you have in mind.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But I argue a bit about it.",
                    "label": 0
                },
                {
                    "sent": "So when I talk about linear classifiers, you should all know that.",
                    "label": 0
                },
                {
                    "sent": "I mean these things so we have for the online model we have a stream of data instances that is actually.",
                    "label": 1
                },
                {
                    "sent": "This is our data.",
                    "label": 0
                },
                {
                    "sent": "And these data instances are encoded as vectors in a real dimensional space.",
                    "label": 1
                },
                {
                    "sent": "As usual for many applications.",
                    "label": 0
                },
                {
                    "sent": "For most learning algorithms.",
                    "label": 1
                },
                {
                    "sent": "And actually, for linear learning algorithms, let's say and there's a with each data instance, there's a binary label associated with it which tells the true classification of the document or that data item.",
                    "label": 0
                },
                {
                    "sent": "So YT is going to be the true label of XT and.",
                    "label": 0
                },
                {
                    "sent": "We will assume there is a learner maintains a linear classifier WT minus one which predicts the label of the current distance with by taking the sign on the sign of the inner product between the weight parameter and the current distance.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the picture.",
                    "label": 0
                },
                {
                    "sent": "And of course you know the you might know this notion of margin, which is can be associated with certain with confidence.",
                    "label": 0
                },
                {
                    "sent": "Of classification of a certain X by given with parameter W and so the margin is just the distance between the distance of.",
                    "label": 0
                },
                {
                    "sent": "Let the tip of X, the point X to the hyperplane that defines that separates the positive classification of W from the negative classification of W. So everything that lies in this half space where the determinant by the direction of W is going to be classified positive as positive by the binary classifiers associated with W, and everything that goes in the other half space is going to be classified as negative just by looking at.",
                    "label": 1
                },
                {
                    "sent": "The sign of that inner product.",
                    "label": 0
                },
                {
                    "sent": "So this is what we mean by by linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "So you see it's very simple because labels are binary, so there is a very simple decision, the decision space.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's just a hyperplane.",
                    "label": 0
                },
                {
                    "sent": "And in the if we.",
                    "label": 0
                },
                {
                    "sent": "If we look at online learning with linear classifier that now we look at this kind of.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Protocol in which you have at each time step you have a data instance which is given to the online classifier.",
                    "label": 0
                },
                {
                    "sent": "The online classifier predicts with the current weight vector.",
                    "label": 0
                },
                {
                    "sent": "I'm using this indexing at T -- 1 for the current with parameter of the classifier to stress the fact that the classifier.",
                    "label": 0
                },
                {
                    "sent": "Has a before the theater round starts.",
                    "label": 0
                },
                {
                    "sent": "Comes up with some hypothesis with some linear hypothesis which does not depend on the on the on the next instance to be observed.",
                    "label": 0
                },
                {
                    "sent": "So that's why I'm using this indexing and then after the prediction the true label is observed and then the learner learner as the chance to update the.",
                    "label": 0
                },
                {
                    "sent": "The wait to come up with a new weight vector WT which will be used in the next.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next iteration.",
                    "label": 0
                },
                {
                    "sent": "And so you can think of as in response.",
                    "label": 0
                },
                {
                    "sent": "Of the sequence of examples being observed, the learner produces a corresponding sequence of classifiers.",
                    "label": 1
                },
                {
                    "sent": "That will generate that will grow as more data from the stream are fed into the classifier, and when we we will look at conversions between online learning results to statistical learning results.",
                    "label": 0
                },
                {
                    "sent": "We will specifically concentrate on this ensemble of classifiers generated by the online learning as iterating over the data.",
                    "label": 0
                },
                {
                    "sent": "The in this case, but if you look simply at the the online problem, we don't want to make any specific and some assumption on the way this stream is generated so.",
                    "label": 0
                },
                {
                    "sent": "The strength of the beauty of the results in online learning, that is that they do not depend on stochasticity of data generation system, so the source of data is completely arbitrary an we are facing every time we are facing a prediction problem on an individual stream of data.",
                    "label": 0
                },
                {
                    "sent": "So without making making any assumption on the way this stream is generated and the results.",
                    "label": 1
                },
                {
                    "sent": "The that we prove will depend on the specific properties of the of the individual stream of data that the learner is observing.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why we cannot define a notion of risk, because there is no.",
                    "label": 0
                },
                {
                    "sent": "There's no probability here, and the only quantity we can look at our the empirical quantities.",
                    "label": 0
                },
                {
                    "sent": "That are the in case of binary classification.",
                    "label": 1
                },
                {
                    "sent": "This is going to be the number of classification mistakes that the learner makes on any specific stream.",
                    "label": 0
                },
                {
                    "sent": "So this is going to be the main measure of performance for the learner here.",
                    "label": 0
                },
                {
                    "sent": "So we will look at the evolution of the number of mistakes made as the stream becomes longer and longer.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a few remarks, so this models of online learning are natural on certain tasks.",
                    "label": 1
                },
                {
                    "sent": "Especially, I mean if you think of a problem in which you always get the true label after every prediction, then you should be thinking of application like market or weather forecasting.",
                    "label": 0
                },
                {
                    "sent": "In which you know after you know you predict tomorrow's weather and the day after you actually observe the what the weather is like.",
                    "label": 0
                },
                {
                    "sent": "So this is our application in which you have a natural.",
                    "label": 0
                },
                {
                    "sent": "And a natural nature is giving you the label after each prediction.",
                    "label": 0
                },
                {
                    "sent": "In otherwise, you might also think of.",
                    "label": 0
                },
                {
                    "sent": "In cases in which you want to actually solve a batch problem, but for instance you have a huge amount of data, and so to avoid.",
                    "label": 0
                },
                {
                    "sent": "Time and space complexity issues that other learning algorithms might incur.",
                    "label": 0
                },
                {
                    "sent": "You might use an online learning algorithm that is usually very efficient and very scalable on the size of data and you can.",
                    "label": 0
                },
                {
                    "sent": "You can just cycle the online online learning algorithm on the data set and then to use some aggregation criteria not to.",
                    "label": 0
                },
                {
                    "sent": "To combine the sequence of hypothesis that has been generated by the algorithm as cycling over this data set.",
                    "label": 0
                },
                {
                    "sent": "So the problem itself doesn't have to be strictly online for to have a good motivation to use this algorithm.",
                    "label": 0
                },
                {
                    "sent": "And so I said that this usually these are very easy, really easy to code, so there's just few lines of code and they and they have good scaling properties.",
                    "label": 0
                },
                {
                    "sent": "Most of them they can be implemented with kernels, so you can run them in a reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "We would maybe mention a little bit about it and they have a strong performance guarantees as I said because they do not need the stochastic.",
                    "label": 0
                },
                {
                    "sent": "Data generation model to be analyzed in the.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, you can derive a stochastic risk bounds using those online to batch conversions.",
                    "label": 1
                },
                {
                    "sent": "I'm going to mention at the end of the talk and you have active natural active learning variants whenever the true label comes at the cost, you don't have it for free, but you have to explicitly ask for it.",
                    "label": 0
                },
                {
                    "sent": "And last but not least, you can actually easily turn.",
                    "label": 0
                },
                {
                    "sent": "In most cases you can turn.",
                    "label": 0
                },
                {
                    "sent": "Usual mistake bounds.",
                    "label": 0
                },
                {
                    "sent": "So the analysis for an online algorithms into an analysis that takes into account the fact that the data might not be well explained by single linear classifier.",
                    "label": 0
                },
                {
                    "sent": "So I won't mention this, but this is interesting, so this is the case in which, for instance there is again the stream of data is such that there is very very good linear fit for the four of the first part, then some other.",
                    "label": 0
                },
                {
                    "sent": "Good linear feet for the central part and then as yet another different linear feet for the third part.",
                    "label": 0
                },
                {
                    "sent": "An online algorithms can be.",
                    "label": 0
                },
                {
                    "sent": "Modified in such a way that they are able to detect this change points in the best linear feet of the stream that is facing without actually knowing how many change points are there and where they are situated into the stream.",
                    "label": 0
                },
                {
                    "sent": "And of course, the bound that you are able to prove in this case is will depend on the degree of nonstationarity of the data, so this is an interesting, interesting path, but I don't have time to.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cover it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if you let's start with the perception of the perception is the simplest online learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "It comes from the 50s was introduced by Rosenblatt to study the visual abilities that visual recognition abilities of the brain.",
                    "label": 0
                },
                {
                    "sent": "But it's a very simple linear model, so you have an initial empty weight, which is the zero vector and the app to the update is just like that.",
                    "label": 0
                },
                {
                    "sent": "So you remember any linear online linear learn.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The algorithm is just characterized by the by this step here, everything else is fixed that this is the only place in which you can.",
                    "label": 0
                },
                {
                    "sent": "We can say something we can.",
                    "label": 0
                },
                {
                    "sent": "Show how the algorithm what is the action of the algorithm on the stream.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and the update rule for the perception is a is that easy whenever and no mistake is made on the current instance, then the weights remain.",
                    "label": 0
                },
                {
                    "sent": "The vector remains the same, otherwise we add the.",
                    "label": 0
                },
                {
                    "sent": "Current distance multiplied by the label, so that's the very simple.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Update rules, so now a little bit of intuition because this is should be beneficial for learning.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that.",
                    "label": 0
                },
                {
                    "sent": "Clearly you're going to make a mistake if you use a linear classifier whenever your margin, which is that quantity over there is negative.",
                    "label": 0
                },
                {
                    "sent": "So whenever the inner product between the current weight and distance doesn't have the same sign as the true label, of course in that case you're going to make a mistake.",
                    "label": 0
                },
                {
                    "sent": "So if you made a mistake on the current example, then you want to imagine was negative, and you want to increase it to make it positive so that you won't incur any mistake if the same.",
                    "label": 0
                },
                {
                    "sent": "Element comes up again in the stream, so now you can look at the effect of the perception update on the margin and you can easily see that the if the the margin of the updated weight is measured on the same example that caused the mistake on the previous with vector, then you might observe that the margin.",
                    "label": 0
                },
                {
                    "sent": "Is the old margin plus the squared norm of distance so the margin has increased by the squared norm of distance?",
                    "label": 1
                },
                {
                    "sent": "Of the current distance, this might be enough or not to fix the mistake, so maybe enough or not for the this guy to be corrected on the same instance that caused the mistake on this guy, but.",
                    "label": 0
                },
                {
                    "sent": "This might not be sufficient because imagine was so small that even if we increase it by that it remains negative.",
                    "label": 0
                },
                {
                    "sent": "So for this reason that perceptual update is called non corrective because it increases the margin by a fixed amount.",
                    "label": 0
                },
                {
                    "sent": "But we are not guaranteed that is going to correct the mistake.",
                    "label": 0
                },
                {
                    "sent": "And even if it corrects the mistake, then it's also unclear that correcting a mistake is important because you don't know whether this example will come up again in the stream, or whether the stream the next part of the stream is related to the previous part.",
                    "label": 0
                },
                {
                    "sent": "So in principle it's not clear whether these things is going to are going.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work.",
                    "label": 0
                },
                {
                    "sent": "Sorry OK.",
                    "label": 0
                },
                {
                    "sent": "So it's very special case of data sequences or streams.",
                    "label": 0
                },
                {
                    "sent": "The leader is a parable sequences.",
                    "label": 0
                },
                {
                    "sent": "So for instance, suppose that you have a finite sequence of data with my collected data set, and this is clearly separable, so there is some U that provides the correct classification in the sense that the margin is always strictly positive on any example of this data set.",
                    "label": 0
                },
                {
                    "sent": "And notice that.",
                    "label": 0
                },
                {
                    "sent": "And then in this case, the normal the length of this vector doesn't really matter, so you might just think that the zoo is unit norm.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you might wonder first whether if we cycle, if we have the perception perception cycling over this little separable data set, then by this using this very simple and non corrective learning rule anything will happen.",
                    "label": 0
                },
                {
                    "sent": "So that will find any such linear separator.",
                    "label": 1
                },
                {
                    "sent": "If the perception whenever it's cycle my stream OK and what happens?",
                    "label": 0
                },
                {
                    "sent": "Can you prove anything if you have actually?",
                    "label": 0
                },
                {
                    "sent": "An infinite stream of data that is not linearly separable.",
                    "label": 1
                },
                {
                    "sent": "So what can you prove about the behavior of the perception when it goes on into the stream so OK, can you say anything about learning in this case when there is no clear notion of convergence?",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the questions that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to answer in very quickly, so the results in online early learning have usually this form.",
                    "label": 0
                },
                {
                    "sent": "So this is called the relative loss bound and is a notion that is.",
                    "label": 1
                },
                {
                    "sent": "It's a non statistical notion of risk bound when you compare the risk of a classifier with the risk of the best classifier in a given class.",
                    "label": 0
                },
                {
                    "sent": "For instance, in the class of all linear classifiers this is the non statistical version of that bounds.",
                    "label": 0
                },
                {
                    "sent": "An reminds of analogous bounds in competitive analysis of online online algorithms in computer science.",
                    "label": 0
                },
                {
                    "sent": "So I'm using this notation M sub T of WT minus one to indicate the event that the W T -- 1 made made a mistake on the teeth.",
                    "label": 0
                },
                {
                    "sent": "Example of a stream.",
                    "label": 0
                },
                {
                    "sent": "So this is the indicator indicator function of a mistake for the for the element of the stream.",
                    "label": 0
                },
                {
                    "sent": "That example of the stream.",
                    "label": 0
                },
                {
                    "sent": "So in general we might want to compare.",
                    "label": 0
                },
                {
                    "sent": "The number of mistakes made by a online learner, such the perceptron with the number of mistakes on the same stream made by the best linear classifier for that specific stream.",
                    "label": 0
                },
                {
                    "sent": "So I'm taking the infamous overall linear classifiers you OK?",
                    "label": 0
                },
                {
                    "sent": "So I'm comparing the these guys that change at every step with the best fixed linear fit for the same stream and I'm doing it on any finite prefix of the stream, so I'm stopping this some at some obitury point OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this this is an interesting goal, but unfortunately it's easy to see that.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you think about it and.",
                    "label": 0
                },
                {
                    "sent": "There are a number of very nice papers that prove this informal way that computing and solving that batch optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So even if you're given the whole sequence of data in advance, computing the hyperplane computed the linear classifier that minimizes the number of misclassified examples on arbitrary sequence of examples is an NP hard problem.",
                    "label": 1
                },
                {
                    "sent": "So even if you're given the whole sequence of data in advance.",
                    "label": 0
                },
                {
                    "sent": "You cannot compute the optimal guy, so it's unlikely that it in.",
                    "label": 0
                },
                {
                    "sent": "If you can efficiently.",
                    "label": 0
                },
                {
                    "sent": "Compete with that best linear fit in an online way.",
                    "label": 0
                },
                {
                    "sent": "When the data is revealed to you.",
                    "label": 0
                },
                {
                    "sent": "1 by 1 one element after the other so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Make it feasible that this is this goal is relaxed, that by replacing the mistake indicator function in that in the right hand side with the different loss which is this one, this is called the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "An is a convex upper bound on the mistake indicator function.",
                    "label": 1
                },
                {
                    "sent": "So in this graph here you have the margin.",
                    "label": 0
                },
                {
                    "sent": "So this is the mistake indicator function so that you have a mistake.",
                    "label": 0
                },
                {
                    "sent": "When the margin is negative, you have no mistake.",
                    "label": 1
                },
                {
                    "sent": "When the margin is positive and this is the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Which is zero whenever the margin is bigger than one, an grows linearly with the margin, as the margin is grows into the negative from 1.",
                    "label": 0
                },
                {
                    "sent": "OK, and so the hinge loss wants to not only to have a positive margin, but once we have a margin bigger than one on each example.",
                    "label": 0
                },
                {
                    "sent": "OK, so of course we are making the our competitor to pay more than us because we are using upper bound.",
                    "label": 0
                },
                {
                    "sent": "But the fact that the upper bound is convex mixed the problem feasible so makes the problem of competing in an online fashion again against the best linear feet feasible problem for us.",
                    "label": 0
                },
                {
                    "sent": "So now all bounds that I'm going to show will have this specific form here with maybe constants here and there.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "K. So now it's immediate to see that the norm of U of the best linear classifier that we're competing against the matters becausw the hinge loss cares about the scale of the managing.",
                    "label": 1
                },
                {
                    "sent": "The classification error doesn't care about the scale at the length of you, because we're just looking for just looking at the sign plus or minus.",
                    "label": 0
                },
                {
                    "sent": "But the hinge loss which make us pay whenever the margin is more than one.",
                    "label": 0
                },
                {
                    "sent": "Looks at this game, so essentially.",
                    "label": 0
                },
                {
                    "sent": "So this is for those of you who know supports vector support vector machines, so this is the.",
                    "label": 0
                },
                {
                    "sent": "Exactly the same kind of constraint that we want to enforce on every example in a data set on which the support vector machine is applied.",
                    "label": 0
                },
                {
                    "sent": "So if you look at for instance, in this case, we have linearly separable data set of examples.",
                    "label": 0
                },
                {
                    "sent": "All of them have unit norm.",
                    "label": 0
                },
                {
                    "sent": "Then for a linear classifier a linear classifier U that uses this hyperplane and to have zero hinge loss, you want to have that the projection of you on every example has length at least one.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have to make it you long enough so that this length will be will so that the zero loss will be minimized.",
                    "label": 0
                },
                {
                    "sent": "The hinge loss will be minimized, so now.",
                    "label": 0
                },
                {
                    "sent": "The normal view for hinge loss becomes a complexity measure of the hypothesis and natural complexity measure for the apophysis.",
                    "label": 0
                },
                {
                    "sent": "So now you might think that there must be some kind of tradeoff between the length of you and the hinge.",
                    "label": 0
                },
                {
                    "sent": "Cumulative hinge loss achieved by that you on the on a given data set, and this tradeoff will arise will be observed in the bounces.",
                    "label": 0
                },
                {
                    "sent": "We're going to prove in a very short one.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's now look at the Knights of Perceptron, which is just a couple of slides very simple on any non separable stream.",
                    "label": 0
                },
                {
                    "sent": "So we do the general case and then we look at the specific case for the separable streams.",
                    "label": 0
                },
                {
                    "sent": "So we now looking at any sequence, any sequence of examples.",
                    "label": 0
                },
                {
                    "sent": "And we compete against an arbitrary linear classifier, fixed U, and for simplicity we assume that all examples have unit norm, so they lie on the atmosphere.",
                    "label": 0
                },
                {
                    "sent": "And then we want to prove something about the number of some we want to bound.",
                    "label": 0
                },
                {
                    "sent": "Somehow the number of mistakes made by the perceptron on this stream as a function of the hinge loss of you where you is completely arbitrary, OK?",
                    "label": 0
                },
                {
                    "sent": "And so there are two proof techniques to do that, and they look at the evolution of two quantities.",
                    "label": 1
                },
                {
                    "sent": "One is the is this quantity here, which is the square that difference the squared distance between you and W. The current hypothesis of the perception and the other way is devolution of the inner product.",
                    "label": 0
                },
                {
                    "sent": "You is just using analysis.",
                    "label": 0
                },
                {
                    "sent": "So the perception is simultaneously competing against all linear classifiers, all fixed linear classifiers for that stream.",
                    "label": 0
                },
                {
                    "sent": "So in particularly.",
                    "label": 0
                },
                {
                    "sent": "Is competing with the best fixed linear classifier for this particular stream that is working on.",
                    "label": 0
                },
                {
                    "sent": "So even the algorithm, even though the algorithm is very sick.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it has some unexpectedly strong properties.",
                    "label": 0
                },
                {
                    "sent": "So here comes the proof.",
                    "label": 0
                },
                {
                    "sent": "It's very, very easy, so we look at the evolution of this inner product between you and a beauty on a mistaken round.",
                    "label": 0
                },
                {
                    "sent": "So we assume that WT minus one has made a mistake on YTXT and has been updated so we can expand the WT using the update rule.",
                    "label": 0
                },
                {
                    "sent": "That's the.",
                    "label": 0
                },
                {
                    "sent": "Let's actually quality, sorry, and then we can just.",
                    "label": 0
                },
                {
                    "sent": "Go there, go to the second line and observe that we can just add and subtract 1 to that quantity and now we get this quantity over here in parenthesis and this is the definition.",
                    "label": 0
                },
                {
                    "sent": "If we take the positive part of this quantity, we get the hinge loss of you.",
                    "label": 0
                },
                {
                    "sent": "That's the positive part of this quantity.",
                    "label": 0
                },
                {
                    "sent": "Here is by definition the hinge loss of you.",
                    "label": 0
                },
                {
                    "sent": "So if you could take the positive path because we have a - we're going to make this quantity is smaller.",
                    "label": 0
                },
                {
                    "sent": "So we have proven this line here.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the the indicator of the fact that there was a mistake by the perception for that time step.",
                    "label": 0
                },
                {
                    "sent": "And here we have the recurrence.",
                    "label": 0
                },
                {
                    "sent": "So we get the the inner product before the updated inner product after the update.",
                    "label": 0
                },
                {
                    "sent": "And here we have the hinge loss of the competitor.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can expand this recurrence.",
                    "label": 0
                },
                {
                    "sent": "And what we get when we iterate it, we get the inner product between the final weight a final, let's say after T after any number T of predictions is related to this, which is the initial weight which is 0 because the perception starts with O.",
                    "label": 0
                },
                {
                    "sent": "This ones sum up to the number of mistakes and hear this quantity sum up to the cumulative hinge loss of the fixed the linear classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is half of the story.",
                    "label": 0
                },
                {
                    "sent": "Now you should notice that we have this quantity coming up here, which is the our quantity of interest, because this is just the number of mistakes made by the perception on the First Capital T element.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So examples of the stream so we can do the other part so the other half we look at this.",
                    "label": 0
                },
                {
                    "sent": "First we look at the squared norm of T on a mistake it round, so again we expand the WT using the update rule.",
                    "label": 0
                },
                {
                    "sent": "Then we expand the square square over there and we just observe that cause WT made a mistake on that example.",
                    "label": 0
                },
                {
                    "sent": "Then this must be negative.",
                    "label": 0
                },
                {
                    "sent": "This is the margin, and because it was a mistake, the margin as to be negative.",
                    "label": 0
                },
                {
                    "sent": "So we can drop these, get an upper bound and because we are assuming that these guys are have all unit norm, we get this relationship here.",
                    "label": 0
                },
                {
                    "sent": "So again, we have a recurrence we can work.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The recurrence an we get this OK where again these things sum up to the number of mistakes.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we can combine using cash Schwarz inequality to recover.",
                    "label": 0
                },
                {
                    "sent": "To relate to this with the quantity that we used to prove the other part.",
                    "label": 0
                },
                {
                    "sent": "We can just solve the for this, which we happen to have in both side in both inequalities.",
                    "label": 0
                },
                {
                    "sent": "This one in the previous one.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can solve for it and we can get this which is you can think of it as generalized perception convergence theorem.",
                    "label": 0
                },
                {
                    "sent": "So this test tells you that the number of mistakes made by the perception on any arbitrary stream is bounded by the.",
                    "label": 0
                },
                {
                    "sent": "Best tradeoff in from of the hinge loss of fixed the linear classifier U.",
                    "label": 0
                },
                {
                    "sent": "Plus squared norm plus across term that involves both both things.",
                    "label": 0
                },
                {
                    "sent": "So this is you see this is a.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Has the interesting properties.",
                    "label": 0
                },
                {
                    "sent": "It's an Oracle inequality because it basically without the knowing anything about this you is able.",
                    "label": 0
                },
                {
                    "sent": "You're able to compare the number of mistakes with the best tradeoff between hinge loss and norm.",
                    "label": 0
                },
                {
                    "sent": "OK, so for those who knows of you who know support vector machines, you know that if we drop this path here, this is actually the functional which is minimized by the support vector machine solution.",
                    "label": 0
                },
                {
                    "sent": "So it shows that there is some kind of relationship even though you have this additional cross term here.",
                    "label": 0
                },
                {
                    "sent": "And now you can get, of course the special case of separable streams.",
                    "label": 0
                },
                {
                    "sent": "So if you cycle the perception of the linearly separable stream then that you have that.",
                    "label": 0
                },
                {
                    "sent": "You then this term disappear.",
                    "label": 0
                },
                {
                    "sent": "This term disappear because the hinge loss of the best guy you on a linear separable stream is going to be 0.",
                    "label": 0
                },
                {
                    "sent": "And then you're left with the infimum U, the infamous overview of normal.",
                    "label": 0
                },
                {
                    "sent": "You squared where you squared is any separator achieving 0 inch loss, so in particular you can take the support vector machine solution, which is the shortest linear separator achieving zero hinge loss.",
                    "label": 0
                },
                {
                    "sent": "So basically shows that the number of mistakes made the best section on an English separable stream as is bounded by the squared norm of the support.",
                    "label": 0
                },
                {
                    "sent": "Vector machine solution for the same stream.",
                    "label": 0
                },
                {
                    "sent": "And another consequence is that.",
                    "label": 0
                },
                {
                    "sent": "If you have a separable stream, there's always exist linear separator expressible as a linear combination of.",
                    "label": 1
                },
                {
                    "sent": "At most these many supports this big cause the update.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rule of the perceptron.",
                    "label": 0
                },
                {
                    "sent": "You can view it as adding a support, which is a mistake.",
                    "label": 0
                },
                {
                    "sent": "An example every time a mistake is made.",
                    "label": 0
                },
                {
                    "sent": "So you see you add this guy in the summer.",
                    "label": 0
                },
                {
                    "sent": "So after mistakes this guy here will be a linear combination of M mistaken instances which you can view as a support.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's why you can.",
                    "label": 0
                },
                {
                    "sent": "Since the.",
                    "label": 0
                },
                {
                    "sent": "Most this many mistakes you're going to make, you're going to have at most this many supports in your soul in your solution.",
                    "label": 0
                },
                {
                    "sent": "So, so there's a number of an interesting property, so this is a very basic example of online mistake bound for the simplest banner classification algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me move on.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Owner.",
                    "label": 0
                },
                {
                    "sent": "Two, the special case of separable stream.",
                    "label": 0
                },
                {
                    "sent": "I want to say.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something more about here and the first thing is OK, what?",
                    "label": 0
                },
                {
                    "sent": "One thing that you could, you can be asking now is OK. We saw that the perception doesn't doesn't have a correcting app update, so the margin after every update the margin increases by a constant additive term which is the square.",
                    "label": 0
                },
                {
                    "sent": "The norm of distance.",
                    "label": 0
                },
                {
                    "sent": "So now what you might want to do is that you might want to modify the perception rules so that so to enforce a large margin, so you want to come up with a corrective update, and now you can wonder whether corrective update might be might work better.",
                    "label": 1
                },
                {
                    "sent": "Then the perception update.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So in particularly what you might define a corrective update as follows as an update that enforces a zero hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Zero hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Wait after the update.",
                    "label": 0
                },
                {
                    "sent": "OK, so we want to be.",
                    "label": 1
                },
                {
                    "sent": "We want to modify the perception to sort of make it more aggressive in responding to mistakes and see what happens in terms of the solution.",
                    "label": 0
                },
                {
                    "sent": "So now you again, you made it, you made.",
                    "label": 0
                },
                {
                    "sent": "You made a.",
                    "label": 0
                },
                {
                    "sent": "Now you're looking at the hinge loss of the of the online learner.",
                    "label": 0
                },
                {
                    "sent": "So whenever the online learner incurs some huge loss, then you make an update so that the updated weight will have no no hinge loss.",
                    "label": 0
                },
                {
                    "sent": "On the same example.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "More definitely, more aggressive update.",
                    "label": 0
                },
                {
                    "sent": "So we now can try to analyze the properties of this different update here and this.",
                    "label": 0
                },
                {
                    "sent": "These things this what I'm going to say in a moment is strongly related to an algorithm proposed by Hildreth in convex optimization in the 50s.",
                    "label": 0
                },
                {
                    "sent": "That has.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can view this as a very special case of that general general algorithm by hildred.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not going to tell more about this relationships.",
                    "label": 0
                },
                {
                    "sent": "That you start to see in learning and convex optimization.",
                    "label": 0
                },
                {
                    "sent": "So now let's guess that the solution of this solution of the weight will have this form, which is just the perception update with the learning rate which are going to determine.",
                    "label": 0
                },
                {
                    "sent": "So now we want to choose 80 such that the updated weight as margin big enough so we can just solve and find this solution here.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So since we made an update this is this is positive, so we can just write the hinge loss here.",
                    "label": 0
                },
                {
                    "sent": "This is actually equivalent, so the hinge loss again is the positive part of this.",
                    "label": 0
                },
                {
                    "sent": "But since we made an update, since we make an update whenever this is positive then we are fine.",
                    "label": 0
                },
                {
                    "sent": "So this is the learning rate that is used to enforce that condition of.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Over there.",
                    "label": 0
                },
                {
                    "sent": "And you can also view.",
                    "label": 0
                },
                {
                    "sent": "This update here with this additive chosen in this way so that you enforce large hinge loss as a sort of a first order SVM.",
                    "label": 1
                },
                {
                    "sent": "So basically this is the solution.",
                    "label": 1
                },
                {
                    "sent": "The update this update here with this choice is the solution of this constrained optimization problem, in which you want to stay close to which you choose W, so to be close to the previous weight WT minus one.",
                    "label": 1
                },
                {
                    "sent": "But satisfying this constraint also to have the.",
                    "label": 1
                },
                {
                    "sent": "Aging large margin on the.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Current example, so in other words, geometrically, you're just projecting the previous weight onto the hyperplane defined by this condition here.",
                    "label": 0
                },
                {
                    "sent": "So onto the hyperplane on of weights that have margin exactly equal to 1 on that fixed example, white TXTYT.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "So this is a very very easy modification.",
                    "label": 0
                },
                {
                    "sent": "So just by introducing a learning rate you can enforce a large margin at each update, so it's a sort of again online SVM or first order SVM, because we're just looking at a single example.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the time.",
                    "label": 0
                },
                {
                    "sent": "OK, now it's actually very easy to analyze this one.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is a nice proof.",
                    "label": 0
                },
                {
                    "sent": "So now we have to recall that.",
                    "label": 0
                },
                {
                    "sent": "The hinge loss on the theater example of WT0.",
                    "label": 0
                },
                {
                    "sent": "By definition, because you've made a corrective update a corrective in the sense of the hinge loss.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now you can write the hinge loss of of an example T of the previous guy as this.",
                    "label": 0
                },
                {
                    "sent": "Since this is zero, you can write it like that.",
                    "label": 0
                },
                {
                    "sent": "Then you can expand this by removing the positive part, so you're making it bigger and this is again 0, so you can keep writing it.",
                    "label": 0
                },
                {
                    "sent": "Then you collect Whitey and then you get this expression here and then you use Koshy Schwartz to get these two.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "You can square both sides again.",
                    "label": 0
                },
                {
                    "sent": "This relation here between the squared hinge loss.",
                    "label": 0
                },
                {
                    "sent": "And this this product between the squared norms.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this is half of the story again.",
                    "label": 0
                },
                {
                    "sent": "Now you get you again.",
                    "label": 0
                },
                {
                    "sent": "You fix some linear separator you.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You remember this is analysis for separable streams, be 'cause it's very simple in this case, so I'm not covering the analysis of this algorithm for no super, I'll do it later.",
                    "label": 0
                },
                {
                    "sent": "Actually, I'll do it later, but for now I'm going to show a very simple analysis for this.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parable streams.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's going to be some.",
                    "label": 0
                },
                {
                    "sent": "A separator linear separator U for the stream which has a zero hinge loss.",
                    "label": 1
                },
                {
                    "sent": "And actually I can choose the short test and not the shorter, but the shortest such linear separator with zero hinge loss, which is the SVM solution so I can pick that one for instance.",
                    "label": 0
                },
                {
                    "sent": "And by the law of cosines, since this since WT minus one and U leave on the opposite sides of the half space of this half space here.",
                    "label": 1
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is how space W T -- 1 and you live on opposite sides of this half past space.",
                    "label": 0
                },
                {
                    "sent": "Becaused abate minus one makes a mistake on this example, whereas you doesn't because US.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Separator then you can apply the law of cosine and get that inequality over there.",
                    "label": 0
                },
                {
                    "sent": "That relates the squared difference between TT TT's minus minus one with the debt difference that involves you.",
                    "label": 0
                },
                {
                    "sent": "OK, so now you can you have that.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you had the previous one.",
                    "label": 0
                },
                {
                    "sent": "Which had the same term and you had the disk.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Red hinge loss here.",
                    "label": 0
                },
                {
                    "sent": "So you can sum up and use the telescoping property over there and what you get at the end there is this expression here.",
                    "label": 0
                },
                {
                    "sent": "OK, now this thing.",
                    "label": 0
                },
                {
                    "sent": "Here is the squared hinge loss, which is this red curve over here.",
                    "label": 0
                },
                {
                    "sent": "This is clearly an upper bound on the number of mistakes, so this is squared.",
                    "label": 0
                },
                {
                    "sent": "That doesn't make any sense, so there's no square here.",
                    "label": 0
                },
                {
                    "sent": "This is just some.",
                    "label": 0
                },
                {
                    "sent": "You can square indicator function will remain an indicator function.",
                    "label": 0
                },
                {
                    "sent": "So this is just a sum of number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "This curve here and this is the upper bound that you get, which is exactly the what you had in the perception case for the separable stream analysis squared normal view squared norm of the largest XD.",
                    "label": 0
                },
                {
                    "sent": "When I saw it when I showed you the analysis purpose for perception, this thing was one because the XD had norm one.",
                    "label": 0
                },
                {
                    "sent": "But if XDR obitury then you have this term here.",
                    "label": 0
                },
                {
                    "sent": "So this is the perception bound.",
                    "label": 0
                },
                {
                    "sent": "But now we can prove a tighter relationship because this squared hinge loss is an upper bound on a mistake on the mistake indicator function.",
                    "label": 0
                },
                {
                    "sent": "So for perception we had this.",
                    "label": 0
                },
                {
                    "sent": "Smaller equipment that and now we have this thing here, which is an upper bound smaller liquid date, so it's a it's a stronger result.",
                    "label": 0
                },
                {
                    "sent": "Is then perception convergence theorem.",
                    "label": 0
                },
                {
                    "sent": "However, it doesn't really imply.",
                    "label": 0
                },
                {
                    "sent": "Any convergence to the SVM solution?",
                    "label": 1
                },
                {
                    "sent": "So again, we cannot, as in the perception case that we cannot guarantee that this algorithm or the perceptron algorithm will find a solution.",
                    "label": 0
                },
                {
                    "sent": "Linear separator with large margin.",
                    "label": 0
                },
                {
                    "sent": "OK, if you think about it, it is.",
                    "label": 0
                },
                {
                    "sent": "It might seem that actually this shows that OK, you're minimizing the.",
                    "label": 0
                },
                {
                    "sent": "So this is a constant, so this discriminative squared hinge loss is going to be bounded by a constant.",
                    "label": 1
                },
                {
                    "sent": "OK, so it is this is going to converge to something.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's going to converge to something, but it's not going to converge to linear separated with with a large margin.",
                    "label": 0
                },
                {
                    "sent": "Again, as in the perception case will find some linear separator OK. Good so.",
                    "label": 0
                },
                {
                    "sent": "Now these these algorithms are.",
                    "label": 0
                },
                {
                    "sent": "Ann, so this more aggressive updates are actually.",
                    "label": 0
                },
                {
                    "sent": "Not some somehow disappointing because you cannot prove the candle analysis that you can make is almost is not much stronger than what you can prove for the for the perception case.",
                    "label": 0
                },
                {
                    "sent": "But in practice in many practical examples aggressive updates will work well.",
                    "label": 0
                },
                {
                    "sent": "We work better than a perception updates for for in terms of practical performance.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is anyway this is still a very special case because we are looking at separable streams and of as in the.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the support vector machine analysis.",
                    "label": 0
                },
                {
                    "sent": "Just that, enforcing strictly enforcing a positive zero hinge loss on each example by bit too much if the stream is not.",
                    "label": 0
                },
                {
                    "sent": "Actually linearly separable.",
                    "label": 0
                },
                {
                    "sent": "It might in a real case it might be useful.",
                    "label": 0
                },
                {
                    "sent": "It might be more beneficial to introduce some kind of slack, just like in the case of support vector machines too.",
                    "label": 0
                },
                {
                    "sent": "Be partially corrected.",
                    "label": 0
                },
                {
                    "sent": "So to introduce a sort of a tuning.",
                    "label": 0
                },
                {
                    "sent": "Bing parameter to measure the aggressiveness of the update so that because you're not actually confident that the stream that you are trying to predict is linearly separable one.",
                    "label": 0
                },
                {
                    "sent": "So we will look at this tunable updates.",
                    "label": 0
                },
                {
                    "sent": "Tunable aggressive updates in a moment.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we make a little detour.",
                    "label": 0
                },
                {
                    "sent": "How much time do I have?",
                    "label": 0
                },
                {
                    "sent": "Half an hour.",
                    "label": 0
                },
                {
                    "sent": "Yes, I see someone on the.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. OK, so now let me.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we have seen bounce of, you know.",
                    "label": 0
                },
                {
                    "sent": "In both perception and this, more aggressive updates in terms of number of mistakes made on a separable stream of the same bound.",
                    "label": 0
                },
                {
                    "sent": "Now you can wonder, is this the best possible boundaries can prove in the case of a separable stream.",
                    "label": 0
                },
                {
                    "sent": "So this is a very very basic and Chris question.",
                    "label": 0
                },
                {
                    "sent": "You have a data set which is linearly separable.",
                    "label": 0
                },
                {
                    "sent": "You want to find in an incremental way you want to find the linear separator.",
                    "label": 0
                },
                {
                    "sent": "How many mistakes?",
                    "label": 0
                },
                {
                    "sent": "How many updates are is going to take you to find the any linear separator with no specific.",
                    "label": 0
                },
                {
                    "sent": "Condition on the margin.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a question that it's you know perception as a solution.",
                    "label": 1
                },
                {
                    "sent": "It takes you if all the examples in the data set have unit norm is going to take you squared normal view where you is the SVM solution for that data set.",
                    "label": 0
                },
                {
                    "sent": "Is this the best possible bound in terms of number of mistakes?",
                    "label": 0
                },
                {
                    "sent": "OK, so you can.",
                    "label": 0
                },
                {
                    "sent": "You can relate this problem again at 2.",
                    "label": 0
                },
                {
                    "sent": "Convex optimization constrained convex optimization, so this questions has been studied in convex optimization in the past 30 years.",
                    "label": 0
                },
                {
                    "sent": "So you can use the solutions you can express.",
                    "label": 1
                },
                {
                    "sent": "You can view optimization algorithm for constrained convex optimization problems as online linear online learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "And now I'm going to make this relationship relationship to show you how this more sophisticated learners from from convex optimization.",
                    "label": 0
                },
                {
                    "sent": "What kind of properties have, if you just look at the number of mistakes they make in this problem of finding a consistent hyperplane?",
                    "label": 1
                },
                {
                    "sent": "OK, so you have again an arbitrary.",
                    "label": 0
                },
                {
                    "sent": "Legally separable data.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the unit norm.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can symmetrize it, so you can basically multiply each point by its label so the negative.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Points will be flipped to the positive side.",
                    "label": 0
                },
                {
                    "sent": "The problem hasn't changed, but just have just all the points.",
                    "label": 0
                },
                {
                    "sent": "Have positive you can.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of all the points of.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In positive labels and now you can look at.",
                    "label": 0
                },
                {
                    "sent": "In this case you can look at the two extreme points.",
                    "label": 0
                },
                {
                    "sent": "These two extremes.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Points.",
                    "label": 0
                },
                {
                    "sent": "We'll define two hyperplanes.",
                    "label": 0
                },
                {
                    "sent": "This one particular to this one and this one particular to that one.",
                    "label": 0
                },
                {
                    "sent": "And then these two hyperplanes define a cone into dimension.",
                    "label": 0
                },
                {
                    "sent": "Is just this in many dimension will be a political cone and any your linear separator, any vector that is a linear separator for this stream will have to lie within this cone, because in this case it will correctly classifier all the points that were that were limited by these two more most extreme ones.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the cone of the consistent solutions.",
                    "label": 1
                },
                {
                    "sent": "The cone of the solutions for this very simple, nearly separable data set.",
                    "label": 0
                },
                {
                    "sent": "So now you're looking at this.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is an online problem, so you think you have some solutions on current solution.",
                    "label": 0
                },
                {
                    "sent": "For the example you've seen.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So far and now you'll see a new example.",
                    "label": 0
                },
                {
                    "sent": "An example, for instance over here.",
                    "label": 0
                },
                {
                    "sent": "So this is the hyperplane that is cutting through the cone and giving you a new constraint that the solution has to satisfy.",
                    "label": 0
                },
                {
                    "sent": "So for instance, now this is the new consistent cone and you have to update the solution over here.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what perception does is is basically it observes that the current solution as a negative margin with respect to the example so increases the margin by fixed amount.",
                    "label": 0
                },
                {
                    "sent": "So it might correct in this case it corrects the solution and so it finds actually new solution in the new consistent code.",
                    "label": 0
                },
                {
                    "sent": "But it's really it's really.",
                    "label": 0
                },
                {
                    "sent": "It might be, it might be non corrective or may overshoot over there.",
                    "label": 0
                },
                {
                    "sent": "I mean it's just an update.",
                    "label": 0
                },
                {
                    "sent": "That increases the margin by a fixed amount.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the Hildred algorithm that we saw.",
                    "label": 0
                },
                {
                    "sent": "Makes an update such that the margin.",
                    "label": 0
                },
                {
                    "sent": "The margin on the with respect to the new example that was mistaken is 1, so this this is the hyperplane of the solutions that have margin at least one with respect to this guy here.",
                    "label": 0
                },
                {
                    "sent": "And so the new solution of the hidden algorithm is just the projection of this onto this hyperplane here.",
                    "label": 0
                },
                {
                    "sent": "And so in this case you see this guy overshoots.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "End.",
                    "label": 0
                },
                {
                    "sent": "So now you can say OK, so let's assume that you have a sequence of examples that literally separable and Now suppose that capital U is the length of the norm of the support vectors.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lucian and now you can look at a bunch of algorithms that solve this problem and somehow have been developed.",
                    "label": 0
                },
                {
                    "sent": "I mean just a perception and 2nd perception come from learning theory and these are algorithms that can be used to solve the same problem from convex optimization.",
                    "label": 0
                },
                {
                    "sent": "So now you see that.",
                    "label": 0
                },
                {
                    "sent": "These are the bounds that we proved for the perception and Hilbert algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, and the number of mistakes they make before converging to the linear separator is.",
                    "label": 0
                },
                {
                    "sent": "Squared normal of the support vector machine solution and the update time is linear in the number of dimensions because we have to make an update that is.",
                    "label": 0
                },
                {
                    "sent": "The update of the perception and the passive aggressive is just you just add the current distance into the weight.",
                    "label": 0
                },
                {
                    "sent": "So you just linear in the number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "So modification of perception which is called 2nd order perception, has a more complicated update which is quadratic and gets you a bound which now depends on D, which is the dimension of the space where the problem leave where all the data leave and it's only.",
                    "label": 0
                },
                {
                    "sent": "It replaces factor of you with the log.",
                    "label": 0
                },
                {
                    "sent": "More sophisticated algorithms are the ellipsoid, the volumetric center, and the geometric center.",
                    "label": 1
                },
                {
                    "sent": "So the ellipsoid is a classical algorithm for linear programming an this is the number of updates it takes.",
                    "label": 0
                },
                {
                    "sent": "And you have an increase in the number or in the complexity of the update each time and the the two best algorithms for with respect to the number of mistakes at the volumetric center and the geometric center of the.",
                    "label": 0
                },
                {
                    "sent": "Of.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm not going to explain a lot about these ones because it's going to take too much time, so these are going to need actually to know the length of the solution so that this scalar U an have a higher complexity but achieve this.",
                    "label": 0
                },
                {
                    "sent": "Bound here on the number of mistakes and.",
                    "label": 0
                },
                {
                    "sent": "And this is.",
                    "label": 0
                },
                {
                    "sent": "And this is optimal.",
                    "label": 0
                },
                {
                    "sent": "Whenever you and D are fixed, so it's actually very easy to prove that you squared is an optimal bound for fixed U whenever D is is, a is free is a free parameter.",
                    "label": 0
                },
                {
                    "sent": "But if you fix both U&D then DLNU is the optimal is the smallest number of miss of updates that you can make.",
                    "label": 0
                },
                {
                    "sent": "We're finding a solution, so these are.",
                    "label": 1
                },
                {
                    "sent": "Best possible algorithms for solving this problem, and of course I. I mean, I didn't tell you I didn't tell explicitly.",
                    "label": 0
                },
                {
                    "sent": "This is a.",
                    "label": 0
                },
                {
                    "sent": "This is just a linear programming problem because you have you have you have a bunch of a bunch of sorry.",
                    "label": 0
                },
                {
                    "sent": "You have a bunch of linear constraints that you want to want to satisfy, so it's just a feasibility problem for the linear programming, and this is all the whole range of algorithms that you can use, so you can.",
                    "label": 0
                },
                {
                    "sent": "You can see you can situate perception and this other corrective update in this whole range of Al.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rims and you have difference different costs in terms of update time that you're going to pay.",
                    "label": 0
                },
                {
                    "sent": "OK. Now.",
                    "label": 0
                },
                {
                    "sent": "How much time difference?",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now let me let me talk a bit more about relationships between online learning and convex optimization so.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "In in this form in this sense, so we want to see now whether we can extend this concept of aggressive update to the case on non separable streams.",
                    "label": 0
                },
                {
                    "sent": "So basically we are taking the algorithm we saw before this Hildreth algorithm that makes an update that is completely corrective and we're going to introduce azzlack parameter that makes.",
                    "label": 0
                },
                {
                    "sent": "That let us decide that the amount of aggressiveness that we're going to use to fix to.",
                    "label": 0
                },
                {
                    "sent": "To enforce the constraint of large margin on the next example.",
                    "label": 0
                },
                {
                    "sent": "So this is an algorithm that is called passive aggressive that in with respect to perceptible, has a tuneable parameter, which is always a nuisance.",
                    "label": 0
                },
                {
                    "sent": "But in general it is if it has better performance than perception on practical problems, and it is easy to implement as perception and doesn't have any additional complexity.",
                    "label": 0
                },
                {
                    "sent": "So this is interesting because the analysis of this algorithm will bring interesting connections with.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An optimization.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me explain this so now you remember this Hidrate algorithm was just the perception with this learning rate here and the learning rate was chosen in such a way that you had.",
                    "label": 0
                },
                {
                    "sent": "Was chosen like this like this term here.",
                    "label": 0
                },
                {
                    "sent": "So now you have a variant of this algorithm that has a tuneable parameter to measure the degree of aggressiveness.",
                    "label": 0
                },
                {
                    "sent": "In the update is called passive passive aggressive.",
                    "label": 0
                },
                {
                    "sent": "I agree and the learning rate is chosen in this way as a minimum between C, which is a parameter and the same quantity as before.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, why what is the meaning of that?",
                    "label": 0
                },
                {
                    "sent": "OK, you see now.",
                    "label": 0
                },
                {
                    "sent": "This is this.",
                    "label": 0
                },
                {
                    "sent": "You can view this as the solution of this.",
                    "label": 1
                },
                {
                    "sent": "Constrained optimization problem in which, as before, this is again then one one step SVM online SVM.",
                    "label": 0
                },
                {
                    "sent": "In which again, you have a tradeoff between the distance between the solution to the previous weight and the hinge loss of the of the update.",
                    "label": 0
                },
                {
                    "sent": "But now you have introduced.",
                    "label": 0
                },
                {
                    "sent": "You have introduced this this.",
                    "label": 0
                },
                {
                    "sent": "Trade off term CK2.",
                    "label": 0
                },
                {
                    "sent": "So basically you want to you want to decide how important is to be corrective to reduce this with respect to B.",
                    "label": 1
                },
                {
                    "sent": "Close to the previous solution, so this is the bias term and this is the variance term.",
                    "label": 0
                },
                {
                    "sent": "If you if you want to.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good them way.",
                    "label": 0
                },
                {
                    "sent": "And you have a similar update.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is this if you have?",
                    "label": 0
                },
                {
                    "sent": "If you put the squared hinge loss here, so this is like this is the analogon of the SVM with squared hinge loss.",
                    "label": 1
                },
                {
                    "sent": "One step again.",
                    "label": 0
                },
                {
                    "sent": "OK, so now you have do 2 updates here.",
                    "label": 0
                },
                {
                    "sent": "You have a way of trading off aggressiveness, so now you can think of using this parameter here to be able to analyze the performance of this and that on arbitrary streams that are not necessarily separable.",
                    "label": 0
                },
                {
                    "sent": "OK, so the presence of this parameter should help you to deal with the fact that you cannot strictly enforce a large margin on each example because the whole stream might not be clearly separable.",
                    "label": 0
                },
                {
                    "sent": "So, so there's no point in enforcing this this merging.",
                    "label": 0
                },
                {
                    "sent": "Insisting on having such a large margin on a no separable on non separable stream, so that's why you are introducing this term here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good so.",
                    "label": 0
                },
                {
                    "sent": "Now the interest in doing this is that you can.",
                    "label": 0
                },
                {
                    "sent": "Again you can related related to the analysis of support vector machines in the following ways so.",
                    "label": 0
                },
                {
                    "sent": "This is a primal, objective function of support vector machines.",
                    "label": 0
                },
                {
                    "sent": "So again in general, not necessarily the separable data set.",
                    "label": 0
                },
                {
                    "sent": "The support vector linear classifier is the minimizer of the squared norm plus the sum of the slacks city, where this lacks the amount by which the solutions fail to achieve, fails to achieve the margin of one of each example.",
                    "label": 0
                },
                {
                    "sent": "OK, so the SVM is in general is minimizing this functional here subject to these constraints?",
                    "label": 0
                },
                {
                    "sent": "OK, so we call this.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "SVM, APTA, and you can.",
                    "label": 0
                },
                {
                    "sent": "You can define the dual LaGrange function of this primal objective, which is something that gives rise to the well known quadratic optimization problem solved by any SVM package.",
                    "label": 1
                },
                {
                    "sent": "So now you introduce a LaGrange multiplier for each linear constraint and now you have an expression function D that depends on this on these LaGrange multipliers and has this form.",
                    "label": 0
                },
                {
                    "sent": "Ann, you have a constant box constraint on the LaGrange multipliers of this thing here.",
                    "label": 0
                },
                {
                    "sent": "Now this is a function of of any choice of the T LaGrange multipliers for the constraints.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "K. And by duality, actually you can prove that.",
                    "label": 0
                },
                {
                    "sent": "By weak duality, any choice of the LaGrange multipliers will be a lower bound on the value of the optimal value of the primal objective.",
                    "label": 0
                },
                {
                    "sent": "And by strong duality, the best optimal choice of the LaGrange multipliers will.",
                    "label": 0
                },
                {
                    "sent": "We let give a value of the dual LaGrange function which is equal to the value of the primal optimal.",
                    "label": 0
                },
                {
                    "sent": "So solving the dual an optimal solution and the dual provides an optimal solution for the primer OK, but in general any solution for the dual is the lower bound for the solution of the primary, so we're going to use this dual here to analyze the passive aggressive.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's look.",
                    "label": 0
                },
                {
                    "sent": "It's actually very easy.",
                    "label": 0
                },
                {
                    "sent": "So this is was into this kind of a nicer introduced by Schwartzenbur Amsinger last year.",
                    "label": 0
                },
                {
                    "sent": "So now let's look at the SVM dual.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Was that function over there?",
                    "label": 0
                },
                {
                    "sent": "OK, if you set all of them to 0 then this is cool.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "0.",
                    "label": 0
                },
                {
                    "sent": "Which is what I wrote there, and since that is zero, you can write it.",
                    "label": 0
                },
                {
                    "sent": "You can write this as a telescoping difference.",
                    "label": 0
                },
                {
                    "sent": "Trivially.",
                    "label": 0
                },
                {
                    "sent": "Just look at that and you simplify and then you have that initial term is 0.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's equality.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is of that.",
                    "label": 0
                },
                {
                    "sent": "So now you can look at these differences here.",
                    "label": 0
                },
                {
                    "sent": "OK, you can just go there.",
                    "label": 0
                },
                {
                    "sent": "You have the function.",
                    "label": 0
                },
                {
                    "sent": "The dual the dual you have it, you have it.",
                    "label": 0
                },
                {
                    "sent": "So you compute out this difference and you get this expression here.",
                    "label": 0
                },
                {
                    "sent": "OK, where you use this notation to denote this thing here?",
                    "label": 0
                },
                {
                    "sent": "OK, you just have to believe me, it's you.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to do it.",
                    "label": 0
                },
                {
                    "sent": "OK so you have an expression for these differences of duals which.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why are you interested in it?",
                    "label": 0
                },
                {
                    "sent": "It's not clear yet, but now let's look at this passive aggressive online algorithm that does this tunable aggressive update.",
                    "label": 0
                },
                {
                    "sent": "So again, I told you that this update corresponds to solving this primal objective function subject to that constraint here.",
                    "label": 0
                },
                {
                    "sent": "So I just rewrote the same thing introducing this select variables instead of the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "This is as this is equal and equivalent to writer wrote before using the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "But now I replace the hinge loss by select variables, same thing.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the online algorithm, so you can compute the do LaGrange function for this specific.",
                    "label": 0
                },
                {
                    "sent": "Primal objective defined on a single example now, and you have this expression here but.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is what exactly what we wrote before, so now it turns out that the update, not surprisingly, that the learning rate we chose is the maximizer of this dual LaGrange function for the primal objective of passive aggressive.",
                    "label": 1
                },
                {
                    "sent": "And so we can call it Alpha T because we recognize that this entity is the optimal choice for the LaGrange multiplier of this guy.",
                    "label": 0
                },
                {
                    "sent": "Gay.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, another thing is that.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This quantity for.",
                    "label": 0
                },
                {
                    "sent": "Any choice of Alpha, so in particular also for Alpha T. This quantity here is equal to what we.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before for the.",
                    "label": 0
                },
                {
                    "sent": "For the SVN so.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we take if you take any sequence of LaGrange multipliers A1 through Alpha T, you can write these qualities here for any sequence.",
                    "label": 0
                },
                {
                    "sent": "So in particular also for the sequence chosen by the passive aggressive.",
                    "label": 0
                },
                {
                    "sent": "Note that these are not the optimal LaGrange multipliers for the SVM solution, just obitury LaGrange multipliers.",
                    "label": 0
                },
                {
                    "sent": "But again, I'm using the fact that this thing here for any choice of the LaGrange multipliers is a lower bound on the VMS solution on the value of the CL.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Primal objective OK, so now you know that the sum of these things because of telescoping.",
                    "label": 0
                },
                {
                    "sent": "The sum of these things?",
                    "label": 0
                },
                {
                    "sent": "Is at most the value of the SVM objective primal objective that one so now you see this is something that reminds you of the upper bound.",
                    "label": 0
                },
                {
                    "sent": "The perception upper bound.",
                    "label": 0
                },
                {
                    "sent": "You have the same quantities squared normal view and the sum of the hinge losses of you.",
                    "label": 0
                },
                {
                    "sent": "OK, and here you have this match.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is this some over there?",
                    "label": 0
                },
                {
                    "sent": "So now you can use this to prove mistakes bound for passive aggressive.",
                    "label": 1
                },
                {
                    "sent": "How do you do it?",
                    "label": 1
                },
                {
                    "sent": "You basically you prove a lower bound improve that.",
                    "label": 0
                },
                {
                    "sent": "This quantity.",
                    "label": 0
                },
                {
                    "sent": "Has to increase by at most a certain amount every every time the algorithm makes a mistake.",
                    "label": 0
                },
                {
                    "sent": "Since it cannot discount, it is upper bounded by this, which is constant.",
                    "label": 0
                },
                {
                    "sent": "For any, any sequence of any given sequence of examples, you can use this to prove.",
                    "label": 0
                },
                {
                    "sent": "An upper bound on the number of mistakes made by passive aggressive in terms of this.",
                    "label": 0
                },
                {
                    "sent": "Quantity here and now.",
                    "label": 0
                },
                {
                    "sent": "See is the same see parameter used by the passive aggressive update.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in particular, you can again assume.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For simplicity that instances of unit norm.",
                    "label": 0
                },
                {
                    "sent": "You can prove that every time passive aggressive makes an update, the dual is going to increase by at least this match.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now what you do now, you can prove that the indicator function of a mistake for passive aggressive at 20 multiplied by this.",
                    "label": 0
                },
                {
                    "sent": "Is at most this thing over here 'cause?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Because this is going to be at least one.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is going to be at least this is an upper bound on the mistake indicator function.",
                    "label": 0
                },
                {
                    "sent": "And the claim over there showed that this is upper bounded.",
                    "label": 0
                },
                {
                    "sent": "You see it here by twice 'cause we have the true do it twice the dual.",
                    "label": 0
                },
                {
                    "sent": "OK, so now you have a relationship between this number of mistakes and the dual, but you know that they do the sum of the duals is upper bounded by the SVM at primal objective function.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what you ever the end is this bound over here, which is abandoned.",
                    "label": 0
                },
                {
                    "sent": "The sum of the number of mistakes made by the passive aggressive in terms of the value of the.",
                    "label": 0
                },
                {
                    "sent": "SVM function with.",
                    "label": 0
                },
                {
                    "sent": "You have some additional factors here and you see now the role played by the trade off parameter C. OK so.",
                    "label": 0
                },
                {
                    "sent": "Of course you don't know the value of this for any any sequence of examples, so it's not clear how you should tune C. But it I mean, and also this bound is not.",
                    "label": 0
                },
                {
                    "sent": "Particularly, is not more always better than the perception bound, which is another disappointing fact, so it's not a very strong proof for the behavior of the passive aggressive, even though in practice for reasonable choices of C, this algorithm, passive aggressive performs better than perceptron in terms of the analysis, this doesn't reveal that this is this bound is actually not.",
                    "label": 0
                },
                {
                    "sent": "It's many times it might be worse than the perception bound, but is interesting to show the fact that you used this.",
                    "label": 0
                },
                {
                    "sent": "Duality in optimization relating the SVM solution with the with the dual dual function of the passive aggressive to prove it, and this is essentially a way this is a kind of route that you can follow to prove mistake bounds for many variants of algorithms, and this is also an easy way to prove mistake bounds for versions of.",
                    "label": 0
                },
                {
                    "sent": "Algorithms such that the perception and the passive aggressive that are modified to predict structured outputs.",
                    "label": 0
                },
                {
                    "sent": "So for the case in which you don't have a binary classification problems, but your labels are discrete, combinatorial objects like sequences or trees or more complicated combinatorial objects.",
                    "label": 0
                },
                {
                    "sent": "So it's a versatile framework that can be used to prove.",
                    "label": 0
                },
                {
                    "sent": "And related mistake bounds for online algorithms.",
                    "label": 0
                },
                {
                    "sent": "And it is nice because it is suggestive suggestive in terms of relationships within the local optimization problems solved by the online algorithm that looks at one example of the time and the global optimization problem that is solved by batch algorithms like the support vector machine that looks at the whole set of examples at once.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is to close this part OK and now in the remaining time, let's see.",
                    "label": 0
                },
                {
                    "sent": "Maybe I will, I will talk about bit about this one, which is some some work I've done recently and then I will skip this last two parts, OK?",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I still do like this.",
                    "label": 0
                },
                {
                    "sent": "OK, I don't know whether you've seen kernels so far.",
                    "label": 0
                },
                {
                    "sent": "Everything kernels in learning.",
                    "label": 0
                },
                {
                    "sent": "Anything?",
                    "label": 0
                },
                {
                    "sent": "Did anybody talk about cognition learning no.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is not.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So maybe it was not a good choice.",
                    "label": 0
                },
                {
                    "sent": "This one if I had to introduce kernels in 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "So maybe let me let me move to the.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who knows kernels?",
                    "label": 0
                },
                {
                    "sent": "It was discussed a bit, so you know a little bit of it.",
                    "label": 0
                },
                {
                    "sent": "OK, so I.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can I can, maybe this is not so depending on knowing intimately what the kernel is, but it certainly helps.",
                    "label": 0
                },
                {
                    "sent": "So let's say that the kernel is is a feature map that Maps the original space in which inputs are expressed in a complicated space.",
                    "label": 0
                },
                {
                    "sent": "Complex, high dimensional space which is reproducing.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kernel Hilbert space and you know that the kernel is an object that implements the inner product between the images of distances in this.",
                    "label": 0
                },
                {
                    "sent": "An complex feature space.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "K so so kernels are a way to represent complicated decision surfaces for, for instance, for binary classification problem.",
                    "label": 0
                },
                {
                    "sent": "So decision services that are not necessarily hyperplane in the case of linear classification, but because of the kernel machinery, you can use a linear learning algorithm to learn this complicated surface because they can represent it as hyperplanes in the reproducing in the.",
                    "label": 0
                },
                {
                    "sent": "Features based induced by the kernel in this reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "So now we know that all these algorithms, the weights of all these algorithms are expressed as linear combination of supports just like support vector machines.",
                    "label": 0
                },
                {
                    "sent": "But now the supports are just the elements of the stream where the algorithm is take.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so whenever you have a, you have your classifiers expressing in such way.",
                    "label": 0
                },
                {
                    "sent": "Then you can learn the classifier in the reproducing kernel Hilbert space by using the kernel to compute predictions.",
                    "label": 1
                },
                {
                    "sent": "So all you have to remember is the set of supports that make up your weight.",
                    "label": 0
                },
                {
                    "sent": "So whenever you have to make, you have to make a prediction of of data X.",
                    "label": 0
                },
                {
                    "sent": "You map implicitly map the data into the feature space with five.",
                    "label": 0
                },
                {
                    "sent": "But the computing this prediction just amounts because of the kernel property, and because the way the way the weight is defined just amounts to compute the sum of labels times the kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, so all you have to remember is just the set of I that make up your weight W and you have to have an Oracle to compute your kernel function, which is a piece of code in practice.",
                    "label": 0
                },
                {
                    "sent": "So this is a way in which.",
                    "label": 0
                },
                {
                    "sent": "All these algorithms I've showed you can be implemented.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a kernel space, for instance, this is a perception.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Etc you start with.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The empty set to support examples you read next systems and then you predict just by taking all the supports you have in your list.",
                    "label": 0
                },
                {
                    "sent": "Computing the inner product, the kernel using the kernel, you compute the inner product of these guys in the current guy that you have to predict.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you then have turn obtain a true label, and then if you make a mistake then you can store then you support which is the instance multiplied by the label in the list L and that's it.",
                    "label": 0
                },
                {
                    "sent": "So basically this is implementation of the kernel of the perceptron.",
                    "label": 0
                },
                {
                    "sent": "In the run in the inner reproducing kernel Hilbert space, reproducing kernel space generated by the kernel key, so you're now running.",
                    "label": 0
                },
                {
                    "sent": "You're running the same algorithm, but in this very complex high dimensional feature space.",
                    "label": 0
                },
                {
                    "sent": "So the only thing that changes and is that the algorithm Now the complexity of the algorithm, is now not expressed the in terms of the dimension of the original space.",
                    "label": 0
                },
                {
                    "sent": "But this is expressed in terms of the size of this list.",
                    "label": 0
                },
                {
                    "sent": "So you see now and update any evaluation, any prediction is going to cost you.",
                    "label": 0
                },
                {
                    "sent": "The length of this list so and the length of the list will grow as you make more mistakes.",
                    "label": 0
                },
                {
                    "sent": "So now you're the number of mistakes is ruling your the time complexity of your prediction and the space.",
                    "label": 0
                },
                {
                    "sent": "Also, because we have to keep all these, you have to keep this list stored in memory.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, the mistakes bound mistake bounds will hold.",
                    "label": 0
                },
                {
                    "sent": "We're not competing now with the best linear classifier, but you're now competing with the best function in the reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "So all the all of your bounds are lifted.",
                    "label": 0
                },
                {
                    "sent": "Two elements that live in the reproducing kernel Hilbert space computed by the kernel.",
                    "label": 0
                },
                {
                    "sent": "So it's a huge power that you gain by this very simple change in the code OK.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now one interesting thing that you run in practical issue that you run into when you use kernel based learning with Prince with perceptions is that you know you run the algorithm over stream.",
                    "label": 0
                },
                {
                    "sent": "You keep on making mistakes and your list is going to grow, grow, grow, grow, grow and especially if the data is very noisy even for the kernel that are used using then this list grows.",
                    "label": 0
                },
                {
                    "sent": "And to grow an unlimited Lee.",
                    "label": 0
                },
                {
                    "sent": "OK, so in practice what you do is that you're keeping a cache with fixed memory cache of size B for instance, so you are willing to spend at most time linear in B on each day update, so that's why you're keeping at most B supports, and whenever you're making more mistakes, then you have to decide to throw away some support from the list.",
                    "label": 1
                },
                {
                    "sent": "Of support that you're keeping to make room for the new mistake that the new supports that you have gained by making a new mistake.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now interested.",
                    "label": 0
                },
                {
                    "sent": "The interesting question is that what?",
                    "label": 0
                },
                {
                    "sent": "How is this going to affect the performance of your algorithm?",
                    "label": 0
                },
                {
                    "sent": "So if you don't remember all of your mistakes?",
                    "label": 0
                },
                {
                    "sent": "How can you prove something about the performance of the algorithm so it's it's easy to prove a lower bound on a very simple lower bound, so if you.",
                    "label": 0
                },
                {
                    "sent": "Now again your.",
                    "label": 0
                },
                {
                    "sent": "Suppose let's look at let's for.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Implicitly suppose that.",
                    "label": 0
                },
                {
                    "sent": "Colonel here is an inner product.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's just this kernel is implementing the inner product between the vectors in the original space, so you are not gaining anything here, but for the sake of simplicity you're still running the algorithm in the in with in the kernel version, even though the kernel implements.",
                    "label": 0
                },
                {
                    "sent": "Linear just implements the inner product between the.",
                    "label": 0
                },
                {
                    "sent": "To Easter, since in the original space.",
                    "label": 0
                },
                {
                    "sent": "So now this algorithm is.",
                    "label": 0
                },
                {
                    "sent": "This is the inner product between VXT is just the perception algorithm in the original input.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Face no difference.",
                    "label": 0
                },
                {
                    "sent": "So now you're competing against again as before.",
                    "label": 0
                },
                {
                    "sent": "You're competing against linear classifiers, but still you're keeping a bounded number of supports in your list.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Before you could compete with any U and you had a trade off in the bound within the hinge loss of yuan, it's squared norm.",
                    "label": 0
                },
                {
                    "sent": "You remember this trade off.",
                    "label": 0
                },
                {
                    "sent": "Now it's easy to show that you cannot compete with the huge U whose norm is larger than the roughly the square root of the number of supports.",
                    "label": 0
                },
                {
                    "sent": "So basically.",
                    "label": 0
                },
                {
                    "sent": "If you're you is.",
                    "label": 0
                },
                {
                    "sent": "If you have data set which is nearly separable by some you which is longer than that, then there is a sequence of examples that is perfectly classified by the zoo on which your algorithm that only remembers be supports is going to make a mistake, at which time the proof is is very, very easy.",
                    "label": 1
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not going to make it, but it's easy.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You have this, you have this necessity condition, so you need at least this many supports to compete against any you have length you.",
                    "label": 0
                },
                {
                    "sent": "Otherwise there's no lower bounds prevents you.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Proving anything interesting.",
                    "label": 0
                },
                {
                    "sent": "So now the question is, can you compete against any you which is ever so slightly shorter than this bound here?",
                    "label": 0
                },
                {
                    "sent": "So basically is.",
                    "label": 0
                },
                {
                    "sent": "Just one plus epsilon, longer than the limit you squared.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there is a very simple algorithm that does that, and we call it randomized budget perception.",
                    "label": 0
                },
                {
                    "sent": "And this this algorithm is again.",
                    "label": 0
                },
                {
                    "sent": "Now as a size B of.",
                    "label": 1
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cash for the support systems with an empty cache an.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like before and loops over examples and makes predictions just like before using the examples from the list.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But whenever.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Makes a mistake then, if.",
                    "label": 0
                },
                {
                    "sent": "The list is already full so that you have already be supports in your list.",
                    "label": 0
                },
                {
                    "sent": "Then it throws away random support from the list to make room.",
                    "label": 1
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Room for the new one and then add.",
                    "label": 0
                },
                {
                    "sent": "The new one to the list.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "Very simple, deceivingly simple policy for a victim.",
                    "label": 0
                },
                {
                    "sent": "All the supports to make room for new ones so, but indeed, because your analysis is adverse aerial so you're bound is going to be you.",
                    "label": 0
                },
                {
                    "sent": "One you're bound to hold for any individual sequence of examples OK?",
                    "label": 0
                },
                {
                    "sent": "A simple random policy for managing your cash turns out to be opt in.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimal in the sense that.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "You can achieve this.",
                    "label": 0
                },
                {
                    "sent": "Bound here so you can compete with use that are arbitrarily close to the limit to EU squared limit, beyond which you cannot go, and the bound on a number of mistakes is going to that you're going to incur into is going to scale by this quantity here, so as your epsilon goes to 0, meaning that you're approaching this threshold that after which you are going to make in the worst case, a mistake at every step.",
                    "label": 0
                },
                {
                    "sent": "Then you're bound.",
                    "label": 0
                },
                {
                    "sent": "You're going to pay this in your bound.",
                    "label": 0
                },
                {
                    "sent": "The bond is going to blow up according to the inverse power of that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this gives you a precise relationship.",
                    "label": 0
                },
                {
                    "sent": "I mean it's we don't know whether this is optimal, so you can whether you can get any better relationship between the parameter epsilon and the actual number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "But it's definitely give you an idea of that.",
                    "label": 0
                },
                {
                    "sent": "You're bound is not going to crash as an effect of using a bounded number of supports.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to close.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This talk by showing a couple of of of experimental results.",
                    "label": 0
                },
                {
                    "sent": "So here you have.",
                    "label": 0
                },
                {
                    "sent": "A document classification problem.",
                    "label": 0
                },
                {
                    "sent": "This is actually the average of, I think, 50 classification binary classification problems on document, categorized according to different news categories.",
                    "label": 0
                },
                {
                    "sent": "So for each category you have a binary classification problem.",
                    "label": 0
                },
                {
                    "sent": "You're running a 50 perceptrons in parallel, and you're making the average, so this is the.",
                    "label": 0
                },
                {
                    "sent": "An average number of prediction mistakes on I think were 40,000 examples.",
                    "label": 0
                },
                {
                    "sent": "A stream of 40,000 examples.",
                    "label": 0
                },
                {
                    "sent": "This is the performance.",
                    "label": 0
                },
                {
                    "sent": "This is the error of the perceptron which.",
                    "label": 0
                },
                {
                    "sent": "Uses an unlimited budget size, so this is the budget size and this is a completely stationary problem in the sense that.",
                    "label": 0
                },
                {
                    "sent": "Every stream, every binary problem has the same.",
                    "label": 0
                },
                {
                    "sent": "Is is so basically every perception is facing the same binary classification problem for the whole length of the stream.",
                    "label": 0
                },
                {
                    "sent": "So this data, this data set is stationary in the sense that you are not changing the classification problem as the perception runs.",
                    "label": 0
                },
                {
                    "sent": "Then your as your budget to decrease decreases then your mistake is going to rate mistakes is going to increase correspondingly and these are different versions of this randomized budget perception algorithm that you saw is around here.",
                    "label": 0
                },
                {
                    "sent": "It's over here.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is OK.",
                    "label": 0
                },
                {
                    "sent": "This is what you expect.",
                    "label": 0
                },
                {
                    "sent": "OK, there is some degradation in performance.",
                    "label": 0
                },
                {
                    "sent": "As you decrease the budget side, but this is now, let's, let's suppose that the data is highly nonstationary.",
                    "label": 0
                },
                {
                    "sent": "So basically you are changing the binary classification problem.",
                    "label": 0
                },
                {
                    "sent": "As you run each person as you run a perception, so every perception is fit is facing a classification problem where the meaning of the positive and negative examples is being changed after awhile.",
                    "label": 0
                },
                {
                    "sent": "So for instance, for awhile positive was a sport news item and then after a while becomes a politics news item and then a finance news item.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now you see that again this is the over here.",
                    "label": 0
                },
                {
                    "sent": "The performance of the perception is here.",
                    "label": 0
                },
                {
                    "sent": "Which is higher is worse than before because it's highly non stationary problem.",
                    "label": 0
                },
                {
                    "sent": "But now you see that for the randomized budget budget perception here and some other versions of them of it actually have there's no degradation in performance, because remember fewer examples from the past helps.",
                    "label": 0
                },
                {
                    "sent": "If the data is highly non stationary.",
                    "label": 0
                },
                {
                    "sent": "So this memory boundedness feature helps in the case the.",
                    "label": 0
                },
                {
                    "sent": "Problem is highly non stationary so you are trying to track an the best linear classifier which is changing from time to time.",
                    "label": 0
                },
                {
                    "sent": "So actually you see that in certain cases some versions of this budgeted perceptual actually perform better than the perception itself.",
                    "label": 0
                },
                {
                    "sent": "Because of this nationality.",
                    "label": 0
                },
                {
                    "sent": "But then if you.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use a second order, a second order perception version of the budget algorithm.",
                    "label": 0
                },
                {
                    "sent": "So better perception algorithms run with the budget.",
                    "label": 0
                },
                {
                    "sent": "Then it's actually you can actually see that this memory boundedness helps in the sense helps in a concretely significantly so miss that.",
                    "label": 0
                },
                {
                    "sent": "If the budget goes beyond a certain amount, then that the number of mistakes is.",
                    "label": 0
                },
                {
                    "sent": "Number of mistakes is is really really small, are significantly smaller than the number of mistakes made by the perception that hasn't a limited budget.",
                    "label": 0
                },
                {
                    "sent": "And again because these algorithms are able to exploit the combination of a non stationary target and the bounded memory more effectively.",
                    "label": 0
                },
                {
                    "sent": "So this tells you that this budget this memory boundedness.",
                    "label": 0
                },
                {
                    "sent": "Feature is beneficial in terms of keeping the space complexity bounded, so the time complexity of the algorithm bounded by constant.",
                    "label": 0
                },
                {
                    "sent": "But it's also very can be very useful in the case in which you are.",
                    "label": 0
                },
                {
                    "sent": "Prediction problem is is it hard.",
                    "label": 0
                },
                {
                    "sent": "The nonstationary prediction problem.",
                    "label": 0
                },
                {
                    "sent": "So in this case the two things Dennis stationarity and the memory boundedness go together in a in a in a profitable way.",
                    "label": 0
                },
                {
                    "sent": "So you can actually save on the number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "OK I think I will stop here, thank you.",
                    "label": 0
                }
            ]
        }
    }
}