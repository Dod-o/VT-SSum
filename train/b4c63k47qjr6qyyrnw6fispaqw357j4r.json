{
    "id": "b4c63k47qjr6qyyrnw6fispaqw357j4r",
    "title": "High-dimensional learning with deep network contractions",
    "info": {
        "presenter": [
            "St\u00e9phane Mallat, Applied Mathematics - CMAP, \u00c9cole Polytechnique"
        ],
        "published": "Oct. 29, 2014",
        "recorded": "September 2014",
        "category": [
            "Top->Computer Science->Digital Signal Processing",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Information Theory",
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/sahd2014_mallat_dimensional_learning/",
    "segmentation": [
        [
            "Yes, I'm going to try to give a slightly different view of after this very beautiful talk that Yan gave on these."
        ],
        [
            "Deep networks, so the problem is a high dimensional classification problem, at least in the beginning.",
            "You have data with labels, which are the why of I why I you have signals which lives in very high dimensional space and I will mostly be speaking about images and you would like to compute the label of your signal.",
            "And of course the problem is that the natural way to compare signals which are Euclidean distance are very inappropriate way.",
            "In very high dimension, to measure the notion of similarity, Euclidean distance of two image gives you no information about how similar they are.",
            "So the standard approach that has been developed in learning in general is to try to find a representation fire effects which you can think of a big vectors of features, and so that if you now apply a Euclidean distance over this big feature vector, then for some miracle it gives you.",
            "Relevant notion of similarity, and if that's the case, then you can try to do a simple classification with a simple linear classifier.",
            "So you divide your space with just a hyperplane and you get very good estimation of your classes.",
            "So that's the standard approach and the core problem in all this is how to define this miracle function, which is giving going to give you this notion of similarity with the signal.",
            "Legion distance, so that's where deep learning comes in."
        ],
        [
            "And the work of Jan says, OK, I have an amazing way to do that.",
            "You take your signal and he explained all that he does cascade of convolution, nonlinear pooling and so on.",
            "Then he gets a big feature vector and out of this big feature vector you just apply a simple linear classifier.",
            "You can do more sophisticated but linear classifier sufficient an you obtain amazing classification results with state of the art results on huge number of databases.",
            "So if you think that this depth, you can think of it as a scale variable, because as you go you get bigger and bigger convolution, so you get information over larger and larger domain of your signal and all the parameters in your network or learned with this supervised learning algorithm.",
            "Now there is something interesting that has been observed is that if you learn.",
            "Well, your classifier, your network and that was done in particular for this Alex Net the same network.",
            "Well workout for many many very different database, so I'm some sense it looks like there's something a bit generic about this network and it's not so much tuned to the specific database it's being used for.",
            "It has been trained on and these data network have been trained on image NET as was mentioned by Jan which have literally millions of images."
        ],
        [
            "So the goal of the talk will be to make a few outrageous claims.",
            "The first one is you don't need to learn for images.",
            "For audio, I strongly believe that you don't need to learn now an.",
            "To achieve the kind of results, beautiful results that Jen have been showing and the reason is that.",
            "What is behind all that is the geometry of the image is the geometry of the world, and that's the key element that you need to run.",
            "The second outrageous claim will be to say that in fact, these things provide coders you can encode.",
            "They carry most of the signal information and you can recover.",
            "I'll be showing that this signals from these very strange coders.",
            "The last one will be related to physics is that I do think that these techniques are very close to what's happening in physics, and I'll be showing that through example of learning of quantum chemistry problems.",
            "So that will be the goal is to try in ways or not in order to substantiate these claims and the first one being the longer one to substantiate so."
        ],
        [
            "Let me begin.",
            "OK, so the problem is try to get a representation which is going to give you a good approximation of the underlying metric of similarity.",
            "The problem is what is the metric of similarity for image for an image.",
            "If you have a simple shape like that low dimensional image, a natural metric will be a metric of deformation.",
            "Why is this three closer than this one?",
            "Because you can deform it with a small deformation, whereas it will require much bigger information to reach the five.",
            "And these two five are much closer than they are to three, and this idea has been very much that studied.",
            "These deformation metrics like people like hernander, true value Ness and the basic idea is how do you deformity formation?",
            "Well, you take your signal and you work this space spatial domain and then the distance will basically be given by how big should be the warping when you try to match one on the other one.",
            "And how big is the L2 error when you try to deform one?",
            "Into the other one, because the deformation is not going to give you an ideal results.",
            "OK, so that's typically the kind of deformation metric which have been used very much for that kind of simple shape, and in particular this metric are invariant to translation because if you just translate, the deformation is going to be 0 here.",
            "OK, so that's simple here."
        ],
        [
            "What if you deal with real high dimensional image?",
            "So let's say textures.",
            "If you deal with the texture, one of the difficulties these two textures.",
            "You'll say they are the same.",
            "So what metric over these realization of stationary processes?",
            "OK, you can say my distance should be controlled by this deformation.",
            "Why?",
            "Because if I take my texture and I deform it, you still see a very similar texture.",
            "The problem is that the reverse inequality is totally wrong.",
            "The reverse inequality is wrong because you can have two textures like these two over there.",
            "This and this.",
            "They are very far away from that point of view, but you would like to say that the distance is zero.",
            "OK, you cannot deform a white noise into another white noise, but you can say that they are identical from the point of view of stochastic processes."
        ],
        [
            "What about much more complex images?",
            "Well, you can say still you want to maintain this environments to translation and stability to deformation, but what else?",
            "And that will be the underlying questions.",
            "How do you build metrics which are generic, not just for low dimension but for high dimensional images?"
        ],
        [
            "OK, so let me just for now stick with what I know.",
            "I would like to build a metric which is equivalent.",
            "You clean metric to this similarity metric and the only thing I don't this similarity metric is that if I have a small deformation, the distance should be small.",
            "So what does that mean?",
            "This distance it really means two things.",
            "First of all, you want to be stable to additive components.",
            "Small addition should not modify mature signals, so stability now to why the deformation is identity.",
            "It's controlled by this first term.",
            "The second thing you want to be stable by deformation if X prime is a small deformation of X.",
            "Then the distance should be of the order of the deformation, so small deformation should give you close signal.",
            "And that in particular implies invariants to translation.",
            "If you just have a translation, the distance is going to be 0.",
            "OK, so that will be the beginning point problem.",
            "You don't get that kind of environment and stability with usual mathematical tools like Fourier Canonical invariants.",
            "And that's where the."
        ],
        [
            "Network coming, so I'm going to use wavelets.",
            "Why wavelets because to get stability to the formation you need to do a separation of scales OK and we're going to do it in 2D with a simple wavelet, which is like a Gaussian modulated by a sine wave.",
            "OK to complex function.",
            "This function you have real part imaginary part.",
            "You rotate it.",
            "These are all rotation and you scale it.",
            "These are all the scale OK in 48.",
            "That's how it looks like.",
            "And then when you rotate you cover and analysts.",
            "When you dilate, you cover the next sentences in full.",
            "So what is the wavelet transform?",
            "Justin averaging with the low frequency?",
            "And then the bandpass component.",
            "All the wave let components in the different frequency band.",
            "And if you construct well your wave leads, you get a conservation of norm.",
            "There is no tonality here, it's very redundant, but this is easy to get."
        ],
        [
            "OK, so that's how it looks like.",
            "You take an image.",
            "You can build that with a fast filterbank algorithm, so that's the different wavelet coefficient at find scale, and the average and then from the average U sub decompose with filters wavelength and the average the wave leads and the average wavelength and the average up to the end.",
            "And this is the final averaging of your image and all the wavelet coefficients."
        ],
        [
            "Good, now you would like to have which is something which is translation environment.",
            "So let me show it in one dimension because it's easier, so translation invariants.",
            "How can you do it?",
            "You just average your signal over domain to the J is going to be locali translation invariant.",
            "You can do that with a wavelet transform and then you have the wavelet coefficients.",
            "If you want it to be fully translation invariants then you just like 2 to Jake, one Infinity and that you just get the average which is completely translation invariant.",
            "The problem is, what are you going to do with the wavelet coefficients?",
            "They are absolutely not translation invariant.",
            "You can do the same thing that for Fourier transform.",
            "First kill the phase.",
            "If you kill the phase, you're going to get the envelope, which is more regular and therefore slightly more translation environment.",
            "Next stage you want to make it really translation invariant, so let's average this modulus.",
            "You get something very regular, and now it's going to be translation invariant.",
            "How do you do that?",
            "You applied and you wavelet transform over.",
            "This way they transform modulus.",
            "So you took that, you compute its second wavelet transform.",
            "You get the average and the next set of wavelet coefficients.",
            "The idea is that we're going to iterate on that, so."
        ],
        [
            "Let's do it first.",
            "Wavelet transform.",
            "That's the scale variable is going to correspond to the depths in our deep network.",
            "Second, wavelet transform.",
            "I want to take this, make it invariant.",
            "That's the average and all the wavelet coefficient.",
            "But I want to do it for every wavelets images.",
            "So I'm going to do it for this one.",
            "This one OK, all of them then this this is not translation invariant, I have to bring it back down here.",
            "So I'm going to reapply a wavelet transform third wavelet transform where the coefficient and that's where they arrive at the bottom.",
            "This hasn't yet arrived to the bottom.",
            "I have to apply a force wavelet transform and slow.",
            "So this is the network.",
            "Now there's something important to realize if you think of it that way.",
            "This is the depth this is scared, but observe it's a product of wavelet transform.",
            "The first one, the second Red 1/3 force, and so.",
            "If you want to analyze it mathematically rather than looking that way, we're going to look at it that way so."
        ],
        [
            "Let me represent the same thing first.",
            "Order this is the first wavelet transform.",
            "What did we do we?"
        ],
        [
            "RE applied the second wavelet transform, this first one we got the average, that was whether the bottom and the next layer of wavelet coefficient we."
        ],
        [
            "Re apply a third nonlinearity with their transform.",
            "That's the average the next play.",
            "That's how you can view the problem, OK?"
        ],
        [
            "If you do that, what are you doing?",
            "You are cascading operator.",
            "You are getting a set of invariant coefficients, which are your averages.",
            "First, the average of the signal.",
            "The average of the wavelet coefficients, 2nd order, and so on.",
            "If you let Jay go to Infinity, these coefficients are going to converge to L1 norms.",
            "So basically you are building a representation of your signal as a series of L1 norm.",
            "These are obviously invariant to translation.",
            "Here you have very few of them, just log of the size of your signal.",
            "Here you have squared more log N squared log Cuban so.",
            "This will be the representation.",
            "Now the first important thing is that the energy of the last layer is going to converge to zero.",
            "Why?",
            "Because you're compressing compressing your operator an all the energy is going to go out as an invariant as a low frequency information.",
            "This is not the depths of your deep network.",
            "This is the number of nonlinearity which is applied in the deep network."
        ],
        [
            "Second property, let's look at this operator.",
            "It's a cascade of this.",
            "Preserve the norm, the absolute value.",
            "It's a contractive operator, so it's a cascade of contractive operators.",
            "So first thing you're going to get a contractive operator, that's obvious, so the distance in the scattering domain is always smaller than the distance between the original two element will get the L2 stability that we need.",
            "Second thing, in fact, you preserve the norm.",
            "You contract, but the norm is preserved.",
            "Third thing, the most important, if now you are deforming your signal.",
            "If you look at the distance because we did this scale separation and wave, let's nearly commute with deformation, you can prove that the distance is of the order of the deformation.",
            "Plus a component which is the maximum translation multiplied by the scale.",
            "Because you only localy translation environment, if you let Jay go to Infinity, this disappear and you exactly get what you wanted, which is you are stable to deformation and stable in out OK."
        ],
        [
            "So what can you do with that?",
            "So that's the work of 1.9.",
            "The first kind of thing that he did quite some time ago for his PhD was to try to see what's happening for classification.",
            "So if you have a simple classification problem like MNIST, the digits are deformed.",
            "It's not surprising that you are going to capture well the problem, and if you just, sorry do.",
            "Scattering transform in the simple linear classifier you get state of the art result of 0.4%.",
            "But nowadays who cares about them list OK.",
            "Classification of texture.",
            "This is much more challenging.",
            "Here you are going to do a fully invariant representation, so you are going to make something which is completely translation invariant to.",
            "Ajay is essentially Infinity or the image size.",
            "You get very small classification error.",
            "Again state of the art with a big jump relatively to previous results, zero point 2% there."
        ],
        [
            "What's happening here?",
            "What's happening is that here you have a stationary process.",
            "If you do this convolution, this is still going to be stationary and then you are averaging.",
            "So because you average on something very large, if you have a stationary process which is agatic, this is going to converge to expected values.",
            "So really what you're computing our estimators of expected value, so you're trying to characterize your process with these expected values and you have low variance expected values because you don't do any square, you just do contractions.",
            "So.",
            "This is why you can classify textures because you are characterizing your texture by estimation of moments.",
            "The question is.",
            "Why can you characterize two textures which are different?",
            "Are these moments sufficient to distinguish them and that will be related?"
        ],
        [
            "Reconstruction I'll come back to that now if we want to go to more sophisticated classification, we need to go beyond translation, so that's the work of lasip.",
            "Suppose you compute your first wavelet transform.",
            "Observe this lives in a four dimensional space translation.",
            "Angles this is the angle of the wavelength and scale."
        ],
        [
            "So what we didn't tell that was just convolution.",
            "The second wavelet transform was convolution along translation because we were interested in environment on translation.",
            "Suppose now that you want invariants to rotation.",
            "Then you want also make a convolution along the rotation variable.",
            "With wavelengths, so we'll use wavelets along translation wavelets along rotation if you want something."
        ],
        [
            "Is invited to scale.",
            "Then you are going to do convolution along rotation, convolution along translation and scale.",
            "So you're going to have a wavelength which lives in this 4 dimensional space.",
            "But it's just product of convolution."
        ],
        [
            "Is that important?",
            "Yes, it is.",
            "If you take a problem where you have a lot of variability in terms of rotations and scaling.",
            "If you just do a representation which is invariant to translation, the classification error is about 20%.",
            "If you add up rotation, you get about 2% scaling invariants, about 0.6%.",
            "So there you are reaching your state of the art."
        ],
        [
            "OK, so can you do that now?",
            "Very complex images, so that's the Caltech database that.",
            "West, shown by the young and he explained it took some time because there was not enough data to learn deep network on that.",
            "So we're going to do the same thing.",
            "You just compute your roto translation and that's the work of Eduardo Yellow of the image.",
            "And then a simple linear classifier.",
            "So.",
            "State of the art in 2012.",
            "That's the one.",
            "In fact.",
            "It was often not using deep network, 80 percent, 50%.",
            "That's the second database, which is more complicated.",
            "Cifar 10 is the third database, very different with complex objects.",
            "Deep Network in 2012.",
            "Because these deep networks are now trained with millions of images, you are getting much better errors which are 8570% and 90 on these words.",
            "If you just use one layer of the scattering networks, there is not good.",
            "If you use a second layer, we're basically back to 2012.",
            "In other words, no.",
            "Learning whatsoever you get your 80% on Caltech here, 50% here 80%.",
            "We are not very far from the deep network.",
            "We're basically still three years late.",
            "However, no dilation invariance has been implemented.",
            "No optimization of filter.",
            "This is a work of the first year PhD student who took the thing and applied it.",
            "OK, so there is a lot of room and he's the Mr.",
            "Very gifted, very good, but still there is a lot of rooms to work out of there."
        ],
        [
            "OK, that was the same, sorry."
        ],
        [
            "It made a mistake, OK inversion.",
            "That's the work that's done with Joanne Quinn up.",
            "Can you recover?",
            "Suppose that you're given your scattering transform.",
            "You would like to compute a signal which has exactly the same scattering coefficients, and I'm going to optimize it by minimizing itself to normal.",
            "Can that work?",
            "Let's try to see.",
            "So the first problem is obviously this is a completely non convex optimization problem.",
            "I thought it wouldn't work at all and there was a big surprise about three weeks.",
            "Four weeks ago is the fact that we suddenly realized the inversion can be done.",
            "So let me first suppose that I only keep the 1st order coefficient.",
            "OK, so first order coefficient I just keep these.",
            "I'm going to say that the scale is Infinity, so I basically only keep.",
            "L1 norms of wavelet coefficients and just the average of the signal.",
            "I have very few questions.",
            "What can I recover?",
            "One thing you can prove, and these are just the beginning results, is that if X is very sparse, you can recover it.",
            "If X is a sine wave completely other extreme you can recover.",
            "I would like to point out this is a very non classical problem because usually what you do is you have measurements and you minimize in our one note.",
            "Here are measurements are L1 norm and we are minimizing in Altoona.",
            "OK, so the measurements are the unknowns.",
            "That's why it's very non convex.",
            "Let me now show you."
        ],
        [
            "Results these are first examples of sparseimage plus a sine wave.",
            "1st order recover.",
            "You have very few coefficients.",
            "OK here up, sorry here you have N square pixels.",
            "This is a recover from log.",
            "You can almost recover your one.",
            "There is a problem of convergence.",
            "Your circle is almost recovered, the others are not.",
            "What if you add the second line?",
            "If you add the second layer, you recover completely your geometry of your elements.",
            "What I believe is that in fact you have a BI Lipschitz property whenever you have real geometrical shapes.",
            "But you also recover sine wave.",
            "Observe this is not recovered too much complexity, not enough coefficient.",
            "What about?"
        ],
        [
            "Textures here are the original textures.",
            "These are still work with Genvoya.",
            "These are recovered textures with Gaussian random processes.",
            "This is perfect because it happens to be close to a Gaussian process.",
            "Here it's very bad they have exactly the same 2nd order moments.",
            "OK, but they are completely different.",
            "If you recover with the 2nd order scattering coefficients, that's what you get, you get, and there are still problems of convergence, but you get realization of up, sorry.",
            "You get.",
            "Risation of textures, which are visually pretty close.",
            "Which explains, I believe, why you can discriminate so well the texture, because in fact most of the texture information are encoded within these coefficients.",
            "Now let's"
        ],
        [
            "Move to real complex images like the one in Caltech.",
            "These are the original image and square pixels.",
            "There is no more sparsity, no more equity city property.",
            "If you reconstruct from small windows OK, so the scattering transform was computed with an invariant, a very small window, this is what you recover.",
            "You recover an image which is almost perfect, but the number of coefficients that you're using is bigger than the number of pixels in the image.",
            "OK, you have introduced very little invariant here.",
            "What if you increase the amount of invariants?",
            "The number of coefficients is going to divide by 4.",
            "Here you still recover, see something.",
            "A very decent quality.",
            "You further increase.",
            "That's what you recover.",
            "Further increase here the number of coefficients are Logn squared compared to an image of N squared, so you have that it's an amazing compression in terms of compression.",
            "What do you recover?",
            "It looks like a texture kind of equivalent texture of what you had.",
            "OK, these images are not smooth, they're sharp, you still see perfect sharp edges with the kind of geometry that you originally had, but you don't have all the details.",
            "Let me just zoom."
        ],
        [
            "Into one of these images, this was reconstructed at the scale that was used by Edward, who happened to optimize his scale for doing classification on Caltech.",
            "32 What you can see is when you use a scale of size 32 which is used for classification.",
            "This is a deep net of size 5.",
            "You can recover your image.",
            "There are errors, but look again, the edges are OK. You see kind of the geometry of the elements are here.",
            "The textures are recovered.",
            "So you do capture the geometry of these elements."
        ],
        [
            "Quantum physics, I'd like to fill physics.",
            "There's a very beautiful problem.",
            "Why physics is interesting, because the problem on image is that you have no idea of what you're really having as a similarity measure.",
            "In physics, you do have information and the problem is the following.",
            "In an N body problems, whether it's astronomy or in quantum chemistry, infinitely small.",
            "Can you learn the physics?",
            "Can you learn the energy if you learn the energy, you can learn the force and everything how from examples?",
            "So what is The xx and is initially the set of positions and Chargers or mass in that case?"
        ],
        [
            "So here's the problem.",
            "If you are in a classic setting, X is the distribution of mass, so their positions and the mass or the electrostatic charges and F of X is going to be just for example in the case of electrostatic, the current energy.",
            "So the product of the charges divided by the distance OK. Now, one of the very important element in physics is that the number of interaction is huge, but they can be regrouped.",
            "They can be regrouped because you can consider that the charge interact with next close, but then faraway charge can be summarized providing a single interaction.",
            "With this one an even more far away chart can be further regroups for their interaction.",
            "That's a well known principle in physics.",
            "The consequence of that is that these are multi scale problems.",
            "The consequence of that is yet you can prove that this function F of X can be exactly expanded in terms of these scattering coefficient.",
            "Now this is not a very interesting."
        ],
        [
            "And to learn what about quantum physics?",
            "The interesting thing about quantum chemistry is that now F affect is an amazingly complicated function that people compute on distribution of atoms with days of supercomputers.",
            "It's very you have to solve a complex optimization problem over the Schroedinger equation, however.",
            "You know that the solution is translation invariant.",
            "You know that if you rotate your system, the energy doesn't change.",
            "You know that if you slightly deform your system, the energy is going to gracefully evolve.",
            "So.",
            "We're going to do the following learning experiments.",
            "You have a database of molecules.",
            "That's a standard database which is used by people who wants to try to learn on this problem.",
            "With the Energy 702 dimensional molecule, each molecule has about 20 items OK. Then we are going to try to get this energy and to do a regression of this energy over our first and 2nd order scattering vectors coefficients.",
            "OK, so if we do that from the training data, that's what we get.",
            "This is the error of the energy computation expanded in scattering coefficient, given that this is again F of X, is the energy of the quantum system.",
            "When you increase the number of term.",
            "You are getting a better, better and better approximation, but if you look at the error it decays much faster than what would be expected here.",
            "You are having a curse of dimensionality problem.",
            "Normally the error should decay like 1 / D. These the number of degrees of freedom it's about 60 here the error decays like 1 / 2 so very fast.",
            "Here we are reaching the state of the art in this database compared to existing methods which are in putting a lot of physics.",
            "And what is interesting is that the number of terms that you need is very small.",
            "Is of the order of the number of degree of freedoms that you need.",
            "So there's a lot of things to be understood what is behind, but what that seems to indicate is that these elements that we are computing here seems to have relation with the important variable in the physical problem.",
            "The fact that wavelets are very important in that kind of physical system that's known the fact that these kind of cascade can be useful for computing energy that.",
            "A much less clear."
        ],
        [
            "OK, so the conclusion.",
            "So the conclusion to try to come back to the question.",
            "Do we need to learn with deep deep network filters?",
            "I think that whenever you are dealing with a problem where you have prior information on the geometry.",
            "Like images like audio, because you know about time you know about frequency and so on.",
            "I think that the learning of the filters is not needed, and by imputing the appropriate invariant and the way that's related to inappropriate environment.",
            "We should and of course I have to see whether reach the kind of classification results that people are getting on image.",
            "Net.",
            "You do learn at the end.",
            "With the supervised learning with an SVM, but for the kernel, I don't think it's needed.",
            "I think it's completely needed when you are dealing with non structured data like.",
            "Sorry, words, language and so on because you don't know the geometry and you have to learn it.",
            "But if you do know that geometry, I think you don't need to learn.",
            "The second interesting thing from a mass point of view is that what we're really doing here, we are taking the geometry or we are embedding it into a geometrical simple Euclidean space with BI Lipschitz property.",
            "I think there absolutely no true proves, and that's a conjecture over simple shapes.",
            "But you are also embedding random processes and that what seems to give the stability you have the two extreme of the problem, but there's a lot of mathematics to understand, which is what is behind.",
            "Physics is very interesting because physics is entirely built over the notion of invariants.",
            "That's how you can recover all the Lagrangian.",
            "For example, in quantum physics.",
            "Now that's the same kind of point of view.",
            "Is there reason why one could arrive to the same kind of construction over these environments?",
            "It's possible because the requirements are very similar, and if that's the case, that kind of rich could also be very interesting.",
            "That's one of the dream behind all that.",
            "So like everybody, I'm looking for a postdoc."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, I'm going to try to give a slightly different view of after this very beautiful talk that Yan gave on these.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Deep networks, so the problem is a high dimensional classification problem, at least in the beginning.",
                    "label": 0
                },
                {
                    "sent": "You have data with labels, which are the why of I why I you have signals which lives in very high dimensional space and I will mostly be speaking about images and you would like to compute the label of your signal.",
                    "label": 0
                },
                {
                    "sent": "And of course the problem is that the natural way to compare signals which are Euclidean distance are very inappropriate way.",
                    "label": 0
                },
                {
                    "sent": "In very high dimension, to measure the notion of similarity, Euclidean distance of two image gives you no information about how similar they are.",
                    "label": 1
                },
                {
                    "sent": "So the standard approach that has been developed in learning in general is to try to find a representation fire effects which you can think of a big vectors of features, and so that if you now apply a Euclidean distance over this big feature vector, then for some miracle it gives you.",
                    "label": 0
                },
                {
                    "sent": "Relevant notion of similarity, and if that's the case, then you can try to do a simple classification with a simple linear classifier.",
                    "label": 1
                },
                {
                    "sent": "So you divide your space with just a hyperplane and you get very good estimation of your classes.",
                    "label": 0
                },
                {
                    "sent": "So that's the standard approach and the core problem in all this is how to define this miracle function, which is giving going to give you this notion of similarity with the signal.",
                    "label": 1
                },
                {
                    "sent": "Legion distance, so that's where deep learning comes in.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the work of Jan says, OK, I have an amazing way to do that.",
                    "label": 0
                },
                {
                    "sent": "You take your signal and he explained all that he does cascade of convolution, nonlinear pooling and so on.",
                    "label": 0
                },
                {
                    "sent": "Then he gets a big feature vector and out of this big feature vector you just apply a simple linear classifier.",
                    "label": 0
                },
                {
                    "sent": "You can do more sophisticated but linear classifier sufficient an you obtain amazing classification results with state of the art results on huge number of databases.",
                    "label": 0
                },
                {
                    "sent": "So if you think that this depth, you can think of it as a scale variable, because as you go you get bigger and bigger convolution, so you get information over larger and larger domain of your signal and all the parameters in your network or learned with this supervised learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now there is something interesting that has been observed is that if you learn.",
                    "label": 0
                },
                {
                    "sent": "Well, your classifier, your network and that was done in particular for this Alex Net the same network.",
                    "label": 0
                },
                {
                    "sent": "Well workout for many many very different database, so I'm some sense it looks like there's something a bit generic about this network and it's not so much tuned to the specific database it's being used for.",
                    "label": 0
                },
                {
                    "sent": "It has been trained on and these data network have been trained on image NET as was mentioned by Jan which have literally millions of images.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the goal of the talk will be to make a few outrageous claims.",
                    "label": 1
                },
                {
                    "sent": "The first one is you don't need to learn for images.",
                    "label": 1
                },
                {
                    "sent": "For audio, I strongly believe that you don't need to learn now an.",
                    "label": 0
                },
                {
                    "sent": "To achieve the kind of results, beautiful results that Jen have been showing and the reason is that.",
                    "label": 0
                },
                {
                    "sent": "What is behind all that is the geometry of the image is the geometry of the world, and that's the key element that you need to run.",
                    "label": 0
                },
                {
                    "sent": "The second outrageous claim will be to say that in fact, these things provide coders you can encode.",
                    "label": 0
                },
                {
                    "sent": "They carry most of the signal information and you can recover.",
                    "label": 0
                },
                {
                    "sent": "I'll be showing that this signals from these very strange coders.",
                    "label": 0
                },
                {
                    "sent": "The last one will be related to physics is that I do think that these techniques are very close to what's happening in physics, and I'll be showing that through example of learning of quantum chemistry problems.",
                    "label": 0
                },
                {
                    "sent": "So that will be the goal is to try in ways or not in order to substantiate these claims and the first one being the longer one to substantiate so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me begin.",
                    "label": 0
                },
                {
                    "sent": "OK, so the problem is try to get a representation which is going to give you a good approximation of the underlying metric of similarity.",
                    "label": 0
                },
                {
                    "sent": "The problem is what is the metric of similarity for image for an image.",
                    "label": 0
                },
                {
                    "sent": "If you have a simple shape like that low dimensional image, a natural metric will be a metric of deformation.",
                    "label": 0
                },
                {
                    "sent": "Why is this three closer than this one?",
                    "label": 0
                },
                {
                    "sent": "Because you can deform it with a small deformation, whereas it will require much bigger information to reach the five.",
                    "label": 0
                },
                {
                    "sent": "And these two five are much closer than they are to three, and this idea has been very much that studied.",
                    "label": 0
                },
                {
                    "sent": "These deformation metrics like people like hernander, true value Ness and the basic idea is how do you deformity formation?",
                    "label": 0
                },
                {
                    "sent": "Well, you take your signal and you work this space spatial domain and then the distance will basically be given by how big should be the warping when you try to match one on the other one.",
                    "label": 0
                },
                {
                    "sent": "And how big is the L2 error when you try to deform one?",
                    "label": 0
                },
                {
                    "sent": "Into the other one, because the deformation is not going to give you an ideal results.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's typically the kind of deformation metric which have been used very much for that kind of simple shape, and in particular this metric are invariant to translation because if you just translate, the deformation is going to be 0 here.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's simple here.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What if you deal with real high dimensional image?",
                    "label": 1
                },
                {
                    "sent": "So let's say textures.",
                    "label": 0
                },
                {
                    "sent": "If you deal with the texture, one of the difficulties these two textures.",
                    "label": 0
                },
                {
                    "sent": "You'll say they are the same.",
                    "label": 0
                },
                {
                    "sent": "So what metric over these realization of stationary processes?",
                    "label": 1
                },
                {
                    "sent": "OK, you can say my distance should be controlled by this deformation.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because if I take my texture and I deform it, you still see a very similar texture.",
                    "label": 0
                },
                {
                    "sent": "The problem is that the reverse inequality is totally wrong.",
                    "label": 0
                },
                {
                    "sent": "The reverse inequality is wrong because you can have two textures like these two over there.",
                    "label": 1
                },
                {
                    "sent": "This and this.",
                    "label": 0
                },
                {
                    "sent": "They are very far away from that point of view, but you would like to say that the distance is zero.",
                    "label": 0
                },
                {
                    "sent": "OK, you cannot deform a white noise into another white noise, but you can say that they are identical from the point of view of stochastic processes.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What about much more complex images?",
                    "label": 1
                },
                {
                    "sent": "Well, you can say still you want to maintain this environments to translation and stability to deformation, but what else?",
                    "label": 0
                },
                {
                    "sent": "And that will be the underlying questions.",
                    "label": 0
                },
                {
                    "sent": "How do you build metrics which are generic, not just for low dimension but for high dimensional images?",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me just for now stick with what I know.",
                    "label": 0
                },
                {
                    "sent": "I would like to build a metric which is equivalent.",
                    "label": 0
                },
                {
                    "sent": "You clean metric to this similarity metric and the only thing I don't this similarity metric is that if I have a small deformation, the distance should be small.",
                    "label": 0
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "This distance it really means two things.",
                    "label": 0
                },
                {
                    "sent": "First of all, you want to be stable to additive components.",
                    "label": 1
                },
                {
                    "sent": "Small addition should not modify mature signals, so stability now to why the deformation is identity.",
                    "label": 0
                },
                {
                    "sent": "It's controlled by this first term.",
                    "label": 0
                },
                {
                    "sent": "The second thing you want to be stable by deformation if X prime is a small deformation of X.",
                    "label": 0
                },
                {
                    "sent": "Then the distance should be of the order of the deformation, so small deformation should give you close signal.",
                    "label": 0
                },
                {
                    "sent": "And that in particular implies invariants to translation.",
                    "label": 1
                },
                {
                    "sent": "If you just have a translation, the distance is going to be 0.",
                    "label": 1
                },
                {
                    "sent": "OK, so that will be the beginning point problem.",
                    "label": 0
                },
                {
                    "sent": "You don't get that kind of environment and stability with usual mathematical tools like Fourier Canonical invariants.",
                    "label": 0
                },
                {
                    "sent": "And that's where the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Network coming, so I'm going to use wavelets.",
                    "label": 0
                },
                {
                    "sent": "Why wavelets because to get stability to the formation you need to do a separation of scales OK and we're going to do it in 2D with a simple wavelet, which is like a Gaussian modulated by a sine wave.",
                    "label": 0
                },
                {
                    "sent": "OK to complex function.",
                    "label": 0
                },
                {
                    "sent": "This function you have real part imaginary part.",
                    "label": 0
                },
                {
                    "sent": "You rotate it.",
                    "label": 0
                },
                {
                    "sent": "These are all rotation and you scale it.",
                    "label": 0
                },
                {
                    "sent": "These are all the scale OK in 48.",
                    "label": 0
                },
                {
                    "sent": "That's how it looks like.",
                    "label": 0
                },
                {
                    "sent": "And then when you rotate you cover and analysts.",
                    "label": 0
                },
                {
                    "sent": "When you dilate, you cover the next sentences in full.",
                    "label": 0
                },
                {
                    "sent": "So what is the wavelet transform?",
                    "label": 0
                },
                {
                    "sent": "Justin averaging with the low frequency?",
                    "label": 0
                },
                {
                    "sent": "And then the bandpass component.",
                    "label": 0
                },
                {
                    "sent": "All the wave let components in the different frequency band.",
                    "label": 0
                },
                {
                    "sent": "And if you construct well your wave leads, you get a conservation of norm.",
                    "label": 0
                },
                {
                    "sent": "There is no tonality here, it's very redundant, but this is easy to get.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's how it looks like.",
                    "label": 0
                },
                {
                    "sent": "You take an image.",
                    "label": 0
                },
                {
                    "sent": "You can build that with a fast filterbank algorithm, so that's the different wavelet coefficient at find scale, and the average and then from the average U sub decompose with filters wavelength and the average the wave leads and the average wavelength and the average up to the end.",
                    "label": 0
                },
                {
                    "sent": "And this is the final averaging of your image and all the wavelet coefficients.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good, now you would like to have which is something which is translation environment.",
                    "label": 0
                },
                {
                    "sent": "So let me show it in one dimension because it's easier, so translation invariants.",
                    "label": 0
                },
                {
                    "sent": "How can you do it?",
                    "label": 0
                },
                {
                    "sent": "You just average your signal over domain to the J is going to be locali translation invariant.",
                    "label": 0
                },
                {
                    "sent": "You can do that with a wavelet transform and then you have the wavelet coefficients.",
                    "label": 0
                },
                {
                    "sent": "If you want it to be fully translation invariants then you just like 2 to Jake, one Infinity and that you just get the average which is completely translation invariant.",
                    "label": 0
                },
                {
                    "sent": "The problem is, what are you going to do with the wavelet coefficients?",
                    "label": 0
                },
                {
                    "sent": "They are absolutely not translation invariant.",
                    "label": 0
                },
                {
                    "sent": "You can do the same thing that for Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "First kill the phase.",
                    "label": 0
                },
                {
                    "sent": "If you kill the phase, you're going to get the envelope, which is more regular and therefore slightly more translation environment.",
                    "label": 0
                },
                {
                    "sent": "Next stage you want to make it really translation invariant, so let's average this modulus.",
                    "label": 0
                },
                {
                    "sent": "You get something very regular, and now it's going to be translation invariant.",
                    "label": 0
                },
                {
                    "sent": "How do you do that?",
                    "label": 0
                },
                {
                    "sent": "You applied and you wavelet transform over.",
                    "label": 1
                },
                {
                    "sent": "This way they transform modulus.",
                    "label": 1
                },
                {
                    "sent": "So you took that, you compute its second wavelet transform.",
                    "label": 1
                },
                {
                    "sent": "You get the average and the next set of wavelet coefficients.",
                    "label": 0
                },
                {
                    "sent": "The idea is that we're going to iterate on that, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's do it first.",
                    "label": 0
                },
                {
                    "sent": "Wavelet transform.",
                    "label": 0
                },
                {
                    "sent": "That's the scale variable is going to correspond to the depths in our deep network.",
                    "label": 0
                },
                {
                    "sent": "Second, wavelet transform.",
                    "label": 0
                },
                {
                    "sent": "I want to take this, make it invariant.",
                    "label": 0
                },
                {
                    "sent": "That's the average and all the wavelet coefficient.",
                    "label": 0
                },
                {
                    "sent": "But I want to do it for every wavelets images.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to do it for this one.",
                    "label": 0
                },
                {
                    "sent": "This one OK, all of them then this this is not translation invariant, I have to bring it back down here.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to reapply a wavelet transform third wavelet transform where the coefficient and that's where they arrive at the bottom.",
                    "label": 0
                },
                {
                    "sent": "This hasn't yet arrived to the bottom.",
                    "label": 0
                },
                {
                    "sent": "I have to apply a force wavelet transform and slow.",
                    "label": 0
                },
                {
                    "sent": "So this is the network.",
                    "label": 0
                },
                {
                    "sent": "Now there's something important to realize if you think of it that way.",
                    "label": 0
                },
                {
                    "sent": "This is the depth this is scared, but observe it's a product of wavelet transform.",
                    "label": 0
                },
                {
                    "sent": "The first one, the second Red 1/3 force, and so.",
                    "label": 0
                },
                {
                    "sent": "If you want to analyze it mathematically rather than looking that way, we're going to look at it that way so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me represent the same thing first.",
                    "label": 0
                },
                {
                    "sent": "Order this is the first wavelet transform.",
                    "label": 0
                },
                {
                    "sent": "What did we do we?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "RE applied the second wavelet transform, this first one we got the average, that was whether the bottom and the next layer of wavelet coefficient we.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Re apply a third nonlinearity with their transform.",
                    "label": 0
                },
                {
                    "sent": "That's the average the next play.",
                    "label": 0
                },
                {
                    "sent": "That's how you can view the problem, OK?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you do that, what are you doing?",
                    "label": 0
                },
                {
                    "sent": "You are cascading operator.",
                    "label": 0
                },
                {
                    "sent": "You are getting a set of invariant coefficients, which are your averages.",
                    "label": 0
                },
                {
                    "sent": "First, the average of the signal.",
                    "label": 0
                },
                {
                    "sent": "The average of the wavelet coefficients, 2nd order, and so on.",
                    "label": 0
                },
                {
                    "sent": "If you let Jay go to Infinity, these coefficients are going to converge to L1 norms.",
                    "label": 0
                },
                {
                    "sent": "So basically you are building a representation of your signal as a series of L1 norm.",
                    "label": 0
                },
                {
                    "sent": "These are obviously invariant to translation.",
                    "label": 0
                },
                {
                    "sent": "Here you have very few of them, just log of the size of your signal.",
                    "label": 0
                },
                {
                    "sent": "Here you have squared more log N squared log Cuban so.",
                    "label": 0
                },
                {
                    "sent": "This will be the representation.",
                    "label": 0
                },
                {
                    "sent": "Now the first important thing is that the energy of the last layer is going to converge to zero.",
                    "label": 1
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because you're compressing compressing your operator an all the energy is going to go out as an invariant as a low frequency information.",
                    "label": 0
                },
                {
                    "sent": "This is not the depths of your deep network.",
                    "label": 1
                },
                {
                    "sent": "This is the number of nonlinearity which is applied in the deep network.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second property, let's look at this operator.",
                    "label": 0
                },
                {
                    "sent": "It's a cascade of this.",
                    "label": 0
                },
                {
                    "sent": "Preserve the norm, the absolute value.",
                    "label": 0
                },
                {
                    "sent": "It's a contractive operator, so it's a cascade of contractive operators.",
                    "label": 0
                },
                {
                    "sent": "So first thing you're going to get a contractive operator, that's obvious, so the distance in the scattering domain is always smaller than the distance between the original two element will get the L2 stability that we need.",
                    "label": 0
                },
                {
                    "sent": "Second thing, in fact, you preserve the norm.",
                    "label": 0
                },
                {
                    "sent": "You contract, but the norm is preserved.",
                    "label": 0
                },
                {
                    "sent": "Third thing, the most important, if now you are deforming your signal.",
                    "label": 0
                },
                {
                    "sent": "If you look at the distance because we did this scale separation and wave, let's nearly commute with deformation, you can prove that the distance is of the order of the deformation.",
                    "label": 0
                },
                {
                    "sent": "Plus a component which is the maximum translation multiplied by the scale.",
                    "label": 0
                },
                {
                    "sent": "Because you only localy translation environment, if you let Jay go to Infinity, this disappear and you exactly get what you wanted, which is you are stable to deformation and stable in out OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what can you do with that?",
                    "label": 0
                },
                {
                    "sent": "So that's the work of 1.9.",
                    "label": 0
                },
                {
                    "sent": "The first kind of thing that he did quite some time ago for his PhD was to try to see what's happening for classification.",
                    "label": 0
                },
                {
                    "sent": "So if you have a simple classification problem like MNIST, the digits are deformed.",
                    "label": 0
                },
                {
                    "sent": "It's not surprising that you are going to capture well the problem, and if you just, sorry do.",
                    "label": 0
                },
                {
                    "sent": "Scattering transform in the simple linear classifier you get state of the art result of 0.4%.",
                    "label": 0
                },
                {
                    "sent": "But nowadays who cares about them list OK.",
                    "label": 0
                },
                {
                    "sent": "Classification of texture.",
                    "label": 0
                },
                {
                    "sent": "This is much more challenging.",
                    "label": 0
                },
                {
                    "sent": "Here you are going to do a fully invariant representation, so you are going to make something which is completely translation invariant to.",
                    "label": 0
                },
                {
                    "sent": "Ajay is essentially Infinity or the image size.",
                    "label": 1
                },
                {
                    "sent": "You get very small classification error.",
                    "label": 0
                },
                {
                    "sent": "Again state of the art with a big jump relatively to previous results, zero point 2% there.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's happening here?",
                    "label": 0
                },
                {
                    "sent": "What's happening is that here you have a stationary process.",
                    "label": 0
                },
                {
                    "sent": "If you do this convolution, this is still going to be stationary and then you are averaging.",
                    "label": 0
                },
                {
                    "sent": "So because you average on something very large, if you have a stationary process which is agatic, this is going to converge to expected values.",
                    "label": 0
                },
                {
                    "sent": "So really what you're computing our estimators of expected value, so you're trying to characterize your process with these expected values and you have low variance expected values because you don't do any square, you just do contractions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is why you can classify textures because you are characterizing your texture by estimation of moments.",
                    "label": 0
                },
                {
                    "sent": "The question is.",
                    "label": 0
                },
                {
                    "sent": "Why can you characterize two textures which are different?",
                    "label": 0
                },
                {
                    "sent": "Are these moments sufficient to distinguish them and that will be related?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reconstruction I'll come back to that now if we want to go to more sophisticated classification, we need to go beyond translation, so that's the work of lasip.",
                    "label": 0
                },
                {
                    "sent": "Suppose you compute your first wavelet transform.",
                    "label": 0
                },
                {
                    "sent": "Observe this lives in a four dimensional space translation.",
                    "label": 0
                },
                {
                    "sent": "Angles this is the angle of the wavelength and scale.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we didn't tell that was just convolution.",
                    "label": 0
                },
                {
                    "sent": "The second wavelet transform was convolution along translation because we were interested in environment on translation.",
                    "label": 0
                },
                {
                    "sent": "Suppose now that you want invariants to rotation.",
                    "label": 0
                },
                {
                    "sent": "Then you want also make a convolution along the rotation variable.",
                    "label": 0
                },
                {
                    "sent": "With wavelengths, so we'll use wavelets along translation wavelets along rotation if you want something.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is invited to scale.",
                    "label": 0
                },
                {
                    "sent": "Then you are going to do convolution along rotation, convolution along translation and scale.",
                    "label": 0
                },
                {
                    "sent": "So you're going to have a wavelength which lives in this 4 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "But it's just product of convolution.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that important?",
                    "label": 0
                },
                {
                    "sent": "Yes, it is.",
                    "label": 0
                },
                {
                    "sent": "If you take a problem where you have a lot of variability in terms of rotations and scaling.",
                    "label": 1
                },
                {
                    "sent": "If you just do a representation which is invariant to translation, the classification error is about 20%.",
                    "label": 0
                },
                {
                    "sent": "If you add up rotation, you get about 2% scaling invariants, about 0.6%.",
                    "label": 0
                },
                {
                    "sent": "So there you are reaching your state of the art.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so can you do that now?",
                    "label": 0
                },
                {
                    "sent": "Very complex images, so that's the Caltech database that.",
                    "label": 0
                },
                {
                    "sent": "West, shown by the young and he explained it took some time because there was not enough data to learn deep network on that.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do the same thing.",
                    "label": 0
                },
                {
                    "sent": "You just compute your roto translation and that's the work of Eduardo Yellow of the image.",
                    "label": 0
                },
                {
                    "sent": "And then a simple linear classifier.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "State of the art in 2012.",
                    "label": 0
                },
                {
                    "sent": "That's the one.",
                    "label": 0
                },
                {
                    "sent": "In fact.",
                    "label": 0
                },
                {
                    "sent": "It was often not using deep network, 80 percent, 50%.",
                    "label": 0
                },
                {
                    "sent": "That's the second database, which is more complicated.",
                    "label": 0
                },
                {
                    "sent": "Cifar 10 is the third database, very different with complex objects.",
                    "label": 0
                },
                {
                    "sent": "Deep Network in 2012.",
                    "label": 0
                },
                {
                    "sent": "Because these deep networks are now trained with millions of images, you are getting much better errors which are 8570% and 90 on these words.",
                    "label": 0
                },
                {
                    "sent": "If you just use one layer of the scattering networks, there is not good.",
                    "label": 0
                },
                {
                    "sent": "If you use a second layer, we're basically back to 2012.",
                    "label": 0
                },
                {
                    "sent": "In other words, no.",
                    "label": 0
                },
                {
                    "sent": "Learning whatsoever you get your 80% on Caltech here, 50% here 80%.",
                    "label": 0
                },
                {
                    "sent": "We are not very far from the deep network.",
                    "label": 0
                },
                {
                    "sent": "We're basically still three years late.",
                    "label": 0
                },
                {
                    "sent": "However, no dilation invariance has been implemented.",
                    "label": 0
                },
                {
                    "sent": "No optimization of filter.",
                    "label": 0
                },
                {
                    "sent": "This is a work of the first year PhD student who took the thing and applied it.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is a lot of room and he's the Mr.",
                    "label": 0
                },
                {
                    "sent": "Very gifted, very good, but still there is a lot of rooms to work out of there.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that was the same, sorry.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It made a mistake, OK inversion.",
                    "label": 0
                },
                {
                    "sent": "That's the work that's done with Joanne Quinn up.",
                    "label": 0
                },
                {
                    "sent": "Can you recover?",
                    "label": 0
                },
                {
                    "sent": "Suppose that you're given your scattering transform.",
                    "label": 0
                },
                {
                    "sent": "You would like to compute a signal which has exactly the same scattering coefficients, and I'm going to optimize it by minimizing itself to normal.",
                    "label": 0
                },
                {
                    "sent": "Can that work?",
                    "label": 0
                },
                {
                    "sent": "Let's try to see.",
                    "label": 0
                },
                {
                    "sent": "So the first problem is obviously this is a completely non convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "I thought it wouldn't work at all and there was a big surprise about three weeks.",
                    "label": 0
                },
                {
                    "sent": "Four weeks ago is the fact that we suddenly realized the inversion can be done.",
                    "label": 0
                },
                {
                    "sent": "So let me first suppose that I only keep the 1st order coefficient.",
                    "label": 0
                },
                {
                    "sent": "OK, so first order coefficient I just keep these.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say that the scale is Infinity, so I basically only keep.",
                    "label": 0
                },
                {
                    "sent": "L1 norms of wavelet coefficients and just the average of the signal.",
                    "label": 0
                },
                {
                    "sent": "I have very few questions.",
                    "label": 0
                },
                {
                    "sent": "What can I recover?",
                    "label": 0
                },
                {
                    "sent": "One thing you can prove, and these are just the beginning results, is that if X is very sparse, you can recover it.",
                    "label": 0
                },
                {
                    "sent": "If X is a sine wave completely other extreme you can recover.",
                    "label": 0
                },
                {
                    "sent": "I would like to point out this is a very non classical problem because usually what you do is you have measurements and you minimize in our one note.",
                    "label": 0
                },
                {
                    "sent": "Here are measurements are L1 norm and we are minimizing in Altoona.",
                    "label": 0
                },
                {
                    "sent": "OK, so the measurements are the unknowns.",
                    "label": 0
                },
                {
                    "sent": "That's why it's very non convex.",
                    "label": 0
                },
                {
                    "sent": "Let me now show you.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results these are first examples of sparseimage plus a sine wave.",
                    "label": 0
                },
                {
                    "sent": "1st order recover.",
                    "label": 0
                },
                {
                    "sent": "You have very few coefficients.",
                    "label": 0
                },
                {
                    "sent": "OK here up, sorry here you have N square pixels.",
                    "label": 0
                },
                {
                    "sent": "This is a recover from log.",
                    "label": 0
                },
                {
                    "sent": "You can almost recover your one.",
                    "label": 0
                },
                {
                    "sent": "There is a problem of convergence.",
                    "label": 0
                },
                {
                    "sent": "Your circle is almost recovered, the others are not.",
                    "label": 0
                },
                {
                    "sent": "What if you add the second line?",
                    "label": 0
                },
                {
                    "sent": "If you add the second layer, you recover completely your geometry of your elements.",
                    "label": 0
                },
                {
                    "sent": "What I believe is that in fact you have a BI Lipschitz property whenever you have real geometrical shapes.",
                    "label": 0
                },
                {
                    "sent": "But you also recover sine wave.",
                    "label": 0
                },
                {
                    "sent": "Observe this is not recovered too much complexity, not enough coefficient.",
                    "label": 0
                },
                {
                    "sent": "What about?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Textures here are the original textures.",
                    "label": 1
                },
                {
                    "sent": "These are still work with Genvoya.",
                    "label": 0
                },
                {
                    "sent": "These are recovered textures with Gaussian random processes.",
                    "label": 1
                },
                {
                    "sent": "This is perfect because it happens to be close to a Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "Here it's very bad they have exactly the same 2nd order moments.",
                    "label": 0
                },
                {
                    "sent": "OK, but they are completely different.",
                    "label": 0
                },
                {
                    "sent": "If you recover with the 2nd order scattering coefficients, that's what you get, you get, and there are still problems of convergence, but you get realization of up, sorry.",
                    "label": 0
                },
                {
                    "sent": "You get.",
                    "label": 0
                },
                {
                    "sent": "Risation of textures, which are visually pretty close.",
                    "label": 0
                },
                {
                    "sent": "Which explains, I believe, why you can discriminate so well the texture, because in fact most of the texture information are encoded within these coefficients.",
                    "label": 0
                },
                {
                    "sent": "Now let's",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move to real complex images like the one in Caltech.",
                    "label": 0
                },
                {
                    "sent": "These are the original image and square pixels.",
                    "label": 0
                },
                {
                    "sent": "There is no more sparsity, no more equity city property.",
                    "label": 0
                },
                {
                    "sent": "If you reconstruct from small windows OK, so the scattering transform was computed with an invariant, a very small window, this is what you recover.",
                    "label": 0
                },
                {
                    "sent": "You recover an image which is almost perfect, but the number of coefficients that you're using is bigger than the number of pixels in the image.",
                    "label": 0
                },
                {
                    "sent": "OK, you have introduced very little invariant here.",
                    "label": 0
                },
                {
                    "sent": "What if you increase the amount of invariants?",
                    "label": 0
                },
                {
                    "sent": "The number of coefficients is going to divide by 4.",
                    "label": 0
                },
                {
                    "sent": "Here you still recover, see something.",
                    "label": 0
                },
                {
                    "sent": "A very decent quality.",
                    "label": 0
                },
                {
                    "sent": "You further increase.",
                    "label": 0
                },
                {
                    "sent": "That's what you recover.",
                    "label": 0
                },
                {
                    "sent": "Further increase here the number of coefficients are Logn squared compared to an image of N squared, so you have that it's an amazing compression in terms of compression.",
                    "label": 0
                },
                {
                    "sent": "What do you recover?",
                    "label": 0
                },
                {
                    "sent": "It looks like a texture kind of equivalent texture of what you had.",
                    "label": 0
                },
                {
                    "sent": "OK, these images are not smooth, they're sharp, you still see perfect sharp edges with the kind of geometry that you originally had, but you don't have all the details.",
                    "label": 0
                },
                {
                    "sent": "Let me just zoom.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into one of these images, this was reconstructed at the scale that was used by Edward, who happened to optimize his scale for doing classification on Caltech.",
                    "label": 0
                },
                {
                    "sent": "32 What you can see is when you use a scale of size 32 which is used for classification.",
                    "label": 0
                },
                {
                    "sent": "This is a deep net of size 5.",
                    "label": 0
                },
                {
                    "sent": "You can recover your image.",
                    "label": 0
                },
                {
                    "sent": "There are errors, but look again, the edges are OK. You see kind of the geometry of the elements are here.",
                    "label": 0
                },
                {
                    "sent": "The textures are recovered.",
                    "label": 0
                },
                {
                    "sent": "So you do capture the geometry of these elements.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quantum physics, I'd like to fill physics.",
                    "label": 0
                },
                {
                    "sent": "There's a very beautiful problem.",
                    "label": 0
                },
                {
                    "sent": "Why physics is interesting, because the problem on image is that you have no idea of what you're really having as a similarity measure.",
                    "label": 0
                },
                {
                    "sent": "In physics, you do have information and the problem is the following.",
                    "label": 0
                },
                {
                    "sent": "In an N body problems, whether it's astronomy or in quantum chemistry, infinitely small.",
                    "label": 1
                },
                {
                    "sent": "Can you learn the physics?",
                    "label": 1
                },
                {
                    "sent": "Can you learn the energy if you learn the energy, you can learn the force and everything how from examples?",
                    "label": 0
                },
                {
                    "sent": "So what is The xx and is initially the set of positions and Chargers or mass in that case?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the problem.",
                    "label": 0
                },
                {
                    "sent": "If you are in a classic setting, X is the distribution of mass, so their positions and the mass or the electrostatic charges and F of X is going to be just for example in the case of electrostatic, the current energy.",
                    "label": 0
                },
                {
                    "sent": "So the product of the charges divided by the distance OK. Now, one of the very important element in physics is that the number of interaction is huge, but they can be regrouped.",
                    "label": 0
                },
                {
                    "sent": "They can be regrouped because you can consider that the charge interact with next close, but then faraway charge can be summarized providing a single interaction.",
                    "label": 0
                },
                {
                    "sent": "With this one an even more far away chart can be further regroups for their interaction.",
                    "label": 0
                },
                {
                    "sent": "That's a well known principle in physics.",
                    "label": 0
                },
                {
                    "sent": "The consequence of that is that these are multi scale problems.",
                    "label": 0
                },
                {
                    "sent": "The consequence of that is yet you can prove that this function F of X can be exactly expanded in terms of these scattering coefficient.",
                    "label": 0
                },
                {
                    "sent": "Now this is not a very interesting.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to learn what about quantum physics?",
                    "label": 0
                },
                {
                    "sent": "The interesting thing about quantum chemistry is that now F affect is an amazingly complicated function that people compute on distribution of atoms with days of supercomputers.",
                    "label": 1
                },
                {
                    "sent": "It's very you have to solve a complex optimization problem over the Schroedinger equation, however.",
                    "label": 0
                },
                {
                    "sent": "You know that the solution is translation invariant.",
                    "label": 0
                },
                {
                    "sent": "You know that if you rotate your system, the energy doesn't change.",
                    "label": 0
                },
                {
                    "sent": "You know that if you slightly deform your system, the energy is going to gracefully evolve.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We're going to do the following learning experiments.",
                    "label": 0
                },
                {
                    "sent": "You have a database of molecules.",
                    "label": 0
                },
                {
                    "sent": "That's a standard database which is used by people who wants to try to learn on this problem.",
                    "label": 0
                },
                {
                    "sent": "With the Energy 702 dimensional molecule, each molecule has about 20 items OK. Then we are going to try to get this energy and to do a regression of this energy over our first and 2nd order scattering vectors coefficients.",
                    "label": 1
                },
                {
                    "sent": "OK, so if we do that from the training data, that's what we get.",
                    "label": 0
                },
                {
                    "sent": "This is the error of the energy computation expanded in scattering coefficient, given that this is again F of X, is the energy of the quantum system.",
                    "label": 0
                },
                {
                    "sent": "When you increase the number of term.",
                    "label": 0
                },
                {
                    "sent": "You are getting a better, better and better approximation, but if you look at the error it decays much faster than what would be expected here.",
                    "label": 0
                },
                {
                    "sent": "You are having a curse of dimensionality problem.",
                    "label": 0
                },
                {
                    "sent": "Normally the error should decay like 1 / D. These the number of degrees of freedom it's about 60 here the error decays like 1 / 2 so very fast.",
                    "label": 1
                },
                {
                    "sent": "Here we are reaching the state of the art in this database compared to existing methods which are in putting a lot of physics.",
                    "label": 0
                },
                {
                    "sent": "And what is interesting is that the number of terms that you need is very small.",
                    "label": 0
                },
                {
                    "sent": "Is of the order of the number of degree of freedoms that you need.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of things to be understood what is behind, but what that seems to indicate is that these elements that we are computing here seems to have relation with the important variable in the physical problem.",
                    "label": 0
                },
                {
                    "sent": "The fact that wavelets are very important in that kind of physical system that's known the fact that these kind of cascade can be useful for computing energy that.",
                    "label": 0
                },
                {
                    "sent": "A much less clear.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the conclusion.",
                    "label": 0
                },
                {
                    "sent": "So the conclusion to try to come back to the question.",
                    "label": 0
                },
                {
                    "sent": "Do we need to learn with deep deep network filters?",
                    "label": 1
                },
                {
                    "sent": "I think that whenever you are dealing with a problem where you have prior information on the geometry.",
                    "label": 0
                },
                {
                    "sent": "Like images like audio, because you know about time you know about frequency and so on.",
                    "label": 0
                },
                {
                    "sent": "I think that the learning of the filters is not needed, and by imputing the appropriate invariant and the way that's related to inappropriate environment.",
                    "label": 0
                },
                {
                    "sent": "We should and of course I have to see whether reach the kind of classification results that people are getting on image.",
                    "label": 0
                },
                {
                    "sent": "Net.",
                    "label": 0
                },
                {
                    "sent": "You do learn at the end.",
                    "label": 0
                },
                {
                    "sent": "With the supervised learning with an SVM, but for the kernel, I don't think it's needed.",
                    "label": 0
                },
                {
                    "sent": "I think it's completely needed when you are dealing with non structured data like.",
                    "label": 0
                },
                {
                    "sent": "Sorry, words, language and so on because you don't know the geometry and you have to learn it.",
                    "label": 0
                },
                {
                    "sent": "But if you do know that geometry, I think you don't need to learn.",
                    "label": 0
                },
                {
                    "sent": "The second interesting thing from a mass point of view is that what we're really doing here, we are taking the geometry or we are embedding it into a geometrical simple Euclidean space with BI Lipschitz property.",
                    "label": 0
                },
                {
                    "sent": "I think there absolutely no true proves, and that's a conjecture over simple shapes.",
                    "label": 0
                },
                {
                    "sent": "But you are also embedding random processes and that what seems to give the stability you have the two extreme of the problem, but there's a lot of mathematics to understand, which is what is behind.",
                    "label": 0
                },
                {
                    "sent": "Physics is very interesting because physics is entirely built over the notion of invariants.",
                    "label": 0
                },
                {
                    "sent": "That's how you can recover all the Lagrangian.",
                    "label": 0
                },
                {
                    "sent": "For example, in quantum physics.",
                    "label": 0
                },
                {
                    "sent": "Now that's the same kind of point of view.",
                    "label": 0
                },
                {
                    "sent": "Is there reason why one could arrive to the same kind of construction over these environments?",
                    "label": 0
                },
                {
                    "sent": "It's possible because the requirements are very similar, and if that's the case, that kind of rich could also be very interesting.",
                    "label": 0
                },
                {
                    "sent": "That's one of the dream behind all that.",
                    "label": 0
                },
                {
                    "sent": "So like everybody, I'm looking for a postdoc.",
                    "label": 0
                }
            ]
        }
    }
}