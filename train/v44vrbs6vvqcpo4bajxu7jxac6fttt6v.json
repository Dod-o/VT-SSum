{
    "id": "v44vrbs6vvqcpo4bajxu7jxac6fttt6v",
    "title": "Cross-lingual Bootstrapping for Semantic Role Labeling",
    "info": {
        "author": [
            "Ivan Titov, Cluster of Excellence Multimodal Computing and Interaction, Saarland University"
        ],
        "published": "Jan. 11, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Computational Linguistics"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_titov_role_labeling/",
    "segmentation": [
        [
            "OK, so my name is Ivan and this is actually work of my student McHale and this is going to be really a short talk so we look at the problem of."
        ],
        [
            "Semantic role labeling.",
            "So our goal is to induce basically predicate argument structure for a sentence.",
            "So this problem is very closely connected to more probably more better known problem for you relation extraction problem.",
            "So for example, for this sentence you would want to identify that Jack is the argument zero opener if you use Pro Bank terminology that the lock is I think which is opened and then that argument two is a paper clip.",
            "So the instrument is a paper clip.",
            "The problem is that this type of task very tight to lexical information, so you generally need a lot of data to learn an accurate model for for this task, and you normally don't have this data for for most of the language, so maybe for a handful of languages so you look for this data.",
            "So we look in a very specific setting which we think is fairly realistic, so we assume that we have just a little bit annotated data for every language.",
            "Maybe for one of the two languages we have relatively strong system, but at least for one of these systems we have really little annotation, so this kind of makes sense because even if you don't have annotation, you can usually obtain this annotation, but we also have parallel data for the for the considerate pair of languages and we want to induce semantic role labelers good semantic role labels for both languages.",
            "So the idea is pretty simple and you have already seen this idea in a couple of previous talks, so you want to learn systems for each language, but also you want to enforce agreement on the parallel data.",
            "So basically you have objectives, for example likelihoods for your model on one language, one language 2 on the labeled data, and you have some regularization which enforces agreement.",
            "But the problem here is that enforcing agreement on semantic representations.",
            "Is not a simple thing as you might think, so this is a challenge.",
            "So how to define or how to spell Salon as agreement model for two languages?"
        ],
        [
            "So why is it really hard?",
            "So first of all, we often the situation that for both languages we have different semantic formalisms.",
            "They often derived from, for example, deep grammar from completely incompatible linguistic theories and underlying mapping.",
            "Even if some kind of result much mapping between roles exist and might be different for each predicate pair and then you have also some objective reasons for the complexity, because if you can see the two predicates in one case, you can have a situation.",
            "Of drunk selling card described as setting another case, it can be about buying right and so you have this shift and shift is represented by.",
            "Swapping's arguments in the semantic argument structure.",
            "So what we want we want basically to estimate this agreement agreement model from the data, and I'd like to say from the very beginning, it's not really the success story don't, so don't expect a happy agent for this.",
            "So this is a work in progress, and maybe it can be quite a long time in progress."
        ],
        [
            "So so basically what what we do we use some kind of cross lingual training algorithm to optimize this objective.",
            "So we start with string semantic role labeling systems on our data for each label for each language and then we label the parallel data resista models and then we estimate an agreement model from the parallel data.",
            "We have an agreement model is relatively simple.",
            "We can talk about this in the poster, but it uses some global features but also it uses some.",
            "Predicate specific features so it has some smoothing embedded in this model.",
            "OK, and then you run the joint inference step on a parallel data.",
            "We use both semantic role labeling models for each language and your agreement model.",
            "So basically translation model which translates between semantic representations and you can do this using the form of Lagrangian relaxation.",
            "If you want to be fancy, but you can actually use some kind of approximate decoding beam search to get about the same results.",
            "So we then retrain semantic role labeling models using both initial Volt data, so hand annotated gate, maybe putting a bit of more weight on this data and also using this outer label it parallel data and then we iterate this way.",
            "OK, so this basically procedure is very similar to training, but the only problem is that in Katrina Q generally have a very simple fixed agreement function whereas.",
            "Here we don't know the agreement function and rely on it along the way.",
            "So this a."
        ],
        [
            "Results and results for English and a couple other language pairs.",
            "So it's always English and something and we evaluate on this other language and we use 500 labeling sentences for in each case for the target language and.",
            "I'll explain what kind of setups we have so supervised setup is rest our baseline, so you just use your 5 Congress labeling sentences to get to get your semantic role labeling self training.",
            "He already knows that it's kind of doesn't really work for semantic role labeling, so we were not able is usually to get an improvement from using self training, so this is not very informative.",
            "Of course you can always makes offering just tickle to supervise training by tuning tenant summary, but it doesn't work OK and then we have two setups one is.",
            "Symmetric sit up and another one is symmetric, so isometrics, it happens, that we have weak semantical labeling for another language.",
            "So so basically in two languages we have semantic label is trained on 500 examples and the symmetric setup we have a stronger English semantic role labeling, so basically trained on the entire pro bank.",
            "Entire standard datasets for this task so you can see that our improvements are really not reliable so sometimes we get some improvement, sometimes not.",
            "So our idea was maybe we should try to test it out of domain because our.",
            "For a level, data is coming, usually from some other domains and we got initially very excited because we tested for English and German.",
            "We got quite a bit of improvement from from this isometric setup and symmetric as well.",
            "But then for some reason for English and share it didn't really work out, so we don't know very well what was the reason is.",
            "And then we looked to try try to understand why why we're not getting there, and one of the experiments we run is we improved our agreement model, so we estimated it from accurate semantic role labeling's using the same data.",
            "So it's not the kind of upper bound, but it's still much more informed agreement models.",
            "Then you can realistically get.",
            "And then indeed, you can see that you get a bit more improvement from using agreement model.",
            "But still it even is a better agreement model as the results are not terribly exciting.",
            "So I think we have the same problems as partially as we had with self training because the problem is very unbalanced and in general what happens the most frequent semantic role labeling start to pop up more frequently?",
            "So this in balance usually kills you in a soft self training and core training setups, and it kills us.",
            "SNL so So yeah, so we can talk about it if you're interested in about some details because we did some additional analysis.",
            "And the poster, but I believe, but by using better agreement model you can get a bit of improvement.",
            "But as you can see, this might be not exactly the way to go.",
            "That's why my invited talk will be actually about something different, so more like unsupervised learning across languages, rezen relying on the translation of semantic representations.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so my name is Ivan and this is actually work of my student McHale and this is going to be really a short talk so we look at the problem of.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Semantic role labeling.",
                    "label": 0
                },
                {
                    "sent": "So our goal is to induce basically predicate argument structure for a sentence.",
                    "label": 1
                },
                {
                    "sent": "So this problem is very closely connected to more probably more better known problem for you relation extraction problem.",
                    "label": 0
                },
                {
                    "sent": "So for example, for this sentence you would want to identify that Jack is the argument zero opener if you use Pro Bank terminology that the lock is I think which is opened and then that argument two is a paper clip.",
                    "label": 1
                },
                {
                    "sent": "So the instrument is a paper clip.",
                    "label": 0
                },
                {
                    "sent": "The problem is that this type of task very tight to lexical information, so you generally need a lot of data to learn an accurate model for for this task, and you normally don't have this data for for most of the language, so maybe for a handful of languages so you look for this data.",
                    "label": 1
                },
                {
                    "sent": "So we look in a very specific setting which we think is fairly realistic, so we assume that we have just a little bit annotated data for every language.",
                    "label": 0
                },
                {
                    "sent": "Maybe for one of the two languages we have relatively strong system, but at least for one of these systems we have really little annotation, so this kind of makes sense because even if you don't have annotation, you can usually obtain this annotation, but we also have parallel data for the for the considerate pair of languages and we want to induce semantic role labelers good semantic role labels for both languages.",
                    "label": 0
                },
                {
                    "sent": "So the idea is pretty simple and you have already seen this idea in a couple of previous talks, so you want to learn systems for each language, but also you want to enforce agreement on the parallel data.",
                    "label": 0
                },
                {
                    "sent": "So basically you have objectives, for example likelihoods for your model on one language, one language 2 on the labeled data, and you have some regularization which enforces agreement.",
                    "label": 0
                },
                {
                    "sent": "But the problem here is that enforcing agreement on semantic representations.",
                    "label": 0
                },
                {
                    "sent": "Is not a simple thing as you might think, so this is a challenge.",
                    "label": 0
                },
                {
                    "sent": "So how to define or how to spell Salon as agreement model for two languages?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why is it really hard?",
                    "label": 0
                },
                {
                    "sent": "So first of all, we often the situation that for both languages we have different semantic formalisms.",
                    "label": 0
                },
                {
                    "sent": "They often derived from, for example, deep grammar from completely incompatible linguistic theories and underlying mapping.",
                    "label": 1
                },
                {
                    "sent": "Even if some kind of result much mapping between roles exist and might be different for each predicate pair and then you have also some objective reasons for the complexity, because if you can see the two predicates in one case, you can have a situation.",
                    "label": 1
                },
                {
                    "sent": "Of drunk selling card described as setting another case, it can be about buying right and so you have this shift and shift is represented by.",
                    "label": 1
                },
                {
                    "sent": "Swapping's arguments in the semantic argument structure.",
                    "label": 0
                },
                {
                    "sent": "So what we want we want basically to estimate this agreement agreement model from the data, and I'd like to say from the very beginning, it's not really the success story don't, so don't expect a happy agent for this.",
                    "label": 0
                },
                {
                    "sent": "So this is a work in progress, and maybe it can be quite a long time in progress.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so basically what what we do we use some kind of cross lingual training algorithm to optimize this objective.",
                    "label": 0
                },
                {
                    "sent": "So we start with string semantic role labeling systems on our data for each label for each language and then we label the parallel data resista models and then we estimate an agreement model from the parallel data.",
                    "label": 1
                },
                {
                    "sent": "We have an agreement model is relatively simple.",
                    "label": 0
                },
                {
                    "sent": "We can talk about this in the poster, but it uses some global features but also it uses some.",
                    "label": 0
                },
                {
                    "sent": "Predicate specific features so it has some smoothing embedded in this model.",
                    "label": 0
                },
                {
                    "sent": "OK, and then you run the joint inference step on a parallel data.",
                    "label": 1
                },
                {
                    "sent": "We use both semantic role labeling models for each language and your agreement model.",
                    "label": 0
                },
                {
                    "sent": "So basically translation model which translates between semantic representations and you can do this using the form of Lagrangian relaxation.",
                    "label": 0
                },
                {
                    "sent": "If you want to be fancy, but you can actually use some kind of approximate decoding beam search to get about the same results.",
                    "label": 0
                },
                {
                    "sent": "So we then retrain semantic role labeling models using both initial Volt data, so hand annotated gate, maybe putting a bit of more weight on this data and also using this outer label it parallel data and then we iterate this way.",
                    "label": 0
                },
                {
                    "sent": "OK, so this basically procedure is very similar to training, but the only problem is that in Katrina Q generally have a very simple fixed agreement function whereas.",
                    "label": 0
                },
                {
                    "sent": "Here we don't know the agreement function and rely on it along the way.",
                    "label": 0
                },
                {
                    "sent": "So this a.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results and results for English and a couple other language pairs.",
                    "label": 0
                },
                {
                    "sent": "So it's always English and something and we evaluate on this other language and we use 500 labeling sentences for in each case for the target language and.",
                    "label": 0
                },
                {
                    "sent": "I'll explain what kind of setups we have so supervised setup is rest our baseline, so you just use your 5 Congress labeling sentences to get to get your semantic role labeling self training.",
                    "label": 0
                },
                {
                    "sent": "He already knows that it's kind of doesn't really work for semantic role labeling, so we were not able is usually to get an improvement from using self training, so this is not very informative.",
                    "label": 0
                },
                {
                    "sent": "Of course you can always makes offering just tickle to supervise training by tuning tenant summary, but it doesn't work OK and then we have two setups one is.",
                    "label": 0
                },
                {
                    "sent": "Symmetric sit up and another one is symmetric, so isometrics, it happens, that we have weak semantical labeling for another language.",
                    "label": 0
                },
                {
                    "sent": "So so basically in two languages we have semantic label is trained on 500 examples and the symmetric setup we have a stronger English semantic role labeling, so basically trained on the entire pro bank.",
                    "label": 0
                },
                {
                    "sent": "Entire standard datasets for this task so you can see that our improvements are really not reliable so sometimes we get some improvement, sometimes not.",
                    "label": 0
                },
                {
                    "sent": "So our idea was maybe we should try to test it out of domain because our.",
                    "label": 0
                },
                {
                    "sent": "For a level, data is coming, usually from some other domains and we got initially very excited because we tested for English and German.",
                    "label": 0
                },
                {
                    "sent": "We got quite a bit of improvement from from this isometric setup and symmetric as well.",
                    "label": 0
                },
                {
                    "sent": "But then for some reason for English and share it didn't really work out, so we don't know very well what was the reason is.",
                    "label": 0
                },
                {
                    "sent": "And then we looked to try try to understand why why we're not getting there, and one of the experiments we run is we improved our agreement model, so we estimated it from accurate semantic role labeling's using the same data.",
                    "label": 0
                },
                {
                    "sent": "So it's not the kind of upper bound, but it's still much more informed agreement models.",
                    "label": 0
                },
                {
                    "sent": "Then you can realistically get.",
                    "label": 0
                },
                {
                    "sent": "And then indeed, you can see that you get a bit more improvement from using agreement model.",
                    "label": 0
                },
                {
                    "sent": "But still it even is a better agreement model as the results are not terribly exciting.",
                    "label": 0
                },
                {
                    "sent": "So I think we have the same problems as partially as we had with self training because the problem is very unbalanced and in general what happens the most frequent semantic role labeling start to pop up more frequently?",
                    "label": 0
                },
                {
                    "sent": "So this in balance usually kills you in a soft self training and core training setups, and it kills us.",
                    "label": 0
                },
                {
                    "sent": "SNL so So yeah, so we can talk about it if you're interested in about some details because we did some additional analysis.",
                    "label": 0
                },
                {
                    "sent": "And the poster, but I believe, but by using better agreement model you can get a bit of improvement.",
                    "label": 0
                },
                {
                    "sent": "But as you can see, this might be not exactly the way to go.",
                    "label": 0
                },
                {
                    "sent": "That's why my invited talk will be actually about something different, so more like unsupervised learning across languages, rezen relying on the translation of semantic representations.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}