{
    "id": "xgln2qfyir5f4qzmgehl4zpwxwu6zz7d",
    "title": "Large-Scale Collaborative Prediction Using a Nonparametric Random Effects Model",
    "info": {
        "author": [
            "Kai Yu, NEC Laboratories America, Inc."
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_yu_lscp/",
    "segmentation": [
        [
            "OK, so my name is Kyle.",
            "This is joint work with John FT from CMU and chocolate roll from NEC Labs."
        ],
        [
            "So this work is very much related to multi task learning, so let's first look at a traditional setting of multitask learning, where we have input data and for each input data we have multiple responses.",
            "Some of them observed some of them not observed.",
            "That means we have multiple different tasks here in each row is 1 task for one regression function.",
            "So here we use MI to resent the ice regression function."
        ],
        [
            "So a very elegant way to model the dependency between those functions within the hierarchical Bayesian framework, where we assume there's a Gaussian process prior with covariance Sigma and then all the different tasks as random functions ID sampled from the common Gaussian process prior.",
            "So then by learning.",
            "Or adapting such a covariance function so we can learn the dependency across different tasks.",
            "So essentially many are or maybe most of the current multitask learning approaches, Bayesian or non Bayesian, there quite related to this idea.",
            "For example in the kernel system you learn essentially a kernel with some proper regularization on the kernel function like trace norm stuff."
        ],
        [
            "Here we probably consider a more interesting setting where we have, in addition to the data features.",
            "We also have a task specific features, so that means this is in a relational setting.",
            "We have column wise features and then realize features.",
            "So in this case.",
            "Is more of a more flexible way is to model the entire function jointly so where we have a so called relational function taking input from roll features an and polemize features."
        ],
        [
            "So then.",
            "A very natural extension of the hierarchical Bayesian approach in Gaussian process framework is that we introduce a additional covariance function.",
            "Taking input from task specific features right?",
            "So then we have two set of kernels, one kernel describing the dependency across the data points and then we have another kernel describing the dependency across functions.",
            "So.",
            "In a special case, if we let this task specific kernel as Delta function, then the model reduced to the original model.",
            "The previous model where you have lowered functions independently sampled, sampled from this Gaussian process prior.",
            "So in this in general case if we have a task specific features then we don't have those.",
            "You know independent sampling process because all the data points across different locations and the functions they depend on each other.",
            "So we simply here in this case we draw a single function.",
            "There is no ID structure there."
        ],
        [
            "So this is pretty much the setting of a. Collaborative prediction problem because we have may have a movie.",
            "Features user features.",
            "We have user ratings or movies.",
            "Anne.",
            "As we are, this is essentially a multi task learning problem.",
            "Is task specific features.",
            "And this framework actually also go beyond normal matrix factorization problem, because here we use additional row and column attributes.",
            "So this formulation is quite general.",
            "It's applied to many related relational prediction problems like link prediction.",
            "Gene protein protein interaction prediction and friendship prediction.",
            "Yeah, where we we can utilize those node wise features.",
            "Well."
        ],
        [
            "So far the modeling the model sounds pretty powerful and elegant, but.",
            "The big problem is the computation.",
            "Because the inference complexity is the product of 2 cubic terms, it's a huge complexity to the inference.",
            "I mean, in a typical for example in the Netflix data, so we have about half million users and approximately two 20,000 movies.",
            "Then it's almost definitely impossible to apply that model to such a huge database.",
            "Another challenge is from the modeling perspective because.",
            "Actually a very common property in many relational learning problems is that those relational observations there are highly dependent.",
            "That means even conditioned on those known predictors.",
            "Those observations there still depends there.",
            "Still, some dependency cannot be fully resolved by all predicted by those attributes, right?",
            "So that means conditions on those latent function.",
            "There is no ID structure.",
            "Right, we cannot make such a conditional independence assumption.",
            "The reasons are probably from 2 perspectives.",
            "The first is those attributes are often weak predictors, right?",
            "Because they just based on gender or age.",
            "Those kind of information it cannot answer all the questions.",
            "The second thing is that those relation observations, their storage independency.",
            "That means if we are based on some of the observed element, they are informative enough to some extent to predict those missing guys."
        ],
        [
            "So in this work.",
            "We introduce a multi task model using both input and task specific features.",
            "How we introduce a nonparametric random effect to resolve this?",
            "Depending this kind of dependent noise.",
            "Those parts that cannot be explained or predicted by known attributes.",
            "And also always introduce a very efficient algorithm to do large scale inference on very large scale data.",
            "So we will show experiment result on Netflix data.",
            "So here is a very high level picture of."
        ],
        [
            "Model so essentially we model the relation observations as a linear model.",
            "And here is a one additive function, essentially depending on known attributes, right?",
            "If we make solely make use.",
            "This function to make predictions then this is just traditional supervised learning problem.",
            "But in addition to this, we introduce a random effect to model this kind of dependent noise.",
            "So in statistics, random effect means.",
            "A way to model dependency in observations.",
            "Across repeated structures or group structures.",
            "In collaborative prediction problems, it's very natural to apply this kind of idea because we have naturally observations grouped in columns or rows, right?",
            "Specifically, we let this random effect to be nonparametric.",
            "That means we don't pre restrict the dimensionality of this random variable or random function.",
            "So essentially less dimensionality grows with data size.",
            "So this is essentially an idea of generalizing low rank matrix factorization to nonparametric matrix factorization where we don't pray limit the dimensionality of our model."
        ],
        [
            "Since we're going to apply this model to very large scale data we have, we have to always keep in mind the scalability and efficiency issue.",
            "So we had to do some model simplification.",
            "The first thing we have done is to absorb this independent noise into rendering effect.",
            "Actually, this is quite a.",
            "Legal way, because here this this guy.",
            "This random effect is totally nonparametric.",
            "So that means it's.",
            "40 powerful enough to describe dependent noise and also independent noise.",
            "And then the next step will introduce a kind of generative process to generate the end function depending on known predictors and random effect function.",
            "The idea is that we assume we have two kinds of prior kernels, right?",
            "One kernel on the tasks task side, the other kernel on the data side.",
            "So starting from this kind of prior knowledge, we described the following generative process."
        ],
        [
            "So here we go.",
            "So here is the prior kernel.",
            "Across the.",
            "Data points, right?",
            "So then we generate a covariance function Sigma."
        ],
        [
            "Then we are.",
            "Generate that function depending on known attributes, and here is the another prior kernel across different tasks."
        ],
        [
            "Then we generate the random effect parts."
        ],
        [
            "Row by row.",
            "In dependently"
        ],
        [
            "In the end.",
            "We at least two times."
        ],
        [
            "Together and Top 10 the final response.",
            "So.",
            "This generated process looks a little bit special and even a bit strange because.",
            "There's no way to you know, sample, why do we?",
            "Sample is a covariance function only between.",
            "Columns are between data points.",
            "Why don't we we can do the same thing from the other side?",
            "Like here we generate."
        ],
        [
            "Covariance on this side and then this is a generate a random effect column by column right?",
            "So there is no reason to prefer one over the other.",
            "Play soul."
        ],
        [
            "Here is summarization of this two generated process and in this model we call is columnwise generated model.",
            "We generate, we generate a covariance across rows.",
            "And here in the other model we call Roy's model we generate.",
            "Covariance across columns, right?",
            "They seem to be different, but it turns out if."
        ],
        [
            "We marginalized all the latent functions latent variables.",
            "We got a marginal distribution for the observation ull matrix.",
            "They follow a so called matrix variate student process.",
            "So.",
            "This process has several parameters.",
            "Here is a prior covariance function across different rows, and here is the prior covariance function.",
            "Across different columns.",
            "And even we can make this two guys this Sigma know and.",
            "Oh my God, no.",
            "In the signal to be low rank, but the whole model is still nonparametric because we have this diagonal term here, right?",
            "So the function on my floor.",
            "Why is still essentially in the infinite dimensional space?",
            "And the model does something interesting because it's somehow learns or adapt, kind of latent covariance simultaneously.",
            "Due to this equivalence of these two sampling process.",
            "But there is something we can benefit from this equivalence, because in reality we always deal with finite matrix, right?",
            "And sometimes one dimensionality is smaller than the other size of dimensionality, so we can choose one model, which might be computationally cheaper than the other.",
            "Here is the exact case in a collaborative produce."
        ],
        [
            "In case typically we assume.",
            "I.",
            "One side of the matrix observational matrix is smaller than the outside, particularly in the movie rating case.",
            "They are in Netflix.",
            "There 20 about 20,000 movies and half million users, right?",
            "So one side is much smaller than the other, so in this case we choose the right model because we can simply the smaller matrix.",
            "And then draw random effect independently.",
            "Roll by roll.",
            "And the next thing we do is we let the kernel on this side the prior kernel on this side to be low rank.",
            "Yeah.",
            "So the whole model.",
            "On the final day to follow a matrix variaty distribution.",
            "Right?",
            "Although we make it as I mentioned before, although I I let this model to be low rank, but the distribution for why is still nonparametric.",
            "So here is a solid bit more details."
        ],
        [
            "We can write any essentially covariance function into kind of in the feature space as the inner product, and then we can let the this function can be also represented in a linear form.",
            "So without lots of generality with at least at the inner product between two linear features and then we can transform the generative process into the way that we don't sample instead of example another variable beta.",
            "Essentially, linear weights on the feature space."
        ],
        [
            "Then we do a approximate inference in the EM algorithm.",
            "Essentially we.",
            "Estimate Sigma and beta.",
            "Do and then treat other other things as random variables.",
            "So here's a pretty standard EM step we for for each role of the observations we estimate the posterior covariance and mean, and then we update the global parameters."
        ],
        [
            "So here's the yams equations."
        ],
        [
            "Is on the notation.",
            "This is a submatrix of the covariance matrix, essentially obtained by keeping the columns of the covariance matrix indexed by Set Ji.",
            "So this is a Ji is the subset of ratings given by user I.",
            "So if we directly."
        ],
        [
            "We implemented this algorithm.",
            "It's a, it's a very complex because we have to go through all the half million users and then it's it takes several thousands of hours for just a single iteration.",
            "And Luckily, actually we can simplify and the algorithm and avoid this complexity, because here for example, we can introduce a column selection operator.",
            "Essentially, if we multiply the covariance by this selector, we get this submatrix.",
            "Here.",
            "Then we are key observation is that in order to update those, keep those parameters.",
            "Actually, we don't need to compute the order mean and covariance in the posterior essentially.",
            "For example.",
            "We have to compute the summation of other posterior covariance is right in order to get this submission, we don't have to compute every CI because in this little bit details here essentially here is the inverse of a small matrix.",
            "Across the ratings given by a particular user, and then we can represent this multiplication by using this column selection operator.",
            "Then we can certainly find actually sear they share some common multiplication to the entire covariance matrix across all the users, right?",
            "That means there is some redundancy in the computation.",
            "That means we can remove this redundancy, but just keep only calculating the.",
            "Inverse on a small matrix is right, and the modification on UI that means this column selection operator is just a memory access.",
            "It doesn't cost any computation.",
            "Yes, and then we can reduce by applying similar tricks to computing other terms.",
            "In the end we can reduce the thousands of hours to five hours only for one computation one iteration."
        ],
        [
            "So here is the result on each movie data.",
            "We compared with the facts."
        ],
        [
            "Fast, several algorithms, including faster Max margin matrix, factorization, probabilistic principal component analysis based in here is based on stochastic relational model.",
            "And here is our model nonparametric rendering factor model."
        ],
        [
            "So here's the result, and those those models are pretty powerful or and really state of art performance and our model does better.",
            "And in terms of runtime, it's even faster.",
            "That's really surprising to us.",
            "I'm not sure.",
            "Surprising to you.",
            "So OK here I should mention that.",
            "The second version, the first version of model.",
            "Our model doesn't use any attributes.",
            "The second model use attributes from obtained from.",
            "Singular value decomposition of the binary matrix."
        ],
        [
            "Indicating iterating is observed or not."
        ],
        [
            "And here is the result on on Netflix data on the test set with comparisons with restricted Boltzmann machine and probably seek matrix factorization.",
            "And here Sir."
        ],
        [
            "And also this is a runtime performance in terms of."
        ],
        [
            "Hours.",
            "And the lost plot is a bit interesting.",
            "So here is the predictive standard deviation.",
            "And here is the measured standard deviation of residuals.",
            "It seems our model does pretty well not just in terms of runtime performance and also predictive accuracy, but also it provides quite nice assessment of the prediction uncertainties.",
            "So here's a devious summary of our work.",
            "Essentially, is a model to combine render effect known attributes and we make the model very efficient, so.",
            "Is actually very promising to perform a full Bayesian inference by Gibbs sampling and and we are currently working on this, thank you."
        ],
        [
            "Yes.",
            "Best per single model.",
            "I'm not sure.",
            "I think it's very competitive.",
            "I have read some paper they use some by top players in the in the competition.",
            "Those guys they are very smart and they come up with some model.",
            "They combine kind of know features, known features that with matrix factorization also in some way nonparametric they provide result almost seen at the same level.",
            "That is, that that one was not.",
            "Yeah, but this one is almost best, I think, based on the single model.",
            "Yes.",
            "Expect full Beijing treatment to be.",
            "Oh yeah, I think we have some a little bit of smart way to run the Gibbs sampler so that for one iteration for iteration, go over the whole data database.",
            "It took 8 hours.",
            "But now the Mo step took five hours per iteration, but we're trying to improve that.",
            "But I think you're right of phobias, and inference is more interesting because we 40 sample the covariance, right?",
            "Can you discuss the connection to the recalls model that she presented at the beginning of this section?",
            "Which model?",
            "Oh, that's what, OK?",
            "Yes.",
            "I think first my comment is that.",
            "Actually, in our previous work with focus and we have applied G GP models to collaborative filtering in our several of our NIPS papers and ICML papers, we, but we never made that large scale.",
            "One difference of our at least work to the previous work is that our case in the hierarchical Bayesian framework we put prior on the covariance, and I'm not sure if you knew Lawrence's work.",
            "For the prior on the covariance.",
            "Yeah, so it took essentially the inside is that in the end we got a model is a T distribution rights.",
            "Mateer distribution means you can tolerate.",
            "It's more robust.",
            "Ray square right?",
            "So it's not such a big difference.",
            "Actually, if you turn, if you look at the equivalence of the two generated process, you can see you can think about this model integrated one parameter.",
            "But you can think about is also generate a integrate out the other parameters.",
            "So this is kind of interesting perspective.",
            "So that's why I made the comments that the model somehow deal with uncertainties of the covariance on both sides.",
            "OK, well let's thank our speaker again, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so my name is Kyle.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with John FT from CMU and chocolate roll from NEC Labs.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this work is very much related to multi task learning, so let's first look at a traditional setting of multitask learning, where we have input data and for each input data we have multiple responses.",
                    "label": 0
                },
                {
                    "sent": "Some of them observed some of them not observed.",
                    "label": 0
                },
                {
                    "sent": "That means we have multiple different tasks here in each row is 1 task for one regression function.",
                    "label": 0
                },
                {
                    "sent": "So here we use MI to resent the ice regression function.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a very elegant way to model the dependency between those functions within the hierarchical Bayesian framework, where we assume there's a Gaussian process prior with covariance Sigma and then all the different tasks as random functions ID sampled from the common Gaussian process prior.",
                    "label": 1
                },
                {
                    "sent": "So then by learning.",
                    "label": 0
                },
                {
                    "sent": "Or adapting such a covariance function so we can learn the dependency across different tasks.",
                    "label": 0
                },
                {
                    "sent": "So essentially many are or maybe most of the current multitask learning approaches, Bayesian or non Bayesian, there quite related to this idea.",
                    "label": 0
                },
                {
                    "sent": "For example in the kernel system you learn essentially a kernel with some proper regularization on the kernel function like trace norm stuff.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we probably consider a more interesting setting where we have, in addition to the data features.",
                    "label": 0
                },
                {
                    "sent": "We also have a task specific features, so that means this is in a relational setting.",
                    "label": 0
                },
                {
                    "sent": "We have column wise features and then realize features.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "Is more of a more flexible way is to model the entire function jointly so where we have a so called relational function taking input from roll features an and polemize features.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then.",
                    "label": 0
                },
                {
                    "sent": "A very natural extension of the hierarchical Bayesian approach in Gaussian process framework is that we introduce a additional covariance function.",
                    "label": 0
                },
                {
                    "sent": "Taking input from task specific features right?",
                    "label": 0
                },
                {
                    "sent": "So then we have two set of kernels, one kernel describing the dependency across the data points and then we have another kernel describing the dependency across functions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In a special case, if we let this task specific kernel as Delta function, then the model reduced to the original model.",
                    "label": 0
                },
                {
                    "sent": "The previous model where you have lowered functions independently sampled, sampled from this Gaussian process prior.",
                    "label": 0
                },
                {
                    "sent": "So in this in general case if we have a task specific features then we don't have those.",
                    "label": 0
                },
                {
                    "sent": "You know independent sampling process because all the data points across different locations and the functions they depend on each other.",
                    "label": 0
                },
                {
                    "sent": "So we simply here in this case we draw a single function.",
                    "label": 0
                },
                {
                    "sent": "There is no ID structure there.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is pretty much the setting of a. Collaborative prediction problem because we have may have a movie.",
                    "label": 1
                },
                {
                    "sent": "Features user features.",
                    "label": 0
                },
                {
                    "sent": "We have user ratings or movies.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "As we are, this is essentially a multi task learning problem.",
                    "label": 1
                },
                {
                    "sent": "Is task specific features.",
                    "label": 0
                },
                {
                    "sent": "And this framework actually also go beyond normal matrix factorization problem, because here we use additional row and column attributes.",
                    "label": 0
                },
                {
                    "sent": "So this formulation is quite general.",
                    "label": 0
                },
                {
                    "sent": "It's applied to many related relational prediction problems like link prediction.",
                    "label": 1
                },
                {
                    "sent": "Gene protein protein interaction prediction and friendship prediction.",
                    "label": 0
                },
                {
                    "sent": "Yeah, where we we can utilize those node wise features.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So far the modeling the model sounds pretty powerful and elegant, but.",
                    "label": 0
                },
                {
                    "sent": "The big problem is the computation.",
                    "label": 0
                },
                {
                    "sent": "Because the inference complexity is the product of 2 cubic terms, it's a huge complexity to the inference.",
                    "label": 1
                },
                {
                    "sent": "I mean, in a typical for example in the Netflix data, so we have about half million users and approximately two 20,000 movies.",
                    "label": 0
                },
                {
                    "sent": "Then it's almost definitely impossible to apply that model to such a huge database.",
                    "label": 0
                },
                {
                    "sent": "Another challenge is from the modeling perspective because.",
                    "label": 1
                },
                {
                    "sent": "Actually a very common property in many relational learning problems is that those relational observations there are highly dependent.",
                    "label": 0
                },
                {
                    "sent": "That means even conditioned on those known predictors.",
                    "label": 0
                },
                {
                    "sent": "Those observations there still depends there.",
                    "label": 0
                },
                {
                    "sent": "Still, some dependency cannot be fully resolved by all predicted by those attributes, right?",
                    "label": 1
                },
                {
                    "sent": "So that means conditions on those latent function.",
                    "label": 0
                },
                {
                    "sent": "There is no ID structure.",
                    "label": 0
                },
                {
                    "sent": "Right, we cannot make such a conditional independence assumption.",
                    "label": 1
                },
                {
                    "sent": "The reasons are probably from 2 perspectives.",
                    "label": 0
                },
                {
                    "sent": "The first is those attributes are often weak predictors, right?",
                    "label": 0
                },
                {
                    "sent": "Because they just based on gender or age.",
                    "label": 0
                },
                {
                    "sent": "Those kind of information it cannot answer all the questions.",
                    "label": 0
                },
                {
                    "sent": "The second thing is that those relation observations, their storage independency.",
                    "label": 0
                },
                {
                    "sent": "That means if we are based on some of the observed element, they are informative enough to some extent to predict those missing guys.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this work.",
                    "label": 0
                },
                {
                    "sent": "We introduce a multi task model using both input and task specific features.",
                    "label": 1
                },
                {
                    "sent": "How we introduce a nonparametric random effect to resolve this?",
                    "label": 0
                },
                {
                    "sent": "Depending this kind of dependent noise.",
                    "label": 0
                },
                {
                    "sent": "Those parts that cannot be explained or predicted by known attributes.",
                    "label": 0
                },
                {
                    "sent": "And also always introduce a very efficient algorithm to do large scale inference on very large scale data.",
                    "label": 0
                },
                {
                    "sent": "So we will show experiment result on Netflix data.",
                    "label": 0
                },
                {
                    "sent": "So here is a very high level picture of.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model so essentially we model the relation observations as a linear model.",
                    "label": 0
                },
                {
                    "sent": "And here is a one additive function, essentially depending on known attributes, right?",
                    "label": 1
                },
                {
                    "sent": "If we make solely make use.",
                    "label": 0
                },
                {
                    "sent": "This function to make predictions then this is just traditional supervised learning problem.",
                    "label": 0
                },
                {
                    "sent": "But in addition to this, we introduce a random effect to model this kind of dependent noise.",
                    "label": 0
                },
                {
                    "sent": "So in statistics, random effect means.",
                    "label": 1
                },
                {
                    "sent": "A way to model dependency in observations.",
                    "label": 1
                },
                {
                    "sent": "Across repeated structures or group structures.",
                    "label": 0
                },
                {
                    "sent": "In collaborative prediction problems, it's very natural to apply this kind of idea because we have naturally observations grouped in columns or rows, right?",
                    "label": 0
                },
                {
                    "sent": "Specifically, we let this random effect to be nonparametric.",
                    "label": 1
                },
                {
                    "sent": "That means we don't pre restrict the dimensionality of this random variable or random function.",
                    "label": 0
                },
                {
                    "sent": "So essentially less dimensionality grows with data size.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially an idea of generalizing low rank matrix factorization to nonparametric matrix factorization where we don't pray limit the dimensionality of our model.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Since we're going to apply this model to very large scale data we have, we have to always keep in mind the scalability and efficiency issue.",
                    "label": 0
                },
                {
                    "sent": "So we had to do some model simplification.",
                    "label": 0
                },
                {
                    "sent": "The first thing we have done is to absorb this independent noise into rendering effect.",
                    "label": 0
                },
                {
                    "sent": "Actually, this is quite a.",
                    "label": 0
                },
                {
                    "sent": "Legal way, because here this this guy.",
                    "label": 0
                },
                {
                    "sent": "This random effect is totally nonparametric.",
                    "label": 0
                },
                {
                    "sent": "So that means it's.",
                    "label": 0
                },
                {
                    "sent": "40 powerful enough to describe dependent noise and also independent noise.",
                    "label": 0
                },
                {
                    "sent": "And then the next step will introduce a kind of generative process to generate the end function depending on known predictors and random effect function.",
                    "label": 1
                },
                {
                    "sent": "The idea is that we assume we have two kinds of prior kernels, right?",
                    "label": 0
                },
                {
                    "sent": "One kernel on the tasks task side, the other kernel on the data side.",
                    "label": 0
                },
                {
                    "sent": "So starting from this kind of prior knowledge, we described the following generative process.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we go.",
                    "label": 0
                },
                {
                    "sent": "So here is the prior kernel.",
                    "label": 0
                },
                {
                    "sent": "Across the.",
                    "label": 0
                },
                {
                    "sent": "Data points, right?",
                    "label": 0
                },
                {
                    "sent": "So then we generate a covariance function Sigma.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we are.",
                    "label": 0
                },
                {
                    "sent": "Generate that function depending on known attributes, and here is the another prior kernel across different tasks.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we generate the random effect parts.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Row by row.",
                    "label": 0
                },
                {
                    "sent": "In dependently",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the end.",
                    "label": 0
                },
                {
                    "sent": "We at least two times.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Together and Top 10 the final response.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This generated process looks a little bit special and even a bit strange because.",
                    "label": 0
                },
                {
                    "sent": "There's no way to you know, sample, why do we?",
                    "label": 0
                },
                {
                    "sent": "Sample is a covariance function only between.",
                    "label": 0
                },
                {
                    "sent": "Columns are between data points.",
                    "label": 0
                },
                {
                    "sent": "Why don't we we can do the same thing from the other side?",
                    "label": 0
                },
                {
                    "sent": "Like here we generate.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Covariance on this side and then this is a generate a random effect column by column right?",
                    "label": 0
                },
                {
                    "sent": "So there is no reason to prefer one over the other.",
                    "label": 0
                },
                {
                    "sent": "Play soul.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is summarization of this two generated process and in this model we call is columnwise generated model.",
                    "label": 0
                },
                {
                    "sent": "We generate, we generate a covariance across rows.",
                    "label": 0
                },
                {
                    "sent": "And here in the other model we call Roy's model we generate.",
                    "label": 0
                },
                {
                    "sent": "Covariance across columns, right?",
                    "label": 0
                },
                {
                    "sent": "They seem to be different, but it turns out if.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We marginalized all the latent functions latent variables.",
                    "label": 0
                },
                {
                    "sent": "We got a marginal distribution for the observation ull matrix.",
                    "label": 0
                },
                {
                    "sent": "They follow a so called matrix variate student process.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This process has several parameters.",
                    "label": 0
                },
                {
                    "sent": "Here is a prior covariance function across different rows, and here is the prior covariance function.",
                    "label": 0
                },
                {
                    "sent": "Across different columns.",
                    "label": 0
                },
                {
                    "sent": "And even we can make this two guys this Sigma know and.",
                    "label": 0
                },
                {
                    "sent": "Oh my God, no.",
                    "label": 0
                },
                {
                    "sent": "In the signal to be low rank, but the whole model is still nonparametric because we have this diagonal term here, right?",
                    "label": 0
                },
                {
                    "sent": "So the function on my floor.",
                    "label": 0
                },
                {
                    "sent": "Why is still essentially in the infinite dimensional space?",
                    "label": 0
                },
                {
                    "sent": "And the model does something interesting because it's somehow learns or adapt, kind of latent covariance simultaneously.",
                    "label": 0
                },
                {
                    "sent": "Due to this equivalence of these two sampling process.",
                    "label": 0
                },
                {
                    "sent": "But there is something we can benefit from this equivalence, because in reality we always deal with finite matrix, right?",
                    "label": 0
                },
                {
                    "sent": "And sometimes one dimensionality is smaller than the other size of dimensionality, so we can choose one model, which might be computationally cheaper than the other.",
                    "label": 1
                },
                {
                    "sent": "Here is the exact case in a collaborative produce.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In case typically we assume.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "One side of the matrix observational matrix is smaller than the outside, particularly in the movie rating case.",
                    "label": 0
                },
                {
                    "sent": "They are in Netflix.",
                    "label": 0
                },
                {
                    "sent": "There 20 about 20,000 movies and half million users, right?",
                    "label": 0
                },
                {
                    "sent": "So one side is much smaller than the other, so in this case we choose the right model because we can simply the smaller matrix.",
                    "label": 0
                },
                {
                    "sent": "And then draw random effect independently.",
                    "label": 0
                },
                {
                    "sent": "Roll by roll.",
                    "label": 0
                },
                {
                    "sent": "And the next thing we do is we let the kernel on this side the prior kernel on this side to be low rank.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So the whole model.",
                    "label": 0
                },
                {
                    "sent": "On the final day to follow a matrix variaty distribution.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Although we make it as I mentioned before, although I I let this model to be low rank, but the distribution for why is still nonparametric.",
                    "label": 0
                },
                {
                    "sent": "So here is a solid bit more details.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can write any essentially covariance function into kind of in the feature space as the inner product, and then we can let the this function can be also represented in a linear form.",
                    "label": 0
                },
                {
                    "sent": "So without lots of generality with at least at the inner product between two linear features and then we can transform the generative process into the way that we don't sample instead of example another variable beta.",
                    "label": 0
                },
                {
                    "sent": "Essentially, linear weights on the feature space.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we do a approximate inference in the EM algorithm.",
                    "label": 1
                },
                {
                    "sent": "Essentially we.",
                    "label": 0
                },
                {
                    "sent": "Estimate Sigma and beta.",
                    "label": 0
                },
                {
                    "sent": "Do and then treat other other things as random variables.",
                    "label": 0
                },
                {
                    "sent": "So here's a pretty standard EM step we for for each role of the observations we estimate the posterior covariance and mean, and then we update the global parameters.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the yams equations.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is on the notation.",
                    "label": 0
                },
                {
                    "sent": "This is a submatrix of the covariance matrix, essentially obtained by keeping the columns of the covariance matrix indexed by Set Ji.",
                    "label": 0
                },
                {
                    "sent": "So this is a Ji is the subset of ratings given by user I.",
                    "label": 0
                },
                {
                    "sent": "So if we directly.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We implemented this algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's a, it's a very complex because we have to go through all the half million users and then it's it takes several thousands of hours for just a single iteration.",
                    "label": 0
                },
                {
                    "sent": "And Luckily, actually we can simplify and the algorithm and avoid this complexity, because here for example, we can introduce a column selection operator.",
                    "label": 1
                },
                {
                    "sent": "Essentially, if we multiply the covariance by this selector, we get this submatrix.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Then we are key observation is that in order to update those, keep those parameters.",
                    "label": 1
                },
                {
                    "sent": "Actually, we don't need to compute the order mean and covariance in the posterior essentially.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "We have to compute the summation of other posterior covariance is right in order to get this submission, we don't have to compute every CI because in this little bit details here essentially here is the inverse of a small matrix.",
                    "label": 0
                },
                {
                    "sent": "Across the ratings given by a particular user, and then we can represent this multiplication by using this column selection operator.",
                    "label": 0
                },
                {
                    "sent": "Then we can certainly find actually sear they share some common multiplication to the entire covariance matrix across all the users, right?",
                    "label": 0
                },
                {
                    "sent": "That means there is some redundancy in the computation.",
                    "label": 0
                },
                {
                    "sent": "That means we can remove this redundancy, but just keep only calculating the.",
                    "label": 0
                },
                {
                    "sent": "Inverse on a small matrix is right, and the modification on UI that means this column selection operator is just a memory access.",
                    "label": 0
                },
                {
                    "sent": "It doesn't cost any computation.",
                    "label": 1
                },
                {
                    "sent": "Yes, and then we can reduce by applying similar tricks to computing other terms.",
                    "label": 0
                },
                {
                    "sent": "In the end we can reduce the thousands of hours to five hours only for one computation one iteration.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the result on each movie data.",
                    "label": 0
                },
                {
                    "sent": "We compared with the facts.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fast, several algorithms, including faster Max margin matrix, factorization, probabilistic principal component analysis based in here is based on stochastic relational model.",
                    "label": 0
                },
                {
                    "sent": "And here is our model nonparametric rendering factor model.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the result, and those those models are pretty powerful or and really state of art performance and our model does better.",
                    "label": 0
                },
                {
                    "sent": "And in terms of runtime, it's even faster.",
                    "label": 0
                },
                {
                    "sent": "That's really surprising to us.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "Surprising to you.",
                    "label": 0
                },
                {
                    "sent": "So OK here I should mention that.",
                    "label": 0
                },
                {
                    "sent": "The second version, the first version of model.",
                    "label": 0
                },
                {
                    "sent": "Our model doesn't use any attributes.",
                    "label": 0
                },
                {
                    "sent": "The second model use attributes from obtained from.",
                    "label": 0
                },
                {
                    "sent": "Singular value decomposition of the binary matrix.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Indicating iterating is observed or not.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is the result on on Netflix data on the test set with comparisons with restricted Boltzmann machine and probably seek matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "And here Sir.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also this is a runtime performance in terms of.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hours.",
                    "label": 0
                },
                {
                    "sent": "And the lost plot is a bit interesting.",
                    "label": 0
                },
                {
                    "sent": "So here is the predictive standard deviation.",
                    "label": 0
                },
                {
                    "sent": "And here is the measured standard deviation of residuals.",
                    "label": 0
                },
                {
                    "sent": "It seems our model does pretty well not just in terms of runtime performance and also predictive accuracy, but also it provides quite nice assessment of the prediction uncertainties.",
                    "label": 0
                },
                {
                    "sent": "So here's a devious summary of our work.",
                    "label": 0
                },
                {
                    "sent": "Essentially, is a model to combine render effect known attributes and we make the model very efficient, so.",
                    "label": 0
                },
                {
                    "sent": "Is actually very promising to perform a full Bayesian inference by Gibbs sampling and and we are currently working on this, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Best per single model.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "I think it's very competitive.",
                    "label": 0
                },
                {
                    "sent": "I have read some paper they use some by top players in the in the competition.",
                    "label": 0
                },
                {
                    "sent": "Those guys they are very smart and they come up with some model.",
                    "label": 0
                },
                {
                    "sent": "They combine kind of know features, known features that with matrix factorization also in some way nonparametric they provide result almost seen at the same level.",
                    "label": 0
                },
                {
                    "sent": "That is, that that one was not.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but this one is almost best, I think, based on the single model.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Expect full Beijing treatment to be.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, I think we have some a little bit of smart way to run the Gibbs sampler so that for one iteration for iteration, go over the whole data database.",
                    "label": 0
                },
                {
                    "sent": "It took 8 hours.",
                    "label": 0
                },
                {
                    "sent": "But now the Mo step took five hours per iteration, but we're trying to improve that.",
                    "label": 0
                },
                {
                    "sent": "But I think you're right of phobias, and inference is more interesting because we 40 sample the covariance, right?",
                    "label": 0
                },
                {
                    "sent": "Can you discuss the connection to the recalls model that she presented at the beginning of this section?",
                    "label": 0
                },
                {
                    "sent": "Which model?",
                    "label": 0
                },
                {
                    "sent": "Oh, that's what, OK?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I think first my comment is that.",
                    "label": 0
                },
                {
                    "sent": "Actually, in our previous work with focus and we have applied G GP models to collaborative filtering in our several of our NIPS papers and ICML papers, we, but we never made that large scale.",
                    "label": 0
                },
                {
                    "sent": "One difference of our at least work to the previous work is that our case in the hierarchical Bayesian framework we put prior on the covariance, and I'm not sure if you knew Lawrence's work.",
                    "label": 0
                },
                {
                    "sent": "For the prior on the covariance.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it took essentially the inside is that in the end we got a model is a T distribution rights.",
                    "label": 0
                },
                {
                    "sent": "Mateer distribution means you can tolerate.",
                    "label": 0
                },
                {
                    "sent": "It's more robust.",
                    "label": 0
                },
                {
                    "sent": "Ray square right?",
                    "label": 0
                },
                {
                    "sent": "So it's not such a big difference.",
                    "label": 0
                },
                {
                    "sent": "Actually, if you turn, if you look at the equivalence of the two generated process, you can see you can think about this model integrated one parameter.",
                    "label": 0
                },
                {
                    "sent": "But you can think about is also generate a integrate out the other parameters.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of interesting perspective.",
                    "label": 0
                },
                {
                    "sent": "So that's why I made the comments that the model somehow deal with uncertainties of the covariance on both sides.",
                    "label": 0
                },
                {
                    "sent": "OK, well let's thank our speaker again, thank you.",
                    "label": 0
                }
            ]
        }
    }
}