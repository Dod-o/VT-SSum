{
    "id": "aywbqpzaorbhfpucce6pebcctozmo56t",
    "title": "Machine Learning, Probability and Graphical Models",
    "info": {
        "author": [
            "Sam Roweis"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/mlss06tw_roweis_mlpgm/",
    "segmentation": [
        [
            "OK.",
            "Thanks.",
            "I'm not used to being half an hour behind before even start, but I'll try and try and get a slightly back on schedule so it's my job to give you sort of introduction to the whole idea of machine learning and then to try and introduce one of the many different important theoretical foundations of machine learning, which is the probabilistic approach.",
            "So yeah, I think is this microphone just for the camera?",
            "OK, so I'm going to use two microphones.",
            "OK.",
            "Thanks so I want to emphasize before I start that the probabilistic approach is one of the core ideas in machine learning, but it's not the only approach and you'll hear a lot more during the rest of the lectures about other approaches, but whether or not you use the probabilistic approach in any particular problem, it's important to know about it, because it's very influential in how ideas develop and how you think, and it is often an important technique that you should be using in your problem, but it's not the only one, so I want to start off by showing you a.",
            "Video if I can find it here.",
            "And then hopefully this video will get you thinking about.",
            "About the whole idea of machine learning here, so I'll let this run for a few minutes.",
            "This is a video that was originally produced by Paul Viola who's at Microsoft Research in in Seattle, and it's sort of easy if you look at this video for a few minutes to figure out what's happening in this video, right?",
            "What's happening is the computer is putting a blue box around every face that it sees in every frame, and this video is impressive for several reasons.",
            "One is that you can actually run this system in real time on a laptop.",
            "So it's not like they took the video and then they computed on it for six days and they made this video.",
            "You can actually do this in real time and the other thing that's impressive, although you could argue as a weakness of the system, is that it's doing the analysis each frame at a time.",
            "It's not taking advantage of the fact that the frames are sequential, so if you just gave it a single photo, the performance would actually be similar.",
            "So the question that I want to try and get you to think about is if you had to write a computer program like this.",
            "Let's assume you don't know bout Viola Jones, face detectors and all that stuff.",
            "How would you go about doing it?",
            "OK, that's really the kind of core problem of machine learning is you have a task which is relatively well defined, like detecting faces and images, and you need to write a program, but it's not really at all clear how you should start writing that program.",
            "So the approach that I'm going to discourage you from taking is the approach of trying to just say, well, I'm smarter than the world.",
            "I know what it faces.",
            "A face has two eyes and a nose and a mouth.",
            "And I'm just going to write two I detectors and a nose detector and a mouse detector, and I'm going to run them on the image and then I'm going to, you know, specify in my code exactly what it means to be an eye of something round with a dive in the middle and and that's going to work, and what you should convince yourself before you do anything in machine learning is that is never going to work.",
            "The world is too complicated for that.",
            "OK so?"
        ],
        [
            "The essential approach to building intelligent systems, which is codified in machine learning, is the idea of using examples from the real world to train a system which will then do the task that you want.",
            "So for example, if you want to train a system to do face identification to go from a picture like this to a name, then again hand programming is not really possible here.",
            "So what's the solution?",
            "The solution is to get the computer to program itself at some level.",
            "By showing examples of the kind of behavior we want so in the face problem we would show the computer a lot of images and say these images have faces in them and then we would show the computer a lot of images and say these images don't have faces in them and then we would try and automatically from that data set learn what it is about a face.",
            "OK so this is the learning approach to AI and I think that it's very you can make a very strong argument that this approach essentially dominates intelligent systems in the modern day.",
            "There were older approaches based on rules that were coded by hand and expert systems and stuff, but those have almost universally been eclipsed by by learning machines in a huge range of applications.",
            "So the way you should think of this, it's not really that the computer is programming itself completely, really what you're doing is you're writing a program that has thousands or millions or 10s of millions of undefined constants in it.",
            "So imagine you know those of you who are C programmers.",
            "Imagine you literally wrote a program, but at the top of the seat program there were number defined constant 19 constant, two never like 100,000, or a million of those.",
            "OK, and now you want a way of setting those constants.",
            "Those constants control the behavior of your program for any setting of those constants is a different program that runs.",
            "How do you set those constants?",
            "That's really what the game of machine learning is about.",
            "Tweaking these real numbers inside a computer program that you already wrote to make the computer program behave the way you want.",
            "So geometrically, you can think of this as any particular program that you write with undefined constants defines a space of possible executables, right?",
            "If there's a million parameters, it's a million dimensional space, and the goal of machine learning is to put you in the right spot in that million dimensional face.",
            "To solve your problem, face detection or spam email or whatever it is.",
            "So our goal in life is to estimate internal parameters of a computer program whose structure we specified by using an existing data set.",
            "And that data set somehow cap."
        ],
        [
            "Sure is the correct behavior that we want, so you can think of statistical machine learning models as sort of probabilistic databases.",
            "So traditional database technology can't really answer any query about items that were never loaded into the data set, and the idea of a probabilistic model or statistical models that will talk about for the next few lectures is that they can build a probabilistic answer to any query about the about the database.",
            "Oh thanks.",
            "OK, so the these models also have a lot of other advantages.",
            "They allow you to make decisions given partial information by taking into account the uncertainty or unobserved ability of what you don't know they count for noisy sensors or actuators because a lot of measurements that you take in the real world are not accurate for physical reasons.",
            "They can explain things that aren't part of your model explicitly, because they allow for uncertain or stochastic behavior and.",
            "So the probabilistic approach has a lot of other advantages, and it's really.",
            "Allows you to do automatic system building in a way that expert systems don't."
        ],
        [
            "Don't allow you to.",
            "So just to complete the sort of three minute sales pitch at the beginning here, this is just a sort of laundry list of applications where machine learning, and in particular probabilistic approaches have been have been used, so speech recognition.",
            "OK, this doesn't seem to be over, here goes.",
            "So speech recognition and speaker verification.",
            "If you've ever interacted with the telephone system where you read your credit card number or you ask it to navigate through voice menu, those are using probabilistic models.",
            "In fact, hidden Markov models of the kind you'll learn about at the end of these lectures.",
            "Same thing for text, handwritten text or OCR face location tracking and video search and recommendation.",
            "Financial prediction.",
            "An fraud.",
            "All of these things here.",
            "Medical diagnosis AI in games is now becoming a big deal, so modern video games have adaptive opponents.",
            "Computer opponents that use machine learning, scientific analysis, and then all kinds of other things.",
            "So I just want to convince you that this is not just you know the problem of detecting faces are saying whether your email is spam or not, that these systems are deployed much more widely than than you know, almost all big companies that have huge datasets are doing some kind of learning on that data.",
            "If only just to detect outliers or problems or make decisions about which things they should pursue and which things."
        ],
        [
            "I should drop.",
            "OK, so in machine learning in general there's some Canonical problems that are sort of overall labels or groups for general tasks that we might want to might want to attack, and actually this afternoon and tomorrow morning John Langford is going to talk about the equivalence between different problems in this set, but for the purposes of getting started, you can think of these as different kinds of problems that you might want to solve, each of which has.",
            "Its own sort of setting its own inputs and outputs and desired goals.",
            "So the most common approach or set of problems is supervised learning.",
            "In supervised learning, you're given some examples of the inputs that correspond to what you are going to see in the real world.",
            "The desired outputs and your goal is to predict the outputs on future inputs.",
            "So, for example, is this microphone working properly?",
            "Is sort of cutting in and out, or if you're in the back row, put up your hand.",
            "OK, good.",
            "That was a test of both the microphone and the people in the back row.",
            "OK, so in the supervised learning problem you have some inputs and you have a desired output.",
            "So the input might be some description of your email and the desired output might be a label spam or not spinning, or the input might be a frame from a video camera and the desired output might be the location of any faces or the desired input might be some history about a currency and the desired output might be sell or buy.",
            "OK, so these are all supervised learning problems.",
            "And the names that you're probably familiar with are things like classification, regression, Time series prediction, usaral, supervised learning problems.",
            "Unsupervised learning is in some ways more exciting, and in some ways less well defined.",
            "So unsupervised learning is kind of like the radical cousin of supervised learning, right?",
            "Everyone thinks it's kind of cool and interesting, but nobody's really sure exactly what it's all about.",
            "So unsupervised learning you're given only the inputs.",
            "And the goal is to automatically discover some kind of structure or representation or features.",
            "So clustering, outlier detection, compression, or coding.",
            "These are unsupervised learning problems and the problem is, for example for clustering.",
            "It's not really clear how you measure a clustering.",
            "If I tell you I got all this input and I decided to cluster these hundred guys together and these other hundred guys together in these under hundreds together, and then I say, well, do you think I did a good job?",
            "We have to have some quantitative way of measuring that, which I'll show you a bit later.",
            "There's a couple of other problems.",
            "One is rule learning, which is really a sort of subset of unsupervised learning for finding very, very common joint settings of the measurements in your data set and reinforcement learning, in which in addition to inputs and outputs, you add the idea of actions and state of the world.",
            "So reinforcement learning is really a scenario where not only can you observe things in the world, but your agent.",
            "Your computer program can take specific actions that affect the world.",
            "It's not just that you.",
            "Receive some input and you do some processing and you reporting output.",
            "You can actually take an action which will affect the world and in particular affect the future distribution of inputs that you see.",
            "These last two.",
            "I'm not going to talk about just because we don't have very much time, but there's a lot of people here who know much more about reinforcement learning when I do so."
        ],
        [
            "Here's something from them.",
            "OK, so I just want to try and press forward here and keep focusing on this image example and actually trying to say how would we actually start solving this problem.",
            "So one key issue here is how do we represent information about the world?",
            "Your computer can't actually see anything, so when you plug your USB camera into your computer, what actually comes out of the USB camera is just some stream of pixels which pixel values which represent the image, right?",
            "So we need some way of encoding this image into that computer program that I was talking about, whose structure is defined, but whose constants are undefined.",
            "In order for us to start doing learning an.",
            "This unfortunately is probably the most important part of a machine learning problem, and the part which we formally know the least about in terms of how I can recommend to you that you do this so it's crucial how you numerically represent the input to your problem in the program.",
            "It's also hard to decide a prayer and what the best representation is, but.",
            "For example, example here.",
            "Imagine that you just listed the pixel values we just listed this pixel and then this pixel in this pixel, and so on.",
            "Every row and that would turn this image into a huge vector of numbers, and that would be our representation.",
            "So no matter what we have to pick a way of numerically representing the input.",
            "If it's spam, we might decide on a fixed set of words, and we might represent it as a binary vector.",
            "Whether or not that word occurs in the email 01011007.",
            "OK, that would be one representation of the email.",
            "It's not perfect.",
            "Remember that this image representation loses a lot of information.",
            "It loses the information about which pixels are beside each other.",
            "The spam representation loses the information about the order of the words.",
            "If you just do binary word occurrence.",
            "So there's some problems with every representation, but you need to pick one.",
            "Once we've done this, we're going to think of these numerical values as random variables.",
            "So for those of you who are bit rusty on your probability and statistics, a random variable you can think of is just like a variable in a computer program.",
            "It represents a certain quantity, like the intensity at this pixel here, but its value changes depending on which data our program is looking at, or, depending on what uncertainty we have about that value.",
            "So what we do instead is we use probabilities to represent distributions over these values."
        ],
        [
            "So that's the first important concept here.",
            "In the probabilistic approach.",
            "So the probabilistic approach.",
            "This is now.",
            "This is almost like a religious statement that I'm going to make now, right?",
            "The idea of the probabilistic approach is that you should use random variables to represent the world.",
            "Not everyone agrees with this statement.",
            "In fact, I don't really even fully agree with this statement all the time.",
            "But in explaining the probabilistic approach, what you do is you start off with this philosophy.",
            "We're going to use random variables to represent the world so.",
            "Each pixel or each word flag is considered a random variable, and random variables can be different types.",
            "They can be discrete, so for example 01 or email.",
            "Regular email versus spam or the weather.",
            "Sunny, overcast, raining.",
            "Maybe we should add a value here tornado.",
            "Or they can be continuous quantities, like a temperature or an income or a blood pressure or an exchange rate.",
            "Or the Las Vegas odds on some event.",
            "And these random variables represent both the inputs of our problem, the outputs of our problem, and any internal states that we want.",
            "And generally we have root same quantities.",
            "For example, we might have many pixels.",
            "Now, instead of we're going to quickly run out of both Roman and Greek letters.",
            "If we given you letter to each random variable, right?",
            "If I say I'm going to represent the first pixel by X and then the second pixel by Y, and then the third pixel lights it OK if my image has a mega pixel.",
            "I'm going to run out of letters, so we just use subscripts to index these things so X sub I is typically the value of the input variable be high pixel, and if we have lots of different images, lots of frames.",
            "Then it's on the end frame and the same thing.",
            "Why I'm going to typically use Y for the output, so X is the input.",
            "Why is the output and then when we want to write these things as a vector, we just make it a bold.",
            "That's just a sort of notational convention.",
            "If I use a capital letter here to represent a matrix, that's all of the data stack together."
        ],
        [
            "So that's just notation.",
            "OK, so now we have the first piece of the puzzle.",
            "We know that we're going to take what we know about the world we're going to represent it as a numerical vector.",
            "We're going to treat that numerical vector as a set of random variables.",
            "OK, in a probabilistic model, and now we're ready to sort of, say how we're going to set up the problem of learning.",
            "What is the structure of our computer program, whose constants that we don't know if we're going to fill it.",
            "So the idea here is that we have some inputs and their expressed in whatever representation, and we want to calculate sum.",
            "But based on those inputs, for example, maybe it's a binary decision.",
            "Is this a picture of Sam or not?",
            "And our computer program is going to just be a mathematical function.",
            "It's going to be some function F applied to the inputs X and then it's going to be the output that our computer program produces.",
            "And the crucial idea here is that we don't just make up functions out of thin air.",
            "That would be like going back to the way of doing things, which I criticize, which was just to design everything by hand.",
            "If you just sat down here, So what function smells good to me too?",
            "I like this function and then you just wrote that function down and you put some random constants in it that wouldn't really give you enough flexibility to tackle machine learning problems in general, so we don't make up functions out of thin air.",
            "We select functions from a pre specified set which is known as our hypothesis space.",
            "OK, so you can think of our hypothesis space as a set of functions, and for every function in that set there is 1 setting of the parameters, Theta which indexes back to that function.",
            "Or equivalently for any parameter Theta, it gives us a single function.",
            "So you think of this is just a nob on the box of functions.",
            "Every setting of data gives you a new function from input outputs, and your job is to find the setting of the NOB, but gives you the function you're most happy with.",
            "So the hardest part of doing probabilistic learning is deciding on this line here.",
            "This line looks very innocuous, right?",
            "It took me like 3 seconds to type this in latex, but this line carries with it a lot of responsibility.",
            "You have to come up with the representation of the inputs you have to come up with a hypothesis class and you have to come up with some way of expressing the outputs of the program.",
            "Once you've done this, you could argue that everything else is just mechanical.",
            "Once you've written this down.",
            "There are very well specified rules of probabilistic inference that tell you how to proceed.",
            "You receive the data from the world you in further posterior distribution over the parameters given your model in the data you use that posterior distribution to make predictions about what's going to happen.",
            "It's basically all mechanical.",
            "Now.",
            "Sometimes those mechanical operations are so large in the computer that you can't do them directly, and then you have to be smart about how to do them.",
            "But in principle, once you write down this line.",
            "That's the end of your contribution to the probabilistic approach, right?",
            "OK, so I can't emphasize enough how important this is elections."
        ],
        [
            "OK, so I said something just on the last slide that hopefully didn't just wash past you, which was that your job in life is to set the NOB beta to pick the function from the function class, which makes you happiest.",
            "Yeah, but what does that mean makes you happy right?",
            "You just like you like the way it looks.",
            "OK so we have to specify some quantitative way of saying which functions in the function class are good and which functions in function class are bad.",
            "So this is the last piece of the puzzle.",
            "There's a representation of the inputs and the outputs.",
            "There's a hypothesis class, which is the set of functions that we're considering, and now there is the loss function and that loss function tells you for any particular function that you select from your hypothesis class, how happy are you with it?",
            "How much do you think?",
            "It's going to really solve the problem that you can you know the problem that you're meant to do now in the probabilistic approach, the hypothesis class and the loss function are sort of the coupled.",
            "That is to say, once you write down your probabilistic model of the world learning about the world from the data that you have in principle doesn't really have to interact with the loss function.",
            "You should be able to just infer the posterior of the parameters given what you saw in the world and then.",
            "From those posterior parameters you can decide what to do, what output to give, what action to take independently by introducing the loss function.",
            "It's rare that you can make that separation completely clean, so for today's lecture we can think of a loss function as just the third component.",
            "Did our learning approach, and we're going to use them altogether.",
            "So the idea here is the inputs that you have in front of you are X.",
            "The correct answers that you're given RY, so this would be like when we're training the face detector.",
            "The inputs are all of the images that we're going to use to train in the answers.",
            "Why are the labels that you know we gave or the undergraduates that we hired to label the frames gave to the frames whether they have a face or not?",
            "And then are the outputs of our machine, which we hope are going to match the correct answers.",
            "Why not only on the input data X that we have, but on all future data.",
            "OK, so once we have this, how do we select the parameters and the idea is you come up with a loss function or a penalty function which says for any particular function in your hypothesis class, how well is it doing OK?",
            "So the intuition is very simple, safer classification.",
            "Veloc function tells you how many mistakes are you making on the training data.",
            "So how many times did the output of your machine?",
            "This is Ed on the input?",
            "How many times was it different from the correct output line?",
            "So this is again just a piece of notation in the square brackets.",
            "I'm going to use to mean a binary indicator of whether the thing inside is true or not.",
            "So this is just counts the number of mistakes you made on the training data, and that's just a very simple thing.",
            "It says I like functions from my hypothesis class which do well on the training data which gives small number of errors on the training data.",
            "In regression, you might ask for the minimum you might try and make the loss function the mean squared error.",
            "So this is the squared error between your target outputs Y and your prediction in clustering.",
            "Remember, I said it's very difficult to come up with a good cost function for clustering.",
            "Well, here's one that people use a lot in clustering.",
            "What you want to do is you want to minimize the distance between each example X and the cluster center, or the average of the other examples that were assigned to its cluster.",
            "So this is like saying things in the same cluster should be close together.",
            "OK, so all of these are different loss functions and there's thousands more.",
            "OK, so it's up to you to pick a loss function which trades off two things.",
            "So one thing that your loss function needs to do is it needs to indicate conditions similar to the conditions that you really care bout in the real world.",
            "So for example in spam if your loss function says I care more about false positives and false negatives then that should reflect what your users of your email system really care about.",
            "Or in face detection, your loss function should indicate how many faces in the image were detected.",
            "Maybe if you say that the image has six faces and they were really seven, that's better than saying that the image has no faces when they were really simple.",
            "OK, so that's one thing is the loss function should be as close as possible to what you really care about in solving the task.",
            "But there's another thing which is The Dirty secret of machine learning that people don't tell you that your loss functions job is.",
            "Your loss function has a second job in life, which is to make your problem tractable.",
            "OK, and just like in the rest of life where there's the ideal way to do something, and then there's you know the way you have to do it to get things done.",
            "The same thing is true in machine learning.",
            "There's the ideal loss function which we would really love to minimize, but there is sometimes a tradeoff that you have to make to back off to a more tractable loss function that's close to the loss function you like, but much easier to deal with computation.",
            "So sometimes you'll see people write down the loss function, like hinge loss or like log likelihood or conditional likelihood or.",
            "Posterior probability and you'll say why are they writing down that loss function?",
            "I mean, do I really care about the conditional log likelihood of the label?",
            "No, what I really care about is did I get it right or wrong?",
            "Why didn't they write down the loss function?",
            "You know, 01 loss and the answer is because the loss function in addition to trying to help your problem workout well, has this second job of making your problem tractor."
        ],
        [
            "OK, so there is an important issue here about training versus testing, which it's crucial that you understand for these lectures and everyone who comes after me.",
            "So training data are the inputs and outputs we were given at the time when we have the task of building the computer program.",
            "That's the data we have sitting on our disk.",
            "You know, when our boss tells us I want you to write a spam detector or face detector or currency predictor.",
            "The testing data are the inputs and outputs that we'll see in the future, and the problem is we don't know the testing data training time.",
            "If we knew the testing data, training time, everyone's life would be a lot easier.",
            "OK, so the training error is the average value of the loss function that we get on the training data, and that error we can see for any setting of data.",
            "We have some training error and we like thetas that give us low training error.",
            "The testing error is the average value of the loss function on the test data and that we don't know 'cause we don't have the test data.",
            "And once our real goal in life is our goal in life, to do well on the training data, well, no.",
            "We we already have the answers for the training data.",
            "We don't care whether those emails are spam or not.",
            "Spam, because they were already sent.",
            "But we care about his future emails, whether their spam or not spam.",
            "So ideally we'd like to minimize the test error.",
            "So how can we minimize the test error if we don't have the test data?",
            "OK, and this is where the probabilistic framework is going to come and rescue us.",
            "We're going to make a crucial assumption here, which is going to allow us to actually do something which minimizes the test error, even though we never saw the test data.",
            "That might sound spooky to you, but I'll tell you this."
        ],
        [
            "And then you'll see how it works.",
            "So here's the assumption.",
            "It's the sort of IID sampling assumption.",
            "Imagine that our data is created randomly from a joint probability distribution over the inputs X and the outputs Y, which we don't know, and then the core assumption here is that both the training data and the testing data come from the same distribution, P. Annan training time were given us finite sample X1Y1X2Y2 up to XNYN from this.",
            "And the members of this the input output pairs are generated independently and identically from this distribution P. The test data is also going to be generated IID from the same distribution, and the question is looking only at the training data.",
            "Can we build a machine that has a low loss, low expected loss on the test data?",
            "So when you sample is drawn for testing from the same distribution and we run the machine and we get some loss and we want to know by looking at the machine we built the training data and the training error, what can we say about the tester?"
        ],
        [
            "And one of the sort of most amazing results in, you know, the theory of computer science and applied statistics, is the result that yes, you can actually say something about the test error if you just have the training data under the assumption that the training data and testing data come from the same unknown distribution, that's a like a phenomenal watershed result.",
            "I think in applied statistics that goes all the way back to Vapnik and Turban Ankerson even earlier and.",
            "I think that it's really, you know, started off people thinking along these along these lines.",
            "So the crucial concept here is a concept called generalization.",
            "OK, so what's the danger?",
            "The danger in the above setup is that we're going to do really well on the training data, but very poorly on the testing data, and that phenomenon is doing well on the training data, but poorly on the testing data is called overfitting.",
            "So for example, there's something called the sure thing hypothesis.",
            "As far as I know, David Mackay is to be credited with coming up with this.",
            "OK, so here's the sure thing hypothesis.",
            "You memorize the training data and you produce garbage on the test data.",
            "OK, that's if I want.",
            "I can put that function in my hypothesis class, right?",
            "Now that function is kind of ridiculous, right?",
            "I mean, it just memorizes the training data, so of course it gets perfect results on the training data and it gives random outputs on the test data source is going terribly on the test data, so why should we care about this function within that scary about this function?",
            "It has very low loss at Test time at training time.",
            "Excuse me.",
            "So if our only guide to picking functions is a big functions that have low training loss, then that function the sure thing hypothesis is going to swoop in and confuse us and cause us to pick a very stupid learning machine.",
            "So.",
            "The idea here is that you can't learn anything about the world without making assumptions.",
            "OK.",
            "So this has been formalized in many, many ways, including the so called no free lunch theorems by Wolpert.",
            "Those theorems have their problems, but they they indicate an important point, which is that if you really believe a priority that any function in the world is possible, you can never learn anything.",
            "So the ability to achieve small loss on test data is called generalization, and that's the real goal is to build a machine that has good generalization.",
            "We could care less about the training data we already have delayed."
        ],
        [
            "Right?",
            "So in order to be able to say something strong to guarantee the ability to generalize to get low loss on test error, we have to control something called the capacity, which is the complexity of the hypothesis.",
            "So remember, I told you learning is really searching hypothesis, space learning is all about trying to find the magic setting of that knob, Theta, which gives you low loss at Test time, and then we sort of said, well, how are we going to evaluate the test loss when we don't have the test data.",
            "So we're going to make this a sampling assumption.",
            "The test data comes from the same distribution as the training data.",
            "OK, so here is the inductive learning hypothesis.",
            "Generalization is possible.",
            "OK, that's a very strong statement.",
            "That statement says if you make assumptions about the world, you can sometimes actually prove that you're not going to do too badly at Test time under sampling assumptions.",
            "So in words, what the inductive learning hypothesis tells you is that if the machine performs well on most of the training data, and it's not too complex, it will probably do well on similar tested.",
            "So this statement can breakdown in many ways.",
            "The obvious way it can breakdown is you can fail the first condition.",
            "You can train your classifier and it can do really badly with the training data.",
            "OK, well then you really shouldn't be that surprised when it does badly on the test data, right?",
            "Didn't even do well on the training data and that's the way that most people focus on most people try and correct this problem, but there's two more things you need to correct if you really want to protect yourself, one is you need to make sure that it's not too complex.",
            "Now the sure thing hypothesis, which memorized the entire training set, was very complicated, right?",
            "'cause it memorized the whole training set, so the training set was.",
            "3 gigabytes on disk.",
            "Then that classifier had a memory footprint of at least three gigabytes, so it's very complicated beast.",
            "So that's why the sure thing hypothesis is bad, because it's not a simple classifier, and this is the third assumption that you have to satisfy.",
            "The test data has to be similar.",
            "You would be amazed how many people go out into the world in the world, and they say I'm going to do machine learning and they collect their training data, and then they trained our classifier and they make sure that their classifier isn't too complicated and they could quit raining here and then they apply it to test data.",
            "That's completely different than their training data.",
            "And then it doesn't work.",
            "And they say, oh, I'm so stunned.",
            "Didn't the inductive learning hypothesis protect me?",
            "What's going wrong?",
            "Well, what's going wrong is you're applying it to totally different testing data, so make sure please when you go and you do machine learning, three things, one you actually managed to train a good classifier.",
            "Most people can do that too, but your classifier is not too complicated.",
            "You have to control capacity and three do a sanity check that you're testing.",
            "Data is at least somewhat similar to your training data before you start complaining that things aren't working.",
            "So these three things, OK, but the amazing fact if you satisfy all these then.",
            "You can actually prove that generalization is possible.",
            "There's a whole beautiful branch of mathematics which, in my opinion sort of represents the best possible intersection of computer science and mathematics called learning theory, and there's you know many, many theorems about VC dimension and PAC Learning and PAC Bayes and stuff in this area.",
            "Roughly speaking, if our hypothesis space is not too complicated, it has a low capacity in some formal sense, and if our training set is large enough and we do well on it, then you can bound the probability of doing significantly worse on the test data than on the training data.",
            "OK, having sort of told you about all this, you know beautiful formalism.",
            "We're now going to do something very simple.",
            "You have to crawl before you can walk, right?",
            "So we're just going to control capacity by adding a penalty to the loss function.",
            "We're going to say there's certain parameters, which I think allow my model to be complex.",
            "Those I'm going to penalize 'cause they're bad and there's other parameters which allowed my model to be simpler and those I'm going to allow because they're good and that's the job of this function P. So he looks at your parameters and he said, Nah, I don't like those parameters, they're going to make a complicated model.",
            "For those parameters look fine.",
            "They're going to be simple, and then we just take the sum of the loss function, which is the training error and the penalty on the parameters, and this is called penalized or regularised learning, and that's how we control capacity.",
            "So it says it's good to fit the data well.",
            "It's good to get low training error, but it's also good to bias yourself towards simpler models."
        ],
        [
            "OK, so this is just a picture to drive this home.",
            "I want you to focus on this right hand side here.",
            "This is a truth table.",
            "So let's say I gave you this training data.",
            "It's a function which has three binary inputs.",
            "Here they are X1X2X3 and a single binary output.",
            "Why there's why?",
            "And here's the training data I tell you, if the input is 000, the output is 0.",
            "If the input is 010, the output is 1, and so on.",
            "All of these lines that are filled in or your training data.",
            "And then I go over here and I say, could you please use your fancy machine learning technique to predict for me what the output is going to be when the input is 001?",
            "So if you look at this, you should start to get really scared.",
            "You should be thinking to yourself this is ridiculous.",
            "This is arbitrary truth table right?",
            "This value could be either zero or one.",
            "I mean, how am I ever going to know what this value is right?",
            "These values here have nothing to do with this value in principle.",
            "Now you're starting to understand if you don't make assumptions about the world, you can never learn anything.",
            "So this is an example where the fact that you have to make an assumption really is obvious.",
            "It really comes in slaps you in the face.",
            "Of course you have to make some assumption, like maybe this function is a conjunctive normal form.",
            "Or maybe it's a sum of two clause literals.",
            "Make some assumption about the form of this.",
            "And now you can start to fill in these values because there's only one function which is consistent with the training data.",
            "Now this is a picture where the fact that you have to make assumptions doesn't seem so obvious.",
            "Imagine I gave you the training data which were these circles here, and I told you, please predict the value of the function somewhere in between.",
            "Most of you would have no problem doing that, right?",
            "You would say what's the big deal.",
            "Let's just draw some kind of line like this through the data and you know interpolate on this line.",
            "Drawing that line through the data is exactly the same as making some assumption to fill in the truth table.",
            "Don't fool yourself that because you have a big visual cortex and it like smooth things that this is you know more natural assumption than any assumption you would make here.",
            "When you draw a line and you interpolate, you are making an assumption.",
            "You're making an assumption that this function is smooth in some very simple L2 cents of smooth, and that's fine if that's the assumption you really believe after know that you're making an assumption.",
            "OK, so an unbiased learner can never generalize if you don't make assumptions about the."
        ],
        [
            "But you can never learn anything.",
            "OK, so enough of me sort of pontificating.",
            "I want to sort of get moving on the probabilistic approach here.",
            "So given the above, what we can think of learning as doing is estimating joint probability functions given samples from those functions.",
            "So classification and regression supervised learning are about estimating the conditional density of the outputs.",
            "Why given the inputs supervised learning problem, I show you the representation of the image and you tell me whether it has a face in it.",
            "I show you the word counts in the spam in the email and you tell me whether it's camera unsupervised learning.",
            "You can think of is just density estimation.",
            "I give you some input sex and you want to estimate UX.",
            "But either way, the central object of interest is the joint distribution between the inputs and the outputs, or just between the inputs.",
            "If there's no outputs and the main difficulty is compactly representing this joint distribution and robustly learning its shape from a small number of samples an our inductive bias.",
            "Is expressed as a prior assumption about these joint distributions.",
            "So in the probabilistic framework, everything is about estimating large joint distributions between inputs and outputs.",
            "That's the whole game and all of your assumptions are prior assumptions on what these distributions can and can't be.",
            "They represent prior either prior distributions over explicit parameters or implicit distributions over classes of distributions.",
            "So the main computations we're going to need to do in the probabilistic approach are to efficiently calculate marginal and conditional distributions from whatever compact representation of the joint distribution we choose."
        ],
        [
            "So the goal here, focusing on the probabilistic approach now is to represent a joint probability distribution P of X compactly even when there are many variables OK.",
            "So just to drive this home, imagine you have a 1 megapixel image that comes off your camera and you have a pretty good CCD, so records you know 16 bits of information about each color channel, and now you want to ask how many possible images could my camera take?",
            "OK. Well, that's one megapixel raised to the 3 * 16.",
            "That's like an unbelievably mind blowingly large number.",
            "OK so if you just wanted to represent the distribution over images by saying, well, for every possible image I'm just going to have a probability and those probabilities are just numbers and they sum up to one.",
            "OK, now you're going to be in deep trouble 'cause that probability table that you're thinking of has so many entries that you could never write it down.",
            "Right not on the biggest computer on the planet ever.",
            "And even if you could write it down, there would be no way for you to estimate that distribution because you only saw, like you know, Paul Viola's face detectors only trained on a few 1000 images, so you have a few thousand observations in this billion billion dimensional space.",
            "How are you ever going to do that?",
            "So just raw representation of joint probabilities is never going to work.",
            "OK, so you need to make some assumptions about the distribution.",
            "So here's the simplest assumption you could make.",
            "The distribution is completely factorized, so the probability of all of the joint probability of all of the observations X is just factorized into the product of some marginal distributions P of XI.",
            "Now this is completely ludicrous.",
            "It says the probability at every pixel in the image is completely independent of all the other probabilities.",
            "Or to put it another way, if I gave you 100 by 100 subpart of the image, and I asked you to predict the central pixel that would be.",
            "No easier than predicting a random pixel having seen none of the image, that's clearly false, but it's a very strong assumption.",
            "OK, so the independence assumption is too restrictive, and what we want to do is we want to have some intermediate family of assumptions between this assumption, which is like the most aggressive assumption you can make, but is clearly false.",
            "And this assumption, which is the most general form you could have and is clearly intractable.",
            "So I'm going to try and tell you about it for the rest of today.",
            "Is a family of assumptions you can make which smoothly interpolate in some sense between this which is crazy and this which is also crazy.",
            "OK so this is crazy because it's too general and if you don't make any assumptions you can't learn anything.",
            "We're never going to have enough data or computational power to deal with, making no assumptions, and this is too crazy because it's such a ridiculous arranges aggressive assumption that it's like shooting ourselves in the foot before we even start.",
            "No matter how carefully we proceed with this assumption.",
            "Basically, over once we start, so we want something in between.",
            "So what we're going to do instead of making complete factorization marginal independence assumptions, we're going to make what's called."
        ],
        [
            "Conditional independence assumptions.",
            "So just a piece of notation I'm going to write.",
            "X is conditionally independent of BXA is conditionally independent of XP given XY this way.",
            "Nets.",
            "A very old notation goes back to Whitaker and Pearl used it in his books.",
            "It's pretty traditional statistical notation.",
            "The definition says that two sets of variables XA and XB, are conditionally independent given a third set EXE.",
            "If the joint distribution between A&B given C Factorizes.",
            "OK, so this says informally.",
            "Once you know C. Eh, doesn't really tell you anything more about B or B.",
            "Doesn't really tell you anything more about a then you already knew from C. It doesn't say A&B or unrelated, it just says once I tell you see, there's no more information that they can tell you between themselves beyond what she already gave you about both.",
            "Another thing it's quivalent to saying is that the conditional distribution of a given both B&C is exactly the same.",
            "Your belief about a.",
            "If I show you both B&C is exactly the same as your belief about a.",
            "If I just show you see.",
            "So only a subset, a very small subset of all distributions in the world respect some conditional independence assumptions like this that we might make when you make the conditional independence assumption, you're basically throwing away a huge set of joint distributions which no longer are appropriate because they don't satisfy that assumption, and that's good.",
            "Remember our goal in life is to take all of the probability distributions in the world and chop some of them away to get a restricted set so we can work with that as I offices.",
            "So the subset of distributions that respect.",
            "All of the conditional assumptions we make is going to be the family of distributions consistent with our assumptions or hypothesis class.",
            "That's what our about.",
            "This class is going to be an probabilistic.",
            "Graphical models are powerful and simple way to specify."
        ],
        [
            "This family.",
            "So what is a probabilistic graphical model probabilistic graphical model is a way of representing large joint distributions compactly using a set of local relationships that are specified by a graph.",
            "So probably a graphical model is nothing more than a macro language.",
            "Which is a fast way for me to tell you, know Alex Smola or John or any one of you.",
            "This is the particular family of joint distributions.",
            "I wish to consider.",
            "So there's a way of writing a picture.",
            "If I show you this picture, you instantly know what family of joint distributions are considering.",
            "OK, so it's convenient macro language for talking to each other about conditional independence assumptions, or equivalently about subsets of distributions that we're going to consider about conditional independence.",
            "Assumptions were going to make.",
            "And the way you decode this macro language is that each random variable in R model is a node in the graph.",
            "So this node X1 corresponds to the random variable X1, which might be pixel one they're directed or undirected.",
            "Although in this section will only talk about directed their directed edges between the nodes, which tell us something about the factorization of the joint probability.",
            "And although you can't see it on this picture, if you open this up here inside there is a quantitative function at the node which tells us.",
            "The details of the pieces into which the distribution factor is OK, so graphical models are, like you know.",
            "This famous criminal that goes under many names, right?",
            "So they are also known as Bayesian networks, Bayesian belief networks, belief networks.",
            "You know any sort of permutation of this.",
            "So when you hear people talk about belief Nets or Bayes Nets or Bayesian belief networks, those are all the same thing as probabilistic graphical models.",
            "They're just this macro language for this."
        ],
        [
            "Testing condition with dependence assumptions.",
            "So we're going to focus on directed graphical models, and here is the basic factorization.",
            "Oh, let me ask.",
            "Actually, what should we do about time?",
            "Should I just go half an hour later than we were supposed to go, or should I?",
            "I hope everyone had some breakfast this morning.",
            "OK, so we're going to consider these directed graphical models and I'm going to focus on the subset of graphs which are directed acyclic graphs, so there's no directed cycles in the graph.",
            "Each node in such a graph has a possibly empty, but has a set of parents, which I'll call \u03c0, and each node stores inside it a function, and that function is the conditional distribution probability of XI given its parents.",
            "So really, the factorization of the joint distribution here is in terms of local."
        ],
        [
            "Additional probabilities, so let's look at this example and that will help you understand when I write this picture.",
            "What I'm really telling you is.",
            "I think the joint distribution between these six random variables.",
            "Is factorized in this way.",
            "This joint distribution probability of X1X2X3 etc is the probability of X1 given nothing because he has no parents times the probability of X2 given all of his parents?",
            "That's just X one and so on probability of X3 given all of his parents.",
            "This is the picture that I draw.",
            "This is the assumption that I'm making and this is the structure of the computer program.",
            "The computer program has at each node these undefined constants which control the conditional distributions and our job is to fill in these probability distributions.",
            "So all that's left once I write down this model is the values that live in this table.",
            "Everything else has already been specified.",
            "We specified the structure of the hypothesis class just by reading this graph, which hypothesis we select depends on what values I put in these conditional probability tables, which is equivalent to specifying these assumptions.",
            "So the graph tells you the factorization.",
            "The parameters that live in the nodes of the graph tell you the details which are the specific member of your office class.",
            "So this picture does not specify a model.",
            "A specific model.",
            "It specifies a hypothesis Class A set of models only when you add the particular choice of parameters.",
            "When you instantiate it.",
            "That way, you get to the specific probabilistic model that you're talking about, so this is the hypothesis class an within it we can select anything we want by setting these."
        ],
        [
            "These local distributions.",
            "So the key point about directed graphical models is that missing edges imply conditional independence.",
            "So remember that by the chain rule, we can always write the full joint distribution as a product of conditions, so I could always take any joint distribution without making any assumptions about it and start writing it out.",
            "This way it's the probability of X, one times the probability of X2 given X 1 * X Three given everything that came before X4.",
            "Given everything that came before, and so on.",
            "And what does a DAG do a dad goes in and he crosses off some of these things?",
            "He said no, no, no X.",
            "Let's go back to our picture."
        ],
        [
            "X4 here doesn't have anyone except X2 as his parents."
        ],
        [
            "So our assumption would go in and say oh let's cross off X1 and X3 and X6 those are no longer parents of export.",
            "So could missing edges.",
            "Correspond to conditional independence assumptions.",
            "Remember, I told you that the conditional independence assumption is saying that the probability of something given many things is the same as the probability of that given a subset.",
            "That's what conditional independence says, and that's exactly what we do when we remove edges, we cross off these conditioning elements.",
            "So the DAG is telling us that each variable is conditionally independent of its non descendants given its parents.",
            "Once you know your parents then your conditional independence your conditional distribution is completely specified.",
            "And removing an edge into node I eliminates an argument from the right hand side."
        ],
        [
            "So there's a very important effect in directed graphical models which I want to draw your attention to before we get started.",
            "This effect is called explaining away and it addresses a confusion that a lot of people have when they start thinking for the first time about graphical models.",
            "If you look at this graph here.",
            "You might think to yourself well.",
            "X&Z are kind of unrelated.",
            "Because there's no edge between them.",
            "And they don't have any common parents, so there's sort of no coupling between X&Z, but because they share a common child when we condition on this child, when you observe the value of YX&Z become coupled.",
            "OK, so X&Z are marginally independent.",
            "That's true if you just generate samples from the family of distributions specified this by this graph and look at the marginal correlations, you'll find the Texans better uncorrelated.",
            "In fact, they're completely independent, but conditioned on why?",
            "These things are highly dependent.",
            "For example, imagine that there are two coins that we flip independently, so X is the flip of coin one and then is the flip a coin.",
            "Two we flipped on completely independently.",
            "We just flip one and we flip the other.",
            "Have nothing to do with each other.",
            "And why is 1 if the coins are the same in zero, the coins are different.",
            "OK, so now let's assume I don't tell you why.",
            "I just tell you X came up heads.",
            "What do you think about why?",
            "We can say, well, I don't know.",
            "I mean, why is either heads or tails and they're unrelated.",
            "So I guess I think that why is either heads or tails.",
            "I don't know anything about one.",
            "Now I tell you OK, observe X came up heads and why is 1 now?",
            "What do you think about that?",
            "Well, now you have very strong belief about them, right?",
            "'cause I told you why is 1 only if the coins of the same and I told you X's head.",
            "So he knows that it's so.",
            "This is a simple example to indicate the Texans Ed can be conditionally dependent if they share a common child and that effect is called explaining away.",
            "This came from the sort of decision analysis literature where the idea was to, you know, your car isn't starting.",
            "That's the variable Y, and it could be either because your battery is dead or because you know you're out of gas.",
            "And being out of gas doesn't cause your battery to die in your battery.",
            "Dying doesn't cause you to be out of gas, so marginally these things are unrelated.",
            "But now if you try and start your car in your car doesn't start.",
            "Now you say Oh well, could be the battery could be the gas you know.",
            "And then you read your gas gauge in your gas gauge is full.",
            "Now suddenly your belief about your battery change, right?",
            "So the observing that your gas tank was full cause this explaining effect explained away.",
            "The other cause, so that's the where that name comes from.",
            "So if you really already come up with a plausible assumption that it's your battery or lose your gas, and the other assumption the other cause gets gets explained away, it gets suppressed.",
            "Your belief about it because they share a common."
        ],
        [
            "OK, so in order to actually start writing a computer program using this style of probabilistic graphical models, you need to know something about what's inside the nodes.",
            "So for directed models you need some kind of prior functions P of XI or conditional functions for nodes that have parents P of XI given their parents.",
            "And remember, we have these various types of variables that we're considering.",
            "Binary or discrete variables, sometimes called categorical variables, continuous variables.",
            "We could have other things, integer counts, interval valued variables, whatever, so.",
            "On the slides I put some very basic probability models.",
            "I'm not going to sort of go over them, I just put them on the slides.",
            "For those of you who are bit rusty on your probability, which are just parameterized foul families of distributions, and those are the models that live inside the nodes of these directed models.",
            "So when I drew."
        ],
        [
            "Here on this picture I drew these these blocks here.",
            "These blocks are supposed to represent these conditional probability distributions, and I drew them here as though the variables were discrete and we were just going to use a table based parameterisation.",
            "But there's many, many things.",
            "If these variables are continuous then you have different families of analytic functions here, and if these variables are discrete, you can still have more structure than in this table.",
            "So in general you should think of this is just a schematic which tells you you need to put some conditional distribution.",
            "In there, but which one is up to you?"
        ],
        [
            "So as I said, for discrete or categorical variables, the most basic parameterisation is just the table.",
            "It just says what is the probability that X takes on its case value, or if it's a conditional table you just say for each possible value value of the parents we have a table which says what is the probability that X takes on a certain value given that setting of the parents.",
            "That's why it's called a CPT conditional probability table.",
            "For each value of the parents, we have a conditional probability table.",
            "And of course these numbers in the table are non negative and they sum to one.",
            "So if you have K possible settings of the value there K -- 1 degrees of."
        ],
        [
            "And these are the distributions that I told you I listed on this slide just to kind of remind you all.",
            "Generally, for analytic convenience, people in the past have considered for continuous numerical random variables have considered distributions in the exponential family, so these include all of these distributions whose names are hopefully somewhat familiar with the Bernoulli, binomial, Gaussian gamma for positive variables, and so on."
        ],
        [
            "And.",
            "And for continuous variables, when nodes have parents, the situation is a little different.",
            "OK, so there's four cases, and three of them are easy.",
            "If I had a nickel for every time I said that.",
            "OK, so the four cases are.",
            "Discrete parent discrete child.",
            "That's pretty easy.",
            "You can use a conditional probability table.",
            "OK, continuous parent continuous child.",
            "In that case you have to use some kind of functional dependence between the parameters of the distribution.",
            "The continuous distribution on the child and the parameters of the continuous distribution on the parents.",
            "So that's just a completely analytic dependence.",
            "Discrete parent, continuous child.",
            "There you can again do conditional probability distributions, not conditional probability tables.",
            "But for each setting of the parent fits a discrete, you just have discrete number of settings of the parents.",
            "You specify a different set of parameters for the distribution of the child.",
            "Now the hard case which I want to talk about at all is continuous parent discreet child.",
            "In there it's much harder to come up with elegant general families of conditional distributions.",
            "The one form that we really know about there that we can use well is log linear distribution, which I'll tell you.",
            "So when the parent is numeric, very common instance here is to use what's called the linear Gaussian form.",
            "So you say that the probability distribution over the child or the children, why, given their parents X is Gaussian with a mean which is a linear function of the parents."
        ],
        [
            "So that's that's sort of the most most common form here.",
            "OK, so I just I just want to finish up and then send you guys off to the break and then when we come back all I'll tell you will get into the nitty gritty of directed probabilistic model.",
            "So it's just a review here.",
            "What's the goal of probabilistic graphical models?",
            "So first I tried to tell you about the general set up in machine learning about the idea of training and testing and generalization and about the assumptions that you have to make and now.",
            "We're going to focus on this one set of techniques, which is the probabilistic approach.",
            "Mcglone graphical models the aim to provide a compact factorization of large joint probability distributions.",
            "When you're dealing with high dimensional data.",
            "When you want to represent things probability probabilistically, you have no choice but to represent a large joint distribution.",
            "And if you just write it down naively, you'll never be able to estimate it, so they provide compact factorizations.",
            "These factorizations are achieved using local functions.",
            "Which exploit conditional independence ease?",
            "And that's what this picture tells us.",
            "What family of independence assumptions are getting so the graph is a basic set of conditional independencies that we require are true of our joint distribution, and actually from those.",
            "We can derive more conditional independence ease that also must be true.",
            "All of these in dependencies together are crucial in developing algorithms that are going to be efficient ways of doing inference in these models were going to exploit these computationally, but more importantly, we exploit them in terms of sample complexity.",
            "Since we made this factorization assumption, we vastly restricted the universe of distributions.",
            "Now we have some hope of estimating them based on the limited amount of data, so the graph tells us about the hypothesis class, the factorization, and the local functions tell us the quantitative details exactly.",
            "What are the?",
            "Settings of the parameters.",
            "What member of that bosses classroom or now?",
            "Here's a sort of tricky point.",
            "Certain numerical settings of this distribution may have more in dependencies.",
            "If I choose a distribution at one of these nodes that happens itself to have some extra factorization, and that's fine, but those didn't really come from hypothesis class, so in the same way that if my hypothesis class you know includes all models, some subset of them may satisfy extra assumptions, that's fine, but just because I pick.",
            "Particularly medical setting which is factorized doesn't mean it was part of the graphical model."
        ],
        [
            "Original.",
            "And and what's the motivation for doing all this?",
            "The motivation is knowledge acquisition, so we want to build intelligent systems and the bottleneck is really how do we get knowledge from the world into the computer program, right?",
            "And it turns out that the method of getting it from the world into the computer program where we go through your brain is not a very good method.",
            "You say I know about the world, 'cause I'm human, I'm smart, and I can tell the computer.",
            "But I know by typing that's actually not true.",
            "Doesn't work so.",
            "Human experts are rare.",
            "They're expensive, they're unreliable, and their slow.",
            "Unfortunately, they're all these things and more so, but we have lots of machine readable data, right?",
            "I mean, you want to know if a piece of email is spam.",
            "You know what's the number John?",
            "How many Yahoo Mail users are there?",
            "Millions right?",
            "And they are all clicking on that button that says I don't like this.",
            "It's Pam right?",
            "So we have tons of infinite amount of labeled data for that problem and we want to build systems automatically based on the data.",
            "So a small amount of prior knowledge which I trust you to put into your system and a huge amount of statistical knowledge which I'm hoping to suck out of the data and then we can build a system which looks at an image and gives it a label or looks at an email and tell us whether it's been so for now our systems that we build.",
            "Are going to be probabilistic graphical models.",
            "We assume that the prior information that I let you put in specifies the type and structure of the graphical model, the joint distribution, and the mathematical form of those parent conditional distributions and the learning part where we suck the information out of the data is setting the parameters of those distributions.",
            "OK, there's a whole interesting area of structure learning where we also try and learn the structure of this graphical model."
        ],
        [
            "OK, and then the last thing just to keep your mind warm for after the break in for John Langford socks this afternoon is the basic statistical problems.",
            "We're going to talk about.",
            "So regression tries to estimate the value of a continuous output given some inputs.",
            "Classification tries to estimate a Class A discrete class label.",
            "Given some inputs.",
            "Clustering is sort of like classification, where the class labels are UN observed.",
            "You're trying to.",
            "Label"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "I'm not used to being half an hour behind before even start, but I'll try and try and get a slightly back on schedule so it's my job to give you sort of introduction to the whole idea of machine learning and then to try and introduce one of the many different important theoretical foundations of machine learning, which is the probabilistic approach.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I think is this microphone just for the camera?",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to use two microphones.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Thanks so I want to emphasize before I start that the probabilistic approach is one of the core ideas in machine learning, but it's not the only approach and you'll hear a lot more during the rest of the lectures about other approaches, but whether or not you use the probabilistic approach in any particular problem, it's important to know about it, because it's very influential in how ideas develop and how you think, and it is often an important technique that you should be using in your problem, but it's not the only one, so I want to start off by showing you a.",
                    "label": 0
                },
                {
                    "sent": "Video if I can find it here.",
                    "label": 0
                },
                {
                    "sent": "And then hopefully this video will get you thinking about.",
                    "label": 0
                },
                {
                    "sent": "About the whole idea of machine learning here, so I'll let this run for a few minutes.",
                    "label": 0
                },
                {
                    "sent": "This is a video that was originally produced by Paul Viola who's at Microsoft Research in in Seattle, and it's sort of easy if you look at this video for a few minutes to figure out what's happening in this video, right?",
                    "label": 0
                },
                {
                    "sent": "What's happening is the computer is putting a blue box around every face that it sees in every frame, and this video is impressive for several reasons.",
                    "label": 0
                },
                {
                    "sent": "One is that you can actually run this system in real time on a laptop.",
                    "label": 0
                },
                {
                    "sent": "So it's not like they took the video and then they computed on it for six days and they made this video.",
                    "label": 0
                },
                {
                    "sent": "You can actually do this in real time and the other thing that's impressive, although you could argue as a weakness of the system, is that it's doing the analysis each frame at a time.",
                    "label": 0
                },
                {
                    "sent": "It's not taking advantage of the fact that the frames are sequential, so if you just gave it a single photo, the performance would actually be similar.",
                    "label": 0
                },
                {
                    "sent": "So the question that I want to try and get you to think about is if you had to write a computer program like this.",
                    "label": 0
                },
                {
                    "sent": "Let's assume you don't know bout Viola Jones, face detectors and all that stuff.",
                    "label": 0
                },
                {
                    "sent": "How would you go about doing it?",
                    "label": 0
                },
                {
                    "sent": "OK, that's really the kind of core problem of machine learning is you have a task which is relatively well defined, like detecting faces and images, and you need to write a program, but it's not really at all clear how you should start writing that program.",
                    "label": 0
                },
                {
                    "sent": "So the approach that I'm going to discourage you from taking is the approach of trying to just say, well, I'm smarter than the world.",
                    "label": 0
                },
                {
                    "sent": "I know what it faces.",
                    "label": 0
                },
                {
                    "sent": "A face has two eyes and a nose and a mouth.",
                    "label": 0
                },
                {
                    "sent": "And I'm just going to write two I detectors and a nose detector and a mouse detector, and I'm going to run them on the image and then I'm going to, you know, specify in my code exactly what it means to be an eye of something round with a dive in the middle and and that's going to work, and what you should convince yourself before you do anything in machine learning is that is never going to work.",
                    "label": 0
                },
                {
                    "sent": "The world is too complicated for that.",
                    "label": 0
                },
                {
                    "sent": "OK so?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The essential approach to building intelligent systems, which is codified in machine learning, is the idea of using examples from the real world to train a system which will then do the task that you want.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you want to train a system to do face identification to go from a picture like this to a name, then again hand programming is not really possible here.",
                    "label": 0
                },
                {
                    "sent": "So what's the solution?",
                    "label": 0
                },
                {
                    "sent": "The solution is to get the computer to program itself at some level.",
                    "label": 1
                },
                {
                    "sent": "By showing examples of the kind of behavior we want so in the face problem we would show the computer a lot of images and say these images have faces in them and then we would show the computer a lot of images and say these images don't have faces in them and then we would try and automatically from that data set learn what it is about a face.",
                    "label": 1
                },
                {
                    "sent": "OK so this is the learning approach to AI and I think that it's very you can make a very strong argument that this approach essentially dominates intelligent systems in the modern day.",
                    "label": 0
                },
                {
                    "sent": "There were older approaches based on rules that were coded by hand and expert systems and stuff, but those have almost universally been eclipsed by by learning machines in a huge range of applications.",
                    "label": 0
                },
                {
                    "sent": "So the way you should think of this, it's not really that the computer is programming itself completely, really what you're doing is you're writing a program that has thousands or millions or 10s of millions of undefined constants in it.",
                    "label": 0
                },
                {
                    "sent": "So imagine you know those of you who are C programmers.",
                    "label": 0
                },
                {
                    "sent": "Imagine you literally wrote a program, but at the top of the seat program there were number defined constant 19 constant, two never like 100,000, or a million of those.",
                    "label": 0
                },
                {
                    "sent": "OK, and now you want a way of setting those constants.",
                    "label": 0
                },
                {
                    "sent": "Those constants control the behavior of your program for any setting of those constants is a different program that runs.",
                    "label": 0
                },
                {
                    "sent": "How do you set those constants?",
                    "label": 0
                },
                {
                    "sent": "That's really what the game of machine learning is about.",
                    "label": 0
                },
                {
                    "sent": "Tweaking these real numbers inside a computer program that you already wrote to make the computer program behave the way you want.",
                    "label": 0
                },
                {
                    "sent": "So geometrically, you can think of this as any particular program that you write with undefined constants defines a space of possible executables, right?",
                    "label": 0
                },
                {
                    "sent": "If there's a million parameters, it's a million dimensional space, and the goal of machine learning is to put you in the right spot in that million dimensional face.",
                    "label": 0
                },
                {
                    "sent": "To solve your problem, face detection or spam email or whatever it is.",
                    "label": 0
                },
                {
                    "sent": "So our goal in life is to estimate internal parameters of a computer program whose structure we specified by using an existing data set.",
                    "label": 0
                },
                {
                    "sent": "And that data set somehow cap.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sure is the correct behavior that we want, so you can think of statistical machine learning models as sort of probabilistic databases.",
                    "label": 0
                },
                {
                    "sent": "So traditional database technology can't really answer any query about items that were never loaded into the data set, and the idea of a probabilistic model or statistical models that will talk about for the next few lectures is that they can build a probabilistic answer to any query about the about the database.",
                    "label": 1
                },
                {
                    "sent": "Oh thanks.",
                    "label": 0
                },
                {
                    "sent": "OK, so the these models also have a lot of other advantages.",
                    "label": 0
                },
                {
                    "sent": "They allow you to make decisions given partial information by taking into account the uncertainty or unobserved ability of what you don't know they count for noisy sensors or actuators because a lot of measurements that you take in the real world are not accurate for physical reasons.",
                    "label": 1
                },
                {
                    "sent": "They can explain things that aren't part of your model explicitly, because they allow for uncertain or stochastic behavior and.",
                    "label": 1
                },
                {
                    "sent": "So the probabilistic approach has a lot of other advantages, and it's really.",
                    "label": 0
                },
                {
                    "sent": "Allows you to do automatic system building in a way that expert systems don't.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't allow you to.",
                    "label": 0
                },
                {
                    "sent": "So just to complete the sort of three minute sales pitch at the beginning here, this is just a sort of laundry list of applications where machine learning, and in particular probabilistic approaches have been have been used, so speech recognition.",
                    "label": 0
                },
                {
                    "sent": "OK, this doesn't seem to be over, here goes.",
                    "label": 0
                },
                {
                    "sent": "So speech recognition and speaker verification.",
                    "label": 0
                },
                {
                    "sent": "If you've ever interacted with the telephone system where you read your credit card number or you ask it to navigate through voice menu, those are using probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "In fact, hidden Markov models of the kind you'll learn about at the end of these lectures.",
                    "label": 0
                },
                {
                    "sent": "Same thing for text, handwritten text or OCR face location tracking and video search and recommendation.",
                    "label": 0
                },
                {
                    "sent": "Financial prediction.",
                    "label": 0
                },
                {
                    "sent": "An fraud.",
                    "label": 0
                },
                {
                    "sent": "All of these things here.",
                    "label": 0
                },
                {
                    "sent": "Medical diagnosis AI in games is now becoming a big deal, so modern video games have adaptive opponents.",
                    "label": 0
                },
                {
                    "sent": "Computer opponents that use machine learning, scientific analysis, and then all kinds of other things.",
                    "label": 0
                },
                {
                    "sent": "So I just want to convince you that this is not just you know the problem of detecting faces are saying whether your email is spam or not, that these systems are deployed much more widely than than you know, almost all big companies that have huge datasets are doing some kind of learning on that data.",
                    "label": 0
                },
                {
                    "sent": "If only just to detect outliers or problems or make decisions about which things they should pursue and which things.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I should drop.",
                    "label": 0
                },
                {
                    "sent": "OK, so in machine learning in general there's some Canonical problems that are sort of overall labels or groups for general tasks that we might want to might want to attack, and actually this afternoon and tomorrow morning John Langford is going to talk about the equivalence between different problems in this set, but for the purposes of getting started, you can think of these as different kinds of problems that you might want to solve, each of which has.",
                    "label": 0
                },
                {
                    "sent": "Its own sort of setting its own inputs and outputs and desired goals.",
                    "label": 0
                },
                {
                    "sent": "So the most common approach or set of problems is supervised learning.",
                    "label": 0
                },
                {
                    "sent": "In supervised learning, you're given some examples of the inputs that correspond to what you are going to see in the real world.",
                    "label": 0
                },
                {
                    "sent": "The desired outputs and your goal is to predict the outputs on future inputs.",
                    "label": 1
                },
                {
                    "sent": "So, for example, is this microphone working properly?",
                    "label": 0
                },
                {
                    "sent": "Is sort of cutting in and out, or if you're in the back row, put up your hand.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "That was a test of both the microphone and the people in the back row.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the supervised learning problem you have some inputs and you have a desired output.",
                    "label": 0
                },
                {
                    "sent": "So the input might be some description of your email and the desired output might be a label spam or not spinning, or the input might be a frame from a video camera and the desired output might be the location of any faces or the desired input might be some history about a currency and the desired output might be sell or buy.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are all supervised learning problems.",
                    "label": 0
                },
                {
                    "sent": "And the names that you're probably familiar with are things like classification, regression, Time series prediction, usaral, supervised learning problems.",
                    "label": 1
                },
                {
                    "sent": "Unsupervised learning is in some ways more exciting, and in some ways less well defined.",
                    "label": 0
                },
                {
                    "sent": "So unsupervised learning is kind of like the radical cousin of supervised learning, right?",
                    "label": 1
                },
                {
                    "sent": "Everyone thinks it's kind of cool and interesting, but nobody's really sure exactly what it's all about.",
                    "label": 0
                },
                {
                    "sent": "So unsupervised learning you're given only the inputs.",
                    "label": 1
                },
                {
                    "sent": "And the goal is to automatically discover some kind of structure or representation or features.",
                    "label": 0
                },
                {
                    "sent": "So clustering, outlier detection, compression, or coding.",
                    "label": 0
                },
                {
                    "sent": "These are unsupervised learning problems and the problem is, for example for clustering.",
                    "label": 0
                },
                {
                    "sent": "It's not really clear how you measure a clustering.",
                    "label": 0
                },
                {
                    "sent": "If I tell you I got all this input and I decided to cluster these hundred guys together and these other hundred guys together in these under hundreds together, and then I say, well, do you think I did a good job?",
                    "label": 0
                },
                {
                    "sent": "We have to have some quantitative way of measuring that, which I'll show you a bit later.",
                    "label": 0
                },
                {
                    "sent": "There's a couple of other problems.",
                    "label": 0
                },
                {
                    "sent": "One is rule learning, which is really a sort of subset of unsupervised learning for finding very, very common joint settings of the measurements in your data set and reinforcement learning, in which in addition to inputs and outputs, you add the idea of actions and state of the world.",
                    "label": 1
                },
                {
                    "sent": "So reinforcement learning is really a scenario where not only can you observe things in the world, but your agent.",
                    "label": 0
                },
                {
                    "sent": "Your computer program can take specific actions that affect the world.",
                    "label": 0
                },
                {
                    "sent": "It's not just that you.",
                    "label": 0
                },
                {
                    "sent": "Receive some input and you do some processing and you reporting output.",
                    "label": 0
                },
                {
                    "sent": "You can actually take an action which will affect the world and in particular affect the future distribution of inputs that you see.",
                    "label": 0
                },
                {
                    "sent": "These last two.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about just because we don't have very much time, but there's a lot of people here who know much more about reinforcement learning when I do so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's something from them.",
                    "label": 0
                },
                {
                    "sent": "OK, so I just want to try and press forward here and keep focusing on this image example and actually trying to say how would we actually start solving this problem.",
                    "label": 0
                },
                {
                    "sent": "So one key issue here is how do we represent information about the world?",
                    "label": 1
                },
                {
                    "sent": "Your computer can't actually see anything, so when you plug your USB camera into your computer, what actually comes out of the USB camera is just some stream of pixels which pixel values which represent the image, right?",
                    "label": 0
                },
                {
                    "sent": "So we need some way of encoding this image into that computer program that I was talking about, whose structure is defined, but whose constants are undefined.",
                    "label": 0
                },
                {
                    "sent": "In order for us to start doing learning an.",
                    "label": 0
                },
                {
                    "sent": "This unfortunately is probably the most important part of a machine learning problem, and the part which we formally know the least about in terms of how I can recommend to you that you do this so it's crucial how you numerically represent the input to your problem in the program.",
                    "label": 0
                },
                {
                    "sent": "It's also hard to decide a prayer and what the best representation is, but.",
                    "label": 0
                },
                {
                    "sent": "For example, example here.",
                    "label": 0
                },
                {
                    "sent": "Imagine that you just listed the pixel values we just listed this pixel and then this pixel in this pixel, and so on.",
                    "label": 0
                },
                {
                    "sent": "Every row and that would turn this image into a huge vector of numbers, and that would be our representation.",
                    "label": 1
                },
                {
                    "sent": "So no matter what we have to pick a way of numerically representing the input.",
                    "label": 0
                },
                {
                    "sent": "If it's spam, we might decide on a fixed set of words, and we might represent it as a binary vector.",
                    "label": 0
                },
                {
                    "sent": "Whether or not that word occurs in the email 01011007.",
                    "label": 0
                },
                {
                    "sent": "OK, that would be one representation of the email.",
                    "label": 0
                },
                {
                    "sent": "It's not perfect.",
                    "label": 0
                },
                {
                    "sent": "Remember that this image representation loses a lot of information.",
                    "label": 0
                },
                {
                    "sent": "It loses the information about which pixels are beside each other.",
                    "label": 0
                },
                {
                    "sent": "The spam representation loses the information about the order of the words.",
                    "label": 0
                },
                {
                    "sent": "If you just do binary word occurrence.",
                    "label": 0
                },
                {
                    "sent": "So there's some problems with every representation, but you need to pick one.",
                    "label": 0
                },
                {
                    "sent": "Once we've done this, we're going to think of these numerical values as random variables.",
                    "label": 1
                },
                {
                    "sent": "So for those of you who are bit rusty on your probability and statistics, a random variable you can think of is just like a variable in a computer program.",
                    "label": 0
                },
                {
                    "sent": "It represents a certain quantity, like the intensity at this pixel here, but its value changes depending on which data our program is looking at, or, depending on what uncertainty we have about that value.",
                    "label": 1
                },
                {
                    "sent": "So what we do instead is we use probabilities to represent distributions over these values.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the first important concept here.",
                    "label": 0
                },
                {
                    "sent": "In the probabilistic approach.",
                    "label": 0
                },
                {
                    "sent": "So the probabilistic approach.",
                    "label": 0
                },
                {
                    "sent": "This is now.",
                    "label": 0
                },
                {
                    "sent": "This is almost like a religious statement that I'm going to make now, right?",
                    "label": 0
                },
                {
                    "sent": "The idea of the probabilistic approach is that you should use random variables to represent the world.",
                    "label": 1
                },
                {
                    "sent": "Not everyone agrees with this statement.",
                    "label": 0
                },
                {
                    "sent": "In fact, I don't really even fully agree with this statement all the time.",
                    "label": 0
                },
                {
                    "sent": "But in explaining the probabilistic approach, what you do is you start off with this philosophy.",
                    "label": 1
                },
                {
                    "sent": "We're going to use random variables to represent the world so.",
                    "label": 0
                },
                {
                    "sent": "Each pixel or each word flag is considered a random variable, and random variables can be different types.",
                    "label": 0
                },
                {
                    "sent": "They can be discrete, so for example 01 or email.",
                    "label": 0
                },
                {
                    "sent": "Regular email versus spam or the weather.",
                    "label": 0
                },
                {
                    "sent": "Sunny, overcast, raining.",
                    "label": 0
                },
                {
                    "sent": "Maybe we should add a value here tornado.",
                    "label": 0
                },
                {
                    "sent": "Or they can be continuous quantities, like a temperature or an income or a blood pressure or an exchange rate.",
                    "label": 1
                },
                {
                    "sent": "Or the Las Vegas odds on some event.",
                    "label": 1
                },
                {
                    "sent": "And these random variables represent both the inputs of our problem, the outputs of our problem, and any internal states that we want.",
                    "label": 0
                },
                {
                    "sent": "And generally we have root same quantities.",
                    "label": 0
                },
                {
                    "sent": "For example, we might have many pixels.",
                    "label": 1
                },
                {
                    "sent": "Now, instead of we're going to quickly run out of both Roman and Greek letters.",
                    "label": 1
                },
                {
                    "sent": "If we given you letter to each random variable, right?",
                    "label": 1
                },
                {
                    "sent": "If I say I'm going to represent the first pixel by X and then the second pixel by Y, and then the third pixel lights it OK if my image has a mega pixel.",
                    "label": 1
                },
                {
                    "sent": "I'm going to run out of letters, so we just use subscripts to index these things so X sub I is typically the value of the input variable be high pixel, and if we have lots of different images, lots of frames.",
                    "label": 0
                },
                {
                    "sent": "Then it's on the end frame and the same thing.",
                    "label": 0
                },
                {
                    "sent": "Why I'm going to typically use Y for the output, so X is the input.",
                    "label": 0
                },
                {
                    "sent": "Why is the output and then when we want to write these things as a vector, we just make it a bold.",
                    "label": 0
                },
                {
                    "sent": "That's just a sort of notational convention.",
                    "label": 0
                },
                {
                    "sent": "If I use a capital letter here to represent a matrix, that's all of the data stack together.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's just notation.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have the first piece of the puzzle.",
                    "label": 0
                },
                {
                    "sent": "We know that we're going to take what we know about the world we're going to represent it as a numerical vector.",
                    "label": 0
                },
                {
                    "sent": "We're going to treat that numerical vector as a set of random variables.",
                    "label": 1
                },
                {
                    "sent": "OK, in a probabilistic model, and now we're ready to sort of, say how we're going to set up the problem of learning.",
                    "label": 1
                },
                {
                    "sent": "What is the structure of our computer program, whose constants that we don't know if we're going to fill it.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that we have some inputs and their expressed in whatever representation, and we want to calculate sum.",
                    "label": 0
                },
                {
                    "sent": "But based on those inputs, for example, maybe it's a binary decision.",
                    "label": 0
                },
                {
                    "sent": "Is this a picture of Sam or not?",
                    "label": 0
                },
                {
                    "sent": "And our computer program is going to just be a mathematical function.",
                    "label": 1
                },
                {
                    "sent": "It's going to be some function F applied to the inputs X and then it's going to be the output that our computer program produces.",
                    "label": 0
                },
                {
                    "sent": "And the crucial idea here is that we don't just make up functions out of thin air.",
                    "label": 1
                },
                {
                    "sent": "That would be like going back to the way of doing things, which I criticize, which was just to design everything by hand.",
                    "label": 0
                },
                {
                    "sent": "If you just sat down here, So what function smells good to me too?",
                    "label": 0
                },
                {
                    "sent": "I like this function and then you just wrote that function down and you put some random constants in it that wouldn't really give you enough flexibility to tackle machine learning problems in general, so we don't make up functions out of thin air.",
                    "label": 1
                },
                {
                    "sent": "We select functions from a pre specified set which is known as our hypothesis space.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can think of our hypothesis space as a set of functions, and for every function in that set there is 1 setting of the parameters, Theta which indexes back to that function.",
                    "label": 0
                },
                {
                    "sent": "Or equivalently for any parameter Theta, it gives us a single function.",
                    "label": 0
                },
                {
                    "sent": "So you think of this is just a nob on the box of functions.",
                    "label": 1
                },
                {
                    "sent": "Every setting of data gives you a new function from input outputs, and your job is to find the setting of the NOB, but gives you the function you're most happy with.",
                    "label": 0
                },
                {
                    "sent": "So the hardest part of doing probabilistic learning is deciding on this line here.",
                    "label": 0
                },
                {
                    "sent": "This line looks very innocuous, right?",
                    "label": 0
                },
                {
                    "sent": "It took me like 3 seconds to type this in latex, but this line carries with it a lot of responsibility.",
                    "label": 0
                },
                {
                    "sent": "You have to come up with the representation of the inputs you have to come up with a hypothesis class and you have to come up with some way of expressing the outputs of the program.",
                    "label": 0
                },
                {
                    "sent": "Once you've done this, you could argue that everything else is just mechanical.",
                    "label": 0
                },
                {
                    "sent": "Once you've written this down.",
                    "label": 0
                },
                {
                    "sent": "There are very well specified rules of probabilistic inference that tell you how to proceed.",
                    "label": 0
                },
                {
                    "sent": "You receive the data from the world you in further posterior distribution over the parameters given your model in the data you use that posterior distribution to make predictions about what's going to happen.",
                    "label": 0
                },
                {
                    "sent": "It's basically all mechanical.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Sometimes those mechanical operations are so large in the computer that you can't do them directly, and then you have to be smart about how to do them.",
                    "label": 0
                },
                {
                    "sent": "But in principle, once you write down this line.",
                    "label": 0
                },
                {
                    "sent": "That's the end of your contribution to the probabilistic approach, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so I can't emphasize enough how important this is elections.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I said something just on the last slide that hopefully didn't just wash past you, which was that your job in life is to set the NOB beta to pick the function from the function class, which makes you happiest.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but what does that mean makes you happy right?",
                    "label": 0
                },
                {
                    "sent": "You just like you like the way it looks.",
                    "label": 0
                },
                {
                    "sent": "OK so we have to specify some quantitative way of saying which functions in the function class are good and which functions in function class are bad.",
                    "label": 0
                },
                {
                    "sent": "So this is the last piece of the puzzle.",
                    "label": 0
                },
                {
                    "sent": "There's a representation of the inputs and the outputs.",
                    "label": 1
                },
                {
                    "sent": "There's a hypothesis class, which is the set of functions that we're considering, and now there is the loss function and that loss function tells you for any particular function that you select from your hypothesis class, how happy are you with it?",
                    "label": 0
                },
                {
                    "sent": "How much do you think?",
                    "label": 0
                },
                {
                    "sent": "It's going to really solve the problem that you can you know the problem that you're meant to do now in the probabilistic approach, the hypothesis class and the loss function are sort of the coupled.",
                    "label": 0
                },
                {
                    "sent": "That is to say, once you write down your probabilistic model of the world learning about the world from the data that you have in principle doesn't really have to interact with the loss function.",
                    "label": 0
                },
                {
                    "sent": "You should be able to just infer the posterior of the parameters given what you saw in the world and then.",
                    "label": 0
                },
                {
                    "sent": "From those posterior parameters you can decide what to do, what output to give, what action to take independently by introducing the loss function.",
                    "label": 0
                },
                {
                    "sent": "It's rare that you can make that separation completely clean, so for today's lecture we can think of a loss function as just the third component.",
                    "label": 0
                },
                {
                    "sent": "Did our learning approach, and we're going to use them altogether.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is the inputs that you have in front of you are X.",
                    "label": 0
                },
                {
                    "sent": "The correct answers that you're given RY, so this would be like when we're training the face detector.",
                    "label": 0
                },
                {
                    "sent": "The inputs are all of the images that we're going to use to train in the answers.",
                    "label": 0
                },
                {
                    "sent": "Why are the labels that you know we gave or the undergraduates that we hired to label the frames gave to the frames whether they have a face or not?",
                    "label": 0
                },
                {
                    "sent": "And then are the outputs of our machine, which we hope are going to match the correct answers.",
                    "label": 1
                },
                {
                    "sent": "Why not only on the input data X that we have, but on all future data.",
                    "label": 0
                },
                {
                    "sent": "OK, so once we have this, how do we select the parameters and the idea is you come up with a loss function or a penalty function which says for any particular function in your hypothesis class, how well is it doing OK?",
                    "label": 1
                },
                {
                    "sent": "So the intuition is very simple, safer classification.",
                    "label": 0
                },
                {
                    "sent": "Veloc function tells you how many mistakes are you making on the training data.",
                    "label": 0
                },
                {
                    "sent": "So how many times did the output of your machine?",
                    "label": 0
                },
                {
                    "sent": "This is Ed on the input?",
                    "label": 0
                },
                {
                    "sent": "How many times was it different from the correct output line?",
                    "label": 0
                },
                {
                    "sent": "So this is again just a piece of notation in the square brackets.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use to mean a binary indicator of whether the thing inside is true or not.",
                    "label": 0
                },
                {
                    "sent": "So this is just counts the number of mistakes you made on the training data, and that's just a very simple thing.",
                    "label": 0
                },
                {
                    "sent": "It says I like functions from my hypothesis class which do well on the training data which gives small number of errors on the training data.",
                    "label": 0
                },
                {
                    "sent": "In regression, you might ask for the minimum you might try and make the loss function the mean squared error.",
                    "label": 0
                },
                {
                    "sent": "So this is the squared error between your target outputs Y and your prediction in clustering.",
                    "label": 0
                },
                {
                    "sent": "Remember, I said it's very difficult to come up with a good cost function for clustering.",
                    "label": 1
                },
                {
                    "sent": "Well, here's one that people use a lot in clustering.",
                    "label": 1
                },
                {
                    "sent": "What you want to do is you want to minimize the distance between each example X and the cluster center, or the average of the other examples that were assigned to its cluster.",
                    "label": 0
                },
                {
                    "sent": "So this is like saying things in the same cluster should be close together.",
                    "label": 0
                },
                {
                    "sent": "OK, so all of these are different loss functions and there's thousands more.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's up to you to pick a loss function which trades off two things.",
                    "label": 0
                },
                {
                    "sent": "So one thing that your loss function needs to do is it needs to indicate conditions similar to the conditions that you really care bout in the real world.",
                    "label": 0
                },
                {
                    "sent": "So for example in spam if your loss function says I care more about false positives and false negatives then that should reflect what your users of your email system really care about.",
                    "label": 0
                },
                {
                    "sent": "Or in face detection, your loss function should indicate how many faces in the image were detected.",
                    "label": 0
                },
                {
                    "sent": "Maybe if you say that the image has six faces and they were really seven, that's better than saying that the image has no faces when they were really simple.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's one thing is the loss function should be as close as possible to what you really care about in solving the task.",
                    "label": 0
                },
                {
                    "sent": "But there's another thing which is The Dirty secret of machine learning that people don't tell you that your loss functions job is.",
                    "label": 0
                },
                {
                    "sent": "Your loss function has a second job in life, which is to make your problem tractable.",
                    "label": 0
                },
                {
                    "sent": "OK, and just like in the rest of life where there's the ideal way to do something, and then there's you know the way you have to do it to get things done.",
                    "label": 0
                },
                {
                    "sent": "The same thing is true in machine learning.",
                    "label": 0
                },
                {
                    "sent": "There's the ideal loss function which we would really love to minimize, but there is sometimes a tradeoff that you have to make to back off to a more tractable loss function that's close to the loss function you like, but much easier to deal with computation.",
                    "label": 0
                },
                {
                    "sent": "So sometimes you'll see people write down the loss function, like hinge loss or like log likelihood or conditional likelihood or.",
                    "label": 0
                },
                {
                    "sent": "Posterior probability and you'll say why are they writing down that loss function?",
                    "label": 1
                },
                {
                    "sent": "I mean, do I really care about the conditional log likelihood of the label?",
                    "label": 0
                },
                {
                    "sent": "No, what I really care about is did I get it right or wrong?",
                    "label": 0
                },
                {
                    "sent": "Why didn't they write down the loss function?",
                    "label": 0
                },
                {
                    "sent": "You know, 01 loss and the answer is because the loss function in addition to trying to help your problem workout well, has this second job of making your problem tractor.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there is an important issue here about training versus testing, which it's crucial that you understand for these lectures and everyone who comes after me.",
                    "label": 0
                },
                {
                    "sent": "So training data are the inputs and outputs we were given at the time when we have the task of building the computer program.",
                    "label": 0
                },
                {
                    "sent": "That's the data we have sitting on our disk.",
                    "label": 1
                },
                {
                    "sent": "You know, when our boss tells us I want you to write a spam detector or face detector or currency predictor.",
                    "label": 0
                },
                {
                    "sent": "The testing data are the inputs and outputs that we'll see in the future, and the problem is we don't know the testing data training time.",
                    "label": 0
                },
                {
                    "sent": "If we knew the testing data, training time, everyone's life would be a lot easier.",
                    "label": 0
                },
                {
                    "sent": "OK, so the training error is the average value of the loss function that we get on the training data, and that error we can see for any setting of data.",
                    "label": 0
                },
                {
                    "sent": "We have some training error and we like thetas that give us low training error.",
                    "label": 0
                },
                {
                    "sent": "The testing error is the average value of the loss function on the test data and that we don't know 'cause we don't have the test data.",
                    "label": 1
                },
                {
                    "sent": "And once our real goal in life is our goal in life, to do well on the training data, well, no.",
                    "label": 0
                },
                {
                    "sent": "We we already have the answers for the training data.",
                    "label": 0
                },
                {
                    "sent": "We don't care whether those emails are spam or not.",
                    "label": 0
                },
                {
                    "sent": "Spam, because they were already sent.",
                    "label": 1
                },
                {
                    "sent": "But we care about his future emails, whether their spam or not spam.",
                    "label": 0
                },
                {
                    "sent": "So ideally we'd like to minimize the test error.",
                    "label": 0
                },
                {
                    "sent": "So how can we minimize the test error if we don't have the test data?",
                    "label": 0
                },
                {
                    "sent": "OK, and this is where the probabilistic framework is going to come and rescue us.",
                    "label": 0
                },
                {
                    "sent": "We're going to make a crucial assumption here, which is going to allow us to actually do something which minimizes the test error, even though we never saw the test data.",
                    "label": 0
                },
                {
                    "sent": "That might sound spooky to you, but I'll tell you this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then you'll see how it works.",
                    "label": 0
                },
                {
                    "sent": "So here's the assumption.",
                    "label": 0
                },
                {
                    "sent": "It's the sort of IID sampling assumption.",
                    "label": 0
                },
                {
                    "sent": "Imagine that our data is created randomly from a joint probability distribution over the inputs X and the outputs Y, which we don't know, and then the core assumption here is that both the training data and the testing data come from the same distribution, P. Annan training time were given us finite sample X1Y1X2Y2 up to XNYN from this.",
                    "label": 1
                },
                {
                    "sent": "And the members of this the input output pairs are generated independently and identically from this distribution P. The test data is also going to be generated IID from the same distribution, and the question is looking only at the training data.",
                    "label": 0
                },
                {
                    "sent": "Can we build a machine that has a low loss, low expected loss on the test data?",
                    "label": 1
                },
                {
                    "sent": "So when you sample is drawn for testing from the same distribution and we run the machine and we get some loss and we want to know by looking at the machine we built the training data and the training error, what can we say about the tester?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one of the sort of most amazing results in, you know, the theory of computer science and applied statistics, is the result that yes, you can actually say something about the test error if you just have the training data under the assumption that the training data and testing data come from the same unknown distribution, that's a like a phenomenal watershed result.",
                    "label": 0
                },
                {
                    "sent": "I think in applied statistics that goes all the way back to Vapnik and Turban Ankerson even earlier and.",
                    "label": 0
                },
                {
                    "sent": "I think that it's really, you know, started off people thinking along these along these lines.",
                    "label": 0
                },
                {
                    "sent": "So the crucial concept here is a concept called generalization.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the danger?",
                    "label": 1
                },
                {
                    "sent": "The danger in the above setup is that we're going to do really well on the training data, but very poorly on the testing data, and that phenomenon is doing well on the training data, but poorly on the testing data is called overfitting.",
                    "label": 1
                },
                {
                    "sent": "So for example, there's something called the sure thing hypothesis.",
                    "label": 0
                },
                {
                    "sent": "As far as I know, David Mackay is to be credited with coming up with this.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the sure thing hypothesis.",
                    "label": 0
                },
                {
                    "sent": "You memorize the training data and you produce garbage on the test data.",
                    "label": 0
                },
                {
                    "sent": "OK, that's if I want.",
                    "label": 0
                },
                {
                    "sent": "I can put that function in my hypothesis class, right?",
                    "label": 0
                },
                {
                    "sent": "Now that function is kind of ridiculous, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, it just memorizes the training data, so of course it gets perfect results on the training data and it gives random outputs on the test data source is going terribly on the test data, so why should we care about this function within that scary about this function?",
                    "label": 0
                },
                {
                    "sent": "It has very low loss at Test time at training time.",
                    "label": 0
                },
                {
                    "sent": "Excuse me.",
                    "label": 0
                },
                {
                    "sent": "So if our only guide to picking functions is a big functions that have low training loss, then that function the sure thing hypothesis is going to swoop in and confuse us and cause us to pick a very stupid learning machine.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "The idea here is that you can't learn anything about the world without making assumptions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this has been formalized in many, many ways, including the so called no free lunch theorems by Wolpert.",
                    "label": 1
                },
                {
                    "sent": "Those theorems have their problems, but they they indicate an important point, which is that if you really believe a priority that any function in the world is possible, you can never learn anything.",
                    "label": 0
                },
                {
                    "sent": "So the ability to achieve small loss on test data is called generalization, and that's the real goal is to build a machine that has good generalization.",
                    "label": 0
                },
                {
                    "sent": "We could care less about the training data we already have delayed.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So in order to be able to say something strong to guarantee the ability to generalize to get low loss on test error, we have to control something called the capacity, which is the complexity of the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So remember, I told you learning is really searching hypothesis, space learning is all about trying to find the magic setting of that knob, Theta, which gives you low loss at Test time, and then we sort of said, well, how are we going to evaluate the test loss when we don't have the test data.",
                    "label": 0
                },
                {
                    "sent": "So we're going to make this a sampling assumption.",
                    "label": 0
                },
                {
                    "sent": "The test data comes from the same distribution as the training data.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the inductive learning hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Generalization is possible.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a very strong statement.",
                    "label": 0
                },
                {
                    "sent": "That statement says if you make assumptions about the world, you can sometimes actually prove that you're not going to do too badly at Test time under sampling assumptions.",
                    "label": 0
                },
                {
                    "sent": "So in words, what the inductive learning hypothesis tells you is that if the machine performs well on most of the training data, and it's not too complex, it will probably do well on similar tested.",
                    "label": 0
                },
                {
                    "sent": "So this statement can breakdown in many ways.",
                    "label": 0
                },
                {
                    "sent": "The obvious way it can breakdown is you can fail the first condition.",
                    "label": 0
                },
                {
                    "sent": "You can train your classifier and it can do really badly with the training data.",
                    "label": 0
                },
                {
                    "sent": "OK, well then you really shouldn't be that surprised when it does badly on the test data, right?",
                    "label": 0
                },
                {
                    "sent": "Didn't even do well on the training data and that's the way that most people focus on most people try and correct this problem, but there's two more things you need to correct if you really want to protect yourself, one is you need to make sure that it's not too complex.",
                    "label": 0
                },
                {
                    "sent": "Now the sure thing hypothesis, which memorized the entire training set, was very complicated, right?",
                    "label": 0
                },
                {
                    "sent": "'cause it memorized the whole training set, so the training set was.",
                    "label": 0
                },
                {
                    "sent": "3 gigabytes on disk.",
                    "label": 0
                },
                {
                    "sent": "Then that classifier had a memory footprint of at least three gigabytes, so it's very complicated beast.",
                    "label": 0
                },
                {
                    "sent": "So that's why the sure thing hypothesis is bad, because it's not a simple classifier, and this is the third assumption that you have to satisfy.",
                    "label": 0
                },
                {
                    "sent": "The test data has to be similar.",
                    "label": 0
                },
                {
                    "sent": "You would be amazed how many people go out into the world in the world, and they say I'm going to do machine learning and they collect their training data, and then they trained our classifier and they make sure that their classifier isn't too complicated and they could quit raining here and then they apply it to test data.",
                    "label": 0
                },
                {
                    "sent": "That's completely different than their training data.",
                    "label": 0
                },
                {
                    "sent": "And then it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "And they say, oh, I'm so stunned.",
                    "label": 0
                },
                {
                    "sent": "Didn't the inductive learning hypothesis protect me?",
                    "label": 0
                },
                {
                    "sent": "What's going wrong?",
                    "label": 0
                },
                {
                    "sent": "Well, what's going wrong is you're applying it to totally different testing data, so make sure please when you go and you do machine learning, three things, one you actually managed to train a good classifier.",
                    "label": 0
                },
                {
                    "sent": "Most people can do that too, but your classifier is not too complicated.",
                    "label": 0
                },
                {
                    "sent": "You have to control capacity and three do a sanity check that you're testing.",
                    "label": 0
                },
                {
                    "sent": "Data is at least somewhat similar to your training data before you start complaining that things aren't working.",
                    "label": 0
                },
                {
                    "sent": "So these three things, OK, but the amazing fact if you satisfy all these then.",
                    "label": 0
                },
                {
                    "sent": "You can actually prove that generalization is possible.",
                    "label": 0
                },
                {
                    "sent": "There's a whole beautiful branch of mathematics which, in my opinion sort of represents the best possible intersection of computer science and mathematics called learning theory, and there's you know many, many theorems about VC dimension and PAC Learning and PAC Bayes and stuff in this area.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking, if our hypothesis space is not too complicated, it has a low capacity in some formal sense, and if our training set is large enough and we do well on it, then you can bound the probability of doing significantly worse on the test data than on the training data.",
                    "label": 0
                },
                {
                    "sent": "OK, having sort of told you about all this, you know beautiful formalism.",
                    "label": 0
                },
                {
                    "sent": "We're now going to do something very simple.",
                    "label": 0
                },
                {
                    "sent": "You have to crawl before you can walk, right?",
                    "label": 0
                },
                {
                    "sent": "So we're just going to control capacity by adding a penalty to the loss function.",
                    "label": 0
                },
                {
                    "sent": "We're going to say there's certain parameters, which I think allow my model to be complex.",
                    "label": 0
                },
                {
                    "sent": "Those I'm going to penalize 'cause they're bad and there's other parameters which allowed my model to be simpler and those I'm going to allow because they're good and that's the job of this function P. So he looks at your parameters and he said, Nah, I don't like those parameters, they're going to make a complicated model.",
                    "label": 0
                },
                {
                    "sent": "For those parameters look fine.",
                    "label": 0
                },
                {
                    "sent": "They're going to be simple, and then we just take the sum of the loss function, which is the training error and the penalty on the parameters, and this is called penalized or regularised learning, and that's how we control capacity.",
                    "label": 0
                },
                {
                    "sent": "So it says it's good to fit the data well.",
                    "label": 0
                },
                {
                    "sent": "It's good to get low training error, but it's also good to bias yourself towards simpler models.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is just a picture to drive this home.",
                    "label": 0
                },
                {
                    "sent": "I want you to focus on this right hand side here.",
                    "label": 0
                },
                {
                    "sent": "This is a truth table.",
                    "label": 0
                },
                {
                    "sent": "So let's say I gave you this training data.",
                    "label": 0
                },
                {
                    "sent": "It's a function which has three binary inputs.",
                    "label": 0
                },
                {
                    "sent": "Here they are X1X2X3 and a single binary output.",
                    "label": 0
                },
                {
                    "sent": "Why there's why?",
                    "label": 0
                },
                {
                    "sent": "And here's the training data I tell you, if the input is 000, the output is 0.",
                    "label": 0
                },
                {
                    "sent": "If the input is 010, the output is 1, and so on.",
                    "label": 0
                },
                {
                    "sent": "All of these lines that are filled in or your training data.",
                    "label": 0
                },
                {
                    "sent": "And then I go over here and I say, could you please use your fancy machine learning technique to predict for me what the output is going to be when the input is 001?",
                    "label": 0
                },
                {
                    "sent": "So if you look at this, you should start to get really scared.",
                    "label": 0
                },
                {
                    "sent": "You should be thinking to yourself this is ridiculous.",
                    "label": 0
                },
                {
                    "sent": "This is arbitrary truth table right?",
                    "label": 0
                },
                {
                    "sent": "This value could be either zero or one.",
                    "label": 0
                },
                {
                    "sent": "I mean, how am I ever going to know what this value is right?",
                    "label": 0
                },
                {
                    "sent": "These values here have nothing to do with this value in principle.",
                    "label": 0
                },
                {
                    "sent": "Now you're starting to understand if you don't make assumptions about the world, you can never learn anything.",
                    "label": 0
                },
                {
                    "sent": "So this is an example where the fact that you have to make an assumption really is obvious.",
                    "label": 0
                },
                {
                    "sent": "It really comes in slaps you in the face.",
                    "label": 0
                },
                {
                    "sent": "Of course you have to make some assumption, like maybe this function is a conjunctive normal form.",
                    "label": 0
                },
                {
                    "sent": "Or maybe it's a sum of two clause literals.",
                    "label": 0
                },
                {
                    "sent": "Make some assumption about the form of this.",
                    "label": 1
                },
                {
                    "sent": "And now you can start to fill in these values because there's only one function which is consistent with the training data.",
                    "label": 0
                },
                {
                    "sent": "Now this is a picture where the fact that you have to make assumptions doesn't seem so obvious.",
                    "label": 0
                },
                {
                    "sent": "Imagine I gave you the training data which were these circles here, and I told you, please predict the value of the function somewhere in between.",
                    "label": 0
                },
                {
                    "sent": "Most of you would have no problem doing that, right?",
                    "label": 0
                },
                {
                    "sent": "You would say what's the big deal.",
                    "label": 0
                },
                {
                    "sent": "Let's just draw some kind of line like this through the data and you know interpolate on this line.",
                    "label": 0
                },
                {
                    "sent": "Drawing that line through the data is exactly the same as making some assumption to fill in the truth table.",
                    "label": 0
                },
                {
                    "sent": "Don't fool yourself that because you have a big visual cortex and it like smooth things that this is you know more natural assumption than any assumption you would make here.",
                    "label": 0
                },
                {
                    "sent": "When you draw a line and you interpolate, you are making an assumption.",
                    "label": 0
                },
                {
                    "sent": "You're making an assumption that this function is smooth in some very simple L2 cents of smooth, and that's fine if that's the assumption you really believe after know that you're making an assumption.",
                    "label": 0
                },
                {
                    "sent": "OK, so an unbiased learner can never generalize if you don't make assumptions about the.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But you can never learn anything.",
                    "label": 0
                },
                {
                    "sent": "OK, so enough of me sort of pontificating.",
                    "label": 0
                },
                {
                    "sent": "I want to sort of get moving on the probabilistic approach here.",
                    "label": 0
                },
                {
                    "sent": "So given the above, what we can think of learning as doing is estimating joint probability functions given samples from those functions.",
                    "label": 1
                },
                {
                    "sent": "So classification and regression supervised learning are about estimating the conditional density of the outputs.",
                    "label": 0
                },
                {
                    "sent": "Why given the inputs supervised learning problem, I show you the representation of the image and you tell me whether it has a face in it.",
                    "label": 0
                },
                {
                    "sent": "I show you the word counts in the spam in the email and you tell me whether it's camera unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "You can think of is just density estimation.",
                    "label": 0
                },
                {
                    "sent": "I give you some input sex and you want to estimate UX.",
                    "label": 1
                },
                {
                    "sent": "But either way, the central object of interest is the joint distribution between the inputs and the outputs, or just between the inputs.",
                    "label": 0
                },
                {
                    "sent": "If there's no outputs and the main difficulty is compactly representing this joint distribution and robustly learning its shape from a small number of samples an our inductive bias.",
                    "label": 1
                },
                {
                    "sent": "Is expressed as a prior assumption about these joint distributions.",
                    "label": 0
                },
                {
                    "sent": "So in the probabilistic framework, everything is about estimating large joint distributions between inputs and outputs.",
                    "label": 0
                },
                {
                    "sent": "That's the whole game and all of your assumptions are prior assumptions on what these distributions can and can't be.",
                    "label": 0
                },
                {
                    "sent": "They represent prior either prior distributions over explicit parameters or implicit distributions over classes of distributions.",
                    "label": 0
                },
                {
                    "sent": "So the main computations we're going to need to do in the probabilistic approach are to efficiently calculate marginal and conditional distributions from whatever compact representation of the joint distribution we choose.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the goal here, focusing on the probabilistic approach now is to represent a joint probability distribution P of X compactly even when there are many variables OK.",
                    "label": 1
                },
                {
                    "sent": "So just to drive this home, imagine you have a 1 megapixel image that comes off your camera and you have a pretty good CCD, so records you know 16 bits of information about each color channel, and now you want to ask how many possible images could my camera take?",
                    "label": 0
                },
                {
                    "sent": "OK. Well, that's one megapixel raised to the 3 * 16.",
                    "label": 0
                },
                {
                    "sent": "That's like an unbelievably mind blowingly large number.",
                    "label": 0
                },
                {
                    "sent": "OK so if you just wanted to represent the distribution over images by saying, well, for every possible image I'm just going to have a probability and those probabilities are just numbers and they sum up to one.",
                    "label": 0
                },
                {
                    "sent": "OK, now you're going to be in deep trouble 'cause that probability table that you're thinking of has so many entries that you could never write it down.",
                    "label": 0
                },
                {
                    "sent": "Right not on the biggest computer on the planet ever.",
                    "label": 0
                },
                {
                    "sent": "And even if you could write it down, there would be no way for you to estimate that distribution because you only saw, like you know, Paul Viola's face detectors only trained on a few 1000 images, so you have a few thousand observations in this billion billion dimensional space.",
                    "label": 0
                },
                {
                    "sent": "How are you ever going to do that?",
                    "label": 0
                },
                {
                    "sent": "So just raw representation of joint probabilities is never going to work.",
                    "label": 0
                },
                {
                    "sent": "OK, so you need to make some assumptions about the distribution.",
                    "label": 1
                },
                {
                    "sent": "So here's the simplest assumption you could make.",
                    "label": 0
                },
                {
                    "sent": "The distribution is completely factorized, so the probability of all of the joint probability of all of the observations X is just factorized into the product of some marginal distributions P of XI.",
                    "label": 0
                },
                {
                    "sent": "Now this is completely ludicrous.",
                    "label": 0
                },
                {
                    "sent": "It says the probability at every pixel in the image is completely independent of all the other probabilities.",
                    "label": 0
                },
                {
                    "sent": "Or to put it another way, if I gave you 100 by 100 subpart of the image, and I asked you to predict the central pixel that would be.",
                    "label": 0
                },
                {
                    "sent": "No easier than predicting a random pixel having seen none of the image, that's clearly false, but it's a very strong assumption.",
                    "label": 0
                },
                {
                    "sent": "OK, so the independence assumption is too restrictive, and what we want to do is we want to have some intermediate family of assumptions between this assumption, which is like the most aggressive assumption you can make, but is clearly false.",
                    "label": 0
                },
                {
                    "sent": "And this assumption, which is the most general form you could have and is clearly intractable.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to try and tell you about it for the rest of today.",
                    "label": 0
                },
                {
                    "sent": "Is a family of assumptions you can make which smoothly interpolate in some sense between this which is crazy and this which is also crazy.",
                    "label": 0
                },
                {
                    "sent": "OK so this is crazy because it's too general and if you don't make any assumptions you can't learn anything.",
                    "label": 0
                },
                {
                    "sent": "We're never going to have enough data or computational power to deal with, making no assumptions, and this is too crazy because it's such a ridiculous arranges aggressive assumption that it's like shooting ourselves in the foot before we even start.",
                    "label": 0
                },
                {
                    "sent": "No matter how carefully we proceed with this assumption.",
                    "label": 0
                },
                {
                    "sent": "Basically, over once we start, so we want something in between.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do instead of making complete factorization marginal independence assumptions, we're going to make what's called.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conditional independence assumptions.",
                    "label": 0
                },
                {
                    "sent": "So just a piece of notation I'm going to write.",
                    "label": 0
                },
                {
                    "sent": "X is conditionally independent of BXA is conditionally independent of XP given XY this way.",
                    "label": 0
                },
                {
                    "sent": "Nets.",
                    "label": 0
                },
                {
                    "sent": "A very old notation goes back to Whitaker and Pearl used it in his books.",
                    "label": 0
                },
                {
                    "sent": "It's pretty traditional statistical notation.",
                    "label": 0
                },
                {
                    "sent": "The definition says that two sets of variables XA and XB, are conditionally independent given a third set EXE.",
                    "label": 1
                },
                {
                    "sent": "If the joint distribution between A&B given C Factorizes.",
                    "label": 0
                },
                {
                    "sent": "OK, so this says informally.",
                    "label": 0
                },
                {
                    "sent": "Once you know C. Eh, doesn't really tell you anything more about B or B.",
                    "label": 0
                },
                {
                    "sent": "Doesn't really tell you anything more about a then you already knew from C. It doesn't say A&B or unrelated, it just says once I tell you see, there's no more information that they can tell you between themselves beyond what she already gave you about both.",
                    "label": 0
                },
                {
                    "sent": "Another thing it's quivalent to saying is that the conditional distribution of a given both B&C is exactly the same.",
                    "label": 0
                },
                {
                    "sent": "Your belief about a.",
                    "label": 0
                },
                {
                    "sent": "If I show you both B&C is exactly the same as your belief about a.",
                    "label": 0
                },
                {
                    "sent": "If I just show you see.",
                    "label": 0
                },
                {
                    "sent": "So only a subset, a very small subset of all distributions in the world respect some conditional independence assumptions like this that we might make when you make the conditional independence assumption, you're basically throwing away a huge set of joint distributions which no longer are appropriate because they don't satisfy that assumption, and that's good.",
                    "label": 0
                },
                {
                    "sent": "Remember our goal in life is to take all of the probability distributions in the world and chop some of them away to get a restricted set so we can work with that as I offices.",
                    "label": 1
                },
                {
                    "sent": "So the subset of distributions that respect.",
                    "label": 1
                },
                {
                    "sent": "All of the conditional assumptions we make is going to be the family of distributions consistent with our assumptions or hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "That's what our about.",
                    "label": 1
                },
                {
                    "sent": "This class is going to be an probabilistic.",
                    "label": 0
                },
                {
                    "sent": "Graphical models are powerful and simple way to specify.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This family.",
                    "label": 0
                },
                {
                    "sent": "So what is a probabilistic graphical model probabilistic graphical model is a way of representing large joint distributions compactly using a set of local relationships that are specified by a graph.",
                    "label": 1
                },
                {
                    "sent": "So probably a graphical model is nothing more than a macro language.",
                    "label": 0
                },
                {
                    "sent": "Which is a fast way for me to tell you, know Alex Smola or John or any one of you.",
                    "label": 0
                },
                {
                    "sent": "This is the particular family of joint distributions.",
                    "label": 0
                },
                {
                    "sent": "I wish to consider.",
                    "label": 0
                },
                {
                    "sent": "So there's a way of writing a picture.",
                    "label": 0
                },
                {
                    "sent": "If I show you this picture, you instantly know what family of joint distributions are considering.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's convenient macro language for talking to each other about conditional independence assumptions, or equivalently about subsets of distributions that we're going to consider about conditional independence.",
                    "label": 0
                },
                {
                    "sent": "Assumptions were going to make.",
                    "label": 0
                },
                {
                    "sent": "And the way you decode this macro language is that each random variable in R model is a node in the graph.",
                    "label": 0
                },
                {
                    "sent": "So this node X1 corresponds to the random variable X1, which might be pixel one they're directed or undirected.",
                    "label": 1
                },
                {
                    "sent": "Although in this section will only talk about directed their directed edges between the nodes, which tell us something about the factorization of the joint probability.",
                    "label": 1
                },
                {
                    "sent": "And although you can't see it on this picture, if you open this up here inside there is a quantitative function at the node which tells us.",
                    "label": 1
                },
                {
                    "sent": "The details of the pieces into which the distribution factor is OK, so graphical models are, like you know.",
                    "label": 0
                },
                {
                    "sent": "This famous criminal that goes under many names, right?",
                    "label": 0
                },
                {
                    "sent": "So they are also known as Bayesian networks, Bayesian belief networks, belief networks.",
                    "label": 0
                },
                {
                    "sent": "You know any sort of permutation of this.",
                    "label": 0
                },
                {
                    "sent": "So when you hear people talk about belief Nets or Bayes Nets or Bayesian belief networks, those are all the same thing as probabilistic graphical models.",
                    "label": 0
                },
                {
                    "sent": "They're just this macro language for this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Testing condition with dependence assumptions.",
                    "label": 0
                },
                {
                    "sent": "So we're going to focus on directed graphical models, and here is the basic factorization.",
                    "label": 0
                },
                {
                    "sent": "Oh, let me ask.",
                    "label": 0
                },
                {
                    "sent": "Actually, what should we do about time?",
                    "label": 0
                },
                {
                    "sent": "Should I just go half an hour later than we were supposed to go, or should I?",
                    "label": 0
                },
                {
                    "sent": "I hope everyone had some breakfast this morning.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to consider these directed graphical models and I'm going to focus on the subset of graphs which are directed acyclic graphs, so there's no directed cycles in the graph.",
                    "label": 0
                },
                {
                    "sent": "Each node in such a graph has a possibly empty, but has a set of parents, which I'll call \u03c0, and each node stores inside it a function, and that function is the conditional distribution probability of XI given its parents.",
                    "label": 1
                },
                {
                    "sent": "So really, the factorization of the joint distribution here is in terms of local.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Additional probabilities, so let's look at this example and that will help you understand when I write this picture.",
                    "label": 0
                },
                {
                    "sent": "What I'm really telling you is.",
                    "label": 0
                },
                {
                    "sent": "I think the joint distribution between these six random variables.",
                    "label": 0
                },
                {
                    "sent": "Is factorized in this way.",
                    "label": 0
                },
                {
                    "sent": "This joint distribution probability of X1X2X3 etc is the probability of X1 given nothing because he has no parents times the probability of X2 given all of his parents?",
                    "label": 0
                },
                {
                    "sent": "That's just X one and so on probability of X3 given all of his parents.",
                    "label": 0
                },
                {
                    "sent": "This is the picture that I draw.",
                    "label": 0
                },
                {
                    "sent": "This is the assumption that I'm making and this is the structure of the computer program.",
                    "label": 0
                },
                {
                    "sent": "The computer program has at each node these undefined constants which control the conditional distributions and our job is to fill in these probability distributions.",
                    "label": 0
                },
                {
                    "sent": "So all that's left once I write down this model is the values that live in this table.",
                    "label": 0
                },
                {
                    "sent": "Everything else has already been specified.",
                    "label": 0
                },
                {
                    "sent": "We specified the structure of the hypothesis class just by reading this graph, which hypothesis we select depends on what values I put in these conditional probability tables, which is equivalent to specifying these assumptions.",
                    "label": 0
                },
                {
                    "sent": "So the graph tells you the factorization.",
                    "label": 0
                },
                {
                    "sent": "The parameters that live in the nodes of the graph tell you the details which are the specific member of your office class.",
                    "label": 0
                },
                {
                    "sent": "So this picture does not specify a model.",
                    "label": 0
                },
                {
                    "sent": "A specific model.",
                    "label": 0
                },
                {
                    "sent": "It specifies a hypothesis Class A set of models only when you add the particular choice of parameters.",
                    "label": 0
                },
                {
                    "sent": "When you instantiate it.",
                    "label": 0
                },
                {
                    "sent": "That way, you get to the specific probabilistic model that you're talking about, so this is the hypothesis class an within it we can select anything we want by setting these.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These local distributions.",
                    "label": 0
                },
                {
                    "sent": "So the key point about directed graphical models is that missing edges imply conditional independence.",
                    "label": 1
                },
                {
                    "sent": "So remember that by the chain rule, we can always write the full joint distribution as a product of conditions, so I could always take any joint distribution without making any assumptions about it and start writing it out.",
                    "label": 1
                },
                {
                    "sent": "This way it's the probability of X, one times the probability of X2 given X 1 * X Three given everything that came before X4.",
                    "label": 0
                },
                {
                    "sent": "Given everything that came before, and so on.",
                    "label": 0
                },
                {
                    "sent": "And what does a DAG do a dad goes in and he crosses off some of these things?",
                    "label": 0
                },
                {
                    "sent": "He said no, no, no X.",
                    "label": 0
                },
                {
                    "sent": "Let's go back to our picture.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "X4 here doesn't have anyone except X2 as his parents.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our assumption would go in and say oh let's cross off X1 and X3 and X6 those are no longer parents of export.",
                    "label": 0
                },
                {
                    "sent": "So could missing edges.",
                    "label": 0
                },
                {
                    "sent": "Correspond to conditional independence assumptions.",
                    "label": 0
                },
                {
                    "sent": "Remember, I told you that the conditional independence assumption is saying that the probability of something given many things is the same as the probability of that given a subset.",
                    "label": 0
                },
                {
                    "sent": "That's what conditional independence says, and that's exactly what we do when we remove edges, we cross off these conditioning elements.",
                    "label": 0
                },
                {
                    "sent": "So the DAG is telling us that each variable is conditionally independent of its non descendants given its parents.",
                    "label": 1
                },
                {
                    "sent": "Once you know your parents then your conditional independence your conditional distribution is completely specified.",
                    "label": 1
                },
                {
                    "sent": "And removing an edge into node I eliminates an argument from the right hand side.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's a very important effect in directed graphical models which I want to draw your attention to before we get started.",
                    "label": 0
                },
                {
                    "sent": "This effect is called explaining away and it addresses a confusion that a lot of people have when they start thinking for the first time about graphical models.",
                    "label": 0
                },
                {
                    "sent": "If you look at this graph here.",
                    "label": 0
                },
                {
                    "sent": "You might think to yourself well.",
                    "label": 0
                },
                {
                    "sent": "X&Z are kind of unrelated.",
                    "label": 0
                },
                {
                    "sent": "Because there's no edge between them.",
                    "label": 0
                },
                {
                    "sent": "And they don't have any common parents, so there's sort of no coupling between X&Z, but because they share a common child when we condition on this child, when you observe the value of YX&Z become coupled.",
                    "label": 1
                },
                {
                    "sent": "OK, so X&Z are marginally independent.",
                    "label": 1
                },
                {
                    "sent": "That's true if you just generate samples from the family of distributions specified this by this graph and look at the marginal correlations, you'll find the Texans better uncorrelated.",
                    "label": 0
                },
                {
                    "sent": "In fact, they're completely independent, but conditioned on why?",
                    "label": 0
                },
                {
                    "sent": "These things are highly dependent.",
                    "label": 0
                },
                {
                    "sent": "For example, imagine that there are two coins that we flip independently, so X is the flip of coin one and then is the flip a coin.",
                    "label": 0
                },
                {
                    "sent": "Two we flipped on completely independently.",
                    "label": 0
                },
                {
                    "sent": "We just flip one and we flip the other.",
                    "label": 0
                },
                {
                    "sent": "Have nothing to do with each other.",
                    "label": 0
                },
                {
                    "sent": "And why is 1 if the coins are the same in zero, the coins are different.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let's assume I don't tell you why.",
                    "label": 0
                },
                {
                    "sent": "I just tell you X came up heads.",
                    "label": 0
                },
                {
                    "sent": "What do you think about why?",
                    "label": 0
                },
                {
                    "sent": "We can say, well, I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean, why is either heads or tails and they're unrelated.",
                    "label": 0
                },
                {
                    "sent": "So I guess I think that why is either heads or tails.",
                    "label": 1
                },
                {
                    "sent": "I don't know anything about one.",
                    "label": 0
                },
                {
                    "sent": "Now I tell you OK, observe X came up heads and why is 1 now?",
                    "label": 0
                },
                {
                    "sent": "What do you think about that?",
                    "label": 0
                },
                {
                    "sent": "Well, now you have very strong belief about them, right?",
                    "label": 0
                },
                {
                    "sent": "'cause I told you why is 1 only if the coins of the same and I told you X's head.",
                    "label": 1
                },
                {
                    "sent": "So he knows that it's so.",
                    "label": 0
                },
                {
                    "sent": "This is a simple example to indicate the Texans Ed can be conditionally dependent if they share a common child and that effect is called explaining away.",
                    "label": 1
                },
                {
                    "sent": "This came from the sort of decision analysis literature where the idea was to, you know, your car isn't starting.",
                    "label": 0
                },
                {
                    "sent": "That's the variable Y, and it could be either because your battery is dead or because you know you're out of gas.",
                    "label": 0
                },
                {
                    "sent": "And being out of gas doesn't cause your battery to die in your battery.",
                    "label": 0
                },
                {
                    "sent": "Dying doesn't cause you to be out of gas, so marginally these things are unrelated.",
                    "label": 0
                },
                {
                    "sent": "But now if you try and start your car in your car doesn't start.",
                    "label": 0
                },
                {
                    "sent": "Now you say Oh well, could be the battery could be the gas you know.",
                    "label": 0
                },
                {
                    "sent": "And then you read your gas gauge in your gas gauge is full.",
                    "label": 0
                },
                {
                    "sent": "Now suddenly your belief about your battery change, right?",
                    "label": 0
                },
                {
                    "sent": "So the observing that your gas tank was full cause this explaining effect explained away.",
                    "label": 0
                },
                {
                    "sent": "The other cause, so that's the where that name comes from.",
                    "label": 0
                },
                {
                    "sent": "So if you really already come up with a plausible assumption that it's your battery or lose your gas, and the other assumption the other cause gets gets explained away, it gets suppressed.",
                    "label": 0
                },
                {
                    "sent": "Your belief about it because they share a common.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in order to actually start writing a computer program using this style of probabilistic graphical models, you need to know something about what's inside the nodes.",
                    "label": 1
                },
                {
                    "sent": "So for directed models you need some kind of prior functions P of XI or conditional functions for nodes that have parents P of XI given their parents.",
                    "label": 0
                },
                {
                    "sent": "And remember, we have these various types of variables that we're considering.",
                    "label": 0
                },
                {
                    "sent": "Binary or discrete variables, sometimes called categorical variables, continuous variables.",
                    "label": 0
                },
                {
                    "sent": "We could have other things, integer counts, interval valued variables, whatever, so.",
                    "label": 0
                },
                {
                    "sent": "On the slides I put some very basic probability models.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to sort of go over them, I just put them on the slides.",
                    "label": 1
                },
                {
                    "sent": "For those of you who are bit rusty on your probability, which are just parameterized foul families of distributions, and those are the models that live inside the nodes of these directed models.",
                    "label": 0
                },
                {
                    "sent": "So when I drew.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here on this picture I drew these these blocks here.",
                    "label": 0
                },
                {
                    "sent": "These blocks are supposed to represent these conditional probability distributions, and I drew them here as though the variables were discrete and we were just going to use a table based parameterisation.",
                    "label": 0
                },
                {
                    "sent": "But there's many, many things.",
                    "label": 0
                },
                {
                    "sent": "If these variables are continuous then you have different families of analytic functions here, and if these variables are discrete, you can still have more structure than in this table.",
                    "label": 0
                },
                {
                    "sent": "So in general you should think of this is just a schematic which tells you you need to put some conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "In there, but which one is up to you?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I said, for discrete or categorical variables, the most basic parameterisation is just the table.",
                    "label": 1
                },
                {
                    "sent": "It just says what is the probability that X takes on its case value, or if it's a conditional table you just say for each possible value value of the parents we have a table which says what is the probability that X takes on a certain value given that setting of the parents.",
                    "label": 0
                },
                {
                    "sent": "That's why it's called a CPT conditional probability table.",
                    "label": 0
                },
                {
                    "sent": "For each value of the parents, we have a conditional probability table.",
                    "label": 1
                },
                {
                    "sent": "And of course these numbers in the table are non negative and they sum to one.",
                    "label": 0
                },
                {
                    "sent": "So if you have K possible settings of the value there K -- 1 degrees of.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are the distributions that I told you I listed on this slide just to kind of remind you all.",
                    "label": 0
                },
                {
                    "sent": "Generally, for analytic convenience, people in the past have considered for continuous numerical random variables have considered distributions in the exponential family, so these include all of these distributions whose names are hopefully somewhat familiar with the Bernoulli, binomial, Gaussian gamma for positive variables, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And for continuous variables, when nodes have parents, the situation is a little different.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's four cases, and three of them are easy.",
                    "label": 0
                },
                {
                    "sent": "If I had a nickel for every time I said that.",
                    "label": 0
                },
                {
                    "sent": "OK, so the four cases are.",
                    "label": 0
                },
                {
                    "sent": "Discrete parent discrete child.",
                    "label": 0
                },
                {
                    "sent": "That's pretty easy.",
                    "label": 0
                },
                {
                    "sent": "You can use a conditional probability table.",
                    "label": 0
                },
                {
                    "sent": "OK, continuous parent continuous child.",
                    "label": 0
                },
                {
                    "sent": "In that case you have to use some kind of functional dependence between the parameters of the distribution.",
                    "label": 0
                },
                {
                    "sent": "The continuous distribution on the child and the parameters of the continuous distribution on the parents.",
                    "label": 0
                },
                {
                    "sent": "So that's just a completely analytic dependence.",
                    "label": 0
                },
                {
                    "sent": "Discrete parent, continuous child.",
                    "label": 0
                },
                {
                    "sent": "There you can again do conditional probability distributions, not conditional probability tables.",
                    "label": 0
                },
                {
                    "sent": "But for each setting of the parent fits a discrete, you just have discrete number of settings of the parents.",
                    "label": 1
                },
                {
                    "sent": "You specify a different set of parameters for the distribution of the child.",
                    "label": 0
                },
                {
                    "sent": "Now the hard case which I want to talk about at all is continuous parent discreet child.",
                    "label": 0
                },
                {
                    "sent": "In there it's much harder to come up with elegant general families of conditional distributions.",
                    "label": 0
                },
                {
                    "sent": "The one form that we really know about there that we can use well is log linear distribution, which I'll tell you.",
                    "label": 1
                },
                {
                    "sent": "So when the parent is numeric, very common instance here is to use what's called the linear Gaussian form.",
                    "label": 0
                },
                {
                    "sent": "So you say that the probability distribution over the child or the children, why, given their parents X is Gaussian with a mean which is a linear function of the parents.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's that's sort of the most most common form here.",
                    "label": 0
                },
                {
                    "sent": "OK, so I just I just want to finish up and then send you guys off to the break and then when we come back all I'll tell you will get into the nitty gritty of directed probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "So it's just a review here.",
                    "label": 0
                },
                {
                    "sent": "What's the goal of probabilistic graphical models?",
                    "label": 0
                },
                {
                    "sent": "So first I tried to tell you about the general set up in machine learning about the idea of training and testing and generalization and about the assumptions that you have to make and now.",
                    "label": 0
                },
                {
                    "sent": "We're going to focus on this one set of techniques, which is the probabilistic approach.",
                    "label": 0
                },
                {
                    "sent": "Mcglone graphical models the aim to provide a compact factorization of large joint probability distributions.",
                    "label": 1
                },
                {
                    "sent": "When you're dealing with high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "When you want to represent things probability probabilistically, you have no choice but to represent a large joint distribution.",
                    "label": 0
                },
                {
                    "sent": "And if you just write it down naively, you'll never be able to estimate it, so they provide compact factorizations.",
                    "label": 1
                },
                {
                    "sent": "These factorizations are achieved using local functions.",
                    "label": 0
                },
                {
                    "sent": "Which exploit conditional independence ease?",
                    "label": 1
                },
                {
                    "sent": "And that's what this picture tells us.",
                    "label": 1
                },
                {
                    "sent": "What family of independence assumptions are getting so the graph is a basic set of conditional independencies that we require are true of our joint distribution, and actually from those.",
                    "label": 0
                },
                {
                    "sent": "We can derive more conditional independence ease that also must be true.",
                    "label": 0
                },
                {
                    "sent": "All of these in dependencies together are crucial in developing algorithms that are going to be efficient ways of doing inference in these models were going to exploit these computationally, but more importantly, we exploit them in terms of sample complexity.",
                    "label": 0
                },
                {
                    "sent": "Since we made this factorization assumption, we vastly restricted the universe of distributions.",
                    "label": 0
                },
                {
                    "sent": "Now we have some hope of estimating them based on the limited amount of data, so the graph tells us about the hypothesis class, the factorization, and the local functions tell us the quantitative details exactly.",
                    "label": 1
                },
                {
                    "sent": "What are the?",
                    "label": 1
                },
                {
                    "sent": "Settings of the parameters.",
                    "label": 0
                },
                {
                    "sent": "What member of that bosses classroom or now?",
                    "label": 0
                },
                {
                    "sent": "Here's a sort of tricky point.",
                    "label": 0
                },
                {
                    "sent": "Certain numerical settings of this distribution may have more in dependencies.",
                    "label": 0
                },
                {
                    "sent": "If I choose a distribution at one of these nodes that happens itself to have some extra factorization, and that's fine, but those didn't really come from hypothesis class, so in the same way that if my hypothesis class you know includes all models, some subset of them may satisfy extra assumptions, that's fine, but just because I pick.",
                    "label": 0
                },
                {
                    "sent": "Particularly medical setting which is factorized doesn't mean it was part of the graphical model.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Original.",
                    "label": 0
                },
                {
                    "sent": "And and what's the motivation for doing all this?",
                    "label": 0
                },
                {
                    "sent": "The motivation is knowledge acquisition, so we want to build intelligent systems and the bottleneck is really how do we get knowledge from the world into the computer program, right?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that the method of getting it from the world into the computer program where we go through your brain is not a very good method.",
                    "label": 0
                },
                {
                    "sent": "You say I know about the world, 'cause I'm human, I'm smart, and I can tell the computer.",
                    "label": 0
                },
                {
                    "sent": "But I know by typing that's actually not true.",
                    "label": 0
                },
                {
                    "sent": "Doesn't work so.",
                    "label": 0
                },
                {
                    "sent": "Human experts are rare.",
                    "label": 0
                },
                {
                    "sent": "They're expensive, they're unreliable, and their slow.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, they're all these things and more so, but we have lots of machine readable data, right?",
                    "label": 1
                },
                {
                    "sent": "I mean, you want to know if a piece of email is spam.",
                    "label": 0
                },
                {
                    "sent": "You know what's the number John?",
                    "label": 0
                },
                {
                    "sent": "How many Yahoo Mail users are there?",
                    "label": 0
                },
                {
                    "sent": "Millions right?",
                    "label": 0
                },
                {
                    "sent": "And they are all clicking on that button that says I don't like this.",
                    "label": 0
                },
                {
                    "sent": "It's Pam right?",
                    "label": 0
                },
                {
                    "sent": "So we have tons of infinite amount of labeled data for that problem and we want to build systems automatically based on the data.",
                    "label": 1
                },
                {
                    "sent": "So a small amount of prior knowledge which I trust you to put into your system and a huge amount of statistical knowledge which I'm hoping to suck out of the data and then we can build a system which looks at an image and gives it a label or looks at an email and tell us whether it's been so for now our systems that we build.",
                    "label": 0
                },
                {
                    "sent": "Are going to be probabilistic graphical models.",
                    "label": 0
                },
                {
                    "sent": "We assume that the prior information that I let you put in specifies the type and structure of the graphical model, the joint distribution, and the mathematical form of those parent conditional distributions and the learning part where we suck the information out of the data is setting the parameters of those distributions.",
                    "label": 0
                },
                {
                    "sent": "OK, there's a whole interesting area of structure learning where we also try and learn the structure of this graphical model.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and then the last thing just to keep your mind warm for after the break in for John Langford socks this afternoon is the basic statistical problems.",
                    "label": 1
                },
                {
                    "sent": "We're going to talk about.",
                    "label": 0
                },
                {
                    "sent": "So regression tries to estimate the value of a continuous output given some inputs.",
                    "label": 0
                },
                {
                    "sent": "Classification tries to estimate a Class A discrete class label.",
                    "label": 0
                },
                {
                    "sent": "Given some inputs.",
                    "label": 0
                },
                {
                    "sent": "Clustering is sort of like classification, where the class labels are UN observed.",
                    "label": 0
                },
                {
                    "sent": "You're trying to.",
                    "label": 0
                },
                {
                    "sent": "Label",
                    "label": 0
                }
            ]
        }
    }
}