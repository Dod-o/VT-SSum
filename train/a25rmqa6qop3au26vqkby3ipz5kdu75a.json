{
    "id": "a25rmqa6qop3au26vqkby3ipz5kdu75a",
    "title": "Recent Advances in Large Linear Classification",
    "info": {
        "author": [
            "Chih-Jen Lin, Department of Computer Science and Information Engineering, National Taiwan University"
        ],
        "published": "March 27, 2014",
        "recorded": "November 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/acml2013_lin_large_linear_classification/",
    "segmentation": [
        [
            "Yes, thank you very much.",
            "So it's my pleasure to be here to give a talk at HTML.",
            "So I I still remember that two years ago in the entire one we had a CMA at that time and I was a general chair.",
            "So we're very.",
            "We always hope to support this conference and we hope that it will be.",
            "Yeah, keep improving to be a major machine learning conference.",
            "So now in this talk I will talk about some of my recent work large scale linear classification."
        ],
        [
            "Actually, this talk is based on my survey paper, so last year in Proceedings of IEEE I have a paper called a recent advances of large scale linear classification.",
            "So all the details can be found there and also it's related to.",
            "Our development of a software package called the League linear.",
            "So this morning in Jeffs talk, so he mentioned how to encourage people to develop machine learning software.",
            "So he mentioned in Journal of Machine learning research, there's a.",
            "There's a software section to encourage people to submit their software paper layer, which here is 1 example.",
            "Lib Linear Paper was published in Journal of Machine Learning Research in 2008.",
            "That's one of the earliest.",
            "Paper in light software section, and that's actually a successful example.",
            "I think that paper has been the maybe the most cited one of all the Journal of Machine learning research Paper seems maybe 2006, so I think that's an example that things that are Journal has an open source section, then actually that encourages a lot more people to develop useful software for for the Community.",
            "And there are a lot of materials about the recent developments, so due to the time constraints I will mainly give some overviews instead of getting into deep technical details."
        ],
        [
            "So here's the outline of the talk."
        ],
        [
            "I will give an introduction about large scale linear classification and learn."
        ],
        [
            "Talk about optimization methods to solve to obtain model and then also talk about some extensions of linear classification.",
            "Then have some discussion and conclusions."
        ],
        [
            "So how's the difference between linear and nonlinear classification?",
            "Will this figure says the difference in for the linear situation?",
            "We basically draw a straight line to separate data.",
            "But for nonlinear we use a nonlinear curve.",
            "But there are many nonlinear methods.",
            "So for example, decision trees, random forest.",
            "They're all nonlinear methods, but here are when I say nonlinear actually mean kernel, so here.",
            "When I say linear, that means we use data in the original space.",
            "So here for the original feature vector, you have two features.",
            "They are height and weight.",
            "But if you do some mapping so you generated and you are new feature called Await Divided by the square of height, that's a new feature.",
            "Then this is a kind of nonlinear mapping, and as I have said, there are many nonlinear methods.",
            "But kernel method is one of the one of the popular nonlinear methods."
        ],
        [
            "So for for linear and kernel basically we can use slamming two ways.",
            "So when one representative kernel methods are support vector machines and logistic regression, so they usually can be used in practice in two ways.",
            "So one is by Colonel.",
            "So that means you map data to a higher dimensional space.",
            "So here you map data X2 to another vector code file of X.",
            "But because this file.",
            "5X may be very high dimensional, therefore it is very difficult to manipulate such feature vectors.",
            "Such vectors in high dimensional space, so we need to use kernel tricks for kernels.",
            "Mapping functions are special so that the inner product can be easily calculated.",
            "That's the kernel trick, so that's one way to use SVM or logistic regression.",
            "But another disadvantage disadvantage of using a kernel is let you don't have a good control on your mapping function, because you won't let the inner product be easy, so you don't.",
            "You lose a good control control on your mapping function, but another popular way of using SVM or logistic regression is.",
            "To do feature engineering plus linear classification.",
            "So feature engineering.",
            "That means are you using your domain knowledge?",
            "You find out good features and once you have good features you don't make your data to a higher dimensional space or to a different space you stay, you stay in the original input space.",
            "That's what I call linear classification.",
            "So in some sense you can say that this is you already done the nonlinear mapping because in your feature engineering you tried all kinds of combinations of features and to try to see which one is better.",
            "That's kind of nonlinear process, but once that is done then you have a linear classification problem.",
            "And in this talk I will focus on the second approach.",
            "Well, today we're not going to talk about kernel, so we want to talk about the second case right.",
            "Once you have done feature engineering, then how can you do large scale linear classification?"
        ],
        [
            "My name is a quick question, that's why we want to do linear classification then.",
            "Well then there are some reasons.",
            "So.",
            "So here I want to convince you that in some situations we should do linear classification.",
            "So if you map data to a high dimensional space, then because this data becomes so long then actually decision function becomes difficult to calculate.",
            "So the decision function once you have mapped test data X.",
            "Too high dimensional space.",
            "Then you do the inner product with the model and to check the sign of this value.",
            "Then this calculation can be expensive.",
            "So kernel methods.",
            "They actually use the kernel tricks to let the model W to be the linear combination of all the map vectors then so so you can see that here this model W is actually a linear combination linear combination of all the map vectors with coefficients Alpha.",
            "Then we use the technique that the kernel element is the inner product between two map vectors.",
            "Then this decision function becomes this submission.",
            "OK, so we just put this.",
            "This formulation of W into the decision function and we get this submission.",
            "So so for this.",
            "So for this I mentioned, as long as you know how to calculate kernels, then you can calculate the summation then.",
            "Then you can do the prediction and that's the way how kernel works.",
            "So so I have mentioned that in order to easily calculate the inner product, your mapping function must must be special.",
            "So here is an example.",
            "This is the so-called degree two polynomial kernel.",
            "So, so this degree to function.",
            "So we see degree to here is actually the inner product between two vectors and each vector each vector is already in North squared dimensional space and here means the number of features.",
            "So so from the front of vector of N features you have mapped data to end square dimensional space.",
            "However you don't need to do inner product in an square dimensional space, you still do dinner.",
            "Product in the original space, then do the square OK, so that's the key."
        ],
        [
            "However, Colonel is still expensive, right?",
            "So if you if you compare doing the original linear prediction and the kernel situation so in the linear case you only need to do one inner product between the test vector and the model vector W. But for kernel you need to do the summation.",
            "So if any, is there a number of features in the linear case you need order an operations to do just want to predict one instance.",
            "But if you do kernel because of this submission, then you need older NL&L is the number of training data.",
            "Of course if you use support vector machines you get sparsity, but still you need less proportional to the number of support vectors.",
            "Yeah so basically here I argue that will kernel is good.",
            "It is more powerful to separate.",
            "Date, however linear, has an advantage that it is cheaper and simpler."
        ],
        [
            "But cheaper and simpler is not good, right?",
            "We also want good accuracy.",
            "So the interesting thing is that for certain problems, the test accuracy by a linear classifier is already as good as the kernel once.",
            "But then by using only linear classifiers, your training and testing can be much faster.",
            "Well then this is good.",
            "You get simultaneously good accuracy, but as well as faster training and testing speed so this.",
            "Especially happens for document classification.",
            "So right now the the most common ways to do document classification is to generate the feature vectors by the so called bag of words model.",
            "That means each each word corresponds to a feature.",
            "So if you have say 2 million English awards and you have 2 million features and you get a very very sparse feature vectors, because each document probably has several 100 words right, only several 100 features will have nonzero values.",
            "And all remaining feature values become zero.",
            "There are large and sparse data and for such data now we have already observed that if you do a linear classification then the accuracy is almost the same as the kernel classifier, but but by using only linear classification Now, you can easily trim millions of data points in just a few seconds."
        ],
        [
            "So here is an example to compare linear and kernel classifiers."
        ],
        [
            "I tried to compare training time as well as test testing accuracy.",
            "For the first 2 columns, that's the result by using a linear classifiers.",
            "So time means training time and accuracy means test accuracy, and for the for the other two columns they are results of using RBF or say, Gaussian kernel.",
            "Again, timing means training time and accuracy means test accuracy.",
            "From this comparison.",
            "So if we look at this data set, this data set has around half million training instances.",
            "If you use linear classifier then the training time is very fast.",
            "You only need 1.4 seconds, but your accuracy is only 76%.",
            "Well this is not very good, but if you spend more time by using kernel method.",
            "So here you need maybe around Health Day but but then your test accuracy is dramatically improved to 96%.",
            "So this is a good case that you may still want to use kernel even though you need a lot more training type."
        ],
        [
            "However.",
            "If you look at last three datasets from the name, you can tell that they are documents like News 20, Yahoo Japan.",
            "Acument data.",
            "Say for news 20 in 1.1 second we achieve 96% test accuracy, but if you use a kernel methods.",
            "Then you need a almost 400 seconds, but your accuracy is actually almost the same in the same situation.",
            "Situation happens for the other two document datasets and the notice read Alouds datasets are pretty large.",
            "So for example for the last one at Yahoo Japan, we actually had a 140,000 instances and.",
            "830,000 features, so this is pretty large, but if you look at the training time, this is 3 seconds, so this is very, very good, so that gives us a reason that for certain problems large scale linear classification is very useful, so we should try to develop something so people can use to use linear classification for such applications."
        ],
        [
            "Now let's get into a more technical details.",
            "I want to formally define what a binary classification problem is.",
            "I assume that we are given a bunch of training labels and features, so why I and XI?",
            "That's a lots of pair of label and the feature vector.",
            "So why is it is plus or minus one to indicate the label of the training data and XI is A is a feature vector in N dimensional space we are using to denote the number of features and I use L to denote the number of training data, the standard.",
            "Formulation to train a linear classifier is to solve an optimization problem, at least in the objective function of this optimization problem.",
            "It contains 2 parts.",
            "The first part this W transpose W / 2 is the so called regularization term.",
            "The reason is to avoid overfitting.",
            "Then the second term is this submission of loss values.",
            "So we use a different kinds of loss functions.",
            "And they do this I mentioned, so that indicates the kind of training error.",
            "So we want to balance between regularization and training errors.",
            "So there's a parameter C which is, which is the so called regularization parameter that must be decided by users.",
            "So this is a standard formulation of a linear classifier."
        ],
        [
            "Are different types of loss functions, so here I give us 3 examples.",
            "Another way to design."
        ],
        [
            "Functions is that are using.",
            "In doing the training, you hope that your model W. Can can be generated such that for this data X, if the label is why then you are W transpose?",
            "X should have the same sign as Y, right?",
            "Because if this data is positive, you are hoping that your model W can give you the value W transpose X to be also positive.",
            "So you hope that this YW transpose X is positive."
        ],
        [
            "So from that principle we can design our loss function.",
            "So if you look at the first one let's the so called hinge loss or sometimes called L1 loss, we do the mix between 0 and 1 -- y W transpose X.",
            "So if if your data is positive and W transpose X is negative, that means YW transpose X becomes negative.",
            "Then 1 -- A negative value.",
            "Becomes positive, then we shouldn't mix with zero.",
            "Then you get a positive value that indicates a kind of training error.",
            "So that's the idea.",
            "But on the other hand, if you're Y&W transpose X, they are both positive.",
            "Then that means 1 -- a positive value may become negative right then after doing the Max with zero you get value zero.",
            "That indicates that there is no training error.",
            "And for the second, let's call the squared hinge loss.",
            "So actually the 1st and the second other commonly used form of support vector machines.",
            "Then for the third one logistic loss.",
            "So if you use that, you get the regularised logistic regression.",
            "You can see that here I can give references for support vector Machine, but I cannot do that for logistic regression, because this technique can actually be traced back to maybe.",
            "19th century.",
            "Then you may ask the differences between those three loss functions."
        ],
        [
            "Turns out that they are very similar, so if we draw the figures of the three loss functions, you can see things like this, so they're very similar on the left hand side.",
            "That means if there is no training error, then the loss function should have the value close to 0.",
            "But then of course for if you have training error then the way they give penalty to the training error are slightly different.",
            "So if you have least squared hinge loss and the value becomes larger.",
            "If you have a large error but but usually OK using the the way how they are designed are similar.",
            "So usually the performance between support vector machine and the logistic regression.",
            "They are actually similar."
        ],
        [
            "Then we need to to minimize the optimize it to solve the optimization problem in order to get the model.",
            "But I but, but we we notice that optimization methods for them.",
            "Maybe different users due to the different properties of the loss functions.",
            "If you are using hinge loss, then the problem."
        ],
        [
            "It is that it is not differentiable at least point, so you cannot use a differentiable optimization technique."
        ],
        [
            "And therefore squared hinge loss is differentiable but not twice differentiable, and therefore logistic regression is twice differentiable is easier to apply optimization techniques?"
        ],
        [
            "OK, so now we come to the second part is how to solve the optimization problems.",
            "But there are many available optimization techniques.",
            "Roughly speaking, you can use high order methods like Newton method or Cosine Newton methods, but you can also consider low older methods such as coordinated decent gradient, decent or stochastic gradient, dissent or such things.",
            "But I will I will here I will discuss.",
            "Representative methods."
        ],
        [
            "Here I consider a second order method.",
            "That's the Newton method.",
            "So how's your basic idea of Newton method?",
            "We do the 2nd order approximation of the function, so here this F of W means the function I'm going to minimize the 2nd order Taylor expansion of the function F becomes this problem.",
            "So if we do a minimization over the direction S, so this is the 2nd order approximation.",
            "If we can find out that direction, is that sort of so called Newton direction, then from the current current iterate code WK?",
            "So Casey iteration index so from WKI take the direction as.",
            "Then I get to the next iterate.",
            "So so Newton method isn't iterative procedure and at each iteration I need to find this so called Newton direction.",
            "And therefore this minimize this quadratic minimization problem is equivalent to solving the so-called Newton linear system.",
            "So this is the so called Hessian matrix where H matrix is likely.",
            "Second derivative of the function.",
            "But because you have multiple variables, so Now this Haitian is a matrix.",
            "So this this Haitian is a matrix and we need to solve the linear system with matrix here and the variable X and this is minus of the gradient of the function F. But so far this is standard Newton massive.",
            "Well, there's nothing special to data classification, but if you would like to directly apply Newton method to do large scale linear classification will we will see some big troubles.",
            "So the problem is that this Haitian matrix can be too large to be stored.",
            "What is the size of this Hessian matrix?",
            "Well, the size of H matrix is actually the same as the number of variables times the number of variables.",
            "But what is the number of variables now with W?",
            "Is our variable right?",
            "The size of W is the same as number of features.",
            "So how many features you have?",
            "Then you have how many variables, but this can be very large, right?",
            "If you have 1,000,000 features and then this H matrix becomes 1,000,000 by 1 million.",
            "Usually this is too large to be stored.",
            "In your computer.",
            "So we need something special.",
            "Then if you do some calculation about SVM, linear SVM or logistic regression then you will find out that the Hessian matrix actually has a special form.",
            "So I take it back because if we want to do second derivative then this must be logistic regression because only it is twice differentiable.",
            "So for logistic regression the Hessian matrix actually has this special form.",
            "Well, the first I is an identity matrix then plus this C is the regularization parameter.",
            "Then here's a special product between three Matrix.",
            "The first is X transpose and lengthy an X.",
            "So how badly?"
        ],
        [
            "Will access basically is simply the data matrix.",
            "So just gather all the data together as one big matrix that's X and then these are diagonal matrix for logistic regression.",
            "This diagonal matrix actually takes this form.",
            "So the ice component actually is exponential.",
            "Something over this.",
            "So once we know this special form."
        ],
        [
            "Then we can use a technique to solve this Newton linear system without actually forming this Haitian matrix.",
            "So the idea is that in solving a linear system, usually there are two types of methods.",
            "One is called direct method, so if you do things like Gaussian elimination and you need to store the whole matrix, in general need to store the whole matrix before doing Gaussian elimination.",
            "But Alternatively you can use the so called iterative methods to solve linear system.",
            "So."
        ],
        [
            "There are many kinds of iterative methods for linear systems, but one way is the so called country gradient method.",
            "In the special thing about country gradient is that each country gradient iteration, all you need is to do a matrix vector product.",
            "So here the matrix is this Haitian matrix and now you want to multiply this Haitian with a vector.",
            "But we can do that without forming the whole matrix.",
            "So recall that our matrix is this X * D * X.",
            "But now we never need to multiply them out to form a huge matrix.",
            "Now we never need to do that because we have sequentially doula matrix vector product and still get the value.",
            "OK, so we do X * S first.",
            "Then we we multiply with this diagonal matrix D and then we multiply with this X transpose.",
            "Then we don't have the storage problem, so this is our difficulties to store that huge Haitian matrix.",
            "But now we don't worry about that.",
            "All you need is to store X, but of course you must be able to store X.",
            "That's the data matrix.",
            "It must be available.",
            "So in optimization this is the so-called hashing free technique.",
            "So you can see that this is a good combination between using optimization knowledge and also the machine learning properties, because only because."
        ],
        [
            "The Hessian matrix has such a special form, then we can apply this kind of."
        ],
        [
            "Patient free approach to use to apply Newton method for for large scale classification.",
            "So this is an interesting example."
        ],
        [
            "Let me skip this."
        ],
        [
            "Select now I will switch to another type of optimization methods where it's different from Newton, but Newton is a second order that use the 2nd order derivative.",
            "But now I'm going to use only the 1st order and that's the gradient.",
            "So now let's say the 1st order makes it, but instead of considering that optimization problem of variable W in.",
            "Optimization we can derive the so-called dual problem.",
            "So for both SVM and logistic regression you can derive the dual problem.",
            "So here I consider this example of using a one loss.",
            "For support vector machine, if you use these hinge loss, let go of the dual problem actually takes this form.",
            "So now I have a function of Alpha and this Alpha the size becomes the same as the number of instances.",
            "So this is different from www.isthesize is the same as number of features.",
            "But now this is the same as the number of instances.",
            "This function takes a following form that is the objective function to be minimized is a quadratic function, so variable is Alpha now and the quadratic term is after transpose matrix Q.",
            "In the time zone Alpha, in the minus linear attendee chance was after this easily vector of all ones.",
            "So where are the data?",
            "Will data embedded into this matrix Q?",
            "So the IJ component is actually the inner product between two training vectors.",
            "Well, actually if you are familiar with kernel methods, this is actually the basically the kernel matrix.",
            "OK, so now this is the objective function, but there are also a bunch of constraints.",
            "Let each other.",
            "I must be between zero and C. And remember, let's see is the regularization parameter.",
            "So now we want to minimize this.",
            "So how to?"
        ],
        [
            "But where we consider very very simple in the classical optimization technique called coding and decent.",
            "So the idea is that now you have so many variables to minimize, but we don't know how to do that.",
            "So just fix all of that except what so you try to fix all variables, try to change one of them.",
            "So now you change one variable at a time.",
            "So here I define some notation.",
            "Suppose currently we are at the vector Alpha, so advise where we have right now.",
            "Then I define a victory.",
            "I wish the ice component to be one and all the remaining elements to be 0.",
            "Then I want to check that little ice component.",
            "How far can I go so from a for Alpha I I tried to check that plus this scalar D times this unit vector EI.",
            "OK then to check the hostel change of D. That I can use so I get the one variable optimization problem because I all I need is to decide the amount of the in order to decide how I update update after I.",
            "And after some calculation, then turns out that this is a very simple one, variable quadratic optimization problem and at least one of course has a closed form.",
            "So we end up with the quadratic term becomes this Qi times the square, then the linear term we have the ice component of the gradient times DN plus some constants.",
            "They're not important.",
            "So now if we don't consider constraints in the in the in the dual problem, then this this one variable quadratic function immediately you know that the optimal optimal solution should be this.",
            "So this is from the linear term and the divided by the leading quadratic term, right?",
            "So you know the optimize this, but now remain."
        ],
        [
            "I believe that the dual problem has a constraint that I must be between zero and see therefore."
        ],
        [
            "One way we can do is that once we have find out this optimal D, if I plus D exceeds the range of zero and see we just project it back.",
            "That's why you see that these minima and makes operation OK. Basically it's beyond the ceiling.",
            "We project it back to see.",
            "So this is very simple.",
            "Now we know how to update the ice component of I."
        ],
        [
            "Let's hope Mr whole procedure procedure in this procedure is actually is actually very can be done in a very efficient way."
        ],
        [
            "Now let's see.",
            "Where are you?",
            "Where is the main computational bottleneck?",
            "If you look at least update.",
            "It is very simple, right?",
            "So where is the main calculation will remain, calculation is on the ice component of the gradient.",
            "QII can be pre calculated before the optimization procedure use.",
            "Qi is basically the length of each data data data point.",
            "Now let's look at."
        ],
        [
            "The ice component of the gradient, will the gradient actually takes this form, so you have this QF I -- 1 and if you expand this and you get there some mention then after this I mentioned then you have a summation of.",
            "We are a submission over Jalen.",
            "Something well, this looks complicated if you look at least then how many operations are needed.",
            "We need to do Eleanor products in this place.",
            "We need to do Eleanor products so this is not very good.",
            "Is not good.",
            "OK so say that again.",
            "So this submission is overall the training data over all the training data and each element is actually an inner product between two training instances, so that's why you need older Ellen cost.",
            "Well this is not good.",
            "This can be a lot, but there is a trick.",
            "There's a trick.",
            "You define a vector U.",
            "Then this you is the summation over J visit so you can see that this is QIJ right?",
            "And you are trying to check the inner product between I and all the instances index by Jay.",
            "But we find out that if we do this summation over J by defining this as a vector, then the gradient ice component of the gradient you can take.",
            "So if you take these XI out then this submission.",
            "Can be together so the whole thing becomes just one single inner product, one single inner product.",
            "So the cost is dramatically reduced from older L and to only order N. So all we need is to maintain this vector U.",
            "Then I'm going to skip all the details of this first order methods, because I don't think I have enough time.",
            "But if you are familiar with kernel methods Now, you may have noticed a difference.",
            "Why we are able to do this here, but we cannot do.",
            "We cannot do that in the kernel.",
            "The reason is when you are using kernel methods, then your data is mapped to a high dimensional space linked.",
            "You never explicitly write down the map vector.",
            "Therefore, if you cannot do this dimension, you can never form this vector U.",
            "But now for dinner because we are not making data to a.",
            "To announce that dimensional space or to an infinite dimensional space, we know how that is, so we can do this dimension, and once we can maintain this vector in this operation becomes very very cheap."
        ],
        [
            "OK."
        ],
        [
            "So let me skip this."
        ],
        [
            "So now let me do some comparisons.",
            "I'm going to compare a few optimization methods, so the 1st two layer coordinate descent method to solve the dual problem layer actually similar the second one.",
            "Some additional techniques called the shooting, but let's not worry about that here.",
            "Lynn, I also consider the primal coding at the sentiment so.",
            "So that means I solve the original problem of the variable W that I could right now I called primal optimization method.",
            "Solo stream aces layout.",
            "The 1st order method.",
            "Then I have a new to miss it.",
            "That's a special type of Newton method called Trust Region Newton method."
        ],
        [
            "And here I showed a decrease or another.",
            "The difference to the optimal objective value.",
            "So of course they are decreasing.",
            "If you look at least MCV, one data set, so as if you want data set has more than half million instances and around maybe 50,000 features.",
            "So it's very large.",
            "Any how many seconds we can achieve very, very good objectivity.",
            "This is actually 3 seconds.",
            "So as I have said right now for this kind of data, we can train millions of data in just a few a few seconds.",
            "And who is the winner?",
            "Well, in this place.",
            "The blue and the pink lines.",
            "They are actually doable in a decent and the green line is actually the Newton method.",
            "So looks like the code in a decent method is much faster than the 2nd order method, which is the Newton method right now.",
            "Well, that's true for last four datasets, but I may not."
        ],
        [
            "The case for others, actually.",
            "So here I give a quick analysis on existing optimization methods for linear classification in general.",
            "Right now in machine learning community, most people think that first order methods are more useful.",
            "The reason is if you use a first order method then you can very quickly obtain a model and immediately you can use it.",
            "But for quadratic for 2nd order method, the first iteration maybe already may already takes a lot of time, so you need to wait for longer.",
            "However, from an optimization viewpoint, so if you look at the optimization history that people still, I mean in the whole optimization area, 2nd order methods are still very important, and the reason is that if you use methods like Newton or 'cause, I knew that they are much more robust, so there they are.",
            "They can be faster for some conditions, situations.",
            "So for some difficult situations then.",
            "Those kind of coding a decent methods may not work.",
            "So in my opinion both types of optimization methods are still useful for linear classification."
        ],
        [
            "So here is an example.",
            "For this example, the number of instances has only 30,000, but features is about 100.",
            "And if you run loose, if you run this data set and the father figure on the left left hand side, less objective value and right hand side, let's leave the accuracy so you can see that the blue one, the blue one, is right now the dual coding a decent method is not working very well, but.",
            "Contrast, the Newton method is not much better, but this is reasonable because for this data set, if you solve the original problem, then has a number of variables 'cause your variable is WW is associated with the number of features, right?",
            "So you basically have 100 variables, But if you go to solve the dual problem then then you and number of variables variables becomes 30,000.",
            "That's a lot more.",
            "So in this case it should solve the primal problem and and also for this case.",
            "Even if you use primal code in a decent here Now, so these are red line.",
            "But the Newton method is still faster in the end, so this is an example to show that higher order method are still useful."
        ],
        [
            "Then I will move on to talk about some extensions of linear class."
        ],
        [
            "Location.",
            "So so based on our discussion so far, so probably we are convinced that linear classification is useful for document classification.",
            "But can we use this kind of techniques for other situations where the answer is yes?",
            "We can extend it in different ways, and an important one interesting interesting Lee is to approximate kernel classifiers.",
            "So so so you can see what we want is that.",
            "This linear is not enough to give us better accuracy.",
            "Write a kernel can give us better accuracy, so we want better accuracy.",
            "However, we also want faster training and testing of linear classification, so we want both.",
            "So right now how people do a do approximation use linear classification technique to approximate kernel classifiers?",
            "There are basically two types.",
            "But today I only have time to talk about the 1st.",
            "So first idea is very simple.",
            "We just do explicit data.",
            "Maybe these are kind of feature engineering.",
            "Then we do linear classification."
        ],
        [
            "So here is an example.",
            "How about let's just do low degree to low degree polynomial mapping?",
            "This is an example of degree two polynomial mapping.",
            "So instead of doing polynomial kernel we just explicitly write down the whole vector, then it becomes a new training set, right?",
            "Then I don't do anything, just apply a linear classifier and that's it.",
            "That's largely the whole idea.",
            "But then use it as it is work, because then the dimensionality quickly increases.",
            "That is right because now the number of features becomes N squared.",
            "So remember any other number of features.",
            "But now you have any square, but so can this still be used?",
            "From our earlier discussion.",
            "Between linear and kernel we mentioned a difference between older and older Ellen, so Robbie lost the difference right and versus L times an L is the number of training data.",
            "But now you see the situation becomes totally different because this becomes in square.",
            "So you have older N square versus older L times and then user square may not be better, right?",
            "If any is extremely large.",
            "Yes, that is right.",
            "However, there are some situations.",
            "Let these unsquare is still better.",
            "That's for sparse data because for sparse data, listen square shouldn't be unsquare should be embarrassed with what is embarked.",
            "Embarq easily.",
            "Average number of nonzeros per instance.",
            "So you can have some millions of features, but only 10 zero feature values.",
            "So your December is still very very small, so your comparison is not an square versus air.",
            "Time is actually in bars square versus L * N bar.",
            "So as long as L is very large and AMBI is very small, this can still be useful."
        ],
        [
            "So let me give an example here.",
            "For this data set, I have almost 50,000 features.",
            "But I want better accuracy.",
            "So I do agree to make.",
            "But then what is the size of an square well in squares here?",
            "That's one billion sure dimension after degree to make becomes 1 billion.",
            "I have about 200,000 instances.",
            "But Fortunately the number of nonzeros per instance is only certain, so this is it isn't is an extremely sparse data.",
            "So that's why we can still do degree two mapping and then do the training.",
            "But but as I have mentioned that the the dimensionality of W becomes this one billion.",
            "So, so you do have some technical difficulties to handle this vector W. Probably you are not able to store the whole vector W. So we actually need some additional techniques, But those can be done."
        ],
        [
            "Now let's just look at the result.",
            "Again, this is a comparison between using our kernel methods and linear classifiers.",
            "For the first part I apply labels via, which is a kernel classifier and I separately use a Gaussian kernel or RBF kernel and the degree two.",
            "So here this this color means degree two polynomial kernel.",
            "And for the linear classification, I apply both linear nicely original one and I do degree two polynomial mapping and I generate explicitly generate the feature vector.",
            "Now let's check your training time.",
            "If you use kernel, the training time is about 3 hours.",
            "But by this way, by using the linear, then if you use linear last three minutes and if we use a polynomial, then it is still 3 minutes.",
            "But you may say that polynomial training time should still should be slightly more than linear.",
            "Well, that is right, because here I have done parameter selections.",
            "So there are some issues related to parameters.",
            "But anyway, you can see that.",
            "If you just compare the the two colors of using degree two polynomial, they're exactly the same thing.",
            "They're the same thing.",
            "The differences like one use degree, two polynomial, another is expanded.",
            "The feature vector then apply a linear classifier and you see a 60 * 64 decrease of the training time.",
            "So this is very good.",
            "Then about the testing speed.",
            "So this is, well, this application is for dependency parsing, so the parsing speed.",
            "If you consider the passing speed of using degree two polynomial as one unit, learn by using linear.",
            "This is 1600 times faster and if we use our least explicit mapping of degree two polynomial then you get 100 times faster.",
            "The 3rd and the 4th rose.",
            "They are actually a kind of testing accuracy.",
            "This is a dependency parsing lesson, natural language processing application, their evaluation criteria.",
            "USMLE is but you can consider them as a kind of test accuracy.",
            "So you can see that.",
            "If you only use linear.",
            "Ladies only 89 but if you use degree 2 then you can get 91 but if you are using kernel then you need 3 hours but by using this explicit mapping then you can you simultaneously get faced training faced testing and also higher test accuracy.",
            "So that's this is a good example and this is actually a real application so this is real application at Google."
        ],
        [
            "Now let's move on to another example.",
            "And here I'm going to deploy classifiers in a very small device.",
            "Any small devices so small that we are allowed to use only 16 kilobytes of red?",
            "So your model cannot be more than 16 K bytes if we apply regular classifiers like decision tree, Adaboost, support vector machines with RBF kernel.",
            "Lyric curacy are reasonable, but model sites are much larger than 16 K, right?",
            "So if you said a boost entries plus 1500 K, well that's too large for us.",
            "Then we thought about this and I said OK, how about using linear using linear but some kind of degree polynomial mappings for this data set?",
            "The number of features is very small, only 5 features.",
            "When we consider a degree three polynomial mapping Hausler dimensionality after degrees, three polynomial normal mapping is actually this 5 + 3 choose three.",
            "And for this case, we actually have a another bias term.",
            "So after this calculation the total number of features is actually 57.",
            "So we generate a new training set of 57 features."
        ],
        [
            "Len, what's the size?",
            "The size is actually.",
            "If you do linear classification, then you all you need is that model vector W, right?",
            "So you are supposed to have 57 * 4 bytes.",
            "Assuming we are doing single precision, so each float consumes 4 bytes, so that's only 57 * 4, But this is a multiclass problem of five classes, and here we are using the so called one against once multiclass strategy.",
            "So we need to find truth to US 10 models.",
            "So we need 10 W vectors, so after this calculation we all we need is 2.2 two K space so this."
        ],
        [
            "Satisfies our requirement of deploying the model into a small device of less than 16 K bytes of RAM."
        ],
        [
            "And how about the test accuracy?",
            "So here is a comparison.",
            "So if we use a copy of Kernel Linda, test accuracy is quite high 85% this is good, but because of using kernel, so from the previous table, Now we need almost 1300 K bytes for the model.",
            "But if you use linear well leader is the problem linear?",
            "The size is very small, we only need a point to care for the model.",
            "However, this is an example.",
            "Left linear doesn't work well.",
            "It doesn't give us good accuracy, only 78%.",
            "Now we're doing something in Liberty.",
            "We do agree St polynomial mapping.",
            "But of course, when you say it's somewhere in between, you don't know where it whether it is closer to the RBF kernel site or closer to the linear side, but Fortunately for this case it's closer to the RBF one.",
            "You can see that the test accuracy is very close to you, too large of using a highly nonlinear kernel like RBF, but now our model size becomes very very small, so this is a successful example of using such a technique."
        ],
        [
            "In this slide.",
            "OK so.",
            "So these slides actually talks about training time.",
            "Talk about talks about training time.",
            "As expected, if you are using a.",
            "A kernel classifier for polynomial for polynomial kernel.",
            "The training time is very long is very long.",
            "But the interesting thing is here, because we just mentioned that there are different types of optimization methods, so you can use a second order or first order.",
            "Are hereby primal actually being a Newton method to solve the to use the 2nd order method and this deal actually is?",
            "Dual coding at decent, so that's first order.",
            "And this is another example where the training time of using the 2nd order method on the primal problem, I mean the original optimization problem rather than the deal is faced.",
            "Yeah, so you can see that this one is fixed."
        ],
        [
            "So you see I have shown two successful examples, but but you can also quickly criticize that this technique may not be that general.",
            "So the problem is that if you do polynomial mappings very quickly, you get very high dimensional vectors, right?",
            "Especially if originally you are number of features is not very small.",
            "So quickly you get very high dimensionality.",
            "But people have sold about this issue, so they have proposed some kind of projection techniques, so some kind of hashing techniques or a random projection techniques to handle this problem.",
            "So the idea is that you do mapping first.",
            "Belen somehow you you project them?",
            "Speak to a full lower dimensional space.",
            "Then you can apply a linear classifier to train such a data set.",
            "And for the second example, second application, one comment.",
            "I want to say here is."
        ],
        [
            "So remember for this state for this application at a time, not only SVM or logistic regression, I also consider tree based models like decision Tree and Ada Boost.",
            "Of course there are some techniques to also reduce the model size of trees.",
            "But seems it's not that easy to control the size of the tree size, but in contrast, for linear the linear is so simple that you can easily.",
            "Ensure that your how large your model size should be, so that's an example this so far I'm still thinking about how to still use tree based model for the same application, but so far I haven't found out a good way, but I did find a way of using linear classifier to do this visa application."
        ],
        [
            "Then let's have some, uh, discuss."
        ],
        [
            "And conclusions, so I want to talk about a big data linear classification.",
            "So from our discussion so far, we talk, so we talk about the millions of data.",
            "Well, sometimes millions are already large scale.",
            "That is right.",
            "Now when we talk about big data with me, like billions of data instances and a special thing is that they in general they can only be stored in a distributed environment.",
            "So.",
            "We must we must recognize that in a shared in the distributed environments then situations can become very different.",
            "So, for example, let optimization methods that we have just discussed may not be directly applicable in a distributed environment, you need maybe a lot of modifications.",
            "Half.",
            "But one important reason to to do distributed data classification is that you can save the data loading time.",
            "He's right now, if you, even if you have a huge computer, we say 1 terabytes of red.",
            "But 2 two to load 1 terabytes of data from your disk to the Rep. That takes a lot of time.",
            "But in a distributed environment, if you can do parallel data loading, Leno saves a lot of loading time.",
            "So loading time is that becomes an advantage you save loading time.",
            "However, fully optimization problem, the communication becomes a problem.",
            "Especially in a distributed environment, communication cost is very very very high.",
            "So right now we are experimenting with the Newton method that we discussed discussed in a distributed environment and we find out that sometimes because machines in a data center are not very stable, so the communication cost can.",
            "Can fluctuate significantly, so that's an issue."
        ],
        [
            "In another interesting thing, if you are interested in this kind of big data classification, is that you need to notice that classification is usually only one component of the whole big data application.",
            "So in one application that we are working on, so in the beginning we say, OK, I'm a researcher on data classification.",
            "I want to do research on distributed data classification.",
            "But after we work on that for awhile we find out that the distributed feature generation for that application takes a lot more time than training.",
            "So this this shows that in a big data problem, then classification is usually only one component of the whole workflow.",
            "So now I think a serious problem is that.",
            "In most big data environment, this workflow is not very smooth yet, so it's very difficult for us to do experiments.",
            "That's that's a problem.",
            "So that also explains why so far we don't have.",
            "We don't see many existing package for available for big data classification, but certainly that means this is still an interesting research area that haven't been.",
            "Bing research to match."
        ],
        [
            "So now let's go to the conclusion.",
            "Of course, we all know that linear classification is a very, very old topic by recently, so one reason is because of the document classification from Internet companies.",
            "Then this becomes a.",
            "The recently learned a lot of new and interesting applications, but even though this talk is about linear, I am not saying kernel is not useful now.",
            "Currently still useful, but but they they should just be used in different situations.",
            "So so so in in my talk I what I have demonstrated is that if you use linear plus feature engineering, then this approach is useful for certain problems.",
            "And we know the advantage of linear is because you work on X on the data X rather than the map vector.",
            "Then it's easier for feature engineering and also optimization.",
            "So we expect that this linear classification can be widely used in.",
            "In posting in situations ranging from, like for example I mentioned at the small devices to big data situations.",
            "OK, so this is the end of my talk.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So it's my pleasure to be here to give a talk at HTML.",
                    "label": 1
                },
                {
                    "sent": "So I I still remember that two years ago in the entire one we had a CMA at that time and I was a general chair.",
                    "label": 0
                },
                {
                    "sent": "So we're very.",
                    "label": 0
                },
                {
                    "sent": "We always hope to support this conference and we hope that it will be.",
                    "label": 1
                },
                {
                    "sent": "Yeah, keep improving to be a major machine learning conference.",
                    "label": 1
                },
                {
                    "sent": "So now in this talk I will talk about some of my recent work large scale linear classification.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, this talk is based on my survey paper, so last year in Proceedings of IEEE I have a paper called a recent advances of large scale linear classification.",
                    "label": 1
                },
                {
                    "sent": "So all the details can be found there and also it's related to.",
                    "label": 1
                },
                {
                    "sent": "Our development of a software package called the League linear.",
                    "label": 0
                },
                {
                    "sent": "So this morning in Jeffs talk, so he mentioned how to encourage people to develop machine learning software.",
                    "label": 0
                },
                {
                    "sent": "So he mentioned in Journal of Machine learning research, there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a software section to encourage people to submit their software paper layer, which here is 1 example.",
                    "label": 0
                },
                {
                    "sent": "Lib Linear Paper was published in Journal of Machine Learning Research in 2008.",
                    "label": 0
                },
                {
                    "sent": "That's one of the earliest.",
                    "label": 0
                },
                {
                    "sent": "Paper in light software section, and that's actually a successful example.",
                    "label": 0
                },
                {
                    "sent": "I think that paper has been the maybe the most cited one of all the Journal of Machine learning research Paper seems maybe 2006, so I think that's an example that things that are Journal has an open source section, then actually that encourages a lot more people to develop useful software for for the Community.",
                    "label": 1
                },
                {
                    "sent": "And there are a lot of materials about the recent developments, so due to the time constraints I will mainly give some overviews instead of getting into deep technical details.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the outline of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will give an introduction about large scale linear classification and learn.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about optimization methods to solve to obtain model and then also talk about some extensions of linear classification.",
                    "label": 0
                },
                {
                    "sent": "Then have some discussion and conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how's the difference between linear and nonlinear classification?",
                    "label": 1
                },
                {
                    "sent": "Will this figure says the difference in for the linear situation?",
                    "label": 1
                },
                {
                    "sent": "We basically draw a straight line to separate data.",
                    "label": 0
                },
                {
                    "sent": "But for nonlinear we use a nonlinear curve.",
                    "label": 0
                },
                {
                    "sent": "But there are many nonlinear methods.",
                    "label": 0
                },
                {
                    "sent": "So for example, decision trees, random forest.",
                    "label": 0
                },
                {
                    "sent": "They're all nonlinear methods, but here are when I say nonlinear actually mean kernel, so here.",
                    "label": 1
                },
                {
                    "sent": "When I say linear, that means we use data in the original space.",
                    "label": 0
                },
                {
                    "sent": "So here for the original feature vector, you have two features.",
                    "label": 0
                },
                {
                    "sent": "They are height and weight.",
                    "label": 0
                },
                {
                    "sent": "But if you do some mapping so you generated and you are new feature called Await Divided by the square of height, that's a new feature.",
                    "label": 1
                },
                {
                    "sent": "Then this is a kind of nonlinear mapping, and as I have said, there are many nonlinear methods.",
                    "label": 0
                },
                {
                    "sent": "But kernel method is one of the one of the popular nonlinear methods.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for for linear and kernel basically we can use slamming two ways.",
                    "label": 1
                },
                {
                    "sent": "So when one representative kernel methods are support vector machines and logistic regression, so they usually can be used in practice in two ways.",
                    "label": 1
                },
                {
                    "sent": "So one is by Colonel.",
                    "label": 0
                },
                {
                    "sent": "So that means you map data to a higher dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So here you map data X2 to another vector code file of X.",
                    "label": 0
                },
                {
                    "sent": "But because this file.",
                    "label": 0
                },
                {
                    "sent": "5X may be very high dimensional, therefore it is very difficult to manipulate such feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Such vectors in high dimensional space, so we need to use kernel tricks for kernels.",
                    "label": 0
                },
                {
                    "sent": "Mapping functions are special so that the inner product can be easily calculated.",
                    "label": 0
                },
                {
                    "sent": "That's the kernel trick, so that's one way to use SVM or logistic regression.",
                    "label": 0
                },
                {
                    "sent": "But another disadvantage disadvantage of using a kernel is let you don't have a good control on your mapping function, because you won't let the inner product be easy, so you don't.",
                    "label": 0
                },
                {
                    "sent": "You lose a good control control on your mapping function, but another popular way of using SVM or logistic regression is.",
                    "label": 1
                },
                {
                    "sent": "To do feature engineering plus linear classification.",
                    "label": 0
                },
                {
                    "sent": "So feature engineering.",
                    "label": 0
                },
                {
                    "sent": "That means are you using your domain knowledge?",
                    "label": 0
                },
                {
                    "sent": "You find out good features and once you have good features you don't make your data to a higher dimensional space or to a different space you stay, you stay in the original input space.",
                    "label": 0
                },
                {
                    "sent": "That's what I call linear classification.",
                    "label": 0
                },
                {
                    "sent": "So in some sense you can say that this is you already done the nonlinear mapping because in your feature engineering you tried all kinds of combinations of features and to try to see which one is better.",
                    "label": 1
                },
                {
                    "sent": "That's kind of nonlinear process, but once that is done then you have a linear classification problem.",
                    "label": 0
                },
                {
                    "sent": "And in this talk I will focus on the second approach.",
                    "label": 0
                },
                {
                    "sent": "Well, today we're not going to talk about kernel, so we want to talk about the second case right.",
                    "label": 0
                },
                {
                    "sent": "Once you have done feature engineering, then how can you do large scale linear classification?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is a quick question, that's why we want to do linear classification then.",
                    "label": 1
                },
                {
                    "sent": "Well then there are some reasons.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So here I want to convince you that in some situations we should do linear classification.",
                    "label": 0
                },
                {
                    "sent": "So if you map data to a high dimensional space, then because this data becomes so long then actually decision function becomes difficult to calculate.",
                    "label": 1
                },
                {
                    "sent": "So the decision function once you have mapped test data X.",
                    "label": 0
                },
                {
                    "sent": "Too high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Then you do the inner product with the model and to check the sign of this value.",
                    "label": 0
                },
                {
                    "sent": "Then this calculation can be expensive.",
                    "label": 0
                },
                {
                    "sent": "So kernel methods.",
                    "label": 0
                },
                {
                    "sent": "They actually use the kernel tricks to let the model W to be the linear combination of all the map vectors then so so you can see that here this model W is actually a linear combination linear combination of all the map vectors with coefficients Alpha.",
                    "label": 0
                },
                {
                    "sent": "Then we use the technique that the kernel element is the inner product between two map vectors.",
                    "label": 1
                },
                {
                    "sent": "Then this decision function becomes this submission.",
                    "label": 0
                },
                {
                    "sent": "OK, so we just put this.",
                    "label": 0
                },
                {
                    "sent": "This formulation of W into the decision function and we get this submission.",
                    "label": 0
                },
                {
                    "sent": "So so for this.",
                    "label": 0
                },
                {
                    "sent": "So for this I mentioned, as long as you know how to calculate kernels, then you can calculate the summation then.",
                    "label": 0
                },
                {
                    "sent": "Then you can do the prediction and that's the way how kernel works.",
                    "label": 0
                },
                {
                    "sent": "So so I have mentioned that in order to easily calculate the inner product, your mapping function must must be special.",
                    "label": 0
                },
                {
                    "sent": "So here is an example.",
                    "label": 0
                },
                {
                    "sent": "This is the so-called degree two polynomial kernel.",
                    "label": 0
                },
                {
                    "sent": "So, so this degree to function.",
                    "label": 0
                },
                {
                    "sent": "So we see degree to here is actually the inner product between two vectors and each vector each vector is already in North squared dimensional space and here means the number of features.",
                    "label": 0
                },
                {
                    "sent": "So so from the front of vector of N features you have mapped data to end square dimensional space.",
                    "label": 0
                },
                {
                    "sent": "However you don't need to do inner product in an square dimensional space, you still do dinner.",
                    "label": 0
                },
                {
                    "sent": "Product in the original space, then do the square OK, so that's the key.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, Colonel is still expensive, right?",
                    "label": 1
                },
                {
                    "sent": "So if you if you compare doing the original linear prediction and the kernel situation so in the linear case you only need to do one inner product between the test vector and the model vector W. But for kernel you need to do the summation.",
                    "label": 0
                },
                {
                    "sent": "So if any, is there a number of features in the linear case you need order an operations to do just want to predict one instance.",
                    "label": 0
                },
                {
                    "sent": "But if you do kernel because of this submission, then you need older NL&L is the number of training data.",
                    "label": 0
                },
                {
                    "sent": "Of course if you use support vector machines you get sparsity, but still you need less proportional to the number of support vectors.",
                    "label": 0
                },
                {
                    "sent": "Yeah so basically here I argue that will kernel is good.",
                    "label": 1
                },
                {
                    "sent": "It is more powerful to separate.",
                    "label": 1
                },
                {
                    "sent": "Date, however linear, has an advantage that it is cheaper and simpler.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But cheaper and simpler is not good, right?",
                    "label": 0
                },
                {
                    "sent": "We also want good accuracy.",
                    "label": 0
                },
                {
                    "sent": "So the interesting thing is that for certain problems, the test accuracy by a linear classifier is already as good as the kernel once.",
                    "label": 1
                },
                {
                    "sent": "But then by using only linear classifiers, your training and testing can be much faster.",
                    "label": 0
                },
                {
                    "sent": "Well then this is good.",
                    "label": 1
                },
                {
                    "sent": "You get simultaneously good accuracy, but as well as faster training and testing speed so this.",
                    "label": 0
                },
                {
                    "sent": "Especially happens for document classification.",
                    "label": 0
                },
                {
                    "sent": "So right now the the most common ways to do document classification is to generate the feature vectors by the so called bag of words model.",
                    "label": 0
                },
                {
                    "sent": "That means each each word corresponds to a feature.",
                    "label": 0
                },
                {
                    "sent": "So if you have say 2 million English awards and you have 2 million features and you get a very very sparse feature vectors, because each document probably has several 100 words right, only several 100 features will have nonzero values.",
                    "label": 0
                },
                {
                    "sent": "And all remaining feature values become zero.",
                    "label": 0
                },
                {
                    "sent": "There are large and sparse data and for such data now we have already observed that if you do a linear classification then the accuracy is almost the same as the kernel classifier, but but by using only linear classification Now, you can easily trim millions of data points in just a few seconds.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is an example to compare linear and kernel classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I tried to compare training time as well as test testing accuracy.",
                    "label": 1
                },
                {
                    "sent": "For the first 2 columns, that's the result by using a linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "So time means training time and accuracy means test accuracy, and for the for the other two columns they are results of using RBF or say, Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "Again, timing means training time and accuracy means test accuracy.",
                    "label": 0
                },
                {
                    "sent": "From this comparison.",
                    "label": 1
                },
                {
                    "sent": "So if we look at this data set, this data set has around half million training instances.",
                    "label": 0
                },
                {
                    "sent": "If you use linear classifier then the training time is very fast.",
                    "label": 0
                },
                {
                    "sent": "You only need 1.4 seconds, but your accuracy is only 76%.",
                    "label": 0
                },
                {
                    "sent": "Well this is not very good, but if you spend more time by using kernel method.",
                    "label": 0
                },
                {
                    "sent": "So here you need maybe around Health Day but but then your test accuracy is dramatically improved to 96%.",
                    "label": 0
                },
                {
                    "sent": "So this is a good case that you may still want to use kernel even though you need a lot more training type.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "If you look at last three datasets from the name, you can tell that they are documents like News 20, Yahoo Japan.",
                    "label": 0
                },
                {
                    "sent": "Acument data.",
                    "label": 0
                },
                {
                    "sent": "Say for news 20 in 1.1 second we achieve 96% test accuracy, but if you use a kernel methods.",
                    "label": 0
                },
                {
                    "sent": "Then you need a almost 400 seconds, but your accuracy is actually almost the same in the same situation.",
                    "label": 0
                },
                {
                    "sent": "Situation happens for the other two document datasets and the notice read Alouds datasets are pretty large.",
                    "label": 0
                },
                {
                    "sent": "So for example for the last one at Yahoo Japan, we actually had a 140,000 instances and.",
                    "label": 0
                },
                {
                    "sent": "830,000 features, so this is pretty large, but if you look at the training time, this is 3 seconds, so this is very, very good, so that gives us a reason that for certain problems large scale linear classification is very useful, so we should try to develop something so people can use to use linear classification for such applications.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's get into a more technical details.",
                    "label": 0
                },
                {
                    "sent": "I want to formally define what a binary classification problem is.",
                    "label": 0
                },
                {
                    "sent": "I assume that we are given a bunch of training labels and features, so why I and XI?",
                    "label": 0
                },
                {
                    "sent": "That's a lots of pair of label and the feature vector.",
                    "label": 0
                },
                {
                    "sent": "So why is it is plus or minus one to indicate the label of the training data and XI is A is a feature vector in N dimensional space we are using to denote the number of features and I use L to denote the number of training data, the standard.",
                    "label": 1
                },
                {
                    "sent": "Formulation to train a linear classifier is to solve an optimization problem, at least in the objective function of this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It contains 2 parts.",
                    "label": 1
                },
                {
                    "sent": "The first part this W transpose W / 2 is the so called regularization term.",
                    "label": 0
                },
                {
                    "sent": "The reason is to avoid overfitting.",
                    "label": 0
                },
                {
                    "sent": "Then the second term is this submission of loss values.",
                    "label": 0
                },
                {
                    "sent": "So we use a different kinds of loss functions.",
                    "label": 0
                },
                {
                    "sent": "And they do this I mentioned, so that indicates the kind of training error.",
                    "label": 1
                },
                {
                    "sent": "So we want to balance between regularization and training errors.",
                    "label": 0
                },
                {
                    "sent": "So there's a parameter C which is, which is the so called regularization parameter that must be decided by users.",
                    "label": 0
                },
                {
                    "sent": "So this is a standard formulation of a linear classifier.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are different types of loss functions, so here I give us 3 examples.",
                    "label": 0
                },
                {
                    "sent": "Another way to design.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functions is that are using.",
                    "label": 0
                },
                {
                    "sent": "In doing the training, you hope that your model W. Can can be generated such that for this data X, if the label is why then you are W transpose?",
                    "label": 0
                },
                {
                    "sent": "X should have the same sign as Y, right?",
                    "label": 0
                },
                {
                    "sent": "Because if this data is positive, you are hoping that your model W can give you the value W transpose X to be also positive.",
                    "label": 0
                },
                {
                    "sent": "So you hope that this YW transpose X is positive.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So from that principle we can design our loss function.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the first one let's the so called hinge loss or sometimes called L1 loss, we do the mix between 0 and 1 -- y W transpose X.",
                    "label": 0
                },
                {
                    "sent": "So if if your data is positive and W transpose X is negative, that means YW transpose X becomes negative.",
                    "label": 0
                },
                {
                    "sent": "Then 1 -- A negative value.",
                    "label": 0
                },
                {
                    "sent": "Becomes positive, then we shouldn't mix with zero.",
                    "label": 0
                },
                {
                    "sent": "Then you get a positive value that indicates a kind of training error.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, if you're Y&W transpose X, they are both positive.",
                    "label": 0
                },
                {
                    "sent": "Then that means 1 -- a positive value may become negative right then after doing the Max with zero you get value zero.",
                    "label": 0
                },
                {
                    "sent": "That indicates that there is no training error.",
                    "label": 0
                },
                {
                    "sent": "And for the second, let's call the squared hinge loss.",
                    "label": 0
                },
                {
                    "sent": "So actually the 1st and the second other commonly used form of support vector machines.",
                    "label": 1
                },
                {
                    "sent": "Then for the third one logistic loss.",
                    "label": 0
                },
                {
                    "sent": "So if you use that, you get the regularised logistic regression.",
                    "label": 0
                },
                {
                    "sent": "You can see that here I can give references for support vector Machine, but I cannot do that for logistic regression, because this technique can actually be traced back to maybe.",
                    "label": 1
                },
                {
                    "sent": "19th century.",
                    "label": 1
                },
                {
                    "sent": "Then you may ask the differences between those three loss functions.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turns out that they are very similar, so if we draw the figures of the three loss functions, you can see things like this, so they're very similar on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "That means if there is no training error, then the loss function should have the value close to 0.",
                    "label": 0
                },
                {
                    "sent": "But then of course for if you have training error then the way they give penalty to the training error are slightly different.",
                    "label": 0
                },
                {
                    "sent": "So if you have least squared hinge loss and the value becomes larger.",
                    "label": 0
                },
                {
                    "sent": "If you have a large error but but usually OK using the the way how they are designed are similar.",
                    "label": 0
                },
                {
                    "sent": "So usually the performance between support vector machine and the logistic regression.",
                    "label": 0
                },
                {
                    "sent": "They are actually similar.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we need to to minimize the optimize it to solve the optimization problem in order to get the model.",
                    "label": 0
                },
                {
                    "sent": "But I but, but we we notice that optimization methods for them.",
                    "label": 1
                },
                {
                    "sent": "Maybe different users due to the different properties of the loss functions.",
                    "label": 0
                },
                {
                    "sent": "If you are using hinge loss, then the problem.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is that it is not differentiable at least point, so you cannot use a differentiable optimization technique.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And therefore squared hinge loss is differentiable but not twice differentiable, and therefore logistic regression is twice differentiable is easier to apply optimization techniques?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we come to the second part is how to solve the optimization problems.",
                    "label": 0
                },
                {
                    "sent": "But there are many available optimization techniques.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking, you can use high order methods like Newton method or Cosine Newton methods, but you can also consider low older methods such as coordinated decent gradient, decent or stochastic gradient, dissent or such things.",
                    "label": 0
                },
                {
                    "sent": "But I will I will here I will discuss.",
                    "label": 0
                },
                {
                    "sent": "Representative methods.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here I consider a second order method.",
                    "label": 0
                },
                {
                    "sent": "That's the Newton method.",
                    "label": 0
                },
                {
                    "sent": "So how's your basic idea of Newton method?",
                    "label": 0
                },
                {
                    "sent": "We do the 2nd order approximation of the function, so here this F of W means the function I'm going to minimize the 2nd order Taylor expansion of the function F becomes this problem.",
                    "label": 0
                },
                {
                    "sent": "So if we do a minimization over the direction S, so this is the 2nd order approximation.",
                    "label": 0
                },
                {
                    "sent": "If we can find out that direction, is that sort of so called Newton direction, then from the current current iterate code WK?",
                    "label": 0
                },
                {
                    "sent": "So Casey iteration index so from WKI take the direction as.",
                    "label": 0
                },
                {
                    "sent": "Then I get to the next iterate.",
                    "label": 0
                },
                {
                    "sent": "So so Newton method isn't iterative procedure and at each iteration I need to find this so called Newton direction.",
                    "label": 0
                },
                {
                    "sent": "And therefore this minimize this quadratic minimization problem is equivalent to solving the so-called Newton linear system.",
                    "label": 0
                },
                {
                    "sent": "So this is the so called Hessian matrix where H matrix is likely.",
                    "label": 0
                },
                {
                    "sent": "Second derivative of the function.",
                    "label": 0
                },
                {
                    "sent": "But because you have multiple variables, so Now this Haitian is a matrix.",
                    "label": 0
                },
                {
                    "sent": "So this this Haitian is a matrix and we need to solve the linear system with matrix here and the variable X and this is minus of the gradient of the function F. But so far this is standard Newton massive.",
                    "label": 0
                },
                {
                    "sent": "Well, there's nothing special to data classification, but if you would like to directly apply Newton method to do large scale linear classification will we will see some big troubles.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that this Haitian matrix can be too large to be stored.",
                    "label": 0
                },
                {
                    "sent": "What is the size of this Hessian matrix?",
                    "label": 0
                },
                {
                    "sent": "Well, the size of H matrix is actually the same as the number of variables times the number of variables.",
                    "label": 0
                },
                {
                    "sent": "But what is the number of variables now with W?",
                    "label": 0
                },
                {
                    "sent": "Is our variable right?",
                    "label": 0
                },
                {
                    "sent": "The size of W is the same as number of features.",
                    "label": 1
                },
                {
                    "sent": "So how many features you have?",
                    "label": 0
                },
                {
                    "sent": "Then you have how many variables, but this can be very large, right?",
                    "label": 0
                },
                {
                    "sent": "If you have 1,000,000 features and then this H matrix becomes 1,000,000 by 1 million.",
                    "label": 0
                },
                {
                    "sent": "Usually this is too large to be stored.",
                    "label": 1
                },
                {
                    "sent": "In your computer.",
                    "label": 1
                },
                {
                    "sent": "So we need something special.",
                    "label": 0
                },
                {
                    "sent": "Then if you do some calculation about SVM, linear SVM or logistic regression then you will find out that the Hessian matrix actually has a special form.",
                    "label": 0
                },
                {
                    "sent": "So I take it back because if we want to do second derivative then this must be logistic regression because only it is twice differentiable.",
                    "label": 0
                },
                {
                    "sent": "So for logistic regression the Hessian matrix actually has this special form.",
                    "label": 0
                },
                {
                    "sent": "Well, the first I is an identity matrix then plus this C is the regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "Then here's a special product between three Matrix.",
                    "label": 0
                },
                {
                    "sent": "The first is X transpose and lengthy an X.",
                    "label": 0
                },
                {
                    "sent": "So how badly?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will access basically is simply the data matrix.",
                    "label": 1
                },
                {
                    "sent": "So just gather all the data together as one big matrix that's X and then these are diagonal matrix for logistic regression.",
                    "label": 1
                },
                {
                    "sent": "This diagonal matrix actually takes this form.",
                    "label": 0
                },
                {
                    "sent": "So the ice component actually is exponential.",
                    "label": 0
                },
                {
                    "sent": "Something over this.",
                    "label": 0
                },
                {
                    "sent": "So once we know this special form.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we can use a technique to solve this Newton linear system without actually forming this Haitian matrix.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that in solving a linear system, usually there are two types of methods.",
                    "label": 0
                },
                {
                    "sent": "One is called direct method, so if you do things like Gaussian elimination and you need to store the whole matrix, in general need to store the whole matrix before doing Gaussian elimination.",
                    "label": 0
                },
                {
                    "sent": "But Alternatively you can use the so called iterative methods to solve linear system.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are many kinds of iterative methods for linear systems, but one way is the so called country gradient method.",
                    "label": 0
                },
                {
                    "sent": "In the special thing about country gradient is that each country gradient iteration, all you need is to do a matrix vector product.",
                    "label": 0
                },
                {
                    "sent": "So here the matrix is this Haitian matrix and now you want to multiply this Haitian with a vector.",
                    "label": 0
                },
                {
                    "sent": "But we can do that without forming the whole matrix.",
                    "label": 0
                },
                {
                    "sent": "So recall that our matrix is this X * D * X.",
                    "label": 0
                },
                {
                    "sent": "But now we never need to multiply them out to form a huge matrix.",
                    "label": 0
                },
                {
                    "sent": "Now we never need to do that because we have sequentially doula matrix vector product and still get the value.",
                    "label": 0
                },
                {
                    "sent": "OK, so we do X * S first.",
                    "label": 0
                },
                {
                    "sent": "Then we we multiply with this diagonal matrix D and then we multiply with this X transpose.",
                    "label": 0
                },
                {
                    "sent": "Then we don't have the storage problem, so this is our difficulties to store that huge Haitian matrix.",
                    "label": 0
                },
                {
                    "sent": "But now we don't worry about that.",
                    "label": 0
                },
                {
                    "sent": "All you need is to store X, but of course you must be able to store X.",
                    "label": 0
                },
                {
                    "sent": "That's the data matrix.",
                    "label": 0
                },
                {
                    "sent": "It must be available.",
                    "label": 0
                },
                {
                    "sent": "So in optimization this is the so-called hashing free technique.",
                    "label": 0
                },
                {
                    "sent": "So you can see that this is a good combination between using optimization knowledge and also the machine learning properties, because only because.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Hessian matrix has such a special form, then we can apply this kind of.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patient free approach to use to apply Newton method for for large scale classification.",
                    "label": 0
                },
                {
                    "sent": "So this is an interesting example.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me skip this.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Select now I will switch to another type of optimization methods where it's different from Newton, but Newton is a second order that use the 2nd order derivative.",
                    "label": 1
                },
                {
                    "sent": "But now I'm going to use only the 1st order and that's the gradient.",
                    "label": 1
                },
                {
                    "sent": "So now let's say the 1st order makes it, but instead of considering that optimization problem of variable W in.",
                    "label": 0
                },
                {
                    "sent": "Optimization we can derive the so-called dual problem.",
                    "label": 0
                },
                {
                    "sent": "So for both SVM and logistic regression you can derive the dual problem.",
                    "label": 1
                },
                {
                    "sent": "So here I consider this example of using a one loss.",
                    "label": 0
                },
                {
                    "sent": "For support vector machine, if you use these hinge loss, let go of the dual problem actually takes this form.",
                    "label": 0
                },
                {
                    "sent": "So now I have a function of Alpha and this Alpha the size becomes the same as the number of instances.",
                    "label": 1
                },
                {
                    "sent": "So this is different from www.isthesize is the same as number of features.",
                    "label": 0
                },
                {
                    "sent": "But now this is the same as the number of instances.",
                    "label": 0
                },
                {
                    "sent": "This function takes a following form that is the objective function to be minimized is a quadratic function, so variable is Alpha now and the quadratic term is after transpose matrix Q.",
                    "label": 0
                },
                {
                    "sent": "In the time zone Alpha, in the minus linear attendee chance was after this easily vector of all ones.",
                    "label": 0
                },
                {
                    "sent": "So where are the data?",
                    "label": 0
                },
                {
                    "sent": "Will data embedded into this matrix Q?",
                    "label": 0
                },
                {
                    "sent": "So the IJ component is actually the inner product between two training vectors.",
                    "label": 0
                },
                {
                    "sent": "Well, actually if you are familiar with kernel methods, this is actually the basically the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, so now this is the objective function, but there are also a bunch of constraints.",
                    "label": 0
                },
                {
                    "sent": "Let each other.",
                    "label": 0
                },
                {
                    "sent": "I must be between zero and C. And remember, let's see is the regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "So now we want to minimize this.",
                    "label": 0
                },
                {
                    "sent": "So how to?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But where we consider very very simple in the classical optimization technique called coding and decent.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that now you have so many variables to minimize, but we don't know how to do that.",
                    "label": 0
                },
                {
                    "sent": "So just fix all of that except what so you try to fix all variables, try to change one of them.",
                    "label": 0
                },
                {
                    "sent": "So now you change one variable at a time.",
                    "label": 1
                },
                {
                    "sent": "So here I define some notation.",
                    "label": 0
                },
                {
                    "sent": "Suppose currently we are at the vector Alpha, so advise where we have right now.",
                    "label": 0
                },
                {
                    "sent": "Then I define a victory.",
                    "label": 0
                },
                {
                    "sent": "I wish the ice component to be one and all the remaining elements to be 0.",
                    "label": 0
                },
                {
                    "sent": "Then I want to check that little ice component.",
                    "label": 0
                },
                {
                    "sent": "How far can I go so from a for Alpha I I tried to check that plus this scalar D times this unit vector EI.",
                    "label": 0
                },
                {
                    "sent": "OK then to check the hostel change of D. That I can use so I get the one variable optimization problem because I all I need is to decide the amount of the in order to decide how I update update after I.",
                    "label": 0
                },
                {
                    "sent": "And after some calculation, then turns out that this is a very simple one, variable quadratic optimization problem and at least one of course has a closed form.",
                    "label": 0
                },
                {
                    "sent": "So we end up with the quadratic term becomes this Qi times the square, then the linear term we have the ice component of the gradient times DN plus some constants.",
                    "label": 0
                },
                {
                    "sent": "They're not important.",
                    "label": 0
                },
                {
                    "sent": "So now if we don't consider constraints in the in the in the dual problem, then this this one variable quadratic function immediately you know that the optimal optimal solution should be this.",
                    "label": 0
                },
                {
                    "sent": "So this is from the linear term and the divided by the leading quadratic term, right?",
                    "label": 0
                },
                {
                    "sent": "So you know the optimize this, but now remain.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I believe that the dual problem has a constraint that I must be between zero and see therefore.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One way we can do is that once we have find out this optimal D, if I plus D exceeds the range of zero and see we just project it back.",
                    "label": 0
                },
                {
                    "sent": "That's why you see that these minima and makes operation OK. Basically it's beyond the ceiling.",
                    "label": 0
                },
                {
                    "sent": "We project it back to see.",
                    "label": 0
                },
                {
                    "sent": "So this is very simple.",
                    "label": 0
                },
                {
                    "sent": "Now we know how to update the ice component of I.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's hope Mr whole procedure procedure in this procedure is actually is actually very can be done in a very efficient way.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's see.",
                    "label": 0
                },
                {
                    "sent": "Where are you?",
                    "label": 0
                },
                {
                    "sent": "Where is the main computational bottleneck?",
                    "label": 0
                },
                {
                    "sent": "If you look at least update.",
                    "label": 0
                },
                {
                    "sent": "It is very simple, right?",
                    "label": 0
                },
                {
                    "sent": "So where is the main calculation will remain, calculation is on the ice component of the gradient.",
                    "label": 0
                },
                {
                    "sent": "QII can be pre calculated before the optimization procedure use.",
                    "label": 0
                },
                {
                    "sent": "Qi is basically the length of each data data data point.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The ice component of the gradient, will the gradient actually takes this form, so you have this QF I -- 1 and if you expand this and you get there some mention then after this I mentioned then you have a summation of.",
                    "label": 0
                },
                {
                    "sent": "We are a submission over Jalen.",
                    "label": 0
                },
                {
                    "sent": "Something well, this looks complicated if you look at least then how many operations are needed.",
                    "label": 0
                },
                {
                    "sent": "We need to do Eleanor products in this place.",
                    "label": 0
                },
                {
                    "sent": "We need to do Eleanor products so this is not very good.",
                    "label": 0
                },
                {
                    "sent": "Is not good.",
                    "label": 0
                },
                {
                    "sent": "OK so say that again.",
                    "label": 0
                },
                {
                    "sent": "So this submission is overall the training data over all the training data and each element is actually an inner product between two training instances, so that's why you need older Ellen cost.",
                    "label": 0
                },
                {
                    "sent": "Well this is not good.",
                    "label": 0
                },
                {
                    "sent": "This can be a lot, but there is a trick.",
                    "label": 0
                },
                {
                    "sent": "There's a trick.",
                    "label": 0
                },
                {
                    "sent": "You define a vector U.",
                    "label": 0
                },
                {
                    "sent": "Then this you is the summation over J visit so you can see that this is QIJ right?",
                    "label": 0
                },
                {
                    "sent": "And you are trying to check the inner product between I and all the instances index by Jay.",
                    "label": 0
                },
                {
                    "sent": "But we find out that if we do this summation over J by defining this as a vector, then the gradient ice component of the gradient you can take.",
                    "label": 0
                },
                {
                    "sent": "So if you take these XI out then this submission.",
                    "label": 0
                },
                {
                    "sent": "Can be together so the whole thing becomes just one single inner product, one single inner product.",
                    "label": 0
                },
                {
                    "sent": "So the cost is dramatically reduced from older L and to only order N. So all we need is to maintain this vector U.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to skip all the details of this first order methods, because I don't think I have enough time.",
                    "label": 0
                },
                {
                    "sent": "But if you are familiar with kernel methods Now, you may have noticed a difference.",
                    "label": 0
                },
                {
                    "sent": "Why we are able to do this here, but we cannot do.",
                    "label": 0
                },
                {
                    "sent": "We cannot do that in the kernel.",
                    "label": 0
                },
                {
                    "sent": "The reason is when you are using kernel methods, then your data is mapped to a high dimensional space linked.",
                    "label": 0
                },
                {
                    "sent": "You never explicitly write down the map vector.",
                    "label": 0
                },
                {
                    "sent": "Therefore, if you cannot do this dimension, you can never form this vector U.",
                    "label": 0
                },
                {
                    "sent": "But now for dinner because we are not making data to a.",
                    "label": 0
                },
                {
                    "sent": "To announce that dimensional space or to an infinite dimensional space, we know how that is, so we can do this dimension, and once we can maintain this vector in this operation becomes very very cheap.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me skip this.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let me do some comparisons.",
                    "label": 0
                },
                {
                    "sent": "I'm going to compare a few optimization methods, so the 1st two layer coordinate descent method to solve the dual problem layer actually similar the second one.",
                    "label": 0
                },
                {
                    "sent": "Some additional techniques called the shooting, but let's not worry about that here.",
                    "label": 0
                },
                {
                    "sent": "Lynn, I also consider the primal coding at the sentiment so.",
                    "label": 0
                },
                {
                    "sent": "So that means I solve the original problem of the variable W that I could right now I called primal optimization method.",
                    "label": 0
                },
                {
                    "sent": "Solo stream aces layout.",
                    "label": 0
                },
                {
                    "sent": "The 1st order method.",
                    "label": 0
                },
                {
                    "sent": "Then I have a new to miss it.",
                    "label": 0
                },
                {
                    "sent": "That's a special type of Newton method called Trust Region Newton method.",
                    "label": 1
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here I showed a decrease or another.",
                    "label": 0
                },
                {
                    "sent": "The difference to the optimal objective value.",
                    "label": 0
                },
                {
                    "sent": "So of course they are decreasing.",
                    "label": 0
                },
                {
                    "sent": "If you look at least MCV, one data set, so as if you want data set has more than half million instances and around maybe 50,000 features.",
                    "label": 0
                },
                {
                    "sent": "So it's very large.",
                    "label": 0
                },
                {
                    "sent": "Any how many seconds we can achieve very, very good objectivity.",
                    "label": 0
                },
                {
                    "sent": "This is actually 3 seconds.",
                    "label": 0
                },
                {
                    "sent": "So as I have said right now for this kind of data, we can train millions of data in just a few a few seconds.",
                    "label": 0
                },
                {
                    "sent": "And who is the winner?",
                    "label": 0
                },
                {
                    "sent": "Well, in this place.",
                    "label": 0
                },
                {
                    "sent": "The blue and the pink lines.",
                    "label": 0
                },
                {
                    "sent": "They are actually doable in a decent and the green line is actually the Newton method.",
                    "label": 0
                },
                {
                    "sent": "So looks like the code in a decent method is much faster than the 2nd order method, which is the Newton method right now.",
                    "label": 0
                },
                {
                    "sent": "Well, that's true for last four datasets, but I may not.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The case for others, actually.",
                    "label": 0
                },
                {
                    "sent": "So here I give a quick analysis on existing optimization methods for linear classification in general.",
                    "label": 0
                },
                {
                    "sent": "Right now in machine learning community, most people think that first order methods are more useful.",
                    "label": 1
                },
                {
                    "sent": "The reason is if you use a first order method then you can very quickly obtain a model and immediately you can use it.",
                    "label": 0
                },
                {
                    "sent": "But for quadratic for 2nd order method, the first iteration maybe already may already takes a lot of time, so you need to wait for longer.",
                    "label": 0
                },
                {
                    "sent": "However, from an optimization viewpoint, so if you look at the optimization history that people still, I mean in the whole optimization area, 2nd order methods are still very important, and the reason is that if you use methods like Newton or 'cause, I knew that they are much more robust, so there they are.",
                    "label": 1
                },
                {
                    "sent": "They can be faster for some conditions, situations.",
                    "label": 0
                },
                {
                    "sent": "So for some difficult situations then.",
                    "label": 0
                },
                {
                    "sent": "Those kind of coding a decent methods may not work.",
                    "label": 0
                },
                {
                    "sent": "So in my opinion both types of optimization methods are still useful for linear classification.",
                    "label": 1
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is an example.",
                    "label": 1
                },
                {
                    "sent": "For this example, the number of instances has only 30,000, but features is about 100.",
                    "label": 1
                },
                {
                    "sent": "And if you run loose, if you run this data set and the father figure on the left left hand side, less objective value and right hand side, let's leave the accuracy so you can see that the blue one, the blue one, is right now the dual coding a decent method is not working very well, but.",
                    "label": 0
                },
                {
                    "sent": "Contrast, the Newton method is not much better, but this is reasonable because for this data set, if you solve the original problem, then has a number of variables 'cause your variable is WW is associated with the number of features, right?",
                    "label": 0
                },
                {
                    "sent": "So you basically have 100 variables, But if you go to solve the dual problem then then you and number of variables variables becomes 30,000.",
                    "label": 0
                },
                {
                    "sent": "That's a lot more.",
                    "label": 0
                },
                {
                    "sent": "So in this case it should solve the primal problem and and also for this case.",
                    "label": 0
                },
                {
                    "sent": "Even if you use primal code in a decent here Now, so these are red line.",
                    "label": 0
                },
                {
                    "sent": "But the Newton method is still faster in the end, so this is an example to show that higher order method are still useful.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I will move on to talk about some extensions of linear class.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Location.",
                    "label": 0
                },
                {
                    "sent": "So so based on our discussion so far, so probably we are convinced that linear classification is useful for document classification.",
                    "label": 0
                },
                {
                    "sent": "But can we use this kind of techniques for other situations where the answer is yes?",
                    "label": 0
                },
                {
                    "sent": "We can extend it in different ways, and an important one interesting interesting Lee is to approximate kernel classifiers.",
                    "label": 1
                },
                {
                    "sent": "So so so you can see what we want is that.",
                    "label": 0
                },
                {
                    "sent": "This linear is not enough to give us better accuracy.",
                    "label": 0
                },
                {
                    "sent": "Write a kernel can give us better accuracy, so we want better accuracy.",
                    "label": 1
                },
                {
                    "sent": "However, we also want faster training and testing of linear classification, so we want both.",
                    "label": 0
                },
                {
                    "sent": "So right now how people do a do approximation use linear classification technique to approximate kernel classifiers?",
                    "label": 0
                },
                {
                    "sent": "There are basically two types.",
                    "label": 0
                },
                {
                    "sent": "But today I only have time to talk about the 1st.",
                    "label": 1
                },
                {
                    "sent": "So first idea is very simple.",
                    "label": 1
                },
                {
                    "sent": "We just do explicit data.",
                    "label": 0
                },
                {
                    "sent": "Maybe these are kind of feature engineering.",
                    "label": 0
                },
                {
                    "sent": "Then we do linear classification.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is an example.",
                    "label": 0
                },
                {
                    "sent": "How about let's just do low degree to low degree polynomial mapping?",
                    "label": 0
                },
                {
                    "sent": "This is an example of degree two polynomial mapping.",
                    "label": 0
                },
                {
                    "sent": "So instead of doing polynomial kernel we just explicitly write down the whole vector, then it becomes a new training set, right?",
                    "label": 0
                },
                {
                    "sent": "Then I don't do anything, just apply a linear classifier and that's it.",
                    "label": 0
                },
                {
                    "sent": "That's largely the whole idea.",
                    "label": 0
                },
                {
                    "sent": "But then use it as it is work, because then the dimensionality quickly increases.",
                    "label": 0
                },
                {
                    "sent": "That is right because now the number of features becomes N squared.",
                    "label": 0
                },
                {
                    "sent": "So remember any other number of features.",
                    "label": 0
                },
                {
                    "sent": "But now you have any square, but so can this still be used?",
                    "label": 0
                },
                {
                    "sent": "From our earlier discussion.",
                    "label": 0
                },
                {
                    "sent": "Between linear and kernel we mentioned a difference between older and older Ellen, so Robbie lost the difference right and versus L times an L is the number of training data.",
                    "label": 0
                },
                {
                    "sent": "But now you see the situation becomes totally different because this becomes in square.",
                    "label": 0
                },
                {
                    "sent": "So you have older N square versus older L times and then user square may not be better, right?",
                    "label": 0
                },
                {
                    "sent": "If any is extremely large.",
                    "label": 0
                },
                {
                    "sent": "Yes, that is right.",
                    "label": 0
                },
                {
                    "sent": "However, there are some situations.",
                    "label": 0
                },
                {
                    "sent": "Let these unsquare is still better.",
                    "label": 0
                },
                {
                    "sent": "That's for sparse data because for sparse data, listen square shouldn't be unsquare should be embarrassed with what is embarked.",
                    "label": 0
                },
                {
                    "sent": "Embarq easily.",
                    "label": 0
                },
                {
                    "sent": "Average number of nonzeros per instance.",
                    "label": 0
                },
                {
                    "sent": "So you can have some millions of features, but only 10 zero feature values.",
                    "label": 0
                },
                {
                    "sent": "So your December is still very very small, so your comparison is not an square versus air.",
                    "label": 0
                },
                {
                    "sent": "Time is actually in bars square versus L * N bar.",
                    "label": 0
                },
                {
                    "sent": "So as long as L is very large and AMBI is very small, this can still be useful.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me give an example here.",
                    "label": 0
                },
                {
                    "sent": "For this data set, I have almost 50,000 features.",
                    "label": 0
                },
                {
                    "sent": "But I want better accuracy.",
                    "label": 0
                },
                {
                    "sent": "So I do agree to make.",
                    "label": 0
                },
                {
                    "sent": "But then what is the size of an square well in squares here?",
                    "label": 0
                },
                {
                    "sent": "That's one billion sure dimension after degree to make becomes 1 billion.",
                    "label": 0
                },
                {
                    "sent": "I have about 200,000 instances.",
                    "label": 0
                },
                {
                    "sent": "But Fortunately the number of nonzeros per instance is only certain, so this is it isn't is an extremely sparse data.",
                    "label": 1
                },
                {
                    "sent": "So that's why we can still do degree two mapping and then do the training.",
                    "label": 1
                },
                {
                    "sent": "But but as I have mentioned that the the dimensionality of W becomes this one billion.",
                    "label": 0
                },
                {
                    "sent": "So, so you do have some technical difficulties to handle this vector W. Probably you are not able to store the whole vector W. So we actually need some additional techniques, But those can be done.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's just look at the result.",
                    "label": 0
                },
                {
                    "sent": "Again, this is a comparison between using our kernel methods and linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "For the first part I apply labels via, which is a kernel classifier and I separately use a Gaussian kernel or RBF kernel and the degree two.",
                    "label": 0
                },
                {
                    "sent": "So here this this color means degree two polynomial kernel.",
                    "label": 0
                },
                {
                    "sent": "And for the linear classification, I apply both linear nicely original one and I do degree two polynomial mapping and I generate explicitly generate the feature vector.",
                    "label": 0
                },
                {
                    "sent": "Now let's check your training time.",
                    "label": 1
                },
                {
                    "sent": "If you use kernel, the training time is about 3 hours.",
                    "label": 0
                },
                {
                    "sent": "But by this way, by using the linear, then if you use linear last three minutes and if we use a polynomial, then it is still 3 minutes.",
                    "label": 0
                },
                {
                    "sent": "But you may say that polynomial training time should still should be slightly more than linear.",
                    "label": 0
                },
                {
                    "sent": "Well, that is right, because here I have done parameter selections.",
                    "label": 0
                },
                {
                    "sent": "So there are some issues related to parameters.",
                    "label": 0
                },
                {
                    "sent": "But anyway, you can see that.",
                    "label": 0
                },
                {
                    "sent": "If you just compare the the two colors of using degree two polynomial, they're exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "They're the same thing.",
                    "label": 0
                },
                {
                    "sent": "The differences like one use degree, two polynomial, another is expanded.",
                    "label": 0
                },
                {
                    "sent": "The feature vector then apply a linear classifier and you see a 60 * 64 decrease of the training time.",
                    "label": 0
                },
                {
                    "sent": "So this is very good.",
                    "label": 0
                },
                {
                    "sent": "Then about the testing speed.",
                    "label": 0
                },
                {
                    "sent": "So this is, well, this application is for dependency parsing, so the parsing speed.",
                    "label": 1
                },
                {
                    "sent": "If you consider the passing speed of using degree two polynomial as one unit, learn by using linear.",
                    "label": 0
                },
                {
                    "sent": "This is 1600 times faster and if we use our least explicit mapping of degree two polynomial then you get 100 times faster.",
                    "label": 0
                },
                {
                    "sent": "The 3rd and the 4th rose.",
                    "label": 0
                },
                {
                    "sent": "They are actually a kind of testing accuracy.",
                    "label": 0
                },
                {
                    "sent": "This is a dependency parsing lesson, natural language processing application, their evaluation criteria.",
                    "label": 0
                },
                {
                    "sent": "USMLE is but you can consider them as a kind of test accuracy.",
                    "label": 0
                },
                {
                    "sent": "So you can see that.",
                    "label": 0
                },
                {
                    "sent": "If you only use linear.",
                    "label": 0
                },
                {
                    "sent": "Ladies only 89 but if you use degree 2 then you can get 91 but if you are using kernel then you need 3 hours but by using this explicit mapping then you can you simultaneously get faced training faced testing and also higher test accuracy.",
                    "label": 0
                },
                {
                    "sent": "So that's this is a good example and this is actually a real application so this is real application at Google.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's move on to another example.",
                    "label": 0
                },
                {
                    "sent": "And here I'm going to deploy classifiers in a very small device.",
                    "label": 1
                },
                {
                    "sent": "Any small devices so small that we are allowed to use only 16 kilobytes of red?",
                    "label": 1
                },
                {
                    "sent": "So your model cannot be more than 16 K bytes if we apply regular classifiers like decision tree, Adaboost, support vector machines with RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "Lyric curacy are reasonable, but model sites are much larger than 16 K, right?",
                    "label": 0
                },
                {
                    "sent": "So if you said a boost entries plus 1500 K, well that's too large for us.",
                    "label": 0
                },
                {
                    "sent": "Then we thought about this and I said OK, how about using linear using linear but some kind of degree polynomial mappings for this data set?",
                    "label": 0
                },
                {
                    "sent": "The number of features is very small, only 5 features.",
                    "label": 1
                },
                {
                    "sent": "When we consider a degree three polynomial mapping Hausler dimensionality after degrees, three polynomial normal mapping is actually this 5 + 3 choose three.",
                    "label": 1
                },
                {
                    "sent": "And for this case, we actually have a another bias term.",
                    "label": 0
                },
                {
                    "sent": "So after this calculation the total number of features is actually 57.",
                    "label": 0
                },
                {
                    "sent": "So we generate a new training set of 57 features.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Len, what's the size?",
                    "label": 0
                },
                {
                    "sent": "The size is actually.",
                    "label": 0
                },
                {
                    "sent": "If you do linear classification, then you all you need is that model vector W, right?",
                    "label": 0
                },
                {
                    "sent": "So you are supposed to have 57 * 4 bytes.",
                    "label": 0
                },
                {
                    "sent": "Assuming we are doing single precision, so each float consumes 4 bytes, so that's only 57 * 4, But this is a multiclass problem of five classes, and here we are using the so called one against once multiclass strategy.",
                    "label": 0
                },
                {
                    "sent": "So we need to find truth to US 10 models.",
                    "label": 0
                },
                {
                    "sent": "So we need 10 W vectors, so after this calculation we all we need is 2.2 two K space so this.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Satisfies our requirement of deploying the model into a small device of less than 16 K bytes of RAM.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And how about the test accuracy?",
                    "label": 0
                },
                {
                    "sent": "So here is a comparison.",
                    "label": 0
                },
                {
                    "sent": "So if we use a copy of Kernel Linda, test accuracy is quite high 85% this is good, but because of using kernel, so from the previous table, Now we need almost 1300 K bytes for the model.",
                    "label": 0
                },
                {
                    "sent": "But if you use linear well leader is the problem linear?",
                    "label": 0
                },
                {
                    "sent": "The size is very small, we only need a point to care for the model.",
                    "label": 0
                },
                {
                    "sent": "However, this is an example.",
                    "label": 0
                },
                {
                    "sent": "Left linear doesn't work well.",
                    "label": 0
                },
                {
                    "sent": "It doesn't give us good accuracy, only 78%.",
                    "label": 0
                },
                {
                    "sent": "Now we're doing something in Liberty.",
                    "label": 0
                },
                {
                    "sent": "We do agree St polynomial mapping.",
                    "label": 0
                },
                {
                    "sent": "But of course, when you say it's somewhere in between, you don't know where it whether it is closer to the RBF kernel site or closer to the linear side, but Fortunately for this case it's closer to the RBF one.",
                    "label": 0
                },
                {
                    "sent": "You can see that the test accuracy is very close to you, too large of using a highly nonlinear kernel like RBF, but now our model size becomes very very small, so this is a successful example of using such a technique.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this slide.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "So these slides actually talks about training time.",
                    "label": 0
                },
                {
                    "sent": "Talk about talks about training time.",
                    "label": 1
                },
                {
                    "sent": "As expected, if you are using a.",
                    "label": 1
                },
                {
                    "sent": "A kernel classifier for polynomial for polynomial kernel.",
                    "label": 0
                },
                {
                    "sent": "The training time is very long is very long.",
                    "label": 0
                },
                {
                    "sent": "But the interesting thing is here, because we just mentioned that there are different types of optimization methods, so you can use a second order or first order.",
                    "label": 0
                },
                {
                    "sent": "Are hereby primal actually being a Newton method to solve the to use the 2nd order method and this deal actually is?",
                    "label": 0
                },
                {
                    "sent": "Dual coding at decent, so that's first order.",
                    "label": 0
                },
                {
                    "sent": "And this is another example where the training time of using the 2nd order method on the primal problem, I mean the original optimization problem rather than the deal is faced.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you can see that this one is fixed.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you see I have shown two successful examples, but but you can also quickly criticize that this technique may not be that general.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that if you do polynomial mappings very quickly, you get very high dimensional vectors, right?",
                    "label": 1
                },
                {
                    "sent": "Especially if originally you are number of features is not very small.",
                    "label": 0
                },
                {
                    "sent": "So quickly you get very high dimensionality.",
                    "label": 1
                },
                {
                    "sent": "But people have sold about this issue, so they have proposed some kind of projection techniques, so some kind of hashing techniques or a random projection techniques to handle this problem.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that you do mapping first.",
                    "label": 0
                },
                {
                    "sent": "Belen somehow you you project them?",
                    "label": 0
                },
                {
                    "sent": "Speak to a full lower dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Then you can apply a linear classifier to train such a data set.",
                    "label": 0
                },
                {
                    "sent": "And for the second example, second application, one comment.",
                    "label": 0
                },
                {
                    "sent": "I want to say here is.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So remember for this state for this application at a time, not only SVM or logistic regression, I also consider tree based models like decision Tree and Ada Boost.",
                    "label": 0
                },
                {
                    "sent": "Of course there are some techniques to also reduce the model size of trees.",
                    "label": 0
                },
                {
                    "sent": "But seems it's not that easy to control the size of the tree size, but in contrast, for linear the linear is so simple that you can easily.",
                    "label": 0
                },
                {
                    "sent": "Ensure that your how large your model size should be, so that's an example this so far I'm still thinking about how to still use tree based model for the same application, but so far I haven't found out a good way, but I did find a way of using linear classifier to do this visa application.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then let's have some, uh, discuss.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And conclusions, so I want to talk about a big data linear classification.",
                    "label": 1
                },
                {
                    "sent": "So from our discussion so far, we talk, so we talk about the millions of data.",
                    "label": 0
                },
                {
                    "sent": "Well, sometimes millions are already large scale.",
                    "label": 0
                },
                {
                    "sent": "That is right.",
                    "label": 0
                },
                {
                    "sent": "Now when we talk about big data with me, like billions of data instances and a special thing is that they in general they can only be stored in a distributed environment.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We must we must recognize that in a shared in the distributed environments then situations can become very different.",
                    "label": 0
                },
                {
                    "sent": "So, for example, let optimization methods that we have just discussed may not be directly applicable in a distributed environment, you need maybe a lot of modifications.",
                    "label": 0
                },
                {
                    "sent": "Half.",
                    "label": 0
                },
                {
                    "sent": "But one important reason to to do distributed data classification is that you can save the data loading time.",
                    "label": 0
                },
                {
                    "sent": "He's right now, if you, even if you have a huge computer, we say 1 terabytes of red.",
                    "label": 0
                },
                {
                    "sent": "But 2 two to load 1 terabytes of data from your disk to the Rep. That takes a lot of time.",
                    "label": 1
                },
                {
                    "sent": "But in a distributed environment, if you can do parallel data loading, Leno saves a lot of loading time.",
                    "label": 0
                },
                {
                    "sent": "So loading time is that becomes an advantage you save loading time.",
                    "label": 0
                },
                {
                    "sent": "However, fully optimization problem, the communication becomes a problem.",
                    "label": 1
                },
                {
                    "sent": "Especially in a distributed environment, communication cost is very very very high.",
                    "label": 0
                },
                {
                    "sent": "So right now we are experimenting with the Newton method that we discussed discussed in a distributed environment and we find out that sometimes because machines in a data center are not very stable, so the communication cost can.",
                    "label": 0
                },
                {
                    "sent": "Can fluctuate significantly, so that's an issue.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In another interesting thing, if you are interested in this kind of big data classification, is that you need to notice that classification is usually only one component of the whole big data application.",
                    "label": 0
                },
                {
                    "sent": "So in one application that we are working on, so in the beginning we say, OK, I'm a researcher on data classification.",
                    "label": 0
                },
                {
                    "sent": "I want to do research on distributed data classification.",
                    "label": 1
                },
                {
                    "sent": "But after we work on that for awhile we find out that the distributed feature generation for that application takes a lot more time than training.",
                    "label": 1
                },
                {
                    "sent": "So this this shows that in a big data problem, then classification is usually only one component of the whole workflow.",
                    "label": 1
                },
                {
                    "sent": "So now I think a serious problem is that.",
                    "label": 0
                },
                {
                    "sent": "In most big data environment, this workflow is not very smooth yet, so it's very difficult for us to do experiments.",
                    "label": 0
                },
                {
                    "sent": "That's that's a problem.",
                    "label": 1
                },
                {
                    "sent": "So that also explains why so far we don't have.",
                    "label": 0
                },
                {
                    "sent": "We don't see many existing package for available for big data classification, but certainly that means this is still an interesting research area that haven't been.",
                    "label": 0
                },
                {
                    "sent": "Bing research to match.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's go to the conclusion.",
                    "label": 0
                },
                {
                    "sent": "Of course, we all know that linear classification is a very, very old topic by recently, so one reason is because of the document classification from Internet companies.",
                    "label": 1
                },
                {
                    "sent": "Then this becomes a.",
                    "label": 1
                },
                {
                    "sent": "The recently learned a lot of new and interesting applications, but even though this talk is about linear, I am not saying kernel is not useful now.",
                    "label": 0
                },
                {
                    "sent": "Currently still useful, but but they they should just be used in different situations.",
                    "label": 0
                },
                {
                    "sent": "So so so in in my talk I what I have demonstrated is that if you use linear plus feature engineering, then this approach is useful for certain problems.",
                    "label": 0
                },
                {
                    "sent": "And we know the advantage of linear is because you work on X on the data X rather than the map vector.",
                    "label": 1
                },
                {
                    "sent": "Then it's easier for feature engineering and also optimization.",
                    "label": 1
                },
                {
                    "sent": "So we expect that this linear classification can be widely used in.",
                    "label": 1
                },
                {
                    "sent": "In posting in situations ranging from, like for example I mentioned at the small devices to big data situations.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the end of my talk.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}