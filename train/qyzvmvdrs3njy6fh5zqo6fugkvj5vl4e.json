{
    "id": "qyzvmvdrs3njy6fh5zqo6fugkvj5vl4e",
    "title": "Synthesizing Knowledge Graphs for Link and Type Prediction Benchmarking",
    "info": {
        "author": [
            "Andr\u00e9 de Oliveira Melo, Institut f\u00fcr Informatik, University of Mannheim"
        ],
        "published": "July 10, 2017",
        "recorded": "May 2017",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2017_de_oliveira_melo_knowledge_graphs/",
    "segmentation": [
        [
            "Hello everybody.",
            "Thanks Joe.",
            "Introduce myself then so my name is Andre and I'm going to present some work coded in together with my supervisor High Court part time.",
            "And as you can read, that's about synthesizing knowledge graphs for Lincoln to prediction benchmarking."
        ],
        [
            "The outline of the presentation will have a short motivation just to tell why we came up with such an idea, then related work.",
            "Are we going to briefly show some other data synthesizers and also what is being done in the semantic web when it comes to synthesizing knowledge graphs?",
            "They are going to present our proposed knowledge Graph synthesis model.",
            "We're going to show the experiments we conducted and what we can we can conclude from it."
        ],
        [
            "Oh yeah, motivation.",
            "As many of you know, there's not many datasets that are generally used in experiments in their semantic web community, so the Pedia Jago Wiki data now.",
            "Freebase so generally most work to have one of these, or subset of these datasets.",
            "Many of you also know that lot of these datasets, some of them have like some specificities that can strongly influence results, so maybe some method that works very well for the pedia might not necessarily work as well on the others.",
            "And another factor is like these datasets are also quite large.",
            "And in many cases the authors perform the valuation, a subset of the data, and it can never tell if there's any kind of bias in the selection of the subset.",
            "So yeah, you actually would actually be desirable to have a benchmarking sort of benchmarking knowledge graphs where we could systematically compare different methods in terms of, for example, scalability and stability.",
            "And it would also be nice to be able to analyze the influence of some specific characteristics, like connectivity in a level of noise."
        ],
        [
            "Um?",
            "She had similar ideas of a data synthesizers in other areas, so maybe the most famous of them is the IBM Quest data generator, which is kind of an inspiration for us as well.",
            "So basically what it does is synthesizes transaction data for frequent bottom mining.",
            "And there's also some other examples of systems like a for synthesizing clustering and outlier detection data, and our spatial temporal datasets."
        ],
        [
            "When it comes to this semantic web community, this quality of a works on benchmarking, but not many that actually do synthesize datasets.",
            "One example is this University benchmarking artificial data general generator.",
            "So basically they use like one universities ontology and it's synthesized in a box.",
            "Basically some manually defined constraints like the number of departments or the number of courses taken by students.",
            "And there's also this I speak to bench, which generates DBO P. Like data is quite similar to the previous one, but then it models some other characteristics specific to the GOP data.",
            "Like the level of incompleteness and the distribution of citations over years.",
            "And.",
            "We also have a DPS sparkle benchmark.",
            "So basically in the work they said they have a data generator which doesn't actually are synthesized data, but what it does is samples instances of the pedia with some different methods.",
            "And there's also this social network and semantic publishing benchmark, so it's quite similar ideas well, but it generates a data flow order specific domains like that case, social network and media organization."
        ],
        [
            "So one thing those works have in common is like they are domain specific, like none of them can be actually applied for any arbitrary kind of knowledge graph, and none of them focus on Lincoln type protection.",
            "So that's what we want to do.",
            "We want to be able to synthesize."
        ],
        [
            "Our knowledge graphs for the evaluation of a Lincoln type prediction methods.",
            "So all of vision for the for synthesizing knowledge graphs can actually be divided in two steps, so we don't want to just simply start synthesizing knowledge graph from scratch at a first step, we want to validate the synthesis process.",
            "So what we do then at first so we try to replicate already existing knowledge graphs.",
            "So there are days that we use our existing schema like we learn our Knowledge Graph model, describe characteristics of the avox and then from there we can actually synthesize.",
            "Instances and facts.",
            "And having our synthesized knowledge graph, then we can actually compare with the original and see how good it is."
        ],
        [
            "Once we have like a validated synthesis model, then then we can not only like synthesized instances, in fact, but we can also in theory also synthesize schema and synthesize the Knowledge Graph model itself, and from that also, like it would be nice to then to be able to synthesize a collection of data sets with some different characteristics that we could use for.",
            "We can use this benchmarking."
        ],
        [
            "She had just made clear this first part swore in presenting now and the second part is basically future work so.",
            "Also, just making clear the goals again so we want to use.",
            "This synthetic data for Lincoln Tech prediction for the valuation link anti prediction matters.",
            "One thing worth noting is that we were considering only the links between entities, so we ignoring, for example numerical attributes and literals.",
            "And yeah, as I said before, we want to replicate the results we get with those link into predictions.",
            "Methods are under synthesized knowledge graph, so it should be as close as possible to that where you get in the original graph.",
            "And yeah, so that.",
            "One thing also is that we would like to enable the user to vary the size of the synthesized knowledge graph, so should be able to tell the number of instances.",
            "In fact, you want to and as we scale up or down the size of the Knowledge Graph.",
            "Ideally we want to preserve the results as well as possible."
        ],
        [
            "Um?",
            "She has stopped, like with the actual model that we propose.",
            "It's quite straightforward.",
            "I mean, at first what we do, we get the type assertions and we learn a distribution of instances over types we we want to capture their dependencies between types.",
            "So actually we have a probability for every set of types there appear in your data.",
            "So for example, we have like the probability that given instance is an actor, politician, an athlete.",
            "At the same time, and.",
            "So with that we model the distribution of type of sessions and after that we get the relation assertions and then we model.",
            "We learn like a joint distribution of facts over relations, subject types and object types.",
            "We basically do that simply with the chain rule.",
            "So we first get elected distribution of facts over relations and then a conditional distribution of subject types given relation and conditional distribution of object types given the other two."
        ],
        [
            "So from that we still cannot synthesize any facts yet we can basically tell the relation type of the subject and type of the object.",
            "We still need to pick the actual subject and object instances.",
            "So one thing worth considering is a biased a selection of instances, so let's give an example.",
            "Here we have lives in, so you synthesizing alfalfa lives in.",
            "And you say that the type of the subject.",
            "Let's say this person and the object country, and then you should now pick, for example, one instance for country.",
            "And you might think you could do that with, let's say, with uniform distribution.",
            "But in practice there shouldn't be the case.",
            "I mean what I'm showing here is the distribution of population of countries as you know it's very.",
            "Skewed distribution, so in the very left of China at the very right, the end of the day we have a Palau and you shouldn't pick those two entities with the same probability.",
            "In fact, China should be about 64,000 times more likely to be picked.",
            "So we would do then as we learn, we compute this kind of distribution for every part of relation and subject type and every pair of relation an object type and with that.",
            "Then we try to fit some different models, namely the uniform distribution, truncated explanation and power law and whatever fits the best that is like has the lowest error.",
            "We adopt them for the.",
            "I know its synthesis model."
        ],
        [
            "So yeah, with that we can already synthesize facts, but there's still some important factor that would be missing so so.",
            "Specially for link prediction, it's very important to be able to replicate some.",
            "More complex path patterns, so let's say in one if if in one graph you have that.",
            "Generally whoever is married to the mother of a given child is also apparent that the father of their given child with some confidence, then we should.",
            "I mean, ideally be able to replicate as well, and we do that by learning horn rules.",
            "In our case, we chose our Amy to do that.",
            "And yeah, with that we get like closed in safe rules and in our case we do not use constants.",
            "Um?"
        ],
        [
            "She had just to quickly show some of the characteristics that we can model with our.",
            "With all Knowledge graph model.",
            "And then we can actually learn from the data.",
            "We do not rely on this scheme itself.",
            "So I mean ideally would be present there.",
            "But you know, many times in practice it's not available, so we can, for example, replicated domain and range restrictions.",
            "I mean that is already included in the joint distribution of a relation type in a subject and type of the object.",
            "We also have a look at the cardinality of the subject and object, and if it's one we assume it to be functional.",
            "The relation to be functional or inverse functional.",
            "We also check for non reflexive needs.",
            "Those things are quite simple to do, they do not add anything to the complexity of learning the model and other characteristics.",
            "We also learn is like symmetry, transitivity, inverson, equivalent relations and these basically come for free with the horn rules like we do not.",
            "We don't have to worry about that extra."
        ],
        [
            "So yeah, this is basically the model.",
            "I mean fairly simple and just to show how we get from there to the actual synthesized knowledge graph.",
            "Just briefly show the synthesis process, which is also quite simple.",
            "So at first first thing we do is synthesize all the instances, so we do that based on the distribution.",
            "Will learn like of instances over types.",
            "Anne Anne, once we have all the instances then we start to synthesize the relation assertion.",
            "So we do that with the chain rule.",
            "So pick a relation and then type the subject and type of the object and then with have any biased selection of instances.",
            "We take that into account and select the actual subject and object there.",
            "Um?",
            "So.",
            "Then we would already have the synthesized fact and then as we add it to the Knowledge Graph.",
            "Then of course going to check if it's if it's not very present there and it's also verified if functionality, inverse functionality and or reflexiveness are not violated.",
            "Oh and then.",
            "Another important thing is that for every fact we add, we need to check if any of the horn rules we have learned is triggered by this new fact.",
            "So many cases actually need to run some query on the what's already being synthesized.",
            "So far an if happens that don't accidents are satisfied, then we should produce a new fact based on the rules confidence.",
            "So this is actually something that adds a bit to the complexity of the synthesis process."
        ],
        [
            "Um?",
            "She had experiments.",
            "I mean, there's no actual standard way of evaluating that.",
            "So their deal we had was to perform type and link prediction with some a set of different methods.",
            "So basically we choose five for each.",
            "So in type prediction we use a cell SNSD type mostly accept run KNN decision tree.",
            "So basically the last Free State of the art, multi label classifiers and basically model prediction is a multi label classification problem and using.",
            "And pulling prediction, we use a buff ranking algorithm as Steve Validate Resco transient holy or the last free embedding models.",
            "And yeah, we basically run all these methods on the original knowledge graphs and we do the same on the all the replicas we have for evaluation measures we have.",
            "We compare those values and so we ideally so the distance should be small as possible and then other things.",
            "We compare the rankings we get for the methods in each other.",
            "And so we compare that by using the spare Monroe measure.",
            "So basically, if not familiar without a rule value of 0 means the rankings are completely different and one beings that they exactly the same."
        ],
        [
            "Um?",
            "CF.",
            "One thing we do is also we show that the model has like some different components, so we split Diana in, increase the complexity of the model progressively to see how each part of the model contributes to having a better result at the end.",
            "So we have then six models.",
            "We start with M1, which is the simplest where we have the distribution of instances of the types and this joint distribution of thoughts over relation type of the subject and type of the object.",
            "Then we have.",
            "M2, which is M1 plus the cardinality restrictions and normal non reflexiveness and having free which is M 2 + 2 horn roots.",
            "We also have the of MI which is basically MI plus the biased selection of instances.",
            "And we use eight different datasets, namely the Pedia wiki data, now opencyc semantica fees as AF, AFP portal, mutagenesis, a Noble prize."
        ],
        [
            "Just illustrate a bit.",
            "Here's like a plot of the distance for one example of link prediction with the Noble Prize data set.",
            "Here we comparing the area under the precision recall curve.",
            "And then we have the five different link prediction methods there, and each of the six bars is 1 different method.",
            "They hope you can read there and as you can see there the yellow bars which are in free, any of them free which use the horn rules or significantly better than the others.",
            "Um?"
        ],
        [
            "There's also not example here again with link prediction.",
            "Now in the mutagenesis data set and we using Holy's method.",
            "And in this case here we are varying the size of the replica.",
            "So we start with 1% of the regional and 10% and 100%.",
            "And as you can see that the methods that profit the most from this increase on the size of the replica is free.",
            "Any of them free.",
            "So that's the problem.",
            "There is that the horn rules that when you use them and the facts that are produced by the horn rules that generally do not follow this original joint distribution.",
            "did I talk before?",
            "So it tends to disrupt it in our synthesis process, we try to compensate that, but generally if the data set to be synthesized rather small, it's difficult to compensate it properly.",
            "So that's why it's more affected by the reduction in size than the others."
        ],
        [
            "So yeah, I don't wanna show too many plots, so that's summary of the results we have in the upper table.",
            "That eye protection and the lower table.",
            "The link prediction results.",
            "So for each of them have two different evaluation measures that F1 score and accuracy for type prediction and under the precision recall curve and area under the Roc curve for link prediction.",
            "So you can see there we have the role an D there.",
            "So role for this palmarola comparison of rankings and D. The distance between the the measure values and have a. Oh, enlarged there.",
            "So all basically we averaging over all the data sets and all the sizes of replica.",
            "So basically for every data set was synthesized like 4 different replica sizes and.",
            "For the large case, we only consider those we average only on the largest replica size we have.",
            "Um?",
            "So yeah, basically.",
            "You can see hopefully that in general I'm free any of EM free, perform the best apart from type prediction.",
            "When it comes to distance, then M2 in their cases is better than the others."
        ],
        [
            "If you're wondering how significant those results are, we also performed many tests.",
            "So we have the critical distance diagrams there, so have the distance on the left side and the row values on the right side.",
            "So in the for the distance, the leftmost methods are the best ones and for roval is the rightmost are the best ones.",
            "And.",
            "When it comes to link prediction and free any of EM free are significantly better than the other ones, but between then there's no significant difference for prediction when it comes to distance and free any of EM freeze significantly worse, and when it comes to comparing the ranking does know that none of the difference is actually that significant."
        ],
        [
            "Another thing is just to illustrate how.",
            "This synthesis process scales.",
            "We have here plot where we vary the size of the of this synthesized data sets, and we measure the runtime.",
            "Basically the X&Y axis there in the log scale, and so basically they scale more less.",
            "Similarly, but the problem is that free any of them free like with the horn rose, it's significantly slower than the other one, so it's about 10 times slower.",
            "Um?",
            "So yeah, just to come."
        ],
        [
            "Clue then, so in generally in general, like N 3 any of EM free are the better ones and two is best when it comes to type prediction and you want to get the evaluation measure value to be as close as possible to the original.",
            "But it's not the best for preserving the rank.",
            "See, yeah, the horn rules to make a significant difference, specially for link prediction.",
            "Since there's no significant difference between N3, any of EM free, I would generally recommend him free because it's a simple model.",
            "And.",
            "Yeah, that's basically it.",
            "I mean in future work."
        ],
        [
            "It's basically the step to our vision that was set before, so maybe you can still also try to improve the OR the knowledge graph synthesis model.",
            "But yeah, we also want to be able to at some point synthesize datasets from scratch and enable users to define some desired characteristics like you do, for example with the IBM data generator.",
            "And we also want then ultimately, to create a collection of knowledge graphs, which we could then use for benchmarking, linking to prediction methods.",
            "Um, yeah that's it.",
            "Thank you."
        ],
        [
            "Attention, thanks.",
            "Thank you very much.",
            "Very interesting work.",
            "Have a small question about since we're generating synthesized knowledge base.",
            "What about also making a synthesized complete knowledge base so we can also evaluate 3 core in this case this?",
            "Do you have anything to say about that?",
            "Yeah, the fingers like.",
            "Um?",
            "Yeah, it's difficult to say whether synthesized datasets complete or not.",
            "I mean, that's.",
            "But one thing we can do is I don't know like since we can synthesize the horn rules as well.",
            "In theory we can make.",
            "We can kind of vary this, see how the patterns are fulfilled and.",
            "And the data so we can make them like be.",
            "More or less complete, I mean with regard to the patterns.",
            "But yeah, that's something we still haven't looked at yet.",
            "What we done so far is basically replicating that, so when we replicate, at least at the moment, we also replicate the incompleteness or the errors even be added.",
            "Something interesting to look at the future as well so the user can set a level of completeness of the synthesized model.",
            "That's something we want to look at.",
            "Suggest that the last isw see in in November in the resources track there was a link data generator presented.",
            "Do you kind of know anything about the key differences?",
            "I'm not sure how I could you know there this work.",
            "Yeah, I don't know exactly so it sounds to me like some.",
            "Very closely related.",
            "Public good.",
            "Any other questions, remarks, comments?",
            "Not sure if you answered this already, but do you have any plan to synthesize mistakes, noise errors as well in the future like to say, simulate at the adversarial?",
            "Addition of noise in the Knowledge Graph.",
            "Yeah, there's something that's definitely one of the characteristics that we want to allow the user to set.",
            "So different level of noise is that.",
            "And yeah, for the moment we're now as I said now, like when we replicate the data, if the data is has errors as well, like some noise, we replicate that as well.",
            "So basically.",
            "Thanks to Russell, it's invisible here, but sometimes there may be just just a few errors here and there which don't actually come up from the distribution.",
            "Yeah, one thing we could usually easily do is just like kind of smooth those distributions and like all of the zeros non zero and not increased.",
            "That's like the one you sent, thanks.",
            "Thanks.",
            "Yes, thanks a lot.",
            "I was wondering in your method in your methodology what is the importance of using the density.",
            "So you I saw in the paper where I just looking rapidly.",
            "So what do you calculate the density of the you are different knowledge graph.",
            "I was wondering what this this formation is important in your in your in your work.",
            "How much is the?",
            "Why do you use that density?",
            "Yeah, we do not explicitly use the density here, but we mentioned there is like something some of the characteristics that we would like to enable the user to set in a synthetic data so that my come.",
            "Developing algorithm you might want to know how it scales.",
            "I mean, if it's if your knowledge graph is denser or less dense and there's something that we want to allow in the future, but at the moment we don't have any specific look at that.",
            "And also on the part of the ontology part of what what?",
            "What's the model that is used to generate the data set?",
            "I don't know.",
            "I, I see that you use the different roles on euros rules, but what about the other aspect of the properties of relations?",
            "That yeah.",
            "Is definitely a lot of aspects that we're missing here, so you could think of like infinite different ways that you could improve this model.",
            "There's also something that kind of discuss with high qubits or like whether we should try to make it like even better.",
            "Above, we should just decided to make a cut point there and just submit and see how it's received by the communities.",
            "But definitely there's a lot of aspects missing there, and it's something that you can definitely improve and like.",
            "Thank you model.",
            "Any other questions, remarks?",
            "OK, then let's conclude this session.",
            "Thanks again.",
            "Is interested in downloading."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everybody.",
                    "label": 0
                },
                {
                    "sent": "Thanks Joe.",
                    "label": 0
                },
                {
                    "sent": "Introduce myself then so my name is Andre and I'm going to present some work coded in together with my supervisor High Court part time.",
                    "label": 0
                },
                {
                    "sent": "And as you can read, that's about synthesizing knowledge graphs for Lincoln to prediction benchmarking.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The outline of the presentation will have a short motivation just to tell why we came up with such an idea, then related work.",
                    "label": 0
                },
                {
                    "sent": "Are we going to briefly show some other data synthesizers and also what is being done in the semantic web when it comes to synthesizing knowledge graphs?",
                    "label": 1
                },
                {
                    "sent": "They are going to present our proposed knowledge Graph synthesis model.",
                    "label": 0
                },
                {
                    "sent": "We're going to show the experiments we conducted and what we can we can conclude from it.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh yeah, motivation.",
                    "label": 0
                },
                {
                    "sent": "As many of you know, there's not many datasets that are generally used in experiments in their semantic web community, so the Pedia Jago Wiki data now.",
                    "label": 0
                },
                {
                    "sent": "Freebase so generally most work to have one of these, or subset of these datasets.",
                    "label": 0
                },
                {
                    "sent": "Many of you also know that lot of these datasets, some of them have like some specificities that can strongly influence results, so maybe some method that works very well for the pedia might not necessarily work as well on the others.",
                    "label": 0
                },
                {
                    "sent": "And another factor is like these datasets are also quite large.",
                    "label": 0
                },
                {
                    "sent": "And in many cases the authors perform the valuation, a subset of the data, and it can never tell if there's any kind of bias in the selection of the subset.",
                    "label": 0
                },
                {
                    "sent": "So yeah, you actually would actually be desirable to have a benchmarking sort of benchmarking knowledge graphs where we could systematically compare different methods in terms of, for example, scalability and stability.",
                    "label": 0
                },
                {
                    "sent": "And it would also be nice to be able to analyze the influence of some specific characteristics, like connectivity in a level of noise.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "She had similar ideas of a data synthesizers in other areas, so maybe the most famous of them is the IBM Quest data generator, which is kind of an inspiration for us as well.",
                    "label": 1
                },
                {
                    "sent": "So basically what it does is synthesizes transaction data for frequent bottom mining.",
                    "label": 0
                },
                {
                    "sent": "And there's also some other examples of systems like a for synthesizing clustering and outlier detection data, and our spatial temporal datasets.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When it comes to this semantic web community, this quality of a works on benchmarking, but not many that actually do synthesize datasets.",
                    "label": 0
                },
                {
                    "sent": "One example is this University benchmarking artificial data general generator.",
                    "label": 1
                },
                {
                    "sent": "So basically they use like one universities ontology and it's synthesized in a box.",
                    "label": 0
                },
                {
                    "sent": "Basically some manually defined constraints like the number of departments or the number of courses taken by students.",
                    "label": 1
                },
                {
                    "sent": "And there's also this I speak to bench, which generates DBO P. Like data is quite similar to the previous one, but then it models some other characteristics specific to the GOP data.",
                    "label": 0
                },
                {
                    "sent": "Like the level of incompleteness and the distribution of citations over years.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We also have a DPS sparkle benchmark.",
                    "label": 0
                },
                {
                    "sent": "So basically in the work they said they have a data generator which doesn't actually are synthesized data, but what it does is samples instances of the pedia with some different methods.",
                    "label": 0
                },
                {
                    "sent": "And there's also this social network and semantic publishing benchmark, so it's quite similar ideas well, but it generates a data flow order specific domains like that case, social network and media organization.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one thing those works have in common is like they are domain specific, like none of them can be actually applied for any arbitrary kind of knowledge graph, and none of them focus on Lincoln type protection.",
                    "label": 0
                },
                {
                    "sent": "So that's what we want to do.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to synthesize.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our knowledge graphs for the evaluation of a Lincoln type prediction methods.",
                    "label": 1
                },
                {
                    "sent": "So all of vision for the for synthesizing knowledge graphs can actually be divided in two steps, so we don't want to just simply start synthesizing knowledge graph from scratch at a first step, we want to validate the synthesis process.",
                    "label": 1
                },
                {
                    "sent": "So what we do then at first so we try to replicate already existing knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "So there are days that we use our existing schema like we learn our Knowledge Graph model, describe characteristics of the avox and then from there we can actually synthesize.",
                    "label": 0
                },
                {
                    "sent": "Instances and facts.",
                    "label": 0
                },
                {
                    "sent": "And having our synthesized knowledge graph, then we can actually compare with the original and see how good it is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once we have like a validated synthesis model, then then we can not only like synthesized instances, in fact, but we can also in theory also synthesize schema and synthesize the Knowledge Graph model itself, and from that also, like it would be nice to then to be able to synthesize a collection of data sets with some different characteristics that we could use for.",
                    "label": 0
                },
                {
                    "sent": "We can use this benchmarking.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "She had just made clear this first part swore in presenting now and the second part is basically future work so.",
                    "label": 0
                },
                {
                    "sent": "Also, just making clear the goals again so we want to use.",
                    "label": 0
                },
                {
                    "sent": "This synthetic data for Lincoln Tech prediction for the valuation link anti prediction matters.",
                    "label": 0
                },
                {
                    "sent": "One thing worth noting is that we were considering only the links between entities, so we ignoring, for example numerical attributes and literals.",
                    "label": 0
                },
                {
                    "sent": "And yeah, as I said before, we want to replicate the results we get with those link into predictions.",
                    "label": 1
                },
                {
                    "sent": "Methods are under synthesized knowledge graph, so it should be as close as possible to that where you get in the original graph.",
                    "label": 1
                },
                {
                    "sent": "And yeah, so that.",
                    "label": 0
                },
                {
                    "sent": "One thing also is that we would like to enable the user to vary the size of the synthesized knowledge graph, so should be able to tell the number of instances.",
                    "label": 0
                },
                {
                    "sent": "In fact, you want to and as we scale up or down the size of the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "Ideally we want to preserve the results as well as possible.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "She has stopped, like with the actual model that we propose.",
                    "label": 0
                },
                {
                    "sent": "It's quite straightforward.",
                    "label": 0
                },
                {
                    "sent": "I mean, at first what we do, we get the type assertions and we learn a distribution of instances over types we we want to capture their dependencies between types.",
                    "label": 1
                },
                {
                    "sent": "So actually we have a probability for every set of types there appear in your data.",
                    "label": 0
                },
                {
                    "sent": "So for example, we have like the probability that given instance is an actor, politician, an athlete.",
                    "label": 0
                },
                {
                    "sent": "At the same time, and.",
                    "label": 0
                },
                {
                    "sent": "So with that we model the distribution of type of sessions and after that we get the relation assertions and then we model.",
                    "label": 0
                },
                {
                    "sent": "We learn like a joint distribution of facts over relations, subject types and object types.",
                    "label": 1
                },
                {
                    "sent": "We basically do that simply with the chain rule.",
                    "label": 0
                },
                {
                    "sent": "So we first get elected distribution of facts over relations and then a conditional distribution of subject types given relation and conditional distribution of object types given the other two.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So from that we still cannot synthesize any facts yet we can basically tell the relation type of the subject and type of the object.",
                    "label": 0
                },
                {
                    "sent": "We still need to pick the actual subject and object instances.",
                    "label": 1
                },
                {
                    "sent": "So one thing worth considering is a biased a selection of instances, so let's give an example.",
                    "label": 0
                },
                {
                    "sent": "Here we have lives in, so you synthesizing alfalfa lives in.",
                    "label": 0
                },
                {
                    "sent": "And you say that the type of the subject.",
                    "label": 0
                },
                {
                    "sent": "Let's say this person and the object country, and then you should now pick, for example, one instance for country.",
                    "label": 0
                },
                {
                    "sent": "And you might think you could do that with, let's say, with uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "But in practice there shouldn't be the case.",
                    "label": 1
                },
                {
                    "sent": "I mean what I'm showing here is the distribution of population of countries as you know it's very.",
                    "label": 0
                },
                {
                    "sent": "Skewed distribution, so in the very left of China at the very right, the end of the day we have a Palau and you shouldn't pick those two entities with the same probability.",
                    "label": 0
                },
                {
                    "sent": "In fact, China should be about 64,000 times more likely to be picked.",
                    "label": 0
                },
                {
                    "sent": "So we would do then as we learn, we compute this kind of distribution for every part of relation and subject type and every pair of relation an object type and with that.",
                    "label": 1
                },
                {
                    "sent": "Then we try to fit some different models, namely the uniform distribution, truncated explanation and power law and whatever fits the best that is like has the lowest error.",
                    "label": 0
                },
                {
                    "sent": "We adopt them for the.",
                    "label": 0
                },
                {
                    "sent": "I know its synthesis model.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, with that we can already synthesize facts, but there's still some important factor that would be missing so so.",
                    "label": 0
                },
                {
                    "sent": "Specially for link prediction, it's very important to be able to replicate some.",
                    "label": 1
                },
                {
                    "sent": "More complex path patterns, so let's say in one if if in one graph you have that.",
                    "label": 0
                },
                {
                    "sent": "Generally whoever is married to the mother of a given child is also apparent that the father of their given child with some confidence, then we should.",
                    "label": 0
                },
                {
                    "sent": "I mean, ideally be able to replicate as well, and we do that by learning horn rules.",
                    "label": 0
                },
                {
                    "sent": "In our case, we chose our Amy to do that.",
                    "label": 0
                },
                {
                    "sent": "And yeah, with that we get like closed in safe rules and in our case we do not use constants.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "She had just to quickly show some of the characteristics that we can model with our.",
                    "label": 0
                },
                {
                    "sent": "With all Knowledge graph model.",
                    "label": 1
                },
                {
                    "sent": "And then we can actually learn from the data.",
                    "label": 0
                },
                {
                    "sent": "We do not rely on this scheme itself.",
                    "label": 1
                },
                {
                    "sent": "So I mean ideally would be present there.",
                    "label": 0
                },
                {
                    "sent": "But you know, many times in practice it's not available, so we can, for example, replicated domain and range restrictions.",
                    "label": 1
                },
                {
                    "sent": "I mean that is already included in the joint distribution of a relation type in a subject and type of the object.",
                    "label": 0
                },
                {
                    "sent": "We also have a look at the cardinality of the subject and object, and if it's one we assume it to be functional.",
                    "label": 0
                },
                {
                    "sent": "The relation to be functional or inverse functional.",
                    "label": 1
                },
                {
                    "sent": "We also check for non reflexive needs.",
                    "label": 0
                },
                {
                    "sent": "Those things are quite simple to do, they do not add anything to the complexity of learning the model and other characteristics.",
                    "label": 0
                },
                {
                    "sent": "We also learn is like symmetry, transitivity, inverson, equivalent relations and these basically come for free with the horn rules like we do not.",
                    "label": 0
                },
                {
                    "sent": "We don't have to worry about that extra.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, this is basically the model.",
                    "label": 0
                },
                {
                    "sent": "I mean fairly simple and just to show how we get from there to the actual synthesized knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "Just briefly show the synthesis process, which is also quite simple.",
                    "label": 1
                },
                {
                    "sent": "So at first first thing we do is synthesize all the instances, so we do that based on the distribution.",
                    "label": 0
                },
                {
                    "sent": "Will learn like of instances over types.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne, once we have all the instances then we start to synthesize the relation assertion.",
                    "label": 0
                },
                {
                    "sent": "So we do that with the chain rule.",
                    "label": 1
                },
                {
                    "sent": "So pick a relation and then type the subject and type of the object and then with have any biased selection of instances.",
                    "label": 1
                },
                {
                    "sent": "We take that into account and select the actual subject and object there.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Then we would already have the synthesized fact and then as we add it to the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "Then of course going to check if it's if it's not very present there and it's also verified if functionality, inverse functionality and or reflexiveness are not violated.",
                    "label": 1
                },
                {
                    "sent": "Oh and then.",
                    "label": 0
                },
                {
                    "sent": "Another important thing is that for every fact we add, we need to check if any of the horn rules we have learned is triggered by this new fact.",
                    "label": 0
                },
                {
                    "sent": "So many cases actually need to run some query on the what's already being synthesized.",
                    "label": 1
                },
                {
                    "sent": "So far an if happens that don't accidents are satisfied, then we should produce a new fact based on the rules confidence.",
                    "label": 0
                },
                {
                    "sent": "So this is actually something that adds a bit to the complexity of the synthesis process.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "She had experiments.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's no actual standard way of evaluating that.",
                    "label": 0
                },
                {
                    "sent": "So their deal we had was to perform type and link prediction with some a set of different methods.",
                    "label": 0
                },
                {
                    "sent": "So basically we choose five for each.",
                    "label": 0
                },
                {
                    "sent": "So in type prediction we use a cell SNSD type mostly accept run KNN decision tree.",
                    "label": 1
                },
                {
                    "sent": "So basically the last Free State of the art, multi label classifiers and basically model prediction is a multi label classification problem and using.",
                    "label": 0
                },
                {
                    "sent": "And pulling prediction, we use a buff ranking algorithm as Steve Validate Resco transient holy or the last free embedding models.",
                    "label": 1
                },
                {
                    "sent": "And yeah, we basically run all these methods on the original knowledge graphs and we do the same on the all the replicas we have for evaluation measures we have.",
                    "label": 0
                },
                {
                    "sent": "We compare those values and so we ideally so the distance should be small as possible and then other things.",
                    "label": 0
                },
                {
                    "sent": "We compare the rankings we get for the methods in each other.",
                    "label": 0
                },
                {
                    "sent": "And so we compare that by using the spare Monroe measure.",
                    "label": 0
                },
                {
                    "sent": "So basically, if not familiar without a rule value of 0 means the rankings are completely different and one beings that they exactly the same.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "CF.",
                    "label": 0
                },
                {
                    "sent": "One thing we do is also we show that the model has like some different components, so we split Diana in, increase the complexity of the model progressively to see how each part of the model contributes to having a better result at the end.",
                    "label": 0
                },
                {
                    "sent": "So we have then six models.",
                    "label": 0
                },
                {
                    "sent": "We start with M1, which is the simplest where we have the distribution of instances of the types and this joint distribution of thoughts over relation type of the subject and type of the object.",
                    "label": 0
                },
                {
                    "sent": "Then we have.",
                    "label": 0
                },
                {
                    "sent": "M2, which is M1 plus the cardinality restrictions and normal non reflexiveness and having free which is M 2 + 2 horn roots.",
                    "label": 1
                },
                {
                    "sent": "We also have the of MI which is basically MI plus the biased selection of instances.",
                    "label": 1
                },
                {
                    "sent": "And we use eight different datasets, namely the Pedia wiki data, now opencyc semantica fees as AF, AFP portal, mutagenesis, a Noble prize.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just illustrate a bit.",
                    "label": 0
                },
                {
                    "sent": "Here's like a plot of the distance for one example of link prediction with the Noble Prize data set.",
                    "label": 0
                },
                {
                    "sent": "Here we comparing the area under the precision recall curve.",
                    "label": 0
                },
                {
                    "sent": "And then we have the five different link prediction methods there, and each of the six bars is 1 different method.",
                    "label": 0
                },
                {
                    "sent": "They hope you can read there and as you can see there the yellow bars which are in free, any of them free which use the horn rules or significantly better than the others.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's also not example here again with link prediction.",
                    "label": 1
                },
                {
                    "sent": "Now in the mutagenesis data set and we using Holy's method.",
                    "label": 0
                },
                {
                    "sent": "And in this case here we are varying the size of the replica.",
                    "label": 0
                },
                {
                    "sent": "So we start with 1% of the regional and 10% and 100%.",
                    "label": 0
                },
                {
                    "sent": "And as you can see that the methods that profit the most from this increase on the size of the replica is free.",
                    "label": 0
                },
                {
                    "sent": "Any of them free.",
                    "label": 0
                },
                {
                    "sent": "So that's the problem.",
                    "label": 0
                },
                {
                    "sent": "There is that the horn rules that when you use them and the facts that are produced by the horn rules that generally do not follow this original joint distribution.",
                    "label": 0
                },
                {
                    "sent": "did I talk before?",
                    "label": 0
                },
                {
                    "sent": "So it tends to disrupt it in our synthesis process, we try to compensate that, but generally if the data set to be synthesized rather small, it's difficult to compensate it properly.",
                    "label": 0
                },
                {
                    "sent": "So that's why it's more affected by the reduction in size than the others.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, I don't wanna show too many plots, so that's summary of the results we have in the upper table.",
                    "label": 0
                },
                {
                    "sent": "That eye protection and the lower table.",
                    "label": 0
                },
                {
                    "sent": "The link prediction results.",
                    "label": 0
                },
                {
                    "sent": "So for each of them have two different evaluation measures that F1 score and accuracy for type prediction and under the precision recall curve and area under the Roc curve for link prediction.",
                    "label": 1
                },
                {
                    "sent": "So you can see there we have the role an D there.",
                    "label": 0
                },
                {
                    "sent": "So role for this palmarola comparison of rankings and D. The distance between the the measure values and have a. Oh, enlarged there.",
                    "label": 0
                },
                {
                    "sent": "So all basically we averaging over all the data sets and all the sizes of replica.",
                    "label": 0
                },
                {
                    "sent": "So basically for every data set was synthesized like 4 different replica sizes and.",
                    "label": 0
                },
                {
                    "sent": "For the large case, we only consider those we average only on the largest replica size we have.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So yeah, basically.",
                    "label": 0
                },
                {
                    "sent": "You can see hopefully that in general I'm free any of EM free, perform the best apart from type prediction.",
                    "label": 0
                },
                {
                    "sent": "When it comes to distance, then M2 in their cases is better than the others.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you're wondering how significant those results are, we also performed many tests.",
                    "label": 0
                },
                {
                    "sent": "So we have the critical distance diagrams there, so have the distance on the left side and the row values on the right side.",
                    "label": 0
                },
                {
                    "sent": "So in the for the distance, the leftmost methods are the best ones and for roval is the rightmost are the best ones.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "When it comes to link prediction and free any of EM free are significantly better than the other ones, but between then there's no significant difference for prediction when it comes to distance and free any of EM freeze significantly worse, and when it comes to comparing the ranking does know that none of the difference is actually that significant.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing is just to illustrate how.",
                    "label": 0
                },
                {
                    "sent": "This synthesis process scales.",
                    "label": 0
                },
                {
                    "sent": "We have here plot where we vary the size of the of this synthesized data sets, and we measure the runtime.",
                    "label": 0
                },
                {
                    "sent": "Basically the X&Y axis there in the log scale, and so basically they scale more less.",
                    "label": 0
                },
                {
                    "sent": "Similarly, but the problem is that free any of them free like with the horn rose, it's significantly slower than the other one, so it's about 10 times slower.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So yeah, just to come.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Clue then, so in generally in general, like N 3 any of EM free are the better ones and two is best when it comes to type prediction and you want to get the evaluation measure value to be as close as possible to the original.",
                    "label": 0
                },
                {
                    "sent": "But it's not the best for preserving the rank.",
                    "label": 1
                },
                {
                    "sent": "See, yeah, the horn rules to make a significant difference, specially for link prediction.",
                    "label": 1
                },
                {
                    "sent": "Since there's no significant difference between N3, any of EM free, I would generally recommend him free because it's a simple model.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's basically it.",
                    "label": 0
                },
                {
                    "sent": "I mean in future work.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's basically the step to our vision that was set before, so maybe you can still also try to improve the OR the knowledge graph synthesis model.",
                    "label": 0
                },
                {
                    "sent": "But yeah, we also want to be able to at some point synthesize datasets from scratch and enable users to define some desired characteristics like you do, for example with the IBM data generator.",
                    "label": 1
                },
                {
                    "sent": "And we also want then ultimately, to create a collection of knowledge graphs, which we could then use for benchmarking, linking to prediction methods.",
                    "label": 1
                },
                {
                    "sent": "Um, yeah that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Attention, thanks.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Very interesting work.",
                    "label": 0
                },
                {
                    "sent": "Have a small question about since we're generating synthesized knowledge base.",
                    "label": 0
                },
                {
                    "sent": "What about also making a synthesized complete knowledge base so we can also evaluate 3 core in this case this?",
                    "label": 0
                },
                {
                    "sent": "Do you have anything to say about that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, the fingers like.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's difficult to say whether synthesized datasets complete or not.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's.",
                    "label": 0
                },
                {
                    "sent": "But one thing we can do is I don't know like since we can synthesize the horn rules as well.",
                    "label": 0
                },
                {
                    "sent": "In theory we can make.",
                    "label": 0
                },
                {
                    "sent": "We can kind of vary this, see how the patterns are fulfilled and.",
                    "label": 0
                },
                {
                    "sent": "And the data so we can make them like be.",
                    "label": 0
                },
                {
                    "sent": "More or less complete, I mean with regard to the patterns.",
                    "label": 0
                },
                {
                    "sent": "But yeah, that's something we still haven't looked at yet.",
                    "label": 0
                },
                {
                    "sent": "What we done so far is basically replicating that, so when we replicate, at least at the moment, we also replicate the incompleteness or the errors even be added.",
                    "label": 0
                },
                {
                    "sent": "Something interesting to look at the future as well so the user can set a level of completeness of the synthesized model.",
                    "label": 0
                },
                {
                    "sent": "That's something we want to look at.",
                    "label": 0
                },
                {
                    "sent": "Suggest that the last isw see in in November in the resources track there was a link data generator presented.",
                    "label": 0
                },
                {
                    "sent": "Do you kind of know anything about the key differences?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure how I could you know there this work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know exactly so it sounds to me like some.",
                    "label": 0
                },
                {
                    "sent": "Very closely related.",
                    "label": 0
                },
                {
                    "sent": "Public good.",
                    "label": 0
                },
                {
                    "sent": "Any other questions, remarks, comments?",
                    "label": 0
                },
                {
                    "sent": "Not sure if you answered this already, but do you have any plan to synthesize mistakes, noise errors as well in the future like to say, simulate at the adversarial?",
                    "label": 0
                },
                {
                    "sent": "Addition of noise in the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's something that's definitely one of the characteristics that we want to allow the user to set.",
                    "label": 0
                },
                {
                    "sent": "So different level of noise is that.",
                    "label": 0
                },
                {
                    "sent": "And yeah, for the moment we're now as I said now, like when we replicate the data, if the data is has errors as well, like some noise, we replicate that as well.",
                    "label": 0
                },
                {
                    "sent": "So basically.",
                    "label": 0
                },
                {
                    "sent": "Thanks to Russell, it's invisible here, but sometimes there may be just just a few errors here and there which don't actually come up from the distribution.",
                    "label": 0
                },
                {
                    "sent": "Yeah, one thing we could usually easily do is just like kind of smooth those distributions and like all of the zeros non zero and not increased.",
                    "label": 0
                },
                {
                    "sent": "That's like the one you sent, thanks.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Yes, thanks a lot.",
                    "label": 0
                },
                {
                    "sent": "I was wondering in your method in your methodology what is the importance of using the density.",
                    "label": 0
                },
                {
                    "sent": "So you I saw in the paper where I just looking rapidly.",
                    "label": 0
                },
                {
                    "sent": "So what do you calculate the density of the you are different knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "I was wondering what this this formation is important in your in your in your work.",
                    "label": 0
                },
                {
                    "sent": "How much is the?",
                    "label": 0
                },
                {
                    "sent": "Why do you use that density?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we do not explicitly use the density here, but we mentioned there is like something some of the characteristics that we would like to enable the user to set in a synthetic data so that my come.",
                    "label": 0
                },
                {
                    "sent": "Developing algorithm you might want to know how it scales.",
                    "label": 0
                },
                {
                    "sent": "I mean, if it's if your knowledge graph is denser or less dense and there's something that we want to allow in the future, but at the moment we don't have any specific look at that.",
                    "label": 0
                },
                {
                    "sent": "And also on the part of the ontology part of what what?",
                    "label": 0
                },
                {
                    "sent": "What's the model that is used to generate the data set?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I, I see that you use the different roles on euros rules, but what about the other aspect of the properties of relations?",
                    "label": 0
                },
                {
                    "sent": "That yeah.",
                    "label": 0
                },
                {
                    "sent": "Is definitely a lot of aspects that we're missing here, so you could think of like infinite different ways that you could improve this model.",
                    "label": 0
                },
                {
                    "sent": "There's also something that kind of discuss with high qubits or like whether we should try to make it like even better.",
                    "label": 0
                },
                {
                    "sent": "Above, we should just decided to make a cut point there and just submit and see how it's received by the communities.",
                    "label": 0
                },
                {
                    "sent": "But definitely there's a lot of aspects missing there, and it's something that you can definitely improve and like.",
                    "label": 0
                },
                {
                    "sent": "Thank you model.",
                    "label": 0
                },
                {
                    "sent": "Any other questions, remarks?",
                    "label": 0
                },
                {
                    "sent": "OK, then let's conclude this session.",
                    "label": 0
                },
                {
                    "sent": "Thanks again.",
                    "label": 0
                },
                {
                    "sent": "Is interested in downloading.",
                    "label": 0
                }
            ]
        }
    }
}