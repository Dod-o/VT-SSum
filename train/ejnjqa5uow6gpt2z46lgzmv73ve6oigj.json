{
    "id": "ejnjqa5uow6gpt2z46lgzmv73ve6oigj",
    "title": "Introduction to Machine Learning",
    "info": {
        "author": [
            "Isabelle Guyon, Clopinet"
        ],
        "published": "July 2, 2007",
        "recorded": "July 2007",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/bootcamp07_guyon_itml/",
    "segmentation": [
        [
            "To machine learning, so those of you who are already familiar with machine learning.",
            "It will be just do a review for you, but perhaps it will be a slightly different perspective and hopefully for new people this will give you a broad overview, not hesitate to interrupt me with questions to slow down my space, my my passive."
        ],
        [
            "I'm going too fast.",
            "What is machine learning?",
            "The process of machine learning consists in having first big training data database and between database that you input to some learning algorithm and comes out the train learning machine.",
            "Then subsequently in the utilization phase you can input queries to the train machine and obtain answers.",
            "So it looks like a very."
        ],
        [
            "General framework.",
            "And what is it good for?",
            "A number of problems, classification problems in which you want to predict an outcome which is binary or which is categorical.",
            "So has a number of.",
            "Finite number of values, time Series, Time series prediction, regression and clustering.",
            "So regression and time series prediction have continuous outcomes to predict and clustering.",
            "You don't know yet with the outcome should be.",
            "You're just trying to find groups in data.",
            "In my lectures I will mostly be focusing on the classification and I'll be alluding to a regression as well, so other speakers will be covering."
        ],
        [
            "Are aspects of machine learning.",
            "Some learning machines include linear methods, kernel methods and neural networks and decision trees, so this is are the ones which are mostly used presently.",
            "And of course you know people keep inventing new ones and hopefully you will be inventing also new ones."
        ],
        [
            "As far as applications are concerned.",
            "You can map applications along 2 axis.",
            "The X axis is the number of inputs that you have.",
            "And the Y axis is the number of training examples that you have.",
            "In this 2 dimensional space you can have you know.",
            "Map the complexity of problems.",
            "The fewer the number of training examples, the harder it is going to be to generate a learning machine, and this is getting worse as the number of inputs is increasing compared to the number of training examples.",
            "So you have you on the.",
            "Lower right side bioinformatics applications which are in worst case scenario in which you have many many inputs but few training examples and the top part here you have.",
            "You know maybe ecology or market analysis problems in which you have tons and tons of examples and few variables.",
            "So you're you're better off with respect to a learning.",
            "But sometimes problems become complex because you have too much data.",
            "So now it's a problem of computational complexity.",
            "Another problem of evaluating the parameters of the learning machine.",
            "And if you are in the upper right side here, you're going to have both a lot of examples and a lot of inputs.",
            "And in this case you may run out of."
        ],
        [
            "Power.",
            "So I'm, you know, briefly going over typical applications, that of machine learning in banking, telecom and retail.",
            "You might want to identify prospective customers, dissatisfied customers, good customers, bad players, and in order to obtain more effective advertising, less credit risk, or fewer fraud, or decrease churn rates.",
            "Churn rate is the rate at which people switch from one company to another.",
            "So how to?",
            "Get better customer.",
            "Retain your custom."
        ],
        [
            "Hours.",
            "In biomedical and biometrics areas you have also many applications of machine learning.",
            "In medicine you might want to be screening people for risk of certain disease, diagnosing disease, or predicting the outcome of a treatment, prognosis or discovering new drugs.",
            "And in the security area, recently there has been a renewal of interest in machine learning because of security problems and their face recognition, signature fingerprint or iris verification and DNA fingerprinting have been important."
        ],
        [
            "Applications of machine learning.",
            "In the area of computer and the Internet.",
            "People have been using machine learning to design better computer interfaces, including troubleshooting Wizards and handwriting recognition interfaces or speech recognition interfaces, and more recently even brain waves mostly for handicapped people.",
            "Right now to control computers with their brainpower directly.",
            "And the in the number of Internet applications, including heat ranking, spam filtering, text categorization, text translation."
        ],
        [
            "Recommendation.",
            "As I mentioned, the when I introduce myself, I organized challenges and the recently I've organised two challenges for NIPS 2003 and WCCI 2006.",
            "And for each challenge, there has been five datasets.",
            "So in total there are now 10 datasets that are formatted in a uniform way and that are available to perform experiments and they spend the whole spectrum of difficulty in terms of number of inputs and number of examples.",
            "Also a wide spectrum of types of applications, so different types of inputs."
        ],
        [
            "And you can see on this slide the variety of difficulty of test from a different point of view from the point of view of the performance of the participants which you see here are the histograms of the performance of the participants.",
            "So on the X axis you have the error rate on the test set, it's balanced error rate, that is the average of the error rate of the positive class and of the negative class.",
            "For two class classification problems.",
            "Since all these problems are two class classification problems.",
            "And.",
            "On some in some cases, like on the silver data set, you have very.",
            "Picked a very big distribution towards the origin, which indicates that the task is relatively easy.",
            "Everybody is doing well on that task on other datasets.",
            "For example the Dority added.",
            "I said you see that there is a very widespread distribution, so some people are doing very well and some people are doing very bad.",
            "On some other datasets you can see that there is a almost bimodal distribution.",
            "That yeah, there are.",
            "Most people do well, but some people do bad.",
            "Actually, on the model data set, it's most apparent that you have a bimodal distribution.",
            "So you'll be having the opportunity in the lab class to play with some of these datasets and compete with.",
            "You know, the best competitors trying to outperform their performance in a class.",
            "I thought a year ago in Zurich I had the students tried to outperform the participants of the first challenge.",
            "The NIPS 2003 Challenge, and they all did very well.",
            "They all you know, outperformed or matched closely the performance of the best participants.",
            "So this."
        ],
        [
            "Is doable.",
            "On this slide, I'm showing how different learning machines perform on different datasets.",
            "So I'm very coarsely grouped the learning machines into four categories.",
            "The linear and kernel methods, the neural Nets, the decision trees, and random forest methods.",
            "The Naive Bayes and these are the 10 datasets.",
            "And as you can see in terms of, you know, relative balanced error rates.",
            "That is, the balanced error rate over the average balanced error rate of all the participants.",
            "The best performers are doing very well in general with linear and curl methods for the first 2 datasets, but not not better than other methods on the last four datasets.",
            "And neural networks can do very well in some cases, but they have quite a bit of variance too.",
            "On some datasets they don't do better than others.",
            "And the decision trees well similarly also they have some data sets on which they don't do well at all, and they have bias, which is a very simple method, making independence assumptions about the inputs.",
            "So it's kind of, you know, the baseline or reference methods for all the machine learning techniques can do well on on some datasets on which actually other ones do well also, but generally performs worse.",
            "So we will see that you know one of the games in machine learning is to predict ahead of time which learning machine is going to be performing well before you've seen data.",
            "Or reserve a chunk of data as validation to decide between various learning machines.",
            "Which one you think is going to be performing best."
        ],
        [
            "The future.",
            "In this class, the convention is going to be that the data are going to be represented as a matrix, the lines of which represent examples and the columns represent features or variables.",
            "So for example, this would be patient in lines, and for each patient you would have a record of the age of the weight of the number of children and whatever information is relevant about this patient.",
            "In terms of learning machines, alot of the learning machines they weigh the either the columns or the lines in order to determine.",
            "The decision function that they're going to be using to make predictions.",
            "So if you if we are waiting the columns, then we're going to be calling the weights W, and if we're waiting, the lines were going to be calling the weights Alpha and there is one special call and that I'm here separating from others, which is the target.",
            "So this is the quantity that you want to predict.",
            "For example, the health status of the patient is the patient, healthy or diseased."
        ],
        [
            "Yeah I'm going.",
            "I'm showing you know several instances of the same matrix.",
            "Inner circle unsupervised learning problems.",
            "We are caring about what the structure of is there or not.",
            "Structure in data.",
            "The problem being that we don't know what the target Y is and in supervised learning we want to predict an outcome.",
            "So an extra column Y.",
            "So I won't be talking about unsupervised learning.",
            "Will is going to be giving you a lot of classes about this next week.",
            "So just for you to understand the difference, I'm here showing you the people call a heat map of the input matrix so all the coefficients in the matrix are mapped to some colors so the positive ones to some orange color and the negative ones to some blue color.",
            "And the matrix will be.",
            "Here is the same as a, but the columns and lines have been rearranged so that columns that are similar are close together and lines that are similar are close together and you see appearing patches of coefficients of the same color, and this indicates that there is some structure in data.",
            "So this is what clustering does for you.",
            "Now, if you take the initial matrix and you randomize the coefficients, so you just permute the lines and columns at random in any way and you try to perform again the same clustering operation.",
            "Then you obtain matrix D, showing that now the structure is gone, and so this is one way you can test whether or not there is structuring data."
        ],
        [
            "Now in this class I will be talking about supervised learning, so I'll be talking about building functions F of XX being the input.",
            "That allows us to make predictions about the target Y, so F of X should be as close as possible to the wipe that we want to predict.",
            "The simplest way of building F of X is to make a dot product between X, which is a vector, so it's going to be no one line of your matrix, for example.",
            "And the weight vector W, then eventually adding a bias.",
            "So this dot product is equivalent of, you know, making a weighted sum of the input features weighted by some coefficients.",
            "So for example, the input features at the age.",
            "The way to know the number of children, etc.",
            "All the information about your patients, each one.",
            "Then it's going to have a weight and the decision is going to be performed according to some kind of a voting and the weights are the voting power of each input feature.",
            "Now these models are very simple, but they are powerful and then there is a family of algorithms which are yet more powerful that are not linear in the inputs like this one, but are linear in the parameters.",
            "So here the the decision function I'm showing first is linear both in the inputs and in the parameters.",
            "If you retain only linear in the parameters, you obtain a family of functions which are called perceptrons.",
            "And now the DOT product carried against, not the original inputs, but some transformed in put in a feature space which I call fi of X.",
            "And if we develop this dot product, you obtain the weighted sum of the five J of X, which are so called.",
            "You know features in a new transformed space.",
            "So for example, imagine that you figured that instead of the weight and age of a patient, something more productive is the product of the two.",
            "Then you would get another feature which is the product of agent weight and this is what I would call a 5 J of X.",
            "Of course you can have you know many, much more complex operations that you can compute on features.",
            "So if you have images, you can compute new features by looking locally.",
            "For example, at pieces of the image and extracting, say corner or pieces of lines and things like that.",
            "So one lecture will be devoted to a feature extraction, how to transform the original input into some newer presentation file of X.",
            "Not in the same family of methods.",
            "There are kernel methods.",
            "In this case you see that I've replaced the weights W with Alphas because now instead of weighing the features that are the columns of the matrix, I'm now weighing the lines of the matrix.",
            "And what we're doing is that we are placing the five functions, but some special kinds of so-called basis functions.",
            "And the difference is that we are using the training examples and comparing the training examples with the new example that is under study and K is a similarity measure between the training example and the new unknown example.",
            "So if you look, you know at the perceptual decision function for vector X, which is your input, you're doing a weight weighted sum of the different features.",
            "Here you're doing also weighted sum, but the features are a special kind.",
            "The features are similarities with the original examples."
        ],
        [
            "Way back, you know, in the 80s there was a renewal of interest in artificial neural networks.",
            "People have thought that by imitating the brain or the way you know, people understand how the brain functions.",
            "We could get better predictions or better machine that make predictions.",
            "And the basic unit processing unit of the brain is the neuron.",
            "And has been modeled very coarsely by Immaculatus in 1943, and that's still the model of a neuron which is most used in machine learning, even though you know people have made many refinements and this very coarse model.",
            "Is just a simple linear model.",
            "Where is the only difference that you have?",
            "So called activation function at the output, so the unit makes a weighted sum.",
            "Of its input.",
            "And then this weighted sum goes through what people call it, squashing function or activation function.",
            "So that the input is bounded between between two values.",
            "The output sorry is bounded between two values and as a special case the activation function can be just a step function.",
            "So making a decision between two values or low value and high value, in which case the neuron is in a way be performing a classification task, deciding whether you are in class 0 or in Class 1."
        ],
        [
            "So all these am linear methods.",
            "End up making.",
            "A decision which separates the two classes according to linear decision boundary.",
            "Here I'm showing you a scatter plot and will be seeing you in the Class A lot of these scatter plots.",
            "It's a simple representation of a classification problem for two classes in two dimensions.",
            "So usually you have many many features.",
            "In some problems like DNA microarray data, each coefficient represents the activity of a gene and there are, you know, 10s of thousands of genes.",
            "So you can have 10s of thousands of features or in text processing you sometimes have 100,000 or more.",
            "Features because each feature represents frequency of word over word in a large vocabulary.",
            "But in order to understand I'll get an intuition of machine learning.",
            "Algorithms people often use only two features and they will represent problems in these two dimensional space.",
            "And for example, you can imagine that X one is the age of a patient.",
            "Annex two is the weight of a patient.",
            "And the little symbols here represent patients, so you can have patients that have a high weight and and are more senior and those would be represented by this population of red stars and the younger patient and lower weight patient that will be presented by the population of the circles here.",
            "And the problem is to find a function that using simply Agent weight, allows you to discriminate between these two populations that are, for example, the patients which are at risk of heart disease and the patients which are healthy.",
            "If you manage to separate these two populations with a single simple line like that, then you're in good shape that it means that with a simple linear function you can make this separation so you can compute a weighted sum of the first feature and the second feature, and make a decision according to that weighted sum.",
            "So here I'm showing an example in three dimensions, so usually you don't go to three dimensions to give examples because it starts being difficult to visualize, but I just want to illustrate the fact that the linear decision boundary in two dimension is a line and a linear decision boundary in three dimension is a plane an in the higher dimension it's going to be a hyperplane, which is a subspace of dimension N -- 1, and being the number of features that you."
        ],
        [
            "Here is now the perceptron, the second simplest.",
            "This learning machine that you can imagine that was invented by back in 1957 by Rosenblatt.",
            "And it's exactly like a linear.",
            "Learning machine, except that as I mentioned before, you replace the inputs by transformed inputs.",
            "These five functions, so each.",
            "Find I of X is a function which is computed from all the original inputs eventually from all the original inputs, not necessarily from all of them.",
            "And you obtain now a new vector of dimension began when the number original number of features was small and and you're waiting these new features and then OK, you eventually perform a decision with a hard threshold to classify in two categories.",
            "What do you know I'm mentioning here in terms of classification, saying that you are performing a decision with the threshold here can be also valid in most cases for regression, in which case you remove.",
            "You know this question function here an F of X is directly predicting your outcome.",
            "It's going to be a continuous value."
        ],
        [
            "The advantage of perceptrons is that they have a non linear decision boundary, so in a sense they are more powerful because they allow you to explain perfectly some data which cannot be separated perfectly with a simple line like I'm showing here.",
            "Any 3 dimensions you will get kind of a soft decision boundary that can have any shape."
        ],
        [
            "Item.",
            "Other just mentioned also kernel methods are also special case of these functions that are linear in the parameters and are very similar to perceptron, except that the five functions are replaced by the K functions, so graphically.",
            "All the inputs of you know your say your new patients are being compared with the inputs of your training examples.",
            "So X1 represents the vector of the first vector of features of the first patient and next to the vector of features of the second patient, etc.",
            "So you're going to compute, in the simplest case, you would be computing the dot product.",
            "The simple dot product between vector X of your new unknown patient.",
            "With a vector X1.",
            "Or presenting the patient one, but you can use any kind of similarity measure that compares your new patient from whom you want you know to do a dialogue that you want to diagnose with other patients, for which you know their health status, and this method carries a lot of similarity with the so-called nearest neighbor method, in which you're going to perform decision about the health status of the patient.",
            "By comparing this patient with patients who minorities.",
            "So if in the nearest neighbor technique, if your new patient is closest to a patient who is diseased, you will be deciding that the patient is disease.",
            "Dan, if it's you know closer to a healthy patient, then you will think if the patient is healthy and the measure of closeness or similarity is ad hoc, it's determined according to your knowledge of the.",
            "Of the domain, and so you have to choose the correct features and you have to choose how you compare two patients.",
            "So people have gone from the simple, you know, one nearest neighbor method to K nearest neighbor, in which you have been comparing the non patient with a number of patients that are in your database.",
            "And for example you take the three closest one and you vote among the three closest one to make the decision.",
            "And then going one step further in 1964, as a man and collaborators have been proposing the potential function method and later, you know, has been.",
            "This has been related to the person window method, which is, you know, an extension of the nearest neighbor method in a sense for classification, in which instead of taking the K nearest neighbors, you just put a little hat over.",
            "The neighborhood and all the patients vote to make the final decision, but they vote with decaying weights according to how close they are to the new unknown patient.",
            "And the decay varies in different ways depending on the kind of potential function or kernel as they're called.",
            "Now, potential functions are, you know, some very special kinds of kernels that have not been used recently.",
            "Because of some computational problems.",
            "So here we have mapped, you know."
        ],
        [
            "Some of the most important learning machines, and now how do we train them?",
            "So back you know, in earlier in the 20th century, HEB has.",
            "Uh.",
            "Made some theory about how neurons learn in the brain.",
            "And as you probably realized, you know in your studies most theories which work are linear, or at least you know the 1st order of the theory.",
            "Of course is linear and the 1st order works quite well.",
            "So that also applies to learning.",
            "The 1st order approximation of Learning is a very simple method based on correlation.",
            "And this is what happens in the brain, according to Hebb.",
            "Neurons are connected to one another via junctions.",
            "Through which some chemicals called neurotransmitters.",
            "Are released and the.",
            "The tip of the you know the output connection of the neuron is called the Axon.",
            "So here we have the very symbolic representation of a neuron.",
            "This input inputs to the neuron are called.",
            "You know the input connections to the neuron are called dendrites, and the output is called excellent.",
            "So kind of a wire and at the very top of the Axon.",
            "Here you have junction called Synapse.",
            "And so this is how the information gets transmitted from one neuron to another neuron.",
            "So another neuron here will input, it's the output it's output through the Axon and gets connected.",
            "Here at this level.",
            "And this is the the learning rule that help has been deriving.",
            "He has said that the synapse.",
            "The synapse is going to adapt according to how much it's used, so the more often there is signal going through the synapse.",
            "The wider in the way the channel is going to be, so you're using this channel a lot.",
            "Then it's going to be easier to transmit information through that Channel if you're not using it a lot.",
            "It kind of weakens.",
            "So meaning that if there is correlation.",
            "Between the activity of two neurons.",
            "And so if it's synapses being used a lot for transmitting.",
            "Then there is increase in the weight of the, so increasing the facility in the easiness of transmission here.",
            "So this translates in this very simple rule.",
            "If there is activity simultaneous activity between one neuron and the neuron to which it's connected, rain forced the synapse.",
            "If there is no activity, decrease the synapse.",
            "Now people have been using instead of having 01 outputs they've been using minus 1 + 1 outputs, which are not necessarily realistic in biologically.",
            "When doesn't necessarily know, usually biologically that the neuron is active or inactive.",
            "But practically, people do algebra instead of you know, doing exact biological modeling and they like having outputs that are negative or positive.",
            "But that doesn't really change very much.",
            "You can use the same kind of learning rule so you multiply the activity of the neuron and the activity of its neighbor.",
            "So basically the output here is going to be multiplied by.",
            "Some input of another neuron.",
            "The index I indicates 11 neuron and index J indicates another neuron, so here I'm representing only one neuron, right?",
            "So so I don't have indexes for four Y for simplicity.",
            "So here's the update rule of habit just says that you know if you have correlation between activities then."
        ],
        [
            "Increase the weight.",
            "So the reason I'm presenting this very simple rule that is not used so much in practice instead, except that you need to represent well the naive bias approach.",
            "Is that you can very easily understand with this simple rule how the kernel trick works.",
            "The kernel trick is the basis for a lot of algorithms or love kernel method algorithms.",
            "And it shows that there is a correspondence between the perceptual representation and the kernel based representation.",
            "So the two you know.",
            "Learning machines I've been presenting you the perceptron.",
            "And the kernel methods they can actually be representing the same decision function.",
            "How is this happening?",
            "Well, consider you know have this rule that I was just mentioning to you, but apply it to the perception.",
            "So the weight vector.",
            "Is going to be.",
            "A weighted sum of the Y I5XI."
        ],
        [
            "So how is this?",
            "If you go back one slide, you see that if you use this update rule, if you rain force always the weight according to the product of the Y, and you know whatever it's plugging into the XI J, then if you go to training examples.",
            "In the end you will be having something which is proportional to a sum of the wire.",
            "The YIXIJI running the summer running over all the I.",
            "So for all the training sample, every time you show the new training example, you're going to rain force all the connections according to product of the outcome.",
            "And the input."
        ],
        [
            "And in the end you just obtain a weighted sum.",
            "So for the case of the simple heads, roll is just.",
            "If you surely every example only once, you're going to be having the sum of all the examples of the Y, I5, XI.",
            "So replacing the inputs by the five functions.",
            "And so this special way of calculating the weight vector makes things very simple.",
            "Because now since F of X is, you know the dot product between W and onfi.",
            "Then you can plug in this form of W into here and expand and you obtain this knew.",
            "Form of the decision function.",
            "And very interesting Lee.",
            "You can exhibit the fact that now your weighted sum is running over dot product's over your fi I which are the Phi computed for all the training examples and your 5X?",
            "And so if you define now and you function.",
            "K of XI&X as you know this dot product then you can transform your F of X into a sum now not over the five functions but over the K functions.",
            "Right so.",
            "You have shown here that by the mere fact that using this hedge rule, you obtained a weight vector, which is a weighted sum.",
            "Of the original templates or general patterns.",
            "So each example each training example enters with a positive weight.",
            "If you're why I, you know, plus and minus one right.",
            "If your outputs are binary, then this simply means that all the examples of the positive class are added.",
            "All the examples of the negative class are subtracted.",
            "And this is the way you compute your W vectors in a way it's very intuitive, right?",
            "If you want to compute for each feature.",
            "The weight with which you're going to be.",
            "Calculating your decision function.",
            "What you what you do is that you say, well, the wait is going to be the difference between the frequency of of how often I see my future positive and how often I see my future negative right?",
            "So all the features positive, added together and all the negative ones are added, subtracted and this is how you calculate the weight.",
            "Of course there are.",
            "You know, much fancier ways of calculating the way this absolutely does not take into account possible redundancies, right?",
            "If you have many examples that are similar, you don't necessarily want to all add them up.",
            "You might want to know to consider that once you have seen one type of example, you don't want to again add more influence of that same example.",
            "If you see it again.",
            "But very roughly, this is.",
            "This is what?",
            "This is what you can do in the simplest way to compute these weights.",
            "And the simple fact that you have you know this weighted sum of the examples and that generates you know your weight vector makes it that you can.",
            "Plugging into into this serve FX represent things in these two different ways."
        ],
        [
            "This is an important consequence.",
            "This is you can do it.",
            "You know you can do it the other way around.",
            "If you go into the in that direction, if you take the perceptual representation and go into the kernel representation, it's not that interesting, right?",
            "Why would you do that?",
            "There might be some computational reason why you want to do that, but generally it doesn't buy you much because you've already computed these five functions.",
            "For example, you have a pattern recognition problem you have.",
            "You know images and you want to extract features like you know edges, corners or small arcs, and these are all your files X.",
            "What is more interesting actually to go in the other direction?",
            "Why is that?",
            "Well, because you can define now.",
            "Instead of defining features, you just have a simple similarity measure between examples and known examples that you have in your database.",
            "And the similarity measures are very flexible.",
            "Features are OK, extracting features OK, and we will be doing that in one of the class.",
            "But similarity measures are very powerful because you can define also similarity measures between objects that are not vectors.",
            "So you can define similarity measures between, say, molecules that are of different shapes.",
            "Or you can define the similarity measures between.",
            "Between.",
            "Spoken words that have different lengths.",
            "Anile not necessarily easy to represent as vectors.",
            "The interesting thing is that once you have a similarity measure like that, you can go back.",
            "To the perception form.",
            "And you may think you know why would we want to go back.",
            "Well, because we can find optimization algorithms that learn the problem in this.",
            "Oops, sorry.",
            "In this file space and and optimize.",
            "In some sense you know the.",
            "Uh, the the the the way the decision is performed in that space.",
            "And you can do that without explicitly computing the file.",
            "Just by the mere fact that you know that you can define the K as a Phi and Phi can be can remain in place it and find some cases can even have an infinite dimension.",
            "So this is, you know the so called kernel trick that you'll be hearing a lot about.",
            "I think in these two weeks in order to derive many kernel method."
        ],
        [
            "Algorithms?",
            "So what is a kernel?",
            "So kernel is a similarity measure.",
            "It's a dot product in some space.",
            "And sometimes you don't know what the space is.",
            "The fire presentation is unknown, so here are a couple of examples of kernels.",
            "The Goshen kernel just computes the Euclidean distance between two vectors.",
            "And the exponential it's the the square of it, so it's what it does is that if you have an unknown example.",
            "It puts kind of a hat around it though.",
            "Now take a take.",
            "An example of training example and the KFC.",
            "So if T represents the non example or the known example, then the annex is and S is the.",
            "A non example then as you go away from the known example, the K of of S&T decays.",
            "So this is what I explained you before that.",
            "As you go away from the center of the kernel.",
            "There is a decay, so you are going to be able to use this kernel.",
            "To A to vote and the voting power of a given pattern is going to be less and less as you go away from the unknown example.",
            "But you have other kernels that are not like that that are not centered.",
            "An indicating, but still you can use the same type of learning algorithm and this is the case of the polynomial kernel and it has a weird shape.",
            "You just do the dot product annual raise it to a given power.",
            "So without going into detail because you will see that in other classes.",
            "Using this kernel allows you to perform a function which instead of being a linear function, is a polynomial function.",
            "Polynomial."
        ],
        [
            "With respect to the inputs.",
            "And one last learning machine.",
            "I would like to mention is the 2nd last is the multilayer perceptron in which you just combine many neurons.",
            "You put them, you stack them.",
            "You know in a in a structure where you know there is one layer of neuron that is going to be the input of another layer of neuron.",
            "Or"
        ],
        [
            "Originally, the multilayer perceptron was.",
            "Designed to solve the so-called chess board problem.",
            "In which people showed that people understood that when you have two classes that are arranged in this chess board way, you cannot separate them with a single linear decision boundary.",
            "But if you have two linear decision boundary, then you can make perfect separation.",
            "Which is how comes you know the idea of stacking several layers of these of these linear decision bound?"
        ],
        [
            "No, yet another important.",
            "Class of decision functions comes from tree classifiers.",
            "So it's very different from the rest of what I've been explaining to you so far, but I cannot go without mentioning it.",
            "Uh, here I mean, you know we are representing again the same scatter plots in each box.",
            "Sorry, and so the way Decission classic tree classifiers work is that they are going to try progressively two separate examples along axis.",
            "Of the feature space.",
            "So this is 1 feature or one input.",
            "For example of the age of a patient and hear the weight of a patient in the other direction.",
            "And the different colored dots represent two populations.",
            "For example, you know the healthy and the disease patients, so we first first threshold is threshold on feature F2.",
            "You can pretty well separate the two classes, but you still have quite a bit of errors.",
            "So, but you are doing this first decision and then now you at the second level.",
            "I'm showing the two half boxes that result from.",
            "You know this first split.",
            "And if these two have boxes, we can again perform a similar operation and finding a threshold that separates this time in the other direction.",
            "And you see that there still remains some errors that still remain.",
            "You know, some sub boxes which which are not pure which contain examples of the other class.",
            "But you do that as much as you can until you know by recursively splitting until you know eventually you get little boxes in which you will have only examples of the same color.",
            "You may want to stop before you have you know exactly pure nodes.",
            "Both because you might be running out of features or because you don't want to just exactly fit your training data.",
            "So we'll be talking about overfitting.",
            "Overfitting means explaining perfectly the training examples, but not necessarily necessarily performing well on new unknown examples.",
            "So then how do you use the decision tree with GNU examples?",
            "Then you would be at the top of the tree showing a new example, an according to the threshold you've set on the feature that you chose at the first node.",
            "For example here feature, I have two you would be deciding that to go right or to go left.",
            "And then you would be looking at the second feature and the feature will tell you go right or go left etc until you get to the bottom and then when you get at the bottom you will decide according to the box in which you have fallen, whether it's going to be an example of one class or another class.",
            "So if you fall into a bugs in which more of these examples, examples of training data are black, then you will say it's a black example.",
            "So the objective that decision trees optimizes is to reduce entropy as much as possible as you go down the tree in order to achieve at the very bottom of the tree as much node purity as possible.",
            "So at the bottom at the top you have, you know, boxes that have a lot of entropy because they contain labels of different classes, whereas at the bottom there is a little entropy because the nodes are pure.",
            "There is less disorder.",
            "There are in states in which almost all boxes are pure.",
            "So this is a very quick."
        ],
        [
            "Introduction to three classifiers and the next slide and showing how different kinds of you know decision boundaries you get with the different classifiers.",
            "Here it's a three class problem, well known, one that was introduced by Fisher in 1936, but classifying different kinds of Iris is according to the length and the width of their petals or samples.",
            "Here we are looking only at petals, so there is the length of the petrol versus the width of the pedal.",
            "And you can see that there are three different kinds of viruses called setosa for chicken versicolor.",
            "And you can try to separate these three.",
            "Classes with linear classifiers are with tree classifiers.",
            "I've haven't mentioned Gaussian mixtures, but this is easily understood.",
            "You create decision boundaries that.",
            "Our little good using a Goshen over each which is centered at the center of the cluster.",
            "And these are lines that are of ice values of the oceans.",
            "And so, when the you can draw the decision boundary here or here corresponding to equal values of the oceans.",
            "And then the kernel methods.",
            "This is the hero support vector machine, so the authors of you know here is one chapter of the book that they they made this example here you can see that you can perfectly fit the training data and have this very distorted decision boundary.",
            "That makes a little notch here, so you don't.",
            "You see on this example that you know it's up to you whether you want to believe that this is a good way of making decision and the tree classifier is making decisions according to.",
            "Lines that separate vertically or horizontally because they're just setting thresholds on the features."
        ],
        [
            "Now there is a so-called fit versus robustness tradeoff.",
            "I told you that you know there is this problem, eventually overfitting.",
            "What should we choose?",
            "You know, a linear decision boundary, for example, or decision boundary that that fits very well the training examples.",
            "Well, not if you've seen you data.",
            "That are you not, that were not your training examples.",
            "And I'm representing here as field stores and field circles.",
            "If you're lucky, they're all going to fall on the right side of the decision boundary that you've inferred from your training example.",
            "If you're unlucky, you're going to make mistakes and are you going to be more likely to make mistakes if you have a simple decision boundary like that, or if you have a more complex one.",
            "So here I'm making one mistake of 1 blue example, which is classified on the red side.",
            "And here I'm actually making many more errors, even though you know I was fitting very well.",
            "The training examples.",
            "With my new test examples.",
            "I'm making tons of hours.",
            "So obviously you know this is a made up example.",
            "I could also have drawn the test examples so that they fall on the right side of the decision boundary in the nonlinear case.",
            "But as it turns out, there are theoretical arguments that tell you that in general.",
            "It's a better idea to use the simpler model and we'll go over that."
        ],
        [
            "In future lectures.",
            "Now there is another dimension that you can play with, which is the bias value.",
            "As you can see here, you can put more weight on the errors on one class as you put weight on the air of the other class.",
            "So if you think that making an error of classifying a blue example into the right class is worse than classifying and red example into the bill class, then you might want to share."
        ],
        [
            "It's your decision boundary.",
            "A little bit upwards so that you have some safety margin and you're not going to be making errors or classifying blew into the right key."
        ],
        [
            "Yes.",
            "Otherwise, if it's the other way around, you might want to shift your decision boundary in the other direction."
        ],
        [
            "In general, you can vary that bias value and monitor the tradeoff between the error making on the positive class in the area making on the negative class, and this is what's this the so called Roc curve or RC curve.",
            "With the rocker is is you know, by varying the bias it plots the success rates of the positive class, also called hit rate versus 1 minus the.",
            "The negative class success rate, or also so called the false alarm rate.",
            "And the the."
        ],
        [
            "Larger the area under that curve is the better from the point of view of the classification accuracy.",
            "And this is one way people measure classification accuracy so as to be independent on the particular choice of the bias that you make.",
            "The idea Roc Curve is this one.",
            "It has an area of 1 and it's, you know, making no error on the negative class and then when you move the bias of the sense making no error on the positive class.",
            "The random case when you're making you know you're flipping a coin and making decisions at random for the positive and negative class gives you the diagonal.",
            "And so you get.",
            "In that case an AUC of 0.5 and this is, you know, one actual Roc curve.",
            "People measure."
        ],
        [
            "In different ways in different domains.",
            "So sometimes people measure lift curve instead of Roc curves an, in which case they plot.",
            "You know the fraction of good customers versus the fractions of customer selected.",
            "I don't want to go into detail of that, but you shouldn't confuse both right?",
            "It's just you're putting a little bit something different.",
            "Here you're putting the hit rate versus the total fraction of."
        ],
        [
            "People selected.",
            "More generally, he's the real picture.",
            "More generally, if you have two classes and making predictions, then you will have several types of errors.",
            "The false positive, which are when you classifying positively an example, but it really was of a negative class, the false negative when you're classifying an example as negative, but it really was of the positive class and then you have the correct classifications, the true negative and true positive.",
            "If you add up to negative plus false positive is the total number of negative examples.",
            "False, negative and positive.",
            "The total number of positive examples.",
            "And then what I call register rejected examples or the total number of examples that you have classified as negative.",
            "So when you made your decision so there's some of that column.",
            "You have the selected examples, the ones that you have classified as positive according to your decision function.",
            "And this is the overall total.",
            "So using these quantities you can compute virtually all the measurements that people used to assess.",
            "Performance of classifiers.",
            "So you talk about false alarm or false alarm rate as the fraction of false positive over the total number of negative examples.",
            "Negative meaning belonging to the negative class.",
            "And he tried is the true positive over the total number of positive examples?",
            "And the fraction of selected is the number of selected over the total number of examples.",
            "OK, you have also precision, so using this table you'll get a picture of all the types of measurements.",
            "Error rate is false.",
            "Negative plus false positive or total number of examples.",
            "Instead of using her right, you can use the couple hit rate false alarm rate or hit rate precision or hit rate fraction of selected so people depending on the domain people use different different things.",
            "You can use balanced error rate, which is the average of the errors of the positive class and over the negative class.",
            "Um, in, you know, text processing.",
            "People use the F measure which I'm writing here for memory.",
            "And you can then also vary the threshold.",
            "As I said before, and if you vary the threshold, you can plug the Roc curve.",
            "You can plug the lift curve, or you can plug the precision recall curves, so this gives you know."
        ],
        [
            "An idea?",
            "What is a wrist functional?",
            "So risk function is a function of the parameters of the learning machine, assessing how much it is expected to fail on a given task an.",
            "I've given you examples already right for classification problem the error rate is going to be a function or 1 minus the area under the Roc curve.",
            "So whatever measurement or objective you said to yourself about the performance of your learning machine that is going to define for you are is functional.",
            "For regression, people use most often the mean squared error.",
            "So in that case you taking the output of your learning machine and the desired output.",
            "Making the difference.",
            "Squaring an averaging so it's the average discrepancy between the target we want to achieve and what you obtain from your learning machine.",
            "Why?"
        ],
        [
            "Do you need a raise function?",
            "Well that's that's used for training, right?",
            "So once you've defined how you are going to be measuring how well you're going to be performing, then you want to optimize it.",
            "So you're going to vary the weights of your learning machine.",
            "Such that you're going to be optimizing the rest functional.",
            "You want to minimize the risk, and that can be done with a variety of methods, including gradient descent, mathematical programming, simulated annealing, genetic algorithms, those are all means of going down.",
            "India Bix you know surface of the the functional defines in parameter space so this is a very simple drawing in which you know considering that there is only one parameter in your learning machine.",
            "Your risk functional defines you know peaks and valleys and what you want is to find the minimum which is the W store your weight, optimal weight.",
            "In reality you have a huge parameter space so you have a very complex.",
            "High dimensional surface in which you're trying to do that."
        ],
        [
            "Some isation.",
            "And so we're going to be continuing that in the next lecture, so we'll be defining risk functionals and defining ways of optimizing it."
        ],
        [
            "Click rate in dissent.",
            "And in summary, for you know this introduction lecture with linear threshold units that are analogous to neurons.",
            "We can build many different kinds of.",
            "Learning machines, including a linear discriminants functions which you know which are special cases.",
            "The Naive Bayes algorithm.",
            "Kernel methods that are all methods that are linear in the parameters, not necessarily in the input components.",
            "Neural networks, which are nonlinear both in the parameters and the input components.",
            "Decision trees that have also elementary nodes that make simple linear decisions.",
            "They're just thresholds in that case.",
            "And the the architecture of the learning machines have hyper local hyper parameters, of which we're going to be talking in the next lectures and those may include the choice of the basis functions.",
            "So which five functions you're using in the perception which kernel?",
            "Using in the kernel machine?",
            "The number of units and how they are arranged?",
            "In the case of a narrow network.",
            "And learning meets fitness means fitting the parameters that are the weights and also the hyperparameters.",
            "And one has to be aware of the fit versus robustness tradeoff, so it's not necessarily best to obtain a decision boundary that learns exactly well.",
            "The training examples what needs to care, which we need to care of is how well we're going to be doing on our future examples that we haven't seen in training data, and in that respect, using a linear decision boundary is sometimes better than using a more complex non linear decision boundary."
        ],
        [
            "And if you want to know to learn more about machine learning, there are a lot of good sources which include the well known textbook of do the heart and talk on pattern classification.",
            "The book of a hefty tip, Shirley and Friedman, the elements of statistical learning.",
            "And.",
            "There is a chapter that covers some of the material I've presented to you here in the perspective of, you know, going from perceptrons to kernel methods and vice versa, presenting some simple algorithms including the head algorithm that I've presented today and some other ones.",
            "And then there is the this book I've mentioned before that I'm using for for the lectures that I'll be talking about this weekend.",
            "That includes also a CD with the data of the challenge on feature selection, and so thanks very much for your attention and we'll see you next.",
            "Interrupt her with questions.",
            "Maybe I was talking too fast.",
            "I was, you know, looking at the Clock and.",
            "Postpone anybody wants to propose a clarification.",
            "Suggest some comment.",
            "Ask any question.",
            "Interest.",
            "First, I should ask a question first.",
            "I don't know very much.",
            "You know the audience yet, and perhaps I should have started with that before I talk, but.",
            "So.",
            "Be asking you where are you from that which is so who?",
            "Who in this class is.",
            "Is preparing a PhD degree.",
            "So who in this class is thinking of preparing a PhD degree?",
            "And who in this case has already completed a 50 degree?",
            "So not not so many.",
            "So many people.",
            "Most of you are actually preparing PhD, so I'm assuming actually for most of this material must must have been easier.",
            "How many of you are working on machine learning already?",
            "Good Sir, so how many of you thought you know this?",
            "This introduction was easy to understand.",
            "Who is not necessarily a bigot.",
            "Intersection.",
            "So how many of you had heard before of kernel methods?",
            "How many of you have worked on neural networks?",
            "How many of you have worked on decision trees?",
            "How many of you have worked on?",
            "What did I talk about?",
            "How many of you have heard about overfitting?",
            "OK. OK, so that that gives me a little bit of idea.",
            "So how many of you understands after this lecture?",
            "What is a risk functional?",
            "OK. How many of you understands?",
            "What is a linear decision boundary?",
            "OK. OK, so well, let's go back very quickly then.",
            "And see.",
            "How are we doing here?",
            "So how many of you understands without it on these axes?",
            "The Axis the X&Y here.",
            "OK, so for those of you who understand looking at this.",
            "Hang out with fast here."
        ],
        [
            "Looking at this matrix, which is the data matrix, OK, you have two dimensions, the number of features which are the number you know of input components and the number of examples.",
            "So."
        ],
        [
            "On the axis of these plots, I'm showing the number of features or inputs and the number of examples.",
            "So this is the aspect ratio of the data of the data and the aspect ratio of the data is very important.",
            "How many examples you have relative to the number of features and I'm going to play playing a lot with that in this class are going to be actually trying to take these."
        ],
        [
            "Put matrix and shrink it in that direction.",
            "Here this card features the features that are not important.",
            "OK, there are other cases in which we might want to discard examples.",
            "Usually you don't want to discard examples right?",
            "Because examples are very useful.",
            "The more examples you have, the better you can train, but in some cases you might want to group examples because if your matrix is very sparse and only few features at a time are active.",
            "Then one example alone might not care enough information, so you might want to smooth things an average examples.",
            "So that's something people do.",
            "But in this class will be working mostly in that dimension.",
            "How to compress data by removing features that are useless or redundant?"
        ],
        [
            "So.",
            "Um?",
            "So how many of you understand this?",
            "This graph here what it does?",
            "So most of you have understood that these are the inputs.",
            "The inputs are weighted by some weights, and in order to compute additional function.",
            "So any number of features in the conventions that I'm using."
        ],
        [
            "Ann is going to be always number of features.",
            "An M number of examples.",
            "So it's very important to notice that."
        ],
        [
            "Because in the perceptron case you see that the sum runs over N. So here is big and because we have transformed first the inputs into new features.",
            "So we have the original matrix which has small end here and then we're doing some transform and either we're going to increase the number of features or to decrease it.",
            "In the case of the Kern."
        ],
        [
            "Message actually we are usually increasing the number of features.",
            "So what happens in the case of the kernel method is that the sum now runs over M. But"
        ],
        [
            "Because of the kernel trick that I've shown you before, you can go between the two or presentations.",
            "You can transform the perceptron into a kernel method and vice versa.",
            "And the reason why this is interesting is that then you can have.",
            "Implicit feature spaces that are of infinite dimension.",
            "Why would you want?"
        ],
        [
            "That well.",
            "The you want some a lot of expressive power.",
            "So if you plunge your original input into a very big feature space and you're going to be able to draw decision boundaries that are very complex in your original input space, whereas in the feature space it could be a linear decision function, so that's the main idea of the main trick you're going to be.",
            "Taking your original feature space and for example, the first people working on that, they just made products of the input features.",
            "So Rosenblatt, when he invented the perceptron, he had this idea.",
            "Well, let's take random.",
            "Let's take random features, some features at random, draw them at random, and next take an arbitrary Boolean function and compute some feature at random from some of the original features and do that many, many times.",
            "So by using these random Boolean functions he expanded.",
            "His original feature space into huge feature space and then he said, well, now that I have this huge feature space, I'm just going to be using a linear decision function and because he plunged his original data in a huge feature space then he could make a perfect separation of the training examples.",
            "With just a simple linear separation, it's a linear separation by this hyperplane in a very very high dimensional space.",
            "So you have big now began can be you know hundreds of thousands of these features that are computed with these random Boolean functions.",
            "Now he did that to address the problem that people would say, well, linear decision functions are too simple.",
            "They can't express the complexity of the data.",
            "There are many datasets that are not linearly separable, that is.",
            "If you take the training data, you can't make a 0 error separation.",
            "You can't make an errorless separation that will put all the examples of one class on one side.",
            "All the examples of the other class on the other side with a simple hyperplane, he said well.",
            "No problem, I'm going to use these random Boolean functions.",
            "Plunged the original space into a much bigger space and now I can separate perfectly the training data.",
            "So this is the essential idea that's used by perceptions and also by kernel methods.",
            "Why?",
            "Because you have this duality between the two representations, and it can be shown that for each valid kernel you can have an expansion into a feature space that eventually can be infinite, but you can prove you know mathematically that.",
            "Such expansion exists.",
            "And in that case, using a simple algorithm that separates that makes a linear decision boundary in this feature space you can obtain the power of nonlinear decision boundaries in the input space in the original space, right?",
            "So here you don't get really this on this picture, but imagine that this is, you know, the original SpaceX 1X2 and imagine that this is 515 two, and that's why are features.",
            "That are derived.",
            "No, it's the opposite.",
            "Sorry, this is.",
            "This is the original space and this is the five space.",
            "Eventually, you know, if you make a transformation that is powerful enough in the file space, you're going to be able to separate with the linear decisions.",
            "That's the basic idea.",
            "And so right.",
            "Yes.",
            "Station.",
            "Extra notes.",
            "Examples of notice.",
            "Under the features.",
            "OK, so you have two indexes.",
            "And yeah, I haven't made a clear distinction in the notation.",
            "When I use boldface.",
            "A bold face character that indicates that it's a vector.",
            "And then the index in the indicates the index of the example.",
            "And when it's a regular phase, it's one input component.",
            "So yes, in some cases people use, you know subscripts and superscripts to distinguish the two indices, but that's also can be confusing.",
            "Confused, you know with the power, so in this case here, when it's not ambiguous, I use.",
            "The index indicates either.",
            "Um, this."
        ],
        [
            "The.",
            "The pattern or the example.",
            "So here you see that I have a lower case and I have two indices, so I is the index of pattern and J is the index of example.",
            "In the matrix here."
        ],
        [
            "In the matrix notation.",
            "Here I have two indices.",
            "So I is the index of line indicating the example and J is the index of column.",
            "And when I put a bold face and I put only one index, then I mean the pattern index.",
            "Sorry if it's a little bit confusing.",
            "There's a tradeoff between making the notation complex and or making it obvious.",
            "Come.",
            "If there is, you know if ever there is a ambiguity please new stop me and ask me but.",
            "So in."
        ],
        [
            "This in this plots here.",
            "What is meant is that you're looking at only one example, so I'm not putting the index of the pattern.",
            "And one means feature one and two means feature 2.",
            "Is that clear to everybody or?"
        ],
        [
            "Like a large number of features.",
            "And then two leaders.",
            "Explain yourself.",
            "Separate data.",
            "Not very well.",
            "Explain.",
            "Let me say this effects the output.",
            "How how you figure out?",
            "The how do you explain which feature, how, how, how much it is involved in the final decision.",
            "Yes, that's that's a very important thing, and there are some methods.",
            "Do this more explicitly than others."
        ],
        [
            "The linear methods do this very simply because each feature is weighted by a given weight, so you know that the importance of this feature here can be assessed to some extent.",
            "You know by this way, particularly if the features are independent of one another.",
            "If there are dependencies between features.",
            "It's not that obvious.",
            "It's not as obvious to interpret the weights, because imagine, for example, that feature X one and feature X-22 copies of the same feature.",
            "In which case you could have, you know, a very small wait here in a very big way here, or vice versa.",
            "And that would mean really match right?",
            "And it's more complex if you have now complex dependencies or instead of just having repeats of features.",
            "Um?",
            "So this is.",
            "This is why 1 one people.",
            "One way people have been overcoming that problem is to transform features into spaces in which features are independent of one another.",
            "In which case you know you can better interpret the weights.",
            "Another case is you can do some sensitivity analysis.",
            "You can wiggle the input and see how much it affects the outputs is another way people have been doing that and then that works also for methods that are different than linear methods.",
            "And, you know, by asking that question you actually getting, you know, at the very heart of the problem of feature selection, because we're going to want to retain those features that impact most the output an you will see you know in the next lecture is that there is a broad variety of approaches to address that problem, and some of the approaches try to reverse engineer the system that generated the data.",
            "Saying that you know the optimal.",
            "Way of knowing how important a feature is is to understand.",
            "How in the original system that generated the data it was you know wired to the output, but in most practical cases you can't reverse engineer the data generating system because you don't have enough examples to do that, or you don't have enough human knowledge to do that.",
            "So you have to resort to some more or less meaningful approximations.",
            "And sometimes you know.",
            "You don't do as well as you should, but.",
            "Are there other questions?",
            "Every time.",
            "Each week, the schedule of today and some local information."
        ],
        [
            "Show you how efficient we are.",
            "We already managed to change the city before you got it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To machine learning, so those of you who are already familiar with machine learning.",
                    "label": 0
                },
                {
                    "sent": "It will be just do a review for you, but perhaps it will be a slightly different perspective and hopefully for new people this will give you a broad overview, not hesitate to interrupt me with questions to slow down my space, my my passive.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going too fast.",
                    "label": 0
                },
                {
                    "sent": "What is machine learning?",
                    "label": 0
                },
                {
                    "sent": "The process of machine learning consists in having first big training data database and between database that you input to some learning algorithm and comes out the train learning machine.",
                    "label": 1
                },
                {
                    "sent": "Then subsequently in the utilization phase you can input queries to the train machine and obtain answers.",
                    "label": 0
                },
                {
                    "sent": "So it looks like a very.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "General framework.",
                    "label": 0
                },
                {
                    "sent": "And what is it good for?",
                    "label": 0
                },
                {
                    "sent": "A number of problems, classification problems in which you want to predict an outcome which is binary or which is categorical.",
                    "label": 0
                },
                {
                    "sent": "So has a number of.",
                    "label": 0
                },
                {
                    "sent": "Finite number of values, time Series, Time series prediction, regression and clustering.",
                    "label": 1
                },
                {
                    "sent": "So regression and time series prediction have continuous outcomes to predict and clustering.",
                    "label": 0
                },
                {
                    "sent": "You don't know yet with the outcome should be.",
                    "label": 0
                },
                {
                    "sent": "You're just trying to find groups in data.",
                    "label": 0
                },
                {
                    "sent": "In my lectures I will mostly be focusing on the classification and I'll be alluding to a regression as well, so other speakers will be covering.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are aspects of machine learning.",
                    "label": 0
                },
                {
                    "sent": "Some learning machines include linear methods, kernel methods and neural networks and decision trees, so this is are the ones which are mostly used presently.",
                    "label": 1
                },
                {
                    "sent": "And of course you know people keep inventing new ones and hopefully you will be inventing also new ones.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As far as applications are concerned.",
                    "label": 0
                },
                {
                    "sent": "You can map applications along 2 axis.",
                    "label": 0
                },
                {
                    "sent": "The X axis is the number of inputs that you have.",
                    "label": 0
                },
                {
                    "sent": "And the Y axis is the number of training examples that you have.",
                    "label": 1
                },
                {
                    "sent": "In this 2 dimensional space you can have you know.",
                    "label": 0
                },
                {
                    "sent": "Map the complexity of problems.",
                    "label": 0
                },
                {
                    "sent": "The fewer the number of training examples, the harder it is going to be to generate a learning machine, and this is getting worse as the number of inputs is increasing compared to the number of training examples.",
                    "label": 0
                },
                {
                    "sent": "So you have you on the.",
                    "label": 0
                },
                {
                    "sent": "Lower right side bioinformatics applications which are in worst case scenario in which you have many many inputs but few training examples and the top part here you have.",
                    "label": 1
                },
                {
                    "sent": "You know maybe ecology or market analysis problems in which you have tons and tons of examples and few variables.",
                    "label": 0
                },
                {
                    "sent": "So you're you're better off with respect to a learning.",
                    "label": 0
                },
                {
                    "sent": "But sometimes problems become complex because you have too much data.",
                    "label": 0
                },
                {
                    "sent": "So now it's a problem of computational complexity.",
                    "label": 0
                },
                {
                    "sent": "Another problem of evaluating the parameters of the learning machine.",
                    "label": 0
                },
                {
                    "sent": "And if you are in the upper right side here, you're going to have both a lot of examples and a lot of inputs.",
                    "label": 0
                },
                {
                    "sent": "And in this case you may run out of.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Power.",
                    "label": 0
                },
                {
                    "sent": "So I'm, you know, briefly going over typical applications, that of machine learning in banking, telecom and retail.",
                    "label": 0
                },
                {
                    "sent": "You might want to identify prospective customers, dissatisfied customers, good customers, bad players, and in order to obtain more effective advertising, less credit risk, or fewer fraud, or decrease churn rates.",
                    "label": 1
                },
                {
                    "sent": "Churn rate is the rate at which people switch from one company to another.",
                    "label": 0
                },
                {
                    "sent": "So how to?",
                    "label": 0
                },
                {
                    "sent": "Get better customer.",
                    "label": 0
                },
                {
                    "sent": "Retain your custom.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hours.",
                    "label": 0
                },
                {
                    "sent": "In biomedical and biometrics areas you have also many applications of machine learning.",
                    "label": 0
                },
                {
                    "sent": "In medicine you might want to be screening people for risk of certain disease, diagnosing disease, or predicting the outcome of a treatment, prognosis or discovering new drugs.",
                    "label": 0
                },
                {
                    "sent": "And in the security area, recently there has been a renewal of interest in machine learning because of security problems and their face recognition, signature fingerprint or iris verification and DNA fingerprinting have been important.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Applications of machine learning.",
                    "label": 0
                },
                {
                    "sent": "In the area of computer and the Internet.",
                    "label": 0
                },
                {
                    "sent": "People have been using machine learning to design better computer interfaces, including troubleshooting Wizards and handwriting recognition interfaces or speech recognition interfaces, and more recently even brain waves mostly for handicapped people.",
                    "label": 0
                },
                {
                    "sent": "Right now to control computers with their brainpower directly.",
                    "label": 0
                },
                {
                    "sent": "And the in the number of Internet applications, including heat ranking, spam filtering, text categorization, text translation.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Recommendation.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, the when I introduce myself, I organized challenges and the recently I've organised two challenges for NIPS 2003 and WCCI 2006.",
                    "label": 1
                },
                {
                    "sent": "And for each challenge, there has been five datasets.",
                    "label": 0
                },
                {
                    "sent": "So in total there are now 10 datasets that are formatted in a uniform way and that are available to perform experiments and they spend the whole spectrum of difficulty in terms of number of inputs and number of examples.",
                    "label": 0
                },
                {
                    "sent": "Also a wide spectrum of types of applications, so different types of inputs.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can see on this slide the variety of difficulty of test from a different point of view from the point of view of the performance of the participants which you see here are the histograms of the performance of the participants.",
                    "label": 0
                },
                {
                    "sent": "So on the X axis you have the error rate on the test set, it's balanced error rate, that is the average of the error rate of the positive class and of the negative class.",
                    "label": 0
                },
                {
                    "sent": "For two class classification problems.",
                    "label": 0
                },
                {
                    "sent": "Since all these problems are two class classification problems.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "On some in some cases, like on the silver data set, you have very.",
                    "label": 0
                },
                {
                    "sent": "Picked a very big distribution towards the origin, which indicates that the task is relatively easy.",
                    "label": 0
                },
                {
                    "sent": "Everybody is doing well on that task on other datasets.",
                    "label": 0
                },
                {
                    "sent": "For example the Dority added.",
                    "label": 0
                },
                {
                    "sent": "I said you see that there is a very widespread distribution, so some people are doing very well and some people are doing very bad.",
                    "label": 0
                },
                {
                    "sent": "On some other datasets you can see that there is a almost bimodal distribution.",
                    "label": 0
                },
                {
                    "sent": "That yeah, there are.",
                    "label": 0
                },
                {
                    "sent": "Most people do well, but some people do bad.",
                    "label": 0
                },
                {
                    "sent": "Actually, on the model data set, it's most apparent that you have a bimodal distribution.",
                    "label": 0
                },
                {
                    "sent": "So you'll be having the opportunity in the lab class to play with some of these datasets and compete with.",
                    "label": 0
                },
                {
                    "sent": "You know, the best competitors trying to outperform their performance in a class.",
                    "label": 0
                },
                {
                    "sent": "I thought a year ago in Zurich I had the students tried to outperform the participants of the first challenge.",
                    "label": 0
                },
                {
                    "sent": "The NIPS 2003 Challenge, and they all did very well.",
                    "label": 0
                },
                {
                    "sent": "They all you know, outperformed or matched closely the performance of the best participants.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is doable.",
                    "label": 0
                },
                {
                    "sent": "On this slide, I'm showing how different learning machines perform on different datasets.",
                    "label": 0
                },
                {
                    "sent": "So I'm very coarsely grouped the learning machines into four categories.",
                    "label": 0
                },
                {
                    "sent": "The linear and kernel methods, the neural Nets, the decision trees, and random forest methods.",
                    "label": 0
                },
                {
                    "sent": "The Naive Bayes and these are the 10 datasets.",
                    "label": 0
                },
                {
                    "sent": "And as you can see in terms of, you know, relative balanced error rates.",
                    "label": 0
                },
                {
                    "sent": "That is, the balanced error rate over the average balanced error rate of all the participants.",
                    "label": 0
                },
                {
                    "sent": "The best performers are doing very well in general with linear and curl methods for the first 2 datasets, but not not better than other methods on the last four datasets.",
                    "label": 0
                },
                {
                    "sent": "And neural networks can do very well in some cases, but they have quite a bit of variance too.",
                    "label": 0
                },
                {
                    "sent": "On some datasets they don't do better than others.",
                    "label": 0
                },
                {
                    "sent": "And the decision trees well similarly also they have some data sets on which they don't do well at all, and they have bias, which is a very simple method, making independence assumptions about the inputs.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of, you know, the baseline or reference methods for all the machine learning techniques can do well on on some datasets on which actually other ones do well also, but generally performs worse.",
                    "label": 0
                },
                {
                    "sent": "So we will see that you know one of the games in machine learning is to predict ahead of time which learning machine is going to be performing well before you've seen data.",
                    "label": 0
                },
                {
                    "sent": "Or reserve a chunk of data as validation to decide between various learning machines.",
                    "label": 0
                },
                {
                    "sent": "Which one you think is going to be performing best.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The future.",
                    "label": 0
                },
                {
                    "sent": "In this class, the convention is going to be that the data are going to be represented as a matrix, the lines of which represent examples and the columns represent features or variables.",
                    "label": 0
                },
                {
                    "sent": "So for example, this would be patient in lines, and for each patient you would have a record of the age of the weight of the number of children and whatever information is relevant about this patient.",
                    "label": 0
                },
                {
                    "sent": "In terms of learning machines, alot of the learning machines they weigh the either the columns or the lines in order to determine.",
                    "label": 0
                },
                {
                    "sent": "The decision function that they're going to be using to make predictions.",
                    "label": 0
                },
                {
                    "sent": "So if you if we are waiting the columns, then we're going to be calling the weights W, and if we're waiting, the lines were going to be calling the weights Alpha and there is one special call and that I'm here separating from others, which is the target.",
                    "label": 0
                },
                {
                    "sent": "So this is the quantity that you want to predict.",
                    "label": 0
                },
                {
                    "sent": "For example, the health status of the patient is the patient, healthy or diseased.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah I'm going.",
                    "label": 0
                },
                {
                    "sent": "I'm showing you know several instances of the same matrix.",
                    "label": 0
                },
                {
                    "sent": "Inner circle unsupervised learning problems.",
                    "label": 1
                },
                {
                    "sent": "We are caring about what the structure of is there or not.",
                    "label": 0
                },
                {
                    "sent": "Structure in data.",
                    "label": 0
                },
                {
                    "sent": "The problem being that we don't know what the target Y is and in supervised learning we want to predict an outcome.",
                    "label": 1
                },
                {
                    "sent": "So an extra column Y.",
                    "label": 0
                },
                {
                    "sent": "So I won't be talking about unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "Will is going to be giving you a lot of classes about this next week.",
                    "label": 0
                },
                {
                    "sent": "So just for you to understand the difference, I'm here showing you the people call a heat map of the input matrix so all the coefficients in the matrix are mapped to some colors so the positive ones to some orange color and the negative ones to some blue color.",
                    "label": 0
                },
                {
                    "sent": "And the matrix will be.",
                    "label": 0
                },
                {
                    "sent": "Here is the same as a, but the columns and lines have been rearranged so that columns that are similar are close together and lines that are similar are close together and you see appearing patches of coefficients of the same color, and this indicates that there is some structure in data.",
                    "label": 0
                },
                {
                    "sent": "So this is what clustering does for you.",
                    "label": 0
                },
                {
                    "sent": "Now, if you take the initial matrix and you randomize the coefficients, so you just permute the lines and columns at random in any way and you try to perform again the same clustering operation.",
                    "label": 0
                },
                {
                    "sent": "Then you obtain matrix D, showing that now the structure is gone, and so this is one way you can test whether or not there is structuring data.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now in this class I will be talking about supervised learning, so I'll be talking about building functions F of XX being the input.",
                    "label": 0
                },
                {
                    "sent": "That allows us to make predictions about the target Y, so F of X should be as close as possible to the wipe that we want to predict.",
                    "label": 0
                },
                {
                    "sent": "The simplest way of building F of X is to make a dot product between X, which is a vector, so it's going to be no one line of your matrix, for example.",
                    "label": 0
                },
                {
                    "sent": "And the weight vector W, then eventually adding a bias.",
                    "label": 0
                },
                {
                    "sent": "So this dot product is equivalent of, you know, making a weighted sum of the input features weighted by some coefficients.",
                    "label": 0
                },
                {
                    "sent": "So for example, the input features at the age.",
                    "label": 0
                },
                {
                    "sent": "The way to know the number of children, etc.",
                    "label": 0
                },
                {
                    "sent": "All the information about your patients, each one.",
                    "label": 0
                },
                {
                    "sent": "Then it's going to have a weight and the decision is going to be performed according to some kind of a voting and the weights are the voting power of each input feature.",
                    "label": 0
                },
                {
                    "sent": "Now these models are very simple, but they are powerful and then there is a family of algorithms which are yet more powerful that are not linear in the inputs like this one, but are linear in the parameters.",
                    "label": 0
                },
                {
                    "sent": "So here the the decision function I'm showing first is linear both in the inputs and in the parameters.",
                    "label": 1
                },
                {
                    "sent": "If you retain only linear in the parameters, you obtain a family of functions which are called perceptrons.",
                    "label": 0
                },
                {
                    "sent": "And now the DOT product carried against, not the original inputs, but some transformed in put in a feature space which I call fi of X.",
                    "label": 0
                },
                {
                    "sent": "And if we develop this dot product, you obtain the weighted sum of the five J of X, which are so called.",
                    "label": 0
                },
                {
                    "sent": "You know features in a new transformed space.",
                    "label": 0
                },
                {
                    "sent": "So for example, imagine that you figured that instead of the weight and age of a patient, something more productive is the product of the two.",
                    "label": 0
                },
                {
                    "sent": "Then you would get another feature which is the product of agent weight and this is what I would call a 5 J of X.",
                    "label": 0
                },
                {
                    "sent": "Of course you can have you know many, much more complex operations that you can compute on features.",
                    "label": 0
                },
                {
                    "sent": "So if you have images, you can compute new features by looking locally.",
                    "label": 0
                },
                {
                    "sent": "For example, at pieces of the image and extracting, say corner or pieces of lines and things like that.",
                    "label": 0
                },
                {
                    "sent": "So one lecture will be devoted to a feature extraction, how to transform the original input into some newer presentation file of X.",
                    "label": 1
                },
                {
                    "sent": "Not in the same family of methods.",
                    "label": 0
                },
                {
                    "sent": "There are kernel methods.",
                    "label": 0
                },
                {
                    "sent": "In this case you see that I've replaced the weights W with Alphas because now instead of weighing the features that are the columns of the matrix, I'm now weighing the lines of the matrix.",
                    "label": 0
                },
                {
                    "sent": "And what we're doing is that we are placing the five functions, but some special kinds of so-called basis functions.",
                    "label": 0
                },
                {
                    "sent": "And the difference is that we are using the training examples and comparing the training examples with the new example that is under study and K is a similarity measure between the training example and the new unknown example.",
                    "label": 0
                },
                {
                    "sent": "So if you look, you know at the perceptual decision function for vector X, which is your input, you're doing a weight weighted sum of the different features.",
                    "label": 0
                },
                {
                    "sent": "Here you're doing also weighted sum, but the features are a special kind.",
                    "label": 0
                },
                {
                    "sent": "The features are similarities with the original examples.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Way back, you know, in the 80s there was a renewal of interest in artificial neural networks.",
                    "label": 0
                },
                {
                    "sent": "People have thought that by imitating the brain or the way you know, people understand how the brain functions.",
                    "label": 0
                },
                {
                    "sent": "We could get better predictions or better machine that make predictions.",
                    "label": 0
                },
                {
                    "sent": "And the basic unit processing unit of the brain is the neuron.",
                    "label": 0
                },
                {
                    "sent": "And has been modeled very coarsely by Immaculatus in 1943, and that's still the model of a neuron which is most used in machine learning, even though you know people have made many refinements and this very coarse model.",
                    "label": 0
                },
                {
                    "sent": "Is just a simple linear model.",
                    "label": 0
                },
                {
                    "sent": "Where is the only difference that you have?",
                    "label": 0
                },
                {
                    "sent": "So called activation function at the output, so the unit makes a weighted sum.",
                    "label": 0
                },
                {
                    "sent": "Of its input.",
                    "label": 0
                },
                {
                    "sent": "And then this weighted sum goes through what people call it, squashing function or activation function.",
                    "label": 0
                },
                {
                    "sent": "So that the input is bounded between between two values.",
                    "label": 0
                },
                {
                    "sent": "The output sorry is bounded between two values and as a special case the activation function can be just a step function.",
                    "label": 0
                },
                {
                    "sent": "So making a decision between two values or low value and high value, in which case the neuron is in a way be performing a classification task, deciding whether you are in class 0 or in Class 1.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So all these am linear methods.",
                    "label": 0
                },
                {
                    "sent": "End up making.",
                    "label": 0
                },
                {
                    "sent": "A decision which separates the two classes according to linear decision boundary.",
                    "label": 1
                },
                {
                    "sent": "Here I'm showing you a scatter plot and will be seeing you in the Class A lot of these scatter plots.",
                    "label": 0
                },
                {
                    "sent": "It's a simple representation of a classification problem for two classes in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "So usually you have many many features.",
                    "label": 0
                },
                {
                    "sent": "In some problems like DNA microarray data, each coefficient represents the activity of a gene and there are, you know, 10s of thousands of genes.",
                    "label": 0
                },
                {
                    "sent": "So you can have 10s of thousands of features or in text processing you sometimes have 100,000 or more.",
                    "label": 0
                },
                {
                    "sent": "Features because each feature represents frequency of word over word in a large vocabulary.",
                    "label": 0
                },
                {
                    "sent": "But in order to understand I'll get an intuition of machine learning.",
                    "label": 0
                },
                {
                    "sent": "Algorithms people often use only two features and they will represent problems in these two dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And for example, you can imagine that X one is the age of a patient.",
                    "label": 0
                },
                {
                    "sent": "Annex two is the weight of a patient.",
                    "label": 0
                },
                {
                    "sent": "And the little symbols here represent patients, so you can have patients that have a high weight and and are more senior and those would be represented by this population of red stars and the younger patient and lower weight patient that will be presented by the population of the circles here.",
                    "label": 0
                },
                {
                    "sent": "And the problem is to find a function that using simply Agent weight, allows you to discriminate between these two populations that are, for example, the patients which are at risk of heart disease and the patients which are healthy.",
                    "label": 0
                },
                {
                    "sent": "If you manage to separate these two populations with a single simple line like that, then you're in good shape that it means that with a simple linear function you can make this separation so you can compute a weighted sum of the first feature and the second feature, and make a decision according to that weighted sum.",
                    "label": 0
                },
                {
                    "sent": "So here I'm showing an example in three dimensions, so usually you don't go to three dimensions to give examples because it starts being difficult to visualize, but I just want to illustrate the fact that the linear decision boundary in two dimension is a line and a linear decision boundary in three dimension is a plane an in the higher dimension it's going to be a hyperplane, which is a subspace of dimension N -- 1, and being the number of features that you.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is now the perceptron, the second simplest.",
                    "label": 0
                },
                {
                    "sent": "This learning machine that you can imagine that was invented by back in 1957 by Rosenblatt.",
                    "label": 0
                },
                {
                    "sent": "And it's exactly like a linear.",
                    "label": 0
                },
                {
                    "sent": "Learning machine, except that as I mentioned before, you replace the inputs by transformed inputs.",
                    "label": 0
                },
                {
                    "sent": "These five functions, so each.",
                    "label": 0
                },
                {
                    "sent": "Find I of X is a function which is computed from all the original inputs eventually from all the original inputs, not necessarily from all of them.",
                    "label": 0
                },
                {
                    "sent": "And you obtain now a new vector of dimension began when the number original number of features was small and and you're waiting these new features and then OK, you eventually perform a decision with a hard threshold to classify in two categories.",
                    "label": 0
                },
                {
                    "sent": "What do you know I'm mentioning here in terms of classification, saying that you are performing a decision with the threshold here can be also valid in most cases for regression, in which case you remove.",
                    "label": 0
                },
                {
                    "sent": "You know this question function here an F of X is directly predicting your outcome.",
                    "label": 0
                },
                {
                    "sent": "It's going to be a continuous value.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The advantage of perceptrons is that they have a non linear decision boundary, so in a sense they are more powerful because they allow you to explain perfectly some data which cannot be separated perfectly with a simple line like I'm showing here.",
                    "label": 0
                },
                {
                    "sent": "Any 3 dimensions you will get kind of a soft decision boundary that can have any shape.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Item.",
                    "label": 0
                },
                {
                    "sent": "Other just mentioned also kernel methods are also special case of these functions that are linear in the parameters and are very similar to perceptron, except that the five functions are replaced by the K functions, so graphically.",
                    "label": 0
                },
                {
                    "sent": "All the inputs of you know your say your new patients are being compared with the inputs of your training examples.",
                    "label": 0
                },
                {
                    "sent": "So X1 represents the vector of the first vector of features of the first patient and next to the vector of features of the second patient, etc.",
                    "label": 0
                },
                {
                    "sent": "So you're going to compute, in the simplest case, you would be computing the dot product.",
                    "label": 0
                },
                {
                    "sent": "The simple dot product between vector X of your new unknown patient.",
                    "label": 0
                },
                {
                    "sent": "With a vector X1.",
                    "label": 0
                },
                {
                    "sent": "Or presenting the patient one, but you can use any kind of similarity measure that compares your new patient from whom you want you know to do a dialogue that you want to diagnose with other patients, for which you know their health status, and this method carries a lot of similarity with the so-called nearest neighbor method, in which you're going to perform decision about the health status of the patient.",
                    "label": 0
                },
                {
                    "sent": "By comparing this patient with patients who minorities.",
                    "label": 0
                },
                {
                    "sent": "So if in the nearest neighbor technique, if your new patient is closest to a patient who is diseased, you will be deciding that the patient is disease.",
                    "label": 0
                },
                {
                    "sent": "Dan, if it's you know closer to a healthy patient, then you will think if the patient is healthy and the measure of closeness or similarity is ad hoc, it's determined according to your knowledge of the.",
                    "label": 0
                },
                {
                    "sent": "Of the domain, and so you have to choose the correct features and you have to choose how you compare two patients.",
                    "label": 0
                },
                {
                    "sent": "So people have gone from the simple, you know, one nearest neighbor method to K nearest neighbor, in which you have been comparing the non patient with a number of patients that are in your database.",
                    "label": 0
                },
                {
                    "sent": "And for example you take the three closest one and you vote among the three closest one to make the decision.",
                    "label": 0
                },
                {
                    "sent": "And then going one step further in 1964, as a man and collaborators have been proposing the potential function method and later, you know, has been.",
                    "label": 0
                },
                {
                    "sent": "This has been related to the person window method, which is, you know, an extension of the nearest neighbor method in a sense for classification, in which instead of taking the K nearest neighbors, you just put a little hat over.",
                    "label": 0
                },
                {
                    "sent": "The neighborhood and all the patients vote to make the final decision, but they vote with decaying weights according to how close they are to the new unknown patient.",
                    "label": 0
                },
                {
                    "sent": "And the decay varies in different ways depending on the kind of potential function or kernel as they're called.",
                    "label": 1
                },
                {
                    "sent": "Now, potential functions are, you know, some very special kinds of kernels that have not been used recently.",
                    "label": 1
                },
                {
                    "sent": "Because of some computational problems.",
                    "label": 0
                },
                {
                    "sent": "So here we have mapped, you know.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some of the most important learning machines, and now how do we train them?",
                    "label": 0
                },
                {
                    "sent": "So back you know, in earlier in the 20th century, HEB has.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Made some theory about how neurons learn in the brain.",
                    "label": 0
                },
                {
                    "sent": "And as you probably realized, you know in your studies most theories which work are linear, or at least you know the 1st order of the theory.",
                    "label": 0
                },
                {
                    "sent": "Of course is linear and the 1st order works quite well.",
                    "label": 0
                },
                {
                    "sent": "So that also applies to learning.",
                    "label": 0
                },
                {
                    "sent": "The 1st order approximation of Learning is a very simple method based on correlation.",
                    "label": 0
                },
                {
                    "sent": "And this is what happens in the brain, according to Hebb.",
                    "label": 0
                },
                {
                    "sent": "Neurons are connected to one another via junctions.",
                    "label": 0
                },
                {
                    "sent": "Through which some chemicals called neurotransmitters.",
                    "label": 0
                },
                {
                    "sent": "Are released and the.",
                    "label": 0
                },
                {
                    "sent": "The tip of the you know the output connection of the neuron is called the Axon.",
                    "label": 0
                },
                {
                    "sent": "So here we have the very symbolic representation of a neuron.",
                    "label": 0
                },
                {
                    "sent": "This input inputs to the neuron are called.",
                    "label": 0
                },
                {
                    "sent": "You know the input connections to the neuron are called dendrites, and the output is called excellent.",
                    "label": 0
                },
                {
                    "sent": "So kind of a wire and at the very top of the Axon.",
                    "label": 0
                },
                {
                    "sent": "Here you have junction called Synapse.",
                    "label": 0
                },
                {
                    "sent": "And so this is how the information gets transmitted from one neuron to another neuron.",
                    "label": 0
                },
                {
                    "sent": "So another neuron here will input, it's the output it's output through the Axon and gets connected.",
                    "label": 0
                },
                {
                    "sent": "Here at this level.",
                    "label": 0
                },
                {
                    "sent": "And this is the the learning rule that help has been deriving.",
                    "label": 0
                },
                {
                    "sent": "He has said that the synapse.",
                    "label": 0
                },
                {
                    "sent": "The synapse is going to adapt according to how much it's used, so the more often there is signal going through the synapse.",
                    "label": 0
                },
                {
                    "sent": "The wider in the way the channel is going to be, so you're using this channel a lot.",
                    "label": 0
                },
                {
                    "sent": "Then it's going to be easier to transmit information through that Channel if you're not using it a lot.",
                    "label": 0
                },
                {
                    "sent": "It kind of weakens.",
                    "label": 0
                },
                {
                    "sent": "So meaning that if there is correlation.",
                    "label": 0
                },
                {
                    "sent": "Between the activity of two neurons.",
                    "label": 0
                },
                {
                    "sent": "And so if it's synapses being used a lot for transmitting.",
                    "label": 0
                },
                {
                    "sent": "Then there is increase in the weight of the, so increasing the facility in the easiness of transmission here.",
                    "label": 0
                },
                {
                    "sent": "So this translates in this very simple rule.",
                    "label": 0
                },
                {
                    "sent": "If there is activity simultaneous activity between one neuron and the neuron to which it's connected, rain forced the synapse.",
                    "label": 0
                },
                {
                    "sent": "If there is no activity, decrease the synapse.",
                    "label": 0
                },
                {
                    "sent": "Now people have been using instead of having 01 outputs they've been using minus 1 + 1 outputs, which are not necessarily realistic in biologically.",
                    "label": 0
                },
                {
                    "sent": "When doesn't necessarily know, usually biologically that the neuron is active or inactive.",
                    "label": 0
                },
                {
                    "sent": "But practically, people do algebra instead of you know, doing exact biological modeling and they like having outputs that are negative or positive.",
                    "label": 0
                },
                {
                    "sent": "But that doesn't really change very much.",
                    "label": 0
                },
                {
                    "sent": "You can use the same kind of learning rule so you multiply the activity of the neuron and the activity of its neighbor.",
                    "label": 0
                },
                {
                    "sent": "So basically the output here is going to be multiplied by.",
                    "label": 0
                },
                {
                    "sent": "Some input of another neuron.",
                    "label": 1
                },
                {
                    "sent": "The index I indicates 11 neuron and index J indicates another neuron, so here I'm representing only one neuron, right?",
                    "label": 0
                },
                {
                    "sent": "So so I don't have indexes for four Y for simplicity.",
                    "label": 0
                },
                {
                    "sent": "So here's the update rule of habit just says that you know if you have correlation between activities then.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Increase the weight.",
                    "label": 0
                },
                {
                    "sent": "So the reason I'm presenting this very simple rule that is not used so much in practice instead, except that you need to represent well the naive bias approach.",
                    "label": 0
                },
                {
                    "sent": "Is that you can very easily understand with this simple rule how the kernel trick works.",
                    "label": 1
                },
                {
                    "sent": "The kernel trick is the basis for a lot of algorithms or love kernel method algorithms.",
                    "label": 0
                },
                {
                    "sent": "And it shows that there is a correspondence between the perceptual representation and the kernel based representation.",
                    "label": 0
                },
                {
                    "sent": "So the two you know.",
                    "label": 0
                },
                {
                    "sent": "Learning machines I've been presenting you the perceptron.",
                    "label": 1
                },
                {
                    "sent": "And the kernel methods they can actually be representing the same decision function.",
                    "label": 0
                },
                {
                    "sent": "How is this happening?",
                    "label": 0
                },
                {
                    "sent": "Well, consider you know have this rule that I was just mentioning to you, but apply it to the perception.",
                    "label": 0
                },
                {
                    "sent": "So the weight vector.",
                    "label": 0
                },
                {
                    "sent": "Is going to be.",
                    "label": 0
                },
                {
                    "sent": "A weighted sum of the Y I5XI.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how is this?",
                    "label": 0
                },
                {
                    "sent": "If you go back one slide, you see that if you use this update rule, if you rain force always the weight according to the product of the Y, and you know whatever it's plugging into the XI J, then if you go to training examples.",
                    "label": 0
                },
                {
                    "sent": "In the end you will be having something which is proportional to a sum of the wire.",
                    "label": 0
                },
                {
                    "sent": "The YIXIJI running the summer running over all the I.",
                    "label": 0
                },
                {
                    "sent": "So for all the training sample, every time you show the new training example, you're going to rain force all the connections according to product of the outcome.",
                    "label": 0
                },
                {
                    "sent": "And the input.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in the end you just obtain a weighted sum.",
                    "label": 0
                },
                {
                    "sent": "So for the case of the simple heads, roll is just.",
                    "label": 1
                },
                {
                    "sent": "If you surely every example only once, you're going to be having the sum of all the examples of the Y, I5, XI.",
                    "label": 0
                },
                {
                    "sent": "So replacing the inputs by the five functions.",
                    "label": 0
                },
                {
                    "sent": "And so this special way of calculating the weight vector makes things very simple.",
                    "label": 1
                },
                {
                    "sent": "Because now since F of X is, you know the dot product between W and onfi.",
                    "label": 0
                },
                {
                    "sent": "Then you can plug in this form of W into here and expand and you obtain this knew.",
                    "label": 0
                },
                {
                    "sent": "Form of the decision function.",
                    "label": 0
                },
                {
                    "sent": "And very interesting Lee.",
                    "label": 0
                },
                {
                    "sent": "You can exhibit the fact that now your weighted sum is running over dot product's over your fi I which are the Phi computed for all the training examples and your 5X?",
                    "label": 0
                },
                {
                    "sent": "And so if you define now and you function.",
                    "label": 0
                },
                {
                    "sent": "K of XI&X as you know this dot product then you can transform your F of X into a sum now not over the five functions but over the K functions.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "You have shown here that by the mere fact that using this hedge rule, you obtained a weight vector, which is a weighted sum.",
                    "label": 0
                },
                {
                    "sent": "Of the original templates or general patterns.",
                    "label": 0
                },
                {
                    "sent": "So each example each training example enters with a positive weight.",
                    "label": 0
                },
                {
                    "sent": "If you're why I, you know, plus and minus one right.",
                    "label": 0
                },
                {
                    "sent": "If your outputs are binary, then this simply means that all the examples of the positive class are added.",
                    "label": 0
                },
                {
                    "sent": "All the examples of the negative class are subtracted.",
                    "label": 0
                },
                {
                    "sent": "And this is the way you compute your W vectors in a way it's very intuitive, right?",
                    "label": 0
                },
                {
                    "sent": "If you want to compute for each feature.",
                    "label": 0
                },
                {
                    "sent": "The weight with which you're going to be.",
                    "label": 0
                },
                {
                    "sent": "Calculating your decision function.",
                    "label": 0
                },
                {
                    "sent": "What you what you do is that you say, well, the wait is going to be the difference between the frequency of of how often I see my future positive and how often I see my future negative right?",
                    "label": 0
                },
                {
                    "sent": "So all the features positive, added together and all the negative ones are added, subtracted and this is how you calculate the weight.",
                    "label": 0
                },
                {
                    "sent": "Of course there are.",
                    "label": 0
                },
                {
                    "sent": "You know, much fancier ways of calculating the way this absolutely does not take into account possible redundancies, right?",
                    "label": 0
                },
                {
                    "sent": "If you have many examples that are similar, you don't necessarily want to all add them up.",
                    "label": 0
                },
                {
                    "sent": "You might want to know to consider that once you have seen one type of example, you don't want to again add more influence of that same example.",
                    "label": 0
                },
                {
                    "sent": "If you see it again.",
                    "label": 0
                },
                {
                    "sent": "But very roughly, this is.",
                    "label": 0
                },
                {
                    "sent": "This is what?",
                    "label": 0
                },
                {
                    "sent": "This is what you can do in the simplest way to compute these weights.",
                    "label": 0
                },
                {
                    "sent": "And the simple fact that you have you know this weighted sum of the examples and that generates you know your weight vector makes it that you can.",
                    "label": 0
                },
                {
                    "sent": "Plugging into into this serve FX represent things in these two different ways.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is an important consequence.",
                    "label": 0
                },
                {
                    "sent": "This is you can do it.",
                    "label": 0
                },
                {
                    "sent": "You know you can do it the other way around.",
                    "label": 0
                },
                {
                    "sent": "If you go into the in that direction, if you take the perceptual representation and go into the kernel representation, it's not that interesting, right?",
                    "label": 0
                },
                {
                    "sent": "Why would you do that?",
                    "label": 0
                },
                {
                    "sent": "There might be some computational reason why you want to do that, but generally it doesn't buy you much because you've already computed these five functions.",
                    "label": 0
                },
                {
                    "sent": "For example, you have a pattern recognition problem you have.",
                    "label": 0
                },
                {
                    "sent": "You know images and you want to extract features like you know edges, corners or small arcs, and these are all your files X.",
                    "label": 0
                },
                {
                    "sent": "What is more interesting actually to go in the other direction?",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "Well, because you can define now.",
                    "label": 0
                },
                {
                    "sent": "Instead of defining features, you just have a simple similarity measure between examples and known examples that you have in your database.",
                    "label": 0
                },
                {
                    "sent": "And the similarity measures are very flexible.",
                    "label": 0
                },
                {
                    "sent": "Features are OK, extracting features OK, and we will be doing that in one of the class.",
                    "label": 0
                },
                {
                    "sent": "But similarity measures are very powerful because you can define also similarity measures between objects that are not vectors.",
                    "label": 0
                },
                {
                    "sent": "So you can define similarity measures between, say, molecules that are of different shapes.",
                    "label": 0
                },
                {
                    "sent": "Or you can define the similarity measures between.",
                    "label": 0
                },
                {
                    "sent": "Between.",
                    "label": 0
                },
                {
                    "sent": "Spoken words that have different lengths.",
                    "label": 0
                },
                {
                    "sent": "Anile not necessarily easy to represent as vectors.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is that once you have a similarity measure like that, you can go back.",
                    "label": 0
                },
                {
                    "sent": "To the perception form.",
                    "label": 0
                },
                {
                    "sent": "And you may think you know why would we want to go back.",
                    "label": 0
                },
                {
                    "sent": "Well, because we can find optimization algorithms that learn the problem in this.",
                    "label": 0
                },
                {
                    "sent": "Oops, sorry.",
                    "label": 0
                },
                {
                    "sent": "In this file space and and optimize.",
                    "label": 0
                },
                {
                    "sent": "In some sense you know the.",
                    "label": 0
                },
                {
                    "sent": "Uh, the the the the way the decision is performed in that space.",
                    "label": 0
                },
                {
                    "sent": "And you can do that without explicitly computing the file.",
                    "label": 0
                },
                {
                    "sent": "Just by the mere fact that you know that you can define the K as a Phi and Phi can be can remain in place it and find some cases can even have an infinite dimension.",
                    "label": 0
                },
                {
                    "sent": "So this is, you know the so called kernel trick that you'll be hearing a lot about.",
                    "label": 1
                },
                {
                    "sent": "I think in these two weeks in order to derive many kernel method.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algorithms?",
                    "label": 0
                },
                {
                    "sent": "So what is a kernel?",
                    "label": 1
                },
                {
                    "sent": "So kernel is a similarity measure.",
                    "label": 1
                },
                {
                    "sent": "It's a dot product in some space.",
                    "label": 0
                },
                {
                    "sent": "And sometimes you don't know what the space is.",
                    "label": 0
                },
                {
                    "sent": "The fire presentation is unknown, so here are a couple of examples of kernels.",
                    "label": 0
                },
                {
                    "sent": "The Goshen kernel just computes the Euclidean distance between two vectors.",
                    "label": 0
                },
                {
                    "sent": "And the exponential it's the the square of it, so it's what it does is that if you have an unknown example.",
                    "label": 0
                },
                {
                    "sent": "It puts kind of a hat around it though.",
                    "label": 0
                },
                {
                    "sent": "Now take a take.",
                    "label": 0
                },
                {
                    "sent": "An example of training example and the KFC.",
                    "label": 0
                },
                {
                    "sent": "So if T represents the non example or the known example, then the annex is and S is the.",
                    "label": 0
                },
                {
                    "sent": "A non example then as you go away from the known example, the K of of S&T decays.",
                    "label": 0
                },
                {
                    "sent": "So this is what I explained you before that.",
                    "label": 0
                },
                {
                    "sent": "As you go away from the center of the kernel.",
                    "label": 0
                },
                {
                    "sent": "There is a decay, so you are going to be able to use this kernel.",
                    "label": 0
                },
                {
                    "sent": "To A to vote and the voting power of a given pattern is going to be less and less as you go away from the unknown example.",
                    "label": 0
                },
                {
                    "sent": "But you have other kernels that are not like that that are not centered.",
                    "label": 0
                },
                {
                    "sent": "An indicating, but still you can use the same type of learning algorithm and this is the case of the polynomial kernel and it has a weird shape.",
                    "label": 0
                },
                {
                    "sent": "You just do the dot product annual raise it to a given power.",
                    "label": 0
                },
                {
                    "sent": "So without going into detail because you will see that in other classes.",
                    "label": 0
                },
                {
                    "sent": "Using this kernel allows you to perform a function which instead of being a linear function, is a polynomial function.",
                    "label": 0
                },
                {
                    "sent": "Polynomial.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With respect to the inputs.",
                    "label": 0
                },
                {
                    "sent": "And one last learning machine.",
                    "label": 0
                },
                {
                    "sent": "I would like to mention is the 2nd last is the multilayer perceptron in which you just combine many neurons.",
                    "label": 1
                },
                {
                    "sent": "You put them, you stack them.",
                    "label": 0
                },
                {
                    "sent": "You know in a in a structure where you know there is one layer of neuron that is going to be the input of another layer of neuron.",
                    "label": 0
                },
                {
                    "sent": "Or",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Originally, the multilayer perceptron was.",
                    "label": 0
                },
                {
                    "sent": "Designed to solve the so-called chess board problem.",
                    "label": 0
                },
                {
                    "sent": "In which people showed that people understood that when you have two classes that are arranged in this chess board way, you cannot separate them with a single linear decision boundary.",
                    "label": 0
                },
                {
                    "sent": "But if you have two linear decision boundary, then you can make perfect separation.",
                    "label": 0
                },
                {
                    "sent": "Which is how comes you know the idea of stacking several layers of these of these linear decision bound?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, yet another important.",
                    "label": 0
                },
                {
                    "sent": "Class of decision functions comes from tree classifiers.",
                    "label": 1
                },
                {
                    "sent": "So it's very different from the rest of what I've been explaining to you so far, but I cannot go without mentioning it.",
                    "label": 0
                },
                {
                    "sent": "Uh, here I mean, you know we are representing again the same scatter plots in each box.",
                    "label": 0
                },
                {
                    "sent": "Sorry, and so the way Decission classic tree classifiers work is that they are going to try progressively two separate examples along axis.",
                    "label": 0
                },
                {
                    "sent": "Of the feature space.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 feature or one input.",
                    "label": 0
                },
                {
                    "sent": "For example of the age of a patient and hear the weight of a patient in the other direction.",
                    "label": 0
                },
                {
                    "sent": "And the different colored dots represent two populations.",
                    "label": 0
                },
                {
                    "sent": "For example, you know the healthy and the disease patients, so we first first threshold is threshold on feature F2.",
                    "label": 0
                },
                {
                    "sent": "You can pretty well separate the two classes, but you still have quite a bit of errors.",
                    "label": 0
                },
                {
                    "sent": "So, but you are doing this first decision and then now you at the second level.",
                    "label": 0
                },
                {
                    "sent": "I'm showing the two half boxes that result from.",
                    "label": 0
                },
                {
                    "sent": "You know this first split.",
                    "label": 0
                },
                {
                    "sent": "And if these two have boxes, we can again perform a similar operation and finding a threshold that separates this time in the other direction.",
                    "label": 0
                },
                {
                    "sent": "And you see that there still remains some errors that still remain.",
                    "label": 0
                },
                {
                    "sent": "You know, some sub boxes which which are not pure which contain examples of the other class.",
                    "label": 0
                },
                {
                    "sent": "But you do that as much as you can until you know by recursively splitting until you know eventually you get little boxes in which you will have only examples of the same color.",
                    "label": 0
                },
                {
                    "sent": "You may want to stop before you have you know exactly pure nodes.",
                    "label": 0
                },
                {
                    "sent": "Both because you might be running out of features or because you don't want to just exactly fit your training data.",
                    "label": 0
                },
                {
                    "sent": "So we'll be talking about overfitting.",
                    "label": 0
                },
                {
                    "sent": "Overfitting means explaining perfectly the training examples, but not necessarily necessarily performing well on new unknown examples.",
                    "label": 0
                },
                {
                    "sent": "So then how do you use the decision tree with GNU examples?",
                    "label": 0
                },
                {
                    "sent": "Then you would be at the top of the tree showing a new example, an according to the threshold you've set on the feature that you chose at the first node.",
                    "label": 1
                },
                {
                    "sent": "For example here feature, I have two you would be deciding that to go right or to go left.",
                    "label": 0
                },
                {
                    "sent": "And then you would be looking at the second feature and the feature will tell you go right or go left etc until you get to the bottom and then when you get at the bottom you will decide according to the box in which you have fallen, whether it's going to be an example of one class or another class.",
                    "label": 0
                },
                {
                    "sent": "So if you fall into a bugs in which more of these examples, examples of training data are black, then you will say it's a black example.",
                    "label": 0
                },
                {
                    "sent": "So the objective that decision trees optimizes is to reduce entropy as much as possible as you go down the tree in order to achieve at the very bottom of the tree as much node purity as possible.",
                    "label": 0
                },
                {
                    "sent": "So at the bottom at the top you have, you know, boxes that have a lot of entropy because they contain labels of different classes, whereas at the bottom there is a little entropy because the nodes are pure.",
                    "label": 0
                },
                {
                    "sent": "There is less disorder.",
                    "label": 0
                },
                {
                    "sent": "There are in states in which almost all boxes are pure.",
                    "label": 0
                },
                {
                    "sent": "So this is a very quick.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Introduction to three classifiers and the next slide and showing how different kinds of you know decision boundaries you get with the different classifiers.",
                    "label": 0
                },
                {
                    "sent": "Here it's a three class problem, well known, one that was introduced by Fisher in 1936, but classifying different kinds of Iris is according to the length and the width of their petals or samples.",
                    "label": 0
                },
                {
                    "sent": "Here we are looking only at petals, so there is the length of the petrol versus the width of the pedal.",
                    "label": 0
                },
                {
                    "sent": "And you can see that there are three different kinds of viruses called setosa for chicken versicolor.",
                    "label": 0
                },
                {
                    "sent": "And you can try to separate these three.",
                    "label": 0
                },
                {
                    "sent": "Classes with linear classifiers are with tree classifiers.",
                    "label": 0
                },
                {
                    "sent": "I've haven't mentioned Gaussian mixtures, but this is easily understood.",
                    "label": 0
                },
                {
                    "sent": "You create decision boundaries that.",
                    "label": 0
                },
                {
                    "sent": "Our little good using a Goshen over each which is centered at the center of the cluster.",
                    "label": 0
                },
                {
                    "sent": "And these are lines that are of ice values of the oceans.",
                    "label": 0
                },
                {
                    "sent": "And so, when the you can draw the decision boundary here or here corresponding to equal values of the oceans.",
                    "label": 0
                },
                {
                    "sent": "And then the kernel methods.",
                    "label": 0
                },
                {
                    "sent": "This is the hero support vector machine, so the authors of you know here is one chapter of the book that they they made this example here you can see that you can perfectly fit the training data and have this very distorted decision boundary.",
                    "label": 0
                },
                {
                    "sent": "That makes a little notch here, so you don't.",
                    "label": 0
                },
                {
                    "sent": "You see on this example that you know it's up to you whether you want to believe that this is a good way of making decision and the tree classifier is making decisions according to.",
                    "label": 0
                },
                {
                    "sent": "Lines that separate vertically or horizontally because they're just setting thresholds on the features.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there is a so-called fit versus robustness tradeoff.",
                    "label": 1
                },
                {
                    "sent": "I told you that you know there is this problem, eventually overfitting.",
                    "label": 0
                },
                {
                    "sent": "What should we choose?",
                    "label": 0
                },
                {
                    "sent": "You know, a linear decision boundary, for example, or decision boundary that that fits very well the training examples.",
                    "label": 0
                },
                {
                    "sent": "Well, not if you've seen you data.",
                    "label": 0
                },
                {
                    "sent": "That are you not, that were not your training examples.",
                    "label": 0
                },
                {
                    "sent": "And I'm representing here as field stores and field circles.",
                    "label": 0
                },
                {
                    "sent": "If you're lucky, they're all going to fall on the right side of the decision boundary that you've inferred from your training example.",
                    "label": 0
                },
                {
                    "sent": "If you're unlucky, you're going to make mistakes and are you going to be more likely to make mistakes if you have a simple decision boundary like that, or if you have a more complex one.",
                    "label": 0
                },
                {
                    "sent": "So here I'm making one mistake of 1 blue example, which is classified on the red side.",
                    "label": 0
                },
                {
                    "sent": "And here I'm actually making many more errors, even though you know I was fitting very well.",
                    "label": 0
                },
                {
                    "sent": "The training examples.",
                    "label": 0
                },
                {
                    "sent": "With my new test examples.",
                    "label": 0
                },
                {
                    "sent": "I'm making tons of hours.",
                    "label": 0
                },
                {
                    "sent": "So obviously you know this is a made up example.",
                    "label": 0
                },
                {
                    "sent": "I could also have drawn the test examples so that they fall on the right side of the decision boundary in the nonlinear case.",
                    "label": 0
                },
                {
                    "sent": "But as it turns out, there are theoretical arguments that tell you that in general.",
                    "label": 0
                },
                {
                    "sent": "It's a better idea to use the simpler model and we'll go over that.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In future lectures.",
                    "label": 0
                },
                {
                    "sent": "Now there is another dimension that you can play with, which is the bias value.",
                    "label": 0
                },
                {
                    "sent": "As you can see here, you can put more weight on the errors on one class as you put weight on the air of the other class.",
                    "label": 0
                },
                {
                    "sent": "So if you think that making an error of classifying a blue example into the right class is worse than classifying and red example into the bill class, then you might want to share.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's your decision boundary.",
                    "label": 0
                },
                {
                    "sent": "A little bit upwards so that you have some safety margin and you're not going to be making errors or classifying blew into the right key.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, if it's the other way around, you might want to shift your decision boundary in the other direction.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In general, you can vary that bias value and monitor the tradeoff between the error making on the positive class in the area making on the negative class, and this is what's this the so called Roc curve or RC curve.",
                    "label": 0
                },
                {
                    "sent": "With the rocker is is you know, by varying the bias it plots the success rates of the positive class, also called hit rate versus 1 minus the.",
                    "label": 0
                },
                {
                    "sent": "The negative class success rate, or also so called the false alarm rate.",
                    "label": 1
                },
                {
                    "sent": "And the the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Larger the area under that curve is the better from the point of view of the classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "And this is one way people measure classification accuracy so as to be independent on the particular choice of the bias that you make.",
                    "label": 0
                },
                {
                    "sent": "The idea Roc Curve is this one.",
                    "label": 0
                },
                {
                    "sent": "It has an area of 1 and it's, you know, making no error on the negative class and then when you move the bias of the sense making no error on the positive class.",
                    "label": 1
                },
                {
                    "sent": "The random case when you're making you know you're flipping a coin and making decisions at random for the positive and negative class gives you the diagonal.",
                    "label": 0
                },
                {
                    "sent": "And so you get.",
                    "label": 1
                },
                {
                    "sent": "In that case an AUC of 0.5 and this is, you know, one actual Roc curve.",
                    "label": 0
                },
                {
                    "sent": "People measure.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In different ways in different domains.",
                    "label": 0
                },
                {
                    "sent": "So sometimes people measure lift curve instead of Roc curves an, in which case they plot.",
                    "label": 1
                },
                {
                    "sent": "You know the fraction of good customers versus the fractions of customer selected.",
                    "label": 1
                },
                {
                    "sent": "I don't want to go into detail of that, but you shouldn't confuse both right?",
                    "label": 0
                },
                {
                    "sent": "It's just you're putting a little bit something different.",
                    "label": 0
                },
                {
                    "sent": "Here you're putting the hit rate versus the total fraction of.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "People selected.",
                    "label": 0
                },
                {
                    "sent": "More generally, he's the real picture.",
                    "label": 0
                },
                {
                    "sent": "More generally, if you have two classes and making predictions, then you will have several types of errors.",
                    "label": 0
                },
                {
                    "sent": "The false positive, which are when you classifying positively an example, but it really was of a negative class, the false negative when you're classifying an example as negative, but it really was of the positive class and then you have the correct classifications, the true negative and true positive.",
                    "label": 0
                },
                {
                    "sent": "If you add up to negative plus false positive is the total number of negative examples.",
                    "label": 0
                },
                {
                    "sent": "False, negative and positive.",
                    "label": 0
                },
                {
                    "sent": "The total number of positive examples.",
                    "label": 0
                },
                {
                    "sent": "And then what I call register rejected examples or the total number of examples that you have classified as negative.",
                    "label": 0
                },
                {
                    "sent": "So when you made your decision so there's some of that column.",
                    "label": 0
                },
                {
                    "sent": "You have the selected examples, the ones that you have classified as positive according to your decision function.",
                    "label": 0
                },
                {
                    "sent": "And this is the overall total.",
                    "label": 0
                },
                {
                    "sent": "So using these quantities you can compute virtually all the measurements that people used to assess.",
                    "label": 0
                },
                {
                    "sent": "Performance of classifiers.",
                    "label": 0
                },
                {
                    "sent": "So you talk about false alarm or false alarm rate as the fraction of false positive over the total number of negative examples.",
                    "label": 1
                },
                {
                    "sent": "Negative meaning belonging to the negative class.",
                    "label": 0
                },
                {
                    "sent": "And he tried is the true positive over the total number of positive examples?",
                    "label": 0
                },
                {
                    "sent": "And the fraction of selected is the number of selected over the total number of examples.",
                    "label": 0
                },
                {
                    "sent": "OK, you have also precision, so using this table you'll get a picture of all the types of measurements.",
                    "label": 0
                },
                {
                    "sent": "Error rate is false.",
                    "label": 0
                },
                {
                    "sent": "Negative plus false positive or total number of examples.",
                    "label": 0
                },
                {
                    "sent": "Instead of using her right, you can use the couple hit rate false alarm rate or hit rate precision or hit rate fraction of selected so people depending on the domain people use different different things.",
                    "label": 1
                },
                {
                    "sent": "You can use balanced error rate, which is the average of the errors of the positive class and over the negative class.",
                    "label": 0
                },
                {
                    "sent": "Um, in, you know, text processing.",
                    "label": 0
                },
                {
                    "sent": "People use the F measure which I'm writing here for memory.",
                    "label": 1
                },
                {
                    "sent": "And you can then also vary the threshold.",
                    "label": 0
                },
                {
                    "sent": "As I said before, and if you vary the threshold, you can plug the Roc curve.",
                    "label": 0
                },
                {
                    "sent": "You can plug the lift curve, or you can plug the precision recall curves, so this gives you know.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An idea?",
                    "label": 0
                },
                {
                    "sent": "What is a wrist functional?",
                    "label": 0
                },
                {
                    "sent": "So risk function is a function of the parameters of the learning machine, assessing how much it is expected to fail on a given task an.",
                    "label": 1
                },
                {
                    "sent": "I've given you examples already right for classification problem the error rate is going to be a function or 1 minus the area under the Roc curve.",
                    "label": 0
                },
                {
                    "sent": "So whatever measurement or objective you said to yourself about the performance of your learning machine that is going to define for you are is functional.",
                    "label": 0
                },
                {
                    "sent": "For regression, people use most often the mean squared error.",
                    "label": 0
                },
                {
                    "sent": "So in that case you taking the output of your learning machine and the desired output.",
                    "label": 0
                },
                {
                    "sent": "Making the difference.",
                    "label": 0
                },
                {
                    "sent": "Squaring an averaging so it's the average discrepancy between the target we want to achieve and what you obtain from your learning machine.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do you need a raise function?",
                    "label": 0
                },
                {
                    "sent": "Well that's that's used for training, right?",
                    "label": 0
                },
                {
                    "sent": "So once you've defined how you are going to be measuring how well you're going to be performing, then you want to optimize it.",
                    "label": 0
                },
                {
                    "sent": "So you're going to vary the weights of your learning machine.",
                    "label": 0
                },
                {
                    "sent": "Such that you're going to be optimizing the rest functional.",
                    "label": 0
                },
                {
                    "sent": "You want to minimize the risk, and that can be done with a variety of methods, including gradient descent, mathematical programming, simulated annealing, genetic algorithms, those are all means of going down.",
                    "label": 1
                },
                {
                    "sent": "India Bix you know surface of the the functional defines in parameter space so this is a very simple drawing in which you know considering that there is only one parameter in your learning machine.",
                    "label": 0
                },
                {
                    "sent": "Your risk functional defines you know peaks and valleys and what you want is to find the minimum which is the W store your weight, optimal weight.",
                    "label": 0
                },
                {
                    "sent": "In reality you have a huge parameter space so you have a very complex.",
                    "label": 0
                },
                {
                    "sent": "High dimensional surface in which you're trying to do that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some isation.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to be continuing that in the next lecture, so we'll be defining risk functionals and defining ways of optimizing it.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Click rate in dissent.",
                    "label": 0
                },
                {
                    "sent": "And in summary, for you know this introduction lecture with linear threshold units that are analogous to neurons.",
                    "label": 1
                },
                {
                    "sent": "We can build many different kinds of.",
                    "label": 1
                },
                {
                    "sent": "Learning machines, including a linear discriminants functions which you know which are special cases.",
                    "label": 0
                },
                {
                    "sent": "The Naive Bayes algorithm.",
                    "label": 0
                },
                {
                    "sent": "Kernel methods that are all methods that are linear in the parameters, not necessarily in the input components.",
                    "label": 0
                },
                {
                    "sent": "Neural networks, which are nonlinear both in the parameters and the input components.",
                    "label": 0
                },
                {
                    "sent": "Decision trees that have also elementary nodes that make simple linear decisions.",
                    "label": 0
                },
                {
                    "sent": "They're just thresholds in that case.",
                    "label": 1
                },
                {
                    "sent": "And the the architecture of the learning machines have hyper local hyper parameters, of which we're going to be talking in the next lectures and those may include the choice of the basis functions.",
                    "label": 1
                },
                {
                    "sent": "So which five functions you're using in the perception which kernel?",
                    "label": 0
                },
                {
                    "sent": "Using in the kernel machine?",
                    "label": 1
                },
                {
                    "sent": "The number of units and how they are arranged?",
                    "label": 0
                },
                {
                    "sent": "In the case of a narrow network.",
                    "label": 1
                },
                {
                    "sent": "And learning meets fitness means fitting the parameters that are the weights and also the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "And one has to be aware of the fit versus robustness tradeoff, so it's not necessarily best to obtain a decision boundary that learns exactly well.",
                    "label": 0
                },
                {
                    "sent": "The training examples what needs to care, which we need to care of is how well we're going to be doing on our future examples that we haven't seen in training data, and in that respect, using a linear decision boundary is sometimes better than using a more complex non linear decision boundary.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if you want to know to learn more about machine learning, there are a lot of good sources which include the well known textbook of do the heart and talk on pattern classification.",
                    "label": 1
                },
                {
                    "sent": "The book of a hefty tip, Shirley and Friedman, the elements of statistical learning.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "There is a chapter that covers some of the material I've presented to you here in the perspective of, you know, going from perceptrons to kernel methods and vice versa, presenting some simple algorithms including the head algorithm that I've presented today and some other ones.",
                    "label": 0
                },
                {
                    "sent": "And then there is the this book I've mentioned before that I'm using for for the lectures that I'll be talking about this weekend.",
                    "label": 0
                },
                {
                    "sent": "That includes also a CD with the data of the challenge on feature selection, and so thanks very much for your attention and we'll see you next.",
                    "label": 0
                },
                {
                    "sent": "Interrupt her with questions.",
                    "label": 0
                },
                {
                    "sent": "Maybe I was talking too fast.",
                    "label": 0
                },
                {
                    "sent": "I was, you know, looking at the Clock and.",
                    "label": 0
                },
                {
                    "sent": "Postpone anybody wants to propose a clarification.",
                    "label": 0
                },
                {
                    "sent": "Suggest some comment.",
                    "label": 0
                },
                {
                    "sent": "Ask any question.",
                    "label": 0
                },
                {
                    "sent": "Interest.",
                    "label": 0
                },
                {
                    "sent": "First, I should ask a question first.",
                    "label": 0
                },
                {
                    "sent": "I don't know very much.",
                    "label": 0
                },
                {
                    "sent": "You know the audience yet, and perhaps I should have started with that before I talk, but.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Be asking you where are you from that which is so who?",
                    "label": 0
                },
                {
                    "sent": "Who in this class is.",
                    "label": 0
                },
                {
                    "sent": "Is preparing a PhD degree.",
                    "label": 0
                },
                {
                    "sent": "So who in this class is thinking of preparing a PhD degree?",
                    "label": 0
                },
                {
                    "sent": "And who in this case has already completed a 50 degree?",
                    "label": 0
                },
                {
                    "sent": "So not not so many.",
                    "label": 0
                },
                {
                    "sent": "So many people.",
                    "label": 0
                },
                {
                    "sent": "Most of you are actually preparing PhD, so I'm assuming actually for most of this material must must have been easier.",
                    "label": 0
                },
                {
                    "sent": "How many of you are working on machine learning already?",
                    "label": 0
                },
                {
                    "sent": "Good Sir, so how many of you thought you know this?",
                    "label": 0
                },
                {
                    "sent": "This introduction was easy to understand.",
                    "label": 0
                },
                {
                    "sent": "Who is not necessarily a bigot.",
                    "label": 0
                },
                {
                    "sent": "Intersection.",
                    "label": 0
                },
                {
                    "sent": "So how many of you had heard before of kernel methods?",
                    "label": 0
                },
                {
                    "sent": "How many of you have worked on neural networks?",
                    "label": 0
                },
                {
                    "sent": "How many of you have worked on decision trees?",
                    "label": 0
                },
                {
                    "sent": "How many of you have worked on?",
                    "label": 0
                },
                {
                    "sent": "What did I talk about?",
                    "label": 0
                },
                {
                    "sent": "How many of you have heard about overfitting?",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so that that gives me a little bit of idea.",
                    "label": 0
                },
                {
                    "sent": "So how many of you understands after this lecture?",
                    "label": 0
                },
                {
                    "sent": "What is a risk functional?",
                    "label": 0
                },
                {
                    "sent": "OK. How many of you understands?",
                    "label": 0
                },
                {
                    "sent": "What is a linear decision boundary?",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so well, let's go back very quickly then.",
                    "label": 0
                },
                {
                    "sent": "And see.",
                    "label": 0
                },
                {
                    "sent": "How are we doing here?",
                    "label": 0
                },
                {
                    "sent": "So how many of you understands without it on these axes?",
                    "label": 0
                },
                {
                    "sent": "The Axis the X&Y here.",
                    "label": 0
                },
                {
                    "sent": "OK, so for those of you who understand looking at this.",
                    "label": 0
                },
                {
                    "sent": "Hang out with fast here.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looking at this matrix, which is the data matrix, OK, you have two dimensions, the number of features which are the number you know of input components and the number of examples.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the axis of these plots, I'm showing the number of features or inputs and the number of examples.",
                    "label": 0
                },
                {
                    "sent": "So this is the aspect ratio of the data of the data and the aspect ratio of the data is very important.",
                    "label": 0
                },
                {
                    "sent": "How many examples you have relative to the number of features and I'm going to play playing a lot with that in this class are going to be actually trying to take these.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Put matrix and shrink it in that direction.",
                    "label": 0
                },
                {
                    "sent": "Here this card features the features that are not important.",
                    "label": 0
                },
                {
                    "sent": "OK, there are other cases in which we might want to discard examples.",
                    "label": 0
                },
                {
                    "sent": "Usually you don't want to discard examples right?",
                    "label": 0
                },
                {
                    "sent": "Because examples are very useful.",
                    "label": 0
                },
                {
                    "sent": "The more examples you have, the better you can train, but in some cases you might want to group examples because if your matrix is very sparse and only few features at a time are active.",
                    "label": 0
                },
                {
                    "sent": "Then one example alone might not care enough information, so you might want to smooth things an average examples.",
                    "label": 0
                },
                {
                    "sent": "So that's something people do.",
                    "label": 0
                },
                {
                    "sent": "But in this class will be working mostly in that dimension.",
                    "label": 0
                },
                {
                    "sent": "How to compress data by removing features that are useless or redundant?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So how many of you understand this?",
                    "label": 0
                },
                {
                    "sent": "This graph here what it does?",
                    "label": 0
                },
                {
                    "sent": "So most of you have understood that these are the inputs.",
                    "label": 0
                },
                {
                    "sent": "The inputs are weighted by some weights, and in order to compute additional function.",
                    "label": 0
                },
                {
                    "sent": "So any number of features in the conventions that I'm using.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ann is going to be always number of features.",
                    "label": 0
                },
                {
                    "sent": "An M number of examples.",
                    "label": 0
                },
                {
                    "sent": "So it's very important to notice that.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because in the perceptron case you see that the sum runs over N. So here is big and because we have transformed first the inputs into new features.",
                    "label": 0
                },
                {
                    "sent": "So we have the original matrix which has small end here and then we're doing some transform and either we're going to increase the number of features or to decrease it.",
                    "label": 0
                },
                {
                    "sent": "In the case of the Kern.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Message actually we are usually increasing the number of features.",
                    "label": 0
                },
                {
                    "sent": "So what happens in the case of the kernel method is that the sum now runs over M. But",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because of the kernel trick that I've shown you before, you can go between the two or presentations.",
                    "label": 0
                },
                {
                    "sent": "You can transform the perceptron into a kernel method and vice versa.",
                    "label": 0
                },
                {
                    "sent": "And the reason why this is interesting is that then you can have.",
                    "label": 0
                },
                {
                    "sent": "Implicit feature spaces that are of infinite dimension.",
                    "label": 0
                },
                {
                    "sent": "Why would you want?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That well.",
                    "label": 0
                },
                {
                    "sent": "The you want some a lot of expressive power.",
                    "label": 0
                },
                {
                    "sent": "So if you plunge your original input into a very big feature space and you're going to be able to draw decision boundaries that are very complex in your original input space, whereas in the feature space it could be a linear decision function, so that's the main idea of the main trick you're going to be.",
                    "label": 0
                },
                {
                    "sent": "Taking your original feature space and for example, the first people working on that, they just made products of the input features.",
                    "label": 0
                },
                {
                    "sent": "So Rosenblatt, when he invented the perceptron, he had this idea.",
                    "label": 0
                },
                {
                    "sent": "Well, let's take random.",
                    "label": 0
                },
                {
                    "sent": "Let's take random features, some features at random, draw them at random, and next take an arbitrary Boolean function and compute some feature at random from some of the original features and do that many, many times.",
                    "label": 0
                },
                {
                    "sent": "So by using these random Boolean functions he expanded.",
                    "label": 0
                },
                {
                    "sent": "His original feature space into huge feature space and then he said, well, now that I have this huge feature space, I'm just going to be using a linear decision function and because he plunged his original data in a huge feature space then he could make a perfect separation of the training examples.",
                    "label": 0
                },
                {
                    "sent": "With just a simple linear separation, it's a linear separation by this hyperplane in a very very high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So you have big now began can be you know hundreds of thousands of these features that are computed with these random Boolean functions.",
                    "label": 0
                },
                {
                    "sent": "Now he did that to address the problem that people would say, well, linear decision functions are too simple.",
                    "label": 0
                },
                {
                    "sent": "They can't express the complexity of the data.",
                    "label": 0
                },
                {
                    "sent": "There are many datasets that are not linearly separable, that is.",
                    "label": 0
                },
                {
                    "sent": "If you take the training data, you can't make a 0 error separation.",
                    "label": 0
                },
                {
                    "sent": "You can't make an errorless separation that will put all the examples of one class on one side.",
                    "label": 0
                },
                {
                    "sent": "All the examples of the other class on the other side with a simple hyperplane, he said well.",
                    "label": 0
                },
                {
                    "sent": "No problem, I'm going to use these random Boolean functions.",
                    "label": 0
                },
                {
                    "sent": "Plunged the original space into a much bigger space and now I can separate perfectly the training data.",
                    "label": 0
                },
                {
                    "sent": "So this is the essential idea that's used by perceptions and also by kernel methods.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because you have this duality between the two representations, and it can be shown that for each valid kernel you can have an expansion into a feature space that eventually can be infinite, but you can prove you know mathematically that.",
                    "label": 0
                },
                {
                    "sent": "Such expansion exists.",
                    "label": 0
                },
                {
                    "sent": "And in that case, using a simple algorithm that separates that makes a linear decision boundary in this feature space you can obtain the power of nonlinear decision boundaries in the input space in the original space, right?",
                    "label": 0
                },
                {
                    "sent": "So here you don't get really this on this picture, but imagine that this is, you know, the original SpaceX 1X2 and imagine that this is 515 two, and that's why are features.",
                    "label": 0
                },
                {
                    "sent": "That are derived.",
                    "label": 0
                },
                {
                    "sent": "No, it's the opposite.",
                    "label": 0
                },
                {
                    "sent": "Sorry, this is.",
                    "label": 0
                },
                {
                    "sent": "This is the original space and this is the five space.",
                    "label": 0
                },
                {
                    "sent": "Eventually, you know, if you make a transformation that is powerful enough in the file space, you're going to be able to separate with the linear decisions.",
                    "label": 0
                },
                {
                    "sent": "That's the basic idea.",
                    "label": 0
                },
                {
                    "sent": "And so right.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Station.",
                    "label": 0
                },
                {
                    "sent": "Extra notes.",
                    "label": 0
                },
                {
                    "sent": "Examples of notice.",
                    "label": 0
                },
                {
                    "sent": "Under the features.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have two indexes.",
                    "label": 0
                },
                {
                    "sent": "And yeah, I haven't made a clear distinction in the notation.",
                    "label": 0
                },
                {
                    "sent": "When I use boldface.",
                    "label": 0
                },
                {
                    "sent": "A bold face character that indicates that it's a vector.",
                    "label": 0
                },
                {
                    "sent": "And then the index in the indicates the index of the example.",
                    "label": 0
                },
                {
                    "sent": "And when it's a regular phase, it's one input component.",
                    "label": 0
                },
                {
                    "sent": "So yes, in some cases people use, you know subscripts and superscripts to distinguish the two indices, but that's also can be confusing.",
                    "label": 0
                },
                {
                    "sent": "Confused, you know with the power, so in this case here, when it's not ambiguous, I use.",
                    "label": 0
                },
                {
                    "sent": "The index indicates either.",
                    "label": 0
                },
                {
                    "sent": "Um, this.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The pattern or the example.",
                    "label": 0
                },
                {
                    "sent": "So here you see that I have a lower case and I have two indices, so I is the index of pattern and J is the index of example.",
                    "label": 0
                },
                {
                    "sent": "In the matrix here.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the matrix notation.",
                    "label": 0
                },
                {
                    "sent": "Here I have two indices.",
                    "label": 0
                },
                {
                    "sent": "So I is the index of line indicating the example and J is the index of column.",
                    "label": 0
                },
                {
                    "sent": "And when I put a bold face and I put only one index, then I mean the pattern index.",
                    "label": 0
                },
                {
                    "sent": "Sorry if it's a little bit confusing.",
                    "label": 0
                },
                {
                    "sent": "There's a tradeoff between making the notation complex and or making it obvious.",
                    "label": 0
                },
                {
                    "sent": "Come.",
                    "label": 0
                },
                {
                    "sent": "If there is, you know if ever there is a ambiguity please new stop me and ask me but.",
                    "label": 0
                },
                {
                    "sent": "So in.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This in this plots here.",
                    "label": 0
                },
                {
                    "sent": "What is meant is that you're looking at only one example, so I'm not putting the index of the pattern.",
                    "label": 0
                },
                {
                    "sent": "And one means feature one and two means feature 2.",
                    "label": 0
                },
                {
                    "sent": "Is that clear to everybody or?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like a large number of features.",
                    "label": 0
                },
                {
                    "sent": "And then two leaders.",
                    "label": 0
                },
                {
                    "sent": "Explain yourself.",
                    "label": 0
                },
                {
                    "sent": "Separate data.",
                    "label": 0
                },
                {
                    "sent": "Not very well.",
                    "label": 0
                },
                {
                    "sent": "Explain.",
                    "label": 0
                },
                {
                    "sent": "Let me say this effects the output.",
                    "label": 0
                },
                {
                    "sent": "How how you figure out?",
                    "label": 0
                },
                {
                    "sent": "The how do you explain which feature, how, how, how much it is involved in the final decision.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's that's a very important thing, and there are some methods.",
                    "label": 0
                },
                {
                    "sent": "Do this more explicitly than others.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The linear methods do this very simply because each feature is weighted by a given weight, so you know that the importance of this feature here can be assessed to some extent.",
                    "label": 0
                },
                {
                    "sent": "You know by this way, particularly if the features are independent of one another.",
                    "label": 0
                },
                {
                    "sent": "If there are dependencies between features.",
                    "label": 0
                },
                {
                    "sent": "It's not that obvious.",
                    "label": 0
                },
                {
                    "sent": "It's not as obvious to interpret the weights, because imagine, for example, that feature X one and feature X-22 copies of the same feature.",
                    "label": 0
                },
                {
                    "sent": "In which case you could have, you know, a very small wait here in a very big way here, or vice versa.",
                    "label": 0
                },
                {
                    "sent": "And that would mean really match right?",
                    "label": 0
                },
                {
                    "sent": "And it's more complex if you have now complex dependencies or instead of just having repeats of features.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "This is why 1 one people.",
                    "label": 0
                },
                {
                    "sent": "One way people have been overcoming that problem is to transform features into spaces in which features are independent of one another.",
                    "label": 0
                },
                {
                    "sent": "In which case you know you can better interpret the weights.",
                    "label": 0
                },
                {
                    "sent": "Another case is you can do some sensitivity analysis.",
                    "label": 0
                },
                {
                    "sent": "You can wiggle the input and see how much it affects the outputs is another way people have been doing that and then that works also for methods that are different than linear methods.",
                    "label": 0
                },
                {
                    "sent": "And, you know, by asking that question you actually getting, you know, at the very heart of the problem of feature selection, because we're going to want to retain those features that impact most the output an you will see you know in the next lecture is that there is a broad variety of approaches to address that problem, and some of the approaches try to reverse engineer the system that generated the data.",
                    "label": 0
                },
                {
                    "sent": "Saying that you know the optimal.",
                    "label": 0
                },
                {
                    "sent": "Way of knowing how important a feature is is to understand.",
                    "label": 0
                },
                {
                    "sent": "How in the original system that generated the data it was you know wired to the output, but in most practical cases you can't reverse engineer the data generating system because you don't have enough examples to do that, or you don't have enough human knowledge to do that.",
                    "label": 0
                },
                {
                    "sent": "So you have to resort to some more or less meaningful approximations.",
                    "label": 0
                },
                {
                    "sent": "And sometimes you know.",
                    "label": 0
                },
                {
                    "sent": "You don't do as well as you should, but.",
                    "label": 0
                },
                {
                    "sent": "Are there other questions?",
                    "label": 0
                },
                {
                    "sent": "Every time.",
                    "label": 0
                },
                {
                    "sent": "Each week, the schedule of today and some local information.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show you how efficient we are.",
                    "label": 0
                },
                {
                    "sent": "We already managed to change the city before you got it.",
                    "label": 0
                }
            ]
        }
    }
}