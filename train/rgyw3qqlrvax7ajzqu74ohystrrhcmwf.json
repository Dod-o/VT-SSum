{
    "id": "rgyw3qqlrvax7ajzqu74ohystrrhcmwf",
    "title": "Independent Component Analysis",
    "info": {
        "author": [
            "Aapo Hyv\u00e4rinen, University of Helsinki"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "January 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Principal Component Analysis"
        ]
    },
    "url": "http://videolectures.net/mlss05au_hyvarinen_ica/",
    "segmentation": [
        [
            "Our speaker is Apple and he'll be talking about independent component analysis and this will be for one hour 45 minutes.",
            "OK, hello everybody so.",
            "Basically, my my fundamental goal is to talk about independent component analysis, but to begin with I will first use some slides stolen from my coauthor jarma.",
            "Hurry to talk about some back some kind of background on things like principal component analysis, which is often confused with ICA.",
            "An other kinds of stuff.",
            "So."
        ],
        [
            "This is the contents of this introductory part, which takes something like one hour, 2 hours.",
            "Actually, I have no idea, perhaps depending also on your questions, you may I interrupt me at anytime and ask, because, well, I suppose it is.",
            "I have.",
            "I don't know very well what back what is your back level of background knowledge on these these subjects?",
            "I suppose it is also quite variable, so first I will show something some basic ICA examples.",
            "So what is ICA all about?",
            "And then I will go into this kind of stuff like motivated optimization and especially.",
            "Motivated study sticks and estimation theory.",
            "That's all the necessary theoretical back."
        ],
        [
            "So here's a very nice illustration, and probably I think also very impressive illustration of what ICA does.",
            "So here what we have.",
            "We have we.",
            "Originally we had six images that you will see in awhile, but here what we see is only linear mixtures of those images.",
            "So I mean this, this grayscale images have simply been linearly mixed by adding the grayscale values together with some certain random coefficients that we don't know.",
            "Now the point is that is it possible to find.",
            "The original images without knowing the coefficients of that of that mixing so that is the only data that we have is this thing here.",
            "So six mixed images and nothing else at all.",
            "So can we find the original images?",
            "Well, you may have here.",
            "He made this with ICA week on this when you do ICA."
        ],
        [
            "Compute the independent components, so ICA means independent component analysis.",
            "What you get is this 6 original images which.",
            "Some of them may look a bit weird.",
            "That's because they have been inverted.",
            "I mean, the polarity has been inverted.",
            "Black and white have been inverted.",
            "But still if you invent if you can invite them back.",
            "And then, well, you will see that.",
            "Actually, yes, you did get all of those originally 6 original images."
        ],
        [
            "So in terms of statistical estimation, what we have here is is latent variables model and this independent components are latent variables, latent meaning.",
            "Here we will have something about that's a bit later on.",
            "And so this mixing was completely linear, ICA ICA as I will consider.",
            "Here is a completely linear method in the sense of having a completely."
        ],
        [
            "Mixing.",
            "And here's another example.",
            "Well, a bit more practical example, because you may not encounter that kind of linear mixing some images everyday, but this is really a practical practical example where we have recorded brain activity, electrical or electromagnetic brain activity using a sensor that has, I mean a large number of sensors, basically something like one or 200 sensors that are located on just outside of the head and.",
            "So here basically these three last ones are just for reference.",
            "I mean they are not really recorded, they are not really of interest here, but the point is that all these signals here.",
            "Given some of the outputs of these sensors an what you can see is that there's some kind of a.",
            "Kind of mixing of certain kinds of phenomena.",
            "You say that, like this kind of stuff here occurs in many different sensors, but well into table you would think that's this kind of this signals are mixture of a lot of different kinds of interesting original signals.",
            "A laser pointer you mean?",
            "Please sing OK.",
            "But this is also very nice.",
            "Kind of Alex symbol.",
            "So.",
            "So this is actually.",
            "This is really a real practical application of ICA people."
        ],
        [
            "I've done this kind of stuff for a couple of years and here's an example of what you might get.",
            "Here are some nine independent components that have been estimated from that, and if you analyze them further, you will see that there are actually some some of the independent components correspond to something like really independent sources of activity.",
            "Well, not necessarily in the brain.",
            "Some of these corresponds, for example heartbeat, and so on, well, but then you guys ICA will find some kind of underlying.",
            "Underlying sources of activity in this brain signals we will have more about this in there towards the end so."
        ],
        [
            "This lectures.",
            "So.",
            "ICA has found application in many different application areas recently.",
            "Here's another arbitrary list of those.",
            "Well, most probably the the most prominent and most successful application has been in analyzing this kind of a biomedical signals in techniques called EG or MG, which are about measurement of brain activity and MRI and fMRI, which are measurements of well also partly about brain activity.",
            "Or then something like about brain anatomy.",
            "But also you can do.",
            "It's not necessarily.",
            "It doesn't need to be related to the brain.",
            "You can measure, for example, electrical activity of the heart and that kind of stuff.",
            "And there I see seems to be successful then, well, my own speciality is mainly in computational neuroscience, where the idea is that we tried to model some kind of sensory processing in the brain.",
            "The idea is that the brain, the brain processes sensory information in a statistically highly sophisticated way and it happens that this.",
            "Sophisticated way is very very similar to independent component analysis.",
            "Then another application that certainly growing in importance recently is bioinformatics, especially transcript on analysis or gene expression analysis.",
            "I suppose we will have a talks on that here, but there's only so far there has been only a couple of papers published on this application, but I think that will become very important in the future and there's well, my coauthor then has written some rather rather arbitrarily, some further applications.",
            "The communication is this of course also one very big area for this kind of techniques."
        ],
        [
            "So now we will start.",
            "I will start looking at some background knowledge.",
            "Well, first I'm going to talk about optimization.",
            "Of course, optimization is a very complex issue, an issue and Nick will give us a lot of.",
            "Will give us a lot of details on how to optimize motivated functions, so here we have just so slides and optimization, one of the shortest courses and optimization that you could imagine so well.",
            "Of course the very point is that we have a function of many many variables.",
            "We are maximizing in a multi dimensional space.",
            "If you are maximizing in one dimensional space, things are rather trivial.",
            "You can well basically we just block the function and see where the maximum is.",
            "Well, you can also numerically find it rather easily, but when we have a multi dimensional space, let's say 101 thousand dimensions then.",
            "Things get very complicated.",
            "The fundamental basis for the for almost all these methods is to use the derivative or the gradients well, the gradient is basically the vector given by given by the different derivatives.",
            "The idea is that since the data that gives a local linear approximation of the change in that function well, then we can use that approximation and go to the direction where the app this approximation grows as fast as possible or decreases as fast as possible.",
            "If you want to minimize the function.",
            "So while basic basic color, basic differential calculus will tell us that the change when you add a small Delta W to the, so that's the argument.",
            "I mean when you move just a little bit in that space, that change will be proportional to the.",
            "Actually it will be equal and so the dot products of the derivative of the gradient times the change.",
            "So then it's obvious that to kind of go in the best direction you should actually go into the direction.",
            "Of these gradients.",
            "Because then this dot product will be positive and as large as possible, and so that gives rise to the gradient rule which simply says that you, as when you when you when you are in a certain point and you want to find a point when you want to find the point where this function is bigger than you, just add the gradient multiplied by a very small constant plus or minus.",
            "Here means well, it depends on whether you want to maximize or minimize your function."
        ],
        [
            "Then fix a bit complicated in practice for various reasons.",
            "One is that you usually have some kind of a constraints you don't want to maximize over all possible values in this N dimensional space, but you have some kind of constraints.",
            "For example, let's say that you want to maximize on the unit sphere.",
            "You say that's the norm of W should be equal to 1, something like that.",
            "In general, you usually express it as some kind of as an equation where you have some function of W equal to 0.",
            "So then the so called moon cuddles, cuddles, kromboom Tucker conditions tell you, OK Katie conditions tell you that.",
            "And that's in an optimal point.",
            "What you should have is that the gradient of this function to maximized is is proportional to the gradient of this constraint.",
            "In other words, gradient plus some constant times the gradient of the constraint equals 0.",
            "And well, yes.",
            "I mean this is really I mean, just giving the basic ideas, but then you can based on these ideas you can then develop different kinds of iterative methods which are well the basic methods are kind of.",
            "A slight variance of this of the gradient method, for example, projected gradient is simply based on the idea that you kind of go into into the direction of the gradient, but still you kind of always you always project the change onto these constraints surface on the surface where age of W = 0.",
            "OK, that's what."
        ],
        [
            "So.",
            "Another short course on optimization theory.",
            "But that's, well, that's something that we.",
            "I suppose we always need in machine learning.",
            "Kind of kinds of stuff.",
            "Then the question is, that's what kind of functions are you actually going to optimize?",
            "Where do you get your objective functions?",
            "Well, in ICA we get them paid basically always using from principles of statistical estimation theory and that's what I'm going to talk about next.",
            "So I'm going to 1st some basic after some basic notations and very basic concept.",
            "I will talk about especially about the multivariate Gaussian or normal density, which is like the fundamental density in N dimensional spaces.",
            "So we are basically all we are all the time here talking about multivariate statistics, so I'm not going to give you examples of univariate densities like like like exponential densities or loveless density.",
            "So what other people have been talking about?",
            "This is all about multivariate statistics, which is again of course much more complicated than than one dimensional statistical modeling.",
            "And then then I will talk about principle component analysis, which is some kind of ancestor independent component analysis and then of the very important principle of statistical independence was when we took over independent component analysis.",
            "It is not.",
            "Perhaps it shouldn't be very surprising that statistical independence will be very important."
        ],
        [
            "So just some basic notation, so we have random variables.",
            "Random variables are variables who take random values or something like that.",
            "Well, there are.",
            "I think some somewhat more rigorous definitions available, but.",
            "But I seem to forget them.",
            "So random variables the distribution of a random variable is often described by your density function.",
            "Well, in the case which is, this is possible in the case where where we have random variables which take which take like all kinds of values on the real axis.",
            "For example, we're not going to talk about anything that takes discrete values in this in this lecture, so it is now we are never going to have something like the probability that a random variable equals zero or that it equals one.",
            "Everything is always continuous value than actually based in basically all cases.",
            "This random variables can take.",
            "All values on the real axis.",
            "And so then in that case we can express the probability distribution function using the density function.",
            "And then in particular we will be talking about random vectors, which means simply that we have large number of random random variables.",
            "So it's like kind of you could say call it multidimensional random variable, but usually random variables on the word variable here is restricted to 1 dimensional valuable so.",
            "And yes again, you can then.",
            "You can express the distribution of continuous valued random variables using a probability distribution function.",
            "Well, here.",
            "Michael author gives his shows how, how, the distribution, how the density function is related, so the cumulative distribution function, which is, well, a well known stuff in basic basic probability theory.",
            "But well, let's not."
        ],
        [
            "Going through that, yeah.",
            "OK.",
            "So.",
            "Now.",
            "Well after this stuff, the instincts like densities and so on.",
            "Certainly the most fundamental thing in probability theory and statistics is the expectation.",
            "Expectation is.",
            "Expectation well, in the basic case we talk about the mean, which is the expectation of a random vector itself.",
            "And well, there are different kinds of terminologies available, but then people often talk about expectation when they talk about some kind of the expectation of some of some function of this of this variant of this random vector.",
            "So the expectations are computed basically by an integral where you have well first this function of X and then multiplied by the probability density function, and then well, that's integrated over over the dimensional space.",
            "And so the most interesting expectations in addition to mean the correlations.",
            "Well, perhaps I shouldn't say most interesting once, but let's say the most basic ones actually in I see one of the points in ICA is that is that these correlations are just like the first step in analyzing random variables or random vectors and you should not be content in just using the correlations.",
            "So, um.",
            "I should perhaps here explain what is actually the meaning of correlations.",
            "Well, the correlations are basically tell you something about linear dependencies of these variables.",
            "So let's say that we want to predict X2.",
            "By X1 so this is classically expressed as a linear regression model that I'm sure you have all seen somewhere.",
            "We have a coefficient a here and then we have some kind of noise or residual to this random.",
            "So another point is that a tells us, well gives us what is loosely called the correlation between X1 and X2.",
            "Well, if it's if it's positive and large large in some sense, then it tells you that that's that's when that's X2 in a certain way follows X one, so that when X one is large, this one will be large, and when the next one is small, this one will be typical large.",
            "Now it turns out that if we normalize these variables properly.",
            "Then we will see actually if we normalize them.",
            "So if we normalize them so that the variances of X1 and X2.",
            "Both equal to 1.",
            "And well, just for those of you who don't remember, I will.",
            "I will remind you of the definition of variance, which is which is a measure of the spread of the random variable.",
            "So now what happens is that under these conditions this A equals the correlation equals expectation of X 1 * X Two.",
            "So it so this correlation tells you something about the dependencies between the variables, but also it is important to note in the context of ICA it is important to note that correlation is only tells you the dependants under the framework of this kind of a linear dependency.",
            "So correlation is really like the first step in the direction of analyzing the dependencies between these vectors.",
            "For example, if you wanted to know how the square of X X2 is related to the square of X1.",
            "Then you wouldn't be.",
            "Then you wouldn't be able to use this correlation anymore.",
            "You would need something completely different, but actually something like the correlation of of the squares.",
            "So I just also warn you that this daemonology here is very confusing in sense that, well, this definition of correlation is what people usually use in signal processing.",
            "In in statistics, when they talk about correlation, they usually use the word correlation coefficient and correlation coefficient is is a different thing from correlation.",
            "In this sense, it is actually a normalized version of this.",
            "Well, not even other normalized.",
            "But it's really.",
            "It's a rather different.",
            "Well, let's consider very different thing.",
            "Either it's it's kind of the same thing, but it has slightly different constraints.",
            "Actually.",
            "Well, we will see something about that on the next slide, so."
        ],
        [
            "So.",
            "In addition to covariance, I mean the correlation matrix.",
            "People talk about the covariance matrix, which is done, so that's basically you.",
            "Subtract the mean of each of the of each of the variables.",
            "And then you compute the correlation matrix.",
            "So which means that if the mean is 0, then actually the covariance matrix is just the same thing as the color color correlation matrix.",
            "Now what is not mentioned here is this is the definition of the correlation coefficient, which is the same thing as the covariance divided by the variance is.",
            "I've sorry, the standard deviations of these variables, so it's kind of so the correlation coefficient is renormalized version of covariance.",
            "And now there's actually something even more confusing.",
            "Yes, first.",
            "Yes, the second moment matrix, yes exactly.",
            "Now maybe I should?",
            "Well this confused with you with this, but I have to now another very widespread terminology is to say that if the covariance is 0, then these variables are called uncorrelated.",
            "So uncorrelated does uncorrelated does not mean that correlation is zero, which would be very lot too logical I think.",
            "So uncorrelated means that covariant the covariance is zero.",
            "Well, Fortunately, Fortunately during all these lectures and in many other contexts as well, the expectations are all zero.",
            "I mean if expectations.",
            "So I mean the means means of the variables are zero, so then uncorrelatedness is the same thing as zero covariance, which is the same as.",
            "Zero correlation.",
            "But in some cases you know there will be some differences.",
            "And in that case also, it is the same as having a zero correlation coefficient.",
            "OK.",
            "Yes.",
            "OK, that's it.",
            "Oh yes, I'm sorry I made a mistake, so again here.",
            "Here this thing here.",
            "This would actually be the covariance.",
            "So.",
            "So actually we let's make here the assumption that the expectations are zero.",
            "Only then this thing.",
            "This thing here is correct.",
            "And that's why people talk about that.",
            "Then it's easy to talk about correlation.",
            "You know, in in all kinds of Sciences, social Sciences and Biological Sciences, people always talk about variables being correlated now.",
            "Then it well for them.",
            "It is usually a question of having a non zero correlation coefficients.",
            "Now if if this condition well having a non zero correlation coefficient is the same thing as having.",
            "A non zero covariance which means an non 0A.",
            "Which means that you can.",
            "You can like linearly predict one variable by the other one.",
            "Or I mean you can predict part of the variable by the other variable.",
            "People often talk about like percentage of variance which is predicted so that if there's like a weak correlation, then you can perhaps predict 10% of the variance of the other variable.",
            "If the correlation is is extremely good, then you can.",
            "Perhaps you can predict maybe 90% of the variance, and so if the variables uncorrelated then you can predict 0% of the variance."
        ],
        [
            "So here's a classical example we have here a certain number of people and we have their weights in kilograms and the Heights in centimeters.",
            "And you can then compute the covariance matrix so in the covariance matrix.",
            "But actually the diagonal elements are nothing but the variance is an.",
            "It is off diagonal elements that tell you that.",
            "Give the covariances and tell you about the dependencies between the variables.",
            "So what you see here is that there is a rather substantial dependency.",
            "I mean rather substantial correlation between the Heights and weights of the persons, which is not at all surprising, of course.",
            "This is this is seen in the fact that the covariance is larger than 0.",
            "Another thing that you can see is that one.",
            "You can also compare the variances if you like well.",
            "Then well, that is perhaps not so so interesting."
        ],
        [
            "What people usually used to model."
        ],
        [
            "This kind of data also also this kind of a data.",
            "I mean this is.",
            "This is kind of a very typical kind of cloud as some people would call it, call it a Gaussian cloud because it's kind of a kind of an ellipse."
        ],
        [
            "Check out cloud.",
            "It is very often, well, almost always modeled by this kind of a Gaussian densities.",
            "So this is this is this is a contour plot of a Gaussian profile multivariate Gaussian density where the para meters have been chosen so that it's kind of slightly similar to this real example, so here.",
            "So this is this is these are just two representations of the same distribution, So what you will see is that the contours are elliptical.",
            "That's always true for Gaussian, Gaussian multivariate Gaussian densities.",
            "So it will give you if you take a sample from this thing.",
            "You will typically get some cloud that looks something like elliptical."
        ],
        [
            "And then there's you know, some.",
            "Some points which I would further."
        ],
        [
            "From the center.",
            "And well, yeah he is just the same thing in presented."
        ],
        [
            "In different way.",
            "An well the mathematical expression for that kind of a density is given here.",
            "So the probability density function of X is some constant.",
            "Normalizing constant times the exponential of a certain kind of a quadratic form.",
            "So here what we from we subtract from X.",
            "It's mean actually MX.",
            "It's a parameter which turns out to be equal to the mean, and then we use.",
            "We use take a quadratic form.",
            "Multiplying by this X minus mean certain matrix from both sides.",
            "Now it turns out that this matrix here is the inverse of the covariance matrix, so we will see we see that this is an extremely.",
            "This is like one of the simplest just looking at it algebraically.",
            "You will see that this is one of the simplest imaginable densities that you can imagine in an N dimensional real space.",
            "What you see is that first of all these para meters, it has only two para meters.",
            "One is the mean which."
        ],
        [
            "Basically tells you like well."
        ],
        [
            "Center point here is and then we have the other matrix is the covariance."
        ],
        [
            "That basically tells you like what is the shape of this of these... what direction they are pointing at and how big they are."
        ],
        [
            "Kind of things.",
            "Well, usually in statistics, what we are what is more interesting than the densities themselves, is the logarithms.",
            "For reasons that will become clear in some in a moment.",
            "So what we're having here, so the logarithm of this probability density function is simply a quadratic form an well.",
            "The quadratic form is one of the simplest functions possible.",
            "Of course, the linear function would be even simpler, but we cannot have a linear function as the logarithm logarithm of a density function.",
            "A question, why can't we have a linear function as the PDF probability density function?",
            "Sorry.",
            "Yes, exactly has to integrate to one and a linear function would go to Infinity in one, in some other directions.",
            "Yeah, and so yeah, yeah the the.",
            "OK, the the meaning of this constant K is too nice to make sure that this probability density function integrates to one to one.",
            "It has a slightly ugly looking expression, but but still it can be computed for these.",
            "So another point to note well about the Gaussian distribution is that once you know the 1st and 2nd order statistics, that means the mean and the Cove area and the covariance matrix.",
            "Then you have completely specified the probability distribution function.",
            "It doesn't, you don't need any other information and any other information will not bring anything new about the distribution."
        ],
        [
            "OK. Do you have any questions at this point?",
            "Is this too trivial?",
            "Is this too difficult?",
            "So.",
            "The classic way to approach when you have.",
            "Data from a multidimensional and in dimensional distribution.",
            "The classic Classic Way to analyze one of the simplest ways is to do principal component analysis.",
            "Well, you can do things like cluster analysis for example, but well for data data clouds like this you will see that, well, cluster analysis is not very interesting because there's basically just one cluster.",
            "So what you can do, what people do is principal component analysis, which is closely related to the fact to answer that to the fact that people use a Gaussian model for modeling this kind of data.",
            "It has the mathematics are very very similar meta mathematics and the underlying assumptions are very similar.",
            "So principle, well, I think there will be some talks here who were principal component analysis described in much detail, but I have talked about it for awhile.",
            "Because otherwise he wouldn't be able to understand the contrast to independent component analysis an what is very new complaint with this, but it's really different in independent component analysis, so usually principal component analysis is is defined in a recursive manner.",
            "Well there are a couple of different ways, but let's start with the recursive manner.",
            "That is, we want to find a single principle component.",
            "A single principle component means a single signal direction in the data space.",
            "That is, that which is given by a single vector in that data space, which we didn't buy W. So so now the point is that.",
            "We we at proximates the data using the projection of the data onto this single principal components.",
            "Well, the projection, assuming that we normalize W so that its norm is equal to 1, the projection of the data of the projection of a single data point on.",
            "So this one dimensional subspace on this one line in the sand dimensional space is given by this this expression here.",
            "Now the question is.",
            "What is the best W. Yeah, well, the question is what is the best W?",
            "Well there are certainly many different answers on what is the best W you have to 1st define what is best.",
            "Well the way best is usually defined here is that what is the best W so that you can add you so that you approximate your data in the as well as possible.",
            "Approximation means that now the expected error or the average error.",
            "All between the real data and this projected upper Summated data is as small as possible.",
            "So now we have a typical optimization problem.",
            "We want to minimize this function, which is the approximation error expectation, meaning average over the whole whole datasets hold data distribution and we have.",
            "This is a constraint optimization optimization problem because we say that W has unit no.",
            "So the David."
        ],
        [
            "Action on how to do this is given here, so this was written by my coauthor, so I'm not quite sure if I understand it correctly myself, but so let's try to go through it.",
            "Maybe we can learn it so the point is that.",
            "First, we define we make a change of variables.",
            "Basically, instead of considering the variable W, we look at the orthogonal direction.",
            "Well, it's simple here, especially because we have only two dimensions.",
            "So once we define one dimension of W without being uniquely define the orthogonal direction.",
            "And so now we transform this objective function.",
            "Only sister fun.",
            "Yes, yes we express X, sorry.",
            "Yes, we expect Express X as the sum of the projections onto W&U.",
            "Because, well, that is equal to X because we only have 2 dimensions and and we have two orthogonal projections and then the approximation is just as given by the by the by the definition and so this these two terms cancel out.",
            "And what we have is is this kind of a thing.",
            "Now when you just express this norm well norm of a factor is the square of the norm of a vector is simply the transpose of the vector times the vector itself.",
            "So here we transpose select an effector itself.",
            "Now the point is that is that.",
            "Well, first of all.",
            "Well, here we we change the value, change the order of these things.",
            "I wonder if this is quite quite correct well.",
            "Yeah, it it.",
            "OK yeah, so now we try we commute these two terms here and then we take the expectation operator in here into the middle.",
            "We can do that because you is you is a constant and not random quantity.",
            "So then what we get is the expectation of XXXX transpose here in the middle.",
            "And.",
            "Yes, I somehow I don't quite get it, but I also trust Michael was it that is correct?",
            "Or do you think there's an error here?",
            "You transpose exist Skype, so you can actually.",
            "Yes, some of the use cancel out, but I somehow get the impression that.",
            "OK yes yeah.",
            "So it has to go somewhere.",
            "So that this thing here is equal to X * U T. Times UTX and then we commit those X T * U and then this thing will be.",
            "Multiply by.",
            "You right, yeah, so now these things here cancel each other.",
            "So because this is this is this is a scalar, so we can.",
            "We can actually put this here, and because the norm of U is equal to 1 then these things cancel each other.",
            "And what we are we can we can put the expectation operator here in the middle.",
            "Can you see this?",
            "And it's equal to this, yes?",
            "And so now what the expectation thing here is nothing but the the correlation matrix.",
            "So we see here two things.",
            "First of all, a principal component analysis basically boils down to looking at the correlation matrix or the covariance matrix.",
            "Usually this all zero mean variables.",
            "And the second thing is that it boils down to something like an analysis of some kind of quadratic forms well.",
            "The next next slide is then how to maximize this kind of sorry.",
            "Actually, here we want to minimize this quadratic function."
        ],
        [
            "Quadratic form.",
            "Now again, let's try to understand this.",
            "So what we want to minimize this kind of a quadratic form under the constraint that the norm is equal to 1.",
            "Now when we look at the gradient of this thing so well, this is.",
            "This is the condition.",
            "Sorry, I said you so your car or spoon two conditions.",
            "But actually I think that's kind of an exaggeration because that's those are the more general conditions that are used in with inequality constraints.",
            "So this is just the classical.",
            "Advantage condition so we will have that the gradient minor is proportional or I mean gradient minus some constant times the gradient of the of the constraint equals 0 and now it happens that the gradient.",
            "Well, OK here Michael author uses not the gradient, but the transpose of the gradient, which is more, properly speaking the derivative.",
            "But it doesn't make any difference really, so here the derivative is equal to its actually is the corresponding linear function.",
            "This is an interesting point that the gradient of a quadratic form is the corresponding linear function with the same matrix multiplied by two and then here.",
            "Well, we had the same thing, because this is also a quadratic form with.",
            "An identity matrix in the middle.",
            "So this this equations when we just put this, take the tools out and put this you on the other side.",
            "It gives you this.",
            "This is nothing but the definition of an eigenvalue of a matrix, so this this is actually a very general general phenomenon.",
            "Usually when we have some kind of a quadratic functions as quadratic forms, to maximize the solutions will be given by some kind of eigenvalues.",
            "Because you always get this set in the same kind of equations and here.",
            "Well, then it depends on what.",
            "Depending on the situation it you have different eigen values that you want.",
            "Sometimes you have largest eigenvalue.",
            "Sometimes the smallest eigenvalues, sometimes something in the middle.",
            "So what happens here is that we should actually find the the largest eigenvalue.",
            "Sorry in this case for you you should correspond to the smallest eigenvalue.",
            "Of the correlation matrix, which is a bit confusing.",
            "Actually W what we actually wanted to find in the 1st place, which was the original variable, corresponds to the largest eigenvalue."
        ],
        [
            "Well.",
            "Whatever the point is that we can do PCA by taking the correlation matrix and then computing the eigenvalues.",
            "And this I can the the the the eigenvector with the largest eigenvalue will give us this principle component.",
            "Now, as I said in the beginning, PC is basically recursive thing.",
            "So you first now we have just shown how to compute one principal component, but in an N dimensional space you will.",
            "You can define NN principal components, so the following ones are defined simply, so that's not so that's.",
            "So that we subtract the previous projection that we had in the 1st place from the data.",
            "So we reduce the dimension of the data by one and then do exactly the same thing.",
            "So what we're doing is that we are well in this 2 dimensional case.",
            "It is not very very obvious, but in an dimensional scale, in dimensional case we are successively by computing components that give us the best approximation best approximation.",
            "Of the data using this limited number of components, actually it can be proven that the first KIK principal components give the best activation of the data that is possible with a K dimensional subspace.",
            "Change.",
            "Sorry.",
            "Yes.",
            "Yes, it is one problem with PCA is that the result depends on the units of measurement.",
            "So.",
            "Yeah, make it Scandinavian.",
            "Yes again.",
            "So now we're talking about the coral Emma in the matrix correlation coefficients as defined in statistics.",
            "Yes, so correlation coefficient as defined here.",
            "An in signal processing has had the same problem of of.",
            "Depending on the that it depends on the scale.",
            "But yes, one the classical way of getting rid of this problem is to simply define the scale of your variables so that you so that you say that the variance of each variable has to be equal to 1.",
            "But yes, I mean that kind of solves the problem, but then I mean it.",
            "It gives you unique answer to the problem, but it may not be.",
            "It may not be there.",
            "The answer to the second question that you wanted to ask in the 1st place because in some cases the units units have an intrinsic meaning and you don't want to re scale everything.",
            "Yeah, but it depends on the situation.",
            "But as I said, the case first principle components give the best approximation of the data in the sense that when you project the data onto the subspace defined by those principal components, the the expected square error will be minimized.",
            "So that is why PCA is used for dimensionality reduction.",
            "So if you have data that has like 10,000 dimensions for example, and let's say that for that you want to do some computationally intensive methods.",
            "That cannot really cope with 10,000 dimensions and and you need to need to reduce the dimension of the data to be able to use your May.",
            "Your favorite method then.",
            "A PCA is the classical way of reducing the dimension.",
            "Now, if with PCA you can reduce the dimension to say 100, and you know that you have the best possible approximation of the original data that the best possible with 100 dimensions.",
            "Though of course this is, well, best is always extremely relative.",
            "People always prove some kind of optimality results for different kinds of methods, but of course what is optimal is all optimality is always in the eye of the beholder.",
            "I suppose one could say so we must remember, that's like PCA here for as defined here, has for example the constraint that it is completely a linear method.",
            "So then people some people have.",
            "Well, actually many people have been developing methods for nonlinear dimension reduction, which are something like nonlinear PCA methods.",
            "There are methods like hold for example, non metric multidimensional scaling, which is like the oldest group of methods and then the self organizing Maps curvilinear component analysis.",
            "Kernel PCA.",
            "And like he's on map or local linear embedding, which is which was developed by Sam.",
            "So it is certainly very best one available.",
            "And so and so on.",
            "And of course, you could also question the very objective of over this very measure of what is best approximation.",
            "So what we are talking about here is the approximation with respect to the Euclidean norm.",
            "But of course, you might argue that perhaps some other kind of a major might be better in under some circumstances, but in any case the the nice thing about PCA is that it is computationally extremely simple, because we can just do the eigenvalue decomposition of the covariance matrix, and that's one of the that's extremely simple compared to all other obvious nonlinear methods, and also compared to ICA and so on.",
            "In signal processing, people also talk about.",
            "Noise at the nation.",
            "As soon as a as a great benefit of PCA, there the idea is that, like you have a signal that leaves in some kind of low dimensional subspace called the signal subspace, and then you have noise all over like noise in all other directions, not which are which.",
            "Also also this signal subspace.",
            "So when you get rid of all those noise dimensions and just concentrate on the signal subspace then you probably get rid of a lot of noise as well.",
            "Well, that may be true.",
            "Or not, depending on circumstances.",
            "But really, the good thing about PCA is dimensionality reduction and I would say that that is really the only thing where we know that PCA is really good, like.",
            "The point is that people have trying to use PCA for things that actually ICA does properly, but which PCA does not do properly at all.",
            "That's PCA completely fails to do at all, so we have so.",
            "Even so.",
            "So we will see that PCs PCs put for dimensional reduction and not for many other things."
        ],
        [
            "So here's a classical example of this dimensional reduction.",
            "Property, so here we have an image.",
            "This is like the this is an image that we want to know.",
            "Compress compression means here that we take we divide the image into small squares, small windows and then we project all of those windows onto the low dimension lower dimensional subspace.",
            "But this is not very different from methods used in well known.",
            "Well, image compression methods as JPEG for example.",
            "They don't use PCA, but they use this idea of looking at small windows and."
        ],
        [
            "And dimension reduction.",
            "So if you do, if you take this kind of a small windows and then and sample the image and get all of all possible Windows non overlapping windows then you get then you can actually.",
            "Then you can compute the PCA for those for that kind of a data and what you get is this kind of basis vectors.",
            "These basis vectors are rather similar to something like 44 basis vectors, partly similar partly nodes.",
            "At least these first ones are something like.",
            "Not not very different."
        ],
        [
            "40 basis vectors.",
            "Now the point is that we can have different kinds of compression.",
            "Which is.",
            "Which means that by choosing by by ignoring a certain number, a certain percentage of the vectors.",
            "So now here this.",
            "In this case what we have done is to ignore 90% of the principle components and we have only taken the 10 most 10 first.",
            "That is the most important principle components as I think.",
            "So we have basically in a certain way you could say we could roughly say that we have reduced the amount of data by 90%, but as you can see it's not."
        ],
        [
            "Not terribly different from."
        ],
        [
            "The original image.",
            "You can see some kind of smoothing but not too much so."
        ],
        [
            "If you take more and more principle components, here we have."
        ],
        [
            "85% and here we have 50%.",
            "You will see that the picture gets better and better.",
            "Yes.",
            "If you sent me these 10% of the coefficients, how would I reconstruct?",
            "You also need to send me the basis.",
            "Yes, yes I do, yes.",
            "But of course it depending on circumstance.",
            "Single stance is the basis.",
            "The amount of coding needed for the basis may be quite negligible compared to the coefficients.",
            "Of course, this is also why people don't use PCA for real compression.",
            "Review it easy.",
            "Use the fixed basis sets.",
            "But I mean, the idea is that with PCA you could find some kind of optimal basis for doing that.",
            "But I mean you, you wouldn't compare the PCs Africa for each image.",
            "You could.",
            "For example, if you have a certain class of images to complete the PCA and then you you know, then it doesn't take much much storage."
        ],
        [
            "Yeah, here's the regional one again."
        ],
        [
            "OK, that was about PCA.",
            "Now when we talk about the independent component of this, we have to understand what this independence.",
            "And well, there are different kinds of independence.",
            "This is about statistical independence.",
            "It should.",
            "It should not be confused with linear independence, which has, which is a completely different phenomenon then.",
            "So step, mistaken independence basically means that when we have, let's say 2 random variables X&Y, when we know the value of X, that doesn't give us any information about the distribution of Y.",
            "Or in other words, no, X doesn't give us any information about why.",
            "To put it a bit short, So what this means to formulate this mathematically?",
            "Well, we use we just the concept of conditional distribution I. I hope that's that's familiar to everybody.",
            "Well, yeah, it must be some.",
            "I think some gave us a good introduction, so that kinda yeah.",
            "So we all know what is a conditional distribution.",
            "So this is a conditional distribution.",
            "Now for continuous valid valid variables.",
            "So it's a conditional probability density function, but it doesn't really make any difference whether we're talking about conditional probabilities for discrete variables or discrete or sorry, whether we're talking about conditional probabilities for discrete variables or conditional probability density functions.",
            "For continuous value variance.",
            "So the conditional probabilities of Y given X is defined by the joint probability density divided by the probability density of X for this given values of X&Y.",
            "So if we now if we say that X doesn't give any information, then it may be about why then it means that the conditional distribution all considered conditional density should be equal to the original density.",
            "Of why?",
            "Well, if you just now then multiply that this equation and multiply by PX of X and put it here.",
            "Then what you see is that this is the same thing as saying that the joint distribution factorizes.",
            "That means the joint distribution joint density is simply a product of the marginal densities of X&Y.",
            "And yes, this this.",
            "This lower equation here is the typical definition.",
            "While the formal definition of independence.",
            "And so as I talk about when I talked about here about correlations, I was saying that, well, yes, correlations are about some kind of a dependency between random variables.",
            "But the point is that correlation is only talk about one very particular form of dependencies.",
            "It is this linear dependence in the sense that when X one is large, then X2 is large when and X one is smaller than X2 is small.",
            "But independence here is something much.",
            "Stronger.",
            "So independence, statistical independence.",
            "I mean, so there are many different kinds of dependencies.",
            "So when we say the two variables are independent, it means that they have no kinds of dependencies there, so they don't have this kind of correlations, but they don't have any other kinds of dependencies either.",
            "So statistical independence actually implies that the covariance and correlation coefficients zero between those variables.",
            "But it is not true in the other way, just having zero or just being uncorrelated does not imply independence.",
            "We will talk more about that later when we actually get to the stuff on independent component analysis.",
            "If that ever happens."
        ],
        [
            "OK, so maybe now we have a short break and then continue."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our speaker is Apple and he'll be talking about independent component analysis and this will be for one hour 45 minutes.",
                    "label": 1
                },
                {
                    "sent": "OK, hello everybody so.",
                    "label": 0
                },
                {
                    "sent": "Basically, my my fundamental goal is to talk about independent component analysis, but to begin with I will first use some slides stolen from my coauthor jarma.",
                    "label": 0
                },
                {
                    "sent": "Hurry to talk about some back some kind of background on things like principal component analysis, which is often confused with ICA.",
                    "label": 0
                },
                {
                    "sent": "An other kinds of stuff.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the contents of this introductory part, which takes something like one hour, 2 hours.",
                    "label": 0
                },
                {
                    "sent": "Actually, I have no idea, perhaps depending also on your questions, you may I interrupt me at anytime and ask, because, well, I suppose it is.",
                    "label": 0
                },
                {
                    "sent": "I have.",
                    "label": 0
                },
                {
                    "sent": "I don't know very well what back what is your back level of background knowledge on these these subjects?",
                    "label": 0
                },
                {
                    "sent": "I suppose it is also quite variable, so first I will show something some basic ICA examples.",
                    "label": 1
                },
                {
                    "sent": "So what is ICA all about?",
                    "label": 0
                },
                {
                    "sent": "And then I will go into this kind of stuff like motivated optimization and especially.",
                    "label": 0
                },
                {
                    "sent": "Motivated study sticks and estimation theory.",
                    "label": 1
                },
                {
                    "sent": "That's all the necessary theoretical back.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a very nice illustration, and probably I think also very impressive illustration of what ICA does.",
                    "label": 0
                },
                {
                    "sent": "So here what we have.",
                    "label": 0
                },
                {
                    "sent": "We have we.",
                    "label": 0
                },
                {
                    "sent": "Originally we had six images that you will see in awhile, but here what we see is only linear mixtures of those images.",
                    "label": 1
                },
                {
                    "sent": "So I mean this, this grayscale images have simply been linearly mixed by adding the grayscale values together with some certain random coefficients that we don't know.",
                    "label": 0
                },
                {
                    "sent": "Now the point is that is it possible to find.",
                    "label": 0
                },
                {
                    "sent": "The original images without knowing the coefficients of that of that mixing so that is the only data that we have is this thing here.",
                    "label": 0
                },
                {
                    "sent": "So six mixed images and nothing else at all.",
                    "label": 0
                },
                {
                    "sent": "So can we find the original images?",
                    "label": 0
                },
                {
                    "sent": "Well, you may have here.",
                    "label": 0
                },
                {
                    "sent": "He made this with ICA week on this when you do ICA.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compute the independent components, so ICA means independent component analysis.",
                    "label": 0
                },
                {
                    "sent": "What you get is this 6 original images which.",
                    "label": 0
                },
                {
                    "sent": "Some of them may look a bit weird.",
                    "label": 0
                },
                {
                    "sent": "That's because they have been inverted.",
                    "label": 0
                },
                {
                    "sent": "I mean, the polarity has been inverted.",
                    "label": 0
                },
                {
                    "sent": "Black and white have been inverted.",
                    "label": 0
                },
                {
                    "sent": "But still if you invent if you can invite them back.",
                    "label": 0
                },
                {
                    "sent": "And then, well, you will see that.",
                    "label": 0
                },
                {
                    "sent": "Actually, yes, you did get all of those originally 6 original images.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in terms of statistical estimation, what we have here is is latent variables model and this independent components are latent variables, latent meaning.",
                    "label": 1
                },
                {
                    "sent": "Here we will have something about that's a bit later on.",
                    "label": 0
                },
                {
                    "sent": "And so this mixing was completely linear, ICA ICA as I will consider.",
                    "label": 0
                },
                {
                    "sent": "Here is a completely linear method in the sense of having a completely.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mixing.",
                    "label": 0
                },
                {
                    "sent": "And here's another example.",
                    "label": 0
                },
                {
                    "sent": "Well, a bit more practical example, because you may not encounter that kind of linear mixing some images everyday, but this is really a practical practical example where we have recorded brain activity, electrical or electromagnetic brain activity using a sensor that has, I mean a large number of sensors, basically something like one or 200 sensors that are located on just outside of the head and.",
                    "label": 0
                },
                {
                    "sent": "So here basically these three last ones are just for reference.",
                    "label": 0
                },
                {
                    "sent": "I mean they are not really recorded, they are not really of interest here, but the point is that all these signals here.",
                    "label": 0
                },
                {
                    "sent": "Given some of the outputs of these sensors an what you can see is that there's some kind of a.",
                    "label": 0
                },
                {
                    "sent": "Kind of mixing of certain kinds of phenomena.",
                    "label": 0
                },
                {
                    "sent": "You say that, like this kind of stuff here occurs in many different sensors, but well into table you would think that's this kind of this signals are mixture of a lot of different kinds of interesting original signals.",
                    "label": 0
                },
                {
                    "sent": "A laser pointer you mean?",
                    "label": 0
                },
                {
                    "sent": "Please sing OK.",
                    "label": 0
                },
                {
                    "sent": "But this is also very nice.",
                    "label": 0
                },
                {
                    "sent": "Kind of Alex symbol.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is actually.",
                    "label": 0
                },
                {
                    "sent": "This is really a real practical application of ICA people.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've done this kind of stuff for a couple of years and here's an example of what you might get.",
                    "label": 0
                },
                {
                    "sent": "Here are some nine independent components that have been estimated from that, and if you analyze them further, you will see that there are actually some some of the independent components correspond to something like really independent sources of activity.",
                    "label": 0
                },
                {
                    "sent": "Well, not necessarily in the brain.",
                    "label": 0
                },
                {
                    "sent": "Some of these corresponds, for example heartbeat, and so on, well, but then you guys ICA will find some kind of underlying.",
                    "label": 0
                },
                {
                    "sent": "Underlying sources of activity in this brain signals we will have more about this in there towards the end so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This lectures.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "ICA has found application in many different application areas recently.",
                    "label": 1
                },
                {
                    "sent": "Here's another arbitrary list of those.",
                    "label": 0
                },
                {
                    "sent": "Well, most probably the the most prominent and most successful application has been in analyzing this kind of a biomedical signals in techniques called EG or MG, which are about measurement of brain activity and MRI and fMRI, which are measurements of well also partly about brain activity.",
                    "label": 0
                },
                {
                    "sent": "Or then something like about brain anatomy.",
                    "label": 0
                },
                {
                    "sent": "But also you can do.",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily.",
                    "label": 0
                },
                {
                    "sent": "It doesn't need to be related to the brain.",
                    "label": 0
                },
                {
                    "sent": "You can measure, for example, electrical activity of the heart and that kind of stuff.",
                    "label": 0
                },
                {
                    "sent": "And there I see seems to be successful then, well, my own speciality is mainly in computational neuroscience, where the idea is that we tried to model some kind of sensory processing in the brain.",
                    "label": 0
                },
                {
                    "sent": "The idea is that the brain, the brain processes sensory information in a statistically highly sophisticated way and it happens that this.",
                    "label": 0
                },
                {
                    "sent": "Sophisticated way is very very similar to independent component analysis.",
                    "label": 0
                },
                {
                    "sent": "Then another application that certainly growing in importance recently is bioinformatics, especially transcript on analysis or gene expression analysis.",
                    "label": 0
                },
                {
                    "sent": "I suppose we will have a talks on that here, but there's only so far there has been only a couple of papers published on this application, but I think that will become very important in the future and there's well, my coauthor then has written some rather rather arbitrarily, some further applications.",
                    "label": 0
                },
                {
                    "sent": "The communication is this of course also one very big area for this kind of techniques.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we will start.",
                    "label": 0
                },
                {
                    "sent": "I will start looking at some background knowledge.",
                    "label": 0
                },
                {
                    "sent": "Well, first I'm going to talk about optimization.",
                    "label": 0
                },
                {
                    "sent": "Of course, optimization is a very complex issue, an issue and Nick will give us a lot of.",
                    "label": 0
                },
                {
                    "sent": "Will give us a lot of details on how to optimize motivated functions, so here we have just so slides and optimization, one of the shortest courses and optimization that you could imagine so well.",
                    "label": 0
                },
                {
                    "sent": "Of course the very point is that we have a function of many many variables.",
                    "label": 0
                },
                {
                    "sent": "We are maximizing in a multi dimensional space.",
                    "label": 0
                },
                {
                    "sent": "If you are maximizing in one dimensional space, things are rather trivial.",
                    "label": 0
                },
                {
                    "sent": "You can well basically we just block the function and see where the maximum is.",
                    "label": 0
                },
                {
                    "sent": "Well, you can also numerically find it rather easily, but when we have a multi dimensional space, let's say 101 thousand dimensions then.",
                    "label": 0
                },
                {
                    "sent": "Things get very complicated.",
                    "label": 0
                },
                {
                    "sent": "The fundamental basis for the for almost all these methods is to use the derivative or the gradients well, the gradient is basically the vector given by given by the different derivatives.",
                    "label": 0
                },
                {
                    "sent": "The idea is that since the data that gives a local linear approximation of the change in that function well, then we can use that approximation and go to the direction where the app this approximation grows as fast as possible or decreases as fast as possible.",
                    "label": 1
                },
                {
                    "sent": "If you want to minimize the function.",
                    "label": 0
                },
                {
                    "sent": "So while basic basic color, basic differential calculus will tell us that the change when you add a small Delta W to the, so that's the argument.",
                    "label": 0
                },
                {
                    "sent": "I mean when you move just a little bit in that space, that change will be proportional to the.",
                    "label": 0
                },
                {
                    "sent": "Actually it will be equal and so the dot products of the derivative of the gradient times the change.",
                    "label": 0
                },
                {
                    "sent": "So then it's obvious that to kind of go in the best direction you should actually go into the direction.",
                    "label": 0
                },
                {
                    "sent": "Of these gradients.",
                    "label": 0
                },
                {
                    "sent": "Because then this dot product will be positive and as large as possible, and so that gives rise to the gradient rule which simply says that you, as when you when you when you are in a certain point and you want to find a point when you want to find the point where this function is bigger than you, just add the gradient multiplied by a very small constant plus or minus.",
                    "label": 0
                },
                {
                    "sent": "Here means well, it depends on whether you want to maximize or minimize your function.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then fix a bit complicated in practice for various reasons.",
                    "label": 0
                },
                {
                    "sent": "One is that you usually have some kind of a constraints you don't want to maximize over all possible values in this N dimensional space, but you have some kind of constraints.",
                    "label": 0
                },
                {
                    "sent": "For example, let's say that you want to maximize on the unit sphere.",
                    "label": 0
                },
                {
                    "sent": "You say that's the norm of W should be equal to 1, something like that.",
                    "label": 0
                },
                {
                    "sent": "In general, you usually express it as some kind of as an equation where you have some function of W equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So then the so called moon cuddles, cuddles, kromboom Tucker conditions tell you, OK Katie conditions tell you that.",
                    "label": 0
                },
                {
                    "sent": "And that's in an optimal point.",
                    "label": 0
                },
                {
                    "sent": "What you should have is that the gradient of this function to maximized is is proportional to the gradient of this constraint.",
                    "label": 0
                },
                {
                    "sent": "In other words, gradient plus some constant times the gradient of the constraint equals 0.",
                    "label": 0
                },
                {
                    "sent": "And well, yes.",
                    "label": 0
                },
                {
                    "sent": "I mean this is really I mean, just giving the basic ideas, but then you can based on these ideas you can then develop different kinds of iterative methods which are well the basic methods are kind of.",
                    "label": 0
                },
                {
                    "sent": "A slight variance of this of the gradient method, for example, projected gradient is simply based on the idea that you kind of go into into the direction of the gradient, but still you kind of always you always project the change onto these constraints surface on the surface where age of W = 0.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Another short course on optimization theory.",
                    "label": 0
                },
                {
                    "sent": "But that's, well, that's something that we.",
                    "label": 0
                },
                {
                    "sent": "I suppose we always need in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Kind of kinds of stuff.",
                    "label": 0
                },
                {
                    "sent": "Then the question is, that's what kind of functions are you actually going to optimize?",
                    "label": 0
                },
                {
                    "sent": "Where do you get your objective functions?",
                    "label": 0
                },
                {
                    "sent": "Well, in ICA we get them paid basically always using from principles of statistical estimation theory and that's what I'm going to talk about next.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to 1st some basic after some basic notations and very basic concept.",
                    "label": 0
                },
                {
                    "sent": "I will talk about especially about the multivariate Gaussian or normal density, which is like the fundamental density in N dimensional spaces.",
                    "label": 1
                },
                {
                    "sent": "So we are basically all we are all the time here talking about multivariate statistics, so I'm not going to give you examples of univariate densities like like like exponential densities or loveless density.",
                    "label": 0
                },
                {
                    "sent": "So what other people have been talking about?",
                    "label": 0
                },
                {
                    "sent": "This is all about multivariate statistics, which is again of course much more complicated than than one dimensional statistical modeling.",
                    "label": 1
                },
                {
                    "sent": "And then then I will talk about principle component analysis, which is some kind of ancestor independent component analysis and then of the very important principle of statistical independence was when we took over independent component analysis.",
                    "label": 1
                },
                {
                    "sent": "It is not.",
                    "label": 0
                },
                {
                    "sent": "Perhaps it shouldn't be very surprising that statistical independence will be very important.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just some basic notation, so we have random variables.",
                    "label": 0
                },
                {
                    "sent": "Random variables are variables who take random values or something like that.",
                    "label": 0
                },
                {
                    "sent": "Well, there are.",
                    "label": 0
                },
                {
                    "sent": "I think some somewhat more rigorous definitions available, but.",
                    "label": 0
                },
                {
                    "sent": "But I seem to forget them.",
                    "label": 0
                },
                {
                    "sent": "So random variables the distribution of a random variable is often described by your density function.",
                    "label": 1
                },
                {
                    "sent": "Well, in the case which is, this is possible in the case where where we have random variables which take which take like all kinds of values on the real axis.",
                    "label": 0
                },
                {
                    "sent": "For example, we're not going to talk about anything that takes discrete values in this in this lecture, so it is now we are never going to have something like the probability that a random variable equals zero or that it equals one.",
                    "label": 0
                },
                {
                    "sent": "Everything is always continuous value than actually based in basically all cases.",
                    "label": 0
                },
                {
                    "sent": "This random variables can take.",
                    "label": 0
                },
                {
                    "sent": "All values on the real axis.",
                    "label": 0
                },
                {
                    "sent": "And so then in that case we can express the probability distribution function using the density function.",
                    "label": 1
                },
                {
                    "sent": "And then in particular we will be talking about random vectors, which means simply that we have large number of random random variables.",
                    "label": 0
                },
                {
                    "sent": "So it's like kind of you could say call it multidimensional random variable, but usually random variables on the word variable here is restricted to 1 dimensional valuable so.",
                    "label": 0
                },
                {
                    "sent": "And yes again, you can then.",
                    "label": 0
                },
                {
                    "sent": "You can express the distribution of continuous valued random variables using a probability distribution function.",
                    "label": 0
                },
                {
                    "sent": "Well, here.",
                    "label": 0
                },
                {
                    "sent": "Michael author gives his shows how, how, the distribution, how the density function is related, so the cumulative distribution function, which is, well, a well known stuff in basic basic probability theory.",
                    "label": 0
                },
                {
                    "sent": "But well, let's not.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going through that, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Well after this stuff, the instincts like densities and so on.",
                    "label": 0
                },
                {
                    "sent": "Certainly the most fundamental thing in probability theory and statistics is the expectation.",
                    "label": 0
                },
                {
                    "sent": "Expectation is.",
                    "label": 0
                },
                {
                    "sent": "Expectation well, in the basic case we talk about the mean, which is the expectation of a random vector itself.",
                    "label": 0
                },
                {
                    "sent": "And well, there are different kinds of terminologies available, but then people often talk about expectation when they talk about some kind of the expectation of some of some function of this of this variant of this random vector.",
                    "label": 0
                },
                {
                    "sent": "So the expectations are computed basically by an integral where you have well first this function of X and then multiplied by the probability density function, and then well, that's integrated over over the dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And so the most interesting expectations in addition to mean the correlations.",
                    "label": 0
                },
                {
                    "sent": "Well, perhaps I shouldn't say most interesting once, but let's say the most basic ones actually in I see one of the points in ICA is that is that these correlations are just like the first step in analyzing random variables or random vectors and you should not be content in just using the correlations.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "I should perhaps here explain what is actually the meaning of correlations.",
                    "label": 0
                },
                {
                    "sent": "Well, the correlations are basically tell you something about linear dependencies of these variables.",
                    "label": 0
                },
                {
                    "sent": "So let's say that we want to predict X2.",
                    "label": 0
                },
                {
                    "sent": "By X1 so this is classically expressed as a linear regression model that I'm sure you have all seen somewhere.",
                    "label": 0
                },
                {
                    "sent": "We have a coefficient a here and then we have some kind of noise or residual to this random.",
                    "label": 0
                },
                {
                    "sent": "So another point is that a tells us, well gives us what is loosely called the correlation between X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "Well, if it's if it's positive and large large in some sense, then it tells you that that's that's when that's X2 in a certain way follows X one, so that when X one is large, this one will be large, and when the next one is small, this one will be typical large.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that if we normalize these variables properly.",
                    "label": 0
                },
                {
                    "sent": "Then we will see actually if we normalize them.",
                    "label": 0
                },
                {
                    "sent": "So if we normalize them so that the variances of X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "Both equal to 1.",
                    "label": 0
                },
                {
                    "sent": "And well, just for those of you who don't remember, I will.",
                    "label": 0
                },
                {
                    "sent": "I will remind you of the definition of variance, which is which is a measure of the spread of the random variable.",
                    "label": 0
                },
                {
                    "sent": "So now what happens is that under these conditions this A equals the correlation equals expectation of X 1 * X Two.",
                    "label": 0
                },
                {
                    "sent": "So it so this correlation tells you something about the dependencies between the variables, but also it is important to note in the context of ICA it is important to note that correlation is only tells you the dependants under the framework of this kind of a linear dependency.",
                    "label": 0
                },
                {
                    "sent": "So correlation is really like the first step in the direction of analyzing the dependencies between these vectors.",
                    "label": 0
                },
                {
                    "sent": "For example, if you wanted to know how the square of X X2 is related to the square of X1.",
                    "label": 0
                },
                {
                    "sent": "Then you wouldn't be.",
                    "label": 0
                },
                {
                    "sent": "Then you wouldn't be able to use this correlation anymore.",
                    "label": 0
                },
                {
                    "sent": "You would need something completely different, but actually something like the correlation of of the squares.",
                    "label": 0
                },
                {
                    "sent": "So I just also warn you that this daemonology here is very confusing in sense that, well, this definition of correlation is what people usually use in signal processing.",
                    "label": 0
                },
                {
                    "sent": "In in statistics, when they talk about correlation, they usually use the word correlation coefficient and correlation coefficient is is a different thing from correlation.",
                    "label": 0
                },
                {
                    "sent": "In this sense, it is actually a normalized version of this.",
                    "label": 0
                },
                {
                    "sent": "Well, not even other normalized.",
                    "label": 0
                },
                {
                    "sent": "But it's really.",
                    "label": 0
                },
                {
                    "sent": "It's a rather different.",
                    "label": 0
                },
                {
                    "sent": "Well, let's consider very different thing.",
                    "label": 0
                },
                {
                    "sent": "Either it's it's kind of the same thing, but it has slightly different constraints.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "Well, we will see something about that on the next slide, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In addition to covariance, I mean the correlation matrix.",
                    "label": 0
                },
                {
                    "sent": "People talk about the covariance matrix, which is done, so that's basically you.",
                    "label": 0
                },
                {
                    "sent": "Subtract the mean of each of the of each of the variables.",
                    "label": 0
                },
                {
                    "sent": "And then you compute the correlation matrix.",
                    "label": 0
                },
                {
                    "sent": "So which means that if the mean is 0, then actually the covariance matrix is just the same thing as the color color correlation matrix.",
                    "label": 1
                },
                {
                    "sent": "Now what is not mentioned here is this is the definition of the correlation coefficient, which is the same thing as the covariance divided by the variance is.",
                    "label": 0
                },
                {
                    "sent": "I've sorry, the standard deviations of these variables, so it's kind of so the correlation coefficient is renormalized version of covariance.",
                    "label": 0
                },
                {
                    "sent": "And now there's actually something even more confusing.",
                    "label": 0
                },
                {
                    "sent": "Yes, first.",
                    "label": 0
                },
                {
                    "sent": "Yes, the second moment matrix, yes exactly.",
                    "label": 0
                },
                {
                    "sent": "Now maybe I should?",
                    "label": 0
                },
                {
                    "sent": "Well this confused with you with this, but I have to now another very widespread terminology is to say that if the covariance is 0, then these variables are called uncorrelated.",
                    "label": 0
                },
                {
                    "sent": "So uncorrelated does uncorrelated does not mean that correlation is zero, which would be very lot too logical I think.",
                    "label": 0
                },
                {
                    "sent": "So uncorrelated means that covariant the covariance is zero.",
                    "label": 0
                },
                {
                    "sent": "Well, Fortunately, Fortunately during all these lectures and in many other contexts as well, the expectations are all zero.",
                    "label": 0
                },
                {
                    "sent": "I mean if expectations.",
                    "label": 0
                },
                {
                    "sent": "So I mean the means means of the variables are zero, so then uncorrelatedness is the same thing as zero covariance, which is the same as.",
                    "label": 0
                },
                {
                    "sent": "Zero correlation.",
                    "label": 0
                },
                {
                    "sent": "But in some cases you know there will be some differences.",
                    "label": 0
                },
                {
                    "sent": "And in that case also, it is the same as having a zero correlation coefficient.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK, that's it.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, I'm sorry I made a mistake, so again here.",
                    "label": 0
                },
                {
                    "sent": "Here this thing here.",
                    "label": 0
                },
                {
                    "sent": "This would actually be the covariance.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So actually we let's make here the assumption that the expectations are zero.",
                    "label": 0
                },
                {
                    "sent": "Only then this thing.",
                    "label": 0
                },
                {
                    "sent": "This thing here is correct.",
                    "label": 0
                },
                {
                    "sent": "And that's why people talk about that.",
                    "label": 0
                },
                {
                    "sent": "Then it's easy to talk about correlation.",
                    "label": 0
                },
                {
                    "sent": "You know, in in all kinds of Sciences, social Sciences and Biological Sciences, people always talk about variables being correlated now.",
                    "label": 0
                },
                {
                    "sent": "Then it well for them.",
                    "label": 0
                },
                {
                    "sent": "It is usually a question of having a non zero correlation coefficients.",
                    "label": 0
                },
                {
                    "sent": "Now if if this condition well having a non zero correlation coefficient is the same thing as having.",
                    "label": 0
                },
                {
                    "sent": "A non zero covariance which means an non 0A.",
                    "label": 0
                },
                {
                    "sent": "Which means that you can.",
                    "label": 0
                },
                {
                    "sent": "You can like linearly predict one variable by the other one.",
                    "label": 0
                },
                {
                    "sent": "Or I mean you can predict part of the variable by the other variable.",
                    "label": 0
                },
                {
                    "sent": "People often talk about like percentage of variance which is predicted so that if there's like a weak correlation, then you can perhaps predict 10% of the variance of the other variable.",
                    "label": 0
                },
                {
                    "sent": "If the correlation is is extremely good, then you can.",
                    "label": 0
                },
                {
                    "sent": "Perhaps you can predict maybe 90% of the variance, and so if the variables uncorrelated then you can predict 0% of the variance.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a classical example we have here a certain number of people and we have their weights in kilograms and the Heights in centimeters.",
                    "label": 0
                },
                {
                    "sent": "And you can then compute the covariance matrix so in the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "But actually the diagonal elements are nothing but the variance is an.",
                    "label": 0
                },
                {
                    "sent": "It is off diagonal elements that tell you that.",
                    "label": 0
                },
                {
                    "sent": "Give the covariances and tell you about the dependencies between the variables.",
                    "label": 0
                },
                {
                    "sent": "So what you see here is that there is a rather substantial dependency.",
                    "label": 0
                },
                {
                    "sent": "I mean rather substantial correlation between the Heights and weights of the persons, which is not at all surprising, of course.",
                    "label": 0
                },
                {
                    "sent": "This is this is seen in the fact that the covariance is larger than 0.",
                    "label": 0
                },
                {
                    "sent": "Another thing that you can see is that one.",
                    "label": 0
                },
                {
                    "sent": "You can also compare the variances if you like well.",
                    "label": 0
                },
                {
                    "sent": "Then well, that is perhaps not so so interesting.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What people usually used to model.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kind of data also also this kind of a data.",
                    "label": 0
                },
                {
                    "sent": "I mean this is.",
                    "label": 0
                },
                {
                    "sent": "This is kind of a very typical kind of cloud as some people would call it, call it a Gaussian cloud because it's kind of a kind of an ellipse.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Check out cloud.",
                    "label": 0
                },
                {
                    "sent": "It is very often, well, almost always modeled by this kind of a Gaussian densities.",
                    "label": 0
                },
                {
                    "sent": "So this is this is this is a contour plot of a Gaussian profile multivariate Gaussian density where the para meters have been chosen so that it's kind of slightly similar to this real example, so here.",
                    "label": 0
                },
                {
                    "sent": "So this is this is these are just two representations of the same distribution, So what you will see is that the contours are elliptical.",
                    "label": 0
                },
                {
                    "sent": "That's always true for Gaussian, Gaussian multivariate Gaussian densities.",
                    "label": 1
                },
                {
                    "sent": "So it will give you if you take a sample from this thing.",
                    "label": 0
                },
                {
                    "sent": "You will typically get some cloud that looks something like elliptical.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there's you know, some.",
                    "label": 0
                },
                {
                    "sent": "Some points which I would further.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the center.",
                    "label": 0
                },
                {
                    "sent": "And well, yeah he is just the same thing in presented.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In different way.",
                    "label": 0
                },
                {
                    "sent": "An well the mathematical expression for that kind of a density is given here.",
                    "label": 0
                },
                {
                    "sent": "So the probability density function of X is some constant.",
                    "label": 0
                },
                {
                    "sent": "Normalizing constant times the exponential of a certain kind of a quadratic form.",
                    "label": 0
                },
                {
                    "sent": "So here what we from we subtract from X.",
                    "label": 0
                },
                {
                    "sent": "It's mean actually MX.",
                    "label": 0
                },
                {
                    "sent": "It's a parameter which turns out to be equal to the mean, and then we use.",
                    "label": 0
                },
                {
                    "sent": "We use take a quadratic form.",
                    "label": 0
                },
                {
                    "sent": "Multiplying by this X minus mean certain matrix from both sides.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that this matrix here is the inverse of the covariance matrix, so we will see we see that this is an extremely.",
                    "label": 0
                },
                {
                    "sent": "This is like one of the simplest just looking at it algebraically.",
                    "label": 0
                },
                {
                    "sent": "You will see that this is one of the simplest imaginable densities that you can imagine in an N dimensional real space.",
                    "label": 0
                },
                {
                    "sent": "What you see is that first of all these para meters, it has only two para meters.",
                    "label": 0
                },
                {
                    "sent": "One is the mean which.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically tells you like well.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Center point here is and then we have the other matrix is the covariance.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That basically tells you like what is the shape of this of these... what direction they are pointing at and how big they are.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kind of things.",
                    "label": 0
                },
                {
                    "sent": "Well, usually in statistics, what we are what is more interesting than the densities themselves, is the logarithms.",
                    "label": 0
                },
                {
                    "sent": "For reasons that will become clear in some in a moment.",
                    "label": 0
                },
                {
                    "sent": "So what we're having here, so the logarithm of this probability density function is simply a quadratic form an well.",
                    "label": 0
                },
                {
                    "sent": "The quadratic form is one of the simplest functions possible.",
                    "label": 0
                },
                {
                    "sent": "Of course, the linear function would be even simpler, but we cannot have a linear function as the logarithm logarithm of a density function.",
                    "label": 0
                },
                {
                    "sent": "A question, why can't we have a linear function as the PDF probability density function?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes, exactly has to integrate to one and a linear function would go to Infinity in one, in some other directions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and so yeah, yeah the the.",
                    "label": 0
                },
                {
                    "sent": "OK, the the meaning of this constant K is too nice to make sure that this probability density function integrates to one to one.",
                    "label": 0
                },
                {
                    "sent": "It has a slightly ugly looking expression, but but still it can be computed for these.",
                    "label": 0
                },
                {
                    "sent": "So another point to note well about the Gaussian distribution is that once you know the 1st and 2nd order statistics, that means the mean and the Cove area and the covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "Then you have completely specified the probability distribution function.",
                    "label": 0
                },
                {
                    "sent": "It doesn't, you don't need any other information and any other information will not bring anything new about the distribution.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Do you have any questions at this point?",
                    "label": 0
                },
                {
                    "sent": "Is this too trivial?",
                    "label": 0
                },
                {
                    "sent": "Is this too difficult?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The classic way to approach when you have.",
                    "label": 0
                },
                {
                    "sent": "Data from a multidimensional and in dimensional distribution.",
                    "label": 0
                },
                {
                    "sent": "The classic Classic Way to analyze one of the simplest ways is to do principal component analysis.",
                    "label": 1
                },
                {
                    "sent": "Well, you can do things like cluster analysis for example, but well for data data clouds like this you will see that, well, cluster analysis is not very interesting because there's basically just one cluster.",
                    "label": 0
                },
                {
                    "sent": "So what you can do, what people do is principal component analysis, which is closely related to the fact to answer that to the fact that people use a Gaussian model for modeling this kind of data.",
                    "label": 0
                },
                {
                    "sent": "It has the mathematics are very very similar meta mathematics and the underlying assumptions are very similar.",
                    "label": 0
                },
                {
                    "sent": "So principle, well, I think there will be some talks here who were principal component analysis described in much detail, but I have talked about it for awhile.",
                    "label": 0
                },
                {
                    "sent": "Because otherwise he wouldn't be able to understand the contrast to independent component analysis an what is very new complaint with this, but it's really different in independent component analysis, so usually principal component analysis is is defined in a recursive manner.",
                    "label": 0
                },
                {
                    "sent": "Well there are a couple of different ways, but let's start with the recursive manner.",
                    "label": 0
                },
                {
                    "sent": "That is, we want to find a single principle component.",
                    "label": 0
                },
                {
                    "sent": "A single principle component means a single signal direction in the data space.",
                    "label": 0
                },
                {
                    "sent": "That is, that which is given by a single vector in that data space, which we didn't buy W. So so now the point is that.",
                    "label": 0
                },
                {
                    "sent": "We we at proximates the data using the projection of the data onto this single principal components.",
                    "label": 0
                },
                {
                    "sent": "Well, the projection, assuming that we normalize W so that its norm is equal to 1, the projection of the data of the projection of a single data point on.",
                    "label": 0
                },
                {
                    "sent": "So this one dimensional subspace on this one line in the sand dimensional space is given by this this expression here.",
                    "label": 0
                },
                {
                    "sent": "Now the question is.",
                    "label": 0
                },
                {
                    "sent": "What is the best W. Yeah, well, the question is what is the best W?",
                    "label": 0
                },
                {
                    "sent": "Well there are certainly many different answers on what is the best W you have to 1st define what is best.",
                    "label": 0
                },
                {
                    "sent": "Well the way best is usually defined here is that what is the best W so that you can add you so that you approximate your data in the as well as possible.",
                    "label": 0
                },
                {
                    "sent": "Approximation means that now the expected error or the average error.",
                    "label": 0
                },
                {
                    "sent": "All between the real data and this projected upper Summated data is as small as possible.",
                    "label": 0
                },
                {
                    "sent": "So now we have a typical optimization problem.",
                    "label": 0
                },
                {
                    "sent": "We want to minimize this function, which is the approximation error expectation, meaning average over the whole whole datasets hold data distribution and we have.",
                    "label": 0
                },
                {
                    "sent": "This is a constraint optimization optimization problem because we say that W has unit no.",
                    "label": 0
                },
                {
                    "sent": "So the David.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Action on how to do this is given here, so this was written by my coauthor, so I'm not quite sure if I understand it correctly myself, but so let's try to go through it.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can learn it so the point is that.",
                    "label": 0
                },
                {
                    "sent": "First, we define we make a change of variables.",
                    "label": 0
                },
                {
                    "sent": "Basically, instead of considering the variable W, we look at the orthogonal direction.",
                    "label": 0
                },
                {
                    "sent": "Well, it's simple here, especially because we have only two dimensions.",
                    "label": 0
                },
                {
                    "sent": "So once we define one dimension of W without being uniquely define the orthogonal direction.",
                    "label": 0
                },
                {
                    "sent": "And so now we transform this objective function.",
                    "label": 0
                },
                {
                    "sent": "Only sister fun.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes we express X, sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes, we expect Express X as the sum of the projections onto W&U.",
                    "label": 0
                },
                {
                    "sent": "Because, well, that is equal to X because we only have 2 dimensions and and we have two orthogonal projections and then the approximation is just as given by the by the by the definition and so this these two terms cancel out.",
                    "label": 0
                },
                {
                    "sent": "And what we have is is this kind of a thing.",
                    "label": 0
                },
                {
                    "sent": "Now when you just express this norm well norm of a factor is the square of the norm of a vector is simply the transpose of the vector times the vector itself.",
                    "label": 0
                },
                {
                    "sent": "So here we transpose select an effector itself.",
                    "label": 0
                },
                {
                    "sent": "Now the point is that is that.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all.",
                    "label": 0
                },
                {
                    "sent": "Well, here we we change the value, change the order of these things.",
                    "label": 0
                },
                {
                    "sent": "I wonder if this is quite quite correct well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it it.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, so now we try we commute these two terms here and then we take the expectation operator in here into the middle.",
                    "label": 0
                },
                {
                    "sent": "We can do that because you is you is a constant and not random quantity.",
                    "label": 0
                },
                {
                    "sent": "So then what we get is the expectation of XXXX transpose here in the middle.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yes, I somehow I don't quite get it, but I also trust Michael was it that is correct?",
                    "label": 0
                },
                {
                    "sent": "Or do you think there's an error here?",
                    "label": 0
                },
                {
                    "sent": "You transpose exist Skype, so you can actually.",
                    "label": 0
                },
                {
                    "sent": "Yes, some of the use cancel out, but I somehow get the impression that.",
                    "label": 0
                },
                {
                    "sent": "OK yes yeah.",
                    "label": 0
                },
                {
                    "sent": "So it has to go somewhere.",
                    "label": 0
                },
                {
                    "sent": "So that this thing here is equal to X * U T. Times UTX and then we commit those X T * U and then this thing will be.",
                    "label": 0
                },
                {
                    "sent": "Multiply by.",
                    "label": 0
                },
                {
                    "sent": "You right, yeah, so now these things here cancel each other.",
                    "label": 0
                },
                {
                    "sent": "So because this is this is this is a scalar, so we can.",
                    "label": 0
                },
                {
                    "sent": "We can actually put this here, and because the norm of U is equal to 1 then these things cancel each other.",
                    "label": 1
                },
                {
                    "sent": "And what we are we can we can put the expectation operator here in the middle.",
                    "label": 0
                },
                {
                    "sent": "Can you see this?",
                    "label": 0
                },
                {
                    "sent": "And it's equal to this, yes?",
                    "label": 0
                },
                {
                    "sent": "And so now what the expectation thing here is nothing but the the correlation matrix.",
                    "label": 0
                },
                {
                    "sent": "So we see here two things.",
                    "label": 0
                },
                {
                    "sent": "First of all, a principal component analysis basically boils down to looking at the correlation matrix or the covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "Usually this all zero mean variables.",
                    "label": 0
                },
                {
                    "sent": "And the second thing is that it boils down to something like an analysis of some kind of quadratic forms well.",
                    "label": 0
                },
                {
                    "sent": "The next next slide is then how to maximize this kind of sorry.",
                    "label": 0
                },
                {
                    "sent": "Actually, here we want to minimize this quadratic function.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quadratic form.",
                    "label": 0
                },
                {
                    "sent": "Now again, let's try to understand this.",
                    "label": 0
                },
                {
                    "sent": "So what we want to minimize this kind of a quadratic form under the constraint that the norm is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Now when we look at the gradient of this thing so well, this is.",
                    "label": 0
                },
                {
                    "sent": "This is the condition.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I said you so your car or spoon two conditions.",
                    "label": 0
                },
                {
                    "sent": "But actually I think that's kind of an exaggeration because that's those are the more general conditions that are used in with inequality constraints.",
                    "label": 0
                },
                {
                    "sent": "So this is just the classical.",
                    "label": 0
                },
                {
                    "sent": "Advantage condition so we will have that the gradient minor is proportional or I mean gradient minus some constant times the gradient of the of the constraint equals 0 and now it happens that the gradient.",
                    "label": 0
                },
                {
                    "sent": "Well, OK here Michael author uses not the gradient, but the transpose of the gradient, which is more, properly speaking the derivative.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't make any difference really, so here the derivative is equal to its actually is the corresponding linear function.",
                    "label": 0
                },
                {
                    "sent": "This is an interesting point that the gradient of a quadratic form is the corresponding linear function with the same matrix multiplied by two and then here.",
                    "label": 0
                },
                {
                    "sent": "Well, we had the same thing, because this is also a quadratic form with.",
                    "label": 0
                },
                {
                    "sent": "An identity matrix in the middle.",
                    "label": 0
                },
                {
                    "sent": "So this this equations when we just put this, take the tools out and put this you on the other side.",
                    "label": 0
                },
                {
                    "sent": "It gives you this.",
                    "label": 0
                },
                {
                    "sent": "This is nothing but the definition of an eigenvalue of a matrix, so this this is actually a very general general phenomenon.",
                    "label": 0
                },
                {
                    "sent": "Usually when we have some kind of a quadratic functions as quadratic forms, to maximize the solutions will be given by some kind of eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Because you always get this set in the same kind of equations and here.",
                    "label": 0
                },
                {
                    "sent": "Well, then it depends on what.",
                    "label": 0
                },
                {
                    "sent": "Depending on the situation it you have different eigen values that you want.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you have largest eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the smallest eigenvalues, sometimes something in the middle.",
                    "label": 0
                },
                {
                    "sent": "So what happens here is that we should actually find the the largest eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "Sorry in this case for you you should correspond to the smallest eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "Of the correlation matrix, which is a bit confusing.",
                    "label": 0
                },
                {
                    "sent": "Actually W what we actually wanted to find in the 1st place, which was the original variable, corresponds to the largest eigenvalue.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Whatever the point is that we can do PCA by taking the correlation matrix and then computing the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "And this I can the the the the eigenvector with the largest eigenvalue will give us this principle component.",
                    "label": 0
                },
                {
                    "sent": "Now, as I said in the beginning, PC is basically recursive thing.",
                    "label": 0
                },
                {
                    "sent": "So you first now we have just shown how to compute one principal component, but in an N dimensional space you will.",
                    "label": 1
                },
                {
                    "sent": "You can define NN principal components, so the following ones are defined simply, so that's not so that's.",
                    "label": 0
                },
                {
                    "sent": "So that we subtract the previous projection that we had in the 1st place from the data.",
                    "label": 0
                },
                {
                    "sent": "So we reduce the dimension of the data by one and then do exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing is that we are well in this 2 dimensional case.",
                    "label": 0
                },
                {
                    "sent": "It is not very very obvious, but in an dimensional scale, in dimensional case we are successively by computing components that give us the best approximation best approximation.",
                    "label": 0
                },
                {
                    "sent": "Of the data using this limited number of components, actually it can be proven that the first KIK principal components give the best activation of the data that is possible with a K dimensional subspace.",
                    "label": 0
                },
                {
                    "sent": "Change.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, it is one problem with PCA is that the result depends on the units of measurement.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, make it Scandinavian.",
                    "label": 0
                },
                {
                    "sent": "Yes again.",
                    "label": 0
                },
                {
                    "sent": "So now we're talking about the coral Emma in the matrix correlation coefficients as defined in statistics.",
                    "label": 0
                },
                {
                    "sent": "Yes, so correlation coefficient as defined here.",
                    "label": 0
                },
                {
                    "sent": "An in signal processing has had the same problem of of.",
                    "label": 0
                },
                {
                    "sent": "Depending on the that it depends on the scale.",
                    "label": 0
                },
                {
                    "sent": "But yes, one the classical way of getting rid of this problem is to simply define the scale of your variables so that you so that you say that the variance of each variable has to be equal to 1.",
                    "label": 0
                },
                {
                    "sent": "But yes, I mean that kind of solves the problem, but then I mean it.",
                    "label": 0
                },
                {
                    "sent": "It gives you unique answer to the problem, but it may not be.",
                    "label": 0
                },
                {
                    "sent": "It may not be there.",
                    "label": 0
                },
                {
                    "sent": "The answer to the second question that you wanted to ask in the 1st place because in some cases the units units have an intrinsic meaning and you don't want to re scale everything.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but it depends on the situation.",
                    "label": 0
                },
                {
                    "sent": "But as I said, the case first principle components give the best approximation of the data in the sense that when you project the data onto the subspace defined by those principal components, the the expected square error will be minimized.",
                    "label": 0
                },
                {
                    "sent": "So that is why PCA is used for dimensionality reduction.",
                    "label": 1
                },
                {
                    "sent": "So if you have data that has like 10,000 dimensions for example, and let's say that for that you want to do some computationally intensive methods.",
                    "label": 0
                },
                {
                    "sent": "That cannot really cope with 10,000 dimensions and and you need to need to reduce the dimension of the data to be able to use your May.",
                    "label": 0
                },
                {
                    "sent": "Your favorite method then.",
                    "label": 0
                },
                {
                    "sent": "A PCA is the classical way of reducing the dimension.",
                    "label": 0
                },
                {
                    "sent": "Now, if with PCA you can reduce the dimension to say 100, and you know that you have the best possible approximation of the original data that the best possible with 100 dimensions.",
                    "label": 0
                },
                {
                    "sent": "Though of course this is, well, best is always extremely relative.",
                    "label": 0
                },
                {
                    "sent": "People always prove some kind of optimality results for different kinds of methods, but of course what is optimal is all optimality is always in the eye of the beholder.",
                    "label": 0
                },
                {
                    "sent": "I suppose one could say so we must remember, that's like PCA here for as defined here, has for example the constraint that it is completely a linear method.",
                    "label": 0
                },
                {
                    "sent": "So then people some people have.",
                    "label": 1
                },
                {
                    "sent": "Well, actually many people have been developing methods for nonlinear dimension reduction, which are something like nonlinear PCA methods.",
                    "label": 0
                },
                {
                    "sent": "There are methods like hold for example, non metric multidimensional scaling, which is like the oldest group of methods and then the self organizing Maps curvilinear component analysis.",
                    "label": 0
                },
                {
                    "sent": "Kernel PCA.",
                    "label": 0
                },
                {
                    "sent": "And like he's on map or local linear embedding, which is which was developed by Sam.",
                    "label": 0
                },
                {
                    "sent": "So it is certainly very best one available.",
                    "label": 0
                },
                {
                    "sent": "And so and so on.",
                    "label": 0
                },
                {
                    "sent": "And of course, you could also question the very objective of over this very measure of what is best approximation.",
                    "label": 0
                },
                {
                    "sent": "So what we are talking about here is the approximation with respect to the Euclidean norm.",
                    "label": 0
                },
                {
                    "sent": "But of course, you might argue that perhaps some other kind of a major might be better in under some circumstances, but in any case the the nice thing about PCA is that it is computationally extremely simple, because we can just do the eigenvalue decomposition of the covariance matrix, and that's one of the that's extremely simple compared to all other obvious nonlinear methods, and also compared to ICA and so on.",
                    "label": 0
                },
                {
                    "sent": "In signal processing, people also talk about.",
                    "label": 0
                },
                {
                    "sent": "Noise at the nation.",
                    "label": 0
                },
                {
                    "sent": "As soon as a as a great benefit of PCA, there the idea is that, like you have a signal that leaves in some kind of low dimensional subspace called the signal subspace, and then you have noise all over like noise in all other directions, not which are which.",
                    "label": 0
                },
                {
                    "sent": "Also also this signal subspace.",
                    "label": 0
                },
                {
                    "sent": "So when you get rid of all those noise dimensions and just concentrate on the signal subspace then you probably get rid of a lot of noise as well.",
                    "label": 0
                },
                {
                    "sent": "Well, that may be true.",
                    "label": 0
                },
                {
                    "sent": "Or not, depending on circumstances.",
                    "label": 0
                },
                {
                    "sent": "But really, the good thing about PCA is dimensionality reduction and I would say that that is really the only thing where we know that PCA is really good, like.",
                    "label": 0
                },
                {
                    "sent": "The point is that people have trying to use PCA for things that actually ICA does properly, but which PCA does not do properly at all.",
                    "label": 0
                },
                {
                    "sent": "That's PCA completely fails to do at all, so we have so.",
                    "label": 0
                },
                {
                    "sent": "Even so.",
                    "label": 0
                },
                {
                    "sent": "So we will see that PCs PCs put for dimensional reduction and not for many other things.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a classical example of this dimensional reduction.",
                    "label": 0
                },
                {
                    "sent": "Property, so here we have an image.",
                    "label": 0
                },
                {
                    "sent": "This is like the this is an image that we want to know.",
                    "label": 0
                },
                {
                    "sent": "Compress compression means here that we take we divide the image into small squares, small windows and then we project all of those windows onto the low dimension lower dimensional subspace.",
                    "label": 0
                },
                {
                    "sent": "But this is not very different from methods used in well known.",
                    "label": 0
                },
                {
                    "sent": "Well, image compression methods as JPEG for example.",
                    "label": 0
                },
                {
                    "sent": "They don't use PCA, but they use this idea of looking at small windows and.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And dimension reduction.",
                    "label": 0
                },
                {
                    "sent": "So if you do, if you take this kind of a small windows and then and sample the image and get all of all possible Windows non overlapping windows then you get then you can actually.",
                    "label": 0
                },
                {
                    "sent": "Then you can compute the PCA for those for that kind of a data and what you get is this kind of basis vectors.",
                    "label": 0
                },
                {
                    "sent": "These basis vectors are rather similar to something like 44 basis vectors, partly similar partly nodes.",
                    "label": 0
                },
                {
                    "sent": "At least these first ones are something like.",
                    "label": 0
                },
                {
                    "sent": "Not not very different.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "40 basis vectors.",
                    "label": 0
                },
                {
                    "sent": "Now the point is that we can have different kinds of compression.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "Which means that by choosing by by ignoring a certain number, a certain percentage of the vectors.",
                    "label": 0
                },
                {
                    "sent": "So now here this.",
                    "label": 0
                },
                {
                    "sent": "In this case what we have done is to ignore 90% of the principle components and we have only taken the 10 most 10 first.",
                    "label": 0
                },
                {
                    "sent": "That is the most important principle components as I think.",
                    "label": 0
                },
                {
                    "sent": "So we have basically in a certain way you could say we could roughly say that we have reduced the amount of data by 90%, but as you can see it's not.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not terribly different from.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The original image.",
                    "label": 0
                },
                {
                    "sent": "You can see some kind of smoothing but not too much so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you take more and more principle components, here we have.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "85% and here we have 50%.",
                    "label": 0
                },
                {
                    "sent": "You will see that the picture gets better and better.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "If you sent me these 10% of the coefficients, how would I reconstruct?",
                    "label": 0
                },
                {
                    "sent": "You also need to send me the basis.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes I do, yes.",
                    "label": 0
                },
                {
                    "sent": "But of course it depending on circumstance.",
                    "label": 0
                },
                {
                    "sent": "Single stance is the basis.",
                    "label": 0
                },
                {
                    "sent": "The amount of coding needed for the basis may be quite negligible compared to the coefficients.",
                    "label": 0
                },
                {
                    "sent": "Of course, this is also why people don't use PCA for real compression.",
                    "label": 0
                },
                {
                    "sent": "Review it easy.",
                    "label": 0
                },
                {
                    "sent": "Use the fixed basis sets.",
                    "label": 0
                },
                {
                    "sent": "But I mean, the idea is that with PCA you could find some kind of optimal basis for doing that.",
                    "label": 0
                },
                {
                    "sent": "But I mean you, you wouldn't compare the PCs Africa for each image.",
                    "label": 0
                },
                {
                    "sent": "You could.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have a certain class of images to complete the PCA and then you you know, then it doesn't take much much storage.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, here's the regional one again.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that was about PCA.",
                    "label": 0
                },
                {
                    "sent": "Now when we talk about the independent component of this, we have to understand what this independence.",
                    "label": 0
                },
                {
                    "sent": "And well, there are different kinds of independence.",
                    "label": 0
                },
                {
                    "sent": "This is about statistical independence.",
                    "label": 0
                },
                {
                    "sent": "It should.",
                    "label": 0
                },
                {
                    "sent": "It should not be confused with linear independence, which has, which is a completely different phenomenon then.",
                    "label": 0
                },
                {
                    "sent": "So step, mistaken independence basically means that when we have, let's say 2 random variables X&Y, when we know the value of X, that doesn't give us any information about the distribution of Y.",
                    "label": 1
                },
                {
                    "sent": "Or in other words, no, X doesn't give us any information about why.",
                    "label": 0
                },
                {
                    "sent": "To put it a bit short, So what this means to formulate this mathematically?",
                    "label": 0
                },
                {
                    "sent": "Well, we use we just the concept of conditional distribution I. I hope that's that's familiar to everybody.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, it must be some.",
                    "label": 0
                },
                {
                    "sent": "I think some gave us a good introduction, so that kinda yeah.",
                    "label": 0
                },
                {
                    "sent": "So we all know what is a conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is a conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "Now for continuous valid valid variables.",
                    "label": 0
                },
                {
                    "sent": "So it's a conditional probability density function, but it doesn't really make any difference whether we're talking about conditional probabilities for discrete variables or discrete or sorry, whether we're talking about conditional probabilities for discrete variables or conditional probability density functions.",
                    "label": 0
                },
                {
                    "sent": "For continuous value variance.",
                    "label": 0
                },
                {
                    "sent": "So the conditional probabilities of Y given X is defined by the joint probability density divided by the probability density of X for this given values of X&Y.",
                    "label": 0
                },
                {
                    "sent": "So if we now if we say that X doesn't give any information, then it may be about why then it means that the conditional distribution all considered conditional density should be equal to the original density.",
                    "label": 0
                },
                {
                    "sent": "Of why?",
                    "label": 0
                },
                {
                    "sent": "Well, if you just now then multiply that this equation and multiply by PX of X and put it here.",
                    "label": 0
                },
                {
                    "sent": "Then what you see is that this is the same thing as saying that the joint distribution factorizes.",
                    "label": 0
                },
                {
                    "sent": "That means the joint distribution joint density is simply a product of the marginal densities of X&Y.",
                    "label": 0
                },
                {
                    "sent": "And yes, this this.",
                    "label": 0
                },
                {
                    "sent": "This lower equation here is the typical definition.",
                    "label": 0
                },
                {
                    "sent": "While the formal definition of independence.",
                    "label": 0
                },
                {
                    "sent": "And so as I talk about when I talked about here about correlations, I was saying that, well, yes, correlations are about some kind of a dependency between random variables.",
                    "label": 0
                },
                {
                    "sent": "But the point is that correlation is only talk about one very particular form of dependencies.",
                    "label": 0
                },
                {
                    "sent": "It is this linear dependence in the sense that when X one is large, then X2 is large when and X one is smaller than X2 is small.",
                    "label": 0
                },
                {
                    "sent": "But independence here is something much.",
                    "label": 0
                },
                {
                    "sent": "Stronger.",
                    "label": 0
                },
                {
                    "sent": "So independence, statistical independence.",
                    "label": 0
                },
                {
                    "sent": "I mean, so there are many different kinds of dependencies.",
                    "label": 0
                },
                {
                    "sent": "So when we say the two variables are independent, it means that they have no kinds of dependencies there, so they don't have this kind of correlations, but they don't have any other kinds of dependencies either.",
                    "label": 0
                },
                {
                    "sent": "So statistical independence actually implies that the covariance and correlation coefficients zero between those variables.",
                    "label": 0
                },
                {
                    "sent": "But it is not true in the other way, just having zero or just being uncorrelated does not imply independence.",
                    "label": 0
                },
                {
                    "sent": "We will talk more about that later when we actually get to the stuff on independent component analysis.",
                    "label": 0
                },
                {
                    "sent": "If that ever happens.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so maybe now we have a short break and then continue.",
                    "label": 0
                }
            ]
        }
    }
}