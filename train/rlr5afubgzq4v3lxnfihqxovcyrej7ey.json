{
    "id": "rlr5afubgzq4v3lxnfihqxovcyrej7ey",
    "title": "Novel Fusion Methods for Pattern Recognition",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Muhammad Awais, University of Surrey"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_awais_recognition/",
    "segmentation": [
        [
            "I am Muhammad.",
            "The events from University of Study and CSP is my center, so this is joint work with pay and my PhD Supervisor, Christian McCoy, Jake and Rosa Hitler."
        ],
        [
            "So.",
            "I I don't think that I need much time on motivation 'cause the presenter already have done that, so I will skip this.",
            "By saying only that I particularly focus on object recognition and we end datasets for this paper, but the techniques can be applied to any pattern recognition problem, provided that we were we have multiple information."
        ],
        [
            "Channels so the problem statement is something like this that we have multiple information channel multiple feature channel can be expected in many different ways and what is the optimal way to combine these multiple feature channels to have maximum performance out of it?",
            "So before."
        ],
        [
            "I go into the solution of these things.",
            "I will present my outline so I will quickly go through a couple of existing film techniques and then I will go through the proposed method for multi level and multi class.",
            "So this one is multi label and there are these three are multi class and then then a very easy extension of stacking and present result and conclusion."
        ],
        [
            "Event.",
            "So multiple can learning is state of the art and state of the art.",
            "Since I think 2004 or so.",
            "So how they they do the combination of these different information channel is by maximizing the margin to learn these waves.",
            "So consider that these feature channels each feature channel give rise to a kernel.",
            "In some feature space and this kernel get some weight and then you take the linear combination of these these kernels and you get a representative Colonel and you have had it as an nonlinear SVM.",
            "Me and you get your prediction for both binary and multiclass."
        ],
        [
            "OK, the alternative approaches, late fee and you can see classifier level Fusion in which you have.",
            "You have your feature channel.",
            "You apply some classifier.",
            "It don't have to be SVM, it can be any classifier and you have the scores of your classifier and you learn the wait for these scores somehow and you take the weighted sum.",
            "There are many other ways to combine these classifier.",
            "You can take and rule or rule or minimum, maximum, median and so on.",
            "So why I have put this?",
            "Because we are we are taking weighted sum of these Bayes classifier score.",
            "And."
        ],
        [
            "The novelty of this paper is on these four.",
            "Classified free and techniques.",
            "I extended some existing, which is this?",
            "This is a variant of MU LP, Adaboost and this extension of LP beta and LPB, and there is a novel multiclass classifier Freehan approach as well, so these are the major nobility of."
        ],
        [
            "Is this paper?",
            "So.",
            "How how we learned this weights in classified Level 3 and so to understand that.",
            "We have to understand this GRGR of X is so this, particularly in this slider.",
            "This is for binary classification.",
            "If you want so you have this course of base classifier, you wait them by corresponding weights and you take the linear combination and you decide on the sign if it will find reclassification and if it is multi class we go for the maximum response.",
            "So.",
            "I define the classification confidence or margin as multiplying the label with this F of XI.",
            "So if we use this SVM so this GGRFX will be the distance from the hyperplane and if there are if the example is on the positive class, so this number should be positive and if this is from the negative class this number should be negative.",
            "And this why I and F of XI should give us.",
            "A positive number, ideally, and there it should reflect the classification confidence we have in a particular particular example and the normalized class classification confidence is defined as the minimum among these among these individual classifier confidence and this can be considered as a rough definition of the margin so."
        ],
        [
            "What new LP beta does?",
            "Is maximize this minimum margin?",
            "Yeah, but.",
            "The constraint that all then real margin should be greater than greater than the minimum margin.",
            "But we will mistake.",
            "We will make mistakes definitely and to accommodate that mistake we include some slack variables.",
            "So what I have done is that I put.",
            "LP norm constraint on the weights learned for classified version for this particular method.",
            "So this.",
            "Normally we have L1 Norm Dan which give us far solution and in case of informative channel so it throw away useful information while Jen generic norms should give us a different solution.",
            "And if the non value is high we have we have less power solution and we keep the important information information so."
        ],
        [
            "I extend this binary margin definition to multiclass.",
            "Yeah, definitely Martin definition.",
            "To understand this that we first breakdown the problem into one versus all problem.",
            "So what this margin definition is?",
            "It's very intuitive and simple and straightforward.",
            "So what I have is.",
            "For any particular example, I have one true class and the rest classes.",
            "The rest of the one versus all classifier will be from the negative response.",
            "So I take all the confidence from negative from negative classifiers, sum them together by weighting them, and take away from the true class.",
            "Example in this defined multiclass.",
            "Margin and I again proposed maximize this margin.",
            "The this is the normalized margin.",
            "Again the minimum of this margin and with the constraint that each individual margin should be greater than that.",
            "Then the normalized one and accommodate for the negative margin includes like variables and generalized norm constraint."
        ],
        [
            "So there is another variant of this.",
            "In this we have a slightly different definition of margin.",
            "This is actually proposed by Gunnar in late 90s and then reuse by.",
            "I think it's ugly here recently, so.",
            "Now what we do here is we take so there are.",
            "Again.",
            "There is only one positive classifier and the rest are negative.",
            "We take the maximum response among the negative classifier.",
            "So it means we subtract that.",
            "Next we wait that by by Beta R and we take away from the from the true response of that particular example and we do it for all the feature channel.",
            "So like in the previous case.",
            "So we.",
            "Make a we propose to maximize the normalized margin again and with the constraint that each individual margin should we get another normal version.",
            "So basically, this formulation is the same as previous.",
            "So.",
            "We also incorporated this generic norm in this."
        ],
        [
            "So this is another variant of multiclass classifier vision.",
            "In this we add separate constraint for each example in each.",
            "Classifier, so for example, if there are number of classifiers, so for each example there will be N -- 1 number of constraint added.",
            "So this is quite big program and for large data set it will be difficult to and to compute this or even 50 into RAM sometime."
        ],
        [
            "OK, so this is the.",
            "And the next thing which we have done then we extended the stacking.",
            "Normally what people are doing in stacking is they use.",
            "And the labels.",
            "From the classifiers and sometimes the probabilities from the individual based classifier.",
            "But we propose to use this distance from the hyperplane of each.",
            "Example so for multiclass.",
            "Problem we break it into like earlier one versus our problem and.",
            "And then we for each particular example, we take the responses from all the classifier in this act as a base feature.",
            "We make RBF kernel based based on these features and then we do the.",
            "Nonlinear SVM, as the meta level classifier.",
            "So for multi level problem we consider them as.",
            "An independent binary problems, and each of you for each example we consider the response from all these independent minded problem and these act as a base as a feature vector and then we again build the RBF kernel and do nonlinear assume classification as meta level classifier."
        ],
        [
            "So the next thing is that we, along with this BS base kernel, we can consider this extended stacking kernel as another feature channel or another kernel, and it can be combined using MTL.",
            "Well, multiple kernel learning or it can be applied can be combined in the standard classified level fields.",
            "Are you using any of these proposed approaches?"
        ],
        [
            "So far the results we presented results on.",
            "Multi Label an multiclass datasets, particularly in computer vision.",
            "So for multi label data sets I am using Pascal views Video, see 2007 data set.",
            "It's quite challenging data set, Pascal organized.",
            "Every year a competition on their organized several computation and one of them is classification.",
            "And as you can see that in one image you can have truncated included and there a lot of labels, for example human by potted plant, an car and some of the objects are heavily included, so it's quite difficult datasets and the organizer of this data set.",
            "Recommended mean average precision as my year."
        ],
        [
            "So we before going into the results, I would like to show the land wait so these are the weights learned corresponding to Pascal VOC 2007 corresponding to a particular category, and we have different P norms value in there and it goes from one to 10 is 4 four and you can see that far equal to 1.",
            "We have very sparse weight and in this all the other weights are.",
            "And nearly zero, except one which is very high.",
            "And as we increase the norm and the sparsity is decreasing.",
            "So this is important if all the feature channel we have.",
            "If they are informative and we select only one of them, are few of them.",
            "So we will lose performance."
        ],
        [
            "So these are the results.",
            "So top row is.",
            "State of the art.",
            "Multiple kernel learning and with different norms.",
            "So I use.",
            "I Shogun toolbox for MPL.",
            "And then.",
            "L Infinity Norm here is unrated.",
            "Corresponding correspond to embedded sum.",
            "So.",
            "As you can see that multiple kernel learning is not really working because all the channels are informative and we are getting the highest performance, basically without without learning without and putting any weight.",
            "So unweighted sum is the fun better than any of the other multiple confirming approach while classifier Level 2 and improve it.",
            "I am further to buy 1% and the extended stacking further improve it.",
            "Yeah, by another 1% or 2% from the MPL, so it's important to note that there is a machine learning general Paper in 2004.",
            "Where is combining classified with stacking is better than selecting the best one, so we observe quite different things that actually stacking does improve the results.",
            "So.",
            "And combining stacking kernel with the base kernel we.",
            "We get even better performance around 4% better than the."
        ],
        [
            "State of the art.",
            "Then we present results on multiclass datasets we used basically for the paper.",
            "We use three multiclass datasets and these are we in data set flower 17, one or two and Caltech 101, seventeen, 102 and 101 represent the number of classes we have in these datasets.",
            "Then I I think some of the reviewer has a comment that whether these method can is working on pattern general pattern recognition problem or not.",
            "So far that I include for bioinformatic data set.",
            "But these results result for by unformatted data sets that are not in the paper."
        ],
        [
            "So I'm going to get results.",
            "We have state of the art as 66.7.",
            "This is in CPR 2010 and by learning.",
            "In the proposed classifier Fusion schemes.",
            "OK, so the first line here is multiple MCL, multiple kernel learning, which.",
            "Which is from children and all the rest are the proposed yeah so, and for the comparison with state of the art we select best method from the previously published results and these results are compatible because all of these are, these results are by using seven distance matrices provided online by the authors of data set.",
            "So.",
            "We improve the result by by 1% using simple classifier Freeman and then further improve it by by stacking and combining stacking with the base kernels.",
            "Further improve go beyond 90%.",
            "So this is the second data set and we observe basically the same trend here as well.",
            "So this is the state of the art.",
            "OK, this MTL product mean you can consider it as an early fee and becausw we are we are using.",
            "RBF kernels, so if you multiply it so it will be as if you are adding adding the feature the base feature so it can be considered as early Fusion, so it is performing better in this case and that's why we highlighted address as state of the art.",
            "And hour.",
            "Mkell is not giving better result on this data set and you can see a few missing numbers in this and in some other scenarios and cause.",
            "We run this multiple kernel learning and optimize an optimization problem for an LPD for like 3 four weeks and didn't get any result for these norms values or they are quite slow, so this is another disadvantage of these methods.",
            "So we improved our results by roughly 2% and stacking further improve it.",
            "77.7 and combining them with the with the base kernels combining stacking kernel with the base kernel using the proposed classifier Feehan.",
            "Take it beyond 80%.",
            "So this is the last weekend data set Caltech 101.",
            "So the state of the art here is.",
            "68.4 it should be MPL averaging.",
            "I am not using any ham previously reported results be cause these results are by using 10 kernels which I have generated, so it is not fair to compare with the previously proposed method because then the kernels and the number of kernels are different so.",
            "We are.",
            "Talk MPL everything can we consider as all of these feature channels are informative?",
            "So MPL everything can be considered as state of the art, but it's not performing well.",
            "Instead, classifier Fusion with L Infinity norm between equal weighting is giving 68.4.",
            "It's also interesting to note that the empty L with the with some learning of.",
            "It is also performing similar to baselines, which is just simple everything.",
            "So we improve the result 269.",
            "Percent.",
            "So it's taking in this case is not performing better, but not fun.",
            "Better than the baseline, but when combined with the.",
            "Best kernels it.",
            "It helps to improve the result.",
            "So basically by use of stacking kernel it governs the weight because it has information from all these base kernels in it.",
            "So it helps us to determine the optimal weight."
        ],
        [
            "So this is the summary of.",
            "Multiclass data set and data set which.",
            "I have you."
        ],
        [
            "So this these are the results on for bioinformatic data set.",
            "You know that is that the prediction error is measured as the one minus MCC and the results are reported in.",
            "But Sunday, so the smaller the better.",
            "So this is the first 2 lines are GLR this year.",
            "Jennifer Machine learning research and if you go and look at the last line, this is our proposed best one from our proposed classified level Fusion method.",
            "And there we have babies.",
            "State of the art performance with the quite significant difference.",
            "On all of these datasets."
        ],
        [
            "So now the conclusion we proposed.",
            "Multiclass classifier approach, which learned the weights for each class in each feature channel, and it's quite efficient to implement.",
            "It has the number of constraint in.",
            "This is equal to the number of examples in the data set, as opposed to some other previously proposed scheme.",
            "So we also incorporated arbitrary norm and by the addition of these arbitrary norm, these existing classifier vision approaches are now not rejecting the informative channel due to sparsity.",
            "And then they are robust at the same time.",
            "Against my Z ended in general, so I haven't.",
            "Presented a way of selecting the best non value and best P value from the set of values, but it can be learned easily using validation set or using cross validation.",
            "And then we presented a very simple extension of stacking and the father are both multi level and multi class problems and this very simple and intuitive extension is is working better most of the time from from the.",
            "From the state of the art.",
            "MPL, as well as sometime from classified freedom.",
            "So stacking plus base kernels give a state of the art results on all all these datasets and at the end we presented to evaluation on challenging datasets.",
            "Thank you."
        ],
        [
            "Very much for attending and any questions.",
            "So I might agree that this approach might work quite well for feature channels that are not standard vectorial feature presentations.",
            "So I didn't get from the presentation whether the datasets that you analyze whether these were fixed vectorial feature representations as featured channels or not, But the question is basically if you have standard factorial feature representations, would this approach still work or?",
            "At least at that moment, I would consider the best baseline as a method that just concatenates all the features into one big feature vector and and then apply any model on that.",
            "So did you compare with such an approach?",
            "Yep, so and this method which you are saying that you just concatenate the base features first of all the feature which I am using are.",
            "Medical features and.",
            "Basically, most of the time I am using kernels and the low level features are built by the authors and the details of those feature can be found in that effort papers.",
            "But this particular version of.",
            "Mayor of early fee and that take the features and concatenate them simply by putting them next to each other.",
            "Is early Fusion.",
            "And because I am using RBF kernel, so this MKL product everywhere should.",
            "Be back in.",
            "Yeah, so as you can see it's.",
            "Giving 6262.2%, which is like 8% lower.",
            "So do you have an explanation why such an approach is then substantially worse than your approach?",
            "OK, The thing is that you concatenate all the feature and you go it into even more.",
            "This is my estimation, but there can be several different explanations so.",
            "The thing is that.",
            "You are making a big.",
            "Feature vector by concatenating several feature channel.",
            "For example, if you are already in a high for, it's particularly for.",
            "Computer vision application.",
            "So you you work already in like 4000 or 10,000 dimensional feature vector and if you have 30 general so you go straight away in 2.3 million dimensional feature space and then you are relying on on SVM or your classifier will pick up to pick up the best features corresponding to these.",
            "Among these highly, highly high dimensional feature space, so I think.",
            "That's where the bottleneck is for this are leafeon but there can be several other reasons.",
            "So as you can see in other datasets this for example for this data set it's giving better than multiple kernel learning, but still less than classifier Fusion.",
            "So as a classifier can improve it further and you can observe the similar thing on there, so I haven't included the result of early Fusion, which is MPL product 'cause we I am using RBF kernel so that is equal to early Fusion on this, but it should be in the paper and I think it was around.",
            "Around 85% so which is quite low."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I am Muhammad.",
                    "label": 0
                },
                {
                    "sent": "The events from University of Study and CSP is my center, so this is joint work with pay and my PhD Supervisor, Christian McCoy, Jake and Rosa Hitler.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I I don't think that I need much time on motivation 'cause the presenter already have done that, so I will skip this.",
                    "label": 0
                },
                {
                    "sent": "By saying only that I particularly focus on object recognition and we end datasets for this paper, but the techniques can be applied to any pattern recognition problem, provided that we were we have multiple information.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Channels so the problem statement is something like this that we have multiple information channel multiple feature channel can be expected in many different ways and what is the optimal way to combine these multiple feature channels to have maximum performance out of it?",
                    "label": 0
                },
                {
                    "sent": "So before.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I go into the solution of these things.",
                    "label": 0
                },
                {
                    "sent": "I will present my outline so I will quickly go through a couple of existing film techniques and then I will go through the proposed method for multi level and multi class.",
                    "label": 0
                },
                {
                    "sent": "So this one is multi label and there are these three are multi class and then then a very easy extension of stacking and present result and conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Event.",
                    "label": 0
                },
                {
                    "sent": "So multiple can learning is state of the art and state of the art.",
                    "label": 0
                },
                {
                    "sent": "Since I think 2004 or so.",
                    "label": 0
                },
                {
                    "sent": "So how they they do the combination of these different information channel is by maximizing the margin to learn these waves.",
                    "label": 1
                },
                {
                    "sent": "So consider that these feature channels each feature channel give rise to a kernel.",
                    "label": 0
                },
                {
                    "sent": "In some feature space and this kernel get some weight and then you take the linear combination of these these kernels and you get a representative Colonel and you have had it as an nonlinear SVM.",
                    "label": 0
                },
                {
                    "sent": "Me and you get your prediction for both binary and multiclass.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the alternative approaches, late fee and you can see classifier level Fusion in which you have.",
                    "label": 1
                },
                {
                    "sent": "You have your feature channel.",
                    "label": 0
                },
                {
                    "sent": "You apply some classifier.",
                    "label": 0
                },
                {
                    "sent": "It don't have to be SVM, it can be any classifier and you have the scores of your classifier and you learn the wait for these scores somehow and you take the weighted sum.",
                    "label": 0
                },
                {
                    "sent": "There are many other ways to combine these classifier.",
                    "label": 0
                },
                {
                    "sent": "You can take and rule or rule or minimum, maximum, median and so on.",
                    "label": 0
                },
                {
                    "sent": "So why I have put this?",
                    "label": 0
                },
                {
                    "sent": "Because we are we are taking weighted sum of these Bayes classifier score.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The novelty of this paper is on these four.",
                    "label": 0
                },
                {
                    "sent": "Classified free and techniques.",
                    "label": 0
                },
                {
                    "sent": "I extended some existing, which is this?",
                    "label": 0
                },
                {
                    "sent": "This is a variant of MU LP, Adaboost and this extension of LP beta and LPB, and there is a novel multiclass classifier Freehan approach as well, so these are the major nobility of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is this paper?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How how we learned this weights in classified Level 3 and so to understand that.",
                    "label": 0
                },
                {
                    "sent": "We have to understand this GRGR of X is so this, particularly in this slider.",
                    "label": 0
                },
                {
                    "sent": "This is for binary classification.",
                    "label": 0
                },
                {
                    "sent": "If you want so you have this course of base classifier, you wait them by corresponding weights and you take the linear combination and you decide on the sign if it will find reclassification and if it is multi class we go for the maximum response.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I define the classification confidence or margin as multiplying the label with this F of XI.",
                    "label": 0
                },
                {
                    "sent": "So if we use this SVM so this GGRFX will be the distance from the hyperplane and if there are if the example is on the positive class, so this number should be positive and if this is from the negative class this number should be negative.",
                    "label": 0
                },
                {
                    "sent": "And this why I and F of XI should give us.",
                    "label": 0
                },
                {
                    "sent": "A positive number, ideally, and there it should reflect the classification confidence we have in a particular particular example and the normalized class classification confidence is defined as the minimum among these among these individual classifier confidence and this can be considered as a rough definition of the margin so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What new LP beta does?",
                    "label": 0
                },
                {
                    "sent": "Is maximize this minimum margin?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but.",
                    "label": 0
                },
                {
                    "sent": "The constraint that all then real margin should be greater than greater than the minimum margin.",
                    "label": 0
                },
                {
                    "sent": "But we will mistake.",
                    "label": 0
                },
                {
                    "sent": "We will make mistakes definitely and to accommodate that mistake we include some slack variables.",
                    "label": 0
                },
                {
                    "sent": "So what I have done is that I put.",
                    "label": 0
                },
                {
                    "sent": "LP norm constraint on the weights learned for classified version for this particular method.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Normally we have L1 Norm Dan which give us far solution and in case of informative channel so it throw away useful information while Jen generic norms should give us a different solution.",
                    "label": 0
                },
                {
                    "sent": "And if the non value is high we have we have less power solution and we keep the important information information so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I extend this binary margin definition to multiclass.",
                    "label": 0
                },
                {
                    "sent": "Yeah, definitely Martin definition.",
                    "label": 0
                },
                {
                    "sent": "To understand this that we first breakdown the problem into one versus all problem.",
                    "label": 0
                },
                {
                    "sent": "So what this margin definition is?",
                    "label": 0
                },
                {
                    "sent": "It's very intuitive and simple and straightforward.",
                    "label": 0
                },
                {
                    "sent": "So what I have is.",
                    "label": 0
                },
                {
                    "sent": "For any particular example, I have one true class and the rest classes.",
                    "label": 0
                },
                {
                    "sent": "The rest of the one versus all classifier will be from the negative response.",
                    "label": 0
                },
                {
                    "sent": "So I take all the confidence from negative from negative classifiers, sum them together by weighting them, and take away from the true class.",
                    "label": 0
                },
                {
                    "sent": "Example in this defined multiclass.",
                    "label": 0
                },
                {
                    "sent": "Margin and I again proposed maximize this margin.",
                    "label": 0
                },
                {
                    "sent": "The this is the normalized margin.",
                    "label": 0
                },
                {
                    "sent": "Again the minimum of this margin and with the constraint that each individual margin should be greater than that.",
                    "label": 0
                },
                {
                    "sent": "Then the normalized one and accommodate for the negative margin includes like variables and generalized norm constraint.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there is another variant of this.",
                    "label": 0
                },
                {
                    "sent": "In this we have a slightly different definition of margin.",
                    "label": 0
                },
                {
                    "sent": "This is actually proposed by Gunnar in late 90s and then reuse by.",
                    "label": 0
                },
                {
                    "sent": "I think it's ugly here recently, so.",
                    "label": 0
                },
                {
                    "sent": "Now what we do here is we take so there are.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "There is only one positive classifier and the rest are negative.",
                    "label": 0
                },
                {
                    "sent": "We take the maximum response among the negative classifier.",
                    "label": 0
                },
                {
                    "sent": "So it means we subtract that.",
                    "label": 0
                },
                {
                    "sent": "Next we wait that by by Beta R and we take away from the from the true response of that particular example and we do it for all the feature channel.",
                    "label": 0
                },
                {
                    "sent": "So like in the previous case.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Make a we propose to maximize the normalized margin again and with the constraint that each individual margin should we get another normal version.",
                    "label": 0
                },
                {
                    "sent": "So basically, this formulation is the same as previous.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We also incorporated this generic norm in this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is another variant of multiclass classifier vision.",
                    "label": 0
                },
                {
                    "sent": "In this we add separate constraint for each example in each.",
                    "label": 0
                },
                {
                    "sent": "Classifier, so for example, if there are number of classifiers, so for each example there will be N -- 1 number of constraint added.",
                    "label": 0
                },
                {
                    "sent": "So this is quite big program and for large data set it will be difficult to and to compute this or even 50 into RAM sometime.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the.",
                    "label": 0
                },
                {
                    "sent": "And the next thing which we have done then we extended the stacking.",
                    "label": 0
                },
                {
                    "sent": "Normally what people are doing in stacking is they use.",
                    "label": 0
                },
                {
                    "sent": "And the labels.",
                    "label": 0
                },
                {
                    "sent": "From the classifiers and sometimes the probabilities from the individual based classifier.",
                    "label": 0
                },
                {
                    "sent": "But we propose to use this distance from the hyperplane of each.",
                    "label": 0
                },
                {
                    "sent": "Example so for multiclass.",
                    "label": 0
                },
                {
                    "sent": "Problem we break it into like earlier one versus our problem and.",
                    "label": 0
                },
                {
                    "sent": "And then we for each particular example, we take the responses from all the classifier in this act as a base feature.",
                    "label": 1
                },
                {
                    "sent": "We make RBF kernel based based on these features and then we do the.",
                    "label": 1
                },
                {
                    "sent": "Nonlinear SVM, as the meta level classifier.",
                    "label": 1
                },
                {
                    "sent": "So for multi level problem we consider them as.",
                    "label": 0
                },
                {
                    "sent": "An independent binary problems, and each of you for each example we consider the response from all these independent minded problem and these act as a base as a feature vector and then we again build the RBF kernel and do nonlinear assume classification as meta level classifier.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next thing is that we, along with this BS base kernel, we can consider this extended stacking kernel as another feature channel or another kernel, and it can be combined using MTL.",
                    "label": 1
                },
                {
                    "sent": "Well, multiple kernel learning or it can be applied can be combined in the standard classified level fields.",
                    "label": 0
                },
                {
                    "sent": "Are you using any of these proposed approaches?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So far the results we presented results on.",
                    "label": 0
                },
                {
                    "sent": "Multi Label an multiclass datasets, particularly in computer vision.",
                    "label": 0
                },
                {
                    "sent": "So for multi label data sets I am using Pascal views Video, see 2007 data set.",
                    "label": 0
                },
                {
                    "sent": "It's quite challenging data set, Pascal organized.",
                    "label": 0
                },
                {
                    "sent": "Every year a competition on their organized several computation and one of them is classification.",
                    "label": 0
                },
                {
                    "sent": "And as you can see that in one image you can have truncated included and there a lot of labels, for example human by potted plant, an car and some of the objects are heavily included, so it's quite difficult datasets and the organizer of this data set.",
                    "label": 0
                },
                {
                    "sent": "Recommended mean average precision as my year.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we before going into the results, I would like to show the land wait so these are the weights learned corresponding to Pascal VOC 2007 corresponding to a particular category, and we have different P norms value in there and it goes from one to 10 is 4 four and you can see that far equal to 1.",
                    "label": 1
                },
                {
                    "sent": "We have very sparse weight and in this all the other weights are.",
                    "label": 0
                },
                {
                    "sent": "And nearly zero, except one which is very high.",
                    "label": 0
                },
                {
                    "sent": "And as we increase the norm and the sparsity is decreasing.",
                    "label": 0
                },
                {
                    "sent": "So this is important if all the feature channel we have.",
                    "label": 0
                },
                {
                    "sent": "If they are informative and we select only one of them, are few of them.",
                    "label": 0
                },
                {
                    "sent": "So we will lose performance.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the results.",
                    "label": 0
                },
                {
                    "sent": "So top row is.",
                    "label": 0
                },
                {
                    "sent": "State of the art.",
                    "label": 0
                },
                {
                    "sent": "Multiple kernel learning and with different norms.",
                    "label": 0
                },
                {
                    "sent": "So I use.",
                    "label": 0
                },
                {
                    "sent": "I Shogun toolbox for MPL.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "L Infinity Norm here is unrated.",
                    "label": 0
                },
                {
                    "sent": "Corresponding correspond to embedded sum.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As you can see that multiple kernel learning is not really working because all the channels are informative and we are getting the highest performance, basically without without learning without and putting any weight.",
                    "label": 0
                },
                {
                    "sent": "So unweighted sum is the fun better than any of the other multiple confirming approach while classifier Level 2 and improve it.",
                    "label": 0
                },
                {
                    "sent": "I am further to buy 1% and the extended stacking further improve it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, by another 1% or 2% from the MPL, so it's important to note that there is a machine learning general Paper in 2004.",
                    "label": 0
                },
                {
                    "sent": "Where is combining classified with stacking is better than selecting the best one, so we observe quite different things that actually stacking does improve the results.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And combining stacking kernel with the base kernel we.",
                    "label": 0
                },
                {
                    "sent": "We get even better performance around 4% better than the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "State of the art.",
                    "label": 0
                },
                {
                    "sent": "Then we present results on multiclass datasets we used basically for the paper.",
                    "label": 1
                },
                {
                    "sent": "We use three multiclass datasets and these are we in data set flower 17, one or two and Caltech 101, seventeen, 102 and 101 represent the number of classes we have in these datasets.",
                    "label": 1
                },
                {
                    "sent": "Then I I think some of the reviewer has a comment that whether these method can is working on pattern general pattern recognition problem or not.",
                    "label": 0
                },
                {
                    "sent": "So far that I include for bioinformatic data set.",
                    "label": 0
                },
                {
                    "sent": "But these results result for by unformatted data sets that are not in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to get results.",
                    "label": 0
                },
                {
                    "sent": "We have state of the art as 66.7.",
                    "label": 0
                },
                {
                    "sent": "This is in CPR 2010 and by learning.",
                    "label": 0
                },
                {
                    "sent": "In the proposed classifier Fusion schemes.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first line here is multiple MCL, multiple kernel learning, which.",
                    "label": 0
                },
                {
                    "sent": "Which is from children and all the rest are the proposed yeah so, and for the comparison with state of the art we select best method from the previously published results and these results are compatible because all of these are, these results are by using seven distance matrices provided online by the authors of data set.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We improve the result by by 1% using simple classifier Freeman and then further improve it by by stacking and combining stacking with the base kernels.",
                    "label": 0
                },
                {
                    "sent": "Further improve go beyond 90%.",
                    "label": 0
                },
                {
                    "sent": "So this is the second data set and we observe basically the same trend here as well.",
                    "label": 0
                },
                {
                    "sent": "So this is the state of the art.",
                    "label": 0
                },
                {
                    "sent": "OK, this MTL product mean you can consider it as an early fee and becausw we are we are using.",
                    "label": 0
                },
                {
                    "sent": "RBF kernels, so if you multiply it so it will be as if you are adding adding the feature the base feature so it can be considered as early Fusion, so it is performing better in this case and that's why we highlighted address as state of the art.",
                    "label": 0
                },
                {
                    "sent": "And hour.",
                    "label": 0
                },
                {
                    "sent": "Mkell is not giving better result on this data set and you can see a few missing numbers in this and in some other scenarios and cause.",
                    "label": 0
                },
                {
                    "sent": "We run this multiple kernel learning and optimize an optimization problem for an LPD for like 3 four weeks and didn't get any result for these norms values or they are quite slow, so this is another disadvantage of these methods.",
                    "label": 0
                },
                {
                    "sent": "So we improved our results by roughly 2% and stacking further improve it.",
                    "label": 1
                },
                {
                    "sent": "77.7 and combining them with the with the base kernels combining stacking kernel with the base kernel using the proposed classifier Feehan.",
                    "label": 0
                },
                {
                    "sent": "Take it beyond 80%.",
                    "label": 0
                },
                {
                    "sent": "So this is the last weekend data set Caltech 101.",
                    "label": 0
                },
                {
                    "sent": "So the state of the art here is.",
                    "label": 0
                },
                {
                    "sent": "68.4 it should be MPL averaging.",
                    "label": 0
                },
                {
                    "sent": "I am not using any ham previously reported results be cause these results are by using 10 kernels which I have generated, so it is not fair to compare with the previously proposed method because then the kernels and the number of kernels are different so.",
                    "label": 0
                },
                {
                    "sent": "We are.",
                    "label": 0
                },
                {
                    "sent": "Talk MPL everything can we consider as all of these feature channels are informative?",
                    "label": 0
                },
                {
                    "sent": "So MPL everything can be considered as state of the art, but it's not performing well.",
                    "label": 0
                },
                {
                    "sent": "Instead, classifier Fusion with L Infinity norm between equal weighting is giving 68.4.",
                    "label": 0
                },
                {
                    "sent": "It's also interesting to note that the empty L with the with some learning of.",
                    "label": 0
                },
                {
                    "sent": "It is also performing similar to baselines, which is just simple everything.",
                    "label": 0
                },
                {
                    "sent": "So we improve the result 269.",
                    "label": 0
                },
                {
                    "sent": "Percent.",
                    "label": 0
                },
                {
                    "sent": "So it's taking in this case is not performing better, but not fun.",
                    "label": 0
                },
                {
                    "sent": "Better than the baseline, but when combined with the.",
                    "label": 0
                },
                {
                    "sent": "Best kernels it.",
                    "label": 0
                },
                {
                    "sent": "It helps to improve the result.",
                    "label": 0
                },
                {
                    "sent": "So basically by use of stacking kernel it governs the weight because it has information from all these base kernels in it.",
                    "label": 0
                },
                {
                    "sent": "So it helps us to determine the optimal weight.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the summary of.",
                    "label": 0
                },
                {
                    "sent": "Multiclass data set and data set which.",
                    "label": 0
                },
                {
                    "sent": "I have you.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this these are the results on for bioinformatic data set.",
                    "label": 1
                },
                {
                    "sent": "You know that is that the prediction error is measured as the one minus MCC and the results are reported in.",
                    "label": 1
                },
                {
                    "sent": "But Sunday, so the smaller the better.",
                    "label": 0
                },
                {
                    "sent": "So this is the first 2 lines are GLR this year.",
                    "label": 0
                },
                {
                    "sent": "Jennifer Machine learning research and if you go and look at the last line, this is our proposed best one from our proposed classified level Fusion method.",
                    "label": 0
                },
                {
                    "sent": "And there we have babies.",
                    "label": 0
                },
                {
                    "sent": "State of the art performance with the quite significant difference.",
                    "label": 0
                },
                {
                    "sent": "On all of these datasets.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now the conclusion we proposed.",
                    "label": 0
                },
                {
                    "sent": "Multiclass classifier approach, which learned the weights for each class in each feature channel, and it's quite efficient to implement.",
                    "label": 1
                },
                {
                    "sent": "It has the number of constraint in.",
                    "label": 0
                },
                {
                    "sent": "This is equal to the number of examples in the data set, as opposed to some other previously proposed scheme.",
                    "label": 0
                },
                {
                    "sent": "So we also incorporated arbitrary norm and by the addition of these arbitrary norm, these existing classifier vision approaches are now not rejecting the informative channel due to sparsity.",
                    "label": 0
                },
                {
                    "sent": "And then they are robust at the same time.",
                    "label": 0
                },
                {
                    "sent": "Against my Z ended in general, so I haven't.",
                    "label": 1
                },
                {
                    "sent": "Presented a way of selecting the best non value and best P value from the set of values, but it can be learned easily using validation set or using cross validation.",
                    "label": 0
                },
                {
                    "sent": "And then we presented a very simple extension of stacking and the father are both multi level and multi class problems and this very simple and intuitive extension is is working better most of the time from from the.",
                    "label": 0
                },
                {
                    "sent": "From the state of the art.",
                    "label": 0
                },
                {
                    "sent": "MPL, as well as sometime from classified freedom.",
                    "label": 1
                },
                {
                    "sent": "So stacking plus base kernels give a state of the art results on all all these datasets and at the end we presented to evaluation on challenging datasets.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very much for attending and any questions.",
                    "label": 0
                },
                {
                    "sent": "So I might agree that this approach might work quite well for feature channels that are not standard vectorial feature presentations.",
                    "label": 0
                },
                {
                    "sent": "So I didn't get from the presentation whether the datasets that you analyze whether these were fixed vectorial feature representations as featured channels or not, But the question is basically if you have standard factorial feature representations, would this approach still work or?",
                    "label": 0
                },
                {
                    "sent": "At least at that moment, I would consider the best baseline as a method that just concatenates all the features into one big feature vector and and then apply any model on that.",
                    "label": 0
                },
                {
                    "sent": "So did you compare with such an approach?",
                    "label": 0
                },
                {
                    "sent": "Yep, so and this method which you are saying that you just concatenate the base features first of all the feature which I am using are.",
                    "label": 0
                },
                {
                    "sent": "Medical features and.",
                    "label": 0
                },
                {
                    "sent": "Basically, most of the time I am using kernels and the low level features are built by the authors and the details of those feature can be found in that effort papers.",
                    "label": 0
                },
                {
                    "sent": "But this particular version of.",
                    "label": 0
                },
                {
                    "sent": "Mayor of early fee and that take the features and concatenate them simply by putting them next to each other.",
                    "label": 0
                },
                {
                    "sent": "Is early Fusion.",
                    "label": 0
                },
                {
                    "sent": "And because I am using RBF kernel, so this MKL product everywhere should.",
                    "label": 0
                },
                {
                    "sent": "Be back in.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so as you can see it's.",
                    "label": 0
                },
                {
                    "sent": "Giving 6262.2%, which is like 8% lower.",
                    "label": 0
                },
                {
                    "sent": "So do you have an explanation why such an approach is then substantially worse than your approach?",
                    "label": 0
                },
                {
                    "sent": "OK, The thing is that you concatenate all the feature and you go it into even more.",
                    "label": 0
                },
                {
                    "sent": "This is my estimation, but there can be several different explanations so.",
                    "label": 0
                },
                {
                    "sent": "The thing is that.",
                    "label": 0
                },
                {
                    "sent": "You are making a big.",
                    "label": 0
                },
                {
                    "sent": "Feature vector by concatenating several feature channel.",
                    "label": 0
                },
                {
                    "sent": "For example, if you are already in a high for, it's particularly for.",
                    "label": 0
                },
                {
                    "sent": "Computer vision application.",
                    "label": 0
                },
                {
                    "sent": "So you you work already in like 4000 or 10,000 dimensional feature vector and if you have 30 general so you go straight away in 2.3 million dimensional feature space and then you are relying on on SVM or your classifier will pick up to pick up the best features corresponding to these.",
                    "label": 0
                },
                {
                    "sent": "Among these highly, highly high dimensional feature space, so I think.",
                    "label": 0
                },
                {
                    "sent": "That's where the bottleneck is for this are leafeon but there can be several other reasons.",
                    "label": 0
                },
                {
                    "sent": "So as you can see in other datasets this for example for this data set it's giving better than multiple kernel learning, but still less than classifier Fusion.",
                    "label": 0
                },
                {
                    "sent": "So as a classifier can improve it further and you can observe the similar thing on there, so I haven't included the result of early Fusion, which is MPL product 'cause we I am using RBF kernel so that is equal to early Fusion on this, but it should be in the paper and I think it was around.",
                    "label": 0
                },
                {
                    "sent": "Around 85% so which is quite low.",
                    "label": 0
                }
            ]
        }
    }
}