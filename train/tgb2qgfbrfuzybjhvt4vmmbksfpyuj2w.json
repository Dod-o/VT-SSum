{
    "id": "tgb2qgfbrfuzybjhvt4vmmbksfpyuj2w",
    "title": "A Tutorial Introduction to Stochastic Differential Equations: Continuous-time Gaussian Markov Processes",
    "info": {
        "author": [
            "Chris Williams, University of Edinburgh"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "December 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes",
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/dsb06_williams_ctgmp/",
    "segmentation": [
        [
            "Thank you to the to the organizers for inviting me here, and I do want to do is just give a fairly basic introduction to stochastic differential equations so it could be that for a lot of you who work in this area this is actually rather familiar, but hopefully if you're.",
            "Maybe you're just coming out and trying to learn about stochastic differential equations.",
            "I hope this would be of help."
        ],
        [
            "So where I want to start actually is in the discrete time as opposed to the continuous time 'cause that's so perhaps a little bit easier and more intuitive, so so we have some autoregressive process.",
            "X of T is linear combination of the P previous observations plus some noise.",
            "So for example, with this kind of.",
            "I have the physical pointer because.",
            "This is rather small and indistinct dot.",
            "OK thanks OK great OK.",
            "So for example in a R2 process we have this guy is dependent on these two previous time steps and So what we're doing is to generate X of T. We take the values of these guys and add on some linearly weighted and add on some Gaussian noise.",
            "With this coefficient B 0.",
            "So basically if all these all these variables in the past the Gaussian.",
            "Then we know that linear combinations of Gaussians are also Gaussian, so the whole thing will actually be a Gaussian process.",
            "OK, so this is the discrete time thing, and the question is really how do we?",
            "How can we sort of take this kind of model which is a Markov model?",
            "OK right?",
            "We can see that the say this guy here depends only directly on the previous two observations and conditional distribution of this given the whole past in fact only depends on these books two observations.",
            "How could we?"
        ],
        [
            "Move from continuous time to sorry from discrete time to continuous time.",
            "That's the question, and basically the idea is that what we need is not those sort of lagged observations, but in fact we need the derivatives.",
            "So here we have the PTH order derivative an we have this now a differential equation on the left hand side and we're driving that with a white noise process instead of tea on the right hand side.",
            "And just to avoid any sort of stupid rescalings will set a P to be one here OK?",
            "So The thing is that this white noise, this Gaussian white noise has, is covariance of Delta T minus Delta T. So it's a pure white noise process and this is a.",
            "This is basically a stochastic differential equation and we have applications in all sorts of fields, chemistry, Epidemiology, finance and even in appropriately for nips in neural modeling.",
            "And also it's also worth saying that I'm going to talk about driving stochastic differential equations.",
            "Excuse me by white noise, Gaussian white noise.",
            "One can.",
            "One can do more sophisticated things.",
            "One can have non Gaussian distributions and one can even allow to have jumps and so rather than just.",
            "So this is this will just consider the relatively simple case OK.",
            "I should also say that if you want to, if something is not clear, I'm happy for you to ask a question as we go along."
        ],
        [
            "So just one sort of bit of notation.",
            "Obviously fairly clearly what we can do is if we have a discrete time AARP process, we can sort of stack up the state of that to make it a vector AR-1 process.",
            "OK, if if the vector X stores X of T and also the previous P -- 1 observations, and basically we want to do this at the same trick for stochastic differential equation just the same way that you would do this for basically turning high order ODI into a bunch of 1st although dies.",
            "Vector ODS.",
            "What we can do is just define these derivatives so X 2X1 dot and so on.",
            "So this XP is the P -- 1 TH derivative and so and therefore we can just get it in this form, which means that we can also."
        ],
        [
            "Sort of get a vector form for this stochastic differential equation.",
            "OK, so we can deal with the high order derivatives by using this trick where we have some particular structure for this matrix F here.",
            "OK, so that's the."
        ],
        [
            "Set up an.",
            "I want to tell you, sort of what I need to tell you about in order to you bet stochastic differential equations.",
            "First of all, we need to talk about the vena process.",
            "Then what I want to do is and I think this is actually for computer science audiences.",
            "Probably nice way to think about things.",
            "How to actually simulate from stochastic differential equations?",
            "Basically running forward.",
            "That makes a lot of sense.",
            "The Nice the interesting thing about gassing process is derived from stochastic differential equations is that they're both Gaussian, an Markle, so we can certainly apply the sort of Gaussian process machinery that we have to.",
            "To understand, for example, covariance functions and so on with these processes.",
            "Of course, we can also do inference if we have some observations and want to make predictions for other time points, say, and then perhaps an approach that emphasizes more than the mark of aspects, perhaps more than the Gaussian aspect, is the Fokker Planck equation set up, so I want to at least talk briefly about that, and so I guess.",
            "One thing one thing I'm trying to do in this tutorial is to talk about the sort of the SDE view.",
            "This sort of Gaussian process view and also the Fokker Planck view.",
            "Hopefully hopefully I'll connect up."
        ],
        [
            "OK, so what's the new process?",
            "It's a.",
            "It's a stochastic process that's continuous.",
            "It starts off.",
            "Excuse me at zero at Time Zero and anytime T it has variance T and it's just a Gaussian random variable.",
            "What's particularly important is that it has this property of independent increments, so that WF T -- S minus WS is just just depends on essentially on this.",
            "It's just a Gaussian random variable that doesn't depend on the history.",
            "OK, so it has these independent increments.",
            "Given this structure, one can easily workout what the covariance of.",
            "22 Savina process variables at different times.",
            "It's like this and something that's going to be kind of important.",
            "Little bit later is that when we think about stochastic differential equations, we want to interpret this DW of T as being vena process at T plus Delta T minus not at T. So."
        ],
        [
            "I'm sure you've all seen pictures of simulations from the video process.",
            "Obviously this is some discretization, so we're starting at 0 and we just wandering off.",
            "And if we drew many of these with that, of course see that the variance here scales proportionally with time."
        ],
        [
            "OK, and it's probably also fairly familiar, but for a general stochastic process we can define the mean function just the expected value at time T and also the covariance function like this.",
            "And as we know, Gaussian processes are stochastic, process is just defined in terms of their means and covariance function."
        ],
        [
            "OK, so.",
            "We now need to get a little bit deal a little bit more formally with the gastic differential equation.",
            "Y wrote early on was that we had this white noise process.",
            "Is it said of tea and I guess I think the usual name for this is a large of an equation where we have this kind of white noise driving term.",
            "Mathematically though, this is actually a bit.",
            "Pleasant becausw we want to think of that of T as being derivative ovina process, but we know that in fact the Davina process is not differentiable.",
            "So mathematically this kind of thing is actually maybe not the best way to think about things and better way to write this kind of stochastic differential equations in this form.",
            "So we basically think of the change.",
            "The increment here is being given by this term like this just and also that we have this DWF T over on the right hand side and we miss.",
            "We already explained that DW is actually just the increment, so W + T plus Delta T -- W of tea.",
            "OK, so this is the standard way in which stochastic differential equations are written."
        ],
        [
            "So OK, I've got this classic differential equation.",
            "One thing that's very simple that I could do is actually to simulate from it.",
            "And OK, what we could do to do that is to write down a bunch of time points.",
            "Zero, T1 etc and also define these intervals.",
            "Delta T is just T I + 1 -- T I and then what are we going to do with this consentually going to take that differential equation we had and this sort of DX is just sort of X y + 1 -- X or Y.",
            "We've got this this time interval, so we just do the deterministic dynamics, which we just integrate them forward through time Delta T, and then also we're going to add some some white noise and the key thing here is that if we have a time interval length Delta T, we know that we can simulate from the vena process by drawing a standard Gaussian variables that I will then scaling it by the square root of the time interval, right so that.",
            "Basically, so that that means that the variance will have.",
            "Will be Delta T so this is how we can just forward simulate from the static differential equation and OK. And of course what we want to do is to take these these intervals very small and in the in the limit as Delta T goes to zero will actually have a true generative process for the stochastic differential equation.",
            "So it's worth saying that there are in fact this is just like Euler integration of standard ODS.",
            "This is the simplest thing you can do and it's certainly true that there are.",
            "More complex schemes.",
            "Higher order schemes that you can use for this kind of thing.",
            "But I think that it's probably worth thinking about this.",
            "This is perhaps a nice way to just.",
            "To think about how stochastic differential equations work."
        ],
        [
            "We also need to think a little bit and define what stochastic integration means.",
            "So if we have a normal, say Riemann integral, we just basically divide up the interval and define some.",
            "Some some in the limit that then goes to Infinity.",
            "Basically for the stochastic integral, East under the ITO interpretation will have something similar, where now this DWT we said it was just.",
            "The vena process at T plus at the T plus Delta T minus WFT.",
            "Basically, when we do, we have this formula.",
            "Here.",
            "Let me just take the limit as N goes to Infinity will also get.",
            "This is the definition of ITO stochastic integral.",
            "It's worth saying, in this kind of useful mnemonic need to think about this, which is that we've already seen that the.",
            "Does this square root of T scaling for the stochastic part when we are simulating the stochastic differential equation and so we can think of mnemonically, at least that.",
            "EW of T squared is of order.",
            "Delta T OK, so this is important, at least when sometimes when doing some derivations and making sure we've got all the right terms there.",
            "Because this scaling means, for example, if you think you might need to expand to say first order you make, you may actually need to expand the 2nd order to take into account the fact that this DW squared is actually order Delta T."
        ],
        [
            "We've looked at the and the stochastic differential equation.",
            "I actually put up early on is just one.",
            "It's a linear stochastic differential equation, general form of a diffusion process is actually like this, so we have some function here, a of X&T and also some matrix B of X&T here.",
            "So the AI is the drift vector and B is the diffusion matrix.",
            "OK, so we have a general form of this.",
            "The solution by integrating this forward.",
            "And it turns out that for diffusion process, is the sample parts are actually continuous.",
            "It also is password saying that.",
            "If we, if we thinking about how we actually can simulate from this stochastic differential equation, it's clear that these functions A&B must be non anticipating.",
            "That means that basically they should only look back in time and not look forward in time right?",
            "If we if we required values of the process from which we haven't yet created, that would be a problem.",
            "So AMB and an anticipated."
        ],
        [
            "So.",
            "Let's just give some simple examples.",
            "We can have the visa process basically just where the X is, just EW, and then the only thing that's happening here is we get an offset relative to the standard vendor process which starts at 0.",
            "We can also have just re scaling it and having your drift term.",
            "So we in this case will get this kind of thing and also perhaps a little bit more interesting.",
            "We can have the on stoneback process where we have some tendency to revert towards the mean.",
            "The meaning zero.",
            "OK, So what this term is doing is saying OK we're being pulled back towards the origin and then we're adding noise and so in that case we can also get this kind of solution.",
            "So these are some standard, very simple examples."
        ],
        [
            "One other thing that we need to define full stochastic differential equations is this notion of infinitesimal moment.",
            "So we can look at.",
            "Basically, the change between XT&X of T plus Delta T and we just see that for example, from our simulation form.",
            "It's at this kind.",
            "We can look at the first moment.",
            "We just look at the how the change of Delta X the expected value of that scales with Delta T and that in fact defines the drift term.",
            "And also if we compute this guy here, the covariance of Delta X and scale that again by Delta T, then we actually see that this gives this BB transpose.",
            "So this is a nice example where you see with the square root stuff coming in OK.",
            "If you look at if you take this right hand side here expand this up.",
            "And then look at what's going on and take the expectations.",
            "Basically, you're going to get something of order Delta T ^2, you know a A transpose Delta T ^2, but also this.",
            "Terminal the Delta T that comes from the the white noise process.",
            "The visa process so there you can see how the.",
            "Scaling of the white noise here is actually important to get this to get this diffusion term."
        ],
        [
            "OK, so.",
            "That's talking about stochastic differential equations.",
            "What I want to do now is to talk about how to connect that to Gaussian processes, and I'm going to do this for the stationary case.",
            "We're going to have some if we have time invariant coefficients of the stochastic differential equation.",
            "I'm going to basically try to work out what the covariance function is that correspond.",
            "Excuse me, that corresponds to this this process.",
            "And I should say OK, will do it first for the, for the.",
            "The univariate case and we could also generalize that bit later."
        ],
        [
            "So.",
            "The easy way to actually do this analysis is through free analysis.",
            "We've got a linear time invariant system.",
            "That means that the eigenbasis for that is the Fourier basis.",
            "That means it's going to be easy to work in that basis, so we'll just take the take the free transform of some process XT here, and get some X~ of S, and we get the usual Fourier inversion formula and the key thing is that when we differentiate this process, we bring this down 2\u03c0 S to the K. Term in the in the Fourier transform.",
            "That means that looking at the basic stochastic differential equation that we have here, we can.",
            "We can just create transform both sides to get this guy.",
            "And."
        ],
        [
            "Then we can then just take this expectation in Fourier space to get the power spectrum.",
            "And of course, what we want to do is that because we have the V neck Eugene Theorem, we can relate the power spectrum to the covariance function.",
            "So the easy thing to get out of this to this linear time invariant stochastic differential equation is the power spectrum, and we can make that relates directly to the.",
            "To the coefficients in the stochastic differential equation.",
            "OK, so let me just explain a little bit.",
            "This is a polynomial A of zed.",
            "Is this polynomial, akas, enter the K. This is just the so we take a of 2\u03c0 S and we take its complex conjugate and that's this denominator here.",
            "So basically from those.",
            "From the stochastic differential equation, we can get the form of the power spectrum and then obviously if we can do the Fourier inversion, we can then work out what the corresponding covariance function is Guido.",
            "OK.",
            "Right, OK, so you might.",
            "If you're hardcore mathematician, you would certainly want to worry about these.",
            "Whether these integrals exist.",
            "OK, so there is.",
            "There is a bunch of theory that shows that, at least in the right conditions they do.",
            "I mean, I agree you have to worry about that, but that will never actually does work.",
            "Define them as limits 1st and then show that among expectations they have their own.",
            "Never really work in practice.",
            "It's not going to.",
            "And X is not decay, right?",
            "Sure, so.",
            "So you could probably introduce limits, yes, sure.",
            "And then you can use them still, these things.",
            "Under certain expectations I guess, yeah.",
            "OK, so it's clear that what we need to do right is that we need we actually need this expectation right here, right?",
            "So yes.",
            "I don't have a. I don't have a great answer for this, OK, but I guess that you're right that you can.",
            "I know that show that you can be made to work under the usual kinds of tricks that you might need that you're describing.",
            "OK, so let's."
        ],
        [
            "Look at.",
            "Let's look at actually just some simple examples corresponding to that.",
            "So if we just take a first order stochastic differential equation, are all friendly on stone Becca process, then we see past respect from goes down as S squared for large for large S, and that corresponds to this just this.",
            "This covariance function here, and of course."
        ],
        [
            "What?",
            "What we know is if we simulate from this, say, here's a sample at some discretization, then we see this very rough process.",
            "OK, it turns out that this process will be.",
            "It will be continuous, but nowhere well probability one nowhere differentiable, so we'll see this very rough process similar to the vino process.",
            "By contrast, if we have a second order differential equation, so here, just think of this as a damped simple harmonic oscillator.",
            "OK, we've got this X TT, but say not X of T. That would just be.",
            "It's basic simple harmonic oscillator being driven by white noise.",
            "If we did that then in fact what we can see from over here is we'll actually get a blow up in the power spectrum.",
            "If this term doesn't exist, but anyway.",
            "At the resonant frequency.",
            "But if we add in some damping we can get some covariance function that corresponds to that and it said this form and if we sample from that we'll end up with this.",
            "This process here.",
            "Well, here's a sample from the process which has as you would expect, it has a periodicity at least sort of an approximate periodicity here, which comes from the resonant frequency of the simple harmonic oscillator.",
            "You can see this is a peak in the path spectrum OK."
        ],
        [
            "OK, I've been talking about.",
            "The univariate case just want to say something about the multivariate case.",
            "We can have a vector orange to inject process here and here I'm coming back to this linear system.",
            "Basically we can.",
            "It turns out we can actually do the integration to get the solution.",
            "And.",
            "Obviously, one thing we can do looking for stationary solution, we have some initial condition that this is decaying away.",
            "So basically, if we sort of move X0 back to sort of minus Infinity then this is the term that will dominate.",
            "This determines is relevant and then we can workout the covariance structure at anytime, while times T&S in this way, which comes out for these simply like this again, we see exactly this min of TNS, but this is because we've got these two when we've got X of T in excess.",
            "Whichever TUS larger.",
            "Basically we've got this some whole bunch of Gaussian variables which are the same you know, up to the smaller time and then the rest of the stuff here, which actually is irrelevant because the uncorrelated with the previous times.",
            "So we'll end up with this kind of structure here."
        ],
        [
            "And there are various ways of actually say computing this covariance.",
            "That's at time zero corresponding some matrix equation and then you can actually get the covariance that time offsets just by this exponential factor here.",
            "So basically certainly for this is for the.",
            "Is it for the time invariant linear system we can actually get solutions for the covariance function from the underlying matrix stochastic differential equations?",
            "There's a clearly there's a lot more theory here that that one can do.",
            "We can please use the spectral techniques that we use for the univariate case as well, and there's more stuff in Gardner's book, for example on that."
        ],
        [
            "One of the things that we actually saw when we looked at those samples from the free from the Internet process and also from the damp stochastic sorry, simple harmonic oscillator is that we actually saw very different smoothness properties of that first order and 2nd order SD and where this is coming from is basically that.",
            "Casting differential equations of order PA P -- 1 times mean squared differentiable.",
            "The processes correspond to that had this differentiability.",
            "So if P is one, we get zero differentiability.",
            "We get content continuity from the diffusion process but not differentiability.",
            "And if P is 2 we get one, we get one times differentiability and basically an easy intuitive way to think about this is to say that cause a process gets rougher the more times it gets differentiated.",
            "Then basically the roughest thing here X of P is of order white noise which is non differentiable.",
            "So basically as we lift these things up as we integrate it various various times, we'll see we'll see differentiability appearing so we can sort of intuitively at least understand they mean squared differentiability of the resulting process through this kind of argument."
        ],
        [
            "OK, this is this is an interesting point and I guess it's quite related to things that we want them.",
            "I want to talk about today Becausw it's question about how we actually do discretization of continuous time Gaussian Markov process.",
            "Say we have a.",
            "An order P stochastic differential equation.",
            "Then the question is really how will that relate?",
            "Will if we discretize that and some some time interval, say some grid H?",
            "Is it the case that the discretised?",
            "Process will have the same Markov properties as the underlying process, and the answer is you need to be careful.",
            "OK, so let's define certainly for discrete time we can define an ARMA process.",
            "So not only the missing AI in here sorry, so we have this user.",
            "The linear terms that we had before.",
            "Auto regressive terms and here rather than just having B 0 so just driving it with noise at time at the current time will drive it with the lag history of these noises.",
            "This is the Arma process and we can also have a continuous time analog of this where it turns out that the power spectrum now has this rational form.",
            "Pizza in fact, get the order of Q here to be less than the order of P. In order for this to be well defined anyway, the key.",
            "Key theorem here.",
            "Is that if we take a continuous time station Gaussian process and let X be a discretization of that process if.",
            "X is an armor processor.",
            "Has this kind of power spectral density, and XH will be a discrete time armor process.",
            "However, if X is an AR process, and so in other words, it's a stochastic differential equation of order P of the kind we've been talking about, then actually when we discretize, it is not necessarily an AR process.",
            "It actually turns out to be an armor process, so one needs to be a little bit careful about the going from continuous time to discrete time because of this."
        ],
        [
            "OK, I also want to say something about inference.",
            "Of course.",
            "One thing that we certainly want to do a lot in Gaussian process is we want to have some.",
            "We've got this underlying stochastic differential equation that's giving rise to a Gaussian process.",
            "Prior over functions.",
            "We have some observations of that.",
            "That process at various times and you might want to do inference about predicting the values at other times.",
            "If we're dealing with something like an Ornstein Uhlenbeck process, then in fact we've already seen we have this Markov property, which means that we only need to condition.",
            "Essentially, if we're making a prediction at some time T star, then we've got a whole bunch of observations at various times, but we actually need to only worry about those observations in the immediate past and the immediate future of their observed at time T star because of the Markov property.",
            "Everything else is is.",
            "Independent is conditionally independent of the rest of the variables given the past and the future variables, so there's a slight caveat here.",
            "OK, which is that if we have pure observations of the process this is true, but if we have observations which are corrupted by noise, then in fact then this Markov property breaks and we actually need.",
            "If we do get in process, prediction will need to actually make use of all the observations at various times.",
            "So basically this stuff how to do this prediction is just.",
            "An example of Gaussian pro."
        ],
        [
            "This prediction, so we're interested in this conditional distribution of X of T star.",
            "Given these observations under the Gaussian process, we know it's going to be.",
            "It's going to be a Gaussian prediction with some mean and some variants and the usual Gaussian process prediction equations apply.",
            "There kind of slightly simplified cause.",
            "In fact, we only need to worry about these two observations just in the past in the future, so we can just define this two by two covariance matrix where just for example, K star P is.",
            "CFT stone TP.",
            "OK so this is just K of TP and TP and so on and just do this very simple matrix inversion to get these predictions so at least under the if we can really exploit the Markov nature of the autonomic process, for example 2.",
            "To make these to make these predictions in very compact way?"
        ],
        [
            "OK, so.",
            "I've talked about the stochastic differential equation view I've talked about how to workout covariance functions of these processes and try to understand those covariance functions.",
            "I also want to talk a bit about Fokker Planck equations.",
            "So basically one thing that we could be interested in is saying OK, I'm at value X zero at time zero, and what I want to do is to understand how much that evolution of the where I'm going to be at.",
            "So P of X at time T. Given my if I'm at some.",
            "Values X zero time 0 so basically we're interested in this transition PDF which defines this and basically one can show that.",
            "The equations that govern the evolution of the transition probability is is a PDE second well, OK, so just DT by the P on the on the left here and we get terms from the drift and also from the diffusion.",
            "So we get second order derivatives here with respect to the diffusion terms and 1st order derivatives with respect to the drift.",
            "So basically what we want to do, we might want to understand.",
            "Even this evolution of this transition probability in terms of the Fokker Planck equation and so basically how to derive this is just just sort of continuous time version of Chapman Kolmogorov equation.",
            "Basically we have some probability.",
            "At time X0T0 what time T0 and we considering all possible paths to get to how we can be at some at X at time T?",
            "And so this is actually the forward Fokker Planck equation.",
            "And there's also a backward Fokker Planck equation as well.",
            "Sorry, yes.",
            "OK, that sounds bad.",
            "OK, I guess this square bracket is wrong, OK?",
            "So yeah.",
            "That remains a density after losing 1% or.",
            "I think because you're driving it from the Chapman, golf equation table, it's actually it's guaranteed to do that, right?",
            "You are building in that condition.",
            "That's how you're actually driving this thing, right is actually.",
            "So let me give you."
        ],
        [
            "A simple example, very simple example of a Fokker Planck equation.",
            "If we just have the Vilna process with scaling and drift, then we'll have.",
            "Then we said this was the stochastic form of the solution.",
            "It's then very easy to see that the mean.",
            "The mean value of this is at time T is just X 0 + A T and that there will be some variance in fact which is going as Sigma squared T. OK, so this is this is just the cost of Gaussian distribution at time T with mean X 0 + 80 and this variance Sigma squared T. It's very simple example."
        ],
        [
            "I think that perhaps the main point where Fokker Planck equations have particular value is in relation to when we have certain boundary conditions for this PZ.",
            "For example, one problem which relates to neural computation is that if we start off with.",
            "At.",
            "Time 0 here.",
            "Maybe an we can think of this as like the resting potential of a neuron after firing.",
            "We drive it with some noisy input signal on the potential side.",
            "Response to that.",
            "Maybe if it follows the vena process then we might be interested in when that potential crosses the threshold for firing of that neuron and would be interested in this passage time here about when this actually happens, OK?",
            "So in these kinds of conditions, when we're looking at.",
            "One thing we can do, of course, then, is that we could be interested in the first passage time density of essentially what's the probability distribution as a function of time when this when this process on samples in this process first up cross the.",
            "Alright, this barrier here, but we can also be interested in maybe if we were just interested in the.",
            "Probability distribution at of, you know at some particular time here of what the what the value of X is at some time and which essentially what we have to do is to kind of siphon off all the probability that's already crossed.",
            "Here.",
            "All the paths that have already crossed we normalize and look at that distribution.",
            "Of of the process at some time.",
            "Then we can obtain this through Fokker Planck kinds of analysis.",
            "So basically the Fokker Planck stuff depends crucially on the Markov properties of these processes, right?",
            "It's the mark of structure of the Chapman Kolmogorov equations that actually allow us to do this analysis.",
            "And so their various.",
            "Erica, you see that this is not radically new theory.",
            "This goes back to fellow in 1952.",
            "One can characterize various.",
            "Various kinds of boundary conditions we can be interested in.",
            "We could have absorbing barriers we could have, for example, reflecting barriers.",
            "Maybe we could trap this particle in between two barriers here and be interested in the evolution of the probability transition probability as a function of time and so on.",
            "Alright, some other conditions here as well."
        ],
        [
            "One of the things that was mentioned in the introduction of course, is that we.",
            "I described a lot about how we can actually.",
            "Simulate from this process and understand properties of it.",
            "One thing that certainly as machine learners might well want to do is to actually to maybe do some parameter estimation and at least in this simple case that's actually fairly straightforward, 'cause if we have some observations at various times then the key property that actually makes Gaussian process is to work nice to work with in respect to, say parameter estimation is that we can actually just write down likelihood of these observations.",
            "Given the parameters and, this is just.",
            "This is just a sort of multivariate Gaussian right?",
            "It's just basically saying I've got some covariance matrix corresponding to these.",
            "These these end different points X one up to XTI can look at that covariance matrix.",
            "I've got some mean I've got some covariance and well, I've really got is just one data point and end dimensional data point in that.",
            "For Gaussian, well, I'm trying to do is to change the parameters of that Gaussian so as to maximize the likelihood, so that's at least in this simple case that's actually at least conceptually fairly straightforward, and one can, for example, at least, if one can compute the covariance matrix, then you can do this.",
            "OK. And so one can obtain the gradients of this likelihood very very simply just for matrix algebra and then use a numerical methods to optimize the parameters.",
            "So this is exactly how we do, say model fitting for Gaussian processes and the machine learning case is like this.",
            "One can also do theory about when we actually have continuous observations, but I guess that I'm sure, although this makes sense mathematically, I'm not sure it makes so much sense in practice for things that we really want to do, and if it looks upset, OK?"
        ],
        [
            "OK, so maybe just to summarize I guess.",
            "When I tried to do is to give some kind of introduction to SDS talking about.",
            "Being and I think the nice place to start is actually the simulation framework and think how to drive these forward in time.",
            "We have this key property of these processes are both being Gaussian and being Markov, which gives a very special structure and allows us to do certain things.",
            "There is quite a lot of formal machinery for stochastic integration and so on.",
            "And we talked about the meaning covariance, functions of Gaussians and so I had to do that.",
            "How to see the market look at the market properties and get full Planck equations.",
            "OK, so this is.",
            "Simple introduction, of course there's a couple of things that I haven't talked about.",
            "The important one is of course we want to talk about observation.",
            "Or is we actually want to do things like continuous time Kalman filtering.",
            "And again, this is not something that's hot off the press.",
            "And also of course really what the challenges of the workshop are, which has to go beyond these these these linear models to talk about nonlinear dynamics and also nonlinear operations.",
            "OK, thank you.",
            "So Matthias.",
            "Wanted to say OK, just gotta process within the marriage because I think what it feels like it went to complete this time is because it's not.",
            "I want.",
            "Special.",
            "Send an AR.",
            "Processing type of where the matrix has a very constructive so?",
            "I mean are there?",
            "So I certainly agree that.",
            "If we're talking about this parameter estimation case that.",
            "Well.",
            "Yes, you don't need to approach it.",
            "You can exploit the structure in the problem so you don't actually have to work with that.",
            "That necessary that big matrix where you can see how basically how that will you can exploit the structure that comes from the SD, so you don't actually have to scale.",
            "Saying cube like that, it's definitely true.",
            "I guess I think I actually have to think about that.",
            "I'm not exactly sure how that will workout, but pretty confident to locate fairly easily.",
            "This.",
            "Any other questions?",
            "I have a question about the market property of the process when you have noisy observations.",
            "I understand what you say about that.",
            "You lose some of this market market property, but I think if you view the observed the process conditioned on noisy observations as a as a process as a process, then you can show that this is again a Markovian.",
            "Do you OK so?",
            "Maybe I don't understand.",
            "I mean certainly for the.",
            "For example, you know in the hidden Markov model the process on the observations is no longer Markovian, right?",
            "If you just try to integrate process.",
            "Sorry.",
            "Conditioned on the observations.",
            "Right?",
            "Yes.",
            "About that but.",
            "Yes indeed.",
            "Join sure.",
            "Yes, so OK so that yes that's true, Thanks yeah.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you to the to the organizers for inviting me here, and I do want to do is just give a fairly basic introduction to stochastic differential equations so it could be that for a lot of you who work in this area this is actually rather familiar, but hopefully if you're.",
                    "label": 1
                },
                {
                    "sent": "Maybe you're just coming out and trying to learn about stochastic differential equations.",
                    "label": 0
                },
                {
                    "sent": "I hope this would be of help.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So where I want to start actually is in the discrete time as opposed to the continuous time 'cause that's so perhaps a little bit easier and more intuitive, so so we have some autoregressive process.",
                    "label": 0
                },
                {
                    "sent": "X of T is linear combination of the P previous observations plus some noise.",
                    "label": 0
                },
                {
                    "sent": "So for example, with this kind of.",
                    "label": 0
                },
                {
                    "sent": "I have the physical pointer because.",
                    "label": 0
                },
                {
                    "sent": "This is rather small and indistinct dot.",
                    "label": 0
                },
                {
                    "sent": "OK thanks OK great OK.",
                    "label": 0
                },
                {
                    "sent": "So for example in a R2 process we have this guy is dependent on these two previous time steps and So what we're doing is to generate X of T. We take the values of these guys and add on some linearly weighted and add on some Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "With this coefficient B 0.",
                    "label": 0
                },
                {
                    "sent": "So basically if all these all these variables in the past the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Then we know that linear combinations of Gaussians are also Gaussian, so the whole thing will actually be a Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is the discrete time thing, and the question is really how do we?",
                    "label": 0
                },
                {
                    "sent": "How can we sort of take this kind of model which is a Markov model?",
                    "label": 0
                },
                {
                    "sent": "OK right?",
                    "label": 0
                },
                {
                    "sent": "We can see that the say this guy here depends only directly on the previous two observations and conditional distribution of this given the whole past in fact only depends on these books two observations.",
                    "label": 0
                },
                {
                    "sent": "How could we?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Move from continuous time to sorry from discrete time to continuous time.",
                    "label": 1
                },
                {
                    "sent": "That's the question, and basically the idea is that what we need is not those sort of lagged observations, but in fact we need the derivatives.",
                    "label": 0
                },
                {
                    "sent": "So here we have the PTH order derivative an we have this now a differential equation on the left hand side and we're driving that with a white noise process instead of tea on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "And just to avoid any sort of stupid rescalings will set a P to be one here OK?",
                    "label": 0
                },
                {
                    "sent": "So The thing is that this white noise, this Gaussian white noise has, is covariance of Delta T minus Delta T. So it's a pure white noise process and this is a.",
                    "label": 1
                },
                {
                    "sent": "This is basically a stochastic differential equation and we have applications in all sorts of fields, chemistry, Epidemiology, finance and even in appropriately for nips in neural modeling.",
                    "label": 1
                },
                {
                    "sent": "And also it's also worth saying that I'm going to talk about driving stochastic differential equations.",
                    "label": 0
                },
                {
                    "sent": "Excuse me by white noise, Gaussian white noise.",
                    "label": 0
                },
                {
                    "sent": "One can.",
                    "label": 0
                },
                {
                    "sent": "One can do more sophisticated things.",
                    "label": 0
                },
                {
                    "sent": "One can have non Gaussian distributions and one can even allow to have jumps and so rather than just.",
                    "label": 0
                },
                {
                    "sent": "So this is this will just consider the relatively simple case OK.",
                    "label": 0
                },
                {
                    "sent": "I should also say that if you want to, if something is not clear, I'm happy for you to ask a question as we go along.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just one sort of bit of notation.",
                    "label": 0
                },
                {
                    "sent": "Obviously fairly clearly what we can do is if we have a discrete time AARP process, we can sort of stack up the state of that to make it a vector AR-1 process.",
                    "label": 1
                },
                {
                    "sent": "OK, if if the vector X stores X of T and also the previous P -- 1 observations, and basically we want to do this at the same trick for stochastic differential equation just the same way that you would do this for basically turning high order ODI into a bunch of 1st although dies.",
                    "label": 0
                },
                {
                    "sent": "Vector ODS.",
                    "label": 0
                },
                {
                    "sent": "What we can do is just define these derivatives so X 2X1 dot and so on.",
                    "label": 0
                },
                {
                    "sent": "So this XP is the P -- 1 TH derivative and so and therefore we can just get it in this form, which means that we can also.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sort of get a vector form for this stochastic differential equation.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can deal with the high order derivatives by using this trick where we have some particular structure for this matrix F here.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Set up an.",
                    "label": 0
                },
                {
                    "sent": "I want to tell you, sort of what I need to tell you about in order to you bet stochastic differential equations.",
                    "label": 0
                },
                {
                    "sent": "First of all, we need to talk about the vena process.",
                    "label": 0
                },
                {
                    "sent": "Then what I want to do is and I think this is actually for computer science audiences.",
                    "label": 0
                },
                {
                    "sent": "Probably nice way to think about things.",
                    "label": 0
                },
                {
                    "sent": "How to actually simulate from stochastic differential equations?",
                    "label": 0
                },
                {
                    "sent": "Basically running forward.",
                    "label": 0
                },
                {
                    "sent": "That makes a lot of sense.",
                    "label": 0
                },
                {
                    "sent": "The Nice the interesting thing about gassing process is derived from stochastic differential equations is that they're both Gaussian, an Markle, so we can certainly apply the sort of Gaussian process machinery that we have to.",
                    "label": 0
                },
                {
                    "sent": "To understand, for example, covariance functions and so on with these processes.",
                    "label": 1
                },
                {
                    "sent": "Of course, we can also do inference if we have some observations and want to make predictions for other time points, say, and then perhaps an approach that emphasizes more than the mark of aspects, perhaps more than the Gaussian aspect, is the Fokker Planck equation set up, so I want to at least talk briefly about that, and so I guess.",
                    "label": 0
                },
                {
                    "sent": "One thing one thing I'm trying to do in this tutorial is to talk about the sort of the SDE view.",
                    "label": 1
                },
                {
                    "sent": "This sort of Gaussian process view and also the Fokker Planck view.",
                    "label": 0
                },
                {
                    "sent": "Hopefully hopefully I'll connect up.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so what's the new process?",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's a stochastic process that's continuous.",
                    "label": 0
                },
                {
                    "sent": "It starts off.",
                    "label": 0
                },
                {
                    "sent": "Excuse me at zero at Time Zero and anytime T it has variance T and it's just a Gaussian random variable.",
                    "label": 0
                },
                {
                    "sent": "What's particularly important is that it has this property of independent increments, so that WF T -- S minus WS is just just depends on essentially on this.",
                    "label": 1
                },
                {
                    "sent": "It's just a Gaussian random variable that doesn't depend on the history.",
                    "label": 0
                },
                {
                    "sent": "OK, so it has these independent increments.",
                    "label": 0
                },
                {
                    "sent": "Given this structure, one can easily workout what the covariance of.",
                    "label": 0
                },
                {
                    "sent": "22 Savina process variables at different times.",
                    "label": 0
                },
                {
                    "sent": "It's like this and something that's going to be kind of important.",
                    "label": 0
                },
                {
                    "sent": "Little bit later is that when we think about stochastic differential equations, we want to interpret this DW of T as being vena process at T plus Delta T minus not at T. So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm sure you've all seen pictures of simulations from the video process.",
                    "label": 0
                },
                {
                    "sent": "Obviously this is some discretization, so we're starting at 0 and we just wandering off.",
                    "label": 0
                },
                {
                    "sent": "And if we drew many of these with that, of course see that the variance here scales proportionally with time.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and it's probably also fairly familiar, but for a general stochastic process we can define the mean function just the expected value at time T and also the covariance function like this.",
                    "label": 0
                },
                {
                    "sent": "And as we know, Gaussian processes are stochastic, process is just defined in terms of their means and covariance function.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We now need to get a little bit deal a little bit more formally with the gastic differential equation.",
                    "label": 0
                },
                {
                    "sent": "Y wrote early on was that we had this white noise process.",
                    "label": 0
                },
                {
                    "sent": "Is it said of tea and I guess I think the usual name for this is a large of an equation where we have this kind of white noise driving term.",
                    "label": 0
                },
                {
                    "sent": "Mathematically though, this is actually a bit.",
                    "label": 1
                },
                {
                    "sent": "Pleasant becausw we want to think of that of T as being derivative ovina process, but we know that in fact the Davina process is not differentiable.",
                    "label": 1
                },
                {
                    "sent": "So mathematically this kind of thing is actually maybe not the best way to think about things and better way to write this kind of stochastic differential equations in this form.",
                    "label": 0
                },
                {
                    "sent": "So we basically think of the change.",
                    "label": 0
                },
                {
                    "sent": "The increment here is being given by this term like this just and also that we have this DWF T over on the right hand side and we miss.",
                    "label": 0
                },
                {
                    "sent": "We already explained that DW is actually just the increment, so W + T plus Delta T -- W of tea.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the standard way in which stochastic differential equations are written.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, I've got this classic differential equation.",
                    "label": 0
                },
                {
                    "sent": "One thing that's very simple that I could do is actually to simulate from it.",
                    "label": 0
                },
                {
                    "sent": "And OK, what we could do to do that is to write down a bunch of time points.",
                    "label": 0
                },
                {
                    "sent": "Zero, T1 etc and also define these intervals.",
                    "label": 0
                },
                {
                    "sent": "Delta T is just T I + 1 -- T I and then what are we going to do with this consentually going to take that differential equation we had and this sort of DX is just sort of X y + 1 -- X or Y.",
                    "label": 0
                },
                {
                    "sent": "We've got this this time interval, so we just do the deterministic dynamics, which we just integrate them forward through time Delta T, and then also we're going to add some some white noise and the key thing here is that if we have a time interval length Delta T, we know that we can simulate from the vena process by drawing a standard Gaussian variables that I will then scaling it by the square root of the time interval, right so that.",
                    "label": 0
                },
                {
                    "sent": "Basically, so that that means that the variance will have.",
                    "label": 0
                },
                {
                    "sent": "Will be Delta T so this is how we can just forward simulate from the static differential equation and OK. And of course what we want to do is to take these these intervals very small and in the in the limit as Delta T goes to zero will actually have a true generative process for the stochastic differential equation.",
                    "label": 0
                },
                {
                    "sent": "So it's worth saying that there are in fact this is just like Euler integration of standard ODS.",
                    "label": 0
                },
                {
                    "sent": "This is the simplest thing you can do and it's certainly true that there are.",
                    "label": 1
                },
                {
                    "sent": "More complex schemes.",
                    "label": 0
                },
                {
                    "sent": "Higher order schemes that you can use for this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "But I think that it's probably worth thinking about this.",
                    "label": 0
                },
                {
                    "sent": "This is perhaps a nice way to just.",
                    "label": 0
                },
                {
                    "sent": "To think about how stochastic differential equations work.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also need to think a little bit and define what stochastic integration means.",
                    "label": 1
                },
                {
                    "sent": "So if we have a normal, say Riemann integral, we just basically divide up the interval and define some.",
                    "label": 0
                },
                {
                    "sent": "Some some in the limit that then goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Basically for the stochastic integral, East under the ITO interpretation will have something similar, where now this DWT we said it was just.",
                    "label": 0
                },
                {
                    "sent": "The vena process at T plus at the T plus Delta T minus WFT.",
                    "label": 0
                },
                {
                    "sent": "Basically, when we do, we have this formula.",
                    "label": 1
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Let me just take the limit as N goes to Infinity will also get.",
                    "label": 1
                },
                {
                    "sent": "This is the definition of ITO stochastic integral.",
                    "label": 0
                },
                {
                    "sent": "It's worth saying, in this kind of useful mnemonic need to think about this, which is that we've already seen that the.",
                    "label": 0
                },
                {
                    "sent": "Does this square root of T scaling for the stochastic part when we are simulating the stochastic differential equation and so we can think of mnemonically, at least that.",
                    "label": 0
                },
                {
                    "sent": "EW of T squared is of order.",
                    "label": 0
                },
                {
                    "sent": "Delta T OK, so this is important, at least when sometimes when doing some derivations and making sure we've got all the right terms there.",
                    "label": 0
                },
                {
                    "sent": "Because this scaling means, for example, if you think you might need to expand to say first order you make, you may actually need to expand the 2nd order to take into account the fact that this DW squared is actually order Delta T.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We've looked at the and the stochastic differential equation.",
                    "label": 0
                },
                {
                    "sent": "I actually put up early on is just one.",
                    "label": 0
                },
                {
                    "sent": "It's a linear stochastic differential equation, general form of a diffusion process is actually like this, so we have some function here, a of X&T and also some matrix B of X&T here.",
                    "label": 1
                },
                {
                    "sent": "So the AI is the drift vector and B is the diffusion matrix.",
                    "label": 1
                },
                {
                    "sent": "OK, so we have a general form of this.",
                    "label": 1
                },
                {
                    "sent": "The solution by integrating this forward.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that for diffusion process, is the sample parts are actually continuous.",
                    "label": 0
                },
                {
                    "sent": "It also is password saying that.",
                    "label": 0
                },
                {
                    "sent": "If we, if we thinking about how we actually can simulate from this stochastic differential equation, it's clear that these functions A&B must be non anticipating.",
                    "label": 0
                },
                {
                    "sent": "That means that basically they should only look back in time and not look forward in time right?",
                    "label": 0
                },
                {
                    "sent": "If we if we required values of the process from which we haven't yet created, that would be a problem.",
                    "label": 0
                },
                {
                    "sent": "So AMB and an anticipated.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's just give some simple examples.",
                    "label": 1
                },
                {
                    "sent": "We can have the visa process basically just where the X is, just EW, and then the only thing that's happening here is we get an offset relative to the standard vendor process which starts at 0.",
                    "label": 0
                },
                {
                    "sent": "We can also have just re scaling it and having your drift term.",
                    "label": 0
                },
                {
                    "sent": "So we in this case will get this kind of thing and also perhaps a little bit more interesting.",
                    "label": 0
                },
                {
                    "sent": "We can have the on stoneback process where we have some tendency to revert towards the mean.",
                    "label": 0
                },
                {
                    "sent": "The meaning zero.",
                    "label": 0
                },
                {
                    "sent": "OK, So what this term is doing is saying OK we're being pulled back towards the origin and then we're adding noise and so in that case we can also get this kind of solution.",
                    "label": 0
                },
                {
                    "sent": "So these are some standard, very simple examples.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One other thing that we need to define full stochastic differential equations is this notion of infinitesimal moment.",
                    "label": 0
                },
                {
                    "sent": "So we can look at.",
                    "label": 0
                },
                {
                    "sent": "Basically, the change between XT&X of T plus Delta T and we just see that for example, from our simulation form.",
                    "label": 0
                },
                {
                    "sent": "It's at this kind.",
                    "label": 0
                },
                {
                    "sent": "We can look at the first moment.",
                    "label": 0
                },
                {
                    "sent": "We just look at the how the change of Delta X the expected value of that scales with Delta T and that in fact defines the drift term.",
                    "label": 0
                },
                {
                    "sent": "And also if we compute this guy here, the covariance of Delta X and scale that again by Delta T, then we actually see that this gives this BB transpose.",
                    "label": 0
                },
                {
                    "sent": "So this is a nice example where you see with the square root stuff coming in OK.",
                    "label": 0
                },
                {
                    "sent": "If you look at if you take this right hand side here expand this up.",
                    "label": 0
                },
                {
                    "sent": "And then look at what's going on and take the expectations.",
                    "label": 0
                },
                {
                    "sent": "Basically, you're going to get something of order Delta T ^2, you know a A transpose Delta T ^2, but also this.",
                    "label": 0
                },
                {
                    "sent": "Terminal the Delta T that comes from the the white noise process.",
                    "label": 0
                },
                {
                    "sent": "The visa process so there you can see how the.",
                    "label": 0
                },
                {
                    "sent": "Scaling of the white noise here is actually important to get this to get this diffusion term.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "That's talking about stochastic differential equations.",
                    "label": 0
                },
                {
                    "sent": "What I want to do now is to talk about how to connect that to Gaussian processes, and I'm going to do this for the stationary case.",
                    "label": 0
                },
                {
                    "sent": "We're going to have some if we have time invariant coefficients of the stochastic differential equation.",
                    "label": 0
                },
                {
                    "sent": "I'm going to basically try to work out what the covariance function is that correspond.",
                    "label": 0
                },
                {
                    "sent": "Excuse me, that corresponds to this this process.",
                    "label": 0
                },
                {
                    "sent": "And I should say OK, will do it first for the, for the.",
                    "label": 0
                },
                {
                    "sent": "The univariate case and we could also generalize that bit later.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The easy way to actually do this analysis is through free analysis.",
                    "label": 0
                },
                {
                    "sent": "We've got a linear time invariant system.",
                    "label": 0
                },
                {
                    "sent": "That means that the eigenbasis for that is the Fourier basis.",
                    "label": 0
                },
                {
                    "sent": "That means it's going to be easy to work in that basis, so we'll just take the take the free transform of some process XT here, and get some X~ of S, and we get the usual Fourier inversion formula and the key thing is that when we differentiate this process, we bring this down 2\u03c0 S to the K. Term in the in the Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "That means that looking at the basic stochastic differential equation that we have here, we can.",
                    "label": 0
                },
                {
                    "sent": "We can just create transform both sides to get this guy.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we can then just take this expectation in Fourier space to get the power spectrum.",
                    "label": 0
                },
                {
                    "sent": "And of course, what we want to do is that because we have the V neck Eugene Theorem, we can relate the power spectrum to the covariance function.",
                    "label": 0
                },
                {
                    "sent": "So the easy thing to get out of this to this linear time invariant stochastic differential equation is the power spectrum, and we can make that relates directly to the.",
                    "label": 0
                },
                {
                    "sent": "To the coefficients in the stochastic differential equation.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me just explain a little bit.",
                    "label": 0
                },
                {
                    "sent": "This is a polynomial A of zed.",
                    "label": 0
                },
                {
                    "sent": "Is this polynomial, akas, enter the K. This is just the so we take a of 2\u03c0 S and we take its complex conjugate and that's this denominator here.",
                    "label": 0
                },
                {
                    "sent": "So basically from those.",
                    "label": 0
                },
                {
                    "sent": "From the stochastic differential equation, we can get the form of the power spectrum and then obviously if we can do the Fourier inversion, we can then work out what the corresponding covariance function is Guido.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so you might.",
                    "label": 0
                },
                {
                    "sent": "If you're hardcore mathematician, you would certainly want to worry about these.",
                    "label": 0
                },
                {
                    "sent": "Whether these integrals exist.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is.",
                    "label": 0
                },
                {
                    "sent": "There is a bunch of theory that shows that, at least in the right conditions they do.",
                    "label": 0
                },
                {
                    "sent": "I mean, I agree you have to worry about that, but that will never actually does work.",
                    "label": 0
                },
                {
                    "sent": "Define them as limits 1st and then show that among expectations they have their own.",
                    "label": 0
                },
                {
                    "sent": "Never really work in practice.",
                    "label": 0
                },
                {
                    "sent": "It's not going to.",
                    "label": 0
                },
                {
                    "sent": "And X is not decay, right?",
                    "label": 0
                },
                {
                    "sent": "Sure, so.",
                    "label": 0
                },
                {
                    "sent": "So you could probably introduce limits, yes, sure.",
                    "label": 0
                },
                {
                    "sent": "And then you can use them still, these things.",
                    "label": 0
                },
                {
                    "sent": "Under certain expectations I guess, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's clear that what we need to do right is that we need we actually need this expectation right here, right?",
                    "label": 0
                },
                {
                    "sent": "So yes.",
                    "label": 0
                },
                {
                    "sent": "I don't have a. I don't have a great answer for this, OK, but I guess that you're right that you can.",
                    "label": 0
                },
                {
                    "sent": "I know that show that you can be made to work under the usual kinds of tricks that you might need that you're describing.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at.",
                    "label": 0
                },
                {
                    "sent": "Let's look at actually just some simple examples corresponding to that.",
                    "label": 0
                },
                {
                    "sent": "So if we just take a first order stochastic differential equation, are all friendly on stone Becca process, then we see past respect from goes down as S squared for large for large S, and that corresponds to this just this.",
                    "label": 0
                },
                {
                    "sent": "This covariance function here, and of course.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "What we know is if we simulate from this, say, here's a sample at some discretization, then we see this very rough process.",
                    "label": 0
                },
                {
                    "sent": "OK, it turns out that this process will be.",
                    "label": 0
                },
                {
                    "sent": "It will be continuous, but nowhere well probability one nowhere differentiable, so we'll see this very rough process similar to the vino process.",
                    "label": 0
                },
                {
                    "sent": "By contrast, if we have a second order differential equation, so here, just think of this as a damped simple harmonic oscillator.",
                    "label": 0
                },
                {
                    "sent": "OK, we've got this X TT, but say not X of T. That would just be.",
                    "label": 0
                },
                {
                    "sent": "It's basic simple harmonic oscillator being driven by white noise.",
                    "label": 0
                },
                {
                    "sent": "If we did that then in fact what we can see from over here is we'll actually get a blow up in the power spectrum.",
                    "label": 0
                },
                {
                    "sent": "If this term doesn't exist, but anyway.",
                    "label": 0
                },
                {
                    "sent": "At the resonant frequency.",
                    "label": 0
                },
                {
                    "sent": "But if we add in some damping we can get some covariance function that corresponds to that and it said this form and if we sample from that we'll end up with this.",
                    "label": 0
                },
                {
                    "sent": "This process here.",
                    "label": 0
                },
                {
                    "sent": "Well, here's a sample from the process which has as you would expect, it has a periodicity at least sort of an approximate periodicity here, which comes from the resonant frequency of the simple harmonic oscillator.",
                    "label": 0
                },
                {
                    "sent": "You can see this is a peak in the path spectrum OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I've been talking about.",
                    "label": 0
                },
                {
                    "sent": "The univariate case just want to say something about the multivariate case.",
                    "label": 0
                },
                {
                    "sent": "We can have a vector orange to inject process here and here I'm coming back to this linear system.",
                    "label": 0
                },
                {
                    "sent": "Basically we can.",
                    "label": 0
                },
                {
                    "sent": "It turns out we can actually do the integration to get the solution.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Obviously, one thing we can do looking for stationary solution, we have some initial condition that this is decaying away.",
                    "label": 1
                },
                {
                    "sent": "So basically, if we sort of move X0 back to sort of minus Infinity then this is the term that will dominate.",
                    "label": 0
                },
                {
                    "sent": "This determines is relevant and then we can workout the covariance structure at anytime, while times T&S in this way, which comes out for these simply like this again, we see exactly this min of TNS, but this is because we've got these two when we've got X of T in excess.",
                    "label": 0
                },
                {
                    "sent": "Whichever TUS larger.",
                    "label": 0
                },
                {
                    "sent": "Basically we've got this some whole bunch of Gaussian variables which are the same you know, up to the smaller time and then the rest of the stuff here, which actually is irrelevant because the uncorrelated with the previous times.",
                    "label": 0
                },
                {
                    "sent": "So we'll end up with this kind of structure here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there are various ways of actually say computing this covariance.",
                    "label": 0
                },
                {
                    "sent": "That's at time zero corresponding some matrix equation and then you can actually get the covariance that time offsets just by this exponential factor here.",
                    "label": 0
                },
                {
                    "sent": "So basically certainly for this is for the.",
                    "label": 0
                },
                {
                    "sent": "Is it for the time invariant linear system we can actually get solutions for the covariance function from the underlying matrix stochastic differential equations?",
                    "label": 0
                },
                {
                    "sent": "There's a clearly there's a lot more theory here that that one can do.",
                    "label": 0
                },
                {
                    "sent": "We can please use the spectral techniques that we use for the univariate case as well, and there's more stuff in Gardner's book, for example on that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the things that we actually saw when we looked at those samples from the free from the Internet process and also from the damp stochastic sorry, simple harmonic oscillator is that we actually saw very different smoothness properties of that first order and 2nd order SD and where this is coming from is basically that.",
                    "label": 0
                },
                {
                    "sent": "Casting differential equations of order PA P -- 1 times mean squared differentiable.",
                    "label": 1
                },
                {
                    "sent": "The processes correspond to that had this differentiability.",
                    "label": 0
                },
                {
                    "sent": "So if P is one, we get zero differentiability.",
                    "label": 0
                },
                {
                    "sent": "We get content continuity from the diffusion process but not differentiability.",
                    "label": 0
                },
                {
                    "sent": "And if P is 2 we get one, we get one times differentiability and basically an easy intuitive way to think about this is to say that cause a process gets rougher the more times it gets differentiated.",
                    "label": 1
                },
                {
                    "sent": "Then basically the roughest thing here X of P is of order white noise which is non differentiable.",
                    "label": 0
                },
                {
                    "sent": "So basically as we lift these things up as we integrate it various various times, we'll see we'll see differentiability appearing so we can sort of intuitively at least understand they mean squared differentiability of the resulting process through this kind of argument.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is this is an interesting point and I guess it's quite related to things that we want them.",
                    "label": 0
                },
                {
                    "sent": "I want to talk about today Becausw it's question about how we actually do discretization of continuous time Gaussian Markov process.",
                    "label": 0
                },
                {
                    "sent": "Say we have a.",
                    "label": 0
                },
                {
                    "sent": "An order P stochastic differential equation.",
                    "label": 0
                },
                {
                    "sent": "Then the question is really how will that relate?",
                    "label": 0
                },
                {
                    "sent": "Will if we discretize that and some some time interval, say some grid H?",
                    "label": 0
                },
                {
                    "sent": "Is it the case that the discretised?",
                    "label": 0
                },
                {
                    "sent": "Process will have the same Markov properties as the underlying process, and the answer is you need to be careful.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's define certainly for discrete time we can define an ARMA process.",
                    "label": 1
                },
                {
                    "sent": "So not only the missing AI in here sorry, so we have this user.",
                    "label": 0
                },
                {
                    "sent": "The linear terms that we had before.",
                    "label": 0
                },
                {
                    "sent": "Auto regressive terms and here rather than just having B 0 so just driving it with noise at time at the current time will drive it with the lag history of these noises.",
                    "label": 0
                },
                {
                    "sent": "This is the Arma process and we can also have a continuous time analog of this where it turns out that the power spectrum now has this rational form.",
                    "label": 0
                },
                {
                    "sent": "Pizza in fact, get the order of Q here to be less than the order of P. In order for this to be well defined anyway, the key.",
                    "label": 0
                },
                {
                    "sent": "Key theorem here.",
                    "label": 0
                },
                {
                    "sent": "Is that if we take a continuous time station Gaussian process and let X be a discretization of that process if.",
                    "label": 0
                },
                {
                    "sent": "X is an armor processor.",
                    "label": 1
                },
                {
                    "sent": "Has this kind of power spectral density, and XH will be a discrete time armor process.",
                    "label": 0
                },
                {
                    "sent": "However, if X is an AR process, and so in other words, it's a stochastic differential equation of order P of the kind we've been talking about, then actually when we discretize, it is not necessarily an AR process.",
                    "label": 0
                },
                {
                    "sent": "It actually turns out to be an armor process, so one needs to be a little bit careful about the going from continuous time to discrete time because of this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I also want to say something about inference.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "One thing that we certainly want to do a lot in Gaussian process is we want to have some.",
                    "label": 0
                },
                {
                    "sent": "We've got this underlying stochastic differential equation that's giving rise to a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Prior over functions.",
                    "label": 0
                },
                {
                    "sent": "We have some observations of that.",
                    "label": 0
                },
                {
                    "sent": "That process at various times and you might want to do inference about predicting the values at other times.",
                    "label": 0
                },
                {
                    "sent": "If we're dealing with something like an Ornstein Uhlenbeck process, then in fact we've already seen we have this Markov property, which means that we only need to condition.",
                    "label": 0
                },
                {
                    "sent": "Essentially, if we're making a prediction at some time T star, then we've got a whole bunch of observations at various times, but we actually need to only worry about those observations in the immediate past and the immediate future of their observed at time T star because of the Markov property.",
                    "label": 1
                },
                {
                    "sent": "Everything else is is.",
                    "label": 0
                },
                {
                    "sent": "Independent is conditionally independent of the rest of the variables given the past and the future variables, so there's a slight caveat here.",
                    "label": 0
                },
                {
                    "sent": "OK, which is that if we have pure observations of the process this is true, but if we have observations which are corrupted by noise, then in fact then this Markov property breaks and we actually need.",
                    "label": 0
                },
                {
                    "sent": "If we do get in process, prediction will need to actually make use of all the observations at various times.",
                    "label": 0
                },
                {
                    "sent": "So basically this stuff how to do this prediction is just.",
                    "label": 0
                },
                {
                    "sent": "An example of Gaussian pro.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This prediction, so we're interested in this conditional distribution of X of T star.",
                    "label": 0
                },
                {
                    "sent": "Given these observations under the Gaussian process, we know it's going to be.",
                    "label": 0
                },
                {
                    "sent": "It's going to be a Gaussian prediction with some mean and some variants and the usual Gaussian process prediction equations apply.",
                    "label": 0
                },
                {
                    "sent": "There kind of slightly simplified cause.",
                    "label": 0
                },
                {
                    "sent": "In fact, we only need to worry about these two observations just in the past in the future, so we can just define this two by two covariance matrix where just for example, K star P is.",
                    "label": 0
                },
                {
                    "sent": "CFT stone TP.",
                    "label": 0
                },
                {
                    "sent": "OK so this is just K of TP and TP and so on and just do this very simple matrix inversion to get these predictions so at least under the if we can really exploit the Markov nature of the autonomic process, for example 2.",
                    "label": 0
                },
                {
                    "sent": "To make these to make these predictions in very compact way?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I've talked about the stochastic differential equation view I've talked about how to workout covariance functions of these processes and try to understand those covariance functions.",
                    "label": 0
                },
                {
                    "sent": "I also want to talk a bit about Fokker Planck equations.",
                    "label": 0
                },
                {
                    "sent": "So basically one thing that we could be interested in is saying OK, I'm at value X zero at time zero, and what I want to do is to understand how much that evolution of the where I'm going to be at.",
                    "label": 0
                },
                {
                    "sent": "So P of X at time T. Given my if I'm at some.",
                    "label": 0
                },
                {
                    "sent": "Values X zero time 0 so basically we're interested in this transition PDF which defines this and basically one can show that.",
                    "label": 0
                },
                {
                    "sent": "The equations that govern the evolution of the transition probability is is a PDE second well, OK, so just DT by the P on the on the left here and we get terms from the drift and also from the diffusion.",
                    "label": 0
                },
                {
                    "sent": "So we get second order derivatives here with respect to the diffusion terms and 1st order derivatives with respect to the drift.",
                    "label": 1
                },
                {
                    "sent": "So basically what we want to do, we might want to understand.",
                    "label": 0
                },
                {
                    "sent": "Even this evolution of this transition probability in terms of the Fokker Planck equation and so basically how to derive this is just just sort of continuous time version of Chapman Kolmogorov equation.",
                    "label": 1
                },
                {
                    "sent": "Basically we have some probability.",
                    "label": 1
                },
                {
                    "sent": "At time X0T0 what time T0 and we considering all possible paths to get to how we can be at some at X at time T?",
                    "label": 0
                },
                {
                    "sent": "And so this is actually the forward Fokker Planck equation.",
                    "label": 1
                },
                {
                    "sent": "And there's also a backward Fokker Planck equation as well.",
                    "label": 0
                },
                {
                    "sent": "Sorry, yes.",
                    "label": 0
                },
                {
                    "sent": "OK, that sounds bad.",
                    "label": 0
                },
                {
                    "sent": "OK, I guess this square bracket is wrong, OK?",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "That remains a density after losing 1% or.",
                    "label": 0
                },
                {
                    "sent": "I think because you're driving it from the Chapman, golf equation table, it's actually it's guaranteed to do that, right?",
                    "label": 0
                },
                {
                    "sent": "You are building in that condition.",
                    "label": 0
                },
                {
                    "sent": "That's how you're actually driving this thing, right is actually.",
                    "label": 0
                },
                {
                    "sent": "So let me give you.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A simple example, very simple example of a Fokker Planck equation.",
                    "label": 0
                },
                {
                    "sent": "If we just have the Vilna process with scaling and drift, then we'll have.",
                    "label": 1
                },
                {
                    "sent": "Then we said this was the stochastic form of the solution.",
                    "label": 0
                },
                {
                    "sent": "It's then very easy to see that the mean.",
                    "label": 0
                },
                {
                    "sent": "The mean value of this is at time T is just X 0 + A T and that there will be some variance in fact which is going as Sigma squared T. OK, so this is this is just the cost of Gaussian distribution at time T with mean X 0 + 80 and this variance Sigma squared T. It's very simple example.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think that perhaps the main point where Fokker Planck equations have particular value is in relation to when we have certain boundary conditions for this PZ.",
                    "label": 0
                },
                {
                    "sent": "For example, one problem which relates to neural computation is that if we start off with.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "Time 0 here.",
                    "label": 0
                },
                {
                    "sent": "Maybe an we can think of this as like the resting potential of a neuron after firing.",
                    "label": 0
                },
                {
                    "sent": "We drive it with some noisy input signal on the potential side.",
                    "label": 0
                },
                {
                    "sent": "Response to that.",
                    "label": 0
                },
                {
                    "sent": "Maybe if it follows the vena process then we might be interested in when that potential crosses the threshold for firing of that neuron and would be interested in this passage time here about when this actually happens, OK?",
                    "label": 0
                },
                {
                    "sent": "So in these kinds of conditions, when we're looking at.",
                    "label": 0
                },
                {
                    "sent": "One thing we can do, of course, then, is that we could be interested in the first passage time density of essentially what's the probability distribution as a function of time when this when this process on samples in this process first up cross the.",
                    "label": 0
                },
                {
                    "sent": "Alright, this barrier here, but we can also be interested in maybe if we were just interested in the.",
                    "label": 0
                },
                {
                    "sent": "Probability distribution at of, you know at some particular time here of what the what the value of X is at some time and which essentially what we have to do is to kind of siphon off all the probability that's already crossed.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "All the paths that have already crossed we normalize and look at that distribution.",
                    "label": 0
                },
                {
                    "sent": "Of of the process at some time.",
                    "label": 0
                },
                {
                    "sent": "Then we can obtain this through Fokker Planck kinds of analysis.",
                    "label": 0
                },
                {
                    "sent": "So basically the Fokker Planck stuff depends crucially on the Markov properties of these processes, right?",
                    "label": 0
                },
                {
                    "sent": "It's the mark of structure of the Chapman Kolmogorov equations that actually allow us to do this analysis.",
                    "label": 0
                },
                {
                    "sent": "And so their various.",
                    "label": 0
                },
                {
                    "sent": "Erica, you see that this is not radically new theory.",
                    "label": 0
                },
                {
                    "sent": "This goes back to fellow in 1952.",
                    "label": 0
                },
                {
                    "sent": "One can characterize various.",
                    "label": 0
                },
                {
                    "sent": "Various kinds of boundary conditions we can be interested in.",
                    "label": 0
                },
                {
                    "sent": "We could have absorbing barriers we could have, for example, reflecting barriers.",
                    "label": 0
                },
                {
                    "sent": "Maybe we could trap this particle in between two barriers here and be interested in the evolution of the probability transition probability as a function of time and so on.",
                    "label": 0
                },
                {
                    "sent": "Alright, some other conditions here as well.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the things that was mentioned in the introduction of course, is that we.",
                    "label": 0
                },
                {
                    "sent": "I described a lot about how we can actually.",
                    "label": 0
                },
                {
                    "sent": "Simulate from this process and understand properties of it.",
                    "label": 0
                },
                {
                    "sent": "One thing that certainly as machine learners might well want to do is to actually to maybe do some parameter estimation and at least in this simple case that's actually fairly straightforward, 'cause if we have some observations at various times then the key property that actually makes Gaussian process is to work nice to work with in respect to, say parameter estimation is that we can actually just write down likelihood of these observations.",
                    "label": 1
                },
                {
                    "sent": "Given the parameters and, this is just.",
                    "label": 0
                },
                {
                    "sent": "This is just a sort of multivariate Gaussian right?",
                    "label": 0
                },
                {
                    "sent": "It's just basically saying I've got some covariance matrix corresponding to these.",
                    "label": 0
                },
                {
                    "sent": "These these end different points X one up to XTI can look at that covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "I've got some mean I've got some covariance and well, I've really got is just one data point and end dimensional data point in that.",
                    "label": 0
                },
                {
                    "sent": "For Gaussian, well, I'm trying to do is to change the parameters of that Gaussian so as to maximize the likelihood, so that's at least in this simple case that's actually at least conceptually fairly straightforward, and one can, for example, at least, if one can compute the covariance matrix, then you can do this.",
                    "label": 0
                },
                {
                    "sent": "OK. And so one can obtain the gradients of this likelihood very very simply just for matrix algebra and then use a numerical methods to optimize the parameters.",
                    "label": 1
                },
                {
                    "sent": "So this is exactly how we do, say model fitting for Gaussian processes and the machine learning case is like this.",
                    "label": 0
                },
                {
                    "sent": "One can also do theory about when we actually have continuous observations, but I guess that I'm sure, although this makes sense mathematically, I'm not sure it makes so much sense in practice for things that we really want to do, and if it looks upset, OK?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so maybe just to summarize I guess.",
                    "label": 0
                },
                {
                    "sent": "When I tried to do is to give some kind of introduction to SDS talking about.",
                    "label": 0
                },
                {
                    "sent": "Being and I think the nice place to start is actually the simulation framework and think how to drive these forward in time.",
                    "label": 0
                },
                {
                    "sent": "We have this key property of these processes are both being Gaussian and being Markov, which gives a very special structure and allows us to do certain things.",
                    "label": 0
                },
                {
                    "sent": "There is quite a lot of formal machinery for stochastic integration and so on.",
                    "label": 1
                },
                {
                    "sent": "And we talked about the meaning covariance, functions of Gaussians and so I had to do that.",
                    "label": 0
                },
                {
                    "sent": "How to see the market look at the market properties and get full Planck equations.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "Simple introduction, of course there's a couple of things that I haven't talked about.",
                    "label": 0
                },
                {
                    "sent": "The important one is of course we want to talk about observation.",
                    "label": 0
                },
                {
                    "sent": "Or is we actually want to do things like continuous time Kalman filtering.",
                    "label": 0
                },
                {
                    "sent": "And again, this is not something that's hot off the press.",
                    "label": 0
                },
                {
                    "sent": "And also of course really what the challenges of the workshop are, which has to go beyond these these these linear models to talk about nonlinear dynamics and also nonlinear operations.",
                    "label": 1
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "So Matthias.",
                    "label": 0
                },
                {
                    "sent": "Wanted to say OK, just gotta process within the marriage because I think what it feels like it went to complete this time is because it's not.",
                    "label": 0
                },
                {
                    "sent": "I want.",
                    "label": 0
                },
                {
                    "sent": "Special.",
                    "label": 0
                },
                {
                    "sent": "Send an AR.",
                    "label": 0
                },
                {
                    "sent": "Processing type of where the matrix has a very constructive so?",
                    "label": 0
                },
                {
                    "sent": "I mean are there?",
                    "label": 0
                },
                {
                    "sent": "So I certainly agree that.",
                    "label": 0
                },
                {
                    "sent": "If we're talking about this parameter estimation case that.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Yes, you don't need to approach it.",
                    "label": 0
                },
                {
                    "sent": "You can exploit the structure in the problem so you don't actually have to work with that.",
                    "label": 0
                },
                {
                    "sent": "That necessary that big matrix where you can see how basically how that will you can exploit the structure that comes from the SD, so you don't actually have to scale.",
                    "label": 0
                },
                {
                    "sent": "Saying cube like that, it's definitely true.",
                    "label": 0
                },
                {
                    "sent": "I guess I think I actually have to think about that.",
                    "label": 0
                },
                {
                    "sent": "I'm not exactly sure how that will workout, but pretty confident to locate fairly easily.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "I have a question about the market property of the process when you have noisy observations.",
                    "label": 0
                },
                {
                    "sent": "I understand what you say about that.",
                    "label": 0
                },
                {
                    "sent": "You lose some of this market market property, but I think if you view the observed the process conditioned on noisy observations as a as a process as a process, then you can show that this is again a Markovian.",
                    "label": 0
                },
                {
                    "sent": "Do you OK so?",
                    "label": 0
                },
                {
                    "sent": "Maybe I don't understand.",
                    "label": 0
                },
                {
                    "sent": "I mean certainly for the.",
                    "label": 0
                },
                {
                    "sent": "For example, you know in the hidden Markov model the process on the observations is no longer Markovian, right?",
                    "label": 0
                },
                {
                    "sent": "If you just try to integrate process.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Conditioned on the observations.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "About that but.",
                    "label": 0
                },
                {
                    "sent": "Yes indeed.",
                    "label": 0
                },
                {
                    "sent": "Join sure.",
                    "label": 0
                },
                {
                    "sent": "Yes, so OK so that yes that's true, Thanks yeah.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}