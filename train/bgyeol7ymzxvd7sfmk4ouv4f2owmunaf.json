{
    "id": "bgyeol7ymzxvd7sfmk4ouv4f2owmunaf",
    "title": "Information Flow Between News Articles: Slovene Media Case Study",
    "info": {
        "author": [
            "Jan Cho\u0142oniewski, Faculty of Physics, Warsaw University of Technology"
        ],
        "published": "Nov. 15, 2016",
        "recorded": "October 2016",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/sikdd2016_choloniewski_slovene_media/",
    "segmentation": [
        [
            "So well, hello everyone.",
            "Once again my name is Antonio Ski and I will present study on information so I will call this way later.",
            "I will tell you why between news articles in Slovene media this is work I did.",
            "During my statement in Louisiana with Gregor Leben but also with Sebastian Magic, another shot record from sovereign status governor against here.",
            "Eat."
        ],
        [
            "Well, let me start with present presentation plan.",
            "So we'll start.",
            "I will start with this thing and later I will tell you about my aims of the study.",
            "Later I will show the big picture of it few words about data, also about human experimentation.",
            "Annotation experiment, which I guess is most interesting part later I will tell you about results.",
            "I got training classifiers on the on the data so few words about further work and I will finish with."
        ],
        [
            "Summary so.",
            "The main goal of my cooperation with SCA was to.",
            "To make real striking study I don't know.",
            "Maybe maybe in the future.",
            "Maybe in the future it will get someone better.",
            "We have some.",
            "Initial initial initial results but.",
            "I also wanted to use that that cooperation to construct those classifiers and estimate the accuracy, and I will try to convince you that it also is also possible to use the method to.",
            "Detect information flow between newsarticles.",
            "Maybe not the very very present form, but well, it's some distant goal in the in the future.",
            "What is reduce tracking?",
            "OK, it's kind of.",
            "Confusing because there are a few terms about the very same.",
            "Think maybe the next slide."
        ],
        [
            "Would make you will make it a little bit more clear because release tracking is actually very very similar thing as plug it in detection.",
            "Why is that?",
            "Because when you have news provider such as solving status code noggins here then people media from the country and those outside the country I are using your really releases so.",
            "Part of those users are are allowed because these other media are the subscribers, but in some cases it's just plagiarism.",
            "So actually it's very similar thing on this figure I got from Wikipedia.",
            "There is a comparison of.",
            "It's a diagram which shows how different methods can can allow us to detect different time of plagiarism.",
            "Or maybe this in this case also reveals tracking.",
            "Basically my methods were bhagavan grams, so it's actually something between substring matching and bag of words, analyzes.",
            "As you can see on the diagram, those two approaches allow us to detect, copy and paste plagiarism or only part of cases of these guys plagiarism, but actually now now really no possibility to detect any other types if text was processed in.",
            "Had major major ever writes then.",
            "Well actually it's very hard to detect this with these methods.",
            "OK."
        ],
        [
            "OK, uh.",
            "So maybe that means that now.",
            "Tracking how particular piece of content?",
            "Disseminates maybe changes the form, right?",
            "Yeah, actually, that was the idea.",
            "Legal or sometimes illegal, so this is another feature really which.",
            "Message the lawyers, but otherwise, but I don't care about the fact that there are.",
            "Other news agencies writing about the same topic.",
            "They care about their other news agencies or individuals that are copying their work.",
            "In particular, I think big ears, yeah.",
            "Surely what I was trying to detect is that if two articles are not really similar because that's something rather rather cruel, rather easy to detect.",
            "But if they are based on the one on each other, or if both are based on some third article, but I will, I will come to that.",
            "OK so on this slide I would like to present your study outline.",
            "Maybe they just have the picture in that mind so well.",
            "Basically we have to start with gathering data which were solid news outlets in.",
            "They were tracked by Event Registry.",
            "After the after the gathering data, I chose one day of Slovenian newsarticles and for each article in article.",
            "I was trying to find the most similar article in STL database in like 7 days in that day in seven days before.",
            "Using those candy that pairs to trade, I gave those candidate pairs to human human annotators because it would be the best to have people marking each pair, of course, but it would take way too much time.",
            "So in this way we are limiting cases just to few hundreds, which makes annotation study possible.",
            "After that where I am trying.",
            "Well, I'm I'm creating finalists.",
            "Of for a given day, then train some other classifiers and see how good they are.",
            "Well, in the end I will come to improvements that could be."
        ],
        [
            "Possibly made well, very crude slide about data be cause you will see so at the moment of where the study was conducted there were there were about 60 Slovenian media outlets in Event registry, most of them being being well.",
            "Mainstream media, actually, I guess because there were a few blogs there.",
            "There were like smaller, more specific sites, but most of them being.",
            "Yeah, could be considered as somewhere more like on the mainstream, and half would be more like on the block site.",
            "So I was this is kind of getting intermixed at some points.",
            "Yeah, kind of like.",
            "And at that time in June, that would be right definitely for that.",
            "For that time I was checking yesterday and there are about 45 media outlets now.",
            "Eventually history Slovenian and I took a look at the list.",
            "And, well, most of them being through.",
            "Proper media outlets?",
            "OK, so from that day I chose at random 469 articles out of about 900.",
            "Publishing that today.",
            "Well, the reason why is why the numbers so low is that I had.",
            "We had limited workforce and it was hard to make so many annotations."
        ],
        [
            "But as I mentioned, first thing was to gather candidate source releases for each news item published published on the day.",
            "For the sake of further human annotation study.",
            "So how, how, how exactly I chose, I found the candidate source releases.",
            "So for a given day, I know that first thing STD releases published in.",
            "In the day and week before.",
            "I loved it.",
            "Also, Slovene articles from 2 preceding quicks to try, and TF IDF corpus.",
            "And they also did well.",
            "I start equals.",
            "And they need security here are.",
            "Here is specification of TF IDF procedure.",
            "I tried so basically for candidates or services I have used 2, three and four grams at the same time with Atlas smooth things to take care of.",
            "Rare, really rare phrases and they discarded terms which are present in more than 25% of training corpus.",
            "Uh.",
            "So for a given data to find candidates or services, I gathered vocab vocabulary for all these sets.",
            "So basically this.",
            "Types of engrams later trying to TF IDF on.",
            "This set on training corpus and later calculated cosine similarity between each Arctic analyst article and.",
            "This series is which could be source of a given article.",
            "So for each article from this C set I saved the most similar article from.",
            "SD database.",
            "And scored to take through.",
            "Keep track of that and this were counted that purse so."
        ],
        [
            "So here I have.",
            "I have histogram of those cosine similarities between newsarticles and the most similar FDA release.",
            "As you can see here, Y axis is logarithmic.",
            "Also keep that in mind.",
            "Over 75% of all pairs were scored with below .1 result and over about 10% / 10% percent actually was scored with score over .9.",
            "Well, basically.",
            "Uh.",
            "Basically two articles which have very low similarity cosine cosine similarity score in terms of higher grams.",
            "Are not sharing basically any.",
            "Significantly large amount of terms and distinct then you can just you know yeah, they are disconnected completely.",
            "Yeah, that's right.",
            "Actually the threshold for saying that would be around here, but well, I will come to that as well.",
            "So well, basically we have.",
            "We have one case when articles are just a copy of each other with some modifications were coming down, going down with cosine similarity and from the other side this course which are really close to 0 means that particles.",
            "Have actually nothing in common.",
            "So there is also a lot of this this stuff in the middle, which is kind of tricky to tell in the very very moment how to.",
            "Discriminate between.",
            "Cases but."
        ],
        [
            "Well, that's what I was trying to capture with the study.",
            "Basically I asked for persons from SDA to annotate the pairs.",
            "I had two people scoring all of the pairs and two people scoring only only small part.",
            "Like about hundreds of leases, and here are basic statistics of mugs people gave to the person I proposed for scores for the given pair.",
            "Could be could be could be scored with basically the worst case for me would be not found, which means that the proper release has not been found the correct list, but it is present in the STL database.",
            "It means that classifier well failed.",
            "Miserably, and it's a place for improvement.",
            "Another score is no connection, so.",
            "The algorithm has not found the proper source release because it's just not there because there was not a source and I had two types of.",
            "Of relation for similar articles one would be.",
            "Indirect source, I know the name is not very nice, but let's stick with that.",
            "So the.",
            "Candidate release and article which deeper are based both on the same third article.",
            "Actually there are more more cases to cover, but in the very simplified version it would be like that, like if they can be similar, but there is no straight.",
            "Information flow between those, but also they can be based then can be based on the same third article also.",
            "One is based on Article A which produced later Article B and the second article would be based on that, but.",
            "I took all the cases who this one category and the last one, the highest high score, is that.",
            "The classifier properly found.",
            "This sort of series and it was accepted by by annotators.",
            "Well, as I had four people, and as you can see, not all scores.",
            "Only looking here.",
            "You can tell that.",
            "This course will not 100% similar so I had to discuss with the people 'cause you know, like.",
            "You can write something like OK, is that so you can write.",
            "You can propose this course, but it's like everyone would understand it its own in his own way her own way.",
            "So it was needed to make make the final list.",
            "So to have something for comparison to train classifier with.",
            "Well, maybe I."
        ],
        [
            "Should show you as well this this tables which shows agreement agreement among among commentators.",
            "As you can see here.",
            "People are usually.",
            "Usually.",
            "In agreement, in about 87%, if I recall correctly, for the first case.",
            "But it's also good moment to to introduce two simplifications to have two way classifiers later.",
            "So the first simplification would be to merge direct and indirect scores.",
            "So basically, in this simplification A we have trained classifier to discriminate between articles which were based one on each other or or both based on the same source.",
            "Or not.",
            "Sharing a common source, ancestors and the second case would be too much.",
            "Indirect scores, sorry indirect sourcing and not connection.",
            "So basically this classifiers I mean based on this simplification with detect if particles are based on each other.",
            "Buy directly on each other.",
            "No link in between.",
            "So.",
            "OK, as you can see, merging those indirect relation and no no connection yields with higher agreement percentages.",
            "I believe it's because of the."
        ],
        [
            "This disproportion here, so the person they noted the first annotator scored over 50 pairs with ideas, ideas mark.",
            "Second, only 12 of them, but the that.",
            "That set is probably something that is missing from here.",
            "So well, I guess not basically, huh?"
        ],
        [
            "Good read, something.",
            "I mean it's not.",
            "It's not a result actually.",
            "It's agreement among people.",
            "So basically it shows you that it's not.",
            "Really clear it's not always easy to tell to differentiate between between those four professional editors or what.",
            "Yeah, these people were.",
            "Well, professional editors, people working from solving status.",
            "Governor, I can see that there are writing articles on daily basis.",
            "And they spend how much time like 2 hours let me think it was people who did about 500 annotations.",
            "It took them.",
            "It took them about two or three days, but it was like in terms of hours.",
            "It was 6 to 8 hours probably.",
            "Yeah, so it's not really effective here.",
            "Of course it would be really good to have all purse chords and like not only one and top similarity score, but well, we have limited first."
        ],
        [
            "OK, so.",
            "After creating the final list.",
            "I was OK.",
            "The final list was was an Arctic pairs of an article proper source release which were found either by algorithm accepted by human or provided by STAACA little.",
            "So I considered various various values of so many anagrams but also consider sets of.",
            "Of few ends at the same.",
            "Time using those simplifications allows us to train only two A classifiers, which is actually making calculation easier so well for a given and I made the procedure.",
            "As I showed you before.",
            "But I tried to divide to set a threshold on different on different levels and I.",
            "How?",
            "I was keeping track of the threshold for each N, which is a result in highest F1 score.",
            "Well, just a quick recall.",
            "F1 score is a harmonic mean of recall and precision.",
            "These measures are centered about around positive cases, so I guess it's pretty good to use it in this this way.",
            "Well, as shown on the very top I have analyzed also few types of values of ends under the same time.",
            "But in the table you can see only that 234 thing is because of that.",
            "As you can see even for different answer results are really similar, not really using under and doesn't really change anything dramatically, so it would make no sense to put all of those guys here.",
            "It would be awesome.",
            "It will only make this thing harder to read.",
            "So as you can see, when I try to different discriminate between.",
            "If articles were based on the same source, so one on each other or both on the same third article I obtained.",
            "IPhone scores .84 most which was like 91% accuracy.",
            "Bob, even for those those classifiers which obtained the best results.",
            "I got few errors.",
            "I defined errors as situations where, well, the proper.",
            "So this was not found, but it was present in the in the data set that I got the final list.",
            "But the second case deals with kind of better F1 and accuracies.",
            "I got about 90 about .9 F, one scores and about 96.",
            "Percent accuracy and, well, it's also important.",
            "Lower amount of of those errors.",
            "Maybe.",
            "Yep.",
            "Maybe you.",
            "If I were you training, huh?",
            "What kind of classifier were you training?",
            "OK, so I tried to.",
            "It was calculating cosine with different engrams as shown and I built a classifier on top of each of those.",
            "So basically this is training classifier would be to define threshold which yields with best score.",
            "Basically finding some parameter value that you want to apply it to.",
            "This yeah, that's right in this in this version, but it works this way.",
            "Because, well, it's when using only one one feature.",
            "Basically it's generated classifier, so that's it."
        ],
        [
            "OK, so here I have to.",
            "Two cases, 1441 grams, another one for 3 grams.",
            "On the top of both figures, you can see histograms linear on Y axis this time.",
            "On the right hand side, so free grams histogram is very similar to that one I got from 2, three and four grams and on the left hand side there are one gram cosine similarities and this results are.",
            "As you can see, kind of shifted because on the bottom of those figures I show you fractions of given score.",
            "In each similarity bill so.",
            "As you can see on the left hand side, these guys with really low score signs, 1 gram similarity are not related to each other and on the right hand side you can see that.",
            "Most of cases.",
            "When the articles are not related to each other.",
            "Are in the very first bin of this cosine similarity.",
            "What was a little hard for me to understand was that, well, so I told you that if I merge those two.",
            "Then I have lower accuracy, slower reference than if I.",
            "Merge and See and IDs course well.",
            "Because looking at the pictures, if you would match those black and blue regions will get nearly nearly.",
            "Same color.",
            "My rectangle here, but well, most of cases are actually in this very bin, so.",
            "This is the reason why why it's looking like."
        ],
        [
            "That so well there is a lot of of.",
            "Possible improvements, probably.",
            "Using anagrams in the in this way, I did want allow you to want allow you to detect disguised copying.",
            "So if the articles were written.",
            "So.",
            "If some words were missing or were others, it's kind of hard to find those.",
            "Those similarity properly.",
            "With Engrams maybe proper filtering could be would be would make it better, but definitely that.",
            "Like using Climatization, maybe Skip Grams would be would be the way to.",
            "Allow classifier to disguise scoping thing.",
            "Also maybe taking better care of proper nouns quotations and well, stuff like that.",
            "Maybe this really frequent where should be filtered in the in the very first phase, and that would probably allow to lower score between articles which were similar, because sharing the quotation, proper nouns and stuff like that.",
            "Also, nice think would be to use.",
            "So make use of work that.",
            "People from JSR I doing so I I mean excellent.",
            "So cross lingual methods.",
            "And actually I think I should do in the very like few months ago.",
            "So combine those different classes, different similarity scores into one classifier that would make more sense probably."
        ],
        [
            "So.",
            "I will sum sum the sum of the presentation up now.",
            "So basically I detected that.",
            "It is possible to make to make classifiers based on TF IDF weighted bag of engrams representations of particles to the tag if particles are.",
            "Based on each other or not, but the accuracy and other parameters were worse in the second simplification case, so trying to detect if they are both based on each other or.",
            "The both based on the third particle.",
            "So 12 there is a lot of room for improvement.",
            "As I wrote here, so I hope it would be really nice to keep the thing going.",
            "Well, I know it's like the level was.",
            "Kind of slow, but well, I'm a physicist trying to get grasp on that.",
            "Well, thanks for thanks for your attention.",
            "People."
        ],
        [
            "So thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So well, hello everyone.",
                    "label": 0
                },
                {
                    "sent": "Once again my name is Antonio Ski and I will present study on information so I will call this way later.",
                    "label": 0
                },
                {
                    "sent": "I will tell you why between news articles in Slovene media this is work I did.",
                    "label": 1
                },
                {
                    "sent": "During my statement in Louisiana with Gregor Leben but also with Sebastian Magic, another shot record from sovereign status governor against here.",
                    "label": 0
                },
                {
                    "sent": "Eat.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, let me start with present presentation plan.",
                    "label": 1
                },
                {
                    "sent": "So we'll start.",
                    "label": 0
                },
                {
                    "sent": "I will start with this thing and later I will tell you about my aims of the study.",
                    "label": 0
                },
                {
                    "sent": "Later I will show the big picture of it few words about data, also about human experimentation.",
                    "label": 1
                },
                {
                    "sent": "Annotation experiment, which I guess is most interesting part later I will tell you about results.",
                    "label": 0
                },
                {
                    "sent": "I got training classifiers on the on the data so few words about further work and I will finish with.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Summary so.",
                    "label": 0
                },
                {
                    "sent": "The main goal of my cooperation with SCA was to.",
                    "label": 0
                },
                {
                    "sent": "To make real striking study I don't know.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe in the future.",
                    "label": 0
                },
                {
                    "sent": "Maybe in the future it will get someone better.",
                    "label": 0
                },
                {
                    "sent": "We have some.",
                    "label": 0
                },
                {
                    "sent": "Initial initial initial results but.",
                    "label": 0
                },
                {
                    "sent": "I also wanted to use that that cooperation to construct those classifiers and estimate the accuracy, and I will try to convince you that it also is also possible to use the method to.",
                    "label": 1
                },
                {
                    "sent": "Detect information flow between newsarticles.",
                    "label": 0
                },
                {
                    "sent": "Maybe not the very very present form, but well, it's some distant goal in the in the future.",
                    "label": 0
                },
                {
                    "sent": "What is reduce tracking?",
                    "label": 0
                },
                {
                    "sent": "OK, it's kind of.",
                    "label": 0
                },
                {
                    "sent": "Confusing because there are a few terms about the very same.",
                    "label": 0
                },
                {
                    "sent": "Think maybe the next slide.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would make you will make it a little bit more clear because release tracking is actually very very similar thing as plug it in detection.",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "Because when you have news provider such as solving status code noggins here then people media from the country and those outside the country I are using your really releases so.",
                    "label": 0
                },
                {
                    "sent": "Part of those users are are allowed because these other media are the subscribers, but in some cases it's just plagiarism.",
                    "label": 0
                },
                {
                    "sent": "So actually it's very similar thing on this figure I got from Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "There is a comparison of.",
                    "label": 0
                },
                {
                    "sent": "It's a diagram which shows how different methods can can allow us to detect different time of plagiarism.",
                    "label": 0
                },
                {
                    "sent": "Or maybe this in this case also reveals tracking.",
                    "label": 0
                },
                {
                    "sent": "Basically my methods were bhagavan grams, so it's actually something between substring matching and bag of words, analyzes.",
                    "label": 0
                },
                {
                    "sent": "As you can see on the diagram, those two approaches allow us to detect, copy and paste plagiarism or only part of cases of these guys plagiarism, but actually now now really no possibility to detect any other types if text was processed in.",
                    "label": 0
                },
                {
                    "sent": "Had major major ever writes then.",
                    "label": 0
                },
                {
                    "sent": "Well actually it's very hard to detect this with these methods.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, uh.",
                    "label": 0
                },
                {
                    "sent": "So maybe that means that now.",
                    "label": 0
                },
                {
                    "sent": "Tracking how particular piece of content?",
                    "label": 0
                },
                {
                    "sent": "Disseminates maybe changes the form, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually, that was the idea.",
                    "label": 0
                },
                {
                    "sent": "Legal or sometimes illegal, so this is another feature really which.",
                    "label": 0
                },
                {
                    "sent": "Message the lawyers, but otherwise, but I don't care about the fact that there are.",
                    "label": 0
                },
                {
                    "sent": "Other news agencies writing about the same topic.",
                    "label": 0
                },
                {
                    "sent": "They care about their other news agencies or individuals that are copying their work.",
                    "label": 0
                },
                {
                    "sent": "In particular, I think big ears, yeah.",
                    "label": 0
                },
                {
                    "sent": "Surely what I was trying to detect is that if two articles are not really similar because that's something rather rather cruel, rather easy to detect.",
                    "label": 0
                },
                {
                    "sent": "But if they are based on the one on each other, or if both are based on some third article, but I will, I will come to that.",
                    "label": 0
                },
                {
                    "sent": "OK so on this slide I would like to present your study outline.",
                    "label": 1
                },
                {
                    "sent": "Maybe they just have the picture in that mind so well.",
                    "label": 0
                },
                {
                    "sent": "Basically we have to start with gathering data which were solid news outlets in.",
                    "label": 1
                },
                {
                    "sent": "They were tracked by Event Registry.",
                    "label": 0
                },
                {
                    "sent": "After the after the gathering data, I chose one day of Slovenian newsarticles and for each article in article.",
                    "label": 1
                },
                {
                    "sent": "I was trying to find the most similar article in STL database in like 7 days in that day in seven days before.",
                    "label": 0
                },
                {
                    "sent": "Using those candy that pairs to trade, I gave those candidate pairs to human human annotators because it would be the best to have people marking each pair, of course, but it would take way too much time.",
                    "label": 0
                },
                {
                    "sent": "So in this way we are limiting cases just to few hundreds, which makes annotation study possible.",
                    "label": 0
                },
                {
                    "sent": "After that where I am trying.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm I'm creating finalists.",
                    "label": 0
                },
                {
                    "sent": "Of for a given day, then train some other classifiers and see how good they are.",
                    "label": 0
                },
                {
                    "sent": "Well, in the end I will come to improvements that could be.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Possibly made well, very crude slide about data be cause you will see so at the moment of where the study was conducted there were there were about 60 Slovenian media outlets in Event registry, most of them being being well.",
                    "label": 1
                },
                {
                    "sent": "Mainstream media, actually, I guess because there were a few blogs there.",
                    "label": 0
                },
                {
                    "sent": "There were like smaller, more specific sites, but most of them being.",
                    "label": 0
                },
                {
                    "sent": "Yeah, could be considered as somewhere more like on the mainstream, and half would be more like on the block site.",
                    "label": 0
                },
                {
                    "sent": "So I was this is kind of getting intermixed at some points.",
                    "label": 0
                },
                {
                    "sent": "Yeah, kind of like.",
                    "label": 0
                },
                {
                    "sent": "And at that time in June, that would be right definitely for that.",
                    "label": 0
                },
                {
                    "sent": "For that time I was checking yesterday and there are about 45 media outlets now.",
                    "label": 0
                },
                {
                    "sent": "Eventually history Slovenian and I took a look at the list.",
                    "label": 0
                },
                {
                    "sent": "And, well, most of them being through.",
                    "label": 0
                },
                {
                    "sent": "Proper media outlets?",
                    "label": 0
                },
                {
                    "sent": "OK, so from that day I chose at random 469 articles out of about 900.",
                    "label": 0
                },
                {
                    "sent": "Publishing that today.",
                    "label": 0
                },
                {
                    "sent": "Well, the reason why is why the numbers so low is that I had.",
                    "label": 0
                },
                {
                    "sent": "We had limited workforce and it was hard to make so many annotations.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But as I mentioned, first thing was to gather candidate source releases for each news item published published on the day.",
                    "label": 1
                },
                {
                    "sent": "For the sake of further human annotation study.",
                    "label": 0
                },
                {
                    "sent": "So how, how, how exactly I chose, I found the candidate source releases.",
                    "label": 1
                },
                {
                    "sent": "So for a given day, I know that first thing STD releases published in.",
                    "label": 0
                },
                {
                    "sent": "In the day and week before.",
                    "label": 0
                },
                {
                    "sent": "I loved it.",
                    "label": 0
                },
                {
                    "sent": "Also, Slovene articles from 2 preceding quicks to try, and TF IDF corpus.",
                    "label": 0
                },
                {
                    "sent": "And they also did well.",
                    "label": 0
                },
                {
                    "sent": "I start equals.",
                    "label": 0
                },
                {
                    "sent": "And they need security here are.",
                    "label": 0
                },
                {
                    "sent": "Here is specification of TF IDF procedure.",
                    "label": 0
                },
                {
                    "sent": "I tried so basically for candidates or services I have used 2, three and four grams at the same time with Atlas smooth things to take care of.",
                    "label": 0
                },
                {
                    "sent": "Rare, really rare phrases and they discarded terms which are present in more than 25% of training corpus.",
                    "label": 1
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "So for a given data to find candidates or services, I gathered vocab vocabulary for all these sets.",
                    "label": 0
                },
                {
                    "sent": "So basically this.",
                    "label": 0
                },
                {
                    "sent": "Types of engrams later trying to TF IDF on.",
                    "label": 0
                },
                {
                    "sent": "This set on training corpus and later calculated cosine similarity between each Arctic analyst article and.",
                    "label": 0
                },
                {
                    "sent": "This series is which could be source of a given article.",
                    "label": 0
                },
                {
                    "sent": "So for each article from this C set I saved the most similar article from.",
                    "label": 1
                },
                {
                    "sent": "SD database.",
                    "label": 0
                },
                {
                    "sent": "And scored to take through.",
                    "label": 0
                },
                {
                    "sent": "Keep track of that and this were counted that purse so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I have.",
                    "label": 0
                },
                {
                    "sent": "I have histogram of those cosine similarities between newsarticles and the most similar FDA release.",
                    "label": 0
                },
                {
                    "sent": "As you can see here, Y axis is logarithmic.",
                    "label": 0
                },
                {
                    "sent": "Also keep that in mind.",
                    "label": 0
                },
                {
                    "sent": "Over 75% of all pairs were scored with below .1 result and over about 10% / 10% percent actually was scored with score over .9.",
                    "label": 0
                },
                {
                    "sent": "Well, basically.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Basically two articles which have very low similarity cosine cosine similarity score in terms of higher grams.",
                    "label": 0
                },
                {
                    "sent": "Are not sharing basically any.",
                    "label": 0
                },
                {
                    "sent": "Significantly large amount of terms and distinct then you can just you know yeah, they are disconnected completely.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "Actually the threshold for saying that would be around here, but well, I will come to that as well.",
                    "label": 0
                },
                {
                    "sent": "So well, basically we have.",
                    "label": 0
                },
                {
                    "sent": "We have one case when articles are just a copy of each other with some modifications were coming down, going down with cosine similarity and from the other side this course which are really close to 0 means that particles.",
                    "label": 0
                },
                {
                    "sent": "Have actually nothing in common.",
                    "label": 0
                },
                {
                    "sent": "So there is also a lot of this this stuff in the middle, which is kind of tricky to tell in the very very moment how to.",
                    "label": 0
                },
                {
                    "sent": "Discriminate between.",
                    "label": 0
                },
                {
                    "sent": "Cases but.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, that's what I was trying to capture with the study.",
                    "label": 0
                },
                {
                    "sent": "Basically I asked for persons from SDA to annotate the pairs.",
                    "label": 0
                },
                {
                    "sent": "I had two people scoring all of the pairs and two people scoring only only small part.",
                    "label": 0
                },
                {
                    "sent": "Like about hundreds of leases, and here are basic statistics of mugs people gave to the person I proposed for scores for the given pair.",
                    "label": 0
                },
                {
                    "sent": "Could be could be could be scored with basically the worst case for me would be not found, which means that the proper release has not been found the correct list, but it is present in the STL database.",
                    "label": 1
                },
                {
                    "sent": "It means that classifier well failed.",
                    "label": 0
                },
                {
                    "sent": "Miserably, and it's a place for improvement.",
                    "label": 0
                },
                {
                    "sent": "Another score is no connection, so.",
                    "label": 0
                },
                {
                    "sent": "The algorithm has not found the proper source release because it's just not there because there was not a source and I had two types of.",
                    "label": 0
                },
                {
                    "sent": "Of relation for similar articles one would be.",
                    "label": 0
                },
                {
                    "sent": "Indirect source, I know the name is not very nice, but let's stick with that.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 1
                },
                {
                    "sent": "Candidate release and article which deeper are based both on the same third article.",
                    "label": 0
                },
                {
                    "sent": "Actually there are more more cases to cover, but in the very simplified version it would be like that, like if they can be similar, but there is no straight.",
                    "label": 0
                },
                {
                    "sent": "Information flow between those, but also they can be based then can be based on the same third article also.",
                    "label": 0
                },
                {
                    "sent": "One is based on Article A which produced later Article B and the second article would be based on that, but.",
                    "label": 0
                },
                {
                    "sent": "I took all the cases who this one category and the last one, the highest high score, is that.",
                    "label": 0
                },
                {
                    "sent": "The classifier properly found.",
                    "label": 0
                },
                {
                    "sent": "This sort of series and it was accepted by by annotators.",
                    "label": 0
                },
                {
                    "sent": "Well, as I had four people, and as you can see, not all scores.",
                    "label": 0
                },
                {
                    "sent": "Only looking here.",
                    "label": 0
                },
                {
                    "sent": "You can tell that.",
                    "label": 0
                },
                {
                    "sent": "This course will not 100% similar so I had to discuss with the people 'cause you know, like.",
                    "label": 0
                },
                {
                    "sent": "You can write something like OK, is that so you can write.",
                    "label": 0
                },
                {
                    "sent": "You can propose this course, but it's like everyone would understand it its own in his own way her own way.",
                    "label": 0
                },
                {
                    "sent": "So it was needed to make make the final list.",
                    "label": 0
                },
                {
                    "sent": "So to have something for comparison to train classifier with.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe I.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Should show you as well this this tables which shows agreement agreement among among commentators.",
                    "label": 0
                },
                {
                    "sent": "As you can see here.",
                    "label": 0
                },
                {
                    "sent": "People are usually.",
                    "label": 0
                },
                {
                    "sent": "Usually.",
                    "label": 0
                },
                {
                    "sent": "In agreement, in about 87%, if I recall correctly, for the first case.",
                    "label": 0
                },
                {
                    "sent": "But it's also good moment to to introduce two simplifications to have two way classifiers later.",
                    "label": 0
                },
                {
                    "sent": "So the first simplification would be to merge direct and indirect scores.",
                    "label": 0
                },
                {
                    "sent": "So basically, in this simplification A we have trained classifier to discriminate between articles which were based one on each other or or both based on the same source.",
                    "label": 0
                },
                {
                    "sent": "Or not.",
                    "label": 0
                },
                {
                    "sent": "Sharing a common source, ancestors and the second case would be too much.",
                    "label": 0
                },
                {
                    "sent": "Indirect scores, sorry indirect sourcing and not connection.",
                    "label": 0
                },
                {
                    "sent": "So basically this classifiers I mean based on this simplification with detect if particles are based on each other.",
                    "label": 0
                },
                {
                    "sent": "Buy directly on each other.",
                    "label": 0
                },
                {
                    "sent": "No link in between.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, as you can see, merging those indirect relation and no no connection yields with higher agreement percentages.",
                    "label": 0
                },
                {
                    "sent": "I believe it's because of the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This disproportion here, so the person they noted the first annotator scored over 50 pairs with ideas, ideas mark.",
                    "label": 0
                },
                {
                    "sent": "Second, only 12 of them, but the that.",
                    "label": 0
                },
                {
                    "sent": "That set is probably something that is missing from here.",
                    "label": 0
                },
                {
                    "sent": "So well, I guess not basically, huh?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good read, something.",
                    "label": 0
                },
                {
                    "sent": "I mean it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not a result actually.",
                    "label": 0
                },
                {
                    "sent": "It's agreement among people.",
                    "label": 0
                },
                {
                    "sent": "So basically it shows you that it's not.",
                    "label": 0
                },
                {
                    "sent": "Really clear it's not always easy to tell to differentiate between between those four professional editors or what.",
                    "label": 0
                },
                {
                    "sent": "Yeah, these people were.",
                    "label": 0
                },
                {
                    "sent": "Well, professional editors, people working from solving status.",
                    "label": 0
                },
                {
                    "sent": "Governor, I can see that there are writing articles on daily basis.",
                    "label": 0
                },
                {
                    "sent": "And they spend how much time like 2 hours let me think it was people who did about 500 annotations.",
                    "label": 0
                },
                {
                    "sent": "It took them.",
                    "label": 0
                },
                {
                    "sent": "It took them about two or three days, but it was like in terms of hours.",
                    "label": 0
                },
                {
                    "sent": "It was 6 to 8 hours probably.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's not really effective here.",
                    "label": 0
                },
                {
                    "sent": "Of course it would be really good to have all purse chords and like not only one and top similarity score, but well, we have limited first.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "After creating the final list.",
                    "label": 0
                },
                {
                    "sent": "I was OK.",
                    "label": 0
                },
                {
                    "sent": "The final list was was an Arctic pairs of an article proper source release which were found either by algorithm accepted by human or provided by STAACA little.",
                    "label": 0
                },
                {
                    "sent": "So I considered various various values of so many anagrams but also consider sets of.",
                    "label": 0
                },
                {
                    "sent": "Of few ends at the same.",
                    "label": 0
                },
                {
                    "sent": "Time using those simplifications allows us to train only two A classifiers, which is actually making calculation easier so well for a given and I made the procedure.",
                    "label": 0
                },
                {
                    "sent": "As I showed you before.",
                    "label": 0
                },
                {
                    "sent": "But I tried to divide to set a threshold on different on different levels and I.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "I was keeping track of the threshold for each N, which is a result in highest F1 score.",
                    "label": 1
                },
                {
                    "sent": "Well, just a quick recall.",
                    "label": 0
                },
                {
                    "sent": "F1 score is a harmonic mean of recall and precision.",
                    "label": 0
                },
                {
                    "sent": "These measures are centered about around positive cases, so I guess it's pretty good to use it in this this way.",
                    "label": 0
                },
                {
                    "sent": "Well, as shown on the very top I have analyzed also few types of values of ends under the same time.",
                    "label": 0
                },
                {
                    "sent": "But in the table you can see only that 234 thing is because of that.",
                    "label": 0
                },
                {
                    "sent": "As you can see even for different answer results are really similar, not really using under and doesn't really change anything dramatically, so it would make no sense to put all of those guys here.",
                    "label": 0
                },
                {
                    "sent": "It would be awesome.",
                    "label": 0
                },
                {
                    "sent": "It will only make this thing harder to read.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, when I try to different discriminate between.",
                    "label": 0
                },
                {
                    "sent": "If articles were based on the same source, so one on each other or both on the same third article I obtained.",
                    "label": 0
                },
                {
                    "sent": "IPhone scores .84 most which was like 91% accuracy.",
                    "label": 0
                },
                {
                    "sent": "Bob, even for those those classifiers which obtained the best results.",
                    "label": 0
                },
                {
                    "sent": "I got few errors.",
                    "label": 0
                },
                {
                    "sent": "I defined errors as situations where, well, the proper.",
                    "label": 0
                },
                {
                    "sent": "So this was not found, but it was present in the in the data set that I got the final list.",
                    "label": 0
                },
                {
                    "sent": "But the second case deals with kind of better F1 and accuracies.",
                    "label": 0
                },
                {
                    "sent": "I got about 90 about .9 F, one scores and about 96.",
                    "label": 0
                },
                {
                    "sent": "Percent accuracy and, well, it's also important.",
                    "label": 0
                },
                {
                    "sent": "Lower amount of of those errors.",
                    "label": 0
                },
                {
                    "sent": "Maybe.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Maybe you.",
                    "label": 0
                },
                {
                    "sent": "If I were you training, huh?",
                    "label": 0
                },
                {
                    "sent": "What kind of classifier were you training?",
                    "label": 0
                },
                {
                    "sent": "OK, so I tried to.",
                    "label": 0
                },
                {
                    "sent": "It was calculating cosine with different engrams as shown and I built a classifier on top of each of those.",
                    "label": 0
                },
                {
                    "sent": "So basically this is training classifier would be to define threshold which yields with best score.",
                    "label": 0
                },
                {
                    "sent": "Basically finding some parameter value that you want to apply it to.",
                    "label": 0
                },
                {
                    "sent": "This yeah, that's right in this in this version, but it works this way.",
                    "label": 0
                },
                {
                    "sent": "Because, well, it's when using only one one feature.",
                    "label": 0
                },
                {
                    "sent": "Basically it's generated classifier, so that's it.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here I have to.",
                    "label": 0
                },
                {
                    "sent": "Two cases, 1441 grams, another one for 3 grams.",
                    "label": 0
                },
                {
                    "sent": "On the top of both figures, you can see histograms linear on Y axis this time.",
                    "label": 0
                },
                {
                    "sent": "On the right hand side, so free grams histogram is very similar to that one I got from 2, three and four grams and on the left hand side there are one gram cosine similarities and this results are.",
                    "label": 0
                },
                {
                    "sent": "As you can see, kind of shifted because on the bottom of those figures I show you fractions of given score.",
                    "label": 0
                },
                {
                    "sent": "In each similarity bill so.",
                    "label": 0
                },
                {
                    "sent": "As you can see on the left hand side, these guys with really low score signs, 1 gram similarity are not related to each other and on the right hand side you can see that.",
                    "label": 0
                },
                {
                    "sent": "Most of cases.",
                    "label": 0
                },
                {
                    "sent": "When the articles are not related to each other.",
                    "label": 0
                },
                {
                    "sent": "Are in the very first bin of this cosine similarity.",
                    "label": 0
                },
                {
                    "sent": "What was a little hard for me to understand was that, well, so I told you that if I merge those two.",
                    "label": 0
                },
                {
                    "sent": "Then I have lower accuracy, slower reference than if I.",
                    "label": 0
                },
                {
                    "sent": "Merge and See and IDs course well.",
                    "label": 0
                },
                {
                    "sent": "Because looking at the pictures, if you would match those black and blue regions will get nearly nearly.",
                    "label": 0
                },
                {
                    "sent": "Same color.",
                    "label": 0
                },
                {
                    "sent": "My rectangle here, but well, most of cases are actually in this very bin, so.",
                    "label": 0
                },
                {
                    "sent": "This is the reason why why it's looking like.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That so well there is a lot of of.",
                    "label": 0
                },
                {
                    "sent": "Possible improvements, probably.",
                    "label": 0
                },
                {
                    "sent": "Using anagrams in the in this way, I did want allow you to want allow you to detect disguised copying.",
                    "label": 0
                },
                {
                    "sent": "So if the articles were written.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If some words were missing or were others, it's kind of hard to find those.",
                    "label": 0
                },
                {
                    "sent": "Those similarity properly.",
                    "label": 0
                },
                {
                    "sent": "With Engrams maybe proper filtering could be would be would make it better, but definitely that.",
                    "label": 0
                },
                {
                    "sent": "Like using Climatization, maybe Skip Grams would be would be the way to.",
                    "label": 0
                },
                {
                    "sent": "Allow classifier to disguise scoping thing.",
                    "label": 0
                },
                {
                    "sent": "Also maybe taking better care of proper nouns quotations and well, stuff like that.",
                    "label": 0
                },
                {
                    "sent": "Maybe this really frequent where should be filtered in the in the very first phase, and that would probably allow to lower score between articles which were similar, because sharing the quotation, proper nouns and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "Also, nice think would be to use.",
                    "label": 0
                },
                {
                    "sent": "So make use of work that.",
                    "label": 0
                },
                {
                    "sent": "People from JSR I doing so I I mean excellent.",
                    "label": 0
                },
                {
                    "sent": "So cross lingual methods.",
                    "label": 0
                },
                {
                    "sent": "And actually I think I should do in the very like few months ago.",
                    "label": 0
                },
                {
                    "sent": "So combine those different classes, different similarity scores into one classifier that would make more sense probably.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I will sum sum the sum of the presentation up now.",
                    "label": 0
                },
                {
                    "sent": "So basically I detected that.",
                    "label": 0
                },
                {
                    "sent": "It is possible to make to make classifiers based on TF IDF weighted bag of engrams representations of particles to the tag if particles are.",
                    "label": 0
                },
                {
                    "sent": "Based on each other or not, but the accuracy and other parameters were worse in the second simplification case, so trying to detect if they are both based on each other or.",
                    "label": 1
                },
                {
                    "sent": "The both based on the third particle.",
                    "label": 0
                },
                {
                    "sent": "So 12 there is a lot of room for improvement.",
                    "label": 1
                },
                {
                    "sent": "As I wrote here, so I hope it would be really nice to keep the thing going.",
                    "label": 0
                },
                {
                    "sent": "Well, I know it's like the level was.",
                    "label": 0
                },
                {
                    "sent": "Kind of slow, but well, I'm a physicist trying to get grasp on that.",
                    "label": 0
                },
                {
                    "sent": "Well, thanks for thanks for your attention.",
                    "label": 0
                },
                {
                    "sent": "People.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thanks.",
                    "label": 0
                }
            ]
        }
    }
}