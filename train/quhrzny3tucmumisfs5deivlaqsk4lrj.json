{
    "id": "quhrzny3tucmumisfs5deivlaqsk4lrj",
    "title": "FAST-PPR: Scaling Personalized PageRank Estimation for Large Graphs",
    "info": {
        "author": [
            "Peter Lofgren, Computer Science Department, Stanford University"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_lofgren_page_rank_estimation/",
    "segmentation": [
        [
            "They will present a new algorithm for computing personalized Pagerank.",
            "I'm Peter, this is said is also joint work with cestina sheets."
        ],
        [
            "We are motivated by the problem of personalized search.",
            "On social networks.",
            "To give an example, I went to Twitter, signed in and search for a common name Adam to see what would come up.",
            "I follow a lot of people in technology on Twitter so that the top result would be some Adam and technology.",
            "But in fact the top result is Adam Lambert, a singer, and the 2nd result is guitar player who isn't even named Adam.",
            "15 years ago, in the personal and the original page rank paper, the authors proposed goal method used by Google, but they also propose a personalized method."
        ],
        [
            "If you re rank these results using personalized Pagerank.",
            "The top result becomes the CTO of Twitter and the second top result becomes a CEO of Coral.",
            "Both end item.",
            "So we see that, at least in this example, personalized Pagerank can significantly improve the quality of search.",
            "And this is not just a Twitter problem, you can find similar examples on Facebook or Google plus Arlington searches that are not very personalized.",
            "It's not that these companies aren't aware of personalized page rank.",
            "Is that personalized page rank as in very expensive to compute?"
        ],
        [
            "Here are the running time of fast CPR and two state of their algorithms for computing personalized page rank.",
            "This is on a graph of Twitter from 2010.",
            "We find that computing a single personalized present value using Monte Carlo takes 5 minutes, and computing it using local updates, which is like power iteration, takes more than an hour.",
            "But using faster PR takes less than two seconds.",
            "In this talk, I'll explain how we are able to compute personalized Pagerank faster than previous methods."
        ],
        [
            "So we're seeing the definition of personalized page rank in terms of linear algebra, like eigenvectors or stationary distributions.",
            "For this talk, as best to think of it in terms of a single random walk.",
            "Given start node S and target no T. And teleport probably Alpha.",
            "We started S and take a random walk.",
            "After each step was on fixed probability Alpha, we stopped the walk.",
            "Otherwise, we continue to random out neighbor of the current node.",
            "We then see that the personalized page rank from master T is the probability that this walk stops at T. As I said, this is equivalent to the definition in terms of eigenvectors.",
            "Ha.",
            "And also note that our algorithm fast PR can handle arbitrary starting sets.",
            "For example, if you start from a random node in the entire graph, you can estimate GLOW page rank.",
            "If we start from some set of nodes S, like the set of computer scientists, you can find nodes that are interesting for scientists.",
            "Forgiveness.",
            "In this talk, I'll assume that we have a single start node S."
        ],
        [
            "So our goal is given start node S and target T2 estimates piasty the personalized page rank.",
            "Now the difficulty of this problem depends on the size of the probability that we're estimating.",
            "It is very small.",
            "It takes more work to compute whether it's large.",
            "It takes less work.",
            "So parameterized by threshold Delta, which is the smallest personalized Pagerank that we care about.",
            "We require that our algorithm be accurate as long as piracy is larger than Delta.",
            "So three things to know about this goal.",
            "First, there's a natural primitive personalized search.",
            "We said X equal to me on the Twitter graph and T equal to Adam Lambert or Adam D'Angelo.",
            "We can find my interest in those two results and rank them accordingly.",
            "We're computing a single value piussi, not the entire vector, which is millions or hundreds of millions of entries long.",
            "And when choosing Delta we want very small values.",
            "The average value of personalized page rank is one over North, so we want Delta values that are ordered one over North.",
            "We'll see shortly the past algorithms take time, one over Delta, and this means that they take time order N, which makes them not scalable for large graphs.",
            "Another tribe, two previous algorithms, both because we compare against the runtime, and because our algorithm uses them as a routines in its computation."
        ],
        [
            "The first is Montecarlo.",
            "This is the simplest method of estimating any probability.",
            "We simply sample a large number of walks and to see what fraction of them end at the target.",
            "Estimated probability of size Delta.",
            "We need order one over Delta random walks.",
            "So the runtime is order one over Delta."
        ],
        [
            "Another algorithm is local updates, which you can think of as an improved version of power iteration.",
            "Local update works from the target backwards updating estimates from various nodes.",
            "The personalized Pagerank from various nodes to the target will find all the nodes that are close to the target in personalized page rank.",
            "I don't have time to describe it fully, but the average running time is order D over Delta.",
            "What is the average degree and Delta is the size of the probability resonating?",
            "So we see that both pass algorithms took time order one over Delta and Delta is order one over North.",
            "So both pass algorithms are taking time order 1 / N on large networks."
        ],
        [
            "Here is our result.",
            "For any.",
            "In any graph, any source target pair.",
            "We probably get small relative error with high probability.",
            "Furthermore, running time is probably order one over square root, Delta Times Square D for D is the average degree of the graph.",
            "Looking at this running time.",
            "Notice that most large graphs are sparse, so typical value of D might be 100 and Square D might be 10.",
            "But Delta is around one over add, maybe 1 / 100 million.",
            "So going from one over Delta to square root Delta means going from running time order 100 million to order 10,000.",
            "Square root is a huge improvement for Delta on large graphs.",
            "We also prove lower bounds that any algorithm which computes personalized Pagerank must look at one over square Delta nodes in the edges in the graph.",
            "So our dependence on Delta cannot be improved."
        ],
        [
            "To understand how we get this improvement from one over Delta to one over square root Delta, consider an analogy to bidirectional search.",
            "Suppose we're finding a path of length L and a graph where every node has degree D. Using breadth first search from the search, we need to expand the to the L nodes.",
            "But if you do do a bidirectional search of over 2 distance from the targets source analysis from target intersect, we can find a path of length L and time on the order square root G to the L. So the similar improvement from duty altered square root to the L. We do in August thing for personalized Pagerank.",
            "Only we define halfway, not in terms of path distance, but in terms of personalized page rank.",
            "Given start S and target T. We start by running the local update algorithm to find all nodes that are closer to an personalized page rank.",
            "So for all this Gray sets the set of all nodes that have a personalized page rank to the targets bigger than square root Delta."
        ],
        [
            "Then it turns out we need to do one step further and find the frontier.",
            "These are nodes that are not themselves close to the targets.",
            "By this definition, of course, but there one step away from being close to the targets.",
            "Once we have this frontier.",
            "Separating the source from the target, we sample random walks forward from the source node.",
            "Every random rock.",
            "Stops providing the frontier.",
            "It could be 0 to estimate a personalized page rank.",
            "Every random walk reaches the frontier.",
            "Say some nodu.",
            "Then, rather than sampling the rest of the walk, we simulate the rest of the walk.",
            "And use an estimate of Pi OT that was computed earlier by local updates.",
            "This is the entire algorithm.",
            "Twin"
        ],
        [
            "And while this works, notes that every path from the source the target must fit the frontier at some points on the path.",
            "So we can decompose the probability of going from source to target and the sum over all nodes you in the frontier or the probability that the walk goes from the source to that node.",
            "You in the Frontier times the probability of going from that node.",
            "You in the frontier and stopping at the target.",
            "Because their walks remember the memory lists.",
            "We can use that property to decompose this.",
            "The probability of going from source to target."
        ],
        [
            "Now describe our experience here and to measure the running time of fast CPR.",
            "We chose 6 graphs for diversity.",
            "We have two undirected social networks to directed in social networks.",
            "Web graph and a collaboration graph and engine size from a few million edges to a few billion edges.",
            "Also note that all three algorithms have parameters that you trade off accuracy and runtime.",
            "We want a level playing field so that the parameters for all three algorithms.",
            "Such that the relative error is about 10% for all three algorithms.",
            "Author algorithms have some error and we set the empirical error to be the same.",
            "Then we can fairly measure the running time of these three algorithms."
        ],
        [
            "When we do that, this is the result that we get on the X axis, and varying these six diverse graphs.",
            "In blue I have running time with fast PR for an average source target pair.",
            "An orange of the running time of local updates and then green of the running time of Monte Carlo.",
            "Notice that the Y axis is a log scale.",
            "Each horizontal line is a factor of 10.",
            "Improvement in running time.",
            "We see that every graph we tried, personal fast CPR was sometimes faster in the state of the art algorithms and the two largest graphs.",
            "It is 20 times faster.",
            "Going from more than five minutes to two seconds."
        ],
        [
            "So to summarize.",
            "We presented fast CPR.",
            "Which estimates personalized page rank 10 times faster than previous algorithms.",
            "Is probably accurate and probably takes orders.",
            "Square D over Delta Time will previous algorithms took order one over Delta time?",
            "I haven't had time to describe it, but if you do storage of water D over squared Delta per node, you can get a worst case running time of one over Delta.",
            "Future work will be to close the factor of Square D between our lower bound of one over Square Delta and are running time squared over square root Delta.",
            "And to build a more scalable method which can be used in a personalized search system."
        ],
        [
            "I do."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They will present a new algorithm for computing personalized Pagerank.",
                    "label": 0
                },
                {
                    "sent": "I'm Peter, this is said is also joint work with cestina sheets.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are motivated by the problem of personalized search.",
                    "label": 0
                },
                {
                    "sent": "On social networks.",
                    "label": 0
                },
                {
                    "sent": "To give an example, I went to Twitter, signed in and search for a common name Adam to see what would come up.",
                    "label": 0
                },
                {
                    "sent": "I follow a lot of people in technology on Twitter so that the top result would be some Adam and technology.",
                    "label": 0
                },
                {
                    "sent": "But in fact the top result is Adam Lambert, a singer, and the 2nd result is guitar player who isn't even named Adam.",
                    "label": 0
                },
                {
                    "sent": "15 years ago, in the personal and the original page rank paper, the authors proposed goal method used by Google, but they also propose a personalized method.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you re rank these results using personalized Pagerank.",
                    "label": 0
                },
                {
                    "sent": "The top result becomes the CTO of Twitter and the second top result becomes a CEO of Coral.",
                    "label": 0
                },
                {
                    "sent": "Both end item.",
                    "label": 0
                },
                {
                    "sent": "So we see that, at least in this example, personalized Pagerank can significantly improve the quality of search.",
                    "label": 0
                },
                {
                    "sent": "And this is not just a Twitter problem, you can find similar examples on Facebook or Google plus Arlington searches that are not very personalized.",
                    "label": 0
                },
                {
                    "sent": "It's not that these companies aren't aware of personalized page rank.",
                    "label": 0
                },
                {
                    "sent": "Is that personalized page rank as in very expensive to compute?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are the running time of fast CPR and two state of their algorithms for computing personalized page rank.",
                    "label": 0
                },
                {
                    "sent": "This is on a graph of Twitter from 2010.",
                    "label": 0
                },
                {
                    "sent": "We find that computing a single personalized present value using Monte Carlo takes 5 minutes, and computing it using local updates, which is like power iteration, takes more than an hour.",
                    "label": 0
                },
                {
                    "sent": "But using faster PR takes less than two seconds.",
                    "label": 0
                },
                {
                    "sent": "In this talk, I'll explain how we are able to compute personalized Pagerank faster than previous methods.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're seeing the definition of personalized page rank in terms of linear algebra, like eigenvectors or stationary distributions.",
                    "label": 0
                },
                {
                    "sent": "For this talk, as best to think of it in terms of a single random walk.",
                    "label": 0
                },
                {
                    "sent": "Given start node S and target no T. And teleport probably Alpha.",
                    "label": 0
                },
                {
                    "sent": "We started S and take a random walk.",
                    "label": 0
                },
                {
                    "sent": "After each step was on fixed probability Alpha, we stopped the walk.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, we continue to random out neighbor of the current node.",
                    "label": 0
                },
                {
                    "sent": "We then see that the personalized page rank from master T is the probability that this walk stops at T. As I said, this is equivalent to the definition in terms of eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Ha.",
                    "label": 0
                },
                {
                    "sent": "And also note that our algorithm fast PR can handle arbitrary starting sets.",
                    "label": 0
                },
                {
                    "sent": "For example, if you start from a random node in the entire graph, you can estimate GLOW page rank.",
                    "label": 0
                },
                {
                    "sent": "If we start from some set of nodes S, like the set of computer scientists, you can find nodes that are interesting for scientists.",
                    "label": 0
                },
                {
                    "sent": "Forgiveness.",
                    "label": 0
                },
                {
                    "sent": "In this talk, I'll assume that we have a single start node S.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our goal is given start node S and target T2 estimates piasty the personalized page rank.",
                    "label": 0
                },
                {
                    "sent": "Now the difficulty of this problem depends on the size of the probability that we're estimating.",
                    "label": 0
                },
                {
                    "sent": "It is very small.",
                    "label": 0
                },
                {
                    "sent": "It takes more work to compute whether it's large.",
                    "label": 0
                },
                {
                    "sent": "It takes less work.",
                    "label": 0
                },
                {
                    "sent": "So parameterized by threshold Delta, which is the smallest personalized Pagerank that we care about.",
                    "label": 0
                },
                {
                    "sent": "We require that our algorithm be accurate as long as piracy is larger than Delta.",
                    "label": 0
                },
                {
                    "sent": "So three things to know about this goal.",
                    "label": 0
                },
                {
                    "sent": "First, there's a natural primitive personalized search.",
                    "label": 0
                },
                {
                    "sent": "We said X equal to me on the Twitter graph and T equal to Adam Lambert or Adam D'Angelo.",
                    "label": 0
                },
                {
                    "sent": "We can find my interest in those two results and rank them accordingly.",
                    "label": 0
                },
                {
                    "sent": "We're computing a single value piussi, not the entire vector, which is millions or hundreds of millions of entries long.",
                    "label": 0
                },
                {
                    "sent": "And when choosing Delta we want very small values.",
                    "label": 0
                },
                {
                    "sent": "The average value of personalized page rank is one over North, so we want Delta values that are ordered one over North.",
                    "label": 0
                },
                {
                    "sent": "We'll see shortly the past algorithms take time, one over Delta, and this means that they take time order N, which makes them not scalable for large graphs.",
                    "label": 0
                },
                {
                    "sent": "Another tribe, two previous algorithms, both because we compare against the runtime, and because our algorithm uses them as a routines in its computation.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first is Montecarlo.",
                    "label": 0
                },
                {
                    "sent": "This is the simplest method of estimating any probability.",
                    "label": 0
                },
                {
                    "sent": "We simply sample a large number of walks and to see what fraction of them end at the target.",
                    "label": 0
                },
                {
                    "sent": "Estimated probability of size Delta.",
                    "label": 0
                },
                {
                    "sent": "We need order one over Delta random walks.",
                    "label": 0
                },
                {
                    "sent": "So the runtime is order one over Delta.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another algorithm is local updates, which you can think of as an improved version of power iteration.",
                    "label": 0
                },
                {
                    "sent": "Local update works from the target backwards updating estimates from various nodes.",
                    "label": 1
                },
                {
                    "sent": "The personalized Pagerank from various nodes to the target will find all the nodes that are close to the target in personalized page rank.",
                    "label": 0
                },
                {
                    "sent": "I don't have time to describe it fully, but the average running time is order D over Delta.",
                    "label": 0
                },
                {
                    "sent": "What is the average degree and Delta is the size of the probability resonating?",
                    "label": 0
                },
                {
                    "sent": "So we see that both pass algorithms took time order one over Delta and Delta is order one over North.",
                    "label": 0
                },
                {
                    "sent": "So both pass algorithms are taking time order 1 / N on large networks.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is our result.",
                    "label": 0
                },
                {
                    "sent": "For any.",
                    "label": 0
                },
                {
                    "sent": "In any graph, any source target pair.",
                    "label": 0
                },
                {
                    "sent": "We probably get small relative error with high probability.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, running time is probably order one over square root, Delta Times Square D for D is the average degree of the graph.",
                    "label": 0
                },
                {
                    "sent": "Looking at this running time.",
                    "label": 0
                },
                {
                    "sent": "Notice that most large graphs are sparse, so typical value of D might be 100 and Square D might be 10.",
                    "label": 0
                },
                {
                    "sent": "But Delta is around one over add, maybe 1 / 100 million.",
                    "label": 0
                },
                {
                    "sent": "So going from one over Delta to square root Delta means going from running time order 100 million to order 10,000.",
                    "label": 0
                },
                {
                    "sent": "Square root is a huge improvement for Delta on large graphs.",
                    "label": 0
                },
                {
                    "sent": "We also prove lower bounds that any algorithm which computes personalized Pagerank must look at one over square Delta nodes in the edges in the graph.",
                    "label": 0
                },
                {
                    "sent": "So our dependence on Delta cannot be improved.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To understand how we get this improvement from one over Delta to one over square root Delta, consider an analogy to bidirectional search.",
                    "label": 0
                },
                {
                    "sent": "Suppose we're finding a path of length L and a graph where every node has degree D. Using breadth first search from the search, we need to expand the to the L nodes.",
                    "label": 0
                },
                {
                    "sent": "But if you do do a bidirectional search of over 2 distance from the targets source analysis from target intersect, we can find a path of length L and time on the order square root G to the L. So the similar improvement from duty altered square root to the L. We do in August thing for personalized Pagerank.",
                    "label": 0
                },
                {
                    "sent": "Only we define halfway, not in terms of path distance, but in terms of personalized page rank.",
                    "label": 0
                },
                {
                    "sent": "Given start S and target T. We start by running the local update algorithm to find all nodes that are closer to an personalized page rank.",
                    "label": 0
                },
                {
                    "sent": "So for all this Gray sets the set of all nodes that have a personalized page rank to the targets bigger than square root Delta.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then it turns out we need to do one step further and find the frontier.",
                    "label": 0
                },
                {
                    "sent": "These are nodes that are not themselves close to the targets.",
                    "label": 0
                },
                {
                    "sent": "By this definition, of course, but there one step away from being close to the targets.",
                    "label": 0
                },
                {
                    "sent": "Once we have this frontier.",
                    "label": 0
                },
                {
                    "sent": "Separating the source from the target, we sample random walks forward from the source node.",
                    "label": 1
                },
                {
                    "sent": "Every random rock.",
                    "label": 0
                },
                {
                    "sent": "Stops providing the frontier.",
                    "label": 0
                },
                {
                    "sent": "It could be 0 to estimate a personalized page rank.",
                    "label": 0
                },
                {
                    "sent": "Every random walk reaches the frontier.",
                    "label": 0
                },
                {
                    "sent": "Say some nodu.",
                    "label": 0
                },
                {
                    "sent": "Then, rather than sampling the rest of the walk, we simulate the rest of the walk.",
                    "label": 0
                },
                {
                    "sent": "And use an estimate of Pi OT that was computed earlier by local updates.",
                    "label": 0
                },
                {
                    "sent": "This is the entire algorithm.",
                    "label": 0
                },
                {
                    "sent": "Twin",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And while this works, notes that every path from the source the target must fit the frontier at some points on the path.",
                    "label": 0
                },
                {
                    "sent": "So we can decompose the probability of going from source to target and the sum over all nodes you in the frontier or the probability that the walk goes from the source to that node.",
                    "label": 0
                },
                {
                    "sent": "You in the Frontier times the probability of going from that node.",
                    "label": 0
                },
                {
                    "sent": "You in the frontier and stopping at the target.",
                    "label": 0
                },
                {
                    "sent": "Because their walks remember the memory lists.",
                    "label": 0
                },
                {
                    "sent": "We can use that property to decompose this.",
                    "label": 0
                },
                {
                    "sent": "The probability of going from source to target.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now describe our experience here and to measure the running time of fast CPR.",
                    "label": 0
                },
                {
                    "sent": "We chose 6 graphs for diversity.",
                    "label": 0
                },
                {
                    "sent": "We have two undirected social networks to directed in social networks.",
                    "label": 0
                },
                {
                    "sent": "Web graph and a collaboration graph and engine size from a few million edges to a few billion edges.",
                    "label": 0
                },
                {
                    "sent": "Also note that all three algorithms have parameters that you trade off accuracy and runtime.",
                    "label": 0
                },
                {
                    "sent": "We want a level playing field so that the parameters for all three algorithms.",
                    "label": 0
                },
                {
                    "sent": "Such that the relative error is about 10% for all three algorithms.",
                    "label": 0
                },
                {
                    "sent": "Author algorithms have some error and we set the empirical error to be the same.",
                    "label": 0
                },
                {
                    "sent": "Then we can fairly measure the running time of these three algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When we do that, this is the result that we get on the X axis, and varying these six diverse graphs.",
                    "label": 0
                },
                {
                    "sent": "In blue I have running time with fast PR for an average source target pair.",
                    "label": 1
                },
                {
                    "sent": "An orange of the running time of local updates and then green of the running time of Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "Notice that the Y axis is a log scale.",
                    "label": 0
                },
                {
                    "sent": "Each horizontal line is a factor of 10.",
                    "label": 0
                },
                {
                    "sent": "Improvement in running time.",
                    "label": 0
                },
                {
                    "sent": "We see that every graph we tried, personal fast CPR was sometimes faster in the state of the art algorithms and the two largest graphs.",
                    "label": 0
                },
                {
                    "sent": "It is 20 times faster.",
                    "label": 0
                },
                {
                    "sent": "Going from more than five minutes to two seconds.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to summarize.",
                    "label": 0
                },
                {
                    "sent": "We presented fast CPR.",
                    "label": 0
                },
                {
                    "sent": "Which estimates personalized page rank 10 times faster than previous algorithms.",
                    "label": 0
                },
                {
                    "sent": "Is probably accurate and probably takes orders.",
                    "label": 0
                },
                {
                    "sent": "Square D over Delta Time will previous algorithms took order one over Delta time?",
                    "label": 0
                },
                {
                    "sent": "I haven't had time to describe it, but if you do storage of water D over squared Delta per node, you can get a worst case running time of one over Delta.",
                    "label": 0
                },
                {
                    "sent": "Future work will be to close the factor of Square D between our lower bound of one over Square Delta and are running time squared over square root Delta.",
                    "label": 0
                },
                {
                    "sent": "And to build a more scalable method which can be used in a personalized search system.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I do.",
                    "label": 0
                }
            ]
        }
    }
}