{
    "id": "yvbwuh3ihggwq6meldd77ogqliiv4wsa",
    "title": "Logical Linked Data Compression",
    "info": {
        "introducer": [
            "Laura Hollink, Centrum Wiskunde & Informatica (CWI)"
        ],
        "author": [
            "Amit Krishna Joshi, Kno.e.sis, Wright State University"
        ],
        "published": "July 8, 2013",
        "recorded": "May 2013",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Big Data"
        ]
    },
    "url": "http://videolectures.net/eswc2013_joshi_data/",
    "segmentation": [
        [
            "Hello everyone, sorry for the hiccup.",
            "And this is an emergency.",
            "Today I'm going to present our logical link data compression.",
            "This work has been done together with Pascal.",
            "Hitzler.",
            "Ann goes down Pascal.",
            "Hillary is my PhD supervisor as well an.",
            "I work at Northeast Center, Wright State University, Ohio, USA."
        ],
        [
            "Here's the brief outline for my presentation today.",
            "First I will start with linked data.",
            "The especially the growth of linked data and the need of RDF compression.",
            "After that, I'll talk briefly about different existing tools for the audio compression followed by the core.",
            "Come on rule based compression algorithm that is the main topic of this issues and followed by the experiments.",
            "Then at the end we'll conclude with the conclusion as well as some future work."
        ],
        [
            "Yes, the link data at like as of 2007.",
            "I'm sure most of you have already seen this, but I just wanted to reach written how LinkedIn has evolved overtime so.",
            "The initial."
        ],
        [
            "As you can see, like how it has grown."
        ],
        [
            "No time here is the same one in 2008."
        ],
        [
            "In 2009"
        ],
        [
            "10 an."
        ],
        [
            "The dramatics and biggest size of Lincoln in 2011.",
            "So what we have seen is there is a large number of interconnections of datasets.",
            "They're like different government, governments publishing link datasets, actualized publishing houses like New York Times as well as different provide private and public companies from putting a lot of.",
            "Structured data in the format so they decide having growing."
        ],
        [
            "So as of this statistiques, you have already known there are more than 300 link datasets as well as the tournament rules that we have are more than 3 billion triples.",
            "So are these datasets.",
            "Most of them have over a million triples.",
            "Very few of them have over a billion triples, and most of them still have like, fewer, likely less than a million triples.",
            "So what we have seen in so far the link data set diagram is this.",
            "A large number of interconnections of various.",
            "Datasets, but what has happened is the size of linger, assist, number of link data, sisters.",
            "Connick has grown aside from that, the size of individual data set is also growing.",
            "For instance, if you take example of deep area is ever growing and if you take example of DBL pay is always growing so similar to the semantic Web dog food also so.",
            "As the size of individual data set gives growing, we need to think about how we can store it in a compact form."
        ],
        [
            "So the next slide we are going to talk briefly about the art of compressions and before I delve deeper into RDF compression."
        ],
        [
            "Let's see a brief example off the simple definition of RDF here as like a subject points to objects with the predicate.",
            "So yes, the example like we have three different subjects.",
            "One is to history.",
            "The arrows represent RDF type and for instance here is 1 is a part of type DP.",
            "Replace as well as schema place as well as our thing Andy Peter Settlement and these RDF data is represented in representing one of these forms.",
            "Variable was RDF, XML or the other ones like total entry in Triple recently Jason also."
        ],
        [
            "So there was a study done recently, like not recently.",
            "It's like a few years ago where they also tried to compare different artist compression utilities that exist till then as of that.",
            "So the universal compression here RG one visit two which we have always used for compressing lot of our data files regularly.",
            "So this is like universal and the other one the office has created was Sgt with distance for header dictionary triple.",
            "It's a binary format for publishing and extending RDF data.",
            "It is very compact where devices are audiograph into several chunks.",
            "An especially used for high compression rates."
        ],
        [
            "So before we go into detail about the dual based compression, I want to take you to understand the idea and the notion behind this approach and why we are exploring this idea further.",
            "Yes, let's if you're creating a data set to store all the individuals who has attended yes dollars 2013.",
            "So I create this data set with saying that X one to XN are the individuals has attended.",
            "Yes dollars 60,013 an.",
            "I can say that these individual has visited France and the other set.",
            "I can say that all of them speaks English right?",
            "So we have this data set test during these number of triples."
        ],
        [
            "So let's say if any cost 1,000,000 then told proposes trillion.",
            "I know 1,000,000 is too big for yesterday, but if you assume N is big here, the total number of triples that we're getting is even bigger with amounts like 3 million."
        ],
        [
            "So how about we have this tree on this rule that says if someone has attended ESC 2013 then he or she has visited France and here she also speaks English, are we allowed to write this rule just based on the data that I presented just before, or will see is right?",
            "So how about this then from the data set that we've created this earlier?",
            "We just put one of those triple that has the predicate has attendant and remove all the triples that has predicate, has visited and speaks.",
            "Alright, so doing So what we're doing is we're adding one simple rule to the existing data set, but then we're deleting 2 million triples, so there we go like we have eliminated more than 60% of the triples, thereby compressing data sets."
        ],
        [
            "So what we just essentially deal was given a big graph."
        ],
        [
            "We created a compressed graph with a set of rules right.",
            "Essentially this we need to make it lossless."
        ],
        [
            "But not all triples will fit into this scenario.",
            "There will be triples which will not be feeling which will not fit into any of the existing rules.",
            "For instance, if I want to add few triples into our previous data set saying that some of the people speak German and Spanish, then they won't fit into that rule, right?",
            "So in that case, what we want to do is."
        ],
        [
            "Initially split the given big graph into two components."
        ],
        [
            "The force is as usual as what we created before the compressed graph with a set of rules so that by applying rules to the triples in that active graph or the compressed graph, we can inform or triples."
        ],
        [
            "And the other one is the domain graph where we store all the triples that are that do not adhere to the rules that we created earlier.",
            "So essentially what we're creating is exceptional triples, and separating it into a different graph called Dahmen graph."
        ],
        [
            "So in dog and what we are.",
            "Depics what we're trying to do address.",
            "So here's geez ordinal graph in the given graph.",
            "So on compression, what we want to achieve is 3 components in the rules are rules set are the active graph, GA the domain graph GT.",
            "And when we try to decompress we apply rules to every triple in the graph.",
            "GA and all those triples whatever has been in for this now will will.",
            "But from the Union we did those triples in Dorman graph.",
            "Then we will get the graph which is essentially the original graph which actually represents the original graph."
        ],
        [
            "So how do we generate these rules?",
            "Like?",
            "I mean you can generals in any way you can generate manually, or you can use various algorithms.",
            "So here I've used one of the."
        ],
        [
            "Matters in frequent middle school frequent itemset mining.",
            "I know here the one that I used was epic growth, but you can use other algorithms so that supply as well.",
            "So the input for different set frequent itemset mining is always the transition database and the minimum support to define like how many times the pattern should occur to make it in frequent."
        ],
        [
            "So in our case we we try to create two different kinds of transactions, the very forces in the property transaction where we've fixed property and then list subject and object for that particular property.",
            "So essentially, if we choose the property as RDF type, then we'll get all the triples that has pretty good RDF type.",
            "Then at least content subject and corresponding objects only.",
            "On the other hand, there is.",
            "A different transition that we have also employed, which we call as Inter property transaction where we create a pair of property, an object for each subject.",
            "Doing this we get the associations among the properties, not just one single property.",
            "So let's."
        ],
        [
            "Get this, here's a list of triples.",
            "Let's say where a represents the RDF type, and.",
            "S1 to S6.",
            "They're just the subjects right?",
            "Numbers like honey to all these numbers are numerically encoded for one of the RDF datasets.",
            "So for if you are looking for Inter property transition, we're basically saying."
        ],
        [
            "There's like will create a transition database like this, where for S1 we have for the four objects with encoded as hunting 2520, two, 25 and 60.",
            "Similarly for SUS five it is of RDF type 22125, so."
        ],
        [
            "Let's look at what we lose the output of the frequent patterns on this one.",
            "So this is the item and the corresponding frequent patterns for it.",
            "So let me discover what these represent."
        ],
        [
            "So the item here represents the frequent patterns corresponding to a particular key call on 89 here, right?"
        ],
        [
            "And items on the right side of these they represent the number of times this pattern has occurred.",
            "So like 60,000 times this pattern has occurred for an 89."
        ],
        [
            "So just to give more example here, if like if."
        ],
        [
            "You're talking about.",
            "The 230 and then 2013 East of RDF type Schema movie, which essentially is of Type D person, essentially of type schema creative work as well as DP work an hour thing right.",
            "So which is true?"
        ],
        [
            "So.",
            "Based on our experiments, we found some of these as different predicates for various devices that we use.",
            "Like for Geonames, the most frequent predicates where for peace angio name Geo alternate name, whereas for.",
            "Jamendo, there's like RDF type and forth made right."
        ],
        [
            "So what is the outcome of this arbec compression using Inter property?",
            "Basically whatever we are going to get is for any triple of this topics PK we will have fixed.",
            "P then will get different objects, so one of the example is.",
            "Triple that has like RX RDF type of four portion essentially means is of type, schema, person as well as DB portion as well as our thing."
        ],
        [
            "And let's go to the Inter property transactions where we represent items as a pair of property and object."
        ],
        [
            "So similar as before, the 1st component represents the patterns item set and the second component represents the total number of times that pattern has occur."
        ],
        [
            "So yes, one.",
            "How is a positive formally and one of the example?"
        ],
        [
            "To look at is like.",
            "This is for particularly Geonames data center, where it says that if the people is of has like a subject idea feature code B first, then it is essentially of type Geo feature and it had.",
            "Jeff is a class of Gov."
        ],
        [
            "In the next couple of slides, we're going to talk about the compressions results that we have performance based on different carriers that we have sent the four."
        ],
        [
            "One is based on compression based on triple counts.",
            "How many triples have we been able to eliminate?",
            "So looking at this example you see that they are the experiment both into property and into property.",
            "It doesn't perform well in terms of injured property because like we're looking at just one specific property predicate an it doesn't perform that well, whereas in the Inter property we're looking across the properties like the one example that I gave before, where whoever it has attended.",
            "Yes, WC speaks English.",
            "That's like across the properties two different properties.",
            "We've been able to.",
            "We've done the experience across the data link data set, as well as the benchmark data set.",
            "Will come to benchmark data set later on.",
            "So the neck."
        ],
        [
            "One is for the compression based on our data set size, not just on the triple number of triples that we have been able to remove.",
            "Just to note that there are the other compression algorithms.",
            "They do not remove the triples.",
            "They basically tried to compress in terms of data size.",
            "That's why we need this to compare our results with the other compression existing compression algorithms.",
            "Here we normally use we apply busy 2 on top of whatever compression we have already done, so the results here come when we say compress.",
            "It basically means that is a normal busy to compression.",
            "So.",
            "Then we have compared our results with the with SDT, which is the state of the art compression for the RDF datasets.",
            "So as you can see that.",
            "It's not always perfect and I saw not always better than editing, but in most cases it does perform better than yesterday.",
            "The reason why some of in some of the data says it does not perform better than a city is because in Sgt it creates header and dictionary, so there's overhead for us in terms of creating header and dictionary for both the active graph as well as determined graph."
        ],
        [
            "So in terms of time it takes for decompression decompression.",
            "You see that for the compression it takes really long time, whereas for decommission is quite fast.",
            "So it's like one time compression.",
            "If you're talking about the number of times it can we compress, it is for local to the number of times we decompress it."
        ],
        [
            "And here's the same result for the basement dissolution benchmark data set.",
            "We created this with C, An index both set to 0.",
            "So as we see that loom is created based on certain auto losses.",
            "So compression ratio is more or less constant over all the values of.",
            "Michael Moore 52 level 1000.",
            "We get more or less the static compression ratio."
        ],
        [
            "And the compression, as expected compression is very high and the time it takes to compress is very high.",
            "Before decompression, the maximum time it took for Luber 1200 seconds.",
            "That's like under 5 minutes."
        ],
        [
            "So this is what we have done so far.",
            "Rule based compression.",
            "We is a lossless compression.",
            "The results that we have often so far are very good and promising so that we need to explore it further.",
            "One of the details that I wanted to mention is we were able to perform Delta compression whenever we have like a set of additional triples.",
            "We could compare it against the existing rules and if it fits into one of those rules we could just get rid of the triple.",
            "And hence we achieve Delta compression.",
            "But like if there is multiple addition of a set of triples, then it's better to perform complete with this compression.",
            "So in terms of the future work, what we're trying to do is still whether we can use this compressed data set to directly query over.",
            "Like if you can perform sparkle directly on this compressed data set and perform only partial decompression on the fly.",
            "And obviously we want to use this technology for the ontology, alignment and schema generation as well."
        ],
        [
            "Thank you very much and questions.",
            "So there's something I didn't quite understand, so if you look at the.",
            "Transactions that you introduce for the Inter properties OK."
        ],
        [
            "So no, no, I mean you.",
            "OK that's fine.",
            "Or actually maybe the other the other.",
            "Yeah, this one here.",
            "So I mean you need to store with each rules.",
            "Also the instances that fulfill.",
            "This Association basically right, so you don't just store the rule, but you need to store the individual the instance that needs that to which we can apply these rules, yes?",
            "So you do that in addition.",
            "OK, that was not totally clear and the other question is how do you deal with?",
            "I mean if your data set is updated.",
            "So if I add triples.",
            "What do I need to run the whole compression again?",
            "Yeah, that was part of the Delta Commission that I spoke at the conclusion aspect.",
            "So if you have a set of new triples that you want to add into the existing compression, you can just use the existing rules and if those set of triples.",
            "Like if you can generate the rules just using the existing rules.",
            "Sorry for the new set of triples that you're going to add into the compression if that ideas to one of the rules already created, then you could just remove those triples and just one instance on interactive graph.",
            "If not, then you will have to put it into the dormant graph and later like if this process happens multiple times, it's better to do a clean rule based compression.",
            "Again, I was looking at your kind of results across the different.",
            "Datasets where you did compressions and do you have some idea about the kind of.",
            "Heterogeneity of like.",
            "Those data sets in terms of.",
            "Their structure is like love 'em, that's just all super consistent, but you have a way of measuring head head head genetti across those datasets."
        ],
        [
            "We haven't done the measure and measurement any in terms of the heterogeneity, but I just see like in this particular example, like if you're talking about RDF type, which is very much very well and like the Commission is very high if you look at the computer issue because we were able to remove more than 50% of the three points so well structured one.",
            "But no, definitely we have in deal with like how they behave in terms of engineering.",
            "I'm wondering whether you actually find rules that are covered by OWL, and whether you're sometimes in some datasets your problem breaks down into removing redundant triples from the data set that actually are materialized from from axioms that are already in the data set.",
            "Yeah, that's a very good point.",
            "Like, I mean, since a lot of devices are created by just neutralizing the existing ontologies, right?",
            "So we could just be getting back some of the ontologies on those items that already.",
            "That we already know, but right now what we're doing is we're not worrying about whether the data set has been created based on ontology or not, is completely data driven.",
            "So for instance, what we can get from our result is like if the person is the President of America, then here is Mail that you can get from the ontology.",
            "That's not the ontology axioms.",
            "So irrespective of how it is created, we just relate to the existing data.",
            "So we don't create.",
            "We don't look for the source of the creation.",
            "The domain graph only contain additional triples as you described it.",
            "Right Domain graph contest those triples to each of rules can be applied at the time of the company.",
            "OK, wouldn't that be possible to have?",
            "Also exceptions to the rules.",
            "So to have some rules that do not apply all the all the time to other triples that match.",
            "And storing the domain graph, negative triples or triples to be removed from what?",
            "The rules can accept that is very possible and that's why we have done till like we call it as extra treatments.",
            "For instance, if we look at that example where we say everyone who has attended ICE doubles on each dollar, C speaks English and has big difference.",
            "Let's say some of them don't speak English right?",
            "But they become the exceptional.",
            "So if there are like let's say 1000 who do not speak English, then our 1 million will remove these 1000 and put it into the German graph.",
            "So that becomes a set of exceptional triples that goes to.",
            "Without applying rule on it later.",
            "How old is this deal with sort of like ongoing compression things, so you get a data set an you compress it, and then you get more data and you want to compress that and that's that.",
            "You've already compressed?",
            "Yeah, that's what we talked about this earlier about the Delta compression like.",
            "OK, if you keep on adding like set of new triples onto the existing data set, then we can apply Delta compression on to it, but if it is a multiple Delta compression then it's better to restart from first.",
            "Like if like 5 six times then the system at first.",
            "Other questions.",
            "Alright, then, let's.",
            "Thank you very much, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everyone, sorry for the hiccup.",
                    "label": 0
                },
                {
                    "sent": "And this is an emergency.",
                    "label": 0
                },
                {
                    "sent": "Today I'm going to present our logical link data compression.",
                    "label": 0
                },
                {
                    "sent": "This work has been done together with Pascal.",
                    "label": 0
                },
                {
                    "sent": "Hitzler.",
                    "label": 0
                },
                {
                    "sent": "Ann goes down Pascal.",
                    "label": 0
                },
                {
                    "sent": "Hillary is my PhD supervisor as well an.",
                    "label": 0
                },
                {
                    "sent": "I work at Northeast Center, Wright State University, Ohio, USA.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the brief outline for my presentation today.",
                    "label": 0
                },
                {
                    "sent": "First I will start with linked data.",
                    "label": 0
                },
                {
                    "sent": "The especially the growth of linked data and the need of RDF compression.",
                    "label": 1
                },
                {
                    "sent": "After that, I'll talk briefly about different existing tools for the audio compression followed by the core.",
                    "label": 0
                },
                {
                    "sent": "Come on rule based compression algorithm that is the main topic of this issues and followed by the experiments.",
                    "label": 0
                },
                {
                    "sent": "Then at the end we'll conclude with the conclusion as well as some future work.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, the link data at like as of 2007.",
                    "label": 0
                },
                {
                    "sent": "I'm sure most of you have already seen this, but I just wanted to reach written how LinkedIn has evolved overtime so.",
                    "label": 0
                },
                {
                    "sent": "The initial.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As you can see, like how it has grown.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No time here is the same one in 2008.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In 2009",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "10 an.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The dramatics and biggest size of Lincoln in 2011.",
                    "label": 0
                },
                {
                    "sent": "So what we have seen is there is a large number of interconnections of datasets.",
                    "label": 0
                },
                {
                    "sent": "They're like different government, governments publishing link datasets, actualized publishing houses like New York Times as well as different provide private and public companies from putting a lot of.",
                    "label": 0
                },
                {
                    "sent": "Structured data in the format so they decide having growing.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as of this statistiques, you have already known there are more than 300 link datasets as well as the tournament rules that we have are more than 3 billion triples.",
                    "label": 0
                },
                {
                    "sent": "So are these datasets.",
                    "label": 0
                },
                {
                    "sent": "Most of them have over a million triples.",
                    "label": 0
                },
                {
                    "sent": "Very few of them have over a billion triples, and most of them still have like, fewer, likely less than a million triples.",
                    "label": 1
                },
                {
                    "sent": "So what we have seen in so far the link data set diagram is this.",
                    "label": 0
                },
                {
                    "sent": "A large number of interconnections of various.",
                    "label": 0
                },
                {
                    "sent": "Datasets, but what has happened is the size of linger, assist, number of link data, sisters.",
                    "label": 0
                },
                {
                    "sent": "Connick has grown aside from that, the size of individual data set is also growing.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you take example of deep area is ever growing and if you take example of DBL pay is always growing so similar to the semantic Web dog food also so.",
                    "label": 0
                },
                {
                    "sent": "As the size of individual data set gives growing, we need to think about how we can store it in a compact form.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the next slide we are going to talk briefly about the art of compressions and before I delve deeper into RDF compression.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see a brief example off the simple definition of RDF here as like a subject points to objects with the predicate.",
                    "label": 0
                },
                {
                    "sent": "So yes, the example like we have three different subjects.",
                    "label": 0
                },
                {
                    "sent": "One is to history.",
                    "label": 0
                },
                {
                    "sent": "The arrows represent RDF type and for instance here is 1 is a part of type DP.",
                    "label": 0
                },
                {
                    "sent": "Replace as well as schema place as well as our thing Andy Peter Settlement and these RDF data is represented in representing one of these forms.",
                    "label": 0
                },
                {
                    "sent": "Variable was RDF, XML or the other ones like total entry in Triple recently Jason also.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there was a study done recently, like not recently.",
                    "label": 0
                },
                {
                    "sent": "It's like a few years ago where they also tried to compare different artist compression utilities that exist till then as of that.",
                    "label": 0
                },
                {
                    "sent": "So the universal compression here RG one visit two which we have always used for compressing lot of our data files regularly.",
                    "label": 0
                },
                {
                    "sent": "So this is like universal and the other one the office has created was Sgt with distance for header dictionary triple.",
                    "label": 0
                },
                {
                    "sent": "It's a binary format for publishing and extending RDF data.",
                    "label": 0
                },
                {
                    "sent": "It is very compact where devices are audiograph into several chunks.",
                    "label": 0
                },
                {
                    "sent": "An especially used for high compression rates.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before we go into detail about the dual based compression, I want to take you to understand the idea and the notion behind this approach and why we are exploring this idea further.",
                    "label": 0
                },
                {
                    "sent": "Yes, let's if you're creating a data set to store all the individuals who has attended yes dollars 2013.",
                    "label": 0
                },
                {
                    "sent": "So I create this data set with saying that X one to XN are the individuals has attended.",
                    "label": 0
                },
                {
                    "sent": "Yes dollars 60,013 an.",
                    "label": 0
                },
                {
                    "sent": "I can say that these individual has visited France and the other set.",
                    "label": 0
                },
                {
                    "sent": "I can say that all of them speaks English right?",
                    "label": 0
                },
                {
                    "sent": "So we have this data set test during these number of triples.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's say if any cost 1,000,000 then told proposes trillion.",
                    "label": 0
                },
                {
                    "sent": "I know 1,000,000 is too big for yesterday, but if you assume N is big here, the total number of triples that we're getting is even bigger with amounts like 3 million.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how about we have this tree on this rule that says if someone has attended ESC 2013 then he or she has visited France and here she also speaks English, are we allowed to write this rule just based on the data that I presented just before, or will see is right?",
                    "label": 0
                },
                {
                    "sent": "So how about this then from the data set that we've created this earlier?",
                    "label": 0
                },
                {
                    "sent": "We just put one of those triple that has the predicate has attendant and remove all the triples that has predicate, has visited and speaks.",
                    "label": 0
                },
                {
                    "sent": "Alright, so doing So what we're doing is we're adding one simple rule to the existing data set, but then we're deleting 2 million triples, so there we go like we have eliminated more than 60% of the triples, thereby compressing data sets.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we just essentially deal was given a big graph.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We created a compressed graph with a set of rules right.",
                    "label": 0
                },
                {
                    "sent": "Essentially this we need to make it lossless.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But not all triples will fit into this scenario.",
                    "label": 0
                },
                {
                    "sent": "There will be triples which will not be feeling which will not fit into any of the existing rules.",
                    "label": 0
                },
                {
                    "sent": "For instance, if I want to add few triples into our previous data set saying that some of the people speak German and Spanish, then they won't fit into that rule, right?",
                    "label": 0
                },
                {
                    "sent": "So in that case, what we want to do is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Initially split the given big graph into two components.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The force is as usual as what we created before the compressed graph with a set of rules so that by applying rules to the triples in that active graph or the compressed graph, we can inform or triples.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the other one is the domain graph where we store all the triples that are that do not adhere to the rules that we created earlier.",
                    "label": 0
                },
                {
                    "sent": "So essentially what we're creating is exceptional triples, and separating it into a different graph called Dahmen graph.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in dog and what we are.",
                    "label": 0
                },
                {
                    "sent": "Depics what we're trying to do address.",
                    "label": 0
                },
                {
                    "sent": "So here's geez ordinal graph in the given graph.",
                    "label": 0
                },
                {
                    "sent": "So on compression, what we want to achieve is 3 components in the rules are rules set are the active graph, GA the domain graph GT.",
                    "label": 1
                },
                {
                    "sent": "And when we try to decompress we apply rules to every triple in the graph.",
                    "label": 0
                },
                {
                    "sent": "GA and all those triples whatever has been in for this now will will.",
                    "label": 0
                },
                {
                    "sent": "But from the Union we did those triples in Dorman graph.",
                    "label": 0
                },
                {
                    "sent": "Then we will get the graph which is essentially the original graph which actually represents the original graph.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we generate these rules?",
                    "label": 0
                },
                {
                    "sent": "Like?",
                    "label": 0
                },
                {
                    "sent": "I mean you can generals in any way you can generate manually, or you can use various algorithms.",
                    "label": 0
                },
                {
                    "sent": "So here I've used one of the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matters in frequent middle school frequent itemset mining.",
                    "label": 0
                },
                {
                    "sent": "I know here the one that I used was epic growth, but you can use other algorithms so that supply as well.",
                    "label": 0
                },
                {
                    "sent": "So the input for different set frequent itemset mining is always the transition database and the minimum support to define like how many times the pattern should occur to make it in frequent.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in our case we we try to create two different kinds of transactions, the very forces in the property transaction where we've fixed property and then list subject and object for that particular property.",
                    "label": 0
                },
                {
                    "sent": "So essentially, if we choose the property as RDF type, then we'll get all the triples that has pretty good RDF type.",
                    "label": 0
                },
                {
                    "sent": "Then at least content subject and corresponding objects only.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, there is.",
                    "label": 0
                },
                {
                    "sent": "A different transition that we have also employed, which we call as Inter property transaction where we create a pair of property, an object for each subject.",
                    "label": 0
                },
                {
                    "sent": "Doing this we get the associations among the properties, not just one single property.",
                    "label": 0
                },
                {
                    "sent": "So let's.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get this, here's a list of triples.",
                    "label": 0
                },
                {
                    "sent": "Let's say where a represents the RDF type, and.",
                    "label": 0
                },
                {
                    "sent": "S1 to S6.",
                    "label": 0
                },
                {
                    "sent": "They're just the subjects right?",
                    "label": 0
                },
                {
                    "sent": "Numbers like honey to all these numbers are numerically encoded for one of the RDF datasets.",
                    "label": 0
                },
                {
                    "sent": "So for if you are looking for Inter property transition, we're basically saying.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's like will create a transition database like this, where for S1 we have for the four objects with encoded as hunting 2520, two, 25 and 60.",
                    "label": 0
                },
                {
                    "sent": "Similarly for SUS five it is of RDF type 22125, so.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at what we lose the output of the frequent patterns on this one.",
                    "label": 0
                },
                {
                    "sent": "So this is the item and the corresponding frequent patterns for it.",
                    "label": 0
                },
                {
                    "sent": "So let me discover what these represent.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the item here represents the frequent patterns corresponding to a particular key call on 89 here, right?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And items on the right side of these they represent the number of times this pattern has occurred.",
                    "label": 0
                },
                {
                    "sent": "So like 60,000 times this pattern has occurred for an 89.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to give more example here, if like if.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're talking about.",
                    "label": 0
                },
                {
                    "sent": "The 230 and then 2013 East of RDF type Schema movie, which essentially is of Type D person, essentially of type schema creative work as well as DP work an hour thing right.",
                    "label": 0
                },
                {
                    "sent": "So which is true?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Based on our experiments, we found some of these as different predicates for various devices that we use.",
                    "label": 0
                },
                {
                    "sent": "Like for Geonames, the most frequent predicates where for peace angio name Geo alternate name, whereas for.",
                    "label": 0
                },
                {
                    "sent": "Jamendo, there's like RDF type and forth made right.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is the outcome of this arbec compression using Inter property?",
                    "label": 0
                },
                {
                    "sent": "Basically whatever we are going to get is for any triple of this topics PK we will have fixed.",
                    "label": 0
                },
                {
                    "sent": "P then will get different objects, so one of the example is.",
                    "label": 0
                },
                {
                    "sent": "Triple that has like RX RDF type of four portion essentially means is of type, schema, person as well as DB portion as well as our thing.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let's go to the Inter property transactions where we represent items as a pair of property and object.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So similar as before, the 1st component represents the patterns item set and the second component represents the total number of times that pattern has occur.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yes, one.",
                    "label": 0
                },
                {
                    "sent": "How is a positive formally and one of the example?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To look at is like.",
                    "label": 0
                },
                {
                    "sent": "This is for particularly Geonames data center, where it says that if the people is of has like a subject idea feature code B first, then it is essentially of type Geo feature and it had.",
                    "label": 0
                },
                {
                    "sent": "Jeff is a class of Gov.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the next couple of slides, we're going to talk about the compressions results that we have performance based on different carriers that we have sent the four.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One is based on compression based on triple counts.",
                    "label": 0
                },
                {
                    "sent": "How many triples have we been able to eliminate?",
                    "label": 0
                },
                {
                    "sent": "So looking at this example you see that they are the experiment both into property and into property.",
                    "label": 0
                },
                {
                    "sent": "It doesn't perform well in terms of injured property because like we're looking at just one specific property predicate an it doesn't perform that well, whereas in the Inter property we're looking across the properties like the one example that I gave before, where whoever it has attended.",
                    "label": 0
                },
                {
                    "sent": "Yes, WC speaks English.",
                    "label": 0
                },
                {
                    "sent": "That's like across the properties two different properties.",
                    "label": 0
                },
                {
                    "sent": "We've been able to.",
                    "label": 0
                },
                {
                    "sent": "We've done the experience across the data link data set, as well as the benchmark data set.",
                    "label": 0
                },
                {
                    "sent": "Will come to benchmark data set later on.",
                    "label": 0
                },
                {
                    "sent": "So the neck.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One is for the compression based on our data set size, not just on the triple number of triples that we have been able to remove.",
                    "label": 0
                },
                {
                    "sent": "Just to note that there are the other compression algorithms.",
                    "label": 0
                },
                {
                    "sent": "They do not remove the triples.",
                    "label": 0
                },
                {
                    "sent": "They basically tried to compress in terms of data size.",
                    "label": 0
                },
                {
                    "sent": "That's why we need this to compare our results with the other compression existing compression algorithms.",
                    "label": 0
                },
                {
                    "sent": "Here we normally use we apply busy 2 on top of whatever compression we have already done, so the results here come when we say compress.",
                    "label": 0
                },
                {
                    "sent": "It basically means that is a normal busy to compression.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Then we have compared our results with the with SDT, which is the state of the art compression for the RDF datasets.",
                    "label": 0
                },
                {
                    "sent": "So as you can see that.",
                    "label": 0
                },
                {
                    "sent": "It's not always perfect and I saw not always better than editing, but in most cases it does perform better than yesterday.",
                    "label": 0
                },
                {
                    "sent": "The reason why some of in some of the data says it does not perform better than a city is because in Sgt it creates header and dictionary, so there's overhead for us in terms of creating header and dictionary for both the active graph as well as determined graph.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in terms of time it takes for decompression decompression.",
                    "label": 0
                },
                {
                    "sent": "You see that for the compression it takes really long time, whereas for decommission is quite fast.",
                    "label": 0
                },
                {
                    "sent": "So it's like one time compression.",
                    "label": 0
                },
                {
                    "sent": "If you're talking about the number of times it can we compress, it is for local to the number of times we decompress it.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's the same result for the basement dissolution benchmark data set.",
                    "label": 0
                },
                {
                    "sent": "We created this with C, An index both set to 0.",
                    "label": 0
                },
                {
                    "sent": "So as we see that loom is created based on certain auto losses.",
                    "label": 0
                },
                {
                    "sent": "So compression ratio is more or less constant over all the values of.",
                    "label": 0
                },
                {
                    "sent": "Michael Moore 52 level 1000.",
                    "label": 0
                },
                {
                    "sent": "We get more or less the static compression ratio.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the compression, as expected compression is very high and the time it takes to compress is very high.",
                    "label": 0
                },
                {
                    "sent": "Before decompression, the maximum time it took for Luber 1200 seconds.",
                    "label": 0
                },
                {
                    "sent": "That's like under 5 minutes.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is what we have done so far.",
                    "label": 0
                },
                {
                    "sent": "Rule based compression.",
                    "label": 0
                },
                {
                    "sent": "We is a lossless compression.",
                    "label": 0
                },
                {
                    "sent": "The results that we have often so far are very good and promising so that we need to explore it further.",
                    "label": 0
                },
                {
                    "sent": "One of the details that I wanted to mention is we were able to perform Delta compression whenever we have like a set of additional triples.",
                    "label": 0
                },
                {
                    "sent": "We could compare it against the existing rules and if it fits into one of those rules we could just get rid of the triple.",
                    "label": 0
                },
                {
                    "sent": "And hence we achieve Delta compression.",
                    "label": 0
                },
                {
                    "sent": "But like if there is multiple addition of a set of triples, then it's better to perform complete with this compression.",
                    "label": 0
                },
                {
                    "sent": "So in terms of the future work, what we're trying to do is still whether we can use this compressed data set to directly query over.",
                    "label": 0
                },
                {
                    "sent": "Like if you can perform sparkle directly on this compressed data set and perform only partial decompression on the fly.",
                    "label": 0
                },
                {
                    "sent": "And obviously we want to use this technology for the ontology, alignment and schema generation as well.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much and questions.",
                    "label": 0
                },
                {
                    "sent": "So there's something I didn't quite understand, so if you look at the.",
                    "label": 0
                },
                {
                    "sent": "Transactions that you introduce for the Inter properties OK.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So no, no, I mean you.",
                    "label": 0
                },
                {
                    "sent": "OK that's fine.",
                    "label": 0
                },
                {
                    "sent": "Or actually maybe the other the other.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this one here.",
                    "label": 0
                },
                {
                    "sent": "So I mean you need to store with each rules.",
                    "label": 0
                },
                {
                    "sent": "Also the instances that fulfill.",
                    "label": 0
                },
                {
                    "sent": "This Association basically right, so you don't just store the rule, but you need to store the individual the instance that needs that to which we can apply these rules, yes?",
                    "label": 0
                },
                {
                    "sent": "So you do that in addition.",
                    "label": 0
                },
                {
                    "sent": "OK, that was not totally clear and the other question is how do you deal with?",
                    "label": 0
                },
                {
                    "sent": "I mean if your data set is updated.",
                    "label": 0
                },
                {
                    "sent": "So if I add triples.",
                    "label": 0
                },
                {
                    "sent": "What do I need to run the whole compression again?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that was part of the Delta Commission that I spoke at the conclusion aspect.",
                    "label": 0
                },
                {
                    "sent": "So if you have a set of new triples that you want to add into the existing compression, you can just use the existing rules and if those set of triples.",
                    "label": 0
                },
                {
                    "sent": "Like if you can generate the rules just using the existing rules.",
                    "label": 0
                },
                {
                    "sent": "Sorry for the new set of triples that you're going to add into the compression if that ideas to one of the rules already created, then you could just remove those triples and just one instance on interactive graph.",
                    "label": 0
                },
                {
                    "sent": "If not, then you will have to put it into the dormant graph and later like if this process happens multiple times, it's better to do a clean rule based compression.",
                    "label": 0
                },
                {
                    "sent": "Again, I was looking at your kind of results across the different.",
                    "label": 0
                },
                {
                    "sent": "Datasets where you did compressions and do you have some idea about the kind of.",
                    "label": 0
                },
                {
                    "sent": "Heterogeneity of like.",
                    "label": 0
                },
                {
                    "sent": "Those data sets in terms of.",
                    "label": 0
                },
                {
                    "sent": "Their structure is like love 'em, that's just all super consistent, but you have a way of measuring head head head genetti across those datasets.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We haven't done the measure and measurement any in terms of the heterogeneity, but I just see like in this particular example, like if you're talking about RDF type, which is very much very well and like the Commission is very high if you look at the computer issue because we were able to remove more than 50% of the three points so well structured one.",
                    "label": 0
                },
                {
                    "sent": "But no, definitely we have in deal with like how they behave in terms of engineering.",
                    "label": 0
                },
                {
                    "sent": "I'm wondering whether you actually find rules that are covered by OWL, and whether you're sometimes in some datasets your problem breaks down into removing redundant triples from the data set that actually are materialized from from axioms that are already in the data set.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a very good point.",
                    "label": 0
                },
                {
                    "sent": "Like, I mean, since a lot of devices are created by just neutralizing the existing ontologies, right?",
                    "label": 0
                },
                {
                    "sent": "So we could just be getting back some of the ontologies on those items that already.",
                    "label": 0
                },
                {
                    "sent": "That we already know, but right now what we're doing is we're not worrying about whether the data set has been created based on ontology or not, is completely data driven.",
                    "label": 0
                },
                {
                    "sent": "So for instance, what we can get from our result is like if the person is the President of America, then here is Mail that you can get from the ontology.",
                    "label": 0
                },
                {
                    "sent": "That's not the ontology axioms.",
                    "label": 0
                },
                {
                    "sent": "So irrespective of how it is created, we just relate to the existing data.",
                    "label": 0
                },
                {
                    "sent": "So we don't create.",
                    "label": 0
                },
                {
                    "sent": "We don't look for the source of the creation.",
                    "label": 0
                },
                {
                    "sent": "The domain graph only contain additional triples as you described it.",
                    "label": 0
                },
                {
                    "sent": "Right Domain graph contest those triples to each of rules can be applied at the time of the company.",
                    "label": 0
                },
                {
                    "sent": "OK, wouldn't that be possible to have?",
                    "label": 0
                },
                {
                    "sent": "Also exceptions to the rules.",
                    "label": 0
                },
                {
                    "sent": "So to have some rules that do not apply all the all the time to other triples that match.",
                    "label": 0
                },
                {
                    "sent": "And storing the domain graph, negative triples or triples to be removed from what?",
                    "label": 0
                },
                {
                    "sent": "The rules can accept that is very possible and that's why we have done till like we call it as extra treatments.",
                    "label": 0
                },
                {
                    "sent": "For instance, if we look at that example where we say everyone who has attended ICE doubles on each dollar, C speaks English and has big difference.",
                    "label": 0
                },
                {
                    "sent": "Let's say some of them don't speak English right?",
                    "label": 0
                },
                {
                    "sent": "But they become the exceptional.",
                    "label": 0
                },
                {
                    "sent": "So if there are like let's say 1000 who do not speak English, then our 1 million will remove these 1000 and put it into the German graph.",
                    "label": 0
                },
                {
                    "sent": "So that becomes a set of exceptional triples that goes to.",
                    "label": 0
                },
                {
                    "sent": "Without applying rule on it later.",
                    "label": 0
                },
                {
                    "sent": "How old is this deal with sort of like ongoing compression things, so you get a data set an you compress it, and then you get more data and you want to compress that and that's that.",
                    "label": 0
                },
                {
                    "sent": "You've already compressed?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's what we talked about this earlier about the Delta compression like.",
                    "label": 0
                },
                {
                    "sent": "OK, if you keep on adding like set of new triples onto the existing data set, then we can apply Delta compression on to it, but if it is a multiple Delta compression then it's better to restart from first.",
                    "label": 0
                },
                {
                    "sent": "Like if like 5 six times then the system at first.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Alright, then, let's.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much, thank you.",
                    "label": 0
                }
            ]
        }
    }
}