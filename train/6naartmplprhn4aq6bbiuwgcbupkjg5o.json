{
    "id": "6naartmplprhn4aq6bbiuwgcbupkjg5o",
    "title": "EARL: Joint Entity and Relation Linking for Question Answering over Knowledge Graphs",
    "info": {
        "author": [
            "Mohnish Dubey, Department of Computer Science, Bonn-Rhine-Sieg University of Applied Sciences"
        ],
        "published": "Nov. 22, 2018",
        "recorded": "October 2018",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2018_dubey_earl_joint_entity/",
    "segmentation": [
        [
            "Hello everyone so I would be presenting my work on of URL which is for joint relation an entity linking for our knowledge graph for specially specifically for questions.",
            "So."
        ],
        [
            "So this is an outline, so we will be just discussing about question answering entity linking.",
            "Then we will have just the overview of the architecture.",
            "Then we have like 2 approaches.",
            "We will go through them then compare the two approaches and then the evaluation and then some conclusions."
        ],
        [
            "So this is a knowledge graph we do question."
        ],
        [
            "Dancing over it because we just try to translate natural language questions to some kind of a formal language.",
            "Mostly we do it for sparkle."
        ],
        [
            "So question answering could be of like two categories could like vaguely divided into two categories.",
            "One could be like more rule based.",
            "One could be like more machine learning based."
        ],
        [
            "So this is how mostly a question answering pipeline looks like, so you do some entity recognition.",
            "Then you do some relation recognition.",
            "Then you have some kind of question understanding.",
            "Then you basically make entity linking relation linking and then with all this knowledge you try to make up a sparkle and then you finally get an output for your question which is the answer."
        ],
        [
            "So the key components in a QA pipeline could be entity identification and and linking relation identification, linking an query building.",
            "So URL is about the first 2."
        ],
        [
            "Degrees, which is like entity identification and linking and relation identification and linking."
        ],
        [
            "So I'll just go through how entity linking most popularly has been done.",
            "So first you do you have a text.",
            "You do mention detection, then you."
        ],
        [
            "To generate candidate for all your detected keywords and then you."
        ],
        [
            "I to do disambiguation by looking in the Knowledge Graph, or maybe looking the connections how they are in the Knowledge Graph or what's their popularity or some some other para meters?"
        ],
        [
            "But this works good when you have like multiple entities, but let's say in in case of questions, it's mostly the single sentences.",
            "And they're mostly with just one entities and you don't have a lot of previous context, which is basically, these are the three things that are used in.",
            "In Standard entity linking tool.",
            "But this is something which is missing when you're doing it for questions.",
            "So disambiguation is a problem here.",
            "So that's what we deal here."
        ],
        [
            "Also, for just showing how relation linking has been done in pass so there could be like string similarity, you could use some kind of a dictionary.",
            "You could use word net.",
            "You could introduce word embeddings or you could also like recently machine learning approaches has also tried to do relation linking.",
            "So basically relation linking would be let's say in your questions you could have phrases such as like written by wrote author or screenwriter, but indeed the ordinary or knowledge graph.",
            "They could be just connected to one single relation.",
            "So basically one relation is expressed in any multiple natural language phrases."
        ],
        [
            "So traditionally we have like so we did a bit of literature survey and then we identified that the entity linking and the relation linking in question answering system could be either than sequentially.",
            "So you basically do either.",
            "Let's say mostly you do relational entity linking 1st and then you do any relation linking.",
            "This process could also be done parallelly, so you are doing your not looking how the entity linking is done when you're doing deletion linking or vice versa.",
            "But then we also identified that this if maybe this process could also be done jointly.",
            "So when you're doing relation linking you are looking also the candidates of entity linking when you're doing entity linking.",
            "You're also looking the candidates of your relation linking.",
            "So they have their own pros and cons.",
            "So mostly in joint linking, you're the potentially you would have a better accuracy.",
            "It's just the complexity becomes much more higher.",
            "So this is so.",
            "Now this is the problem would be how to deal with that complexity."
        ],
        [
            "So as a prelim, what we do is we treat the Knowledge graph as a subdivision graph.",
            "So basically in a knowledge graph, your edge, your relations are always on edges.",
            "But in our case we are treating relations also as nodes of knowledge graph.",
            "So the Knowledge Graph actually expands in terms of nodes in.",
            "In this scenario."
        ],
        [
            "So these are our hypothesis.",
            "So the first hypothesis is that like when you will be trying to identify these candidates like the correct candidates of each key words in a question, you will end up getting a minimum cost cycle.",
            "Because in the Knowledge graph they would be obviously connected, so that's our first hypothesis.",
            "The second hypothesis is that these these clothes these would have like a short hop connections, an also the density of these connections between all of the candidates would be high.",
            "And the third hypothesis is here we have is that while doing relation linking and interlinking together, we would eventually get better results."
        ],
        [
            "So this is the general architecture of urn.",
            "I'll go through it, so I'll."
        ],
        [
            "First go through the preprocessing stages.",
            "So first what we do is once you have it."
        ],
        [
            "I have a question.",
            "We try to extract the main keywords and then remove some stop words.",
            "So here in this case, so this is 1 standard example that we will be going through again and again in our slides.",
            "So where was the founder of Tesla and SpaceX born?",
            "So here I've highlighted that these are the keywords identified."
        ],
        [
            "And then what we try to do is once we get this keyword, we try to.",
            "We have a, uh, entity relation predictor.",
            "So given a keyword, it tries to predict whether this is an entity or whether this is an relation so.",
            "Standard lead this could also be done within any error, but we just trained and LCM over the entire Wiki DB Pedia label corpora.",
            "So basically all the labels of wiki data.",
            "An further extended labels that we had, which I'll come in awhile were there for which helped us to classify whether a keyword is an entity or a relation."
        ],
        [
            "So once we identify a keyword, we try to extract a list of candidates for that keyword.",
            "So for example."
        ],
        [
            "For example, let's say for Founder you will search in the elastic search and try to identify what are the keywords that water the closer keywords to.",
            "What are the closer relation to this label?"
        ],
        [
            "So we also expanded the standard labels in DB pedia, so we use wiki data because Wiki data had more labels so we expanded that.",
            "We also use Oxford Dictionary API to expand the labels for relation and then we also use fast X to further get let's say let's say we fixed up our threshold an fetch all the closer words do that.",
            "Let's say the label was author and then we fixed up threshold of .9 of cosine similarity and then fetch.",
            "All the words which are like more closer to let's say the word author.",
            "And then put all of them in Elasticsearch.",
            "So, so now I will."
        ],
        [
            "Go through to the second so we actually have two options.",
            "If you see here we have actually two options in our pipeline, so one is the GSP solver and one is the connection density solver that we have.",
            "So I will go through both of them now.",
            "So GSP"
        ],
        [
            "So now we formulate this problem of disambiguation as a GT SP.",
            "So just that this generalized traveling salesman problem.",
            "So let's say we have this question.",
            "Who is the husband of the leader of Germany?",
            "So.",
            "We have basically three functions.",
            "So basically first is the spot function.",
            "So you basically spot the keywords here and then you generate the candidates for all of them.",
            "So you basically get a multiple candidates for each."
        ],
        [
            "Keyword spotted by candidate here.",
            "I mean EU arise.",
            "And then there is a cost function assigned to them about basically about the distance that they have in the Knowledge Graph."
        ],
        [
            "Basically, in our subdivision graph.",
            "So this is how we will we formulate our problem over GSP.",
            "So GSP is to find a subset V Dash where we want to be end of the contents exactly 1 node from each cluster.",
            "So from each so.",
            "I'll so from this we will have a cluster.",
            "If.",
            "Yeah, so this becomes a cluster.",
            "This becomes a cluster and this becomes a cluster.",
            "Anne.",
            "Then we try to pick one element from each cluster and try to find the cost and so so basically find a cycle an find the the bit with the least cost."
        ],
        [
            "So we come back to our example.",
            "So where was the founder of Tesla and SpaceX born?",
            "I hope that that's visible.",
            "So if you see here, we have shown the cluster for each keyword and then these are the top candidates for each of them and the darker line shows the candidates which actually win in the end.",
            "So if you see."
        ],
        [
            "Each one of them have different costs assigned to them and then with the least cost the cycle wins."
        ],
        [
            "But doing so but doing exactly solving GSP could be NP hard.",
            "It's not good beats, it is NP hard, so GS can be reduced to a TCS problem and then we could use an elk H algorithm which is like the state of the art for GT SP approximate solvers.",
            "Because otherwise the time complexity of a normal USB solver is is too high."
        ],
        [
            "So we still have some drawbacks in this approach.",
            "Is that it only provides you the best candidate for each keyword an.",
            "So the best could be from the algorithm, but could not be the best in for the question.",
            "And then it doesn't provide you kind of a list which would be eventually required when you will be.",
            "So the the list is not re ranked, which could be useful when you will be, let's say, building up your sparkle query.",
            "So basically in question answering, we have seen that when when you make the final sparkle query you try to get you try to use the list as like try to see maybe the second candidate is a better candidate with the first candidate but in GT SP this is something which is not possible to do."
        ],
        [
            "So so now we come with come up with a different solution for this.",
            "So we basically named this it as connection density.",
            "So."
        ],
        [
            "I will not go through connection density, so connection density actually consists of three feature."
        ],
        [
            "Yes.",
            "So initially the is the RI, which is basically the initial rank that you extract out, which you exactly get from the Elasticsearch, which just consists of maybe string similarity and some kind of semantic similarity also.",
            "Of like of the list and the keyword."
        ],
        [
            "Then you have connection count, so it is the number of candidates in another list divided by the total number of N keywords spotted."
        ],
        [
            "So let's say if you have this."
        ],
        [
            "And then.",
            "Hop Count is the sum of the distance from C to the other candidates.",
            "In all the other lists is divided by the total number of keywords spotted.",
            "So once we get all these three features, then we try to re rank."
        ],
        [
            "The list, which is the initial list an.",
            "And that is basically the final list.",
            "That would be the RF here, so we try to we use a class."
        ],
        [
            "The fire to re rank this.",
            "So we."
        ],
        [
            "Basically used exhibits for this we tried with some others also, but for keeping the talk short, I'm not going for that in detail.",
            "So once we get this so this basically ranks our list."
        ],
        [
            "So it is something like this so you.",
            "You have your.",
            "You get all your lists here and then.",
            "Basically the connection count, the hop count and the connection count are fetched from this.",
            "This this box I would say and then you give all the three features here.",
            "I'll come to this red thing in a minute, and then you just basically ask it to rerank the list.",
            "Then we have this red thing which is called the adaptive here learning because what we know tist while evaluating a system was.",
            "That most of the errors that we were getting were becausw because at the ear predictor there were some errors.",
            "So let's say this."
        ],
        [
            "So let's say here the North America has been identified as a property in DB pedia so.",
            "It was so it would eventually give you a very low confidence score from the exit boost also.",
            "So what we did was.",
            "We started sending adaptive signals back to the ear predictor module, so whenever after the RE ranking we find that OK like this has a hello are hello are confidence score.",
            "So just just maybe flip it from entity to relation and then we check is the scores are improving or not.",
            "So this basically improves the performance of the whole system by by significance."
        ],
        [
            "So now in compare the both both the approaches so GSP and connection density.",
            "So the good thing in GSP approaches that you do not require any kind of training data.",
            "So this could be really useful for let's say some industrial application where you do not have a lot of training data but you just have a knowledge graph and the client comes to you and say just I want to.",
            "I want to I want to question answering system or an entity linking system over it so this helps in that way.",
            "The advantage to connection density is that.",
            "It gives you an ordered list, so the order list could be would be much more useful when you will be making sparkle queries so that this is one of the this is.",
            "This is the basically the two advantages, so advantage to each of them.",
            "And then I will come to the and also I have also mentioned the time complexity of both of them."
        ],
        [
            "So I'll just look back the postulate again.",
            "OK, so sorry, there's time running, so I'll be bit more fast.",
            "So here are the three postulates an, so I'll just go through the contributions."
        ],
        [
            "So we took else code data set which was published in last year I SWC which has 5000 question.",
            "We annotated the entire data set with with keywords saying whether this is the for this keyword in the question.",
            "This is the UI.",
            "This data is also available.",
            "We also expanded the DB Pedia entity and relation so this data is also available.",
            "As a contribution in the paper an.",
            "Then the experiment one is for hypothesis one and two, and so on.",
            "As mentioned the slide.",
            "So basically we also calc."
        ],
        [
            "Rated how the efficiency of entity linking is affected when we are trying to do with different approaches, so the brute force.",
            "DSP gives us a really good entity linking.",
            "Result accuracy.",
            "The LKH drop slightly, but then the complexity is significantly less than the connection density has a slightly more complex than the LKH but, but the results are equal to the brute force GSP problem.",
            "This is tested on else called data set."
        ],
        [
            "So the experiment two was basically that we removed the section.",
            "Let's say we were actually giving the correct label only, so so this is from the gold standard data set.",
            "So you have the correct label so that you are not losing anything while doing the keyword detection.",
            "An if you are also providing the correct UI in the list, let's see if it is.",
            "It was not fetched and you put it in the last of the list, then then we see that the accuracy could even go to something like .9.",
            "So basically we so in so."
        ],
        [
            "But for the entity linking, what we tested was that we eventually we're getting numbers, something like .65 and .5 seven and two different datasets, so we can hear see that there is a significant drop in performance just because of the keyword detection.",
            "So that is something to look forward in future."
        ],
        [
            "And for relation linking, these are the numbers.",
            "It's like .47."
        ],
        [
            "OK, so yeah, so I'll just wind up my talk by saying that."
        ],
        [
            "These are the two approaches and.",
            "There are some limitations to all that it cannot do something like hidden relation detection.",
            "If the question has a hidden relation, then it cannot detect.",
            "Yeah, and these we have the two approaches as I mentioned again and again."
        ],
        [
            "Yeah, so, and these are the references.",
            "And thank you and."
        ],
        [
            "Looking for a question.",
            "Time for a couple of questions.",
            "So can I start with the question so you mentioned using DV Pedia and a couple of other sources?",
            "Did you find out that some of them are more useful for certain links than others?",
            "Or did you try to find out when certain ones can be used?",
            "No, I think we didn't explore this, but what we did was let's say for Barrack Obama.",
            "There is already DPD already mentions same as link in wiki data so we just fetched all the labels related to Barack Obama in wiki data and just put them in the Elasticsearch.",
            "And for relations we did it with first expanding the label with the dictionary and from all those labels.",
            "Then we further fetched more label from fast text.",
            "Further questions.",
            "No one more.",
            "Could you use the microphone just so where is the microphone?",
            "This one here notes that.",
            "I don't trust.",
            "So my question is more like.",
            "How far away is production quality of this relation extraction?",
            "Let's say what's the problem?",
            "You see, what companies would be willing to take for their, let's say.",
            "Operations like so if you are too low, I mean you don't trust like Harris doesn't trust me right?",
            "But let's say where would be the levels where the companies would take over and say, well this is good.",
            "So are you talking specifically about this paper or you're talking in general terms of question answering.",
            "I was thinking about both I guess.",
            "OK, so since I also work at front over so I can, I don't know how much I can say, but I can't.",
            "You can be alone.",
            "OK, so yeah we have seen in near so we get a lot of.",
            "What should I say?",
            "We get a lot of offers to do question answering over let's say some some data and we have been doing it quite good up till now, so I think it's somewhat production ready right now, I would say.",
            "You can grill him afterwards.",
            "Yeah, of course it was not really answer, but well, that's why we can talk later.",
            "Maybe I misunderstood something.",
            "Someone I can come up and discuss.",
            "We can discuss.",
            "Yeah exactly yeah OK well switch.",
            "Thank you very much again another clap."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everyone so I would be presenting my work on of URL which is for joint relation an entity linking for our knowledge graph for specially specifically for questions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is an outline, so we will be just discussing about question answering entity linking.",
                    "label": 1
                },
                {
                    "sent": "Then we will have just the overview of the architecture.",
                    "label": 0
                },
                {
                    "sent": "Then we have like 2 approaches.",
                    "label": 0
                },
                {
                    "sent": "We will go through them then compare the two approaches and then the evaluation and then some conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a knowledge graph we do question.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dancing over it because we just try to translate natural language questions to some kind of a formal language.",
                    "label": 0
                },
                {
                    "sent": "Mostly we do it for sparkle.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So question answering could be of like two categories could like vaguely divided into two categories.",
                    "label": 0
                },
                {
                    "sent": "One could be like more rule based.",
                    "label": 1
                },
                {
                    "sent": "One could be like more machine learning based.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is how mostly a question answering pipeline looks like, so you do some entity recognition.",
                    "label": 0
                },
                {
                    "sent": "Then you do some relation recognition.",
                    "label": 0
                },
                {
                    "sent": "Then you have some kind of question understanding.",
                    "label": 1
                },
                {
                    "sent": "Then you basically make entity linking relation linking and then with all this knowledge you try to make up a sparkle and then you finally get an output for your question which is the answer.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the key components in a QA pipeline could be entity identification and and linking relation identification, linking an query building.",
                    "label": 0
                },
                {
                    "sent": "So URL is about the first 2.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Degrees, which is like entity identification and linking and relation identification and linking.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll just go through how entity linking most popularly has been done.",
                    "label": 1
                },
                {
                    "sent": "So first you do you have a text.",
                    "label": 0
                },
                {
                    "sent": "You do mention detection, then you.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To generate candidate for all your detected keywords and then you.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I to do disambiguation by looking in the Knowledge Graph, or maybe looking the connections how they are in the Knowledge Graph or what's their popularity or some some other para meters?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this works good when you have like multiple entities, but let's say in in case of questions, it's mostly the single sentences.",
                    "label": 0
                },
                {
                    "sent": "And they're mostly with just one entities and you don't have a lot of previous context, which is basically, these are the three things that are used in.",
                    "label": 0
                },
                {
                    "sent": "In Standard entity linking tool.",
                    "label": 0
                },
                {
                    "sent": "But this is something which is missing when you're doing it for questions.",
                    "label": 0
                },
                {
                    "sent": "So disambiguation is a problem here.",
                    "label": 0
                },
                {
                    "sent": "So that's what we deal here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, for just showing how relation linking has been done in pass so there could be like string similarity, you could use some kind of a dictionary.",
                    "label": 0
                },
                {
                    "sent": "You could use word net.",
                    "label": 0
                },
                {
                    "sent": "You could introduce word embeddings or you could also like recently machine learning approaches has also tried to do relation linking.",
                    "label": 0
                },
                {
                    "sent": "So basically relation linking would be let's say in your questions you could have phrases such as like written by wrote author or screenwriter, but indeed the ordinary or knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "They could be just connected to one single relation.",
                    "label": 0
                },
                {
                    "sent": "So basically one relation is expressed in any multiple natural language phrases.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So traditionally we have like so we did a bit of literature survey and then we identified that the entity linking and the relation linking in question answering system could be either than sequentially.",
                    "label": 0
                },
                {
                    "sent": "So you basically do either.",
                    "label": 0
                },
                {
                    "sent": "Let's say mostly you do relational entity linking 1st and then you do any relation linking.",
                    "label": 0
                },
                {
                    "sent": "This process could also be done parallelly, so you are doing your not looking how the entity linking is done when you're doing deletion linking or vice versa.",
                    "label": 0
                },
                {
                    "sent": "But then we also identified that this if maybe this process could also be done jointly.",
                    "label": 0
                },
                {
                    "sent": "So when you're doing relation linking you are looking also the candidates of entity linking when you're doing entity linking.",
                    "label": 0
                },
                {
                    "sent": "You're also looking the candidates of your relation linking.",
                    "label": 1
                },
                {
                    "sent": "So they have their own pros and cons.",
                    "label": 0
                },
                {
                    "sent": "So mostly in joint linking, you're the potentially you would have a better accuracy.",
                    "label": 0
                },
                {
                    "sent": "It's just the complexity becomes much more higher.",
                    "label": 0
                },
                {
                    "sent": "So this is so.",
                    "label": 0
                },
                {
                    "sent": "Now this is the problem would be how to deal with that complexity.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as a prelim, what we do is we treat the Knowledge graph as a subdivision graph.",
                    "label": 0
                },
                {
                    "sent": "So basically in a knowledge graph, your edge, your relations are always on edges.",
                    "label": 0
                },
                {
                    "sent": "But in our case we are treating relations also as nodes of knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "So the Knowledge Graph actually expands in terms of nodes in.",
                    "label": 0
                },
                {
                    "sent": "In this scenario.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are our hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So the first hypothesis is that like when you will be trying to identify these candidates like the correct candidates of each key words in a question, you will end up getting a minimum cost cycle.",
                    "label": 0
                },
                {
                    "sent": "Because in the Knowledge graph they would be obviously connected, so that's our first hypothesis.",
                    "label": 0
                },
                {
                    "sent": "The second hypothesis is that these these clothes these would have like a short hop connections, an also the density of these connections between all of the candidates would be high.",
                    "label": 0
                },
                {
                    "sent": "And the third hypothesis is here we have is that while doing relation linking and interlinking together, we would eventually get better results.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the general architecture of urn.",
                    "label": 0
                },
                {
                    "sent": "I'll go through it, so I'll.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First go through the preprocessing stages.",
                    "label": 0
                },
                {
                    "sent": "So first what we do is once you have it.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have a question.",
                    "label": 0
                },
                {
                    "sent": "We try to extract the main keywords and then remove some stop words.",
                    "label": 0
                },
                {
                    "sent": "So here in this case, so this is 1 standard example that we will be going through again and again in our slides.",
                    "label": 0
                },
                {
                    "sent": "So where was the founder of Tesla and SpaceX born?",
                    "label": 0
                },
                {
                    "sent": "So here I've highlighted that these are the keywords identified.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then what we try to do is once we get this keyword, we try to.",
                    "label": 0
                },
                {
                    "sent": "We have a, uh, entity relation predictor.",
                    "label": 0
                },
                {
                    "sent": "So given a keyword, it tries to predict whether this is an entity or whether this is an relation so.",
                    "label": 0
                },
                {
                    "sent": "Standard lead this could also be done within any error, but we just trained and LCM over the entire Wiki DB Pedia label corpora.",
                    "label": 0
                },
                {
                    "sent": "So basically all the labels of wiki data.",
                    "label": 0
                },
                {
                    "sent": "An further extended labels that we had, which I'll come in awhile were there for which helped us to classify whether a keyword is an entity or a relation.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So once we identify a keyword, we try to extract a list of candidates for that keyword.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, let's say for Founder you will search in the elastic search and try to identify what are the keywords that water the closer keywords to.",
                    "label": 0
                },
                {
                    "sent": "What are the closer relation to this label?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we also expanded the standard labels in DB pedia, so we use wiki data because Wiki data had more labels so we expanded that.",
                    "label": 0
                },
                {
                    "sent": "We also use Oxford Dictionary API to expand the labels for relation and then we also use fast X to further get let's say let's say we fixed up our threshold an fetch all the closer words do that.",
                    "label": 0
                },
                {
                    "sent": "Let's say the label was author and then we fixed up threshold of .9 of cosine similarity and then fetch.",
                    "label": 0
                },
                {
                    "sent": "All the words which are like more closer to let's say the word author.",
                    "label": 0
                },
                {
                    "sent": "And then put all of them in Elasticsearch.",
                    "label": 0
                },
                {
                    "sent": "So, so now I will.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go through to the second so we actually have two options.",
                    "label": 0
                },
                {
                    "sent": "If you see here we have actually two options in our pipeline, so one is the GSP solver and one is the connection density solver that we have.",
                    "label": 0
                },
                {
                    "sent": "So I will go through both of them now.",
                    "label": 0
                },
                {
                    "sent": "So GSP",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we formulate this problem of disambiguation as a GT SP.",
                    "label": 0
                },
                {
                    "sent": "So just that this generalized traveling salesman problem.",
                    "label": 0
                },
                {
                    "sent": "So let's say we have this question.",
                    "label": 0
                },
                {
                    "sent": "Who is the husband of the leader of Germany?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have basically three functions.",
                    "label": 0
                },
                {
                    "sent": "So basically first is the spot function.",
                    "label": 0
                },
                {
                    "sent": "So you basically spot the keywords here and then you generate the candidates for all of them.",
                    "label": 0
                },
                {
                    "sent": "So you basically get a multiple candidates for each.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Keyword spotted by candidate here.",
                    "label": 0
                },
                {
                    "sent": "I mean EU arise.",
                    "label": 0
                },
                {
                    "sent": "And then there is a cost function assigned to them about basically about the distance that they have in the Knowledge Graph.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically, in our subdivision graph.",
                    "label": 0
                },
                {
                    "sent": "So this is how we will we formulate our problem over GSP.",
                    "label": 0
                },
                {
                    "sent": "So GSP is to find a subset V Dash where we want to be end of the contents exactly 1 node from each cluster.",
                    "label": 1
                },
                {
                    "sent": "So from each so.",
                    "label": 0
                },
                {
                    "sent": "I'll so from this we will have a cluster.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this becomes a cluster.",
                    "label": 0
                },
                {
                    "sent": "This becomes a cluster and this becomes a cluster.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Then we try to pick one element from each cluster and try to find the cost and so so basically find a cycle an find the the bit with the least cost.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we come back to our example.",
                    "label": 0
                },
                {
                    "sent": "So where was the founder of Tesla and SpaceX born?",
                    "label": 0
                },
                {
                    "sent": "I hope that that's visible.",
                    "label": 0
                },
                {
                    "sent": "So if you see here, we have shown the cluster for each keyword and then these are the top candidates for each of them and the darker line shows the candidates which actually win in the end.",
                    "label": 0
                },
                {
                    "sent": "So if you see.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each one of them have different costs assigned to them and then with the least cost the cycle wins.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But doing so but doing exactly solving GSP could be NP hard.",
                    "label": 0
                },
                {
                    "sent": "It's not good beats, it is NP hard, so GS can be reduced to a TCS problem and then we could use an elk H algorithm which is like the state of the art for GT SP approximate solvers.",
                    "label": 0
                },
                {
                    "sent": "Because otherwise the time complexity of a normal USB solver is is too high.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we still have some drawbacks in this approach.",
                    "label": 0
                },
                {
                    "sent": "Is that it only provides you the best candidate for each keyword an.",
                    "label": 0
                },
                {
                    "sent": "So the best could be from the algorithm, but could not be the best in for the question.",
                    "label": 0
                },
                {
                    "sent": "And then it doesn't provide you kind of a list which would be eventually required when you will be.",
                    "label": 0
                },
                {
                    "sent": "So the the list is not re ranked, which could be useful when you will be, let's say, building up your sparkle query.",
                    "label": 0
                },
                {
                    "sent": "So basically in question answering, we have seen that when when you make the final sparkle query you try to get you try to use the list as like try to see maybe the second candidate is a better candidate with the first candidate but in GT SP this is something which is not possible to do.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so now we come with come up with a different solution for this.",
                    "label": 0
                },
                {
                    "sent": "So we basically named this it as connection density.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will not go through connection density, so connection density actually consists of three feature.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So initially the is the RI, which is basically the initial rank that you extract out, which you exactly get from the Elasticsearch, which just consists of maybe string similarity and some kind of semantic similarity also.",
                    "label": 0
                },
                {
                    "sent": "Of like of the list and the keyword.",
                    "label": 1
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you have connection count, so it is the number of candidates in another list divided by the total number of N keywords spotted.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's say if you have this.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Hop Count is the sum of the distance from C to the other candidates.",
                    "label": 1
                },
                {
                    "sent": "In all the other lists is divided by the total number of keywords spotted.",
                    "label": 1
                },
                {
                    "sent": "So once we get all these three features, then we try to re rank.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The list, which is the initial list an.",
                    "label": 1
                },
                {
                    "sent": "And that is basically the final list.",
                    "label": 0
                },
                {
                    "sent": "That would be the RF here, so we try to we use a class.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The fire to re rank this.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically used exhibits for this we tried with some others also, but for keeping the talk short, I'm not going for that in detail.",
                    "label": 0
                },
                {
                    "sent": "So once we get this so this basically ranks our list.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it is something like this so you.",
                    "label": 0
                },
                {
                    "sent": "You have your.",
                    "label": 0
                },
                {
                    "sent": "You get all your lists here and then.",
                    "label": 0
                },
                {
                    "sent": "Basically the connection count, the hop count and the connection count are fetched from this.",
                    "label": 0
                },
                {
                    "sent": "This this box I would say and then you give all the three features here.",
                    "label": 1
                },
                {
                    "sent": "I'll come to this red thing in a minute, and then you just basically ask it to rerank the list.",
                    "label": 1
                },
                {
                    "sent": "Then we have this red thing which is called the adaptive here learning because what we know tist while evaluating a system was.",
                    "label": 0
                },
                {
                    "sent": "That most of the errors that we were getting were becausw because at the ear predictor there were some errors.",
                    "label": 0
                },
                {
                    "sent": "So let's say this.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's say here the North America has been identified as a property in DB pedia so.",
                    "label": 0
                },
                {
                    "sent": "It was so it would eventually give you a very low confidence score from the exit boost also.",
                    "label": 0
                },
                {
                    "sent": "So what we did was.",
                    "label": 0
                },
                {
                    "sent": "We started sending adaptive signals back to the ear predictor module, so whenever after the RE ranking we find that OK like this has a hello are hello are confidence score.",
                    "label": 0
                },
                {
                    "sent": "So just just maybe flip it from entity to relation and then we check is the scores are improving or not.",
                    "label": 0
                },
                {
                    "sent": "So this basically improves the performance of the whole system by by significance.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now in compare the both both the approaches so GSP and connection density.",
                    "label": 0
                },
                {
                    "sent": "So the good thing in GSP approaches that you do not require any kind of training data.",
                    "label": 0
                },
                {
                    "sent": "So this could be really useful for let's say some industrial application where you do not have a lot of training data but you just have a knowledge graph and the client comes to you and say just I want to.",
                    "label": 0
                },
                {
                    "sent": "I want to I want to question answering system or an entity linking system over it so this helps in that way.",
                    "label": 0
                },
                {
                    "sent": "The advantage to connection density is that.",
                    "label": 0
                },
                {
                    "sent": "It gives you an ordered list, so the order list could be would be much more useful when you will be making sparkle queries so that this is one of the this is.",
                    "label": 0
                },
                {
                    "sent": "This is the basically the two advantages, so advantage to each of them.",
                    "label": 0
                },
                {
                    "sent": "And then I will come to the and also I have also mentioned the time complexity of both of them.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll just look back the postulate again.",
                    "label": 0
                },
                {
                    "sent": "OK, so sorry, there's time running, so I'll be bit more fast.",
                    "label": 0
                },
                {
                    "sent": "So here are the three postulates an, so I'll just go through the contributions.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we took else code data set which was published in last year I SWC which has 5000 question.",
                    "label": 0
                },
                {
                    "sent": "We annotated the entire data set with with keywords saying whether this is the for this keyword in the question.",
                    "label": 0
                },
                {
                    "sent": "This is the UI.",
                    "label": 0
                },
                {
                    "sent": "This data is also available.",
                    "label": 0
                },
                {
                    "sent": "We also expanded the DB Pedia entity and relation so this data is also available.",
                    "label": 1
                },
                {
                    "sent": "As a contribution in the paper an.",
                    "label": 0
                },
                {
                    "sent": "Then the experiment one is for hypothesis one and two, and so on.",
                    "label": 0
                },
                {
                    "sent": "As mentioned the slide.",
                    "label": 0
                },
                {
                    "sent": "So basically we also calc.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rated how the efficiency of entity linking is affected when we are trying to do with different approaches, so the brute force.",
                    "label": 0
                },
                {
                    "sent": "DSP gives us a really good entity linking.",
                    "label": 0
                },
                {
                    "sent": "Result accuracy.",
                    "label": 0
                },
                {
                    "sent": "The LKH drop slightly, but then the complexity is significantly less than the connection density has a slightly more complex than the LKH but, but the results are equal to the brute force GSP problem.",
                    "label": 0
                },
                {
                    "sent": "This is tested on else called data set.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the experiment two was basically that we removed the section.",
                    "label": 0
                },
                {
                    "sent": "Let's say we were actually giving the correct label only, so so this is from the gold standard data set.",
                    "label": 0
                },
                {
                    "sent": "So you have the correct label so that you are not losing anything while doing the keyword detection.",
                    "label": 0
                },
                {
                    "sent": "An if you are also providing the correct UI in the list, let's see if it is.",
                    "label": 0
                },
                {
                    "sent": "It was not fetched and you put it in the last of the list, then then we see that the accuracy could even go to something like .9.",
                    "label": 0
                },
                {
                    "sent": "So basically we so in so.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But for the entity linking, what we tested was that we eventually we're getting numbers, something like .65 and .5 seven and two different datasets, so we can hear see that there is a significant drop in performance just because of the keyword detection.",
                    "label": 0
                },
                {
                    "sent": "So that is something to look forward in future.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for relation linking, these are the numbers.",
                    "label": 0
                },
                {
                    "sent": "It's like .47.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so yeah, so I'll just wind up my talk by saying that.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the two approaches and.",
                    "label": 0
                },
                {
                    "sent": "There are some limitations to all that it cannot do something like hidden relation detection.",
                    "label": 0
                },
                {
                    "sent": "If the question has a hidden relation, then it cannot detect.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and these we have the two approaches as I mentioned again and again.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so, and these are the references.",
                    "label": 0
                },
                {
                    "sent": "And thank you and.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looking for a question.",
                    "label": 0
                },
                {
                    "sent": "Time for a couple of questions.",
                    "label": 0
                },
                {
                    "sent": "So can I start with the question so you mentioned using DV Pedia and a couple of other sources?",
                    "label": 0
                },
                {
                    "sent": "Did you find out that some of them are more useful for certain links than others?",
                    "label": 0
                },
                {
                    "sent": "Or did you try to find out when certain ones can be used?",
                    "label": 0
                },
                {
                    "sent": "No, I think we didn't explore this, but what we did was let's say for Barrack Obama.",
                    "label": 0
                },
                {
                    "sent": "There is already DPD already mentions same as link in wiki data so we just fetched all the labels related to Barack Obama in wiki data and just put them in the Elasticsearch.",
                    "label": 0
                },
                {
                    "sent": "And for relations we did it with first expanding the label with the dictionary and from all those labels.",
                    "label": 0
                },
                {
                    "sent": "Then we further fetched more label from fast text.",
                    "label": 0
                },
                {
                    "sent": "Further questions.",
                    "label": 0
                },
                {
                    "sent": "No one more.",
                    "label": 0
                },
                {
                    "sent": "Could you use the microphone just so where is the microphone?",
                    "label": 0
                },
                {
                    "sent": "This one here notes that.",
                    "label": 0
                },
                {
                    "sent": "I don't trust.",
                    "label": 0
                },
                {
                    "sent": "So my question is more like.",
                    "label": 0
                },
                {
                    "sent": "How far away is production quality of this relation extraction?",
                    "label": 0
                },
                {
                    "sent": "Let's say what's the problem?",
                    "label": 1
                },
                {
                    "sent": "You see, what companies would be willing to take for their, let's say.",
                    "label": 0
                },
                {
                    "sent": "Operations like so if you are too low, I mean you don't trust like Harris doesn't trust me right?",
                    "label": 0
                },
                {
                    "sent": "But let's say where would be the levels where the companies would take over and say, well this is good.",
                    "label": 0
                },
                {
                    "sent": "So are you talking specifically about this paper or you're talking in general terms of question answering.",
                    "label": 0
                },
                {
                    "sent": "I was thinking about both I guess.",
                    "label": 0
                },
                {
                    "sent": "OK, so since I also work at front over so I can, I don't know how much I can say, but I can't.",
                    "label": 0
                },
                {
                    "sent": "You can be alone.",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah we have seen in near so we get a lot of.",
                    "label": 0
                },
                {
                    "sent": "What should I say?",
                    "label": 0
                },
                {
                    "sent": "We get a lot of offers to do question answering over let's say some some data and we have been doing it quite good up till now, so I think it's somewhat production ready right now, I would say.",
                    "label": 0
                },
                {
                    "sent": "You can grill him afterwards.",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course it was not really answer, but well, that's why we can talk later.",
                    "label": 0
                },
                {
                    "sent": "Maybe I misunderstood something.",
                    "label": 0
                },
                {
                    "sent": "Someone I can come up and discuss.",
                    "label": 0
                },
                {
                    "sent": "We can discuss.",
                    "label": 0
                },
                {
                    "sent": "Yeah exactly yeah OK well switch.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much again another clap.",
                    "label": 0
                }
            ]
        }
    }
}