{
    "id": "447aygy3n7i5kbi25odzvnfnonix3yac",
    "title": "Fisher Vector Faces in the Wild",
    "info": {
        "author": [
            "Karen Simonyan, Department of Engineering Science, University of Oxford"
        ],
        "published": "April 3, 2014",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/bmvc2013_simonyan_vector_faces/",
    "segmentation": [
        [
            "So I'm going to talk about the Fisher vector phase descriptor."
        ],
        [
            "Our aim here is to develop as a scripter which is most suited for face recognition tasks to this, and we propose a method which is based on dense sampling of local features.",
            "High dimensional feature encoding and discriminates reduction on top of that, our approach is quite different from any conventional paid descriptors which utilizes the sparse face landmarks as shown here on the left.",
            "So in this case, in this example, our face landmarks, the corners of the eyes, the corners of the mouth and nose.",
            "So what many conventional methods do they detect these landmarks?",
            "And then they describe the spatial neighborhood of which of the landmark which makes the face descriptor.",
            "The disadvantage is that it's not clear from the outset which landmarks are important for presentation, which are not so in our case.",
            "We sampled answer as shown on the right, and then we use machine learning to determine which parts of the face are important and which are not."
        ],
        [
            "Such kind of approach is motivated by the fact that the sampled features coupled with high dimensional intelligence they achieve a very good performance in a number of generic image recognition tasks.",
            "In particular, here we can see the density of features coupled with affecting code in this kind of a pipeline it achieves state of the art or next state of the art performance on a number of generic recognition benchmarks.",
            "But here we applies to the phase domain and we demonstrate that."
        ],
        [
            "It achieves state of our performance on the Face verification task.",
            "The Face verification task is when you are given a pair of images like shown here and the tasks determine if both images portray the same person.",
            "For instance, on the left we have to images of Paul McCartney.",
            "So the answer to the question above is yes.",
            "It's the same person on both images but on the right we have to images of similar looking people but actually these are different people showing them so as you can see it's quite challenging.",
            "And we use label faces in the wild datasets, which is a large scale data set.",
            "It contains many images of many people.",
            "It was collected using Viola Jones face detector and exhibits habitability in appearance lighting conditions, so it's challenging task."
        ],
        [
            "Here is the overview of our pipeline.",
            "I will go through this overview quickly now and then we'll go for each of these stages in modulator.",
            "So on the input will get a face image which can be aligned using one of these interconnects, or it can come just straight out of the Viola Jones detector without any alignment at all.",
            "Then we compute density of features that if you select encoding and perform discriminative dimensionality reduction.",
            "So in the output we have a compact and discriminative descriptor."
        ],
        [
            "Now let's go for these stages and with the title.",
            "The first step is feature extraction.",
            "As I mentioned before, we don't use face landmarks, we just samples it densely over the whole image plane over multiple scales.",
            "Then we perform.",
            "That reply explicit counter counter map, which corresponds to elementwise square root and this approach is also known as would sift.",
            "It helps migration tasks, and it's also helpful here.",
            "And after that we perform PCA on our website.",
            "Features BC serves two purposes.",
            "The first purpose is to decrease dimensionality, which is not exactly.",
            "Required, but it just speeds up the post and later and the second aim of PCA is to decorate our features, which is important as I'll explain later.",
            "And after that we augment each our local feature with its spatial coordinates.",
            "Such kind of augmentation is seen as another way to incorporate spatial information into our image representation, so it's A kind of alternative to spatial pyramid, but the advantage of this is that it does not lead to such big increase in dimensionality as special pyramid.",
            "So with the output of this stage, our faces represented by a large number like 25 K local features."
        ],
        [
            "And.",
            "On the next stage.",
            "What we do.",
            "We want to encode this large set of features as a single high dimensional vector and to this end we utilize conventional selection coding which basically uses a Gaussian mixture model is a kind of visual codebook where each Gaussian correspondes social code word which in our case encodes both appearance and location information.",
            "Otherwise, our German here visualized special components.",
            "Basically each Gaussian is shown within the lips such that it's mean it's sensor andreji attached to the mean and variances of each of the Gaussian.",
            "As you can see, the densely cover the face and on the right there's a close up.",
            "So as you can see, some of these Gaussians actually respond to facial landmarks and that should also know that we use diagonal covariance GMM.",
            "So that's why it is important to correlate.",
            "Our local features so that they are amenable to model using diagonal covariance Gaussians."
        ],
        [
            "Once the Germans land, we compute the fish selection code and image is computed as a normalized sum of official act incursions of each of the local features, and these individual incursion, soca muted by soft assignment of each feature X.",
            "And also here of each feature acts to each of the Gaussians.",
            "And then we also encode 1st and 2nd order statistics over the displacement of this feature X with respect to each of these Gaussians.",
            "This is different from bag of words where we just compute the current statistics.",
            "So once these first and 2nd order starts are computed."
        ],
        [
            "I've talked into a single high dimension of vector.",
            "It's dimensionality is equal to the number of Gaussians times two times dimensions of local feature.",
            "So in our particular case that Demelza can be as high as 67.5 K. So what we want to do?"
        ],
        [
            "Stage is to decrease the dimensionality so that our face descriptor is most usable for large scale recognition tasks.",
            "But at the same time we also want to learn the distance between Fisher vectors and we want this to be discriminative as shown here.",
            "We would like to have the distance between images of the same person to be small and the distance between images of different people to be large as stated here, where the pinpoint correspond to the distance between images of the same person.",
            "And thread point responds to distance between images of different people.",
            "We can we can enforce that kind of property by.",
            "Setting up watching constraints shown above.",
            "And then we use.",
            "This constrains it was different distance functions.",
            "So the models which are considered here along with distance which corresponds to Euclidean distance in projective space.",
            "Then we also consider the combined distance similarity score function and we also consider weighted equation distance where you actually learn a single weight for each of the components.",
            "So let's go through these models in more detail."
        ],
        [
            "So the first one is the low rank, another distance as shown here is response to the Euclidean distance in the projective space.",
            "And what we want to do, we want to learn here.",
            "The projection matrix W shown on the right is rectangular matrix because we want to reduce dimensionality from high to low.",
            "So when we put this kind of distance function into our budget constraints, we get an objective which is actually a non convex in terms of W. Better can still optimize it in stochastic subgradient method we initialize W with PCA, whitening projection and then we perform as a team."
        ],
        [
            "An alternative recently proposed method is instead of learning a single projection matrix and then taking Euclidean distance in the projective space, what you can do, you can learn to projection matrices and then you compute Euclidean distance in one space.",
            "You compute inner product in another space and take their difference as kind of a generalized distance function.",
            "Where does this approach here and put it into our larger margin formulation?",
            "So again, it's nonconvex in terms of WMV, so here, web semis over 2 mattresses to protection mattresses.",
            "But we can still use as Jersey.",
            "And as I'll show later, when you use this kind of score function, you get better performance compared to just running a single projection matrix."
        ],
        [
            "Finally, we also consider which declension distance it's basically just distance where you have a single negative weight for each of the fish electric components.",
            "The disadvantage here is that we kind of dimensionality reduction with this formulation, but advantages that we just learn a single back to you.",
            "So the number of parameters is less, and this kind of formulation can be useful where they might have.",
            "Changing data is limited and also it's convex in terms of you."
        ],
        [
            "OK, so we've gone through all the stages of our pipeline, so now let's see how the performance changes with respect to different parameters.",
            "Here we report the performance measure, which is the accuracy which corresponds to equal false positive and false negative rates."
        ],
        [
            "First of all, we can notice that the performance improves when we use more dense sampling.",
            "When we use special colour documentation and also when we use more Gaussians."
        ],
        [
            "Then we can see that's when we add our discriminative dimensionality reduction on superficial access, not only which she 500 fold dimensionality reduction, but we also improve their performance.",
            "So actually here our final representation is just one element, said Dimensional.",
            "So the presentation of the whole phase has the same dimensionality as the dimensionality of a single supervision."
        ],
        [
            "Then we can see that we can get additional gains by doing test set augmentation.",
            "In this particular case, it means that when we compare a pale faces, we also consider that result of flips, which gives us four possible combinations, and then we can compute the average of the distance between these four pairs and use it as our final distance function.",
            "And."
        ],
        [
            "Finally, when we when we learned both.",
            "Low rank distance and low rank similarity would belong to projection matrices.",
            "We get even better performance."
        ],
        [
            "As I mentioned before, our method can be used on top of different schemes, so here we can see how the performance changes with respect to different alignment schemes.",
            "So the first 3 answers here they cross point to different alignment methods and as you can see the performance is pretty close.",
            "What's interesting is that the 4th result is there is not without any element at all, so we just take a face image straight out of a legitimate actor.",
            "We compute our official action face representation and the performance is quite competitive with respect to the results where we have alignment.",
            "But for sure it is this dependent, so it could well be with us on some other datasets where the amount of variability in, say face pose is large.",
            "Maybe in that case do an alignment will be more helpful."
        ],
        [
            "So now we visualize what we have learned.",
            "Left we show all the Gaussians.",
            "It's the same image as you saw before.",
            "So what we want to do want to see which of these Gaussians are important for representation and which are not to this and we can make use of the fact that each Gaussian correspond to part of a Fisher vector, which in turn corresponds to submatrix of the projection matrix W shown here.",
            "So if you want to figure out which Gaussians important at which are not, we can compute the energy business norm which of these sub matrixes and then drank the Gaussians.",
            "With respect to this norm, So what we see in the sense?"
        ],
        [
            "Here we can see the top 50 Golson's the most important Gaussians of our GMM, and as you can see, they correspond to the facial features.",
            "Even though our model was not trained to do so.",
            "So basically we automatically learned from the data that these parts of the face which is bound to eyes mostly eyes, nose and.",
            "Not mouth this part of the face are important, On the contrary, on the right you can see the least important Gaussians which responds to the background areas."
        ],
        [
            "And here we can see the same visualization for different methods and also without any alignment in the center.",
            "We just have a bounding box from Biogen protector and the pattern of the most important and the least important.",
            "Essence is basically the same."
        ],
        [
            "OK, so let's move to the compile some with the state of the art.",
            "He reported the performance in terms of verification accuracy as required by.",
            "Afw benchmark they will face in the wild benchmark.",
            "So we start with system.",
            "In this certain we're not allowed to use any outside training data, so we just use pre Alliant images provided by the organizers of the benchmark.",
            "But what's going on here is that the amount of training data is limited.",
            "We can only use 5400.",
            "Image Path and we found out with this kind of training data, it's not enough to popular learner projection matrix.",
            "So that's why here we just learn what distance but will still get the state of the art and we outperformed the recently published method by more than 3%.",
            "And on the right you can see the LC curve and our curve comes quite a bit on top."
        ],
        [
            "Now we compare in the Android system.",
            "In this certain we can use outside training data, but only for alignment.",
            "So here we employed to learn a simple alignment technique by Evan Haematol.",
            "But as we saw before, our method is robust with respect to different alignment techniques, so very similar performance can be achieved is in different other methods.",
            "But what's more important here is that the number of tenant image pairs is not limited, so we can generate as many pairs as we want, which is well suited for our stochastic optimization.",
            "And the result we get it closely matches the state of the art.",
            "A recently published a CPR.",
            "In their case they used 27 face landmarks and this sample LBP features around them.",
            "But in our case we used a sample SIFT and actually in their case the performance of shift was slightly worse.",
            "So there's some hope that if we use some other local descriptor here, we can get even better performance."
        ],
        [
            "So to conclude this paper, we have proposed the official presentation which is based on off the shelf image recognition pipeline, but he replied to faces we don't rely on sparse facial landmarks instead.",
            "Would sample sifts densely, then we compute official acting code and and we have discrimination reduction on top of that, our method achieves state of that performance in the challenging data set and the performance that performs well on top of different alignment schemes.",
            "Thanks for attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about the Fisher vector phase descriptor.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our aim here is to develop as a scripter which is most suited for face recognition tasks to this, and we propose a method which is based on dense sampling of local features.",
                    "label": 1
                },
                {
                    "sent": "High dimensional feature encoding and discriminates reduction on top of that, our approach is quite different from any conventional paid descriptors which utilizes the sparse face landmarks as shown here on the left.",
                    "label": 0
                },
                {
                    "sent": "So in this case, in this example, our face landmarks, the corners of the eyes, the corners of the mouth and nose.",
                    "label": 0
                },
                {
                    "sent": "So what many conventional methods do they detect these landmarks?",
                    "label": 0
                },
                {
                    "sent": "And then they describe the spatial neighborhood of which of the landmark which makes the face descriptor.",
                    "label": 1
                },
                {
                    "sent": "The disadvantage is that it's not clear from the outset which landmarks are important for presentation, which are not so in our case.",
                    "label": 0
                },
                {
                    "sent": "We sampled answer as shown on the right, and then we use machine learning to determine which parts of the face are important and which are not.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Such kind of approach is motivated by the fact that the sampled features coupled with high dimensional intelligence they achieve a very good performance in a number of generic image recognition tasks.",
                    "label": 1
                },
                {
                    "sent": "In particular, here we can see the density of features coupled with affecting code in this kind of a pipeline it achieves state of the art or next state of the art performance on a number of generic recognition benchmarks.",
                    "label": 0
                },
                {
                    "sent": "But here we applies to the phase domain and we demonstrate that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It achieves state of our performance on the Face verification task.",
                    "label": 1
                },
                {
                    "sent": "The Face verification task is when you are given a pair of images like shown here and the tasks determine if both images portray the same person.",
                    "label": 1
                },
                {
                    "sent": "For instance, on the left we have to images of Paul McCartney.",
                    "label": 0
                },
                {
                    "sent": "So the answer to the question above is yes.",
                    "label": 0
                },
                {
                    "sent": "It's the same person on both images but on the right we have to images of similar looking people but actually these are different people showing them so as you can see it's quite challenging.",
                    "label": 1
                },
                {
                    "sent": "And we use label faces in the wild datasets, which is a large scale data set.",
                    "label": 1
                },
                {
                    "sent": "It contains many images of many people.",
                    "label": 0
                },
                {
                    "sent": "It was collected using Viola Jones face detector and exhibits habitability in appearance lighting conditions, so it's challenging task.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the overview of our pipeline.",
                    "label": 0
                },
                {
                    "sent": "I will go through this overview quickly now and then we'll go for each of these stages in modulator.",
                    "label": 0
                },
                {
                    "sent": "So on the input will get a face image which can be aligned using one of these interconnects, or it can come just straight out of the Viola Jones detector without any alignment at all.",
                    "label": 0
                },
                {
                    "sent": "Then we compute density of features that if you select encoding and perform discriminative dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "So in the output we have a compact and discriminative descriptor.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's go for these stages and with the title.",
                    "label": 0
                },
                {
                    "sent": "The first step is feature extraction.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned before, we don't use face landmarks, we just samples it densely over the whole image plane over multiple scales.",
                    "label": 0
                },
                {
                    "sent": "Then we perform.",
                    "label": 0
                },
                {
                    "sent": "That reply explicit counter counter map, which corresponds to elementwise square root and this approach is also known as would sift.",
                    "label": 0
                },
                {
                    "sent": "It helps migration tasks, and it's also helpful here.",
                    "label": 0
                },
                {
                    "sent": "And after that we perform PCA on our website.",
                    "label": 0
                },
                {
                    "sent": "Features BC serves two purposes.",
                    "label": 0
                },
                {
                    "sent": "The first purpose is to decrease dimensionality, which is not exactly.",
                    "label": 0
                },
                {
                    "sent": "Required, but it just speeds up the post and later and the second aim of PCA is to decorate our features, which is important as I'll explain later.",
                    "label": 0
                },
                {
                    "sent": "And after that we augment each our local feature with its spatial coordinates.",
                    "label": 0
                },
                {
                    "sent": "Such kind of augmentation is seen as another way to incorporate spatial information into our image representation, so it's A kind of alternative to spatial pyramid, but the advantage of this is that it does not lead to such big increase in dimensionality as special pyramid.",
                    "label": 0
                },
                {
                    "sent": "So with the output of this stage, our faces represented by a large number like 25 K local features.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "On the next stage.",
                    "label": 0
                },
                {
                    "sent": "What we do.",
                    "label": 0
                },
                {
                    "sent": "We want to encode this large set of features as a single high dimensional vector and to this end we utilize conventional selection coding which basically uses a Gaussian mixture model is a kind of visual codebook where each Gaussian correspondes social code word which in our case encodes both appearance and location information.",
                    "label": 1
                },
                {
                    "sent": "Otherwise, our German here visualized special components.",
                    "label": 1
                },
                {
                    "sent": "Basically each Gaussian is shown within the lips such that it's mean it's sensor andreji attached to the mean and variances of each of the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "As you can see, the densely cover the face and on the right there's a close up.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, some of these Gaussians actually respond to facial landmarks and that should also know that we use diagonal covariance GMM.",
                    "label": 0
                },
                {
                    "sent": "So that's why it is important to correlate.",
                    "label": 1
                },
                {
                    "sent": "Our local features so that they are amenable to model using diagonal covariance Gaussians.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once the Germans land, we compute the fish selection code and image is computed as a normalized sum of official act incursions of each of the local features, and these individual incursion, soca muted by soft assignment of each feature X.",
                    "label": 1
                },
                {
                    "sent": "And also here of each feature acts to each of the Gaussians.",
                    "label": 0
                },
                {
                    "sent": "And then we also encode 1st and 2nd order statistics over the displacement of this feature X with respect to each of these Gaussians.",
                    "label": 0
                },
                {
                    "sent": "This is different from bag of words where we just compute the current statistics.",
                    "label": 1
                },
                {
                    "sent": "So once these first and 2nd order starts are computed.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've talked into a single high dimension of vector.",
                    "label": 0
                },
                {
                    "sent": "It's dimensionality is equal to the number of Gaussians times two times dimensions of local feature.",
                    "label": 0
                },
                {
                    "sent": "So in our particular case that Demelza can be as high as 67.5 K. So what we want to do?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stage is to decrease the dimensionality so that our face descriptor is most usable for large scale recognition tasks.",
                    "label": 0
                },
                {
                    "sent": "But at the same time we also want to learn the distance between Fisher vectors and we want this to be discriminative as shown here.",
                    "label": 0
                },
                {
                    "sent": "We would like to have the distance between images of the same person to be small and the distance between images of different people to be large as stated here, where the pinpoint correspond to the distance between images of the same person.",
                    "label": 0
                },
                {
                    "sent": "And thread point responds to distance between images of different people.",
                    "label": 0
                },
                {
                    "sent": "We can we can enforce that kind of property by.",
                    "label": 0
                },
                {
                    "sent": "Setting up watching constraints shown above.",
                    "label": 0
                },
                {
                    "sent": "And then we use.",
                    "label": 0
                },
                {
                    "sent": "This constrains it was different distance functions.",
                    "label": 0
                },
                {
                    "sent": "So the models which are considered here along with distance which corresponds to Euclidean distance in projective space.",
                    "label": 0
                },
                {
                    "sent": "Then we also consider the combined distance similarity score function and we also consider weighted equation distance where you actually learn a single weight for each of the components.",
                    "label": 0
                },
                {
                    "sent": "So let's go through these models in more detail.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first one is the low rank, another distance as shown here is response to the Euclidean distance in the projective space.",
                    "label": 0
                },
                {
                    "sent": "And what we want to do, we want to learn here.",
                    "label": 0
                },
                {
                    "sent": "The projection matrix W shown on the right is rectangular matrix because we want to reduce dimensionality from high to low.",
                    "label": 0
                },
                {
                    "sent": "So when we put this kind of distance function into our budget constraints, we get an objective which is actually a non convex in terms of W. Better can still optimize it in stochastic subgradient method we initialize W with PCA, whitening projection and then we perform as a team.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An alternative recently proposed method is instead of learning a single projection matrix and then taking Euclidean distance in the projective space, what you can do, you can learn to projection matrices and then you compute Euclidean distance in one space.",
                    "label": 0
                },
                {
                    "sent": "You compute inner product in another space and take their difference as kind of a generalized distance function.",
                    "label": 0
                },
                {
                    "sent": "Where does this approach here and put it into our larger margin formulation?",
                    "label": 0
                },
                {
                    "sent": "So again, it's nonconvex in terms of WMV, so here, web semis over 2 mattresses to protection mattresses.",
                    "label": 0
                },
                {
                    "sent": "But we can still use as Jersey.",
                    "label": 0
                },
                {
                    "sent": "And as I'll show later, when you use this kind of score function, you get better performance compared to just running a single projection matrix.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, we also consider which declension distance it's basically just distance where you have a single negative weight for each of the fish electric components.",
                    "label": 0
                },
                {
                    "sent": "The disadvantage here is that we kind of dimensionality reduction with this formulation, but advantages that we just learn a single back to you.",
                    "label": 0
                },
                {
                    "sent": "So the number of parameters is less, and this kind of formulation can be useful where they might have.",
                    "label": 0
                },
                {
                    "sent": "Changing data is limited and also it's convex in terms of you.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we've gone through all the stages of our pipeline, so now let's see how the performance changes with respect to different parameters.",
                    "label": 0
                },
                {
                    "sent": "Here we report the performance measure, which is the accuracy which corresponds to equal false positive and false negative rates.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First of all, we can notice that the performance improves when we use more dense sampling.",
                    "label": 0
                },
                {
                    "sent": "When we use special colour documentation and also when we use more Gaussians.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we can see that's when we add our discriminative dimensionality reduction on superficial access, not only which she 500 fold dimensionality reduction, but we also improve their performance.",
                    "label": 0
                },
                {
                    "sent": "So actually here our final representation is just one element, said Dimensional.",
                    "label": 0
                },
                {
                    "sent": "So the presentation of the whole phase has the same dimensionality as the dimensionality of a single supervision.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we can see that we can get additional gains by doing test set augmentation.",
                    "label": 0
                },
                {
                    "sent": "In this particular case, it means that when we compare a pale faces, we also consider that result of flips, which gives us four possible combinations, and then we can compute the average of the distance between these four pairs and use it as our final distance function.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, when we when we learned both.",
                    "label": 0
                },
                {
                    "sent": "Low rank distance and low rank similarity would belong to projection matrices.",
                    "label": 0
                },
                {
                    "sent": "We get even better performance.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I mentioned before, our method can be used on top of different schemes, so here we can see how the performance changes with respect to different alignment schemes.",
                    "label": 0
                },
                {
                    "sent": "So the first 3 answers here they cross point to different alignment methods and as you can see the performance is pretty close.",
                    "label": 0
                },
                {
                    "sent": "What's interesting is that the 4th result is there is not without any element at all, so we just take a face image straight out of a legitimate actor.",
                    "label": 0
                },
                {
                    "sent": "We compute our official action face representation and the performance is quite competitive with respect to the results where we have alignment.",
                    "label": 0
                },
                {
                    "sent": "But for sure it is this dependent, so it could well be with us on some other datasets where the amount of variability in, say face pose is large.",
                    "label": 0
                },
                {
                    "sent": "Maybe in that case do an alignment will be more helpful.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we visualize what we have learned.",
                    "label": 0
                },
                {
                    "sent": "Left we show all the Gaussians.",
                    "label": 0
                },
                {
                    "sent": "It's the same image as you saw before.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do want to see which of these Gaussians are important for representation and which are not to this and we can make use of the fact that each Gaussian correspond to part of a Fisher vector, which in turn corresponds to submatrix of the projection matrix W shown here.",
                    "label": 0
                },
                {
                    "sent": "So if you want to figure out which Gaussians important at which are not, we can compute the energy business norm which of these sub matrixes and then drank the Gaussians.",
                    "label": 0
                },
                {
                    "sent": "With respect to this norm, So what we see in the sense?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we can see the top 50 Golson's the most important Gaussians of our GMM, and as you can see, they correspond to the facial features.",
                    "label": 0
                },
                {
                    "sent": "Even though our model was not trained to do so.",
                    "label": 1
                },
                {
                    "sent": "So basically we automatically learned from the data that these parts of the face which is bound to eyes mostly eyes, nose and.",
                    "label": 0
                },
                {
                    "sent": "Not mouth this part of the face are important, On the contrary, on the right you can see the least important Gaussians which responds to the background areas.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here we can see the same visualization for different methods and also without any alignment in the center.",
                    "label": 0
                },
                {
                    "sent": "We just have a bounding box from Biogen protector and the pattern of the most important and the least important.",
                    "label": 0
                },
                {
                    "sent": "Essence is basically the same.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's move to the compile some with the state of the art.",
                    "label": 0
                },
                {
                    "sent": "He reported the performance in terms of verification accuracy as required by.",
                    "label": 1
                },
                {
                    "sent": "Afw benchmark they will face in the wild benchmark.",
                    "label": 0
                },
                {
                    "sent": "So we start with system.",
                    "label": 1
                },
                {
                    "sent": "In this certain we're not allowed to use any outside training data, so we just use pre Alliant images provided by the organizers of the benchmark.",
                    "label": 1
                },
                {
                    "sent": "But what's going on here is that the amount of training data is limited.",
                    "label": 0
                },
                {
                    "sent": "We can only use 5400.",
                    "label": 0
                },
                {
                    "sent": "Image Path and we found out with this kind of training data, it's not enough to popular learner projection matrix.",
                    "label": 0
                },
                {
                    "sent": "So that's why here we just learn what distance but will still get the state of the art and we outperformed the recently published method by more than 3%.",
                    "label": 0
                },
                {
                    "sent": "And on the right you can see the LC curve and our curve comes quite a bit on top.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we compare in the Android system.",
                    "label": 0
                },
                {
                    "sent": "In this certain we can use outside training data, but only for alignment.",
                    "label": 1
                },
                {
                    "sent": "So here we employed to learn a simple alignment technique by Evan Haematol.",
                    "label": 0
                },
                {
                    "sent": "But as we saw before, our method is robust with respect to different alignment techniques, so very similar performance can be achieved is in different other methods.",
                    "label": 0
                },
                {
                    "sent": "But what's more important here is that the number of tenant image pairs is not limited, so we can generate as many pairs as we want, which is well suited for our stochastic optimization.",
                    "label": 0
                },
                {
                    "sent": "And the result we get it closely matches the state of the art.",
                    "label": 0
                },
                {
                    "sent": "A recently published a CPR.",
                    "label": 0
                },
                {
                    "sent": "In their case they used 27 face landmarks and this sample LBP features around them.",
                    "label": 0
                },
                {
                    "sent": "But in our case we used a sample SIFT and actually in their case the performance of shift was slightly worse.",
                    "label": 0
                },
                {
                    "sent": "So there's some hope that if we use some other local descriptor here, we can get even better performance.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude this paper, we have proposed the official presentation which is based on off the shelf image recognition pipeline, but he replied to faces we don't rely on sparse facial landmarks instead.",
                    "label": 0
                },
                {
                    "sent": "Would sample sifts densely, then we compute official acting code and and we have discrimination reduction on top of that, our method achieves state of that performance in the challenging data set and the performance that performs well on top of different alignment schemes.",
                    "label": 1
                },
                {
                    "sent": "Thanks for attention.",
                    "label": 0
                }
            ]
        }
    }
}