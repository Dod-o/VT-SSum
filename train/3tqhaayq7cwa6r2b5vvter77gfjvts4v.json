{
    "id": "3tqhaayq7cwa6r2b5vvter77gfjvts4v",
    "title": "Efficient sampling for Bayesian inference of conjunctive Bayesian networks",
    "info": {
        "author": [
            "Thomas Sakoparnig, ETH Zurich"
        ],
        "published": "Oct. 23, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Medicine->Oncology",
            "Top->Biology->Genetics"
        ]
    },
    "url": "http://videolectures.net/mlsb2012_sakoparnig_sampling/",
    "segmentation": [
        [
            "So."
        ],
        [
            "First of all, what is cancer?",
            "So cancer is an evolutionary of follows an evolutionary process and is characterized by the accumulation of advantageous mutations and the outgrowth of the South.",
            "With those advantageous mutation in comparison to all the other cells surrounding it, it which do not have this advantageous mutation.",
            "Mutations, so for lots of these mutations they are recurrent.",
            "So if you look at multiple samples of the same cancer subtypes you can see the same mutations over and over again.",
            "For us it's it's different and you just see them in very very small subsets of.",
            "Accounts of the same type or even subtype, so it has been shown.",
            "A couple of almost two decades ago now that certain mutations depend on the present presence of other mutations.",
            "So for example, for a gene, in order for Gene B to contain a mutation, or to get a mutation that already has to be imitation in Chennai.",
            "The order of these dependencies, or of these networks, are in general unknown orders.",
            "Very, very weak idea about them.",
            "So there have been a couple of."
        ],
        [
            "Models on how to model.",
            "These mutations are these dependencies of mutations.",
            "And one of them is conjunctive Bayesian networks.",
            "So here you can see a small example of a conjunctive patient network.",
            "We're basically chain B and Cincy are dependent on chin, a meaning for Gene B can be a mutation and gene be there already has to be be one present in Chennai.",
            "So what we usually get from from cancer patients.",
            "Is cross sectional data so basically.",
            "Cancer patients get diagnosed at some point of their carcinogenesis.",
            "A sample is extracted.",
            "Geno type is measured and basically we just see a snapshot of the cancer progression.",
            "So what is the problem at hand here?",
            "What is the problem that?"
        ],
        [
            "The address so from this basically from these sorts of data that I showed you, you can.",
            "You can infer, or you can train this conjunctive patient networks.",
            "What's been very, very difficult so far is to quantify the uncertainty of these structures.",
            "So what we DEF do is we use Asian approach.",
            "To infer those networks, and also to infer that the uncertainty what's been a problem so far.",
            "And is over with sampling, and Bayesian networks is first of all.",
            "They're very, very inefficient, so the samples.",
            "Usually it's MCMC are very, very slow.",
            "And also they tend to get stuck in local Optima.",
            "So here I can show you the."
        ],
        [
            "The model itself, which is here.",
            "So we have a dependence structure.",
            "Here we have off for each note we have a conditional mutation probability seat OK, which basically means once all its parents or predecessors or mutated then this gene can mutate with probability of seat OK.",
            "So here that is a true Geno type or is.",
            "Yeah and basically the probability of ZT.",
            "So that is a vector of like engine or engines or another.",
            "Sorts of loci could be chromosome arms.",
            "Could be pathways could be basically any logical entity.",
            "And basically we multiply all the all the conditional probabilities of the genes which are already mutated times the.",
            "Probabilities off the chains which could mutate next, which is this exit satir that they have not mutated yet.",
            "So if set is or if the Geno type is compatible with our dependency structure, then we basically get this probability.",
            "If it's not compatible, we get a probability of zero.",
            "Of course, gene typing is very very depends on the methodology, of course, but it's very, very error prone process and This is why we model this error problem.",
            "This error process separately.",
            "So basically for every gene, we have a probability of epsilon that we misdiagnosed it basically and a probability of 1 minus epsilon that it was correct.",
            "Here the is the Hamming distance between the observed Geno type and the true hidden genotype.",
            "Sorry.",
            "We basically here we have the likelihood we."
        ],
        [
            "Integrate out all the hidden Geno types.",
            "And of course we do not only have one observation, but we have a number of observations and we just iterate over them.",
            "Here is the full posterior, so by full posterior I mean the structure of the mutation probabilities.",
            "And the error probability.",
            "We use a couple of, of course, of priors, so we have an informal, uninformative.",
            "Improper prior for the structure.",
            "We have an uninformative prior for the separate mutation probabilities, and we have a weak but informative prior for the excuse me.",
            "Error probability."
        ],
        [
            "So we derived a metropolis Hastings within Gibbs sampling scheme.",
            "It's basically a hybrid sampler sampler.",
            "We mix 8.",
            "Different structure move eight different move types.",
            "Six of them are structure, move types and two of them deal with our continuous parameters.",
            "All the asymmetric structure move types have disjoint neighborhoods, so we don't, so it's.",
            "Very easy to or relatively easy to compute the transition probabilities.",
            "All structure, move types are complemented by moves or changes in imitation probabilities.",
            "So here you can see all the structure moved."
        ],
        [
            "So we have to very very basic and elementary structure move tops, which we already heard today.",
            "I guess twice.",
            "So basically we have we can insert an edge, we can delete an edge.",
            "As conjunctive patient networks are asked at the edges in conjunctive paging networks have very very specific meaning, namely that basically the one note or one gene.",
            "Depends on another gene.",
            "It is easy or it's based very very.",
            "It's obvious in which direction a an edge has to go.",
            "So what we can compute from one network then or from one structure is a transitive closure, so we also.",
            "This is why we are introduced moves not only on on the on our Asian Network itself, but also on the transitive closure.",
            "Here is this move type.",
            "So basically we allow insert insertions and deletions of.",
            "Edges in the transitive closure.",
            "So basically we computed.",
            "We have one graph or network.",
            "We compute the transitive closure.",
            "We delete or insert an edge, not all edges, of course could be inserted or deleted and still result in a valid.",
            "Partially ordered set.",
            "And.",
            "Basically, so we recompute the subset which is allowed to to be inserted and deleted, and then we pick one or we would pick one randomly also.",
            "We have event exchange moves.",
            "So basically when we have a cascade like this we allow moves where we basically just Exchange 2.",
            "Notes, but the structure stays the same.",
            "This is also very, very useful to overcome local Optima.",
            "For example in this over here you can see that if three and two is dependent on one, but the truth about 3 is not dependent on 2 but two is dependent on three, we would not.",
            "Would usually not go from this.",
            "Graph to discraft just using insertion and deletion moves.",
            "Because we are trapped in a local optimum here.",
            "Because once we delete one of one of these edges, then four and three is no longer dependent on one.",
            "And that's very very steep Hill to climb, so we would usually not use it.",
            "So This is why we introduced this move, and we have a also another relatively complicated move very basically delete one edge and right afterwards insert another edge again.",
            "This move is very very nice because it's symmetric, so we don't need.",
            "To compute the transition probabilities.",
            "So it basically just boosts our performance a bit without costing us anything.",
            "So the."
        ],
        [
            "Adding move types are on the continuous parameters.",
            "One is to relocate.",
            "One single.",
            "Mutation probability, which we just pick from a uniform proposal.",
            "The other one is to relocate our error probability which we pick from a overdispersed prior, basically.",
            "So we.",
            "Implemented this.",
            "I'm simply scheme.",
            "We usually run multiple chains, and convergence analysis is done by the comparison of intra and Inter chain variants.",
            "So we validated this sampler based on similar.",
            "Sorry on simulated data."
        ],
        [
            "So this is the.",
            "The partially ordered set we used for generating data.",
            "The small numbers here are the mutation probabilities.",
            "We use always used for chains.",
            "We usually generated 2520, four, 25,000 samples per chain and iteration kept every 20th sample and with R sampler we usually reached convergence within five rounds we had.",
            "We also compared this to a more basic simple MCMC scheme, just using insertions and deletions.",
            "And.",
            "There we had five.",
            "Yeah, we have about five or six different simulation setups.",
            "Convergence for for the more basic sample was only reached for one of the.",
            "For one of the runs within five or six days, all the others did not converge in any reasonable amount of time.",
            "Hours usually converged within a couple of between minutes and hours, so it was reasonable."
        ],
        [
            "So here we can see the results of.",
            "The six runs, so here we have a simulation where we took 100 samples and an error probability of 10%.",
            "Here another 100 samples, an error probability of 1% hits 10%, then we had 400 cases and.",
            "800 cases again with 10% and 1%.",
            "We also simulated data from just an empty person and tried to influence the structure on an empty posted so you can see here is.",
            "The more data we have, so from from 100 to 14 cases, which is the usual amount in.",
            "In large scale cancer studies at the moment.",
            "So TCA for example usually releases data for about 3 to 400 patients.",
            "We can estimate the structure fairly well.",
            "So this are basically the probabilities the posterior probabilities of edges.",
            "So this RDD outgoing notes and these are the incoming notes, and that's basically a perfect reconstruction here.",
            "The last date that we have and the more noise we have on the data, the more difficult, of course, is to infer.",
            "The edges what you can see here also is."
        ],
        [
            "The further down the node is, or the lower the mutation probability."
        ],
        [
            "It is the more difficult it becomes to infer the edge to find the edge.",
            "Here."
        ],
        [
            "You can see density plots and trace plots of 1 specific run, specifically where we had 100 cases and error probability of 1%.",
            "So these are the three networks with the highest frequencies and MCMC samples, so this is in posterior of point 12.1 and .09.",
            "That's the original network we put in.",
            "You can see that the mixing is very nice, although from the lock posterior we still have some.",
            "Some luck in there.",
            "I.",
            "We also applied."
        ],
        [
            "Our algorithm 22 biologic later, so this is data CGH data.",
            "So comparative genome hybridization data for renal cell carcinoma.",
            "Basically this is kind of old data with a very high resolution.",
            "It's basically for for chromosome arms.",
            "We had two more than 250 cases and we only could found.",
            "Three edges which had a posterior probability higher than .5.",
            "This is interesting because there were several studies before using uncle genetic trees.",
            "Contractive paging networks in the with maximum likelihood inference where they always inferred like like 5 to 10.",
            "Dependencies, but apparently, and they didn't really quantify the uncertainty in the end of the dependencies to head, so that just stated here.",
            "Our algorithm optimizes dissenter function and these are the dependencies we get out.",
            "And Alstom interdependencies, we find they also identified, but they identified many more which.",
            "We can now see have a very very low.",
            "Posterior probability, so DEP probably not there.",
            "At least there's not enough evidence in the data.",
            "So what we have derived here is basically an MCMC scheme."
        ],
        [
            "Or for specific patient networks, basically networks where with unambiguous edge directions, and we envision that our algorithm could also be used for useful for basically all other patient networks which have this property.",
            "A problem is that it's computationally especially for our models, because we have to integrate out all these hidden.",
            "Geno types it's computationally very, very intense and we can at the moment to up to 15 loci which.",
            "Might not be that much of a problem because it is assumed that the pressure.",
            "Or the evolutionary pressure is not really unchanged, but rather acts on the level of pathways or groups.",
            "So basically, if three chains are in the same pathway and for example have linear order, it's not really important which one of them gets mutated as long as one of them gets muted.",
            "To disturb basically the function of the pathway so and there is a relatively limited amount of cancer pathways, at least according to recent studies.",
            "So another problem is the convergence calling for this structure is basically at the moment done via the proxy of the of all the continuous parameters and the lock posterior, and that's reasonable to do, but it's not a perfect way to estimate the to call the convergence.",
            "What huge plus point of this method now is, it can be properly used in predictive modeling, for example.",
            "It's interesting if one drug.",
            "What rely on on one dependency being there or not?",
            "It's very, very nice if I know how well I can trust this dependency.",
            "So this is a huge advantage if you use a Bayesian scheme to estimate those structures.",
            "Finally, I want to thank my supervisor."
        ],
        [
            "And also Nico Barnacle and also the whole Bellman could group which was very helpful with discussions of this method and you for your attention.",
            "You described the move types for MCMC, so I mean how?",
            "How depend is it on this exact choice?",
            "I mean so if you remove some of those, how different will the results be?",
            "So point is all those move types were specifically designed for specific local Optima which occur in these structures.",
            "So if you remove one of those, especially the one from the transitive closure.",
            "Then you will end up with local Optima.",
            "You will never get almost certainly never get out again, so you will just end up with them also.",
            "Then you won't reach any convergence because if you simulate a couple of chains they will end up in different in different local Optima and you will never come together.",
            "So I'm not an expert in this matter is how do you notice?",
            "I mean maybe there might be different more complex kind of local Optima which you haven't encountered well.",
            "How do you find those right?",
            "So how do you know how did you come up with those?",
            "I developed the algorithm and found that when other they couldn't recover the structure I put in or I simulated a couple of chains and the state in their their optimum and they just had some differences and check that.",
            "For example, if I look at the transitive closure, then it's a very, very minor difference.",
            "The difference in the partially ordered set is relatively severe.",
            "Don't I mean, after choosing those move types you don't find this anymore?",
            "So no, no no.",
            "OK that's good.",
            "Was wondering have you considered comparing into methods that are designed to overcome this problem but in a more context freeway?",
            "So at least two examples would be sort of annealing important sampling scheme?",
            "Or maybe one of these Gibbs samplers which can cash the cyclicity status.",
            "No I did not.",
            "So basically what was done before is in the maximum for a maximum likelihood estimation of those networks.",
            "It was usually a simulated annealing scheme and similar to needling has some problems, so you don't really know how to change temperatures you don't really know if you end up in a local optimum mean.",
            "Theoretically you don't, but in practice of course you do so this is done before and what it's nice now with this scheme is that we simulate the couple of chains and at some point we know that all of them at least maybe reach the same local optimum.",
            "But at least they reached the same end with the simulated annealing.",
            "That was not really possible.",
            "OK, so let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, what is cancer?",
                    "label": 0
                },
                {
                    "sent": "So cancer is an evolutionary of follows an evolutionary process and is characterized by the accumulation of advantageous mutations and the outgrowth of the South.",
                    "label": 1
                },
                {
                    "sent": "With those advantageous mutation in comparison to all the other cells surrounding it, it which do not have this advantageous mutation.",
                    "label": 0
                },
                {
                    "sent": "Mutations, so for lots of these mutations they are recurrent.",
                    "label": 0
                },
                {
                    "sent": "So if you look at multiple samples of the same cancer subtypes you can see the same mutations over and over again.",
                    "label": 0
                },
                {
                    "sent": "For us it's it's different and you just see them in very very small subsets of.",
                    "label": 0
                },
                {
                    "sent": "Accounts of the same type or even subtype, so it has been shown.",
                    "label": 0
                },
                {
                    "sent": "A couple of almost two decades ago now that certain mutations depend on the present presence of other mutations.",
                    "label": 1
                },
                {
                    "sent": "So for example, for a gene, in order for Gene B to contain a mutation, or to get a mutation that already has to be imitation in Chennai.",
                    "label": 0
                },
                {
                    "sent": "The order of these dependencies, or of these networks, are in general unknown orders.",
                    "label": 0
                },
                {
                    "sent": "Very, very weak idea about them.",
                    "label": 0
                },
                {
                    "sent": "So there have been a couple of.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Models on how to model.",
                    "label": 0
                },
                {
                    "sent": "These mutations are these dependencies of mutations.",
                    "label": 0
                },
                {
                    "sent": "And one of them is conjunctive Bayesian networks.",
                    "label": 0
                },
                {
                    "sent": "So here you can see a small example of a conjunctive patient network.",
                    "label": 0
                },
                {
                    "sent": "We're basically chain B and Cincy are dependent on chin, a meaning for Gene B can be a mutation and gene be there already has to be be one present in Chennai.",
                    "label": 0
                },
                {
                    "sent": "So what we usually get from from cancer patients.",
                    "label": 0
                },
                {
                    "sent": "Is cross sectional data so basically.",
                    "label": 0
                },
                {
                    "sent": "Cancer patients get diagnosed at some point of their carcinogenesis.",
                    "label": 0
                },
                {
                    "sent": "A sample is extracted.",
                    "label": 0
                },
                {
                    "sent": "Geno type is measured and basically we just see a snapshot of the cancer progression.",
                    "label": 0
                },
                {
                    "sent": "So what is the problem at hand here?",
                    "label": 0
                },
                {
                    "sent": "What is the problem that?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The address so from this basically from these sorts of data that I showed you, you can.",
                    "label": 0
                },
                {
                    "sent": "You can infer, or you can train this conjunctive patient networks.",
                    "label": 0
                },
                {
                    "sent": "What's been very, very difficult so far is to quantify the uncertainty of these structures.",
                    "label": 0
                },
                {
                    "sent": "So what we DEF do is we use Asian approach.",
                    "label": 0
                },
                {
                    "sent": "To infer those networks, and also to infer that the uncertainty what's been a problem so far.",
                    "label": 0
                },
                {
                    "sent": "And is over with sampling, and Bayesian networks is first of all.",
                    "label": 0
                },
                {
                    "sent": "They're very, very inefficient, so the samples.",
                    "label": 0
                },
                {
                    "sent": "Usually it's MCMC are very, very slow.",
                    "label": 0
                },
                {
                    "sent": "And also they tend to get stuck in local Optima.",
                    "label": 1
                },
                {
                    "sent": "So here I can show you the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The model itself, which is here.",
                    "label": 0
                },
                {
                    "sent": "So we have a dependence structure.",
                    "label": 0
                },
                {
                    "sent": "Here we have off for each note we have a conditional mutation probability seat OK, which basically means once all its parents or predecessors or mutated then this gene can mutate with probability of seat OK.",
                    "label": 0
                },
                {
                    "sent": "So here that is a true Geno type or is.",
                    "label": 0
                },
                {
                    "sent": "Yeah and basically the probability of ZT.",
                    "label": 0
                },
                {
                    "sent": "So that is a vector of like engine or engines or another.",
                    "label": 0
                },
                {
                    "sent": "Sorts of loci could be chromosome arms.",
                    "label": 0
                },
                {
                    "sent": "Could be pathways could be basically any logical entity.",
                    "label": 0
                },
                {
                    "sent": "And basically we multiply all the all the conditional probabilities of the genes which are already mutated times the.",
                    "label": 0
                },
                {
                    "sent": "Probabilities off the chains which could mutate next, which is this exit satir that they have not mutated yet.",
                    "label": 0
                },
                {
                    "sent": "So if set is or if the Geno type is compatible with our dependency structure, then we basically get this probability.",
                    "label": 0
                },
                {
                    "sent": "If it's not compatible, we get a probability of zero.",
                    "label": 0
                },
                {
                    "sent": "Of course, gene typing is very very depends on the methodology, of course, but it's very, very error prone process and This is why we model this error problem.",
                    "label": 0
                },
                {
                    "sent": "This error process separately.",
                    "label": 0
                },
                {
                    "sent": "So basically for every gene, we have a probability of epsilon that we misdiagnosed it basically and a probability of 1 minus epsilon that it was correct.",
                    "label": 0
                },
                {
                    "sent": "Here the is the Hamming distance between the observed Geno type and the true hidden genotype.",
                    "label": 1
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "We basically here we have the likelihood we.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Integrate out all the hidden Geno types.",
                    "label": 0
                },
                {
                    "sent": "And of course we do not only have one observation, but we have a number of observations and we just iterate over them.",
                    "label": 0
                },
                {
                    "sent": "Here is the full posterior, so by full posterior I mean the structure of the mutation probabilities.",
                    "label": 0
                },
                {
                    "sent": "And the error probability.",
                    "label": 0
                },
                {
                    "sent": "We use a couple of, of course, of priors, so we have an informal, uninformative.",
                    "label": 0
                },
                {
                    "sent": "Improper prior for the structure.",
                    "label": 0
                },
                {
                    "sent": "We have an uninformative prior for the separate mutation probabilities, and we have a weak but informative prior for the excuse me.",
                    "label": 0
                },
                {
                    "sent": "Error probability.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we derived a metropolis Hastings within Gibbs sampling scheme.",
                    "label": 0
                },
                {
                    "sent": "It's basically a hybrid sampler sampler.",
                    "label": 1
                },
                {
                    "sent": "We mix 8.",
                    "label": 0
                },
                {
                    "sent": "Different structure move eight different move types.",
                    "label": 0
                },
                {
                    "sent": "Six of them are structure, move types and two of them deal with our continuous parameters.",
                    "label": 0
                },
                {
                    "sent": "All the asymmetric structure move types have disjoint neighborhoods, so we don't, so it's.",
                    "label": 1
                },
                {
                    "sent": "Very easy to or relatively easy to compute the transition probabilities.",
                    "label": 1
                },
                {
                    "sent": "All structure, move types are complemented by moves or changes in imitation probabilities.",
                    "label": 0
                },
                {
                    "sent": "So here you can see all the structure moved.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have to very very basic and elementary structure move tops, which we already heard today.",
                    "label": 0
                },
                {
                    "sent": "I guess twice.",
                    "label": 0
                },
                {
                    "sent": "So basically we have we can insert an edge, we can delete an edge.",
                    "label": 0
                },
                {
                    "sent": "As conjunctive patient networks are asked at the edges in conjunctive paging networks have very very specific meaning, namely that basically the one note or one gene.",
                    "label": 0
                },
                {
                    "sent": "Depends on another gene.",
                    "label": 0
                },
                {
                    "sent": "It is easy or it's based very very.",
                    "label": 0
                },
                {
                    "sent": "It's obvious in which direction a an edge has to go.",
                    "label": 0
                },
                {
                    "sent": "So what we can compute from one network then or from one structure is a transitive closure, so we also.",
                    "label": 0
                },
                {
                    "sent": "This is why we are introduced moves not only on on the on our Asian Network itself, but also on the transitive closure.",
                    "label": 0
                },
                {
                    "sent": "Here is this move type.",
                    "label": 0
                },
                {
                    "sent": "So basically we allow insert insertions and deletions of.",
                    "label": 0
                },
                {
                    "sent": "Edges in the transitive closure.",
                    "label": 0
                },
                {
                    "sent": "So basically we computed.",
                    "label": 0
                },
                {
                    "sent": "We have one graph or network.",
                    "label": 0
                },
                {
                    "sent": "We compute the transitive closure.",
                    "label": 0
                },
                {
                    "sent": "We delete or insert an edge, not all edges, of course could be inserted or deleted and still result in a valid.",
                    "label": 0
                },
                {
                    "sent": "Partially ordered set.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Basically, so we recompute the subset which is allowed to to be inserted and deleted, and then we pick one or we would pick one randomly also.",
                    "label": 0
                },
                {
                    "sent": "We have event exchange moves.",
                    "label": 0
                },
                {
                    "sent": "So basically when we have a cascade like this we allow moves where we basically just Exchange 2.",
                    "label": 0
                },
                {
                    "sent": "Notes, but the structure stays the same.",
                    "label": 0
                },
                {
                    "sent": "This is also very, very useful to overcome local Optima.",
                    "label": 0
                },
                {
                    "sent": "For example in this over here you can see that if three and two is dependent on one, but the truth about 3 is not dependent on 2 but two is dependent on three, we would not.",
                    "label": 0
                },
                {
                    "sent": "Would usually not go from this.",
                    "label": 0
                },
                {
                    "sent": "Graph to discraft just using insertion and deletion moves.",
                    "label": 0
                },
                {
                    "sent": "Because we are trapped in a local optimum here.",
                    "label": 0
                },
                {
                    "sent": "Because once we delete one of one of these edges, then four and three is no longer dependent on one.",
                    "label": 0
                },
                {
                    "sent": "And that's very very steep Hill to climb, so we would usually not use it.",
                    "label": 0
                },
                {
                    "sent": "So This is why we introduced this move, and we have a also another relatively complicated move very basically delete one edge and right afterwards insert another edge again.",
                    "label": 0
                },
                {
                    "sent": "This move is very very nice because it's symmetric, so we don't need.",
                    "label": 0
                },
                {
                    "sent": "To compute the transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "So it basically just boosts our performance a bit without costing us anything.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Adding move types are on the continuous parameters.",
                    "label": 1
                },
                {
                    "sent": "One is to relocate.",
                    "label": 0
                },
                {
                    "sent": "One single.",
                    "label": 0
                },
                {
                    "sent": "Mutation probability, which we just pick from a uniform proposal.",
                    "label": 0
                },
                {
                    "sent": "The other one is to relocate our error probability which we pick from a overdispersed prior, basically.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Implemented this.",
                    "label": 0
                },
                {
                    "sent": "I'm simply scheme.",
                    "label": 0
                },
                {
                    "sent": "We usually run multiple chains, and convergence analysis is done by the comparison of intra and Inter chain variants.",
                    "label": 1
                },
                {
                    "sent": "So we validated this sampler based on similar.",
                    "label": 0
                },
                {
                    "sent": "Sorry on simulated data.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "The partially ordered set we used for generating data.",
                    "label": 0
                },
                {
                    "sent": "The small numbers here are the mutation probabilities.",
                    "label": 0
                },
                {
                    "sent": "We use always used for chains.",
                    "label": 0
                },
                {
                    "sent": "We usually generated 2520, four, 25,000 samples per chain and iteration kept every 20th sample and with R sampler we usually reached convergence within five rounds we had.",
                    "label": 1
                },
                {
                    "sent": "We also compared this to a more basic simple MCMC scheme, just using insertions and deletions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "There we had five.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we have about five or six different simulation setups.",
                    "label": 0
                },
                {
                    "sent": "Convergence for for the more basic sample was only reached for one of the.",
                    "label": 0
                },
                {
                    "sent": "For one of the runs within five or six days, all the others did not converge in any reasonable amount of time.",
                    "label": 0
                },
                {
                    "sent": "Hours usually converged within a couple of between minutes and hours, so it was reasonable.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we can see the results of.",
                    "label": 0
                },
                {
                    "sent": "The six runs, so here we have a simulation where we took 100 samples and an error probability of 10%.",
                    "label": 0
                },
                {
                    "sent": "Here another 100 samples, an error probability of 1% hits 10%, then we had 400 cases and.",
                    "label": 0
                },
                {
                    "sent": "800 cases again with 10% and 1%.",
                    "label": 0
                },
                {
                    "sent": "We also simulated data from just an empty person and tried to influence the structure on an empty posted so you can see here is.",
                    "label": 0
                },
                {
                    "sent": "The more data we have, so from from 100 to 14 cases, which is the usual amount in.",
                    "label": 0
                },
                {
                    "sent": "In large scale cancer studies at the moment.",
                    "label": 0
                },
                {
                    "sent": "So TCA for example usually releases data for about 3 to 400 patients.",
                    "label": 0
                },
                {
                    "sent": "We can estimate the structure fairly well.",
                    "label": 0
                },
                {
                    "sent": "So this are basically the probabilities the posterior probabilities of edges.",
                    "label": 0
                },
                {
                    "sent": "So this RDD outgoing notes and these are the incoming notes, and that's basically a perfect reconstruction here.",
                    "label": 0
                },
                {
                    "sent": "The last date that we have and the more noise we have on the data, the more difficult, of course, is to infer.",
                    "label": 0
                },
                {
                    "sent": "The edges what you can see here also is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The further down the node is, or the lower the mutation probability.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is the more difficult it becomes to infer the edge to find the edge.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see density plots and trace plots of 1 specific run, specifically where we had 100 cases and error probability of 1%.",
                    "label": 0
                },
                {
                    "sent": "So these are the three networks with the highest frequencies and MCMC samples, so this is in posterior of point 12.1 and .09.",
                    "label": 0
                },
                {
                    "sent": "That's the original network we put in.",
                    "label": 0
                },
                {
                    "sent": "You can see that the mixing is very nice, although from the lock posterior we still have some.",
                    "label": 0
                },
                {
                    "sent": "Some luck in there.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "We also applied.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our algorithm 22 biologic later, so this is data CGH data.",
                    "label": 0
                },
                {
                    "sent": "So comparative genome hybridization data for renal cell carcinoma.",
                    "label": 0
                },
                {
                    "sent": "Basically this is kind of old data with a very high resolution.",
                    "label": 0
                },
                {
                    "sent": "It's basically for for chromosome arms.",
                    "label": 0
                },
                {
                    "sent": "We had two more than 250 cases and we only could found.",
                    "label": 0
                },
                {
                    "sent": "Three edges which had a posterior probability higher than .5.",
                    "label": 0
                },
                {
                    "sent": "This is interesting because there were several studies before using uncle genetic trees.",
                    "label": 0
                },
                {
                    "sent": "Contractive paging networks in the with maximum likelihood inference where they always inferred like like 5 to 10.",
                    "label": 0
                },
                {
                    "sent": "Dependencies, but apparently, and they didn't really quantify the uncertainty in the end of the dependencies to head, so that just stated here.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm optimizes dissenter function and these are the dependencies we get out.",
                    "label": 0
                },
                {
                    "sent": "And Alstom interdependencies, we find they also identified, but they identified many more which.",
                    "label": 0
                },
                {
                    "sent": "We can now see have a very very low.",
                    "label": 0
                },
                {
                    "sent": "Posterior probability, so DEP probably not there.",
                    "label": 0
                },
                {
                    "sent": "At least there's not enough evidence in the data.",
                    "label": 0
                },
                {
                    "sent": "So what we have derived here is basically an MCMC scheme.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or for specific patient networks, basically networks where with unambiguous edge directions, and we envision that our algorithm could also be used for useful for basically all other patient networks which have this property.",
                    "label": 1
                },
                {
                    "sent": "A problem is that it's computationally especially for our models, because we have to integrate out all these hidden.",
                    "label": 1
                },
                {
                    "sent": "Geno types it's computationally very, very intense and we can at the moment to up to 15 loci which.",
                    "label": 0
                },
                {
                    "sent": "Might not be that much of a problem because it is assumed that the pressure.",
                    "label": 0
                },
                {
                    "sent": "Or the evolutionary pressure is not really unchanged, but rather acts on the level of pathways or groups.",
                    "label": 0
                },
                {
                    "sent": "So basically, if three chains are in the same pathway and for example have linear order, it's not really important which one of them gets mutated as long as one of them gets muted.",
                    "label": 0
                },
                {
                    "sent": "To disturb basically the function of the pathway so and there is a relatively limited amount of cancer pathways, at least according to recent studies.",
                    "label": 1
                },
                {
                    "sent": "So another problem is the convergence calling for this structure is basically at the moment done via the proxy of the of all the continuous parameters and the lock posterior, and that's reasonable to do, but it's not a perfect way to estimate the to call the convergence.",
                    "label": 0
                },
                {
                    "sent": "What huge plus point of this method now is, it can be properly used in predictive modeling, for example.",
                    "label": 0
                },
                {
                    "sent": "It's interesting if one drug.",
                    "label": 0
                },
                {
                    "sent": "What rely on on one dependency being there or not?",
                    "label": 0
                },
                {
                    "sent": "It's very, very nice if I know how well I can trust this dependency.",
                    "label": 0
                },
                {
                    "sent": "So this is a huge advantage if you use a Bayesian scheme to estimate those structures.",
                    "label": 0
                },
                {
                    "sent": "Finally, I want to thank my supervisor.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also Nico Barnacle and also the whole Bellman could group which was very helpful with discussions of this method and you for your attention.",
                    "label": 0
                },
                {
                    "sent": "You described the move types for MCMC, so I mean how?",
                    "label": 0
                },
                {
                    "sent": "How depend is it on this exact choice?",
                    "label": 0
                },
                {
                    "sent": "I mean so if you remove some of those, how different will the results be?",
                    "label": 0
                },
                {
                    "sent": "So point is all those move types were specifically designed for specific local Optima which occur in these structures.",
                    "label": 0
                },
                {
                    "sent": "So if you remove one of those, especially the one from the transitive closure.",
                    "label": 0
                },
                {
                    "sent": "Then you will end up with local Optima.",
                    "label": 0
                },
                {
                    "sent": "You will never get almost certainly never get out again, so you will just end up with them also.",
                    "label": 0
                },
                {
                    "sent": "Then you won't reach any convergence because if you simulate a couple of chains they will end up in different in different local Optima and you will never come together.",
                    "label": 0
                },
                {
                    "sent": "So I'm not an expert in this matter is how do you notice?",
                    "label": 0
                },
                {
                    "sent": "I mean maybe there might be different more complex kind of local Optima which you haven't encountered well.",
                    "label": 0
                },
                {
                    "sent": "How do you find those right?",
                    "label": 0
                },
                {
                    "sent": "So how do you know how did you come up with those?",
                    "label": 0
                },
                {
                    "sent": "I developed the algorithm and found that when other they couldn't recover the structure I put in or I simulated a couple of chains and the state in their their optimum and they just had some differences and check that.",
                    "label": 0
                },
                {
                    "sent": "For example, if I look at the transitive closure, then it's a very, very minor difference.",
                    "label": 0
                },
                {
                    "sent": "The difference in the partially ordered set is relatively severe.",
                    "label": 0
                },
                {
                    "sent": "Don't I mean, after choosing those move types you don't find this anymore?",
                    "label": 0
                },
                {
                    "sent": "So no, no no.",
                    "label": 0
                },
                {
                    "sent": "OK that's good.",
                    "label": 0
                },
                {
                    "sent": "Was wondering have you considered comparing into methods that are designed to overcome this problem but in a more context freeway?",
                    "label": 0
                },
                {
                    "sent": "So at least two examples would be sort of annealing important sampling scheme?",
                    "label": 0
                },
                {
                    "sent": "Or maybe one of these Gibbs samplers which can cash the cyclicity status.",
                    "label": 0
                },
                {
                    "sent": "No I did not.",
                    "label": 0
                },
                {
                    "sent": "So basically what was done before is in the maximum for a maximum likelihood estimation of those networks.",
                    "label": 0
                },
                {
                    "sent": "It was usually a simulated annealing scheme and similar to needling has some problems, so you don't really know how to change temperatures you don't really know if you end up in a local optimum mean.",
                    "label": 0
                },
                {
                    "sent": "Theoretically you don't, but in practice of course you do so this is done before and what it's nice now with this scheme is that we simulate the couple of chains and at some point we know that all of them at least maybe reach the same local optimum.",
                    "label": 0
                },
                {
                    "sent": "But at least they reached the same end with the simulated annealing.",
                    "label": 0
                },
                {
                    "sent": "That was not really possible.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}