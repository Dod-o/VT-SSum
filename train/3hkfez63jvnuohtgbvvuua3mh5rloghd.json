{
    "id": "3hkfez63jvnuohtgbvvuua3mh5rloghd",
    "title": "Learning with Dual Heterogeneity: A Nonparametric Bayes Model",
    "info": {
        "author": [
            "Hongxia Yang, IBM Thomas J. Watson Research Center"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_yang_dual_heterogeneity/",
    "segmentation": [
        [
            "Today I'm going to talk about learning dear Heterogeneities through a nonparametric Bayesian framework.",
            "This is a collaborator work with generator from Arizona State University."
        ],
        [
            "So what is heterogeneity?",
            "This is actually an antonym for the homogeneity, which is the basic requirement for the many statistical modeling, and there is multiple types of heterogeneities, for example like task heterogeneity, view heterogeneity or instance heterogeneity, and in statistics we usually provide general framework called hierarchical modeling to account for the different heterogeneities and immersion learning.",
            "People probably have a more detailed you know characterization and mobilization of these different frameworks.",
            "For example, like the multi task learning, multi view learning etc."
        ],
        [
            "So this is just a visualization of the overlapping of the different heterogeneities, so people don't usually focus on only one heterogeneity.",
            "But they probably want to joint learning multiple heterogeneities, so I just, you know, visualize three heterogeneities here.",
            "And there are multiple other combinations of the different heterogeneities."
        ],
        [
            "There are many different applications in the real life that accounts for the different heterogeneities.",
            "For example, like the story that we are talking, we are discussing in our paper our work like the 20 newsgroups data sets.",
            "So we collect the articles from the 20 newsgroups and we want to characterize if that article belongs to a specific culturization character character or the web datasets.",
            "For example, we collect the web pages from the four different universities.",
            "And we want to characterize if that web page is a course web page or not, or the email spam.",
            "You know these datasets.",
            "For example, we collect the emails from the different users and we want to characterize for specific user force basic email for specific user.",
            "If that email is belong to our spam or ham."
        ],
        [
            "So this is the general road map of our of my work of my talk.",
            "In the first part I will give you a brief introduction.",
            "In the second part I will talk about the Multi View Multi task learning that we focus on and especially will speak some challenges still existing in these areas and based on these challenges, the proposed framework that is the nonparametric based on learning with the deal heterogeneity and ended with some conclusions."
        ],
        [
            "So this is just the plots that I shows and in our framework.",
            "In our talk I'll just focus will focus on this task heterogeneity and we'll heterogeneity and potentially it can be extended to other, you know, heterogeneity studies jointly."
        ],
        [
            "Ming.",
            "So actually I still this, you know example from generate.",
            "I think this is very insightful, explicit representation of this multi task.",
            "Multi view learning through a very simple, you know illustration so for."
        ],
        [
            "How if we know that the Queen is a bad guy and the Snow White is a very good guy?",
            "And we want to know if the third person Doc is a good guy or a bad guy.",
            "And also with some other examples and some others.",
            "So the right side, the Westlake beauty of warfare.",
            "They are very, you know, famous Chinese cartoon characteristics.",
            "And also we want to learn in another task if this guys or these examples are good guy or bad guys.",
            "And we know there is some shared shared features and some specific features belongs to the specific example in that specific task learning.",
            "And we know how."
        ],
        [
            "It's obvious you know are related to each of these examples in that task.",
            "For example, some views can be shared with some specific examples in a in the same task, and some views can be specific for only one V1 example in that task, and these views can be shared across these tasks.",
            "So we call.",
            "This is the view heterogeneity."
        ],
        [
            "So for example, we already know this is the bad guy and this is a good guy.",
            "How can we infer through sharing the information from the multiple multiple views and multiple tasks to infer the third guy is a good guy or bad guy and also from the other examples in another task?"
        ],
        [
            "So our framework so as well as I just showed you so these views can be completely, you know, overlapped or only partially overlapped or just totally different from each other and our modeling framework is based on the middle so that they can have some shared views and also some specific views."
        ],
        [
            "OK so I just talked about the introduction for this multi task multi view learning this framework that we are considering.",
            "Now I'm going to talk about, you know the modeling that modeling methodology is that we would like to adopt.",
            "So before that let me quickly help you go through this additional process prior which are very common you know or are very popular methodology currently in statistics.",
            "So broadly speaking statistics can be divided into frequentist statistics and Bayesian statistics.",
            "And regarding Bayesian statistics we can also brother like you.",
            "Divide it to power metric based on statistics and nonparametric Bayesian statistics.",
            "For the nonparametric Bayesian statistics, it is actually a misnomer.",
            "It doesn't mean that we don't have any parameters, but in actual we actually had has an infinite number of parameters that can be considered for that model.",
            "So the benefit for the nonparametric Bayesian statistics is that we can use this methodology to model if the data is multimode nonsymmetric flat tails.",
            "So many different, you know benefits.",
            "So for the digital process models we can model the underlying prior density P for the parameters that we are interested in.",
            "And especially we can actually analytically integrated out the conditional distribution and have the joint or the conditional distribution of the parameters has this following Poly urn distribution and using this distribution.",
            "It is very clearly to see that.",
            "Is had actually the classroom property.",
            "For example, the new parameter can be drawn can be drawn from a soojin.",
            "Out here is the best measure that you can generate the new parameters and Alpha here.",
            "As we can tell, is the prior probability that it can be assigned to a new cluster.",
            "So as we can tell here, the natural output of additional processes that we can cluster the coefficients that we are interested in so.",
            "The next step is what we called.",
            "Additionally, process mixture model.",
            "So we actually there's a more hierarchical level.",
            "So in this sense we can actually cluster the response based on clustering the prior of based on the parameters that we are interested in.",
            "However, there are some limitations for the digital process prior.",
            "For example, it does not allow local classroom of tasks or views.",
            "What does this mean?",
            "For example, if we know we have some prior information that some views are correlated to each other, and however if we just directly apply the digital process mixture originally process prior, we are actually priorly in dependently.",
            "That's what we don't want here."
        ],
        [
            "So before we move on to the models, there's some notations here.",
            "Suppose we have key tasks and videos in total, and for this video we have DB features and EV features can be different from each other and for these tasks there are anti examples with labels with different labels.",
            "So this is the views that are stuck together and without loss of general generality we know the output only for the first few empty examples and for the other.",
            "In T minus empty examples, we don't know the labels and our goal here is to leverage both the label information from all the related tasks as well as to learn the consistency.",
            "The different consistencies from the different views."
        ],
        [
            "So we first decompose each task into multiple single view models as shown here.",
            "So this is a like a similar flavor to the example modeling statistics.",
            "And the activity here is a vector stands for the coefficient vectors that accounts for the variability of the views and absolute TV.",
            "Here is the observational error.",
            "So based on the above model, we estimate the task relatedness as well as the.",
            "You know, the real consistency's."
        ],
        [
            "So how we model the task relatedness?",
            "We model this through this.",
            "You know multivariate distribution for the error part, so we introduce we like this epsilon us.",
            "So absolute as is the T by one vector and each node accounts for the task account for a specific task.",
            "So we let this joint distribution follow a multivariate normal distribution with mean zero and covariance KE here and how shall we define this case so that we can borrow information?",
            "Across the different tasks.",
            "So we start from here.",
            "So we from this adjacency matrix.",
            "BB is defined through through this this equation and we can tell that we also include the examples from the unlabeled data.",
            "So for each specific task, only empty examples are labeled and then we can compute the Laplacian Delta through D minor speed and D is a diagonal matrix with the diagonal equals the sum of each row.",
            "And finally we obtain the speed.",
            "This is the beta multiplied by this lab duction matrix plus one over Sigma squared multiplied by I. I is the identity matrix here and we have less notice that bit here actually controls the sharpness of the distributions.",
            "The larger of the beta values the distribution is more picked and the Sigma square controls the regularization of this distribution.",
            "And because we are basing statisticians will actually put priors for this beta and Sigma Square.",
            "Instead of predefined or through some cross validation techniques to learn it so beta, we put a gamma distribution and Sigma square root inverse gamma distributions."
        ],
        [
            "So we just want to, you know, highlight several points here so this is actually a global relatedness because K is obtained from the inverse distributive inverse of sum.",
            "This is so or we called the precision precision matrices.",
            "So if one of the components of this K is zero, it stands for the conditional distribution conditional independence given or other low or or the left tasks, and especially to learn this K we actually include or or the information from the data, even the unlabeled data.",
            "So it's more robust to noise and more reliable."
        ],
        [
            "So regarding the view consistency, as I just mentioned, we want to cluster the views.",
            "So how should we cluster rules?",
            "We actually clustered views through clustered hyper priors or the covariances for this real coefficient vectors F. So, so we actually like this FTV up to FT FT1, up to FPV drone flight model from a multivariate normal distribution and for example beside 11, here is the covariance matrix stands for how the view one.",
            "Sorry, let's put aside 1 two here is the covariance matrix stands for how the view one and view two.",
            "They are correlated with each other.",
            "So as I just mentioned, because the digital process is not good, the prior is actually independent with each other.",
            "And what we want is not this.",
            "For example, we extend this to a matrix digital process."
        ],
        [
            "So we assume that precise VV prime, independently drawn from a distribution at V Prime, and so this is if we see the second you know formulation here.",
            "This is just the you know stick breaking process of judicial process prior.",
            "If we give this WV prime a beta prior.",
            "However, as I just as I just showed, there's some disadvantages here, so we want to show is that we want to decompose this beta weights to this gamma VH multiplied by gamma V prime H, and each gamma VH.",
            "Stands for.",
            "Stands for the different ways coming from a specific views VVV prime and we can show that the summation of this go back to this original weights is still to be one.",
            "So it's you know validated formula decomposition and formulation and what this is what we want.",
            "So if just Additionally process prior, this will be equivalent.",
            "However, we know that if we know that there are some, you know correlating information between this V, V2 and V3, we want this.",
            "You have a higher probability that.",
            "This the first can be clustered together, so that's what we want.",
            "And because and because of this formulation we can realize that."
        ],
        [
            "So, and especially regarding the base measure, denote we actually give a degenerate distribution that can be forced totally to be 0.",
            "In that sense, you know some view with some other specific views can be totally independent with each other."
        ],
        [
            "So this is just a graphical representation of our framework, so this agree one stands for the data that we observed and this K&V are the direct coefficients that were interested in and we learn the real consistency through this coefficient, FB and task relatedness through this covariance matrix K for the for the for the response and this yellow part are the direct priors and this green green ones are the hyper priors imbedded in the in the modeling."
        ],
        [
            "So we compare our results with some state of the art methodology.",
            "So first you know, as we can show.",
            "So this is our methodology and as you can tell, this is our consistently better compared to the state of the art methodology.",
            "So we compared with some regularised multi task multi view learning through some Co regularization and some basic framework using this semi supervised for the multi task, multi view learning and some novel you know regularizer for this existing AOC methodology."
        ],
        [
            "And through these three methodology three datasets we are in the."
        ],
        [
            "Consistently better compared to our competitors."
        ],
        [
            "So just to quickly generalize.",
            "So this talk we want to solve this problem, so we don't want to.",
            "You know, we want to study from the data how they saw you know tasks are related and how these views are consistently.",
            "What's their consistency?",
            "We don't want to use the predefined are you know Clarence, matrices?",
            "We want to order to use the data twice, so and also we don't want to over fit, so that's the reason we built our methodology totally based on the basic framework and especially.",
            "Because some some data augmentation techniques.",
            "This modeling is realized through an efficient, very efficient Gibbs sampler algorithm.",
            "And also I did not show here back in our paper.",
            "We also showed the heating plots for the clustering of the different views.",
            "That's also another benefit of this methodology, and especially we show some benefit competitive results compared to some state of the art methods in our paper.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today I'm going to talk about learning dear Heterogeneities through a nonparametric Bayesian framework.",
                    "label": 0
                },
                {
                    "sent": "This is a collaborator work with generator from Arizona State University.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is heterogeneity?",
                    "label": 0
                },
                {
                    "sent": "This is actually an antonym for the homogeneity, which is the basic requirement for the many statistical modeling, and there is multiple types of heterogeneities, for example like task heterogeneity, view heterogeneity or instance heterogeneity, and in statistics we usually provide general framework called hierarchical modeling to account for the different heterogeneities and immersion learning.",
                    "label": 1
                },
                {
                    "sent": "People probably have a more detailed you know characterization and mobilization of these different frameworks.",
                    "label": 0
                },
                {
                    "sent": "For example, like the multi task learning, multi view learning etc.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just a visualization of the overlapping of the different heterogeneities, so people don't usually focus on only one heterogeneity.",
                    "label": 0
                },
                {
                    "sent": "But they probably want to joint learning multiple heterogeneities, so I just, you know, visualize three heterogeneities here.",
                    "label": 0
                },
                {
                    "sent": "And there are multiple other combinations of the different heterogeneities.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are many different applications in the real life that accounts for the different heterogeneities.",
                    "label": 0
                },
                {
                    "sent": "For example, like the story that we are talking, we are discussing in our paper our work like the 20 newsgroups data sets.",
                    "label": 0
                },
                {
                    "sent": "So we collect the articles from the 20 newsgroups and we want to characterize if that article belongs to a specific culturization character character or the web datasets.",
                    "label": 0
                },
                {
                    "sent": "For example, we collect the web pages from the four different universities.",
                    "label": 0
                },
                {
                    "sent": "And we want to characterize if that web page is a course web page or not, or the email spam.",
                    "label": 1
                },
                {
                    "sent": "You know these datasets.",
                    "label": 0
                },
                {
                    "sent": "For example, we collect the emails from the different users and we want to characterize for specific user force basic email for specific user.",
                    "label": 0
                },
                {
                    "sent": "If that email is belong to our spam or ham.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the general road map of our of my work of my talk.",
                    "label": 0
                },
                {
                    "sent": "In the first part I will give you a brief introduction.",
                    "label": 0
                },
                {
                    "sent": "In the second part I will talk about the Multi View Multi task learning that we focus on and especially will speak some challenges still existing in these areas and based on these challenges, the proposed framework that is the nonparametric based on learning with the deal heterogeneity and ended with some conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just the plots that I shows and in our framework.",
                    "label": 0
                },
                {
                    "sent": "In our talk I'll just focus will focus on this task heterogeneity and we'll heterogeneity and potentially it can be extended to other, you know, heterogeneity studies jointly.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ming.",
                    "label": 0
                },
                {
                    "sent": "So actually I still this, you know example from generate.",
                    "label": 0
                },
                {
                    "sent": "I think this is very insightful, explicit representation of this multi task.",
                    "label": 0
                },
                {
                    "sent": "Multi view learning through a very simple, you know illustration so for.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How if we know that the Queen is a bad guy and the Snow White is a very good guy?",
                    "label": 1
                },
                {
                    "sent": "And we want to know if the third person Doc is a good guy or a bad guy.",
                    "label": 0
                },
                {
                    "sent": "And also with some other examples and some others.",
                    "label": 0
                },
                {
                    "sent": "So the right side, the Westlake beauty of warfare.",
                    "label": 0
                },
                {
                    "sent": "They are very, you know, famous Chinese cartoon characteristics.",
                    "label": 0
                },
                {
                    "sent": "And also we want to learn in another task if this guys or these examples are good guy or bad guys.",
                    "label": 0
                },
                {
                    "sent": "And we know there is some shared shared features and some specific features belongs to the specific example in that specific task learning.",
                    "label": 0
                },
                {
                    "sent": "And we know how.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's obvious you know are related to each of these examples in that task.",
                    "label": 0
                },
                {
                    "sent": "For example, some views can be shared with some specific examples in a in the same task, and some views can be specific for only one V1 example in that task, and these views can be shared across these tasks.",
                    "label": 0
                },
                {
                    "sent": "So we call.",
                    "label": 0
                },
                {
                    "sent": "This is the view heterogeneity.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, we already know this is the bad guy and this is a good guy.",
                    "label": 0
                },
                {
                    "sent": "How can we infer through sharing the information from the multiple multiple views and multiple tasks to infer the third guy is a good guy or bad guy and also from the other examples in another task?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our framework so as well as I just showed you so these views can be completely, you know, overlapped or only partially overlapped or just totally different from each other and our modeling framework is based on the middle so that they can have some shared views and also some specific views.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I just talked about the introduction for this multi task multi view learning this framework that we are considering.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to talk about, you know the modeling that modeling methodology is that we would like to adopt.",
                    "label": 0
                },
                {
                    "sent": "So before that let me quickly help you go through this additional process prior which are very common you know or are very popular methodology currently in statistics.",
                    "label": 0
                },
                {
                    "sent": "So broadly speaking statistics can be divided into frequentist statistics and Bayesian statistics.",
                    "label": 0
                },
                {
                    "sent": "And regarding Bayesian statistics we can also brother like you.",
                    "label": 0
                },
                {
                    "sent": "Divide it to power metric based on statistics and nonparametric Bayesian statistics.",
                    "label": 0
                },
                {
                    "sent": "For the nonparametric Bayesian statistics, it is actually a misnomer.",
                    "label": 0
                },
                {
                    "sent": "It doesn't mean that we don't have any parameters, but in actual we actually had has an infinite number of parameters that can be considered for that model.",
                    "label": 0
                },
                {
                    "sent": "So the benefit for the nonparametric Bayesian statistics is that we can use this methodology to model if the data is multimode nonsymmetric flat tails.",
                    "label": 0
                },
                {
                    "sent": "So many different, you know benefits.",
                    "label": 0
                },
                {
                    "sent": "So for the digital process models we can model the underlying prior density P for the parameters that we are interested in.",
                    "label": 0
                },
                {
                    "sent": "And especially we can actually analytically integrated out the conditional distribution and have the joint or the conditional distribution of the parameters has this following Poly urn distribution and using this distribution.",
                    "label": 1
                },
                {
                    "sent": "It is very clearly to see that.",
                    "label": 0
                },
                {
                    "sent": "Is had actually the classroom property.",
                    "label": 0
                },
                {
                    "sent": "For example, the new parameter can be drawn can be drawn from a soojin.",
                    "label": 0
                },
                {
                    "sent": "Out here is the best measure that you can generate the new parameters and Alpha here.",
                    "label": 1
                },
                {
                    "sent": "As we can tell, is the prior probability that it can be assigned to a new cluster.",
                    "label": 0
                },
                {
                    "sent": "So as we can tell here, the natural output of additional processes that we can cluster the coefficients that we are interested in so.",
                    "label": 0
                },
                {
                    "sent": "The next step is what we called.",
                    "label": 0
                },
                {
                    "sent": "Additionally, process mixture model.",
                    "label": 0
                },
                {
                    "sent": "So we actually there's a more hierarchical level.",
                    "label": 0
                },
                {
                    "sent": "So in this sense we can actually cluster the response based on clustering the prior of based on the parameters that we are interested in.",
                    "label": 0
                },
                {
                    "sent": "However, there are some limitations for the digital process prior.",
                    "label": 1
                },
                {
                    "sent": "For example, it does not allow local classroom of tasks or views.",
                    "label": 0
                },
                {
                    "sent": "What does this mean?",
                    "label": 0
                },
                {
                    "sent": "For example, if we know we have some prior information that some views are correlated to each other, and however if we just directly apply the digital process mixture originally process prior, we are actually priorly in dependently.",
                    "label": 0
                },
                {
                    "sent": "That's what we don't want here.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before we move on to the models, there's some notations here.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have key tasks and videos in total, and for this video we have DB features and EV features can be different from each other and for these tasks there are anti examples with labels with different labels.",
                    "label": 0
                },
                {
                    "sent": "So this is the views that are stuck together and without loss of general generality we know the output only for the first few empty examples and for the other.",
                    "label": 1
                },
                {
                    "sent": "In T minus empty examples, we don't know the labels and our goal here is to leverage both the label information from all the related tasks as well as to learn the consistency.",
                    "label": 1
                },
                {
                    "sent": "The different consistencies from the different views.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we first decompose each task into multiple single view models as shown here.",
                    "label": 1
                },
                {
                    "sent": "So this is a like a similar flavor to the example modeling statistics.",
                    "label": 0
                },
                {
                    "sent": "And the activity here is a vector stands for the coefficient vectors that accounts for the variability of the views and absolute TV.",
                    "label": 1
                },
                {
                    "sent": "Here is the observational error.",
                    "label": 0
                },
                {
                    "sent": "So based on the above model, we estimate the task relatedness as well as the.",
                    "label": 1
                },
                {
                    "sent": "You know, the real consistency's.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how we model the task relatedness?",
                    "label": 1
                },
                {
                    "sent": "We model this through this.",
                    "label": 0
                },
                {
                    "sent": "You know multivariate distribution for the error part, so we introduce we like this epsilon us.",
                    "label": 0
                },
                {
                    "sent": "So absolute as is the T by one vector and each node accounts for the task account for a specific task.",
                    "label": 0
                },
                {
                    "sent": "So we let this joint distribution follow a multivariate normal distribution with mean zero and covariance KE here and how shall we define this case so that we can borrow information?",
                    "label": 0
                },
                {
                    "sent": "Across the different tasks.",
                    "label": 0
                },
                {
                    "sent": "So we start from here.",
                    "label": 0
                },
                {
                    "sent": "So we from this adjacency matrix.",
                    "label": 1
                },
                {
                    "sent": "BB is defined through through this this equation and we can tell that we also include the examples from the unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "So for each specific task, only empty examples are labeled and then we can compute the Laplacian Delta through D minor speed and D is a diagonal matrix with the diagonal equals the sum of each row.",
                    "label": 1
                },
                {
                    "sent": "And finally we obtain the speed.",
                    "label": 1
                },
                {
                    "sent": "This is the beta multiplied by this lab duction matrix plus one over Sigma squared multiplied by I. I is the identity matrix here and we have less notice that bit here actually controls the sharpness of the distributions.",
                    "label": 0
                },
                {
                    "sent": "The larger of the beta values the distribution is more picked and the Sigma square controls the regularization of this distribution.",
                    "label": 0
                },
                {
                    "sent": "And because we are basing statisticians will actually put priors for this beta and Sigma Square.",
                    "label": 0
                },
                {
                    "sent": "Instead of predefined or through some cross validation techniques to learn it so beta, we put a gamma distribution and Sigma square root inverse gamma distributions.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we just want to, you know, highlight several points here so this is actually a global relatedness because K is obtained from the inverse distributive inverse of sum.",
                    "label": 1
                },
                {
                    "sent": "This is so or we called the precision precision matrices.",
                    "label": 1
                },
                {
                    "sent": "So if one of the components of this K is zero, it stands for the conditional distribution conditional independence given or other low or or the left tasks, and especially to learn this K we actually include or or the information from the data, even the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So it's more robust to noise and more reliable.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So regarding the view consistency, as I just mentioned, we want to cluster the views.",
                    "label": 1
                },
                {
                    "sent": "So how should we cluster rules?",
                    "label": 0
                },
                {
                    "sent": "We actually clustered views through clustered hyper priors or the covariances for this real coefficient vectors F. So, so we actually like this FTV up to FT FT1, up to FPV drone flight model from a multivariate normal distribution and for example beside 11, here is the covariance matrix stands for how the view one.",
                    "label": 0
                },
                {
                    "sent": "Sorry, let's put aside 1 two here is the covariance matrix stands for how the view one and view two.",
                    "label": 1
                },
                {
                    "sent": "They are correlated with each other.",
                    "label": 0
                },
                {
                    "sent": "So as I just mentioned, because the digital process is not good, the prior is actually independent with each other.",
                    "label": 0
                },
                {
                    "sent": "And what we want is not this.",
                    "label": 1
                },
                {
                    "sent": "For example, we extend this to a matrix digital process.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we assume that precise VV prime, independently drawn from a distribution at V Prime, and so this is if we see the second you know formulation here.",
                    "label": 0
                },
                {
                    "sent": "This is just the you know stick breaking process of judicial process prior.",
                    "label": 0
                },
                {
                    "sent": "If we give this WV prime a beta prior.",
                    "label": 0
                },
                {
                    "sent": "However, as I just as I just showed, there's some disadvantages here, so we want to show is that we want to decompose this beta weights to this gamma VH multiplied by gamma V prime H, and each gamma VH.",
                    "label": 0
                },
                {
                    "sent": "Stands for.",
                    "label": 0
                },
                {
                    "sent": "Stands for the different ways coming from a specific views VVV prime and we can show that the summation of this go back to this original weights is still to be one.",
                    "label": 1
                },
                {
                    "sent": "So it's you know validated formula decomposition and formulation and what this is what we want.",
                    "label": 0
                },
                {
                    "sent": "So if just Additionally process prior, this will be equivalent.",
                    "label": 0
                },
                {
                    "sent": "However, we know that if we know that there are some, you know correlating information between this V, V2 and V3, we want this.",
                    "label": 0
                },
                {
                    "sent": "You have a higher probability that.",
                    "label": 0
                },
                {
                    "sent": "This the first can be clustered together, so that's what we want.",
                    "label": 0
                },
                {
                    "sent": "And because and because of this formulation we can realize that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, and especially regarding the base measure, denote we actually give a degenerate distribution that can be forced totally to be 0.",
                    "label": 0
                },
                {
                    "sent": "In that sense, you know some view with some other specific views can be totally independent with each other.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just a graphical representation of our framework, so this agree one stands for the data that we observed and this K&V are the direct coefficients that were interested in and we learn the real consistency through this coefficient, FB and task relatedness through this covariance matrix K for the for the for the response and this yellow part are the direct priors and this green green ones are the hyper priors imbedded in the in the modeling.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we compare our results with some state of the art methodology.",
                    "label": 0
                },
                {
                    "sent": "So first you know, as we can show.",
                    "label": 0
                },
                {
                    "sent": "So this is our methodology and as you can tell, this is our consistently better compared to the state of the art methodology.",
                    "label": 0
                },
                {
                    "sent": "So we compared with some regularised multi task multi view learning through some Co regularization and some basic framework using this semi supervised for the multi task, multi view learning and some novel you know regularizer for this existing AOC methodology.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And through these three methodology three datasets we are in the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consistently better compared to our competitors.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to quickly generalize.",
                    "label": 0
                },
                {
                    "sent": "So this talk we want to solve this problem, so we don't want to.",
                    "label": 0
                },
                {
                    "sent": "You know, we want to study from the data how they saw you know tasks are related and how these views are consistently.",
                    "label": 0
                },
                {
                    "sent": "What's their consistency?",
                    "label": 0
                },
                {
                    "sent": "We don't want to use the predefined are you know Clarence, matrices?",
                    "label": 0
                },
                {
                    "sent": "We want to order to use the data twice, so and also we don't want to over fit, so that's the reason we built our methodology totally based on the basic framework and especially.",
                    "label": 0
                },
                {
                    "sent": "Because some some data augmentation techniques.",
                    "label": 0
                },
                {
                    "sent": "This modeling is realized through an efficient, very efficient Gibbs sampler algorithm.",
                    "label": 1
                },
                {
                    "sent": "And also I did not show here back in our paper.",
                    "label": 0
                },
                {
                    "sent": "We also showed the heating plots for the clustering of the different views.",
                    "label": 0
                },
                {
                    "sent": "That's also another benefit of this methodology, and especially we show some benefit competitive results compared to some state of the art methods in our paper.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}