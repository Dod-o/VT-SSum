{
    "id": "5lvg5hvdpeox2jhhpuolz6lzopagil46",
    "title": "Efficient Active Learning",
    "info": {
        "author": [
            "Nikos Karampatziakis, Department of Computer Science, Cornell University"
        ],
        "published": "July 25, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning"
        ]
    },
    "url": "http://videolectures.net/explorationexploitation2011_karampatziakis_efficie/",
    "segmentation": [
        [
            "Yeah, I'm Nicholson.",
            "This is joint work.",
            "With their Alina Danielle, John and tongue.",
            "And I'll be talking about efficient active."
        ],
        [
            "Learning.",
            "So looking at the audience, I think I I can hardly make just as to why we need active learning.",
            "I can just say that we have many cases where we have a lot of cheap, unlabeled data.",
            "But labeling is expensive.",
            "And we have many cases in computer science where interaction can help us do things more effectively.",
            "And we also know many cases in active learning where.",
            "In machine learning where active learning can give us an exponential reduction in the number of labels so.",
            "Active learning is.",
            "Of pretty well at this idea of how can we learn more effectively with interaction?",
            "And in this talk I will just focus on specific active learning protocol, also known as selective sampling, where in every round we observe a particular example an with decide whether to query the label or not.",
            "And if we do then we observe the label, otherwise we go to the next example and the goal is the active learner should optimize the number of labels it requests.",
            "Have a small number of requested labels and it should also find a good classifier.",
            "And how is this fitting into this workshop well?"
        ],
        [
            "It's basically querying.",
            "The label is exploration.",
            "It has a cost.",
            "It's not immediately obvious if you need this label, but in the in the long run you need to collect labels in order to do well in your task.",
            "So on the other hand, exploitation is just using the current hypothesis."
        ],
        [
            "Predict.",
            "Sorry.",
            "In this talk, I will focus on an algorithm that was proposed in Nice email 2009.",
            "It's a important weighted active learning.",
            "And basically once we get the example, we decide whether to query it with some probability P and this probability is set adaptively.",
            "And if we do decide to query it, then we added on our set of labeled examples.",
            "This S an.",
            "We give it an important weight of 1 / P. What this does is basically.",
            "When once we have enough a label there.",
            "Examples once we have seen a lot of unlabeled examples we have, we will have sampled all of our input space because for all, if this probability will be chosen so that it will be positive.",
            "And.",
            "This allows us to prove that the algorithm will be consistent becausw.",
            "In the long run, we will be sampling for from all all over our input space.",
            "And Furthermore, this consistency theorem sources that.",
            "There is no way that this algorithm can catastrophically fail, so if these are probabilities are not estimated.",
            "In the most optimal way.",
            "Then the algorithm won't want fail completely, it will just converge more slowly to what we could have gotten with a passive learner.",
            "So consistency means here that we're basically learning the same thing that we would have learn with a passive learner if we had access to a lot of unlabeled data.",
            "The important ways that we are using are so that the sample that we collect is random sample from the underlying distribution.",
            "So this importance weights allows us to use this sample even if we want to switch learning algorithms at some point."
        ],
        [
            "So how are these probabilities chosen?",
            "The main quantity that we care about is this important weighted error rate that for a hypothesis aids and the importance weighted data set S we just compute.",
            "This is an indicator function of whether the hypothesis disagrees with the label.",
            "And we just take another edge of the importance, weighted the errors that, well, we have from from this animation.",
            "So there are two hypothesis."
        ],
        [
            "That that are of interest.",
            "One is the empirical risk minimizer.",
            "It's the hypothesis in our hypothesis class that minimizes this error rate.",
            "And the other one."
        ],
        [
            "Is.",
            "Another minimizer of this importance weighted error rate.",
            "The one that disagrees with DRM hypothesis on predicting on the current example on its prediction of the current example, we call this the alternative hypothesis.",
            "These two hypothesis."
        ],
        [
            "But I'm an error rate we can compute and then the difference in error rates tells us how much this two hypothesis differ, and you can imagine that when they differ by a little, we want to query the label and when they differ by a lot, we know that they are in hypothesis is much better than the alternative hypothesis, so we don't want to query the."
        ],
        [
            "Label that much.",
            "Here I have only the functional form of how the probability is set.",
            "For more details you can come to the poster.",
            "But basically it's the idea that I just talked about."
        ],
        [
            "So we can prove a few things about the.",
            "This way of setting the query probabilities and one is that.",
            "The this algorithm will have a similar error rate to supervise the learning on a set of K points.",
            "The same the same number of points that we have for all.",
            "Forever.",
            "For our label set and.",
            "We can also so that we can have we only need to query few labels when there is a small disagreement coefficient."
        ],
        [
            "So now we'll talk about some experiments that we did we say.",
            "Combining this active learning reduction with online gradient descent.",
            "And for the purposes of this talk, we will assume that they are in.",
            "Oracle is given by just doing supervised learning, which is something that it's tractable.",
            "We have actually more experiments in the paper and in the poster with decision trees, but."
        ],
        [
            "For now, just focus on online gradient descent.",
            "So they are in hypothesis will just be the current iterate of online gradient descent and the alternative hypo."
        ],
        [
            "This is you can imagine it's some kind of.",
            "Combination of the current best hypothesis that we have plus the unlabeled example.",
            "But in fact we don't even need.",
            "We don't even need to compute the the alternative hypothesis."
        ],
        [
            "All we need is to be able."
        ],
        [
            "Photo.",
            "Estimate the difference between the error rates of the alternative hypothesis and the current hype recurrent ERM hypothesis.",
            "So, um.",
            "I I won't tell you exactly how we do it, it's pretty simple.",
            "It's a closed form formula.",
            "I have it in the poster.",
            "All I can say is it's related with the important way that example would need to have so that the next iterate would be the alternative hypothesis.",
            "And from that we can estimate you can estimate the deltas."
        ],
        [
            "So now if you have deltas Delta, you can compute the probability and then you can decide whether to query the label.",
            "So if you clear the label, you can do step of online gradient descent.",
            "And if you do that, it won't work.",
            "In practice, and the reason is as."
        ],
        [
            "Close.",
            "What happens is that the active learner takes.",
            "Needs to at some point.",
            "Stop querying so the probabilities of querying becomes small and the importance weights that are used to.",
            "Unbias the sample that we collect become large because they are the inverse of the probabilities.",
            "And to understand what happens is that when we have a large importance weight and this is our loss function here.",
            "What the?",
            "What happens is that if we take a step in the negative gradient direction with some learning rate and we just.",
            "Multiply the importance weight with the gradient in order to make the update.",
            "We can just go very far when the important weight is large.",
            "To fix that, we start with this principle that having an example with a large importance weight is like like I is like having it.",
            "I times in the data set.",
            "So we can think of doing the update I time."
        ],
        [
            "A small update at times, and in fact we have derived a way of doing the update so that.",
            "Um?",
            "We are doing an infinitely now an infinite number of infinitely small updates.",
            "And we can get a closed form by solving a differential equation for its loss function."
        ],
        [
            "So this is a how the update looks for first squared loss and it's also closed form for other losses.",
            "The importance weight comes up here in the exponent."
        ],
        [
            "So with this update we can actually do our experiments we have for datasets and in this plot side.",
            "So the fraction of labels queried on the X axis and on the Y axis.",
            "It's the test error on held out data.",
            "Then yeah, it's the accuracy.",
            "The generalization error you can think of.",
            "And we have three algorithms.",
            "The Red line is the.",
            "Active learner using the updates from the previous slide.",
            "And the green is active learning active learning when we do the naive update by multiplying the gradient with important weight.",
            "And the blue line is just doing passive learning with online gradient descent.",
            "So we can see that the red line is always below the blue line, so active is better than passive.",
            "And we can say that if we implement active learning in a naive way as the green line does.",
            "The benefit of active learning is basically either gone as here, or it can be even worse than doing passive learning, as in this data set.",
            "So some care needs to be taken on how to implement this thing.",
            "And OK, so the results are good, but."
        ],
        [
            "What about running time?",
            "So this is a very fast algorithm.",
            "All the operations that we do are just constant.",
            "Constant time operations in.",
            "Extra on what we do and.",
            "In addition to what we need to do for online gradient descent.",
            "So.",
            "Training it on 800,000 documents where every document has 77 features.",
            "An average we can do online learning.",
            "Passive online gradient descent in 2.5 seconds and we can do active learning in a little bit.",
            "A little bit more time, but we also get a factor of 10 reduction in the number of labels we need."
        ],
        [
            "But it can even be faster than passive learning.",
            "And here we process the received one data set so that every document includes features that are interactions of the words.",
            "So we have 3000 features per document on average, and for this one active learning takes less time than passive learning, which is very impressive in my opinion, because a lot of people are trying to.",
            "Say how can we do faster?",
            "Training algorithm one way is to judiciously choose the examples on which tool to update."
        ],
        [
            "So to conclude my talk I.",
            "Present the consistent active learning algorithm.",
            "It's actually a reduction.",
            "You can plug in your own active learner.",
            "You can plug in your own learner.",
            "Anne, this this reduction will active eyes it.",
            "And we present some experiments with online gradient descent and we have some experiments with decision trees in the poster.",
            "For more details we can look at these two papers that describe the algorithm and the updates that we use, and we also have a implementation available free at this URL.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, I'm Nicholson.",
                    "label": 0
                },
                {
                    "sent": "This is joint work.",
                    "label": 0
                },
                {
                    "sent": "With their Alina Danielle, John and tongue.",
                    "label": 0
                },
                {
                    "sent": "And I'll be talking about efficient active.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning.",
                    "label": 0
                },
                {
                    "sent": "So looking at the audience, I think I I can hardly make just as to why we need active learning.",
                    "label": 0
                },
                {
                    "sent": "I can just say that we have many cases where we have a lot of cheap, unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "But labeling is expensive.",
                    "label": 0
                },
                {
                    "sent": "And we have many cases in computer science where interaction can help us do things more effectively.",
                    "label": 0
                },
                {
                    "sent": "And we also know many cases in active learning where.",
                    "label": 1
                },
                {
                    "sent": "In machine learning where active learning can give us an exponential reduction in the number of labels so.",
                    "label": 1
                },
                {
                    "sent": "Active learning is.",
                    "label": 0
                },
                {
                    "sent": "Of pretty well at this idea of how can we learn more effectively with interaction?",
                    "label": 1
                },
                {
                    "sent": "And in this talk I will just focus on specific active learning protocol, also known as selective sampling, where in every round we observe a particular example an with decide whether to query the label or not.",
                    "label": 0
                },
                {
                    "sent": "And if we do then we observe the label, otherwise we go to the next example and the goal is the active learner should optimize the number of labels it requests.",
                    "label": 0
                },
                {
                    "sent": "Have a small number of requested labels and it should also find a good classifier.",
                    "label": 0
                },
                {
                    "sent": "And how is this fitting into this workshop well?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's basically querying.",
                    "label": 0
                },
                {
                    "sent": "The label is exploration.",
                    "label": 0
                },
                {
                    "sent": "It has a cost.",
                    "label": 0
                },
                {
                    "sent": "It's not immediately obvious if you need this label, but in the in the long run you need to collect labels in order to do well in your task.",
                    "label": 0
                },
                {
                    "sent": "So on the other hand, exploitation is just using the current hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Predict.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "In this talk, I will focus on an algorithm that was proposed in Nice email 2009.",
                    "label": 0
                },
                {
                    "sent": "It's a important weighted active learning.",
                    "label": 1
                },
                {
                    "sent": "And basically once we get the example, we decide whether to query it with some probability P and this probability is set adaptively.",
                    "label": 0
                },
                {
                    "sent": "And if we do decide to query it, then we added on our set of labeled examples.",
                    "label": 0
                },
                {
                    "sent": "This S an.",
                    "label": 1
                },
                {
                    "sent": "We give it an important weight of 1 / P. What this does is basically.",
                    "label": 0
                },
                {
                    "sent": "When once we have enough a label there.",
                    "label": 1
                },
                {
                    "sent": "Examples once we have seen a lot of unlabeled examples we have, we will have sampled all of our input space because for all, if this probability will be chosen so that it will be positive.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This allows us to prove that the algorithm will be consistent becausw.",
                    "label": 0
                },
                {
                    "sent": "In the long run, we will be sampling for from all all over our input space.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, this consistency theorem sources that.",
                    "label": 0
                },
                {
                    "sent": "There is no way that this algorithm can catastrophically fail, so if these are probabilities are not estimated.",
                    "label": 0
                },
                {
                    "sent": "In the most optimal way.",
                    "label": 0
                },
                {
                    "sent": "Then the algorithm won't want fail completely, it will just converge more slowly to what we could have gotten with a passive learner.",
                    "label": 0
                },
                {
                    "sent": "So consistency means here that we're basically learning the same thing that we would have learn with a passive learner if we had access to a lot of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "The important ways that we are using are so that the sample that we collect is random sample from the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "So this importance weights allows us to use this sample even if we want to switch learning algorithms at some point.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how are these probabilities chosen?",
                    "label": 0
                },
                {
                    "sent": "The main quantity that we care about is this important weighted error rate that for a hypothesis aids and the importance weighted data set S we just compute.",
                    "label": 0
                },
                {
                    "sent": "This is an indicator function of whether the hypothesis disagrees with the label.",
                    "label": 0
                },
                {
                    "sent": "And we just take another edge of the importance, weighted the errors that, well, we have from from this animation.",
                    "label": 0
                },
                {
                    "sent": "So there are two hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That that are of interest.",
                    "label": 0
                },
                {
                    "sent": "One is the empirical risk minimizer.",
                    "label": 0
                },
                {
                    "sent": "It's the hypothesis in our hypothesis class that minimizes this error rate.",
                    "label": 0
                },
                {
                    "sent": "And the other one.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Another minimizer of this importance weighted error rate.",
                    "label": 0
                },
                {
                    "sent": "The one that disagrees with DRM hypothesis on predicting on the current example on its prediction of the current example, we call this the alternative hypothesis.",
                    "label": 0
                },
                {
                    "sent": "These two hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I'm an error rate we can compute and then the difference in error rates tells us how much this two hypothesis differ, and you can imagine that when they differ by a little, we want to query the label and when they differ by a lot, we know that they are in hypothesis is much better than the alternative hypothesis, so we don't want to query the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Label that much.",
                    "label": 0
                },
                {
                    "sent": "Here I have only the functional form of how the probability is set.",
                    "label": 0
                },
                {
                    "sent": "For more details you can come to the poster.",
                    "label": 0
                },
                {
                    "sent": "But basically it's the idea that I just talked about.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can prove a few things about the.",
                    "label": 0
                },
                {
                    "sent": "This way of setting the query probabilities and one is that.",
                    "label": 0
                },
                {
                    "sent": "The this algorithm will have a similar error rate to supervise the learning on a set of K points.",
                    "label": 1
                },
                {
                    "sent": "The same the same number of points that we have for all.",
                    "label": 0
                },
                {
                    "sent": "Forever.",
                    "label": 0
                },
                {
                    "sent": "For our label set and.",
                    "label": 1
                },
                {
                    "sent": "We can also so that we can have we only need to query few labels when there is a small disagreement coefficient.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we'll talk about some experiments that we did we say.",
                    "label": 0
                },
                {
                    "sent": "Combining this active learning reduction with online gradient descent.",
                    "label": 1
                },
                {
                    "sent": "And for the purposes of this talk, we will assume that they are in.",
                    "label": 0
                },
                {
                    "sent": "Oracle is given by just doing supervised learning, which is something that it's tractable.",
                    "label": 0
                },
                {
                    "sent": "We have actually more experiments in the paper and in the poster with decision trees, but.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For now, just focus on online gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So they are in hypothesis will just be the current iterate of online gradient descent and the alternative hypo.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is you can imagine it's some kind of.",
                    "label": 0
                },
                {
                    "sent": "Combination of the current best hypothesis that we have plus the unlabeled example.",
                    "label": 0
                },
                {
                    "sent": "But in fact we don't even need.",
                    "label": 0
                },
                {
                    "sent": "We don't even need to compute the the alternative hypothesis.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All we need is to be able.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Photo.",
                    "label": 0
                },
                {
                    "sent": "Estimate the difference between the error rates of the alternative hypothesis and the current hype recurrent ERM hypothesis.",
                    "label": 1
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "I I won't tell you exactly how we do it, it's pretty simple.",
                    "label": 0
                },
                {
                    "sent": "It's a closed form formula.",
                    "label": 0
                },
                {
                    "sent": "I have it in the poster.",
                    "label": 0
                },
                {
                    "sent": "All I can say is it's related with the important way that example would need to have so that the next iterate would be the alternative hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And from that we can estimate you can estimate the deltas.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now if you have deltas Delta, you can compute the probability and then you can decide whether to query the label.",
                    "label": 0
                },
                {
                    "sent": "So if you clear the label, you can do step of online gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And if you do that, it won't work.",
                    "label": 0
                },
                {
                    "sent": "In practice, and the reason is as.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Close.",
                    "label": 0
                },
                {
                    "sent": "What happens is that the active learner takes.",
                    "label": 0
                },
                {
                    "sent": "Needs to at some point.",
                    "label": 0
                },
                {
                    "sent": "Stop querying so the probabilities of querying becomes small and the importance weights that are used to.",
                    "label": 0
                },
                {
                    "sent": "Unbias the sample that we collect become large because they are the inverse of the probabilities.",
                    "label": 0
                },
                {
                    "sent": "And to understand what happens is that when we have a large importance weight and this is our loss function here.",
                    "label": 0
                },
                {
                    "sent": "What the?",
                    "label": 0
                },
                {
                    "sent": "What happens is that if we take a step in the negative gradient direction with some learning rate and we just.",
                    "label": 0
                },
                {
                    "sent": "Multiply the importance weight with the gradient in order to make the update.",
                    "label": 0
                },
                {
                    "sent": "We can just go very far when the important weight is large.",
                    "label": 0
                },
                {
                    "sent": "To fix that, we start with this principle that having an example with a large importance weight is like like I is like having it.",
                    "label": 1
                },
                {
                    "sent": "I times in the data set.",
                    "label": 0
                },
                {
                    "sent": "So we can think of doing the update I time.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A small update at times, and in fact we have derived a way of doing the update so that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We are doing an infinitely now an infinite number of infinitely small updates.",
                    "label": 0
                },
                {
                    "sent": "And we can get a closed form by solving a differential equation for its loss function.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a how the update looks for first squared loss and it's also closed form for other losses.",
                    "label": 0
                },
                {
                    "sent": "The importance weight comes up here in the exponent.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with this update we can actually do our experiments we have for datasets and in this plot side.",
                    "label": 0
                },
                {
                    "sent": "So the fraction of labels queried on the X axis and on the Y axis.",
                    "label": 1
                },
                {
                    "sent": "It's the test error on held out data.",
                    "label": 0
                },
                {
                    "sent": "Then yeah, it's the accuracy.",
                    "label": 0
                },
                {
                    "sent": "The generalization error you can think of.",
                    "label": 0
                },
                {
                    "sent": "And we have three algorithms.",
                    "label": 0
                },
                {
                    "sent": "The Red line is the.",
                    "label": 0
                },
                {
                    "sent": "Active learner using the updates from the previous slide.",
                    "label": 0
                },
                {
                    "sent": "And the green is active learning active learning when we do the naive update by multiplying the gradient with important weight.",
                    "label": 1
                },
                {
                    "sent": "And the blue line is just doing passive learning with online gradient descent.",
                    "label": 1
                },
                {
                    "sent": "So we can see that the red line is always below the blue line, so active is better than passive.",
                    "label": 0
                },
                {
                    "sent": "And we can say that if we implement active learning in a naive way as the green line does.",
                    "label": 0
                },
                {
                    "sent": "The benefit of active learning is basically either gone as here, or it can be even worse than doing passive learning, as in this data set.",
                    "label": 0
                },
                {
                    "sent": "So some care needs to be taken on how to implement this thing.",
                    "label": 0
                },
                {
                    "sent": "And OK, so the results are good, but.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What about running time?",
                    "label": 0
                },
                {
                    "sent": "So this is a very fast algorithm.",
                    "label": 0
                },
                {
                    "sent": "All the operations that we do are just constant.",
                    "label": 0
                },
                {
                    "sent": "Constant time operations in.",
                    "label": 0
                },
                {
                    "sent": "Extra on what we do and.",
                    "label": 0
                },
                {
                    "sent": "In addition to what we need to do for online gradient descent.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Training it on 800,000 documents where every document has 77 features.",
                    "label": 0
                },
                {
                    "sent": "An average we can do online learning.",
                    "label": 0
                },
                {
                    "sent": "Passive online gradient descent in 2.5 seconds and we can do active learning in a little bit.",
                    "label": 1
                },
                {
                    "sent": "A little bit more time, but we also get a factor of 10 reduction in the number of labels we need.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it can even be faster than passive learning.",
                    "label": 0
                },
                {
                    "sent": "And here we process the received one data set so that every document includes features that are interactions of the words.",
                    "label": 0
                },
                {
                    "sent": "So we have 3000 features per document on average, and for this one active learning takes less time than passive learning, which is very impressive in my opinion, because a lot of people are trying to.",
                    "label": 0
                },
                {
                    "sent": "Say how can we do faster?",
                    "label": 0
                },
                {
                    "sent": "Training algorithm one way is to judiciously choose the examples on which tool to update.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude my talk I.",
                    "label": 0
                },
                {
                    "sent": "Present the consistent active learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "It's actually a reduction.",
                    "label": 0
                },
                {
                    "sent": "You can plug in your own active learner.",
                    "label": 1
                },
                {
                    "sent": "You can plug in your own learner.",
                    "label": 1
                },
                {
                    "sent": "Anne, this this reduction will active eyes it.",
                    "label": 0
                },
                {
                    "sent": "And we present some experiments with online gradient descent and we have some experiments with decision trees in the poster.",
                    "label": 0
                },
                {
                    "sent": "For more details we can look at these two papers that describe the algorithm and the updates that we use, and we also have a implementation available free at this URL.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}