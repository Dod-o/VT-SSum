{
    "id": "ovkeuddftdakahvjcw6rbg2ojqfdc7xd",
    "title": "Kernel Methods",
    "info": {
        "author": [
            "Bernhard Sch\u00f6lkopf, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "Nov. 2, 2009",
        "recorded": "August 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/mlss09uk_schoelkopf_km/",
    "segmentation": [
        [
            "So good morning everybody.",
            "You'll be able to hear me in stereo.",
            "I'm all wired up so.",
            "This one is the local local 1.",
            "This is for the camera, everything.",
            "OK so a message to the people who will watch this video later on.",
            "Don't watch it.",
            "It's very similar to the other one that's already on the website.",
            "So I'm gonna check off and I don't know most of you, but I would like to know you.",
            "Of course there's not enough time, but maybe we can take a quick poll, for instance.",
            "Who's who's a PhD student here?",
            "Who's pre PhD?",
            "Who is a postdoc?",
            "Who's who's a lecturer?",
            "Who is a senior lecturer?",
            "Who's a reader who is a professor?",
            "Daniel.",
            "Who is a professor at Cambridge?",
            "So and also about the background, is that a question or a professor?",
            "Turn it up, I'll try.",
            "Was that loud enough?",
            "I can just move it higher.",
            "How about that?",
            "Or I hold it like this?",
            "No?",
            "I mean I can, at least while I don't use my hand for something else, I can only like this.",
            "Where would I stop?",
            "OK, so about the background would be I would be interested to know who's a computer scientist.",
            "Who is engineer?",
            "Who is the mathematician?",
            "Physics.",
            "Psychology.",
            "I don't know cognitive science.",
            "What have I forgotten?",
            "Neuroscience.",
            "Any, any anything else philosophy?",
            "No philosophers we've left the stage of philosophy in this field.",
            "It's becoming technical so.",
            "OK, and maybe also who knows?",
            "Support vector machines.",
            "Maybe it's maybe everybody knows it anyway.",
            "And who knows who would be able to derive the dual optimization problem for support vector classifier?",
            "I won't ask you to come to the back door.",
            "OK.",
            "So I guess most of you have heard of it in know it approximately, and probably I won't spend too much time on support vector machines anyway, I'll I'll try to talk about about kernel methods in general.",
            "And.",
            "So to give you this doesn't work, and I'll try to give you some motivation for why you should be sitting here early in the morning.",
            "So support link machines started around 15 years ago or something like that.",
            "And I was in the lucky situation that I was a PhD student who at that time was working at Bell Labs where.",
            "But Nick and other people were working developing support vector machines, so I gotta know it early on was in that field from the start.",
            "At the beginning nobody knew it at the beginning you had to.",
            "Even when you wrote a paper, you always had to compare things to neural networks at at that time.",
            "Neural networks where the standard words nowadays, if you come up with a new classification algorithm, people reviewers will always ask you to benchmark it well.",
            "How does it work?",
            "For instance, your problem with an SVM?",
            "How does it compare to an SVM?",
            "So it's like a commodity is standard classifier now.",
            "So which is one reason why you should know it?",
            "The other reason why you should know it is that it contains some nice tricks, especially.",
            "Related to kernels.",
            "And these tricks have been used in other domains as well, and it's somehow part of the standard machine learning Toolkit.",
            "Now to know how kernels work, to be able to combine them with whatever else you're working on.",
            "So it's a bit like.",
            "The time of neural network has taught us that there's the chain rule and how to use it.",
            "Now there's something additional is kernel tricks and it'll be the functional analysis.",
            "So we should learn this in our community and be able to use it even if we don't work on support vector machines.",
            "And I don't want to convince anyone to work and support vector machines.",
            "In fact, I don't work on it myself anymore.",
            "I still use them, but.",
            "If you want to do research when it's nowadays it's not so easy to do something new in support vector machines, you can still do a lot of things with kernels of course, which is probably why you're here."
        ],
        [
            "So let's see.",
            "How to operate this?",
            "OK, I have to use the mouse wheel so we might occasionally skip one slide 'cause I moved too far, let's see.",
            "OK, so I'll start talking about kernels coming from the intuitive notion of what's the similarity?",
            "I'll talk a bit about kernels about feature spaces.",
            "And in the next hour, I don't know how long this will take.",
            "Maybe one hour next hour.",
            "I'll be a bit more mathematical.",
            "Tell you what's a positive definite kernel, how to construct the feature space, which is called a reproducing kernel Hilbert space, and I'll tell you a bit about a nice application of kernels which are called kernel means, and finally probably say a little bit about support vector machines, so I think I'll have to find a better way.",
            "Wait, let me try to.",
            "Put this here again."
        ],
        [
            "OK, so suppose we are interested in input output.",
            "Learning the simplest form, super supervised learning and we are given two sets X&Y.",
            "And.",
            "So.",
            "X&Y and training set of pairs taken from X&Y, so XIYI we want to somehow learn from this training set, which means something like given or we want to generalize given at previously unseen X and new input, find a suitable output for it, so somehow this new pair XY should be similar to the ones that we've seen before, so this begs the question, how do we measure similarity and?",
            "For outputs, typically, although not always, but that's the traditional view, similarity somehow was measured using a loss function.",
            "So for instance, if we're talking about classification, we have only two class classification.",
            "We have two possible outputs, plus, minus one.",
            "Let's call in past minus one.",
            "We might want to use the 01 loss that simply checks whether output is 2.",
            "Outputs are the same or whether they're different, or it checks whether our predicted output is the same as the true one, or whether it's different.",
            "For the input, on the other hand, we can measure the similarity using something called a kernel, and maybe I should already add a caveat.",
            "One can construct kernel methods that somehow measure the similarity of this pair to the other ones jointly, taking into account input and output, but that's not the topic."
        ],
        [
            "Our lecture here, so let's focus now on measuring similarity of inputs.",
            "So one simple way of measuring the similarity between two inputs would be to define a symmetric function that takes 2 inputs and assigns them to a real number which somehow captures our similarity in that function could for instance be the Canonical dot product.",
            "So there's a little bit of notation, so this means sometimes X sub I is a training example.",
            "So if I want the right coordinate of a training example, I put this.",
            "Brackets around.",
            "So that's the dot product.",
            "I think everybody knows what a dot product is, or at least everybody knows this Canonical standard product product of the coordinates summed over all coordinates.",
            "Now of course you can only do this if our input domain is a vector space that product space, or in this specific this specific case it's a Euclidean or indoor product space.",
            "Now if our input domain is not a dot product space.",
            "So for instance, if our data are strings or some kind of adenoids, let's say it's a manifold or something like that, then we might still be able to use something like this.",
            "If we assume that our similarity measure K. Has it representation as a dot product in some other space unit or product space which will call H because they will be later on there will be Hilbert spaces.",
            "So in this by representation I mean there exists a map from our input domain to adore product space such that the similarity measure is given by the DOT product in that other space.",
            "So we first map our points into that provide space.",
            "Then we take the dot product in that space.",
            "So we can think of this mapping as a preprocessing of our input patterns.",
            "We can think of the patterns actually as 5X and 5X prime, and then we can do whatever we can do in terms of the product so we can do geometry.",
            "We can come up with all sorts of geometric algorithms in that the product space, which is sometimes called the feature space."
        ],
        [
            "So that's already the basic idea of how kernel methods work.",
            "Of course there are some ramifications later, so let's think of a simple or maybe the simplest possible algorithm one could do in such a representation.",
            "So the simplest possible geometric classification algorithm.",
            "So suppose we have.",
            "A set of, let's say, negative points.",
            "These circles here are set of positive points or two classes, and we want to classify a new point X.",
            "Then we can do it by first computing the two class means.",
            "So suppose we have.",
            "Mplus is the number of positive examples, so we sum overall positive examples.",
            "The Five XI we take.",
            "We divide by the number so we get the mean of the positive class.",
            "Actually this.",
            "Should be C plus.",
            "Here it says C2 and they should be C minus.",
            "Here we have the same for the negative class.",
            "And then.",
            "One can simply assign our new point through the class whose mean is closer to it.",
            "So if we think about this for a second, what kind of decision rule does this induce?",
            "So the set of all points that's closer to this point than it is to this point.",
            "It's a half space which is separated from the other half space by this hyperplane.",
            "And we can check whether a point is closer to here or to here, for instance by computing for this vector W, connecting the two class means.",
            "And then see the midpoint along this connection.",
            "Then we can compute this vector X -- C. X is our test point represented in the feature space.",
            "And then we simply have to check whether this vector in this vector includes an angle smaller than 90 degrees or larger than 90 degrees.",
            "So, and to do that we just have to check we just have to compute the DOT product between these two vectors.",
            "If the dot product between the two vectors is positive, then the angle is smaller than 90 degrees.",
            "If the dot product is negative.",
            "The angle is larger than 90 degrees, so that's one simple way of doing things."
        ],
        [
            "And if we.",
            "Go through the algebra.",
            "We just substitute everything.",
            "We get a simple solution.",
            "So we substitute everything into here.",
            "We then replace the DOT products between points mapped into the feature space by kernel functions.",
            "OK so back here I have a constant which just is all the terms that don't depend on X.",
            "And then we get this decision rule which is now.",
            "A kernel based decision rules, so it's an expansion in terms of kernels.",
            "Sentence here on all positive points when he sent it on, all negative points here some constant.",
            "So you can think of this.",
            "Maybe for the ones actually I didn't ask before.",
            "Are there people who are statisticians here?",
            "That's what I forgot.",
            "OK, we have three statisticians also.",
            "So for the statisticians.",
            "In some cases, depending on what kind of kernel one chooses, but for instance, if we choose a Gaussian function as a kernel and we normalize it such that it has integral one, so then this is some kind of like a valid density model then.",
            "This is also identity model.",
            "We normalized by the number of such kernels.",
            "So this is just like a pulse in Windows density estimator of the positive class and this is a pause in Windows 10 TS density estimator for negative class.",
            "So in this case then this simple geometric classification rule in the feature space ends up giving us something like a something like a classifier based on like a plug-in classification rule based on parsing Windows density estimates.",
            "So that's a very simple algorithm and.",
            "Maybe?",
            "This would be a good time to take a few minutes and try everybody to derive this classifier in a slightly different way.",
            "So what I'd like you to try, it's my experience.",
            "You learn a lot more if you try a little bit yourself also, so I'd like you to try.",
            "To derive this classification rule here.",
            "This one or something like this one.",
            "Maybe you'll have slightly different signs somewhere, but I'd like you to derive this classification rule simply by directing computing the distance.",
            "So you compute distance between this point at this point in the feature space.",
            "And you subtract the distance between these two points.",
            "Now distances something one can compute using their products, right?",
            "You can use this squared distance between these two points.",
            "You subtract the squared distance between these two points.",
            "And then at the end you take a take the sign of this difference.",
            "And let's see whether you get something that's similar to this one here.",
            "So let's take a few minutes and try this out.",
            "Everybody on his own or are in teams or whatever, and I will try to try it myself.",
            "Whether I can still do it also.",
            "Yeah, so if I don't know which slide I should be leaving out, maybe I'll leave this one.",
            "Yeah, so how many people have found the solution yet?",
            "I have to ask this now because we are in this phase transition where the last sound level is increasing.",
            "I remember I was once at a talk not far from here by Stephen Hawking and someone someone asked a question after the talk and then he started.",
            "Preparing the answer, which is difficult for him with some kind of machine that he is using.",
            "So at the beginning people were waiting very quietly for the answer, but it took around.",
            "I would say 10 or 15 minutes to prepare the answers.",
            "So after 10 minutes, like gradually there there was.",
            "This increases the noise level and after when he really answered you were sorry it was as loud as in the train station when you started answering.",
            "Suddenly everything was super quiet again, but the answer was just one sentence so, but how much more time do people need to or how many people have solved it by now?",
            "So I would say that's more than half and is there someone who would like to have more time?",
            "OK, so then we can get to the solution and we can either do the way such that I do it here or someone else does it and I'll have to think about some kind of reward.",
            "Maybe I'll have three or four such problems during my lectures, and whoever solves the largest number will get some kind of reward.",
            "So if you solve this one, you could.",
            "You could do it here.",
            "You should probably be reasonably confident that you have solved it, because the solution is here, so you have solved it.",
            "You know whether you got it right or not so.",
            "There's no reason to be to be worried about doing the black Bolt is like your chance to lecture in the machine Learning Summer School.",
            "So you've done this before.",
            "We've usually always had one or two who were prepared to do it.",
            "Anyone, anyone, courageous?",
            "I raised the bar too high.",
            "No.",
            "OK, so I'll do this one myself.",
            "I think you just want to see me suffer on the blackboard here and I'll do this one myself.",
            "But then the next problem someone will have to do it.",
            "So let's see if there's a chalk.",
            "It must be chopped pieces of mathematics Department.",
            "Actually, I'll use this one.",
            "Or which is the sound program is here or there?",
            "Yeah, that's right.",
            "OK, so we said we have to check whether the point is closer to the positive mean or closer to the negative mean.",
            "So our decision rule.",
            "Anne.",
            "Now should be easy, yeah?",
            "In addition room we should be checking weather.",
            "Our test point is closer to the positive work through the negative million and.",
            "And.",
            "I think this is the right way wrong.",
            "Oh and I'm sure I'll make some mistakes, so whoever splits a mistake, it also gets a point because I'm not usually lecturing.",
            "I'm just a researcher at the Max Planck Institute, so we don't have to.",
            "We go down to get a lecture at universities.",
            "So and so.",
            "Let's see if this is right.",
            "So if the distance to the.",
            "If it's closer to the positive point, then the distance to the negative closer to the positive mean means the distance of the negative mean is larger.",
            "Which case this is positive.",
            "We assign the point of the positive class, so that would be the right way around.",
            "I am so let's work this out.",
            "So here we use the dot product, so this is just the product.",
            "So alright and all the same thing again.",
            "Sorry, negative.",
            "Minus one.",
            "And then we have.",
            "Same thing over here.",
            "Only with plus instead of minus.",
            "So K office is the product of 5X with itself, so that you just pay off XX.",
            "Pump.",
            "I need a square.",
            "This square this square is wrong.",
            "OK, so you have one point already.",
            "Actually, there's a second mistake.",
            "So who cook at that point?",
            "I forgot with you.",
            "OK, the man with the green T shirt?",
            "Alright, the dog.",
            "I'll ask you for your name later.",
            "I won't embarrass you more now.",
            "Then OK so.",
            "So this with this we've got.",
            "Now, let's do the complicated one with the two sums.",
            "And here we always have the dot product between 5XI and 5X J.",
            "So this gives us a kernel between XI.",
            "XJ OK, so I think this should be obvious if it's not obvious to you that one can take the sum out of the dot product.",
            "And how these two summers combined to this single or to this double sum, and so on?",
            "And think about it after the lecture, because that's something you should understand and and now we have a mixed term.",
            "So this thing with this and actually we have this twice because this.",
            "Combining with this will give us the same thing, so let's just write me a stool over.",
            "And then this thing.",
            "And here we have 5 XI with 5X.",
            "OK.",
            "I hope there no mistakes so far and then we've got this.",
            "Basically the same thing.",
            "Now with the difference that here we have a plus one and here we have N plus.",
            "So, and of course, there's a A minus here in the front, so it seems so.",
            "Minus.",
            "And here we have loss.",
            "He's a very soothing sounds.",
            "Reminds me of that with mathematics classes at University, so.",
            "OK, so let's see.",
            "So this one goes away.",
            "And then we are left with something that doesn't depend on X.",
            "There's this thing here, and something that does depend on X.",
            "And let's see if we're already there.",
            "So.",
            "OK, so one difference is a factor of tool, but.",
            "Factor of two.",
            "We can just take out because it's a sine function current so we can multiply the argument of the sine function with any non negative number that we like without changing things.",
            "So let's just.",
            "Divide by two.",
            "Which brings us this thing here and then we will recognize.",
            "OK this these two.",
            "Together they should give us our factor B down here.",
            "And then the other two that are left.",
            "They should exactly give us so.",
            "I've called this in instead of them, but doesn't matter.",
            "They exactly give us this quantity here.",
            "So that we've got these first 2 terms, we've got to be.",
            "And we all set.",
            "Any any questions?",
            "These suggestions?",
            "OK so I have to apologize for those who find this trivial, but I think it's very useful to try to do something to self because it's a way you could.",
            "You say this is 1 simple example of how to Colonel eyes and algorithm.",
            "We take the algorithm.",
            "This trivial algorithm, computing the means in the feature spaces, and then you compute this difference between the square distances and you already have a classification algorithm which gives you something that makes sense from a statistical point of view.",
            "OK, so I want to invite lecture I since I have only four hours, I won't talk about statistical learning theory and there's a lecture of Joshua Taylor will talk about this in great detail, so normally at this point in the lecture I have a couple of hours about statistical learning theory, so I'll I'll skip."
        ],
        [
            "At.",
            "Instead, say a little bit more about kernels and.",
            "Let's see, so that's.",
            "Yeah, let me let me give you some some examples of kernel functions now."
        ],
        [
            "Adjust the weight one introduces polynomial what people called polynomial kernel functions.",
            "So so.",
            "So maybe forget everything that I've said so far and now will do things the other way around.",
            "So let's assume we have some 2 dimensional.",
            "Sorry we have some 2 dimensional inputs.",
            "And we have two class classification problem and suppose the problem is such that the true decision boundary is this ellipse here?",
            "So we have some.",
            "The red class inside the blue class outside we are only given the training points, but not this this decision boundary, which is the ellipse.",
            "Now suppose we first pre process our data points mapping into a feature space into a 3 dimensional feature space.",
            "With this prescription here.",
            "So the prescription is we compute all possible products of two input coordinates.",
            "So in this case there's just three possible such three such products, is X1 squared, X2 squared and X 1 * X Two.",
            "And actually let me put a sqrt 2 in front of the mixed term.",
            "So if we do this, we get a 3 dimensional problem.",
            "But the three dimensional problem in a certain sense is easier now since it turns out that 3 dimensional problem can be solved with a linear decision decision rule.",
            "So why is that so?",
            "If we think about it, the first coordinate let me call it said one is X squared, the third coordinate X X2 squared.",
            "And if you write the ellipse equation for this simple axis aligned ellipse.",
            "In terms of these two coordinates, it becomes an affine equations like a linear equation pass a constant.",
            "So therefore we can.",
            "This ellipse becomes a hyperplane actually hyperplane that's independent of the second call is.",
            "In this simple case, if the... in general position, then it will be a little bit more tricky, but it doesn't matter, it becomes a hyperplane in these representation."
        ],
        [
            "Now, of course, in the general case, we will be interested in inputs that are not 3 dimensional but N dimensional, and we might be interested in product number or two, but of order D where D can be any natural number.",
            "Now, of course, the dimensionality of this feature space is no longer three by three, but it will increase.",
            "It will actually grow like N to the power of the, where is the number of pixels in these images or the whatever the input dimensionality.",
            "So already in this simple 16 by 16 images for prod."
        ],
        [
            "Out of all the five, we have a 10 to the 10 dimensional space.",
            "Now it turns out that what we've been doing so far taking DOT product in that representation sometimes can be done very efficiently.",
            "And an example.",
            "Are these polynomial kernels.",
            "Because if you take this mapping that I defined on the last slide, so the image of X is this thing.",
            "Here the image of another point X prime is this thing here.",
            "Now we take the Canonical products.",
            "So just these vector times this one transposed.",
            "Then we see OK. That's X1 squared exponent prime squared.",
            "We have the same for X2.",
            "Here at the end and then we have a mixed term and if you write it down it turns out it's a complete binomial formula which can be written as the square of the dot product taken directly in the input space.",
            "So this is a special case where the input space also is a dot product space, and it turns out in this special case we can compute this dot product in the feature space directly by computing the DOT product in the input space and then applying some linearity.",
            "In this case just."
        ],
        [
            "Square root.",
            "And the nice thing is that this also works in the N dimensional case with productive already.",
            "It's actually quite simple, easy to see.",
            "You just start with this hypothetical form of.",
            "We just guess this form of the kernels because here it was just square of that product input space.",
            "Let's just take the dot product input space and dimensional raised to the power of the.",
            "So we write this down raised to the power of D. We get a default, some I'll call these columns J one through JD and back.",
            "Here we have all these terms and we can sort them.",
            "Everything in terms of X is here everything.",
            "Everything in terms of X prime is over here.",
            "And then what we can see is this is just a big sum into the power of the terms, but you can think of this as a dot product of a nonlinear function of X with the same nonlinear function of X prime here.",
            "And we can even read off what the nonlinear function does.",
            "It computes all possible products of all the.",
            "And of course, some of these products occur multiple times, so here we have.",
            "We count them multiple times and that's why we have this square root 2 over here.",
            "So we could also just say we use that product X one X2X2X1 if we use both of them, we map into 4 dimensional space, but it's a bit redundant because 2 dimensions will be the same."
        ],
        [
            "So.",
            "Now, traditionally it's a perfect machines.",
            "People have referred to Mercer's theorem to characterize.",
            "The largest order the full class of kernels for which this trick works.",
            "And by this I mean.",
            "Kernels which have the properties that correspond to the product in some other space.",
            "And for completeness I have to show you Mercer Theorem, although I prefer to talk about positive definite kernels because that's slightly larger class and also mathematically.",
            "More elegant anyway, the most most theorem is this theorem from beginning of the 20th century from the integral equations, and it says that if K is account is continuous and it's the kernel of a positive definite integral operator.",
            "By positive definite I mean that for all functions F this inequality holds true.",
            "Then the kernel can be expanded in terms of its eigen functions.",
            "Fine I.",
            "And eigenvalues where the eigenvalues are non negative into this infinite sum.",
            "And if you have such an expansion then from this expansion you can construct or you can you can view this expansion as a third product simply by defining your mapping.",
            "So basically this is already looks like a dot product, you only have to split this slump tie into square root of lamptey over here on the square root of left eye over here.",
            "Then again you have a function of X here and the function of X prime here.",
            "Same function of X prime.",
            "You can view this."
        ],
        [
            "Product so.",
            "So we define the mapping file.",
            "This way we substitute it into the product.",
            "We recover exactly the form of the Mercer kernel expansion in the crucial point here was that the Lambda eyes are non negative, otherwise we couldn't take the square root of them.",
            "So."
        ],
        [
            "So let's see I will actually first talk about this.",
            "So now that the real class of kernel that we should be talking about positive definite kernels.",
            "Just wondering so I have 1 1/2 hours in total, right?",
            "Do people usually take a short break or someone just asked a question during the break, which probably points at a certain omission in the first part of my talk?",
            "The question was what is a kernel?",
            "So I'm going to tell you now, what is a kernel?",
            "And by kernel, if there are no additional qualifications, I will always mean positive definite kernel.",
            "So I currently is a positive definite kernel.",
            "And positive definite kernel.",
            "Is defined as follows.",
            "It's a symmetric functions of two inputs.",
            "The inputs are taken from Sunset X, which I will only assume to be a nonempty set, nothing else.",
            "And the kernel has the property that whatever set of points we take from that set X, you can call it training points, but it doesn't matter.",
            "Take some points from that set X, take a corresponding number of real numbers coefficients, then this quantity.",
            "Here is non negative where this is the kernel matrix or gram matrix.",
            "So that's the matrix that we get if we compute all pairwise similarities between input points.",
            "So this quantity here should be non negative in exactly.",
            "In that case we call it a positive definite kernel.",
            "So this quantity here being non negative is the same as saying that this matrix here is positive definite.",
            "Some people call this positive semidefinite, some colleague positive definite, so I'll call it positive definite.",
            "And.",
            "In the slightly stricter case that other people called positive definite ionicons strictly positive definite.",
            "And in that slightly stricter case, it should be such that if the points are pairwise distinct, then this quantity is only zero if all coefficients are zero.",
            "So obviously, if the coefficients are zero, this quantity is 0, but if the other direction is also true.",
            "Then we call it strictly positive definite and we need this condition that the points are pairwise distinct because it turns out that otherwise, if you are allowed to put in the same points here you get linear dependencies and then this, this being 0 doesn't mean much.",
            "So.",
            "Anyway, let's focus on the positive definite case now.",
            "So remember, positive definite kernel is a kernel such that if we compute this matrix of pairwise similarity or the ground matrix kernel matrix.",
            "This matrix is positive definite.",
            "So it turns out that exactly for this class of kernels there exists a mapping into another space such that the kernel computes the dot product in that other space.",
            "And we're going to prove that, so yes.",
            "Is this the?",
            "It's not exactly the same as the condition in the Mercer kernel.",
            "So it turns out that the conditions in the Mercer kernel is slightly stricter.",
            "So for instance, in the version that I talked about.",
            "For instance, here there's a condition that case continuous.",
            "So in the in the I mean there are other forms of the Mercer theorem that have slightly different conditions.",
            "Here case continuous access, a compact space.",
            "Now in this definition of positive definite kernel, I don't talk about continuity, I I don't even talk about any topological properties.",
            "I just have some nonempty set X.",
            "So you could say this is a slightly more general case, so really this is the case we should be talking about the other one I only mentioned for completeness because you see in the literature occasionally, but we don't really need it, so let's focus on this one.",
            "This is a slightly larger class of kernels.",
            "OK, so let's prove that this is the right class occurrence in the sense that this is exactly the class of kernels which induces a wrapped product in another space.",
            "And this is also something where we will try to do a bit of interaction or a bit of.",
            "Calculations on the site.",
            "And to prove this we need.",
            "A number of properties, or actually the first part of the proof is actually this thing here.",
            "So if we have a mapping into dot product space.",
            "Then from that mapping we get a positive definite kernel.",
            "So that that would be easy to prove the other direction if we have positive definite kernel, how to get that mapping?",
            "That would be a little bit more tricky and it will probably take us half an hour or so.",
            "And as ingredients for the proof of that other direction, we need a couple of properties which I've written down here, and so this is also something that will try to do or everybody should try to do.",
            "And again, my motivation is if you just see this definition.",
            "Maybe if you don't know it before and you see it, then maybe you understand it and probably you understand every line of it, but it doesn't yet make much sense to you, so to understand the definition in mathematics you have to play with it and see how it works.",
            "What does it imply in this kind of stuff?",
            "So that's why we should do a little bit of exercise now and let me write down this definition here.",
            "So.",
            "Sorry.",
            "So we just tried all XI for all AI.",
            "This is true.",
            "OK, so now let's try to prove these four different properties and I'll.",
            "I'll give you some time for this, but maybe I'll give you a couple of hints before and how to."
        ],
        [
            "Seed.",
            "So.",
            "Once you've seen the approach afterwards, you would probably say they're trivial, but if you have never seen this definition of positive definite kernel before, then it will be non trivial to think about it and write them down, even though each one of them is maybe just two lines.",
            "So the first one is we have to prove that this is a positive definite kernel.",
            "To prove that this is a positive definite kernel, we just have to verify that this condition here holds true for a kernel which is defined like that.",
            "So that's a direct verification.",
            "You will have to use things like linearity of the dot product and so on, and then the next one.",
            "This may be slightly more tricky, even though it turns out there's a very simple, foolproof rate.",
            "So in this next one we have to prove that effectively that if the kernel is positive definite.",
            "So here in.",
            "If the kernel is positive definite, then its diagonal elements are positive or non negative.",
            "So Dien elements just K of X, X.",
            "Or if you look over the kernel matrix is the diagonal elements of the kernel matrix.",
            "So here in all these three cases, we assume it's positive definite.",
            "It's a little bit incomplete, so I should have written here, so here this is a complete statement, but these statements should all say if K is positive definite than this holds true.",
            "This or this.",
            "So here there is a simple proof, but maybe I won't tell you now because it's hard to give a hint without giving away the whole idea.",
            "It's very simple, but you think about it a little bit.",
            "Second one I have given you a hint here.",
            "It's with this hint.",
            "It's reasonably simple, but still it requires some thinking, and it's a general generalized Cauchy Schwarz inequality.",
            "Which you can probably see from here and the last one is this property that if a kernel is 0 on the diagonal.",
            "For all possible dragon elements, then it's actually zero everywhere, so that's the last property, so I'll leave these up on the screen and again I will try to remember whether I can still prove in myself and give you some time to do the proofs Meanwhile.",
            "And then we'll do it together.",
            "Smart.",
            "Oh I see the same laptop with me.",
            "Happen to have a spare of this.",
            "Is Ethernet adapters that you could lend me for later?",
            "You have to.",
            "I brought one.",
            "I don't need it.",
            "Otherwise, I cannot check email Michael.",
            "I don't need this.",
            "Wireless that we have for the room in the college.",
            "So is it OK if I give it back to you on Wednesday?",
            "It's my only one, so yeah.",
            "Both of them.",
            "Thank you.",
            "Yeah.",
            "Fancy green pointer with forward and backward parents.",
            "No, no, you know, I think it might not work with.",
            "I don't know because I have these things I need also.",
            "For some reason it doesn't work anymore.",
            "How many million words?",
            "Let me know, don't point it at anybody's eyes.",
            "Scary.",
            "Not less than one.",
            "That's reasonable.",
            "Your personal thing.",
            "Yeah, I'm taking all your gadgets, yeah, what else do you have this if you stick it in then the forward and backward areas will work OK.",
            "I'll try.",
            "You can try.",
            "That's so convenient.",
            "Yeah, sure.",
            "OK, maybe I'll wait.",
            "Now nothing works.",
            "Back now the only difference is I know I cannot do it with the keyboard anymore.",
            "Alright, forget it.",
            "Bad idea.",
            "Yeah.",
            "But it looks elegant, so for you it always works.",
            "Maybe you have to restart the door.",
            "OK, so let me ask.",
            "How many of you have solved one or more problems?",
            "OK, how many have solved two or more?",
            "Three or more.",
            "For Omar OK, how many have solved the first one?",
            "Is this the first sorry first problem is this one?",
            "How many have solved it?",
            "Well enough that they could come when so show us the solution here in the front.",
            "I think most of you probably have to.",
            "OK, we have a volunteer excellent.",
            "Thank you.",
            "You can choose any color, any black box.",
            "And where is the light?",
            "We want to show that.",
            "Wait, I'll give you.",
            "Wanna show that?",
            "Positive.",
            "So in the definition of positives you're showing this one right.",
            "The positivity on the diagonal OK?",
            "Positive semi definite.",
            "We know that this is true.",
            "So choose XI and XJ to be the same.",
            "So X one X2.",
            "So and choose a 1 = a two which is equal to 1.",
            "For example.",
            "So then if you compute this sum.",
            "It's basically come over.",
            "One one times.",
            "Porter so that's equal.",
            "For.",
            "Which is greater equal to 0 by this property.",
            "So that means sex is very important.",
            "OK, thank you very much.",
            "So you get you get a point.",
            "C1 right yeah?",
            "And OK, so maybe this is interesting proof.",
            "That's one that I haven't seen before.",
            "But it is correct, so it's a proof, and it also generalizes to endpoints you you could as well have used endpoints if you wanted to, so the way I usually do it is I just take one point, but it's perfectly fine, so it is perfectly fine to use two points.",
            "So if you just for completeness, if you take only one point.",
            "Because here, OK, this is a bit short, but if we go back to the definition.",
            "It just says any set of training points, so I can also choose a set of one points you chose one of two points for choose just one point, and I also choose the coefficients to be one.",
            "Then it's another possible proof.",
            "OK, so that's great.",
            "So we've done the second one.",
            "Let's do another one.",
            "OK.",
            "Which one are you going to prove?",
            "OK, the first one.",
            "We need to prove.",
            "I.",
            "Engine.",
            "Yes.",
            "32 G. What we do here is we just take IA I2 first.",
            "2nd.",
            "I.",
            "Next here.",
            "AG.",
            "?",
            "The same.",
            "We have the same.",
            "Here is that.",
            "Mark Yep, Yep, that's so because because here we say, is mapping into a dot product space.",
            "So this thing is a dot product and we know that products are positive definite.",
            "And that's exactly what you're using.",
            "So if you take a point with itself, the result would be non negative.",
            "So thank you so I'll have to.",
            "So what's your name?",
            "OK. And maybe for those who cannot read the funds.",
            "Looks a bit like when I'm having latech problems.",
            "I sometimes get these very tiny symbols.",
            "OK, so he's just moving the coefficients and the sums, so some over I and the coefficient AI he moves into the first argument in the sum over J coefficient AJ into the second argument, and then it looks like that.",
            "OK, so.",
            "How about the third one?",
            "Metric.",
            "The 4th one OK, 4th one although the 4th one should be done after the third one because I guess.",
            "But so maybe if someone else does the third one OK.",
            "If you want you can was one of those over there, yeah.",
            "Conditioning definition it's OK has to be understood.",
            "Just applying some quadratic form.",
            "With this yeah.",
            "For all choices of ANX provided, you're on the good dimension.",
            "So this metrics this graph metrics she doesn't need to be positive semidefinite, some definite so it has at least to have a positive determinant.",
            "And in dimension 2 this is exactly this.",
            "Inequality.",
            "Metric.",
            "This proposal was determined.",
            "Why does it have opposed to terminate?",
            "Has at least you have two positive eigenvalues?",
            "So one way to argue is the determinant is the product of the eigenvalues.",
            "Sorry.",
            "So so so that he's requiring a little bit of knowledge of some linear algebra background.",
            "Saying that a positive definite matrix or positive semidefinite in your Note 8 terminology has non negative eigenvalues.",
            "Hence the product of the eigenvalues is also non negative in the product is the determinant.",
            "Invention two we could just choose 2.6 and explain this inequality is just expressing this this determination.",
            "Does everybody everybody see that or you want you want you to work all the time?",
            "Maybe write down the determinant just so that people see it?",
            "Maybe it's too small, but.",
            "Just.",
            "XX.",
            "Explain plan.",
            "Square dysfunction is symmetry.",
            "Yep.",
            "So this is just a question.",
            "Oh, OK, thank you.",
            "Let me see your name.",
            "OK, thank you Amy.",
            "Any questions today me?",
            "OK, so now 4th one.",
            "We have we have here.",
            "Just zeros.",
            "But if you want to point, you'll have to come up here.",
            "Sweet.",
            "And since we have.",
            "X. Brian square 0.",
            "Yeah.",
            "OK, OK yeah.",
            "So that the square of all elements is zero, therefore they have to be 0 is bounded from above by zero in the square is also for bounded from below by zero since it's a square so it has to be equal to 0.",
            "The element has to be equal to 0.",
            "And this argument works for any XX prime.",
            "OK, thank you.",
            "So.",
            "OK, so we have to use all these properties and we're going to prove in.",
            "Our remaining time, or maybe we'll finish tomorrow.",
            "How to construct the feature space?",
            "So basically we prove the opposite direction of this one proof given a positive definite kernel.",
            "How to construct the feature space?",
            "And maybe, maybe we'll manage to to prove it.",
            "So is it like very strict half past the.",
            "Coffee break.",
            "Few minutes, so let's see if maybe I can do it with a few minutes more, otherwise will continue.",
            "Will finish it tomorrow.",
            "So we have one too.",
            "Five slides"
        ],
        [
            "OK, so.",
            "And the feature map will be defined as follows given an input point, we map it to a point to a function.",
            "In this space, R to the power of X.",
            "By this I simply mean functions mapping X to R. And the mapping will be such that the point X is assigned to the function that we get if we substitute X into.",
            "The second argument of the kernel could also be.",
            "The first one doesn't matter.",
            "And leave the first argument open.",
            "So this is a function of the first argument, which, given a point in X that we substitute in here gives us a real number.",
            "So for instance, if we had a Gaussian kernel function, then our point X would be mapped into a Gaussian centered on X.",
            "Point X prime would be mapped into a Gaussian sent on the next prime.",
            "So what we'll have to do is we have to turn this into a linear space.",
            "We are back to space, in other words.",
            "We have to then construct the DOT product which satisfies the condition that we want to be satisfied.",
            "So the product should compute the kernel function.",
            "In our special case, we've already defined the feature map.",
            "If I substitute this feature map file into here, then this equation boils down to this one, which is the reason why people sometimes call these kernels reproducing kernels.",
            "So you put the kernel with itself.",
            "It gives you the kernel.",
            "And then the last step will be a trivial one that we won't talk much about."
        ],
        [
            "OK, so that's basically what's left to do, so the first one in a way is trivial.",
            "We just formally declare that all such.",
            "Linear combinations, so we know that this kind of points they are in our.",
            "They are already in our space, so we I mean if we take this individual point it will be mapped to such a function.",
            "So all these Gaussian bumps are now in our feature space because they are images of input points.",
            "But we'll just declare that linear combinations of such bumps will also be in our space.",
            "So we take the linear completion of the space.",
            "So for instance, this function G in this function F they are elements of our space where M&M prime, arbitrary numbers, natural numbers or integers, natural positive, and these are real numbers of coefficients.",
            "And these points these expansion points.",
            "They are from our input domain.",
            "So we'll just say."
        ],
        [
            "These are in our space, so now we have to construct a dot product and that product takes as inputs, functions, FNG and so far the general form of a function of our space.",
            "Is this or this, so we'll take these functions as inputs and we'll just say that product takes this value here.",
            "So in particular, if the functions F&G, we're just.",
            "So if you forget about this, someone the Alpha I so it's just.",
            "One kernel centered on one point, and here are the same on another point.",
            "In that special case, all this stuff here in the front would be gone in the dot product between these two individual kernels.",
            "So the functions are not individual kernels is a kernel, so this is consistent with what we want to have.",
            "If you remember from before we we wanted to have that this is true, and for our special feature map this boils down to this thing here.",
            "So basically the product that we define is just a linear extension of this requirement.",
            "It's the only way we can do it.",
            "OK, so then from the mathematical point of view we have to define that this is we have to show that this is well defined.",
            "By this I mean.",
            "Principle, it's possible that the same function can be written in terms of different points in different coefficients.",
            "It turns out, for some kernels, usually this is possible, unless the kernel is strictly positive definite, so this need not be unique.",
            "This expansion.",
            "Same for this expansion of trade need not be unique, so of course the definition should be the dot product should just be a property of the functions and not of the expansion coefficients.",
            "If I can write the same function with different sets of expansion.",
            "Some points I should not get different values that the dot product otherwise dot product is not a property of the functions.",
            "So we have to do is well defined in one way to show it is simply.",
            "If we look at this thing here.",
            "So let's remember for a second.",
            "G is defined, so try to put this into your iconic memory for 10 seconds.",
            "So G is the sum over the bit us in the kernel centered on X prime.",
            "First argument is open.",
            "If we now substitute XI into G. What do we get?",
            "We get bet some over the bitters and K of XI, XJ prime.",
            "It's exactly what we have here.",
            "XI XJ prime, the bitterness and the sum over the bed has.",
            "So what we get is just this is equal to sum over Alpha I of the GS evaluated at XI.",
            "OK, so if you didn't follow this, you can.",
            "You will get the slides.",
            "You can look at it afterwards.",
            "With the same argument we can.",
            "We can change this expression into this thing here.",
            "Some over the bitters, and F evaluated the XJ prime.",
            "So what does that mean?",
            "The first equation here shows us that this definition is independent of the betos and independent of the X primes.",
            "OK, it doesn't appear anywhere here.",
            "Second expression shows us that.",
            "The value of this quantity is independent of the alphas and independent of the size.",
            "So overall it doesn't depend on any of these coefficients explicitly.",
            "It only depends on the functions F&G, so it's well defined.",
            "So that's the first step.",
            "Second step is.",
            "It's symmetric, that's clear because the kernel function is symmetric and taking products between Alpha and beta is a symmetric operation, so we can.",
            "We can if we change F&G, it will just effectively change the order of the sums.",
            "The other founder bitters in here the order of the arguments and we can undo these changes.",
            "Back here is no problem.",
            "It's also by linear, so if we had functions F + F + F prime in here if we went through the algebra we would see this would be.",
            "So, for instance we would get.",
            "I don't know can you can you read that so?",
            "So we have by linearity we also would have something like Alpha.",
            "G. So we can take coefficients out.",
            "So all this follows trivially from the fact that we're just taking.",
            "Sums and products here, so it's it's what people call a symmetric bilinear form."
        ],
        [
            "So far.",
            "I've already mentioned one special case where F&G where individual kernels if only one of the functions is an individual kernel.",
            "And then as a special case, we get this quantity here.",
            "So that that here was the special case where both are individual kernels, so that's the reproducing kernel property.",
            "This first property is also interesting because it says that if we take this, what will end up to be a dot product.",
            "So far I can only call it the symmetric bilinear form between an arbitrary function and a kernel centered on a point X, and the result is the function evaluated at X.",
            "So people rephrase this as saying by saying the kernel.",
            "Is a representative point evaluation, so in that space we can evaluate the function at a point by taking a dot product with the kernel.",
            "So that's that's a nice property, turns out.",
            "Anyway, so that's the last slide that we need for today.",
            "So we have a few more steps to show.",
            "So far we know that this thing here is a symmetric bilinear form.",
            "The next thing is will show it's actually a positive definite kernel in its own right.",
            "So we started with the positive definite kernel K. Then we defined this thing here.",
            "This thing here so K is a positive definite term.",
            "Now we define this thing here.",
            "It turns out this is also positive definite kernel.",
            "But on the space of functions.",
            "So this was a positive definite kernel on this SpaceX on this set X Now we have a positive definite."
        ],
        [
            "In the space of functions and once we know this, it will be nice because then we know we can use these properties here, which are true for positive definite kernels.",
            "So let's show it's a positive definite kernel.",
            "So to see this so on this on our set of functions which are linear combinations of individual kernels.",
            "So we have to take some functions FIFJ and put arbitrary real coefficients in front of them.",
            "So we have to check this condition.",
            "Now for the functions is satisfied.",
            "This is not negative this quantity, so let's compute this quantity.",
            "First of all, we know that this thing here is bilinear.",
            "So we take we can take the coefficients inside and we can take the sums inside.",
            "So if we do that, we get this quantity here now.",
            "This again like in our calculation before here.",
            "Will see this is 1 vector taking the symmetric bilinear form with itself, so let's just call this F this vector.",
            "We don't know the exact coefficients, but we know it's still an element of our space because trivially if we take linear combinations of linear combinations, we get just another linear combination.",
            "So this is also can be written in terms of kernels and we'll just right in terms of kernels with these coefficients.",
            "Alpha I and points XI.",
            "We know we don't know how many kernels we will need, but we know this is now space and we can write it this way.",
            "So this function called this F can be written like this.",
            "Now we use the bilinearity again.",
            "Of our bilinear form, we take these coefficients in the sums outside.",
            "Again, we get this quantity here.",
            "Here we have what's left inside is the kernel with itself.",
            "We know that the kernel with itself reproduces gives us a kernel again.",
            "And what we're left with here is this term.",
            "And now we know that K is a positive definite kernel on our input points.",
            "Therefore, we know that this quantity is not negative.",
            "So now we know also that this quantity is non negative, which was all that we had to show.",
            "That is positive definite kernel.",
            "So this.",
            "So if this is a positive definite on our input set then this is a positive definite kernel on our set of functions.",
            "So that's nice.",
            "Now we know it's a symmetric bilinear form, and a positive definite kernel.",
            "To show that it's a dot product.",
            "So what is the dot product dot product is a symmetric bilinear form which is strictly positive definite.",
            "By a strictly positive definite for that product, I mean that whenever I take the dot product of something with itself and the value is 0, then this thing has to be 0.",
            "That's all that's left to proof.",
            "For this to be a dot product.",
            "So let's see.",
            "So the way to show this is we take F of X, let's square it and we first express it in terms of as a PT evaluation.",
            "So the dot product of the function with the kernel at X.",
            "So remember from the previous slide.",
            "I was telling you Colonel represents PT evaluation, so we can write it like this.",
            "So now we're going to use the code Schwarz inequality, which we have proven before, so I don't know where it is.",
            "And.",
            "I think it's this thing here over here.",
            "So we can we can.",
            "We know that this angular bracket thing is a positive definite kernel, so their position Schwartz inequality holds, so the kernel between them external between the two different elements it's square can be upper bounded by Colonel of the first element with itself and the kernel of the second element with itself.",
            "So we do this upper bounding thing now again.",
            "Here we have a kernel with itself, but on X&X, so we get K of XX.",
            "OK.",
            "So therefore.",
            "Here so we have an upper bound on this square.",
            "And if we now try to set the dot product or the angular bracket thing of F with itself, we still cannot call it the dot product to 0 here.",
            "Then we get a zero on the right hand side.",
            "So this quantity is upper bounded by zero.",
            "It's also lower bounded by zero.",
            "Therefore, F of X has to be 0, and that's independent of which point XI put.",
            "So, whenever F, F is zero, we know that F of X is 0 for all X for the function is 0.",
            "So this symmetric bilinear form is positive definite, so it's a dot product.",
            "And now we have what people call a pre Hilbert space or a little product space.",
            "So it's we don't know yet whether it's complete, but we can always complete the space so one can add limit points of course sequences.",
            "But don't worry about this.",
            "To get what's called the Hilbert space.",
            "But basically this is the feature space, so I don't want to eat more into your coffee time.",
            "Maybe this was a little bit fast and I might repeat 2 minutes tomorrow and then tomorrow we'll get to kernel algorithms and support vector machines.",
            "OK, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So good morning everybody.",
                    "label": 0
                },
                {
                    "sent": "You'll be able to hear me in stereo.",
                    "label": 0
                },
                {
                    "sent": "I'm all wired up so.",
                    "label": 0
                },
                {
                    "sent": "This one is the local local 1.",
                    "label": 0
                },
                {
                    "sent": "This is for the camera, everything.",
                    "label": 0
                },
                {
                    "sent": "OK so a message to the people who will watch this video later on.",
                    "label": 0
                },
                {
                    "sent": "Don't watch it.",
                    "label": 0
                },
                {
                    "sent": "It's very similar to the other one that's already on the website.",
                    "label": 0
                },
                {
                    "sent": "So I'm gonna check off and I don't know most of you, but I would like to know you.",
                    "label": 0
                },
                {
                    "sent": "Of course there's not enough time, but maybe we can take a quick poll, for instance.",
                    "label": 0
                },
                {
                    "sent": "Who's who's a PhD student here?",
                    "label": 0
                },
                {
                    "sent": "Who's pre PhD?",
                    "label": 0
                },
                {
                    "sent": "Who is a postdoc?",
                    "label": 0
                },
                {
                    "sent": "Who's who's a lecturer?",
                    "label": 0
                },
                {
                    "sent": "Who is a senior lecturer?",
                    "label": 0
                },
                {
                    "sent": "Who's a reader who is a professor?",
                    "label": 0
                },
                {
                    "sent": "Daniel.",
                    "label": 0
                },
                {
                    "sent": "Who is a professor at Cambridge?",
                    "label": 0
                },
                {
                    "sent": "So and also about the background, is that a question or a professor?",
                    "label": 0
                },
                {
                    "sent": "Turn it up, I'll try.",
                    "label": 0
                },
                {
                    "sent": "Was that loud enough?",
                    "label": 0
                },
                {
                    "sent": "I can just move it higher.",
                    "label": 0
                },
                {
                    "sent": "How about that?",
                    "label": 0
                },
                {
                    "sent": "Or I hold it like this?",
                    "label": 0
                },
                {
                    "sent": "No?",
                    "label": 0
                },
                {
                    "sent": "I mean I can, at least while I don't use my hand for something else, I can only like this.",
                    "label": 0
                },
                {
                    "sent": "Where would I stop?",
                    "label": 0
                },
                {
                    "sent": "OK, so about the background would be I would be interested to know who's a computer scientist.",
                    "label": 0
                },
                {
                    "sent": "Who is engineer?",
                    "label": 0
                },
                {
                    "sent": "Who is the mathematician?",
                    "label": 0
                },
                {
                    "sent": "Physics.",
                    "label": 0
                },
                {
                    "sent": "Psychology.",
                    "label": 0
                },
                {
                    "sent": "I don't know cognitive science.",
                    "label": 0
                },
                {
                    "sent": "What have I forgotten?",
                    "label": 0
                },
                {
                    "sent": "Neuroscience.",
                    "label": 0
                },
                {
                    "sent": "Any, any anything else philosophy?",
                    "label": 0
                },
                {
                    "sent": "No philosophers we've left the stage of philosophy in this field.",
                    "label": 0
                },
                {
                    "sent": "It's becoming technical so.",
                    "label": 0
                },
                {
                    "sent": "OK, and maybe also who knows?",
                    "label": 0
                },
                {
                    "sent": "Support vector machines.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's maybe everybody knows it anyway.",
                    "label": 0
                },
                {
                    "sent": "And who knows who would be able to derive the dual optimization problem for support vector classifier?",
                    "label": 0
                },
                {
                    "sent": "I won't ask you to come to the back door.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I guess most of you have heard of it in know it approximately, and probably I won't spend too much time on support vector machines anyway, I'll I'll try to talk about about kernel methods in general.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So to give you this doesn't work, and I'll try to give you some motivation for why you should be sitting here early in the morning.",
                    "label": 0
                },
                {
                    "sent": "So support link machines started around 15 years ago or something like that.",
                    "label": 0
                },
                {
                    "sent": "And I was in the lucky situation that I was a PhD student who at that time was working at Bell Labs where.",
                    "label": 0
                },
                {
                    "sent": "But Nick and other people were working developing support vector machines, so I gotta know it early on was in that field from the start.",
                    "label": 0
                },
                {
                    "sent": "At the beginning nobody knew it at the beginning you had to.",
                    "label": 0
                },
                {
                    "sent": "Even when you wrote a paper, you always had to compare things to neural networks at at that time.",
                    "label": 0
                },
                {
                    "sent": "Neural networks where the standard words nowadays, if you come up with a new classification algorithm, people reviewers will always ask you to benchmark it well.",
                    "label": 0
                },
                {
                    "sent": "How does it work?",
                    "label": 0
                },
                {
                    "sent": "For instance, your problem with an SVM?",
                    "label": 0
                },
                {
                    "sent": "How does it compare to an SVM?",
                    "label": 0
                },
                {
                    "sent": "So it's like a commodity is standard classifier now.",
                    "label": 0
                },
                {
                    "sent": "So which is one reason why you should know it?",
                    "label": 0
                },
                {
                    "sent": "The other reason why you should know it is that it contains some nice tricks, especially.",
                    "label": 0
                },
                {
                    "sent": "Related to kernels.",
                    "label": 0
                },
                {
                    "sent": "And these tricks have been used in other domains as well, and it's somehow part of the standard machine learning Toolkit.",
                    "label": 0
                },
                {
                    "sent": "Now to know how kernels work, to be able to combine them with whatever else you're working on.",
                    "label": 0
                },
                {
                    "sent": "So it's a bit like.",
                    "label": 0
                },
                {
                    "sent": "The time of neural network has taught us that there's the chain rule and how to use it.",
                    "label": 0
                },
                {
                    "sent": "Now there's something additional is kernel tricks and it'll be the functional analysis.",
                    "label": 0
                },
                {
                    "sent": "So we should learn this in our community and be able to use it even if we don't work on support vector machines.",
                    "label": 0
                },
                {
                    "sent": "And I don't want to convince anyone to work and support vector machines.",
                    "label": 0
                },
                {
                    "sent": "In fact, I don't work on it myself anymore.",
                    "label": 0
                },
                {
                    "sent": "I still use them, but.",
                    "label": 0
                },
                {
                    "sent": "If you want to do research when it's nowadays it's not so easy to do something new in support vector machines, you can still do a lot of things with kernels of course, which is probably why you're here.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's see.",
                    "label": 0
                },
                {
                    "sent": "How to operate this?",
                    "label": 0
                },
                {
                    "sent": "OK, I have to use the mouse wheel so we might occasionally skip one slide 'cause I moved too far, let's see.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll start talking about kernels coming from the intuitive notion of what's the similarity?",
                    "label": 0
                },
                {
                    "sent": "I'll talk a bit about kernels about feature spaces.",
                    "label": 1
                },
                {
                    "sent": "And in the next hour, I don't know how long this will take.",
                    "label": 0
                },
                {
                    "sent": "Maybe one hour next hour.",
                    "label": 0
                },
                {
                    "sent": "I'll be a bit more mathematical.",
                    "label": 0
                },
                {
                    "sent": "Tell you what's a positive definite kernel, how to construct the feature space, which is called a reproducing kernel Hilbert space, and I'll tell you a bit about a nice application of kernels which are called kernel means, and finally probably say a little bit about support vector machines, so I think I'll have to find a better way.",
                    "label": 1
                },
                {
                    "sent": "Wait, let me try to.",
                    "label": 0
                },
                {
                    "sent": "Put this here again.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so suppose we are interested in input output.",
                    "label": 0
                },
                {
                    "sent": "Learning the simplest form, super supervised learning and we are given two sets X&Y.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "X&Y and training set of pairs taken from X&Y, so XIYI we want to somehow learn from this training set, which means something like given or we want to generalize given at previously unseen X and new input, find a suitable output for it, so somehow this new pair XY should be similar to the ones that we've seen before, so this begs the question, how do we measure similarity and?",
                    "label": 1
                },
                {
                    "sent": "For outputs, typically, although not always, but that's the traditional view, similarity somehow was measured using a loss function.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if we're talking about classification, we have only two class classification.",
                    "label": 0
                },
                {
                    "sent": "We have two possible outputs, plus, minus one.",
                    "label": 0
                },
                {
                    "sent": "Let's call in past minus one.",
                    "label": 0
                },
                {
                    "sent": "We might want to use the 01 loss that simply checks whether output is 2.",
                    "label": 0
                },
                {
                    "sent": "Outputs are the same or whether they're different, or it checks whether our predicted output is the same as the true one, or whether it's different.",
                    "label": 0
                },
                {
                    "sent": "For the input, on the other hand, we can measure the similarity using something called a kernel, and maybe I should already add a caveat.",
                    "label": 0
                },
                {
                    "sent": "One can construct kernel methods that somehow measure the similarity of this pair to the other ones jointly, taking into account input and output, but that's not the topic.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our lecture here, so let's focus now on measuring similarity of inputs.",
                    "label": 1
                },
                {
                    "sent": "So one simple way of measuring the similarity between two inputs would be to define a symmetric function that takes 2 inputs and assigns them to a real number which somehow captures our similarity in that function could for instance be the Canonical dot product.",
                    "label": 0
                },
                {
                    "sent": "So there's a little bit of notation, so this means sometimes X sub I is a training example.",
                    "label": 0
                },
                {
                    "sent": "So if I want the right coordinate of a training example, I put this.",
                    "label": 0
                },
                {
                    "sent": "Brackets around.",
                    "label": 0
                },
                {
                    "sent": "So that's the dot product.",
                    "label": 1
                },
                {
                    "sent": "I think everybody knows what a dot product is, or at least everybody knows this Canonical standard product product of the coordinates summed over all coordinates.",
                    "label": 0
                },
                {
                    "sent": "Now of course you can only do this if our input domain is a vector space that product space, or in this specific this specific case it's a Euclidean or indoor product space.",
                    "label": 0
                },
                {
                    "sent": "Now if our input domain is not a dot product space.",
                    "label": 1
                },
                {
                    "sent": "So for instance, if our data are strings or some kind of adenoids, let's say it's a manifold or something like that, then we might still be able to use something like this.",
                    "label": 0
                },
                {
                    "sent": "If we assume that our similarity measure K. Has it representation as a dot product in some other space unit or product space which will call H because they will be later on there will be Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "So in this by representation I mean there exists a map from our input domain to adore product space such that the similarity measure is given by the DOT product in that other space.",
                    "label": 1
                },
                {
                    "sent": "So we first map our points into that provide space.",
                    "label": 1
                },
                {
                    "sent": "Then we take the dot product in that space.",
                    "label": 0
                },
                {
                    "sent": "So we can think of this mapping as a preprocessing of our input patterns.",
                    "label": 0
                },
                {
                    "sent": "We can think of the patterns actually as 5X and 5X prime, and then we can do whatever we can do in terms of the product so we can do geometry.",
                    "label": 0
                },
                {
                    "sent": "We can come up with all sorts of geometric algorithms in that the product space, which is sometimes called the feature space.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's already the basic idea of how kernel methods work.",
                    "label": 0
                },
                {
                    "sent": "Of course there are some ramifications later, so let's think of a simple or maybe the simplest possible algorithm one could do in such a representation.",
                    "label": 0
                },
                {
                    "sent": "So the simplest possible geometric classification algorithm.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have.",
                    "label": 0
                },
                {
                    "sent": "A set of, let's say, negative points.",
                    "label": 0
                },
                {
                    "sent": "These circles here are set of positive points or two classes, and we want to classify a new point X.",
                    "label": 0
                },
                {
                    "sent": "Then we can do it by first computing the two class means.",
                    "label": 1
                },
                {
                    "sent": "So suppose we have.",
                    "label": 0
                },
                {
                    "sent": "Mplus is the number of positive examples, so we sum overall positive examples.",
                    "label": 0
                },
                {
                    "sent": "The Five XI we take.",
                    "label": 0
                },
                {
                    "sent": "We divide by the number so we get the mean of the positive class.",
                    "label": 0
                },
                {
                    "sent": "Actually this.",
                    "label": 0
                },
                {
                    "sent": "Should be C plus.",
                    "label": 0
                },
                {
                    "sent": "Here it says C2 and they should be C minus.",
                    "label": 0
                },
                {
                    "sent": "Here we have the same for the negative class.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "One can simply assign our new point through the class whose mean is closer to it.",
                    "label": 0
                },
                {
                    "sent": "So if we think about this for a second, what kind of decision rule does this induce?",
                    "label": 0
                },
                {
                    "sent": "So the set of all points that's closer to this point than it is to this point.",
                    "label": 0
                },
                {
                    "sent": "It's a half space which is separated from the other half space by this hyperplane.",
                    "label": 0
                },
                {
                    "sent": "And we can check whether a point is closer to here or to here, for instance by computing for this vector W, connecting the two class means.",
                    "label": 0
                },
                {
                    "sent": "And then see the midpoint along this connection.",
                    "label": 1
                },
                {
                    "sent": "Then we can compute this vector X -- C. X is our test point represented in the feature space.",
                    "label": 0
                },
                {
                    "sent": "And then we simply have to check whether this vector in this vector includes an angle smaller than 90 degrees or larger than 90 degrees.",
                    "label": 0
                },
                {
                    "sent": "So, and to do that we just have to check we just have to compute the DOT product between these two vectors.",
                    "label": 0
                },
                {
                    "sent": "If the dot product between the two vectors is positive, then the angle is smaller than 90 degrees.",
                    "label": 1
                },
                {
                    "sent": "If the dot product is negative.",
                    "label": 0
                },
                {
                    "sent": "The angle is larger than 90 degrees, so that's one simple way of doing things.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if we.",
                    "label": 0
                },
                {
                    "sent": "Go through the algebra.",
                    "label": 0
                },
                {
                    "sent": "We just substitute everything.",
                    "label": 0
                },
                {
                    "sent": "We get a simple solution.",
                    "label": 0
                },
                {
                    "sent": "So we substitute everything into here.",
                    "label": 0
                },
                {
                    "sent": "We then replace the DOT products between points mapped into the feature space by kernel functions.",
                    "label": 0
                },
                {
                    "sent": "OK so back here I have a constant which just is all the terms that don't depend on X.",
                    "label": 0
                },
                {
                    "sent": "And then we get this decision rule which is now.",
                    "label": 0
                },
                {
                    "sent": "A kernel based decision rules, so it's an expansion in terms of kernels.",
                    "label": 0
                },
                {
                    "sent": "Sentence here on all positive points when he sent it on, all negative points here some constant.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this.",
                    "label": 0
                },
                {
                    "sent": "Maybe for the ones actually I didn't ask before.",
                    "label": 0
                },
                {
                    "sent": "Are there people who are statisticians here?",
                    "label": 0
                },
                {
                    "sent": "That's what I forgot.",
                    "label": 0
                },
                {
                    "sent": "OK, we have three statisticians also.",
                    "label": 0
                },
                {
                    "sent": "So for the statisticians.",
                    "label": 0
                },
                {
                    "sent": "In some cases, depending on what kind of kernel one chooses, but for instance, if we choose a Gaussian function as a kernel and we normalize it such that it has integral one, so then this is some kind of like a valid density model then.",
                    "label": 0
                },
                {
                    "sent": "This is also identity model.",
                    "label": 0
                },
                {
                    "sent": "We normalized by the number of such kernels.",
                    "label": 0
                },
                {
                    "sent": "So this is just like a pulse in Windows density estimator of the positive class and this is a pause in Windows 10 TS density estimator for negative class.",
                    "label": 0
                },
                {
                    "sent": "So in this case then this simple geometric classification rule in the feature space ends up giving us something like a something like a classifier based on like a plug-in classification rule based on parsing Windows density estimates.",
                    "label": 0
                },
                {
                    "sent": "So that's a very simple algorithm and.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "This would be a good time to take a few minutes and try everybody to derive this classifier in a slightly different way.",
                    "label": 0
                },
                {
                    "sent": "So what I'd like you to try, it's my experience.",
                    "label": 0
                },
                {
                    "sent": "You learn a lot more if you try a little bit yourself also, so I'd like you to try.",
                    "label": 0
                },
                {
                    "sent": "To derive this classification rule here.",
                    "label": 0
                },
                {
                    "sent": "This one or something like this one.",
                    "label": 0
                },
                {
                    "sent": "Maybe you'll have slightly different signs somewhere, but I'd like you to derive this classification rule simply by directing computing the distance.",
                    "label": 0
                },
                {
                    "sent": "So you compute distance between this point at this point in the feature space.",
                    "label": 0
                },
                {
                    "sent": "And you subtract the distance between these two points.",
                    "label": 0
                },
                {
                    "sent": "Now distances something one can compute using their products, right?",
                    "label": 0
                },
                {
                    "sent": "You can use this squared distance between these two points.",
                    "label": 0
                },
                {
                    "sent": "You subtract the squared distance between these two points.",
                    "label": 0
                },
                {
                    "sent": "And then at the end you take a take the sign of this difference.",
                    "label": 0
                },
                {
                    "sent": "And let's see whether you get something that's similar to this one here.",
                    "label": 0
                },
                {
                    "sent": "So let's take a few minutes and try this out.",
                    "label": 0
                },
                {
                    "sent": "Everybody on his own or are in teams or whatever, and I will try to try it myself.",
                    "label": 0
                },
                {
                    "sent": "Whether I can still do it also.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if I don't know which slide I should be leaving out, maybe I'll leave this one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so how many people have found the solution yet?",
                    "label": 0
                },
                {
                    "sent": "I have to ask this now because we are in this phase transition where the last sound level is increasing.",
                    "label": 0
                },
                {
                    "sent": "I remember I was once at a talk not far from here by Stephen Hawking and someone someone asked a question after the talk and then he started.",
                    "label": 0
                },
                {
                    "sent": "Preparing the answer, which is difficult for him with some kind of machine that he is using.",
                    "label": 0
                },
                {
                    "sent": "So at the beginning people were waiting very quietly for the answer, but it took around.",
                    "label": 0
                },
                {
                    "sent": "I would say 10 or 15 minutes to prepare the answers.",
                    "label": 0
                },
                {
                    "sent": "So after 10 minutes, like gradually there there was.",
                    "label": 0
                },
                {
                    "sent": "This increases the noise level and after when he really answered you were sorry it was as loud as in the train station when you started answering.",
                    "label": 0
                },
                {
                    "sent": "Suddenly everything was super quiet again, but the answer was just one sentence so, but how much more time do people need to or how many people have solved it by now?",
                    "label": 0
                },
                {
                    "sent": "So I would say that's more than half and is there someone who would like to have more time?",
                    "label": 0
                },
                {
                    "sent": "OK, so then we can get to the solution and we can either do the way such that I do it here or someone else does it and I'll have to think about some kind of reward.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll have three or four such problems during my lectures, and whoever solves the largest number will get some kind of reward.",
                    "label": 0
                },
                {
                    "sent": "So if you solve this one, you could.",
                    "label": 0
                },
                {
                    "sent": "You could do it here.",
                    "label": 0
                },
                {
                    "sent": "You should probably be reasonably confident that you have solved it, because the solution is here, so you have solved it.",
                    "label": 0
                },
                {
                    "sent": "You know whether you got it right or not so.",
                    "label": 0
                },
                {
                    "sent": "There's no reason to be to be worried about doing the black Bolt is like your chance to lecture in the machine Learning Summer School.",
                    "label": 0
                },
                {
                    "sent": "So you've done this before.",
                    "label": 0
                },
                {
                    "sent": "We've usually always had one or two who were prepared to do it.",
                    "label": 0
                },
                {
                    "sent": "Anyone, anyone, courageous?",
                    "label": 0
                },
                {
                    "sent": "I raised the bar too high.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll do this one myself.",
                    "label": 0
                },
                {
                    "sent": "I think you just want to see me suffer on the blackboard here and I'll do this one myself.",
                    "label": 0
                },
                {
                    "sent": "But then the next problem someone will have to do it.",
                    "label": 0
                },
                {
                    "sent": "So let's see if there's a chalk.",
                    "label": 0
                },
                {
                    "sent": "It must be chopped pieces of mathematics Department.",
                    "label": 0
                },
                {
                    "sent": "Actually, I'll use this one.",
                    "label": 0
                },
                {
                    "sent": "Or which is the sound program is here or there?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "OK, so we said we have to check whether the point is closer to the positive mean or closer to the negative mean.",
                    "label": 0
                },
                {
                    "sent": "So our decision rule.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Now should be easy, yeah?",
                    "label": 0
                },
                {
                    "sent": "In addition room we should be checking weather.",
                    "label": 0
                },
                {
                    "sent": "Our test point is closer to the positive work through the negative million and.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I think this is the right way wrong.",
                    "label": 0
                },
                {
                    "sent": "Oh and I'm sure I'll make some mistakes, so whoever splits a mistake, it also gets a point because I'm not usually lecturing.",
                    "label": 0
                },
                {
                    "sent": "I'm just a researcher at the Max Planck Institute, so we don't have to.",
                    "label": 0
                },
                {
                    "sent": "We go down to get a lecture at universities.",
                    "label": 0
                },
                {
                    "sent": "So and so.",
                    "label": 0
                },
                {
                    "sent": "Let's see if this is right.",
                    "label": 0
                },
                {
                    "sent": "So if the distance to the.",
                    "label": 0
                },
                {
                    "sent": "If it's closer to the positive point, then the distance to the negative closer to the positive mean means the distance of the negative mean is larger.",
                    "label": 0
                },
                {
                    "sent": "Which case this is positive.",
                    "label": 0
                },
                {
                    "sent": "We assign the point of the positive class, so that would be the right way around.",
                    "label": 0
                },
                {
                    "sent": "I am so let's work this out.",
                    "label": 0
                },
                {
                    "sent": "So here we use the dot product, so this is just the product.",
                    "label": 0
                },
                {
                    "sent": "So alright and all the same thing again.",
                    "label": 0
                },
                {
                    "sent": "Sorry, negative.",
                    "label": 0
                },
                {
                    "sent": "Minus one.",
                    "label": 0
                },
                {
                    "sent": "And then we have.",
                    "label": 0
                },
                {
                    "sent": "Same thing over here.",
                    "label": 0
                },
                {
                    "sent": "Only with plus instead of minus.",
                    "label": 0
                },
                {
                    "sent": "So K office is the product of 5X with itself, so that you just pay off XX.",
                    "label": 0
                },
                {
                    "sent": "Pump.",
                    "label": 0
                },
                {
                    "sent": "I need a square.",
                    "label": 0
                },
                {
                    "sent": "This square this square is wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have one point already.",
                    "label": 0
                },
                {
                    "sent": "Actually, there's a second mistake.",
                    "label": 0
                },
                {
                    "sent": "So who cook at that point?",
                    "label": 0
                },
                {
                    "sent": "I forgot with you.",
                    "label": 0
                },
                {
                    "sent": "OK, the man with the green T shirt?",
                    "label": 0
                },
                {
                    "sent": "Alright, the dog.",
                    "label": 0
                },
                {
                    "sent": "I'll ask you for your name later.",
                    "label": 0
                },
                {
                    "sent": "I won't embarrass you more now.",
                    "label": 0
                },
                {
                    "sent": "Then OK so.",
                    "label": 0
                },
                {
                    "sent": "So this with this we've got.",
                    "label": 0
                },
                {
                    "sent": "Now, let's do the complicated one with the two sums.",
                    "label": 0
                },
                {
                    "sent": "And here we always have the dot product between 5XI and 5X J.",
                    "label": 0
                },
                {
                    "sent": "So this gives us a kernel between XI.",
                    "label": 1
                },
                {
                    "sent": "XJ OK, so I think this should be obvious if it's not obvious to you that one can take the sum out of the dot product.",
                    "label": 0
                },
                {
                    "sent": "And how these two summers combined to this single or to this double sum, and so on?",
                    "label": 0
                },
                {
                    "sent": "And think about it after the lecture, because that's something you should understand and and now we have a mixed term.",
                    "label": 0
                },
                {
                    "sent": "So this thing with this and actually we have this twice because this.",
                    "label": 0
                },
                {
                    "sent": "Combining with this will give us the same thing, so let's just write me a stool over.",
                    "label": 0
                },
                {
                    "sent": "And then this thing.",
                    "label": 0
                },
                {
                    "sent": "And here we have 5 XI with 5X.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I hope there no mistakes so far and then we've got this.",
                    "label": 0
                },
                {
                    "sent": "Basically the same thing.",
                    "label": 0
                },
                {
                    "sent": "Now with the difference that here we have a plus one and here we have N plus.",
                    "label": 0
                },
                {
                    "sent": "So, and of course, there's a A minus here in the front, so it seems so.",
                    "label": 0
                },
                {
                    "sent": "Minus.",
                    "label": 0
                },
                {
                    "sent": "And here we have loss.",
                    "label": 0
                },
                {
                    "sent": "He's a very soothing sounds.",
                    "label": 0
                },
                {
                    "sent": "Reminds me of that with mathematics classes at University, so.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see.",
                    "label": 0
                },
                {
                    "sent": "So this one goes away.",
                    "label": 0
                },
                {
                    "sent": "And then we are left with something that doesn't depend on X.",
                    "label": 0
                },
                {
                    "sent": "There's this thing here, and something that does depend on X.",
                    "label": 0
                },
                {
                    "sent": "And let's see if we're already there.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so one difference is a factor of tool, but.",
                    "label": 0
                },
                {
                    "sent": "Factor of two.",
                    "label": 0
                },
                {
                    "sent": "We can just take out because it's a sine function current so we can multiply the argument of the sine function with any non negative number that we like without changing things.",
                    "label": 0
                },
                {
                    "sent": "So let's just.",
                    "label": 0
                },
                {
                    "sent": "Divide by two.",
                    "label": 0
                },
                {
                    "sent": "Which brings us this thing here and then we will recognize.",
                    "label": 0
                },
                {
                    "sent": "OK this these two.",
                    "label": 0
                },
                {
                    "sent": "Together they should give us our factor B down here.",
                    "label": 0
                },
                {
                    "sent": "And then the other two that are left.",
                    "label": 0
                },
                {
                    "sent": "They should exactly give us so.",
                    "label": 0
                },
                {
                    "sent": "I've called this in instead of them, but doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "They exactly give us this quantity here.",
                    "label": 0
                },
                {
                    "sent": "So that we've got these first 2 terms, we've got to be.",
                    "label": 0
                },
                {
                    "sent": "And we all set.",
                    "label": 0
                },
                {
                    "sent": "Any any questions?",
                    "label": 0
                },
                {
                    "sent": "These suggestions?",
                    "label": 0
                },
                {
                    "sent": "OK so I have to apologize for those who find this trivial, but I think it's very useful to try to do something to self because it's a way you could.",
                    "label": 1
                },
                {
                    "sent": "You say this is 1 simple example of how to Colonel eyes and algorithm.",
                    "label": 0
                },
                {
                    "sent": "We take the algorithm.",
                    "label": 0
                },
                {
                    "sent": "This trivial algorithm, computing the means in the feature spaces, and then you compute this difference between the square distances and you already have a classification algorithm which gives you something that makes sense from a statistical point of view.",
                    "label": 0
                },
                {
                    "sent": "OK, so I want to invite lecture I since I have only four hours, I won't talk about statistical learning theory and there's a lecture of Joshua Taylor will talk about this in great detail, so normally at this point in the lecture I have a couple of hours about statistical learning theory, so I'll I'll skip.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "Instead, say a little bit more about kernels and.",
                    "label": 0
                },
                {
                    "sent": "Let's see, so that's.",
                    "label": 0
                },
                {
                    "sent": "Yeah, let me let me give you some some examples of kernel functions now.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adjust the weight one introduces polynomial what people called polynomial kernel functions.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "So maybe forget everything that I've said so far and now will do things the other way around.",
                    "label": 0
                },
                {
                    "sent": "So let's assume we have some 2 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Sorry we have some 2 dimensional inputs.",
                    "label": 0
                },
                {
                    "sent": "And we have two class classification problem and suppose the problem is such that the true decision boundary is this ellipse here?",
                    "label": 0
                },
                {
                    "sent": "So we have some.",
                    "label": 0
                },
                {
                    "sent": "The red class inside the blue class outside we are only given the training points, but not this this decision boundary, which is the ellipse.",
                    "label": 0
                },
                {
                    "sent": "Now suppose we first pre process our data points mapping into a feature space into a 3 dimensional feature space.",
                    "label": 0
                },
                {
                    "sent": "With this prescription here.",
                    "label": 0
                },
                {
                    "sent": "So the prescription is we compute all possible products of two input coordinates.",
                    "label": 0
                },
                {
                    "sent": "So in this case there's just three possible such three such products, is X1 squared, X2 squared and X 1 * X Two.",
                    "label": 0
                },
                {
                    "sent": "And actually let me put a sqrt 2 in front of the mixed term.",
                    "label": 0
                },
                {
                    "sent": "So if we do this, we get a 3 dimensional problem.",
                    "label": 0
                },
                {
                    "sent": "But the three dimensional problem in a certain sense is easier now since it turns out that 3 dimensional problem can be solved with a linear decision decision rule.",
                    "label": 0
                },
                {
                    "sent": "So why is that so?",
                    "label": 0
                },
                {
                    "sent": "If we think about it, the first coordinate let me call it said one is X squared, the third coordinate X X2 squared.",
                    "label": 0
                },
                {
                    "sent": "And if you write the ellipse equation for this simple axis aligned ellipse.",
                    "label": 0
                },
                {
                    "sent": "In terms of these two coordinates, it becomes an affine equations like a linear equation pass a constant.",
                    "label": 0
                },
                {
                    "sent": "So therefore we can.",
                    "label": 0
                },
                {
                    "sent": "This ellipse becomes a hyperplane actually hyperplane that's independent of the second call is.",
                    "label": 0
                },
                {
                    "sent": "In this simple case, if the... in general position, then it will be a little bit more tricky, but it doesn't matter, it becomes a hyperplane in these representation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, of course, in the general case, we will be interested in inputs that are not 3 dimensional but N dimensional, and we might be interested in product number or two, but of order D where D can be any natural number.",
                    "label": 1
                },
                {
                    "sent": "Now, of course, the dimensionality of this feature space is no longer three by three, but it will increase.",
                    "label": 0
                },
                {
                    "sent": "It will actually grow like N to the power of the, where is the number of pixels in these images or the whatever the input dimensionality.",
                    "label": 0
                },
                {
                    "sent": "So already in this simple 16 by 16 images for prod.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Out of all the five, we have a 10 to the 10 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that what we've been doing so far taking DOT product in that representation sometimes can be done very efficiently.",
                    "label": 1
                },
                {
                    "sent": "And an example.",
                    "label": 0
                },
                {
                    "sent": "Are these polynomial kernels.",
                    "label": 0
                },
                {
                    "sent": "Because if you take this mapping that I defined on the last slide, so the image of X is this thing.",
                    "label": 0
                },
                {
                    "sent": "Here the image of another point X prime is this thing here.",
                    "label": 0
                },
                {
                    "sent": "Now we take the Canonical products.",
                    "label": 0
                },
                {
                    "sent": "So just these vector times this one transposed.",
                    "label": 0
                },
                {
                    "sent": "Then we see OK. That's X1 squared exponent prime squared.",
                    "label": 0
                },
                {
                    "sent": "We have the same for X2.",
                    "label": 0
                },
                {
                    "sent": "Here at the end and then we have a mixed term and if you write it down it turns out it's a complete binomial formula which can be written as the square of the dot product taken directly in the input space.",
                    "label": 0
                },
                {
                    "sent": "So this is a special case where the input space also is a dot product space, and it turns out in this special case we can compute this dot product in the feature space directly by computing the DOT product in the input space and then applying some linearity.",
                    "label": 0
                },
                {
                    "sent": "In this case just.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Square root.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing is that this also works in the N dimensional case with productive already.",
                    "label": 0
                },
                {
                    "sent": "It's actually quite simple, easy to see.",
                    "label": 0
                },
                {
                    "sent": "You just start with this hypothetical form of.",
                    "label": 0
                },
                {
                    "sent": "We just guess this form of the kernels because here it was just square of that product input space.",
                    "label": 0
                },
                {
                    "sent": "Let's just take the dot product input space and dimensional raised to the power of the.",
                    "label": 0
                },
                {
                    "sent": "So we write this down raised to the power of D. We get a default, some I'll call these columns J one through JD and back.",
                    "label": 0
                },
                {
                    "sent": "Here we have all these terms and we can sort them.",
                    "label": 0
                },
                {
                    "sent": "Everything in terms of X is here everything.",
                    "label": 0
                },
                {
                    "sent": "Everything in terms of X prime is over here.",
                    "label": 0
                },
                {
                    "sent": "And then what we can see is this is just a big sum into the power of the terms, but you can think of this as a dot product of a nonlinear function of X with the same nonlinear function of X prime here.",
                    "label": 0
                },
                {
                    "sent": "And we can even read off what the nonlinear function does.",
                    "label": 0
                },
                {
                    "sent": "It computes all possible products of all the.",
                    "label": 0
                },
                {
                    "sent": "And of course, some of these products occur multiple times, so here we have.",
                    "label": 0
                },
                {
                    "sent": "We count them multiple times and that's why we have this square root 2 over here.",
                    "label": 0
                },
                {
                    "sent": "So we could also just say we use that product X one X2X2X1 if we use both of them, we map into 4 dimensional space, but it's a bit redundant because 2 dimensions will be the same.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now, traditionally it's a perfect machines.",
                    "label": 0
                },
                {
                    "sent": "People have referred to Mercer's theorem to characterize.",
                    "label": 0
                },
                {
                    "sent": "The largest order the full class of kernels for which this trick works.",
                    "label": 0
                },
                {
                    "sent": "And by this I mean.",
                    "label": 0
                },
                {
                    "sent": "Kernels which have the properties that correspond to the product in some other space.",
                    "label": 0
                },
                {
                    "sent": "And for completeness I have to show you Mercer Theorem, although I prefer to talk about positive definite kernels because that's slightly larger class and also mathematically.",
                    "label": 0
                },
                {
                    "sent": "More elegant anyway, the most most theorem is this theorem from beginning of the 20th century from the integral equations, and it says that if K is account is continuous and it's the kernel of a positive definite integral operator.",
                    "label": 1
                },
                {
                    "sent": "By positive definite I mean that for all functions F this inequality holds true.",
                    "label": 0
                },
                {
                    "sent": "Then the kernel can be expanded in terms of its eigen functions.",
                    "label": 0
                },
                {
                    "sent": "Fine I.",
                    "label": 0
                },
                {
                    "sent": "And eigenvalues where the eigenvalues are non negative into this infinite sum.",
                    "label": 0
                },
                {
                    "sent": "And if you have such an expansion then from this expansion you can construct or you can you can view this expansion as a third product simply by defining your mapping.",
                    "label": 0
                },
                {
                    "sent": "So basically this is already looks like a dot product, you only have to split this slump tie into square root of lamptey over here on the square root of left eye over here.",
                    "label": 0
                },
                {
                    "sent": "Then again you have a function of X here and the function of X prime here.",
                    "label": 0
                },
                {
                    "sent": "Same function of X prime.",
                    "label": 0
                },
                {
                    "sent": "You can view this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Product so.",
                    "label": 0
                },
                {
                    "sent": "So we define the mapping file.",
                    "label": 0
                },
                {
                    "sent": "This way we substitute it into the product.",
                    "label": 0
                },
                {
                    "sent": "We recover exactly the form of the Mercer kernel expansion in the crucial point here was that the Lambda eyes are non negative, otherwise we couldn't take the square root of them.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's see I will actually first talk about this.",
                    "label": 0
                },
                {
                    "sent": "So now that the real class of kernel that we should be talking about positive definite kernels.",
                    "label": 1
                },
                {
                    "sent": "Just wondering so I have 1 1/2 hours in total, right?",
                    "label": 0
                },
                {
                    "sent": "Do people usually take a short break or someone just asked a question during the break, which probably points at a certain omission in the first part of my talk?",
                    "label": 0
                },
                {
                    "sent": "The question was what is a kernel?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to tell you now, what is a kernel?",
                    "label": 0
                },
                {
                    "sent": "And by kernel, if there are no additional qualifications, I will always mean positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "So I currently is a positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "And positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "Is defined as follows.",
                    "label": 0
                },
                {
                    "sent": "It's a symmetric functions of two inputs.",
                    "label": 0
                },
                {
                    "sent": "The inputs are taken from Sunset X, which I will only assume to be a nonempty set, nothing else.",
                    "label": 0
                },
                {
                    "sent": "And the kernel has the property that whatever set of points we take from that set X, you can call it training points, but it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Take some points from that set X, take a corresponding number of real numbers coefficients, then this quantity.",
                    "label": 0
                },
                {
                    "sent": "Here is non negative where this is the kernel matrix or gram matrix.",
                    "label": 1
                },
                {
                    "sent": "So that's the matrix that we get if we compute all pairwise similarities between input points.",
                    "label": 0
                },
                {
                    "sent": "So this quantity here should be non negative in exactly.",
                    "label": 0
                },
                {
                    "sent": "In that case we call it a positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "So this quantity here being non negative is the same as saying that this matrix here is positive definite.",
                    "label": 0
                },
                {
                    "sent": "Some people call this positive semidefinite, some colleague positive definite, so I'll call it positive definite.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In the slightly stricter case that other people called positive definite ionicons strictly positive definite.",
                    "label": 0
                },
                {
                    "sent": "And in that slightly stricter case, it should be such that if the points are pairwise distinct, then this quantity is only zero if all coefficients are zero.",
                    "label": 0
                },
                {
                    "sent": "So obviously, if the coefficients are zero, this quantity is 0, but if the other direction is also true.",
                    "label": 0
                },
                {
                    "sent": "Then we call it strictly positive definite and we need this condition that the points are pairwise distinct because it turns out that otherwise, if you are allowed to put in the same points here you get linear dependencies and then this, this being 0 doesn't mean much.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "Anyway, let's focus on the positive definite case now.",
                    "label": 0
                },
                {
                    "sent": "So remember, positive definite kernel is a kernel such that if we compute this matrix of pairwise similarity or the ground matrix kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "This matrix is positive definite.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that exactly for this class of kernels there exists a mapping into another space such that the kernel computes the dot product in that other space.",
                    "label": 0
                },
                {
                    "sent": "And we're going to prove that, so yes.",
                    "label": 0
                },
                {
                    "sent": "Is this the?",
                    "label": 0
                },
                {
                    "sent": "It's not exactly the same as the condition in the Mercer kernel.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that the conditions in the Mercer kernel is slightly stricter.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in the version that I talked about.",
                    "label": 0
                },
                {
                    "sent": "For instance, here there's a condition that case continuous.",
                    "label": 0
                },
                {
                    "sent": "So in the in the I mean there are other forms of the Mercer theorem that have slightly different conditions.",
                    "label": 0
                },
                {
                    "sent": "Here case continuous access, a compact space.",
                    "label": 1
                },
                {
                    "sent": "Now in this definition of positive definite kernel, I don't talk about continuity, I I don't even talk about any topological properties.",
                    "label": 0
                },
                {
                    "sent": "I just have some nonempty set X.",
                    "label": 0
                },
                {
                    "sent": "So you could say this is a slightly more general case, so really this is the case we should be talking about the other one I only mentioned for completeness because you see in the literature occasionally, but we don't really need it, so let's focus on this one.",
                    "label": 0
                },
                {
                    "sent": "This is a slightly larger class of kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's prove that this is the right class occurrence in the sense that this is exactly the class of kernels which induces a wrapped product in another space.",
                    "label": 0
                },
                {
                    "sent": "And this is also something where we will try to do a bit of interaction or a bit of.",
                    "label": 0
                },
                {
                    "sent": "Calculations on the site.",
                    "label": 0
                },
                {
                    "sent": "And to prove this we need.",
                    "label": 0
                },
                {
                    "sent": "A number of properties, or actually the first part of the proof is actually this thing here.",
                    "label": 0
                },
                {
                    "sent": "So if we have a mapping into dot product space.",
                    "label": 0
                },
                {
                    "sent": "Then from that mapping we get a positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "So that that would be easy to prove the other direction if we have positive definite kernel, how to get that mapping?",
                    "label": 0
                },
                {
                    "sent": "That would be a little bit more tricky and it will probably take us half an hour or so.",
                    "label": 0
                },
                {
                    "sent": "And as ingredients for the proof of that other direction, we need a couple of properties which I've written down here, and so this is also something that will try to do or everybody should try to do.",
                    "label": 0
                },
                {
                    "sent": "And again, my motivation is if you just see this definition.",
                    "label": 0
                },
                {
                    "sent": "Maybe if you don't know it before and you see it, then maybe you understand it and probably you understand every line of it, but it doesn't yet make much sense to you, so to understand the definition in mathematics you have to play with it and see how it works.",
                    "label": 0
                },
                {
                    "sent": "What does it imply in this kind of stuff?",
                    "label": 0
                },
                {
                    "sent": "So that's why we should do a little bit of exercise now and let me write down this definition here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So we just tried all XI for all AI.",
                    "label": 0
                },
                {
                    "sent": "This is true.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let's try to prove these four different properties and I'll.",
                    "label": 0
                },
                {
                    "sent": "I'll give you some time for this, but maybe I'll give you a couple of hints before and how to.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Seed.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Once you've seen the approach afterwards, you would probably say they're trivial, but if you have never seen this definition of positive definite kernel before, then it will be non trivial to think about it and write them down, even though each one of them is maybe just two lines.",
                    "label": 0
                },
                {
                    "sent": "So the first one is we have to prove that this is a positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "To prove that this is a positive definite kernel, we just have to verify that this condition here holds true for a kernel which is defined like that.",
                    "label": 0
                },
                {
                    "sent": "So that's a direct verification.",
                    "label": 0
                },
                {
                    "sent": "You will have to use things like linearity of the dot product and so on, and then the next one.",
                    "label": 0
                },
                {
                    "sent": "This may be slightly more tricky, even though it turns out there's a very simple, foolproof rate.",
                    "label": 0
                },
                {
                    "sent": "So in this next one we have to prove that effectively that if the kernel is positive definite.",
                    "label": 0
                },
                {
                    "sent": "So here in.",
                    "label": 0
                },
                {
                    "sent": "If the kernel is positive definite, then its diagonal elements are positive or non negative.",
                    "label": 0
                },
                {
                    "sent": "So Dien elements just K of X, X.",
                    "label": 1
                },
                {
                    "sent": "Or if you look over the kernel matrix is the diagonal elements of the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "So here in all these three cases, we assume it's positive definite.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit incomplete, so I should have written here, so here this is a complete statement, but these statements should all say if K is positive definite than this holds true.",
                    "label": 0
                },
                {
                    "sent": "This or this.",
                    "label": 0
                },
                {
                    "sent": "So here there is a simple proof, but maybe I won't tell you now because it's hard to give a hint without giving away the whole idea.",
                    "label": 0
                },
                {
                    "sent": "It's very simple, but you think about it a little bit.",
                    "label": 0
                },
                {
                    "sent": "Second one I have given you a hint here.",
                    "label": 0
                },
                {
                    "sent": "It's with this hint.",
                    "label": 0
                },
                {
                    "sent": "It's reasonably simple, but still it requires some thinking, and it's a general generalized Cauchy Schwarz inequality.",
                    "label": 0
                },
                {
                    "sent": "Which you can probably see from here and the last one is this property that if a kernel is 0 on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "For all possible dragon elements, then it's actually zero everywhere, so that's the last property, so I'll leave these up on the screen and again I will try to remember whether I can still prove in myself and give you some time to do the proofs Meanwhile.",
                    "label": 0
                },
                {
                    "sent": "And then we'll do it together.",
                    "label": 0
                },
                {
                    "sent": "Smart.",
                    "label": 0
                },
                {
                    "sent": "Oh I see the same laptop with me.",
                    "label": 0
                },
                {
                    "sent": "Happen to have a spare of this.",
                    "label": 0
                },
                {
                    "sent": "Is Ethernet adapters that you could lend me for later?",
                    "label": 0
                },
                {
                    "sent": "You have to.",
                    "label": 0
                },
                {
                    "sent": "I brought one.",
                    "label": 0
                },
                {
                    "sent": "I don't need it.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, I cannot check email Michael.",
                    "label": 0
                },
                {
                    "sent": "I don't need this.",
                    "label": 0
                },
                {
                    "sent": "Wireless that we have for the room in the college.",
                    "label": 0
                },
                {
                    "sent": "So is it OK if I give it back to you on Wednesday?",
                    "label": 0
                },
                {
                    "sent": "It's my only one, so yeah.",
                    "label": 0
                },
                {
                    "sent": "Both of them.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Fancy green pointer with forward and backward parents.",
                    "label": 0
                },
                {
                    "sent": "No, no, you know, I think it might not work with.",
                    "label": 0
                },
                {
                    "sent": "I don't know because I have these things I need also.",
                    "label": 0
                },
                {
                    "sent": "For some reason it doesn't work anymore.",
                    "label": 0
                },
                {
                    "sent": "How many million words?",
                    "label": 0
                },
                {
                    "sent": "Let me know, don't point it at anybody's eyes.",
                    "label": 0
                },
                {
                    "sent": "Scary.",
                    "label": 0
                },
                {
                    "sent": "Not less than one.",
                    "label": 0
                },
                {
                    "sent": "That's reasonable.",
                    "label": 0
                },
                {
                    "sent": "Your personal thing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm taking all your gadgets, yeah, what else do you have this if you stick it in then the forward and backward areas will work OK.",
                    "label": 0
                },
                {
                    "sent": "I'll try.",
                    "label": 0
                },
                {
                    "sent": "You can try.",
                    "label": 0
                },
                {
                    "sent": "That's so convenient.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe I'll wait.",
                    "label": 0
                },
                {
                    "sent": "Now nothing works.",
                    "label": 0
                },
                {
                    "sent": "Back now the only difference is I know I cannot do it with the keyboard anymore.",
                    "label": 0
                },
                {
                    "sent": "Alright, forget it.",
                    "label": 0
                },
                {
                    "sent": "Bad idea.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "But it looks elegant, so for you it always works.",
                    "label": 0
                },
                {
                    "sent": "Maybe you have to restart the door.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me ask.",
                    "label": 0
                },
                {
                    "sent": "How many of you have solved one or more problems?",
                    "label": 0
                },
                {
                    "sent": "OK, how many have solved two or more?",
                    "label": 0
                },
                {
                    "sent": "Three or more.",
                    "label": 0
                },
                {
                    "sent": "For Omar OK, how many have solved the first one?",
                    "label": 0
                },
                {
                    "sent": "Is this the first sorry first problem is this one?",
                    "label": 0
                },
                {
                    "sent": "How many have solved it?",
                    "label": 0
                },
                {
                    "sent": "Well enough that they could come when so show us the solution here in the front.",
                    "label": 0
                },
                {
                    "sent": "I think most of you probably have to.",
                    "label": 0
                },
                {
                    "sent": "OK, we have a volunteer excellent.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "You can choose any color, any black box.",
                    "label": 0
                },
                {
                    "sent": "And where is the light?",
                    "label": 0
                },
                {
                    "sent": "We want to show that.",
                    "label": 0
                },
                {
                    "sent": "Wait, I'll give you.",
                    "label": 0
                },
                {
                    "sent": "Wanna show that?",
                    "label": 0
                },
                {
                    "sent": "Positive.",
                    "label": 0
                },
                {
                    "sent": "So in the definition of positives you're showing this one right.",
                    "label": 0
                },
                {
                    "sent": "The positivity on the diagonal OK?",
                    "label": 1
                },
                {
                    "sent": "Positive semi definite.",
                    "label": 0
                },
                {
                    "sent": "We know that this is true.",
                    "label": 0
                },
                {
                    "sent": "So choose XI and XJ to be the same.",
                    "label": 0
                },
                {
                    "sent": "So X one X2.",
                    "label": 0
                },
                {
                    "sent": "So and choose a 1 = a two which is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "So then if you compute this sum.",
                    "label": 0
                },
                {
                    "sent": "It's basically come over.",
                    "label": 0
                },
                {
                    "sent": "One one times.",
                    "label": 0
                },
                {
                    "sent": "Porter so that's equal.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "Which is greater equal to 0 by this property.",
                    "label": 0
                },
                {
                    "sent": "So that means sex is very important.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So you get you get a point.",
                    "label": 0
                },
                {
                    "sent": "C1 right yeah?",
                    "label": 0
                },
                {
                    "sent": "And OK, so maybe this is interesting proof.",
                    "label": 0
                },
                {
                    "sent": "That's one that I haven't seen before.",
                    "label": 0
                },
                {
                    "sent": "But it is correct, so it's a proof, and it also generalizes to endpoints you you could as well have used endpoints if you wanted to, so the way I usually do it is I just take one point, but it's perfectly fine, so it is perfectly fine to use two points.",
                    "label": 0
                },
                {
                    "sent": "So if you just for completeness, if you take only one point.",
                    "label": 0
                },
                {
                    "sent": "Because here, OK, this is a bit short, but if we go back to the definition.",
                    "label": 0
                },
                {
                    "sent": "It just says any set of training points, so I can also choose a set of one points you chose one of two points for choose just one point, and I also choose the coefficients to be one.",
                    "label": 0
                },
                {
                    "sent": "Then it's another possible proof.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's great.",
                    "label": 0
                },
                {
                    "sent": "So we've done the second one.",
                    "label": 0
                },
                {
                    "sent": "Let's do another one.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Which one are you going to prove?",
                    "label": 0
                },
                {
                    "sent": "OK, the first one.",
                    "label": 0
                },
                {
                    "sent": "We need to prove.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Engine.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "32 G. What we do here is we just take IA I2 first.",
                    "label": 0
                },
                {
                    "sent": "2nd.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Next here.",
                    "label": 0
                },
                {
                    "sent": "AG.",
                    "label": 0
                },
                {
                    "sent": "?",
                    "label": 0
                },
                {
                    "sent": "The same.",
                    "label": 0
                },
                {
                    "sent": "We have the same.",
                    "label": 0
                },
                {
                    "sent": "Here is that.",
                    "label": 0
                },
                {
                    "sent": "Mark Yep, Yep, that's so because because here we say, is mapping into a dot product space.",
                    "label": 1
                },
                {
                    "sent": "So this thing is a dot product and we know that products are positive definite.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly what you're using.",
                    "label": 0
                },
                {
                    "sent": "So if you take a point with itself, the result would be non negative.",
                    "label": 0
                },
                {
                    "sent": "So thank you so I'll have to.",
                    "label": 0
                },
                {
                    "sent": "So what's your name?",
                    "label": 0
                },
                {
                    "sent": "OK. And maybe for those who cannot read the funds.",
                    "label": 0
                },
                {
                    "sent": "Looks a bit like when I'm having latech problems.",
                    "label": 0
                },
                {
                    "sent": "I sometimes get these very tiny symbols.",
                    "label": 0
                },
                {
                    "sent": "OK, so he's just moving the coefficients and the sums, so some over I and the coefficient AI he moves into the first argument in the sum over J coefficient AJ into the second argument, and then it looks like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "How about the third one?",
                    "label": 0
                },
                {
                    "sent": "Metric.",
                    "label": 0
                },
                {
                    "sent": "The 4th one OK, 4th one although the 4th one should be done after the third one because I guess.",
                    "label": 0
                },
                {
                    "sent": "But so maybe if someone else does the third one OK.",
                    "label": 0
                },
                {
                    "sent": "If you want you can was one of those over there, yeah.",
                    "label": 0
                },
                {
                    "sent": "Conditioning definition it's OK has to be understood.",
                    "label": 0
                },
                {
                    "sent": "Just applying some quadratic form.",
                    "label": 0
                },
                {
                    "sent": "With this yeah.",
                    "label": 0
                },
                {
                    "sent": "For all choices of ANX provided, you're on the good dimension.",
                    "label": 0
                },
                {
                    "sent": "So this metrics this graph metrics she doesn't need to be positive semidefinite, some definite so it has at least to have a positive determinant.",
                    "label": 0
                },
                {
                    "sent": "And in dimension 2 this is exactly this.",
                    "label": 0
                },
                {
                    "sent": "Inequality.",
                    "label": 0
                },
                {
                    "sent": "Metric.",
                    "label": 0
                },
                {
                    "sent": "This proposal was determined.",
                    "label": 0
                },
                {
                    "sent": "Why does it have opposed to terminate?",
                    "label": 1
                },
                {
                    "sent": "Has at least you have two positive eigenvalues?",
                    "label": 0
                },
                {
                    "sent": "So one way to argue is the determinant is the product of the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So so so that he's requiring a little bit of knowledge of some linear algebra background.",
                    "label": 0
                },
                {
                    "sent": "Saying that a positive definite matrix or positive semidefinite in your Note 8 terminology has non negative eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Hence the product of the eigenvalues is also non negative in the product is the determinant.",
                    "label": 0
                },
                {
                    "sent": "Invention two we could just choose 2.6 and explain this inequality is just expressing this this determination.",
                    "label": 0
                },
                {
                    "sent": "Does everybody everybody see that or you want you want you to work all the time?",
                    "label": 0
                },
                {
                    "sent": "Maybe write down the determinant just so that people see it?",
                    "label": 0
                },
                {
                    "sent": "Maybe it's too small, but.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "XX.",
                    "label": 0
                },
                {
                    "sent": "Explain plan.",
                    "label": 0
                },
                {
                    "sent": "Square dysfunction is symmetry.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So this is just a question.",
                    "label": 0
                },
                {
                    "sent": "Oh, OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Let me see your name.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you Amy.",
                    "label": 0
                },
                {
                    "sent": "Any questions today me?",
                    "label": 0
                },
                {
                    "sent": "OK, so now 4th one.",
                    "label": 0
                },
                {
                    "sent": "We have we have here.",
                    "label": 0
                },
                {
                    "sent": "Just zeros.",
                    "label": 0
                },
                {
                    "sent": "But if you want to point, you'll have to come up here.",
                    "label": 0
                },
                {
                    "sent": "Sweet.",
                    "label": 0
                },
                {
                    "sent": "And since we have.",
                    "label": 0
                },
                {
                    "sent": "X. Brian square 0.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, OK yeah.",
                    "label": 0
                },
                {
                    "sent": "So that the square of all elements is zero, therefore they have to be 0 is bounded from above by zero in the square is also for bounded from below by zero since it's a square so it has to be equal to 0.",
                    "label": 0
                },
                {
                    "sent": "The element has to be equal to 0.",
                    "label": 0
                },
                {
                    "sent": "And this argument works for any XX prime.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have to use all these properties and we're going to prove in.",
                    "label": 0
                },
                {
                    "sent": "Our remaining time, or maybe we'll finish tomorrow.",
                    "label": 0
                },
                {
                    "sent": "How to construct the feature space?",
                    "label": 0
                },
                {
                    "sent": "So basically we prove the opposite direction of this one proof given a positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "How to construct the feature space?",
                    "label": 0
                },
                {
                    "sent": "And maybe, maybe we'll manage to to prove it.",
                    "label": 0
                },
                {
                    "sent": "So is it like very strict half past the.",
                    "label": 0
                },
                {
                    "sent": "Coffee break.",
                    "label": 0
                },
                {
                    "sent": "Few minutes, so let's see if maybe I can do it with a few minutes more, otherwise will continue.",
                    "label": 0
                },
                {
                    "sent": "Will finish it tomorrow.",
                    "label": 0
                },
                {
                    "sent": "So we have one too.",
                    "label": 0
                },
                {
                    "sent": "Five slides",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "And the feature map will be defined as follows given an input point, we map it to a point to a function.",
                    "label": 0
                },
                {
                    "sent": "In this space, R to the power of X.",
                    "label": 0
                },
                {
                    "sent": "By this I simply mean functions mapping X to R. And the mapping will be such that the point X is assigned to the function that we get if we substitute X into.",
                    "label": 0
                },
                {
                    "sent": "The second argument of the kernel could also be.",
                    "label": 0
                },
                {
                    "sent": "The first one doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "And leave the first argument open.",
                    "label": 0
                },
                {
                    "sent": "So this is a function of the first argument, which, given a point in X that we substitute in here gives us a real number.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if we had a Gaussian kernel function, then our point X would be mapped into a Gaussian centered on X.",
                    "label": 0
                },
                {
                    "sent": "Point X prime would be mapped into a Gaussian sent on the next prime.",
                    "label": 0
                },
                {
                    "sent": "So what we'll have to do is we have to turn this into a linear space.",
                    "label": 1
                },
                {
                    "sent": "We are back to space, in other words.",
                    "label": 1
                },
                {
                    "sent": "We have to then construct the DOT product which satisfies the condition that we want to be satisfied.",
                    "label": 0
                },
                {
                    "sent": "So the product should compute the kernel function.",
                    "label": 1
                },
                {
                    "sent": "In our special case, we've already defined the feature map.",
                    "label": 0
                },
                {
                    "sent": "If I substitute this feature map file into here, then this equation boils down to this one, which is the reason why people sometimes call these kernels reproducing kernels.",
                    "label": 0
                },
                {
                    "sent": "So you put the kernel with itself.",
                    "label": 0
                },
                {
                    "sent": "It gives you the kernel.",
                    "label": 0
                },
                {
                    "sent": "And then the last step will be a trivial one that we won't talk much about.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's basically what's left to do, so the first one in a way is trivial.",
                    "label": 0
                },
                {
                    "sent": "We just formally declare that all such.",
                    "label": 0
                },
                {
                    "sent": "Linear combinations, so we know that this kind of points they are in our.",
                    "label": 0
                },
                {
                    "sent": "They are already in our space, so we I mean if we take this individual point it will be mapped to such a function.",
                    "label": 0
                },
                {
                    "sent": "So all these Gaussian bumps are now in our feature space because they are images of input points.",
                    "label": 0
                },
                {
                    "sent": "But we'll just declare that linear combinations of such bumps will also be in our space.",
                    "label": 0
                },
                {
                    "sent": "So we take the linear completion of the space.",
                    "label": 0
                },
                {
                    "sent": "So for instance, this function G in this function F they are elements of our space where M&M prime, arbitrary numbers, natural numbers or integers, natural positive, and these are real numbers of coefficients.",
                    "label": 0
                },
                {
                    "sent": "And these points these expansion points.",
                    "label": 0
                },
                {
                    "sent": "They are from our input domain.",
                    "label": 0
                },
                {
                    "sent": "So we'll just say.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These are in our space, so now we have to construct a dot product and that product takes as inputs, functions, FNG and so far the general form of a function of our space.",
                    "label": 1
                },
                {
                    "sent": "Is this or this, so we'll take these functions as inputs and we'll just say that product takes this value here.",
                    "label": 0
                },
                {
                    "sent": "So in particular, if the functions F&G, we're just.",
                    "label": 0
                },
                {
                    "sent": "So if you forget about this, someone the Alpha I so it's just.",
                    "label": 0
                },
                {
                    "sent": "One kernel centered on one point, and here are the same on another point.",
                    "label": 0
                },
                {
                    "sent": "In that special case, all this stuff here in the front would be gone in the dot product between these two individual kernels.",
                    "label": 1
                },
                {
                    "sent": "So the functions are not individual kernels is a kernel, so this is consistent with what we want to have.",
                    "label": 0
                },
                {
                    "sent": "If you remember from before we we wanted to have that this is true, and for our special feature map this boils down to this thing here.",
                    "label": 0
                },
                {
                    "sent": "So basically the product that we define is just a linear extension of this requirement.",
                    "label": 0
                },
                {
                    "sent": "It's the only way we can do it.",
                    "label": 0
                },
                {
                    "sent": "OK, so then from the mathematical point of view we have to define that this is we have to show that this is well defined.",
                    "label": 0
                },
                {
                    "sent": "By this I mean.",
                    "label": 0
                },
                {
                    "sent": "Principle, it's possible that the same function can be written in terms of different points in different coefficients.",
                    "label": 0
                },
                {
                    "sent": "It turns out, for some kernels, usually this is possible, unless the kernel is strictly positive definite, so this need not be unique.",
                    "label": 0
                },
                {
                    "sent": "This expansion.",
                    "label": 0
                },
                {
                    "sent": "Same for this expansion of trade need not be unique, so of course the definition should be the dot product should just be a property of the functions and not of the expansion coefficients.",
                    "label": 0
                },
                {
                    "sent": "If I can write the same function with different sets of expansion.",
                    "label": 0
                },
                {
                    "sent": "Some points I should not get different values that the dot product otherwise dot product is not a property of the functions.",
                    "label": 0
                },
                {
                    "sent": "So we have to do is well defined in one way to show it is simply.",
                    "label": 0
                },
                {
                    "sent": "If we look at this thing here.",
                    "label": 0
                },
                {
                    "sent": "So let's remember for a second.",
                    "label": 0
                },
                {
                    "sent": "G is defined, so try to put this into your iconic memory for 10 seconds.",
                    "label": 0
                },
                {
                    "sent": "So G is the sum over the bit us in the kernel centered on X prime.",
                    "label": 0
                },
                {
                    "sent": "First argument is open.",
                    "label": 0
                },
                {
                    "sent": "If we now substitute XI into G. What do we get?",
                    "label": 0
                },
                {
                    "sent": "We get bet some over the bitters and K of XI, XJ prime.",
                    "label": 0
                },
                {
                    "sent": "It's exactly what we have here.",
                    "label": 0
                },
                {
                    "sent": "XI XJ prime, the bitterness and the sum over the bed has.",
                    "label": 0
                },
                {
                    "sent": "So what we get is just this is equal to sum over Alpha I of the GS evaluated at XI.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you didn't follow this, you can.",
                    "label": 0
                },
                {
                    "sent": "You will get the slides.",
                    "label": 0
                },
                {
                    "sent": "You can look at it afterwards.",
                    "label": 0
                },
                {
                    "sent": "With the same argument we can.",
                    "label": 0
                },
                {
                    "sent": "We can change this expression into this thing here.",
                    "label": 0
                },
                {
                    "sent": "Some over the bitters, and F evaluated the XJ prime.",
                    "label": 0
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "The first equation here shows us that this definition is independent of the betos and independent of the X primes.",
                    "label": 0
                },
                {
                    "sent": "OK, it doesn't appear anywhere here.",
                    "label": 0
                },
                {
                    "sent": "Second expression shows us that.",
                    "label": 0
                },
                {
                    "sent": "The value of this quantity is independent of the alphas and independent of the size.",
                    "label": 0
                },
                {
                    "sent": "So overall it doesn't depend on any of these coefficients explicitly.",
                    "label": 0
                },
                {
                    "sent": "It only depends on the functions F&G, so it's well defined.",
                    "label": 0
                },
                {
                    "sent": "So that's the first step.",
                    "label": 0
                },
                {
                    "sent": "Second step is.",
                    "label": 0
                },
                {
                    "sent": "It's symmetric, that's clear because the kernel function is symmetric and taking products between Alpha and beta is a symmetric operation, so we can.",
                    "label": 0
                },
                {
                    "sent": "We can if we change F&G, it will just effectively change the order of the sums.",
                    "label": 0
                },
                {
                    "sent": "The other founder bitters in here the order of the arguments and we can undo these changes.",
                    "label": 0
                },
                {
                    "sent": "Back here is no problem.",
                    "label": 0
                },
                {
                    "sent": "It's also by linear, so if we had functions F + F + F prime in here if we went through the algebra we would see this would be.",
                    "label": 0
                },
                {
                    "sent": "So, for instance we would get.",
                    "label": 0
                },
                {
                    "sent": "I don't know can you can you read that so?",
                    "label": 0
                },
                {
                    "sent": "So we have by linearity we also would have something like Alpha.",
                    "label": 0
                },
                {
                    "sent": "G. So we can take coefficients out.",
                    "label": 0
                },
                {
                    "sent": "So all this follows trivially from the fact that we're just taking.",
                    "label": 0
                },
                {
                    "sent": "Sums and products here, so it's it's what people call a symmetric bilinear form.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So far.",
                    "label": 0
                },
                {
                    "sent": "I've already mentioned one special case where F&G where individual kernels if only one of the functions is an individual kernel.",
                    "label": 0
                },
                {
                    "sent": "And then as a special case, we get this quantity here.",
                    "label": 0
                },
                {
                    "sent": "So that that here was the special case where both are individual kernels, so that's the reproducing kernel property.",
                    "label": 1
                },
                {
                    "sent": "This first property is also interesting because it says that if we take this, what will end up to be a dot product.",
                    "label": 0
                },
                {
                    "sent": "So far I can only call it the symmetric bilinear form between an arbitrary function and a kernel centered on a point X, and the result is the function evaluated at X.",
                    "label": 0
                },
                {
                    "sent": "So people rephrase this as saying by saying the kernel.",
                    "label": 0
                },
                {
                    "sent": "Is a representative point evaluation, so in that space we can evaluate the function at a point by taking a dot product with the kernel.",
                    "label": 0
                },
                {
                    "sent": "So that's that's a nice property, turns out.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so that's the last slide that we need for today.",
                    "label": 1
                },
                {
                    "sent": "So we have a few more steps to show.",
                    "label": 0
                },
                {
                    "sent": "So far we know that this thing here is a symmetric bilinear form.",
                    "label": 0
                },
                {
                    "sent": "The next thing is will show it's actually a positive definite kernel in its own right.",
                    "label": 1
                },
                {
                    "sent": "So we started with the positive definite kernel K. Then we defined this thing here.",
                    "label": 0
                },
                {
                    "sent": "This thing here so K is a positive definite term.",
                    "label": 0
                },
                {
                    "sent": "Now we define this thing here.",
                    "label": 0
                },
                {
                    "sent": "It turns out this is also positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "But on the space of functions.",
                    "label": 0
                },
                {
                    "sent": "So this was a positive definite kernel on this SpaceX on this set X Now we have a positive definite.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the space of functions and once we know this, it will be nice because then we know we can use these properties here, which are true for positive definite kernels.",
                    "label": 0
                },
                {
                    "sent": "So let's show it's a positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "So to see this so on this on our set of functions which are linear combinations of individual kernels.",
                    "label": 0
                },
                {
                    "sent": "So we have to take some functions FIFJ and put arbitrary real coefficients in front of them.",
                    "label": 0
                },
                {
                    "sent": "So we have to check this condition.",
                    "label": 0
                },
                {
                    "sent": "Now for the functions is satisfied.",
                    "label": 0
                },
                {
                    "sent": "This is not negative this quantity, so let's compute this quantity.",
                    "label": 0
                },
                {
                    "sent": "First of all, we know that this thing here is bilinear.",
                    "label": 0
                },
                {
                    "sent": "So we take we can take the coefficients inside and we can take the sums inside.",
                    "label": 0
                },
                {
                    "sent": "So if we do that, we get this quantity here now.",
                    "label": 0
                },
                {
                    "sent": "This again like in our calculation before here.",
                    "label": 0
                },
                {
                    "sent": "Will see this is 1 vector taking the symmetric bilinear form with itself, so let's just call this F this vector.",
                    "label": 0
                },
                {
                    "sent": "We don't know the exact coefficients, but we know it's still an element of our space because trivially if we take linear combinations of linear combinations, we get just another linear combination.",
                    "label": 0
                },
                {
                    "sent": "So this is also can be written in terms of kernels and we'll just right in terms of kernels with these coefficients.",
                    "label": 0
                },
                {
                    "sent": "Alpha I and points XI.",
                    "label": 0
                },
                {
                    "sent": "We know we don't know how many kernels we will need, but we know this is now space and we can write it this way.",
                    "label": 0
                },
                {
                    "sent": "So this function called this F can be written like this.",
                    "label": 0
                },
                {
                    "sent": "Now we use the bilinearity again.",
                    "label": 0
                },
                {
                    "sent": "Of our bilinear form, we take these coefficients in the sums outside.",
                    "label": 0
                },
                {
                    "sent": "Again, we get this quantity here.",
                    "label": 0
                },
                {
                    "sent": "Here we have what's left inside is the kernel with itself.",
                    "label": 0
                },
                {
                    "sent": "We know that the kernel with itself reproduces gives us a kernel again.",
                    "label": 0
                },
                {
                    "sent": "And what we're left with here is this term.",
                    "label": 0
                },
                {
                    "sent": "And now we know that K is a positive definite kernel on our input points.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we know that this quantity is not negative.",
                    "label": 0
                },
                {
                    "sent": "So now we know also that this quantity is non negative, which was all that we had to show.",
                    "label": 0
                },
                {
                    "sent": "That is positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "So if this is a positive definite on our input set then this is a positive definite kernel on our set of functions.",
                    "label": 0
                },
                {
                    "sent": "So that's nice.",
                    "label": 0
                },
                {
                    "sent": "Now we know it's a symmetric bilinear form, and a positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "To show that it's a dot product.",
                    "label": 0
                },
                {
                    "sent": "So what is the dot product dot product is a symmetric bilinear form which is strictly positive definite.",
                    "label": 1
                },
                {
                    "sent": "By a strictly positive definite for that product, I mean that whenever I take the dot product of something with itself and the value is 0, then this thing has to be 0.",
                    "label": 0
                },
                {
                    "sent": "That's all that's left to proof.",
                    "label": 0
                },
                {
                    "sent": "For this to be a dot product.",
                    "label": 0
                },
                {
                    "sent": "So let's see.",
                    "label": 0
                },
                {
                    "sent": "So the way to show this is we take F of X, let's square it and we first express it in terms of as a PT evaluation.",
                    "label": 0
                },
                {
                    "sent": "So the dot product of the function with the kernel at X.",
                    "label": 0
                },
                {
                    "sent": "So remember from the previous slide.",
                    "label": 0
                },
                {
                    "sent": "I was telling you Colonel represents PT evaluation, so we can write it like this.",
                    "label": 0
                },
                {
                    "sent": "So now we're going to use the code Schwarz inequality, which we have proven before, so I don't know where it is.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I think it's this thing here over here.",
                    "label": 0
                },
                {
                    "sent": "So we can we can.",
                    "label": 0
                },
                {
                    "sent": "We know that this angular bracket thing is a positive definite kernel, so their position Schwartz inequality holds, so the kernel between them external between the two different elements it's square can be upper bounded by Colonel of the first element with itself and the kernel of the second element with itself.",
                    "label": 0
                },
                {
                    "sent": "So we do this upper bounding thing now again.",
                    "label": 0
                },
                {
                    "sent": "Here we have a kernel with itself, but on X&X, so we get K of XX.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So therefore.",
                    "label": 0
                },
                {
                    "sent": "Here so we have an upper bound on this square.",
                    "label": 0
                },
                {
                    "sent": "And if we now try to set the dot product or the angular bracket thing of F with itself, we still cannot call it the dot product to 0 here.",
                    "label": 0
                },
                {
                    "sent": "Then we get a zero on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "So this quantity is upper bounded by zero.",
                    "label": 0
                },
                {
                    "sent": "It's also lower bounded by zero.",
                    "label": 0
                },
                {
                    "sent": "Therefore, F of X has to be 0, and that's independent of which point XI put.",
                    "label": 0
                },
                {
                    "sent": "So, whenever F, F is zero, we know that F of X is 0 for all X for the function is 0.",
                    "label": 0
                },
                {
                    "sent": "So this symmetric bilinear form is positive definite, so it's a dot product.",
                    "label": 0
                },
                {
                    "sent": "And now we have what people call a pre Hilbert space or a little product space.",
                    "label": 0
                },
                {
                    "sent": "So it's we don't know yet whether it's complete, but we can always complete the space so one can add limit points of course sequences.",
                    "label": 0
                },
                {
                    "sent": "But don't worry about this.",
                    "label": 1
                },
                {
                    "sent": "To get what's called the Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "But basically this is the feature space, so I don't want to eat more into your coffee time.",
                    "label": 0
                },
                {
                    "sent": "Maybe this was a little bit fast and I might repeat 2 minutes tomorrow and then tomorrow we'll get to kernel algorithms and support vector machines.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}