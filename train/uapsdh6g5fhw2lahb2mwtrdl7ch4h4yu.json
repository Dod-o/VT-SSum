{
    "id": "uapsdh6g5fhw2lahb2mwtrdl7ch4h4yu",
    "title": "Semi-Supervised Learning",
    "info": {
        "author": [
            "Jerry (Xiaojin) Zhu, Department of Computer Sciences, University of Wisconsin-Madison"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/mlss09us_zhu_ssl/",
    "segmentation": [
        [
            "So with that, let's begin.",
            "We have Jerry shoe from Wisconsin Madison and he will talk about semi supervised learning.",
            "Thank you.",
            "OK, so this tutorial is about semi supervised learning.",
            "A lot of what I'm going to talk today comes from this new book, which is just out."
        ],
        [
            "Saturday hi Andrew and Goldberg introduction to semi supervised learning and it's online.",
            "You can download it.",
            "OK, so here's the plan for today.",
            "We were."
        ],
        [
            "Have two parts.",
            "This roughly corresponds to old stuff and new stuff in the old stuff.",
            "We will first define what is semi supervised learning and then we will review some of the more representative methods including mixture models, Co, training, manifold regularization and support vector machines as applied to the semi supervised learning setting.",
            "Then in the new part we will look.",
            "Briefly, at the theory of semi supervised learning and some new directions, including the online setting and.",
            "More complicated setting.",
            "In the end we will have a discussion on not machine learning but human learning, semi supervised learning in humans.",
            "OK, please feel free to stop me and ask questions at anytime.",
            "So part one, what is semisupervised?"
        ],
        [
            "Running OK. That is broadly defined as learning from both labeled and unlabeled data.",
            "By that definition, there are many paradigms which would fit in semi supervised learning.",
            "The more familiar one is called semi supervised classification, where you are doing supervised learning, in particular classification where you have training data consists of all data points.",
            "I'm going to call it XIYII from one 2L, so I'm going to use L for the number of labeled data points and then you have U unlabeled data points.",
            "So those are just acts the feature vectors without the corresponding labels.",
            "Often people make the assumption that you is much bigger than.",
            "LK the goal here is to trend somehow a classifier F, which would be better than the classifier you trend using only labeled data.",
            "So that's one example.",
            "Then you also have the other extreme where you start from clustering."
        ],
        [
            "And then you do constraint clustering where you put in some domain knowledge.",
            "OK, so here you start with unlabeled data.",
            "But then you may have some supervised information, not necessarily in terms of Labor with, but perhaps in the form of, for example, must link cannot links.",
            "You know that a pair of examples must be in the same cluster or not.",
            "And the goal here is to create better clusterings.",
            "In this talk, we will focus on."
        ],
        [
            "Semi supervised classification.",
            "OK, so why would we want to do this?",
            "Traditionally this is motivated in machine learning because we want better performance for free.",
            "And the story usually goes that label data can be hard to get.",
            "In some cases when you actually need human experts annotators to mark the label for you.",
            "So that can be slow and expensive.",
            "In other cases, you might need experiments in the wet lab and special devices, so that's also expensive.",
            "So often it's the case that labeled data is hard to get.",
            "But on the other hand, unlabeled data is easy to get and.",
            "Well, since you have all those only with data, why don't we use it?",
            "So that's the motivation.",
            "There is also another motivation.",
            "More from the cockpit."
        ],
        [
            "Give science perspective.",
            "So this has to do with the learning in general, and in particular, how do we as humans learn so?",
            "Imagine the other case of concept learning in children, so they see an object.",
            "Now you think of this object as X and they want to get the concept wide.",
            "OK, how do they learn it right?",
            "They receive supervised information so that it would point to this animal and say Doc.",
            "So that's an explicit labeled data instance, but then they also see all those animals just walk around by themselves, right?",
            "And you would imagine that they have to be using that information somehow and semi supervised learning models will develop will be used as computational models for human learning."
        ],
        [
            "OK, so let's go back to the machine learning side and say OK, So what are the cases when labeling is hard?",
            "This is 1 case in speech recognition we saw the talk yesterday.",
            "If you want to label the speech stream the acoustic signal at verifying Gray, i.e.",
            "At the phonetic actually sub phonetic level, you need human annotators to listen to the recordings and do that.",
            "So film would go like this.",
            "Those are special annotations for for phonetic units, you don't need to worry about what they mean, but.",
            "Just.",
            "Just from the look of it, it's hard to do and it actually takes a long time.",
            "400 hours to transcribe one hour of speech, so that's a lot of time.",
            "Similarly, when you do parsing."
        ],
        [
            "This is Chinese sentence.",
            "But when you do parsing, you want to train a password.",
            "You need training data, for example the parts, trees.",
            "Those trees.",
            "Initially I created by a linguist by hand.",
            "Again, that's very slow to create."
        ],
        [
            "OK, so let's.",
            "Let's see OK, so this is the notation we will use.",
            "We will be using both X for instance.",
            "Think of them as feature vectors and labor Y.",
            "Normally their binary labels.",
            "So we want to learn are F which Maps from X to Y label data.",
            "As I said you have, I will be using this notation one 2L, so that's the training label data points and then we have some unlabeled data points.",
            "Now let's distinguish two kinds of unlabeled data points.",
            "One is the set you have available during training, so those are the only data you will be using in training and then there are true test data in the future which you do not see during the training stage.",
            "OK.",
            "Many of you."
        ],
        [
            "You have probably heard for Semi supervised learning and transductive learning.",
            "Let's define them precisely when I say semi supervised learning.",
            "I main inductive semi supervised learning.",
            "So here the goal is if you are given labeled data set and unlabeled data set, you want to create a classifier F such that F is a good predictor in future data on the unlabeled data points you haven't seen.",
            "So you want this F to be able to predict unseen test data points.",
            "In contrast."
        ],
        [
            "When we talk about transductive learning, it looks kind of similar, but your F is defined only on the labeled and unlabeled training set that's given to you.",
            "OK, you don't even need to worry about data points outside that set.",
            "So there is no induction required, and that's the only difference I'm going to use here.",
            "Now.",
            "Let's go back here if you think about the task of semi supervised learning.",
            "It feels a bit weird because what you want to do is to learn.",
            "F&F is a function that Maps X2Y right the whole point of having a training data is to tell you how access mapped to Y.",
            "So you have some examples of that mapping.",
            "Now if all you have in addition is this unlabeled data set.",
            "You certainly don't see that mapping from X to Y from those unlabeled datasets.",
            "And therefore it feels weird to be able to learn anything of the mapping just from the unlabeled data set."
        ],
        [
            "The key lysing the assumption we have to make for semi supervised learning.",
            "So here's one example.",
            "Let's consider the case where X lives in 1D, so those are.",
            "That's the 1D space, let's say, and I have some.",
            "Symbols there.",
            "Red Cross and blue circle.",
            "Those are two labeled data points I have.",
            "The green dots are unlabeled data points I have.",
            "So they're distributing like that.",
            "This is small sample.",
            "If you look at your label data points and you have to put a decision boundary.",
            "Let's say it's a linear decision boundary, then the intuitive decision boundary has to be in the middle of the label data points.",
            "So that's the dashed line.",
            "But if you also have all those unlabeled data points, you look at that and you say, OK, I kind of see patterns.",
            "I kind of see two groups of points.",
            "Then you have to make the assumption you make the assumption that maybe each class is a tight cluster.",
            "Maybe they can each class come from a Gaussian distribution.",
            "If that's the case, then those unlabeled data points will tell you something about those Gaussian distributions.",
            "They also tell you that your label data points are not right at the center of those Gaussian distributions, so if you believe what you believe, then you want to put the decision boundary in the middle of the gap, which is the solid line and therefore.",
            "Knowing the unlabeled data.",
            "In addition to this assumption will give you different decision boundary and that's pretty much what semi supervised learning does.",
            "Kate."
        ],
        [
            "Of course, this is only one way to do semi supervised learning and we will introduce many other ways to do it.",
            "Now, as a concrete example, let's do a very Zen."
        ],
        [
            "No algorithm, it's called self training.",
            "So you have this input.",
            "Our label data points you unlabeled data points.",
            "Then initially let's create two sets.",
            "Let's have the L for label set an U4 unable set.",
            "Then let's do this, let's try."
        ],
        [
            "Train classifier F, whatever you like from our alone, OK, and use supervised learning.",
            "Then you will apply this classifier to the unlabeled data set.",
            "You are just classify them.",
            "Now when you classify them in addition to a label, we also hope that your ass can produce some form of confidence.",
            "It could be in the form of posterior probability.",
            "Then what you do is you would remove.",
            "A subset from this unlabeled data set and this subset is those unlabeled data points where your F made confident predictions.",
            "You would remove those and.",
            "Add their predicted label as if it's the correct label to the label set.",
            "So now you make your unable to set smaller label set bigger you repeat this, you retrain your F. Now this is called."
        ],
        [
            "Self training.",
            "It's a wrapper method.",
            "By that we mean that this procedure applies to any athlete you have, as long as it can give you a confidence measure.",
            "OK, so you can do whatever you can pick whatever whatever you want.",
            "Therefore it's very good for the case when you have a very complicated system, for example in natural language processing.",
            "Any classifiers are fairly complicated.",
            "You don't want to reach into the black box and change that, so that's good.",
            "It's very easy to apply.",
            "There is a downside though.",
            "There's pretty much no guarantee.",
            "OK, it's possible for your F to make some wrong predictions, but have high confidence, and it's possible for those run predictions to get into the labeled data set and therefore reinforce the mistake.",
            "Also notice that OK, so I said that.",
            "So let's look at a con."
        ],
        [
            "Create example where we make a particular.",
            "I've pick up particular function.",
            "So this is exactly the same as before, except that into these two lines I picked a particular classifier.",
            "It's a very simple classifier, so here's the idea.",
            "I'm going to use a one nearest neighbor classifier, but here's my confidence measure.",
            "Among all the unlabeled data point, I'm going to pick the unlabeled data point which is closest to.",
            "A labeled data point, so I'm going to find one label point which is closer to closest to some labeled point already, and I'm going to label it with the label of that label point.",
            "OK, so it's a very simple when you're in a hurry, just take the closest neighbor and repeat.",
            "If there are ties breaking randomly.",
            "OK, so let's see how it."
        ],
        [
            "Does.",
            "Here we have this data set which are in 2D.",
            "Each green dot is an unlabeled data point.",
            "You initially have two labeled data points.",
            "And then when you do this procedure.",
            "It actually gradually label is only valid point around those labor points, so that's what you have in the iteration 25 and then it propagates more.",
            "That's iteration 74.",
            "So you see this arm is being down and we're in the progress of doing that.",
            "So in the end you get that which looks very good.",
            "OK, so this works.",
            "But let's see a case where it doesn't work and all I'm going to do is to add one more unlabeled data point.",
            "To the data set.",
            "Do you see that?",
            "Yeah, the outlier there just that guy and well, so at some point."
        ],
        [
            "You will start to propagate it in the wrong fashion and then that guy will invade this region and eventually get something else.",
            "Alright."
        ],
        [
            "So just to keep this in mind, semi supervised learning may not always work and this is a repeating theme in today's talk.",
            "OK, so let's move on to our first.",
            "Actually, second model mixture model.",
            "You have already seen that.",
            "That's the two Gaussian example, but let's make it more formal.",
            "Yeah."
        ],
        [
            "Again, here's an example, but this time in 2D.",
            "So let's say you have some labeled data points, and I actually tell you that each class is a Gaussian distribution.",
            "K. So if that's the case, what do you do right?",
            "You're given this data point.",
            "This training data set.",
            "Then it's easy, right?",
            "You would estimate the mean and variance of your Gaussian distributions."
        ],
        [
            "So your model is a mixture of Gaussian which has the following parameters.",
            "You have W1W2 and those are the weights of the two Gaussians respectively.",
            "They should sum to one and then you have the mean and covariance covariance matrices.",
            "The here's the full joint model, so the joint model is like first you pick which causing you generate your data points and then you use that Gaussian to generate your data point.",
            "So if you know the parameters of the system, then classification is done by Bayes rule, so that's what you get.",
            "And if you do it here."
        ],
        [
            "Those contours are the sample mean and sample covariance of your label data set.",
            "If you apply Bayes rule, you can plot the decision boundary.",
            "In this case the decision boundary is the very hard to see Green line, so let me trace it for you.",
            "It's kind of like this.",
            "OK. That's all fine, except that when you see."
        ],
        [
            "More unlabeled data points.",
            "OK, so the green dots are unlabeled data points from the same distribution.",
            "Now you realize that.",
            "The maximum likelihood estimate for the Gaussian distributions are not so good.",
            "Instead, what you should have is something."
        ],
        [
            "Like this?",
            "OK, the two Gaussian distributions have more.",
            "Nicer covariance more diagonal ish so.",
            "And the decision boundary estimated from this data is the green line here, almost vertical.",
            "And we want to capture this notion.",
            "So what's the difference here?",
            "Well, the difference is how are you going to estimate the parameters of you?"
        ],
        [
            "Or Gaussian mixture model.",
            "In particular, we will be doing maximum likelihood, but in one case we will maximize the label data likelihood.",
            "But when you have unlabeled data, which we should do is to maximize the different form of likelihood there, taking into consideration the unlabeled data.",
            "OK, so in."
        ],
        [
            "This mixture model framework we make the assumption that you actually know the form of the model.",
            "So that means you know it's a mixture of two Gaussians, let's say.",
            "But then what you want to look at is this quantity.",
            "The probability of both the label data and unlabeled data.",
            "Now, since we do not have labels on the unlabeled data, you need to marginalized that out.",
            "So that means you need some over the labels on the unlabeled data.",
            "So this is joint probability on the label data and marginal probability on the unlabeled data.",
            "K."
        ],
        [
            "Then that's the quantity you want to optimize.",
            "You can do maximum likelihood, or you can do map estimate, or you can be Bayesian whatever you like.",
            "Um?"
        ],
        [
            "But you want to optimize that quantity.",
            "So.",
            "There are many mixture models that has been widely used in semi supervised learning.",
            "You can do mixture of Gaussian.",
            "You can do mixture of multinomial.",
            "That's the naive Bayes classifier used in text classification.",
            "You can certainly trend hidden Markov models when you have unlabeled data, and that's well known.",
            "Typically, the way you estimate the parameter that maximizes the likelihood above is by using the EM algorithm.",
            "And in the in the case of hidden Markov model, this is called an Welch.",
            "See so."
        ],
        [
            "So let's look at the case of Gaussian mixture model.",
            "So with the with only label that, let's say you don't have unlabeled data, then you have is the labeled likelihood, so that you factor it as again the product of the prior or class and the conditional class conditional probability.",
            "If you have any data points because there are independent, you can figure that out, and from that it's very easy to derive the maximum likelihood estimate.",
            "It's just the sample mean and sample covariance.",
            "Where is unlabeled data we want?"
        ],
        [
            "This term here, so the things now factors as follows.",
            "You have exactly the same term label data points but unlabeled data point.",
            "Those are the only way points you have a sum over the inside the log and this is the marginalization over labels.",
            "OK. You're summing over the unknown labels.",
            "This makes it hard to optimize directly.",
            "Things are much harder.",
            "It's no longer convex.",
            "And you will need to use some iterative method, for example the EM algorithm to do that."
        ],
        [
            "Here is how an E American would work as a procedure so.",
            "You start from some Theta.",
            "Typically this can be the maximum likelihood estimate from the label data alone, so you train it, try your model from the from the label data first.",
            "Then you have the class prior you have them in and you have the covariance for each class.",
            "Then you do the following.",
            "Once you have this initial guess of the parameter, you have a classifier at hand, right?",
            "Then you can apply it."
        ],
        [
            "To all the unlabeled data points, for each unlabeled data point X, I'm going to compute the label distribution.",
            "So I'm going to predict its label, and that's just an application of Bayes rule.",
            "OK then, well, let's consider the binary case.",
            "So this prediction is going to be like what's the probability that this point?",
            "It takes class wine and with one minus that probability takes class two.",
            "OK. Now let me treat this probability as if it's a fractional weight.",
            "So what I'm going to do is I'm going to split this point X into 2 virtual copies so they're both X, but for one copy I'm going to attach label one.",
            "For the other copy, I'm going to attach label two and as I'm different weights one wait is this P, y = 1, The other is 1 minus this.",
            "So now I have fractional weight on my data, unlabeled data points, and my unlabeled, unlabeled data points not become labeled.",
            "With this now you have a completely labeled training set, but many points are fractional.",
            "That's fine, you go back and redo your MA."
        ],
        [
            "We estimate using the using the.",
            "This pretend labeled training set.",
            "We'll give you some new W mu and Sigma.",
            "Then you repeat this.",
            "Now we know this will converge to a local optimum.",
            "In the state of space.",
            "So that's how it works.",
            "Well, you can also."
        ],
        [
            "Do it as a special form of self training.",
            "OK, so as I said all along.",
            "In doing this method."
        ],
        [
            "The assumption really is that the data actually comes from the mixture model that you set up.",
            "For example, mixture of two Gaussian."
        ],
        [
            "Normally you don't know whether it's the case or not, and this is a artificial example where it is actually not the case.",
            "And let's see what will happen.",
            "So we have data set like this.",
            "This is the true label, queso.",
            "What you see here is they pretty much form 2 blobs.",
            "However, the desired classification is orthogonal to the clusters.",
            "So here is actually my decision boundary and everything above it is class positive class.",
            "Everything below it is negative class.",
            "This can happen for example in classifying text, because there are many different ways you can define your classic classification goal.",
            "So whether it's by topic or by the writing style.",
            "On the same data set, that may give you very different desired decision boundary and some of them may not correspond to the natural clustering of your data points.",
            "OK, so if that's the case, and then if you apply EM with a mixture of two, say multinomial or Gaussian.",
            "This."
        ],
        [
            "Is what will happen?",
            "OK, let's consider two solutions.",
            "One is where you fit two Gaussian distributions like that they fit the data really well.",
            "So you would have a higher likelihood, which is good from a jems perspective.",
            "That way you will have, of course lower likelihood, but this one will give you wrong classification if you believe each class each cluster is in one class.",
            "So this is a danger."
        ],
        [
            "And there are heuristic ways to hope to lessen this danger.",
            "One is to construct the correct generative model.",
            "Of course, that's easier said than done, but if you know something about your problem domain, you can say that OK, maybe there are.",
            "More on mixture components then two.",
            "Like each class may have multiple components in them and in doing so hopefully you can model the data better."
        ],
        [
            "You can also when you.",
            "Define your objective.",
            "You can also tune down the influence of unlabeled data.",
            "So this is exactly the same as before.",
            "We have joint unlabeled data and the marginal unlabeled data.",
            "Except that now you put a small coefficient in front of the unlabeled data and by tuning Lambda, making it small, you can have a whole range of behavior.",
            "There are some yes.",
            "It did not use any label, yes, so it's a matter of these two terms fighting each other.",
            "And so this is the thing I show up.",
            "There is purely this term and then you also have the other term which comes from here.",
            "OK.",
            "Depending on your fraction of labeled and unlabeled data, sometimes one term will win.",
            "So if you only have a very small number of labeled data, normally this term wins and you will get that kind of solution.",
            "All right, there are other dangers.",
            "For examp"
        ],
        [
            "So the notion of identifiability.",
            "You need to come up with generative models that are identifiable from just unlabeled data.",
            "That means from the unlabeled data alone, you should be able to.",
            "Identify the components in them.",
            "OK, you just don't know which component corresponds to which.",
            "Class, that's the job of label data.",
            "There is also the problem of EM local optimum.",
            "As we said, M will converge to a local optimum.",
            "Therefore where do you start?",
            "Your procedure is important.",
            "OK.",
            "So this is the probabilistic description of mixture model.",
            "In practice, people also do a variation which is more algorithm attic.",
            "This is the procedure."
        ],
        [
            "Label cluster label.",
            "So the idea is very simple if you have a clustering algorithm, think of any clustering algorithm you like.",
            "Hierarchical clustering K means clustering.",
            "You can use it so given labeled an unlabeled data set and clustering algorithm, quite A and a supervised learning algorithm, code L. Whatever you like, you can do the following."
        ],
        [
            "Let's cluster the whole data set using your clustering algorithm, identified those clusters."
        ],
        [
            "Now for each cluster.",
            "Let's see what are the label data points falls into it.",
            "OK, so let's look at a particular cluster an let's say as is the label that labeled data that forcing it."
        ],
        [
            "Let's just train supervised classifier quite FS on this cluster using your supervised learner.",
            "Then apply it to the rest of the cluster."
        ],
        [
            "OK, so that's it."
        ],
        [
            "Again, this is a wrapper method, therefore it's easy to do.",
            "Very easy to do.",
            "It does make a very similar assumption to the mixture model.",
            "We have seen.",
            "That is the clusters you get.",
            "They should somehow correspond to the.",
            "Classification disabled decision boundaries.",
            "Now let's look at one example.",
            "So you need to instantiate."
        ],
        [
            "This with that particular clustering algorithm and a particular classifier.",
            "So let's take hierarchical clustering and the simplest classifier majority vote.",
            "So for each cluster you're going to classify it by the majority class in it.",
            "OK, so again that's our data set with two labeled data points and I do hierarchical clustering with the assumption that I know before hand that there are two classes and therefore I want two clusters, so I would be with the tree up until I have two components.",
            "Now for people who are familiar with hierarchical clustering, there is a critical notion of how do you measure the distance between two clusters, right?",
            "And depending on whether you use the the minimum distance.",
            "Pair of points in those clusters, or the maximum distance you have different.",
            "Results why it's called simple linkage.",
            "The other is called complete linkage, so let's try simple language.",
            "This is the hierarchical clustering result.",
            "You get very nice."
        ],
        [
            "Like minimum spanning tree, except that you have two components.",
            "And then if you do majority vote well, since there is only one labeled data point in each cluster, you get this very nice prediction.",
            "Korrespondent"
        ],
        [
            "Sorry, no shame, but if you do, if you happen to pick something else, for example complete linkage.",
            "That's the cluster you get.",
            "And because our label points are here and here.",
            "Majority vote would be confused and if you have the rule of breaking the ties randomly, that would be the classification you get and which looks terrible.",
            "So this is just another case to show you that be careful with the assumption that you make.",
            "Any questions?",
            "Yes.",
            "Go to closing this thing so majority voting on all of this.",
            "Yeah, true, so it's not bad.",
            "It's not that either, but it just didn't get you there, yeah?",
            "And this is yeah.",
            "And of course this is artificial example to just to show you like what can happen, OK?",
            "Alright."
        ],
        [
            "Um?",
            "So let's move on to a very different kind of a semi supervised learning model.",
            "So called training and multi view learning.",
            "So let's."
        ],
        [
            "Use the example of named entity recognition or classification.",
            "And it's a very simplified task here, so this is a task in natural language processing, in particular when you when some companies crawl the web page, they want to automatically process the text and identify the person, location, organization, etc in the text and they want to do it using a classifier that's called named entity recognition.",
            "Named entity is phrase, which is a name.",
            "OK, and you want to say oh, this name is a is a person or is this name is a place?",
            "Or is an organization?",
            "So let's consider the simplified version where we have.",
            "Person and location.",
            "One example would be Mr. Washington that we know it's a person and then Washington state would be a location.",
            "OK, so."
        ],
        [
            "So in this task.",
            "So here's what make it special.",
            "Beyond the named entity itself, we actually know what's the text surrounding it.",
            "OK, so in this case, so we have the instance one that is the text surrounding it, and the text is headquartered in and I use_for this context.",
            "And then you have the name entity itself, and in the second instance we have missed Washington, the Vice president of so.",
            "This is a case where you first of all, let's assume you know where are the named entity.",
            "But second, you also know what's the context of it, sorry.",
            "Now, because of this, we can represent each object.",
            "In this case, each object is a named entity by a feature vector that consists of two parts or we will call it 2 views.",
            "So this is my notation, but these two views, the first view is the words of the named entity itself.",
            "The second view is the words surrounding it in the context.",
            "So I divide my feature vector into two parts.",
            "Why is this useful?",
            "We will see that immediately.",
            "OK."
        ],
        [
            "But let's first do a little exercise.",
            "If all I gave you is instant one, an instance 2.",
            "And I label them for you so that little out there means it's an location and this P there means it's a person.",
            "And I ask you, that's your training data.",
            "You have to label data points to classify those two points.",
            "How can we do that?",
            "Or can we do that?",
            "Yeah.",
            "Oh wow, yeah.",
            "Yeah.",
            "I didn't expect that.",
            "But that's a very good guess, yes, although I suspect that will not work too well on the whole corpus.",
            "Yes, of course we know English, so we know what's the correct classification of this, but pretend you don't know English.",
            "So then it's not possible to do that now.",
            "My claim is actually if you have more unlabeled data than that might actually help you, so let's see.",
            "So now you have 3 four."
        ],
        [
            "I've has more unlabeled data points.",
            "Let's try to do this so.",
            "I know headquartered in Washington state and that is a location.",
            "So by instance three, although I know nothing about it, I see the same context.",
            "Now you have to make the assumption you make the assumption that if named entities have the same context, they are in the same class.",
            "OK, so now Kazakhstan is labeled as location.",
            "The.",
            "Right and then by instance 4.",
            "Now, since you know this is location, you know that flew to is a context for location.",
            "And then finally we can use that to classify China as a location because of it.",
            "OK. How about the other one?",
            "Yeah partner yet.",
            "So this is the same.",
            "But how do we trace back?",
            "I think.",
            "The link there is I missed her so well, ignore that, but you see what happening with unlabeled data, it's possible to build this multi step in direct connection.",
            "And this leads to the very end."
        ],
        [
            "Interesting Co training algorithm.",
            "It goes as follows.",
            "You have this labeled and unlabeled training set.",
            "Now let's assume that each instance actually has 2 views.",
            "Two subsets of the feature vector there is another tuning knobs K. Here's what you do."
        ],
        [
            "Initially.",
            "Let's create two identical copies of training, set quite L1 and L2.",
            "Those are going to be for two different views.",
            "Then will you repeat?",
            "You were going to try and."
        ],
        [
            "Two classifiers.",
            "F1 that classifier only looks at view one OK yet ignores the second part of the feature vector and F2 only.",
            "Look at looks looks at view too.",
            "OK, so now you have these two feature vectors.",
            "Two classifiers, each trained on its own training set.",
            "Now what you do is you are going to label the unlabeled data."
        ],
        [
            "Using each classifier, but do it separately so you don't confuse which one is which.",
            "Then I'm going to."
        ],
        [
            "Do the following.",
            "It's very much like self training where if you look at the predictions made by F1 on the unlabeled data.",
            "Let's take the most confident ones.",
            "K and it's predicted label, but this time I'm going to take this small set and treat them as pretend label data.",
            "Give it to the training set of L2K.",
            "So I'm going to use that to make the training set of L2 bigger and vice versa.",
            "I'm going to do the same for.",
            "Do.",
            "OK, so I'm always.",
            "Giving the other learner my confident predictions.",
            "So if I'm F1 I give my training data of two and so on.",
            "OK, then I'm going to repeat this procedure until I'm satisfied.",
            "So this is the Co training procedure.",
            "Let's see.",
            "OK."
        ],
        [
            "So interesting, we can actually say something theoretical about it.",
            "It makes the following assumptions when these assumptions are true, we can be certain that the two classifiers will learn the correct concept.",
            "The assumptions are the following one.",
            "There is such a feature split.",
            "OK. 2nd."
        ],
        [
            "Each view or each subset of feature is sufficient to train a good classifier.",
            "So either F1 or F2 if you have.",
            "Enough labor data will be good enough.",
            "And certainly more importantly."
        ],
        [
            "Those two classifiers, sorry those two views are assumed to be conditionally independent given the class.",
            "So what does it mean?",
            "It.",
            "This is the definition.",
            "K but intuitively, intuitively, what is it doing?",
            "OK, here's a pictorial view of why it's important.",
            "If you have.",
            "This is, say, the X1 view.",
            "And X1 now is classifying those unlabeled data points and made it confident predictions.",
            "And we're about to add these guys to F twos training set.",
            "By the conditional independence assumption.",
            "Whatever it is, they are extreme.",
            "These points are extremes in this case, but they will be very mixed in the other one.",
            "Actually there will be kind of like random in the other view and that will help you.",
            "In learning a good classifier in the other view.",
            "OK, so."
        ],
        [
            "You might say that alright, why do I need this two features sets?",
            "If what if I don't have two views?",
            "OK.",
            "Yes.",
            "The right.",
            "Or the example?",
            "OK, so.",
            "Let's see so.",
            "Another yeah."
        ],
        [
            "See if I can get this.",
            "Another possibility is to say that X2 given Y and X1.",
            "Is the same as X2, given why?",
            "K and therefore knowing X1 give you some gives you no information about X2 and that's why it's mixed.",
            "In that case, if you know the location of X1 in this plot here you know nothing about X2 for the same class.",
            "OK.",
            "Right so.",
            "Let's see if I can come up with a good example here that means.",
            "If you know.",
            "The."
        ],
        [
            "For example, if you know the context.",
            "It doesn't tell you anything about the particular location phrase at all.",
            "K. Sorry I cannot do it.",
            "I have a good example but it's not coming out.",
            "OK, but it's in the it's in the book so."
        ],
        [
            "Alright, so you might say that OK, I don't naturally have two views in many real datasets.",
            "What do I do?",
            "Well, people have tried various things.",
            "They artificially split up the feature vector so you can do a random split of your future vectors and just do that.",
            "But that seems a bit artificial and well, and why limit it to two views, right?",
            "You can have multiple views and so on.",
            "So this multi view learning is a slight generalization of code training.",
            "OK, but to introduce multi view.",
            "We have to introduce a bit notation here.",
            "So let's start with the notion of loss function.",
            "I will call it C and the loss function applies to your X.",
            "The correct label Y and the predicted label FX and it's going to be a positive number.",
            "For example, the very familiar squared loss.",
            "In this case we ignore X.",
            "That's this squared difference.",
            "Or you can have the 01 loss which is 1 if you predict the wrong label and 0 otherwise.",
            "So with this loss function."
        ],
        [
            "You can define your empirical risk, which is essentially training set error, so that's our hat of classifier F or a learner.",
            "F is simply the average on the training set of your loss function.",
            "K."
        ],
        [
            "Now we also need to introduce this regularizer, call it big or Mega F. Usually it's something that penalizes complex F. So one possibility is to say what's the norm of your F function.",
            "So."
        ],
        [
            "A standard framework to train a good classifier is to do is to sort of minimize training set error, but we know that leads to overfitting, so you want to regularize it a little bit so it cannot.",
            "Terribly overfit, and that's by adding this regularizer so the training procedure is pretty much that you want to find the best function within some function family that you want, and then you minimize the training set error.",
            "OK, but you also want to minimize the regularizer.",
            "OK. Now multiple learn."
        ],
        [
            "It is very simple, it is.",
            "Just the regularised minimization procedure, with a very special.",
            "Unlabeled data dependent regularizer.",
            "So let's take a look at this.",
            "We assume that you have K views.",
            "They could just be K different classifiers, so it's like an ensemble method.",
            "So you have F1 to FK.",
            "You want to train those K different things.",
            "I'm going to use V here for different views or different classifiers.",
            "So if you look at that line, this is simply saying that I want the standard.",
            "Ignore this for now.",
            "This is the standard regularised minimization I want to for each classifier, I want small training error and its corresponding small.",
            "Normal, let's say that's the supervised.",
            "Regularise but then we add another term which says, OK, all those K different predictors.",
            "They should agree in terms of their predictions, on unlabeled data.",
            "OK, so this is what this term is doing.",
            "We have let's go through all pairs of predictors.",
            "So if you take any two functions.",
            "And then look at all your unlabeled data points.",
            "You want to say something like this, let's pretend.",
            "One of the prediction of FU on this unlabeled point is the true label, and I want the prediction of FV the other guy to be close to that.",
            "OK, and so you do this for every pair.",
            "Overall this has the effect of requiring that or K predictors with B.",
            "Almost predicting the same thing.",
            "OK, so that's it.",
            "Then you minimize this quantity.",
            "So I'm going to stop here for now and see if we have questions.",
            "If not, let's take a short break.",
            "So how about we start at 9:40?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So with that, let's begin.",
                    "label": 0
                },
                {
                    "sent": "We have Jerry shoe from Wisconsin Madison and he will talk about semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, so this tutorial is about semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "A lot of what I'm going to talk today comes from this new book, which is just out.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Saturday hi Andrew and Goldberg introduction to semi supervised learning and it's online.",
                    "label": 1
                },
                {
                    "sent": "You can download it.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the plan for today.",
                    "label": 0
                },
                {
                    "sent": "We were.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have two parts.",
                    "label": 0
                },
                {
                    "sent": "This roughly corresponds to old stuff and new stuff in the old stuff.",
                    "label": 0
                },
                {
                    "sent": "We will first define what is semi supervised learning and then we will review some of the more representative methods including mixture models, Co, training, manifold regularization and support vector machines as applied to the semi supervised learning setting.",
                    "label": 1
                },
                {
                    "sent": "Then in the new part we will look.",
                    "label": 1
                },
                {
                    "sent": "Briefly, at the theory of semi supervised learning and some new directions, including the online setting and.",
                    "label": 0
                },
                {
                    "sent": "More complicated setting.",
                    "label": 0
                },
                {
                    "sent": "In the end we will have a discussion on not machine learning but human learning, semi supervised learning in humans.",
                    "label": 0
                },
                {
                    "sent": "OK, please feel free to stop me and ask questions at anytime.",
                    "label": 0
                },
                {
                    "sent": "So part one, what is semisupervised?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Running OK. That is broadly defined as learning from both labeled and unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "By that definition, there are many paradigms which would fit in semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "The more familiar one is called semi supervised classification, where you are doing supervised learning, in particular classification where you have training data consists of all data points.",
                    "label": 0
                },
                {
                    "sent": "I'm going to call it XIYII from one 2L, so I'm going to use L for the number of labeled data points and then you have U unlabeled data points.",
                    "label": 0
                },
                {
                    "sent": "So those are just acts the feature vectors without the corresponding labels.",
                    "label": 0
                },
                {
                    "sent": "Often people make the assumption that you is much bigger than.",
                    "label": 0
                },
                {
                    "sent": "LK the goal here is to trend somehow a classifier F, which would be better than the classifier you trend using only labeled data.",
                    "label": 0
                },
                {
                    "sent": "So that's one example.",
                    "label": 0
                },
                {
                    "sent": "Then you also have the other extreme where you start from clustering.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you do constraint clustering where you put in some domain knowledge.",
                    "label": 0
                },
                {
                    "sent": "OK, so here you start with unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "But then you may have some supervised information, not necessarily in terms of Labor with, but perhaps in the form of, for example, must link cannot links.",
                    "label": 0
                },
                {
                    "sent": "You know that a pair of examples must be in the same cluster or not.",
                    "label": 0
                },
                {
                    "sent": "And the goal here is to create better clusterings.",
                    "label": 0
                },
                {
                    "sent": "In this talk, we will focus on.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Semi supervised classification.",
                    "label": 0
                },
                {
                    "sent": "OK, so why would we want to do this?",
                    "label": 0
                },
                {
                    "sent": "Traditionally this is motivated in machine learning because we want better performance for free.",
                    "label": 1
                },
                {
                    "sent": "And the story usually goes that label data can be hard to get.",
                    "label": 1
                },
                {
                    "sent": "In some cases when you actually need human experts annotators to mark the label for you.",
                    "label": 0
                },
                {
                    "sent": "So that can be slow and expensive.",
                    "label": 1
                },
                {
                    "sent": "In other cases, you might need experiments in the wet lab and special devices, so that's also expensive.",
                    "label": 0
                },
                {
                    "sent": "So often it's the case that labeled data is hard to get.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, unlabeled data is easy to get and.",
                    "label": 0
                },
                {
                    "sent": "Well, since you have all those only with data, why don't we use it?",
                    "label": 0
                },
                {
                    "sent": "So that's the motivation.",
                    "label": 0
                },
                {
                    "sent": "There is also another motivation.",
                    "label": 0
                },
                {
                    "sent": "More from the cockpit.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give science perspective.",
                    "label": 0
                },
                {
                    "sent": "So this has to do with the learning in general, and in particular, how do we as humans learn so?",
                    "label": 1
                },
                {
                    "sent": "Imagine the other case of concept learning in children, so they see an object.",
                    "label": 1
                },
                {
                    "sent": "Now you think of this object as X and they want to get the concept wide.",
                    "label": 0
                },
                {
                    "sent": "OK, how do they learn it right?",
                    "label": 1
                },
                {
                    "sent": "They receive supervised information so that it would point to this animal and say Doc.",
                    "label": 1
                },
                {
                    "sent": "So that's an explicit labeled data instance, but then they also see all those animals just walk around by themselves, right?",
                    "label": 0
                },
                {
                    "sent": "And you would imagine that they have to be using that information somehow and semi supervised learning models will develop will be used as computational models for human learning.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's go back to the machine learning side and say OK, So what are the cases when labeling is hard?",
                    "label": 0
                },
                {
                    "sent": "This is 1 case in speech recognition we saw the talk yesterday.",
                    "label": 0
                },
                {
                    "sent": "If you want to label the speech stream the acoustic signal at verifying Gray, i.e.",
                    "label": 0
                },
                {
                    "sent": "At the phonetic actually sub phonetic level, you need human annotators to listen to the recordings and do that.",
                    "label": 0
                },
                {
                    "sent": "So film would go like this.",
                    "label": 0
                },
                {
                    "sent": "Those are special annotations for for phonetic units, you don't need to worry about what they mean, but.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "Just from the look of it, it's hard to do and it actually takes a long time.",
                    "label": 0
                },
                {
                    "sent": "400 hours to transcribe one hour of speech, so that's a lot of time.",
                    "label": 1
                },
                {
                    "sent": "Similarly, when you do parsing.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is Chinese sentence.",
                    "label": 0
                },
                {
                    "sent": "But when you do parsing, you want to train a password.",
                    "label": 0
                },
                {
                    "sent": "You need training data, for example the parts, trees.",
                    "label": 0
                },
                {
                    "sent": "Those trees.",
                    "label": 0
                },
                {
                    "sent": "Initially I created by a linguist by hand.",
                    "label": 0
                },
                {
                    "sent": "Again, that's very slow to create.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's.",
                    "label": 0
                },
                {
                    "sent": "Let's see OK, so this is the notation we will use.",
                    "label": 0
                },
                {
                    "sent": "We will be using both X for instance.",
                    "label": 0
                },
                {
                    "sent": "Think of them as feature vectors and labor Y.",
                    "label": 0
                },
                {
                    "sent": "Normally their binary labels.",
                    "label": 0
                },
                {
                    "sent": "So we want to learn are F which Maps from X to Y label data.",
                    "label": 0
                },
                {
                    "sent": "As I said you have, I will be using this notation one 2L, so that's the training label data points and then we have some unlabeled data points.",
                    "label": 0
                },
                {
                    "sent": "Now let's distinguish two kinds of unlabeled data points.",
                    "label": 1
                },
                {
                    "sent": "One is the set you have available during training, so those are the only data you will be using in training and then there are true test data in the future which you do not see during the training stage.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Many of you.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You have probably heard for Semi supervised learning and transductive learning.",
                    "label": 0
                },
                {
                    "sent": "Let's define them precisely when I say semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "I main inductive semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So here the goal is if you are given labeled data set and unlabeled data set, you want to create a classifier F such that F is a good predictor in future data on the unlabeled data points you haven't seen.",
                    "label": 1
                },
                {
                    "sent": "So you want this F to be able to predict unseen test data points.",
                    "label": 0
                },
                {
                    "sent": "In contrast.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When we talk about transductive learning, it looks kind of similar, but your F is defined only on the labeled and unlabeled training set that's given to you.",
                    "label": 1
                },
                {
                    "sent": "OK, you don't even need to worry about data points outside that set.",
                    "label": 0
                },
                {
                    "sent": "So there is no induction required, and that's the only difference I'm going to use here.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Let's go back here if you think about the task of semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "It feels a bit weird because what you want to do is to learn.",
                    "label": 0
                },
                {
                    "sent": "F&F is a function that Maps X2Y right the whole point of having a training data is to tell you how access mapped to Y.",
                    "label": 0
                },
                {
                    "sent": "So you have some examples of that mapping.",
                    "label": 0
                },
                {
                    "sent": "Now if all you have in addition is this unlabeled data set.",
                    "label": 0
                },
                {
                    "sent": "You certainly don't see that mapping from X to Y from those unlabeled datasets.",
                    "label": 1
                },
                {
                    "sent": "And therefore it feels weird to be able to learn anything of the mapping just from the unlabeled data set.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The key lysing the assumption we have to make for semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So here's one example.",
                    "label": 0
                },
                {
                    "sent": "Let's consider the case where X lives in 1D, so those are.",
                    "label": 0
                },
                {
                    "sent": "That's the 1D space, let's say, and I have some.",
                    "label": 0
                },
                {
                    "sent": "Symbols there.",
                    "label": 0
                },
                {
                    "sent": "Red Cross and blue circle.",
                    "label": 0
                },
                {
                    "sent": "Those are two labeled data points I have.",
                    "label": 1
                },
                {
                    "sent": "The green dots are unlabeled data points I have.",
                    "label": 0
                },
                {
                    "sent": "So they're distributing like that.",
                    "label": 1
                },
                {
                    "sent": "This is small sample.",
                    "label": 0
                },
                {
                    "sent": "If you look at your label data points and you have to put a decision boundary.",
                    "label": 0
                },
                {
                    "sent": "Let's say it's a linear decision boundary, then the intuitive decision boundary has to be in the middle of the label data points.",
                    "label": 1
                },
                {
                    "sent": "So that's the dashed line.",
                    "label": 1
                },
                {
                    "sent": "But if you also have all those unlabeled data points, you look at that and you say, OK, I kind of see patterns.",
                    "label": 0
                },
                {
                    "sent": "I kind of see two groups of points.",
                    "label": 0
                },
                {
                    "sent": "Then you have to make the assumption you make the assumption that maybe each class is a tight cluster.",
                    "label": 1
                },
                {
                    "sent": "Maybe they can each class come from a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "If that's the case, then those unlabeled data points will tell you something about those Gaussian distributions.",
                    "label": 0
                },
                {
                    "sent": "They also tell you that your label data points are not right at the center of those Gaussian distributions, so if you believe what you believe, then you want to put the decision boundary in the middle of the gap, which is the solid line and therefore.",
                    "label": 0
                },
                {
                    "sent": "Knowing the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "In addition to this assumption will give you different decision boundary and that's pretty much what semi supervised learning does.",
                    "label": 0
                },
                {
                    "sent": "Kate.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, this is only one way to do semi supervised learning and we will introduce many other ways to do it.",
                    "label": 0
                },
                {
                    "sent": "Now, as a concrete example, let's do a very Zen.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No algorithm, it's called self training.",
                    "label": 0
                },
                {
                    "sent": "So you have this input.",
                    "label": 0
                },
                {
                    "sent": "Our label data points you unlabeled data points.",
                    "label": 0
                },
                {
                    "sent": "Then initially let's create two sets.",
                    "label": 0
                },
                {
                    "sent": "Let's have the L for label set an U4 unable set.",
                    "label": 0
                },
                {
                    "sent": "Then let's do this, let's try.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Train classifier F, whatever you like from our alone, OK, and use supervised learning.",
                    "label": 1
                },
                {
                    "sent": "Then you will apply this classifier to the unlabeled data set.",
                    "label": 1
                },
                {
                    "sent": "You are just classify them.",
                    "label": 0
                },
                {
                    "sent": "Now when you classify them in addition to a label, we also hope that your ass can produce some form of confidence.",
                    "label": 0
                },
                {
                    "sent": "It could be in the form of posterior probability.",
                    "label": 1
                },
                {
                    "sent": "Then what you do is you would remove.",
                    "label": 0
                },
                {
                    "sent": "A subset from this unlabeled data set and this subset is those unlabeled data points where your F made confident predictions.",
                    "label": 0
                },
                {
                    "sent": "You would remove those and.",
                    "label": 0
                },
                {
                    "sent": "Add their predicted label as if it's the correct label to the label set.",
                    "label": 0
                },
                {
                    "sent": "So now you make your unable to set smaller label set bigger you repeat this, you retrain your F. Now this is called.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Self training.",
                    "label": 0
                },
                {
                    "sent": "It's a wrapper method.",
                    "label": 0
                },
                {
                    "sent": "By that we mean that this procedure applies to any athlete you have, as long as it can give you a confidence measure.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can do whatever you can pick whatever whatever you want.",
                    "label": 0
                },
                {
                    "sent": "Therefore it's very good for the case when you have a very complicated system, for example in natural language processing.",
                    "label": 1
                },
                {
                    "sent": "Any classifiers are fairly complicated.",
                    "label": 0
                },
                {
                    "sent": "You don't want to reach into the black box and change that, so that's good.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to apply.",
                    "label": 0
                },
                {
                    "sent": "There is a downside though.",
                    "label": 0
                },
                {
                    "sent": "There's pretty much no guarantee.",
                    "label": 1
                },
                {
                    "sent": "OK, it's possible for your F to make some wrong predictions, but have high confidence, and it's possible for those run predictions to get into the labeled data set and therefore reinforce the mistake.",
                    "label": 0
                },
                {
                    "sent": "Also notice that OK, so I said that.",
                    "label": 0
                },
                {
                    "sent": "So let's look at a con.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Create example where we make a particular.",
                    "label": 0
                },
                {
                    "sent": "I've pick up particular function.",
                    "label": 0
                },
                {
                    "sent": "So this is exactly the same as before, except that into these two lines I picked a particular classifier.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple classifier, so here's the idea.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use a one nearest neighbor classifier, but here's my confidence measure.",
                    "label": 0
                },
                {
                    "sent": "Among all the unlabeled data point, I'm going to pick the unlabeled data point which is closest to.",
                    "label": 1
                },
                {
                    "sent": "A labeled data point, so I'm going to find one label point which is closer to closest to some labeled point already, and I'm going to label it with the label of that label point.",
                    "label": 1
                },
                {
                    "sent": "OK, so it's a very simple when you're in a hurry, just take the closest neighbor and repeat.",
                    "label": 0
                },
                {
                    "sent": "If there are ties breaking randomly.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see how it.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does.",
                    "label": 0
                },
                {
                    "sent": "Here we have this data set which are in 2D.",
                    "label": 0
                },
                {
                    "sent": "Each green dot is an unlabeled data point.",
                    "label": 0
                },
                {
                    "sent": "You initially have two labeled data points.",
                    "label": 0
                },
                {
                    "sent": "And then when you do this procedure.",
                    "label": 0
                },
                {
                    "sent": "It actually gradually label is only valid point around those labor points, so that's what you have in the iteration 25 and then it propagates more.",
                    "label": 0
                },
                {
                    "sent": "That's iteration 74.",
                    "label": 0
                },
                {
                    "sent": "So you see this arm is being down and we're in the progress of doing that.",
                    "label": 0
                },
                {
                    "sent": "So in the end you get that which looks very good.",
                    "label": 0
                },
                {
                    "sent": "OK, so this works.",
                    "label": 0
                },
                {
                    "sent": "But let's see a case where it doesn't work and all I'm going to do is to add one more unlabeled data point.",
                    "label": 0
                },
                {
                    "sent": "To the data set.",
                    "label": 0
                },
                {
                    "sent": "Do you see that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, the outlier there just that guy and well, so at some point.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You will start to propagate it in the wrong fashion and then that guy will invade this region and eventually get something else.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to keep this in mind, semi supervised learning may not always work and this is a repeating theme in today's talk.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's move on to our first.",
                    "label": 0
                },
                {
                    "sent": "Actually, second model mixture model.",
                    "label": 0
                },
                {
                    "sent": "You have already seen that.",
                    "label": 0
                },
                {
                    "sent": "That's the two Gaussian example, but let's make it more formal.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, here's an example, but this time in 2D.",
                    "label": 0
                },
                {
                    "sent": "So let's say you have some labeled data points, and I actually tell you that each class is a Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "K. So if that's the case, what do you do right?",
                    "label": 0
                },
                {
                    "sent": "You're given this data point.",
                    "label": 0
                },
                {
                    "sent": "This training data set.",
                    "label": 0
                },
                {
                    "sent": "Then it's easy, right?",
                    "label": 0
                },
                {
                    "sent": "You would estimate the mean and variance of your Gaussian distributions.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So your model is a mixture of Gaussian which has the following parameters.",
                    "label": 0
                },
                {
                    "sent": "You have W1W2 and those are the weights of the two Gaussians respectively.",
                    "label": 0
                },
                {
                    "sent": "They should sum to one and then you have the mean and covariance covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "The here's the full joint model, so the joint model is like first you pick which causing you generate your data points and then you use that Gaussian to generate your data point.",
                    "label": 0
                },
                {
                    "sent": "So if you know the parameters of the system, then classification is done by Bayes rule, so that's what you get.",
                    "label": 0
                },
                {
                    "sent": "And if you do it here.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those contours are the sample mean and sample covariance of your label data set.",
                    "label": 0
                },
                {
                    "sent": "If you apply Bayes rule, you can plot the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "In this case the decision boundary is the very hard to see Green line, so let me trace it for you.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like this.",
                    "label": 0
                },
                {
                    "sent": "OK. That's all fine, except that when you see.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More unlabeled data points.",
                    "label": 0
                },
                {
                    "sent": "OK, so the green dots are unlabeled data points from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "Now you realize that.",
                    "label": 0
                },
                {
                    "sent": "The maximum likelihood estimate for the Gaussian distributions are not so good.",
                    "label": 0
                },
                {
                    "sent": "Instead, what you should have is something.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "OK, the two Gaussian distributions have more.",
                    "label": 0
                },
                {
                    "sent": "Nicer covariance more diagonal ish so.",
                    "label": 0
                },
                {
                    "sent": "And the decision boundary estimated from this data is the green line here, almost vertical.",
                    "label": 0
                },
                {
                    "sent": "And we want to capture this notion.",
                    "label": 0
                },
                {
                    "sent": "So what's the difference here?",
                    "label": 0
                },
                {
                    "sent": "Well, the difference is how are you going to estimate the parameters of you?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "In particular, we will be doing maximum likelihood, but in one case we will maximize the label data likelihood.",
                    "label": 0
                },
                {
                    "sent": "But when you have unlabeled data, which we should do is to maximize the different form of likelihood there, taking into consideration the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "OK, so in.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This mixture model framework we make the assumption that you actually know the form of the model.",
                    "label": 1
                },
                {
                    "sent": "So that means you know it's a mixture of two Gaussians, let's say.",
                    "label": 0
                },
                {
                    "sent": "But then what you want to look at is this quantity.",
                    "label": 0
                },
                {
                    "sent": "The probability of both the label data and unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Now, since we do not have labels on the unlabeled data, you need to marginalized that out.",
                    "label": 0
                },
                {
                    "sent": "So that means you need some over the labels on the unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "So this is joint probability on the label data and marginal probability on the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "K.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then that's the quantity you want to optimize.",
                    "label": 0
                },
                {
                    "sent": "You can do maximum likelihood, or you can do map estimate, or you can be Bayesian whatever you like.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But you want to optimize that quantity.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There are many mixture models that has been widely used in semi supervised learning.",
                    "label": 1
                },
                {
                    "sent": "You can do mixture of Gaussian.",
                    "label": 1
                },
                {
                    "sent": "You can do mixture of multinomial.",
                    "label": 0
                },
                {
                    "sent": "That's the naive Bayes classifier used in text classification.",
                    "label": 0
                },
                {
                    "sent": "You can certainly trend hidden Markov models when you have unlabeled data, and that's well known.",
                    "label": 0
                },
                {
                    "sent": "Typically, the way you estimate the parameter that maximizes the likelihood above is by using the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "And in the in the case of hidden Markov model, this is called an Welch.",
                    "label": 0
                },
                {
                    "sent": "See so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at the case of Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "So with the with only label that, let's say you don't have unlabeled data, then you have is the labeled likelihood, so that you factor it as again the product of the prior or class and the conditional class conditional probability.",
                    "label": 0
                },
                {
                    "sent": "If you have any data points because there are independent, you can figure that out, and from that it's very easy to derive the maximum likelihood estimate.",
                    "label": 0
                },
                {
                    "sent": "It's just the sample mean and sample covariance.",
                    "label": 1
                },
                {
                    "sent": "Where is unlabeled data we want?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This term here, so the things now factors as follows.",
                    "label": 0
                },
                {
                    "sent": "You have exactly the same term label data points but unlabeled data point.",
                    "label": 0
                },
                {
                    "sent": "Those are the only way points you have a sum over the inside the log and this is the marginalization over labels.",
                    "label": 0
                },
                {
                    "sent": "OK. You're summing over the unknown labels.",
                    "label": 0
                },
                {
                    "sent": "This makes it hard to optimize directly.",
                    "label": 0
                },
                {
                    "sent": "Things are much harder.",
                    "label": 0
                },
                {
                    "sent": "It's no longer convex.",
                    "label": 0
                },
                {
                    "sent": "And you will need to use some iterative method, for example the EM algorithm to do that.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is how an E American would work as a procedure so.",
                    "label": 0
                },
                {
                    "sent": "You start from some Theta.",
                    "label": 0
                },
                {
                    "sent": "Typically this can be the maximum likelihood estimate from the label data alone, so you train it, try your model from the from the label data first.",
                    "label": 0
                },
                {
                    "sent": "Then you have the class prior you have them in and you have the covariance for each class.",
                    "label": 0
                },
                {
                    "sent": "Then you do the following.",
                    "label": 0
                },
                {
                    "sent": "Once you have this initial guess of the parameter, you have a classifier at hand, right?",
                    "label": 0
                },
                {
                    "sent": "Then you can apply it.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To all the unlabeled data points, for each unlabeled data point X, I'm going to compute the label distribution.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to predict its label, and that's just an application of Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "OK then, well, let's consider the binary case.",
                    "label": 0
                },
                {
                    "sent": "So this prediction is going to be like what's the probability that this point?",
                    "label": 0
                },
                {
                    "sent": "It takes class wine and with one minus that probability takes class two.",
                    "label": 0
                },
                {
                    "sent": "OK. Now let me treat this probability as if it's a fractional weight.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is I'm going to split this point X into 2 virtual copies so they're both X, but for one copy I'm going to attach label one.",
                    "label": 0
                },
                {
                    "sent": "For the other copy, I'm going to attach label two and as I'm different weights one wait is this P, y = 1, The other is 1 minus this.",
                    "label": 0
                },
                {
                    "sent": "So now I have fractional weight on my data, unlabeled data points, and my unlabeled, unlabeled data points not become labeled.",
                    "label": 0
                },
                {
                    "sent": "With this now you have a completely labeled training set, but many points are fractional.",
                    "label": 0
                },
                {
                    "sent": "That's fine, you go back and redo your MA.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We estimate using the using the.",
                    "label": 0
                },
                {
                    "sent": "This pretend labeled training set.",
                    "label": 0
                },
                {
                    "sent": "We'll give you some new W mu and Sigma.",
                    "label": 0
                },
                {
                    "sent": "Then you repeat this.",
                    "label": 0
                },
                {
                    "sent": "Now we know this will converge to a local optimum.",
                    "label": 0
                },
                {
                    "sent": "In the state of space.",
                    "label": 0
                },
                {
                    "sent": "So that's how it works.",
                    "label": 0
                },
                {
                    "sent": "Well, you can also.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do it as a special form of self training.",
                    "label": 1
                },
                {
                    "sent": "OK, so as I said all along.",
                    "label": 0
                },
                {
                    "sent": "In doing this method.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The assumption really is that the data actually comes from the mixture model that you set up.",
                    "label": 0
                },
                {
                    "sent": "For example, mixture of two Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Normally you don't know whether it's the case or not, and this is a artificial example where it is actually not the case.",
                    "label": 0
                },
                {
                    "sent": "And let's see what will happen.",
                    "label": 0
                },
                {
                    "sent": "So we have data set like this.",
                    "label": 0
                },
                {
                    "sent": "This is the true label, queso.",
                    "label": 0
                },
                {
                    "sent": "What you see here is they pretty much form 2 blobs.",
                    "label": 0
                },
                {
                    "sent": "However, the desired classification is orthogonal to the clusters.",
                    "label": 0
                },
                {
                    "sent": "So here is actually my decision boundary and everything above it is class positive class.",
                    "label": 0
                },
                {
                    "sent": "Everything below it is negative class.",
                    "label": 0
                },
                {
                    "sent": "This can happen for example in classifying text, because there are many different ways you can define your classic classification goal.",
                    "label": 1
                },
                {
                    "sent": "So whether it's by topic or by the writing style.",
                    "label": 0
                },
                {
                    "sent": "On the same data set, that may give you very different desired decision boundary and some of them may not correspond to the natural clustering of your data points.",
                    "label": 0
                },
                {
                    "sent": "OK, so if that's the case, and then if you apply EM with a mixture of two, say multinomial or Gaussian.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is what will happen?",
                    "label": 0
                },
                {
                    "sent": "OK, let's consider two solutions.",
                    "label": 0
                },
                {
                    "sent": "One is where you fit two Gaussian distributions like that they fit the data really well.",
                    "label": 0
                },
                {
                    "sent": "So you would have a higher likelihood, which is good from a jems perspective.",
                    "label": 0
                },
                {
                    "sent": "That way you will have, of course lower likelihood, but this one will give you wrong classification if you believe each class each cluster is in one class.",
                    "label": 0
                },
                {
                    "sent": "So this is a danger.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are heuristic ways to hope to lessen this danger.",
                    "label": 1
                },
                {
                    "sent": "One is to construct the correct generative model.",
                    "label": 1
                },
                {
                    "sent": "Of course, that's easier said than done, but if you know something about your problem domain, you can say that OK, maybe there are.",
                    "label": 0
                },
                {
                    "sent": "More on mixture components then two.",
                    "label": 0
                },
                {
                    "sent": "Like each class may have multiple components in them and in doing so hopefully you can model the data better.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also when you.",
                    "label": 0
                },
                {
                    "sent": "Define your objective.",
                    "label": 0
                },
                {
                    "sent": "You can also tune down the influence of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So this is exactly the same as before.",
                    "label": 0
                },
                {
                    "sent": "We have joint unlabeled data and the marginal unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Except that now you put a small coefficient in front of the unlabeled data and by tuning Lambda, making it small, you can have a whole range of behavior.",
                    "label": 0
                },
                {
                    "sent": "There are some yes.",
                    "label": 0
                },
                {
                    "sent": "It did not use any label, yes, so it's a matter of these two terms fighting each other.",
                    "label": 0
                },
                {
                    "sent": "And so this is the thing I show up.",
                    "label": 0
                },
                {
                    "sent": "There is purely this term and then you also have the other term which comes from here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Depending on your fraction of labeled and unlabeled data, sometimes one term will win.",
                    "label": 0
                },
                {
                    "sent": "So if you only have a very small number of labeled data, normally this term wins and you will get that kind of solution.",
                    "label": 0
                },
                {
                    "sent": "All right, there are other dangers.",
                    "label": 0
                },
                {
                    "sent": "For examp",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the notion of identifiability.",
                    "label": 0
                },
                {
                    "sent": "You need to come up with generative models that are identifiable from just unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "That means from the unlabeled data alone, you should be able to.",
                    "label": 0
                },
                {
                    "sent": "Identify the components in them.",
                    "label": 0
                },
                {
                    "sent": "OK, you just don't know which component corresponds to which.",
                    "label": 0
                },
                {
                    "sent": "Class, that's the job of label data.",
                    "label": 0
                },
                {
                    "sent": "There is also the problem of EM local optimum.",
                    "label": 0
                },
                {
                    "sent": "As we said, M will converge to a local optimum.",
                    "label": 0
                },
                {
                    "sent": "Therefore where do you start?",
                    "label": 0
                },
                {
                    "sent": "Your procedure is important.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the probabilistic description of mixture model.",
                    "label": 0
                },
                {
                    "sent": "In practice, people also do a variation which is more algorithm attic.",
                    "label": 0
                },
                {
                    "sent": "This is the procedure.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Label cluster label.",
                    "label": 0
                },
                {
                    "sent": "So the idea is very simple if you have a clustering algorithm, think of any clustering algorithm you like.",
                    "label": 0
                },
                {
                    "sent": "Hierarchical clustering K means clustering.",
                    "label": 0
                },
                {
                    "sent": "You can use it so given labeled an unlabeled data set and clustering algorithm, quite A and a supervised learning algorithm, code L. Whatever you like, you can do the following.",
                    "label": 1
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's cluster the whole data set using your clustering algorithm, identified those clusters.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now for each cluster.",
                    "label": 0
                },
                {
                    "sent": "Let's see what are the label data points falls into it.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's look at a particular cluster an let's say as is the label that labeled data that forcing it.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's just train supervised classifier quite FS on this cluster using your supervised learner.",
                    "label": 0
                },
                {
                    "sent": "Then apply it to the rest of the cluster.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's it.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, this is a wrapper method, therefore it's easy to do.",
                    "label": 0
                },
                {
                    "sent": "Very easy to do.",
                    "label": 0
                },
                {
                    "sent": "It does make a very similar assumption to the mixture model.",
                    "label": 0
                },
                {
                    "sent": "We have seen.",
                    "label": 0
                },
                {
                    "sent": "That is the clusters you get.",
                    "label": 0
                },
                {
                    "sent": "They should somehow correspond to the.",
                    "label": 0
                },
                {
                    "sent": "Classification disabled decision boundaries.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at one example.",
                    "label": 0
                },
                {
                    "sent": "So you need to instantiate.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This with that particular clustering algorithm and a particular classifier.",
                    "label": 0
                },
                {
                    "sent": "So let's take hierarchical clustering and the simplest classifier majority vote.",
                    "label": 0
                },
                {
                    "sent": "So for each cluster you're going to classify it by the majority class in it.",
                    "label": 0
                },
                {
                    "sent": "OK, so again that's our data set with two labeled data points and I do hierarchical clustering with the assumption that I know before hand that there are two classes and therefore I want two clusters, so I would be with the tree up until I have two components.",
                    "label": 0
                },
                {
                    "sent": "Now for people who are familiar with hierarchical clustering, there is a critical notion of how do you measure the distance between two clusters, right?",
                    "label": 0
                },
                {
                    "sent": "And depending on whether you use the the minimum distance.",
                    "label": 0
                },
                {
                    "sent": "Pair of points in those clusters, or the maximum distance you have different.",
                    "label": 0
                },
                {
                    "sent": "Results why it's called simple linkage.",
                    "label": 0
                },
                {
                    "sent": "The other is called complete linkage, so let's try simple language.",
                    "label": 0
                },
                {
                    "sent": "This is the hierarchical clustering result.",
                    "label": 0
                },
                {
                    "sent": "You get very nice.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like minimum spanning tree, except that you have two components.",
                    "label": 0
                },
                {
                    "sent": "And then if you do majority vote well, since there is only one labeled data point in each cluster, you get this very nice prediction.",
                    "label": 0
                },
                {
                    "sent": "Korrespondent",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry, no shame, but if you do, if you happen to pick something else, for example complete linkage.",
                    "label": 0
                },
                {
                    "sent": "That's the cluster you get.",
                    "label": 0
                },
                {
                    "sent": "And because our label points are here and here.",
                    "label": 0
                },
                {
                    "sent": "Majority vote would be confused and if you have the rule of breaking the ties randomly, that would be the classification you get and which looks terrible.",
                    "label": 0
                },
                {
                    "sent": "So this is just another case to show you that be careful with the assumption that you make.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Go to closing this thing so majority voting on all of this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, true, so it's not bad.",
                    "label": 0
                },
                {
                    "sent": "It's not that either, but it just didn't get you there, yeah?",
                    "label": 0
                },
                {
                    "sent": "And this is yeah.",
                    "label": 0
                },
                {
                    "sent": "And of course this is artificial example to just to show you like what can happen, OK?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So let's move on to a very different kind of a semi supervised learning model.",
                    "label": 0
                },
                {
                    "sent": "So called training and multi view learning.",
                    "label": 0
                },
                {
                    "sent": "So let's.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use the example of named entity recognition or classification.",
                    "label": 0
                },
                {
                    "sent": "And it's a very simplified task here, so this is a task in natural language processing, in particular when you when some companies crawl the web page, they want to automatically process the text and identify the person, location, organization, etc in the text and they want to do it using a classifier that's called named entity recognition.",
                    "label": 0
                },
                {
                    "sent": "Named entity is phrase, which is a name.",
                    "label": 0
                },
                {
                    "sent": "OK, and you want to say oh, this name is a is a person or is this name is a place?",
                    "label": 0
                },
                {
                    "sent": "Or is an organization?",
                    "label": 0
                },
                {
                    "sent": "So let's consider the simplified version where we have.",
                    "label": 0
                },
                {
                    "sent": "Person and location.",
                    "label": 0
                },
                {
                    "sent": "One example would be Mr. Washington that we know it's a person and then Washington state would be a location.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this task.",
                    "label": 0
                },
                {
                    "sent": "So here's what make it special.",
                    "label": 0
                },
                {
                    "sent": "Beyond the named entity itself, we actually know what's the text surrounding it.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case, so we have the instance one that is the text surrounding it, and the text is headquartered in and I use_for this context.",
                    "label": 0
                },
                {
                    "sent": "And then you have the name entity itself, and in the second instance we have missed Washington, the Vice president of so.",
                    "label": 1
                },
                {
                    "sent": "This is a case where you first of all, let's assume you know where are the named entity.",
                    "label": 0
                },
                {
                    "sent": "But second, you also know what's the context of it, sorry.",
                    "label": 0
                },
                {
                    "sent": "Now, because of this, we can represent each object.",
                    "label": 0
                },
                {
                    "sent": "In this case, each object is a named entity by a feature vector that consists of two parts or we will call it 2 views.",
                    "label": 0
                },
                {
                    "sent": "So this is my notation, but these two views, the first view is the words of the named entity itself.",
                    "label": 1
                },
                {
                    "sent": "The second view is the words surrounding it in the context.",
                    "label": 0
                },
                {
                    "sent": "So I divide my feature vector into two parts.",
                    "label": 0
                },
                {
                    "sent": "Why is this useful?",
                    "label": 0
                },
                {
                    "sent": "We will see that immediately.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But let's first do a little exercise.",
                    "label": 0
                },
                {
                    "sent": "If all I gave you is instant one, an instance 2.",
                    "label": 0
                },
                {
                    "sent": "And I label them for you so that little out there means it's an location and this P there means it's a person.",
                    "label": 0
                },
                {
                    "sent": "And I ask you, that's your training data.",
                    "label": 0
                },
                {
                    "sent": "You have to label data points to classify those two points.",
                    "label": 0
                },
                {
                    "sent": "How can we do that?",
                    "label": 0
                },
                {
                    "sent": "Or can we do that?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh wow, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I didn't expect that.",
                    "label": 0
                },
                {
                    "sent": "But that's a very good guess, yes, although I suspect that will not work too well on the whole corpus.",
                    "label": 0
                },
                {
                    "sent": "Yes, of course we know English, so we know what's the correct classification of this, but pretend you don't know English.",
                    "label": 0
                },
                {
                    "sent": "So then it's not possible to do that now.",
                    "label": 0
                },
                {
                    "sent": "My claim is actually if you have more unlabeled data than that might actually help you, so let's see.",
                    "label": 0
                },
                {
                    "sent": "So now you have 3 four.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I've has more unlabeled data points.",
                    "label": 1
                },
                {
                    "sent": "Let's try to do this so.",
                    "label": 1
                },
                {
                    "sent": "I know headquartered in Washington state and that is a location.",
                    "label": 0
                },
                {
                    "sent": "So by instance three, although I know nothing about it, I see the same context.",
                    "label": 0
                },
                {
                    "sent": "Now you have to make the assumption you make the assumption that if named entities have the same context, they are in the same class.",
                    "label": 0
                },
                {
                    "sent": "OK, so now Kazakhstan is labeled as location.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Right and then by instance 4.",
                    "label": 0
                },
                {
                    "sent": "Now, since you know this is location, you know that flew to is a context for location.",
                    "label": 0
                },
                {
                    "sent": "And then finally we can use that to classify China as a location because of it.",
                    "label": 0
                },
                {
                    "sent": "OK. How about the other one?",
                    "label": 0
                },
                {
                    "sent": "Yeah partner yet.",
                    "label": 0
                },
                {
                    "sent": "So this is the same.",
                    "label": 0
                },
                {
                    "sent": "But how do we trace back?",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "The link there is I missed her so well, ignore that, but you see what happening with unlabeled data, it's possible to build this multi step in direct connection.",
                    "label": 0
                },
                {
                    "sent": "And this leads to the very end.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interesting Co training algorithm.",
                    "label": 0
                },
                {
                    "sent": "It goes as follows.",
                    "label": 0
                },
                {
                    "sent": "You have this labeled and unlabeled training set.",
                    "label": 0
                },
                {
                    "sent": "Now let's assume that each instance actually has 2 views.",
                    "label": 0
                },
                {
                    "sent": "Two subsets of the feature vector there is another tuning knobs K. Here's what you do.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Initially.",
                    "label": 0
                },
                {
                    "sent": "Let's create two identical copies of training, set quite L1 and L2.",
                    "label": 0
                },
                {
                    "sent": "Those are going to be for two different views.",
                    "label": 0
                },
                {
                    "sent": "Then will you repeat?",
                    "label": 0
                },
                {
                    "sent": "You were going to try and.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two classifiers.",
                    "label": 0
                },
                {
                    "sent": "F1 that classifier only looks at view one OK yet ignores the second part of the feature vector and F2 only.",
                    "label": 0
                },
                {
                    "sent": "Look at looks looks at view too.",
                    "label": 0
                },
                {
                    "sent": "OK, so now you have these two feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Two classifiers, each trained on its own training set.",
                    "label": 0
                },
                {
                    "sent": "Now what you do is you are going to label the unlabeled data.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using each classifier, but do it separately so you don't confuse which one is which.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do the following.",
                    "label": 0
                },
                {
                    "sent": "It's very much like self training where if you look at the predictions made by F1 on the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Let's take the most confident ones.",
                    "label": 0
                },
                {
                    "sent": "K and it's predicted label, but this time I'm going to take this small set and treat them as pretend label data.",
                    "label": 0
                },
                {
                    "sent": "Give it to the training set of L2K.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to use that to make the training set of L2 bigger and vice versa.",
                    "label": 0
                },
                {
                    "sent": "I'm going to do the same for.",
                    "label": 0
                },
                {
                    "sent": "Do.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm always.",
                    "label": 0
                },
                {
                    "sent": "Giving the other learner my confident predictions.",
                    "label": 0
                },
                {
                    "sent": "So if I'm F1 I give my training data of two and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, then I'm going to repeat this procedure until I'm satisfied.",
                    "label": 0
                },
                {
                    "sent": "So this is the Co training procedure.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So interesting, we can actually say something theoretical about it.",
                    "label": 0
                },
                {
                    "sent": "It makes the following assumptions when these assumptions are true, we can be certain that the two classifiers will learn the correct concept.",
                    "label": 0
                },
                {
                    "sent": "The assumptions are the following one.",
                    "label": 0
                },
                {
                    "sent": "There is such a feature split.",
                    "label": 1
                },
                {
                    "sent": "OK. 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each view or each subset of feature is sufficient to train a good classifier.",
                    "label": 1
                },
                {
                    "sent": "So either F1 or F2 if you have.",
                    "label": 0
                },
                {
                    "sent": "Enough labor data will be good enough.",
                    "label": 0
                },
                {
                    "sent": "And certainly more importantly.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those two classifiers, sorry those two views are assumed to be conditionally independent given the class.",
                    "label": 1
                },
                {
                    "sent": "So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "This is the definition.",
                    "label": 0
                },
                {
                    "sent": "K but intuitively, intuitively, what is it doing?",
                    "label": 0
                },
                {
                    "sent": "OK, here's a pictorial view of why it's important.",
                    "label": 0
                },
                {
                    "sent": "If you have.",
                    "label": 1
                },
                {
                    "sent": "This is, say, the X1 view.",
                    "label": 0
                },
                {
                    "sent": "And X1 now is classifying those unlabeled data points and made it confident predictions.",
                    "label": 0
                },
                {
                    "sent": "And we're about to add these guys to F twos training set.",
                    "label": 0
                },
                {
                    "sent": "By the conditional independence assumption.",
                    "label": 0
                },
                {
                    "sent": "Whatever it is, they are extreme.",
                    "label": 0
                },
                {
                    "sent": "These points are extremes in this case, but they will be very mixed in the other one.",
                    "label": 0
                },
                {
                    "sent": "Actually there will be kind of like random in the other view and that will help you.",
                    "label": 1
                },
                {
                    "sent": "In learning a good classifier in the other view.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You might say that alright, why do I need this two features sets?",
                    "label": 0
                },
                {
                    "sent": "If what if I don't have two views?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "The right.",
                    "label": 0
                },
                {
                    "sent": "Or the example?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Let's see so.",
                    "label": 0
                },
                {
                    "sent": "Another yeah.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See if I can get this.",
                    "label": 0
                },
                {
                    "sent": "Another possibility is to say that X2 given Y and X1.",
                    "label": 0
                },
                {
                    "sent": "Is the same as X2, given why?",
                    "label": 0
                },
                {
                    "sent": "K and therefore knowing X1 give you some gives you no information about X2 and that's why it's mixed.",
                    "label": 0
                },
                {
                    "sent": "In that case, if you know the location of X1 in this plot here you know nothing about X2 for the same class.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Let's see if I can come up with a good example here that means.",
                    "label": 0
                },
                {
                    "sent": "If you know.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, if you know the context.",
                    "label": 0
                },
                {
                    "sent": "It doesn't tell you anything about the particular location phrase at all.",
                    "label": 0
                },
                {
                    "sent": "K. Sorry I cannot do it.",
                    "label": 0
                },
                {
                    "sent": "I have a good example but it's not coming out.",
                    "label": 0
                },
                {
                    "sent": "OK, but it's in the it's in the book so.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so you might say that OK, I don't naturally have two views in many real datasets.",
                    "label": 0
                },
                {
                    "sent": "What do I do?",
                    "label": 0
                },
                {
                    "sent": "Well, people have tried various things.",
                    "label": 0
                },
                {
                    "sent": "They artificially split up the feature vector so you can do a random split of your future vectors and just do that.",
                    "label": 0
                },
                {
                    "sent": "But that seems a bit artificial and well, and why limit it to two views, right?",
                    "label": 0
                },
                {
                    "sent": "You can have multiple views and so on.",
                    "label": 0
                },
                {
                    "sent": "So this multi view learning is a slight generalization of code training.",
                    "label": 0
                },
                {
                    "sent": "OK, but to introduce multi view.",
                    "label": 0
                },
                {
                    "sent": "We have to introduce a bit notation here.",
                    "label": 0
                },
                {
                    "sent": "So let's start with the notion of loss function.",
                    "label": 0
                },
                {
                    "sent": "I will call it C and the loss function applies to your X.",
                    "label": 0
                },
                {
                    "sent": "The correct label Y and the predicted label FX and it's going to be a positive number.",
                    "label": 0
                },
                {
                    "sent": "For example, the very familiar squared loss.",
                    "label": 1
                },
                {
                    "sent": "In this case we ignore X.",
                    "label": 0
                },
                {
                    "sent": "That's this squared difference.",
                    "label": 0
                },
                {
                    "sent": "Or you can have the 01 loss which is 1 if you predict the wrong label and 0 otherwise.",
                    "label": 1
                },
                {
                    "sent": "So with this loss function.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can define your empirical risk, which is essentially training set error, so that's our hat of classifier F or a learner.",
                    "label": 0
                },
                {
                    "sent": "F is simply the average on the training set of your loss function.",
                    "label": 0
                },
                {
                    "sent": "K.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we also need to introduce this regularizer, call it big or Mega F. Usually it's something that penalizes complex F. So one possibility is to say what's the norm of your F function.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A standard framework to train a good classifier is to do is to sort of minimize training set error, but we know that leads to overfitting, so you want to regularize it a little bit so it cannot.",
                    "label": 0
                },
                {
                    "sent": "Terribly overfit, and that's by adding this regularizer so the training procedure is pretty much that you want to find the best function within some function family that you want, and then you minimize the training set error.",
                    "label": 0
                },
                {
                    "sent": "OK, but you also want to minimize the regularizer.",
                    "label": 0
                },
                {
                    "sent": "OK. Now multiple learn.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is very simple, it is.",
                    "label": 0
                },
                {
                    "sent": "Just the regularised minimization procedure, with a very special.",
                    "label": 0
                },
                {
                    "sent": "Unlabeled data dependent regularizer.",
                    "label": 0
                },
                {
                    "sent": "So let's take a look at this.",
                    "label": 0
                },
                {
                    "sent": "We assume that you have K views.",
                    "label": 0
                },
                {
                    "sent": "They could just be K different classifiers, so it's like an ensemble method.",
                    "label": 0
                },
                {
                    "sent": "So you have F1 to FK.",
                    "label": 0
                },
                {
                    "sent": "You want to train those K different things.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use V here for different views or different classifiers.",
                    "label": 0
                },
                {
                    "sent": "So if you look at that line, this is simply saying that I want the standard.",
                    "label": 0
                },
                {
                    "sent": "Ignore this for now.",
                    "label": 0
                },
                {
                    "sent": "This is the standard regularised minimization I want to for each classifier, I want small training error and its corresponding small.",
                    "label": 0
                },
                {
                    "sent": "Normal, let's say that's the supervised.",
                    "label": 0
                },
                {
                    "sent": "Regularise but then we add another term which says, OK, all those K different predictors.",
                    "label": 0
                },
                {
                    "sent": "They should agree in terms of their predictions, on unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is what this term is doing.",
                    "label": 0
                },
                {
                    "sent": "We have let's go through all pairs of predictors.",
                    "label": 0
                },
                {
                    "sent": "So if you take any two functions.",
                    "label": 0
                },
                {
                    "sent": "And then look at all your unlabeled data points.",
                    "label": 0
                },
                {
                    "sent": "You want to say something like this, let's pretend.",
                    "label": 0
                },
                {
                    "sent": "One of the prediction of FU on this unlabeled point is the true label, and I want the prediction of FV the other guy to be close to that.",
                    "label": 0
                },
                {
                    "sent": "OK, and so you do this for every pair.",
                    "label": 0
                },
                {
                    "sent": "Overall this has the effect of requiring that or K predictors with B.",
                    "label": 0
                },
                {
                    "sent": "Almost predicting the same thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's it.",
                    "label": 0
                },
                {
                    "sent": "Then you minimize this quantity.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to stop here for now and see if we have questions.",
                    "label": 0
                },
                {
                    "sent": "If not, let's take a short break.",
                    "label": 0
                },
                {
                    "sent": "So how about we start at 9:40?",
                    "label": 0
                }
            ]
        }
    }
}