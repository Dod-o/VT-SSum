{
    "id": "ak3imaapzt2rfob7r5uviprjrxp6v6f7",
    "title": "Efficient Sampling for Gaussian Graphical Models via Spectral Sparsification",
    "info": {
        "author": [
            "Dehua Cheng, Computer Science Department, University of Southern California"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_cheng_spectral_sparsification/",
    "segmentation": [
        [
            "And this is joint work with a young young Leo and Shanghai Tang at USC and reachable at MIT."
        ],
        [
            "Now let's consider this.",
            "In dimensional multivariate Gaussian distribution parameterized by the precision matrix Lambda and potential vector H, with the PDF showing here and this type of prioritization is very common with the distribution is presented as a Markov random field and our task here is trying to draw a sample from this distribution very efficiently and."
        ],
        [
            "We have three complexity criterias in mind.",
            "The first one time complexity or the work is actually the total amount of calculation operation we need to generate a sample at parallel complexity or the depth is the length of the critical path longest critical passing the algorithm and also we have this randomness complexity, which is how many of the independent random scalars we need to generate the sample, and ultimately, if possible we would like to bring down the total work to the.",
            "Be linear in the number of non zeros in the precision matrix, which is the input size and also at the same time we want to get sample at constant time when computed in parallel.",
            "And we also want to only use N random random numbers to get a sample however.",
            "This problem is still open even if we have some, even if we have some nontrivial structural assumptions on the precision matrix, such as symmetric diagonal dominant.",
            "But in our work we actually came close more precisely, or actually speaking that for the multivariate Gaussian distribution with STD precision matrix, we can draw a sample from it with nearly linear near linear work and polylogarithmic depth, and the optimal randomness complexity.",
            "Although this is only for the.",
            "Although this is only for the SD precision, we think it's an interesting first step and we think there it can be used as a subroutine in many other algorithms, such as a block if sampling.",
            "Now let me."
        ],
        [
            "Briefly introduce our approach.",
            "Here is a key step which is to factorize covariance matrix.",
            "This is this is equivalent as finding linear operator that is the inverse vector of the precision matrix.",
            "When we have this operator, then we can generate ID random samples very efficiently using this formula.",
            "Milk here is just the mean of the Gaussian which can be obtained by some USD linear system and why?",
            "Here is just a standard spherical Gaussian random vector and also because of the randomness complexity requirements.",
            "We need the factor to be a near square instead of.",
            "It can't be that fat.",
            "And now too."
        ],
        [
            "Illustrate our factorization algorithm.",
            "Let's consider the case that precision matrix is normalized Laplacian graph Laplacian which is introduce a bill with the SVD matrix in our setting to factorize, I managed to find inverse factor of MSX.",
            "We use this matrix identity which very Interestingly actually naturally arise in the Newton's method for finding the inverse square root.",
            "And also we have seen this similar formula yesterday in the keynote talk.",
            "But here we stick with multiplication.",
            "Because we need to return when it will return a separate factor instead of 1 inverse operator.",
            "And also this is already in the symmetric form here.",
            "Notice this middle term is actually a has better condition number then I'm original MSX because the spectral radius of X is bounded by one, so we treated as a new.",
            "I'm SX one and proceed with the new X, so I assume XD will become practically there.",
            "Oh and we can recover the inverse vector by tracing this process backward.",
            "Here at the company."
        ],
        [
            "Computational challenges that as we as we learned yesterday is the XD will become dense very quickly and so we need to specify it.",
            "However, the existing Special special specification algorithm does not directly apply here, because writing down this middle term alone is too expensive.",
            "What we need is a specification algorithm that can directly specify a dense object that is represented in polynomial of sparse matrices, and punishment have solved for degree two polynomial, which which is a similar special case.",
            "Here we are aiming at more general matrix polynomials.",
            "Here we can see that this middle term is actually one instance of a family of matrix polynomial, which we call them random walk matrix polynomial an X.",
            "Here is closely closely connected with the transition matrix of the natural random walk on the graph, and this polynomial is just a convex combination of the random walk with different number of steps and we by studying this connection we have proposed algorithm that can efficiently sample of matrix polynomials in this family with.",
            "Here linear work and polylogarithmic depth, and we believe that this result will enable many more efficient numerical routines and we have more results and so please came by our poster.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is joint work with a young young Leo and Shanghai Tang at USC and reachable at MIT.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's consider this.",
                    "label": 0
                },
                {
                    "sent": "In dimensional multivariate Gaussian distribution parameterized by the precision matrix Lambda and potential vector H, with the PDF showing here and this type of prioritization is very common with the distribution is presented as a Markov random field and our task here is trying to draw a sample from this distribution very efficiently and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have three complexity criterias in mind.",
                    "label": 0
                },
                {
                    "sent": "The first one time complexity or the work is actually the total amount of calculation operation we need to generate a sample at parallel complexity or the depth is the length of the critical path longest critical passing the algorithm and also we have this randomness complexity, which is how many of the independent random scalars we need to generate the sample, and ultimately, if possible we would like to bring down the total work to the.",
                    "label": 0
                },
                {
                    "sent": "Be linear in the number of non zeros in the precision matrix, which is the input size and also at the same time we want to get sample at constant time when computed in parallel.",
                    "label": 0
                },
                {
                    "sent": "And we also want to only use N random random numbers to get a sample however.",
                    "label": 0
                },
                {
                    "sent": "This problem is still open even if we have some, even if we have some nontrivial structural assumptions on the precision matrix, such as symmetric diagonal dominant.",
                    "label": 0
                },
                {
                    "sent": "But in our work we actually came close more precisely, or actually speaking that for the multivariate Gaussian distribution with STD precision matrix, we can draw a sample from it with nearly linear near linear work and polylogarithmic depth, and the optimal randomness complexity.",
                    "label": 1
                },
                {
                    "sent": "Although this is only for the.",
                    "label": 0
                },
                {
                    "sent": "Although this is only for the SD precision, we think it's an interesting first step and we think there it can be used as a subroutine in many other algorithms, such as a block if sampling.",
                    "label": 0
                },
                {
                    "sent": "Now let me.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Briefly introduce our approach.",
                    "label": 0
                },
                {
                    "sent": "Here is a key step which is to factorize covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "This is this is equivalent as finding linear operator that is the inverse vector of the precision matrix.",
                    "label": 1
                },
                {
                    "sent": "When we have this operator, then we can generate ID random samples very efficiently using this formula.",
                    "label": 1
                },
                {
                    "sent": "Milk here is just the mean of the Gaussian which can be obtained by some USD linear system and why?",
                    "label": 0
                },
                {
                    "sent": "Here is just a standard spherical Gaussian random vector and also because of the randomness complexity requirements.",
                    "label": 0
                },
                {
                    "sent": "We need the factor to be a near square instead of.",
                    "label": 0
                },
                {
                    "sent": "It can't be that fat.",
                    "label": 0
                },
                {
                    "sent": "And now too.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Illustrate our factorization algorithm.",
                    "label": 0
                },
                {
                    "sent": "Let's consider the case that precision matrix is normalized Laplacian graph Laplacian which is introduce a bill with the SVD matrix in our setting to factorize, I managed to find inverse factor of MSX.",
                    "label": 0
                },
                {
                    "sent": "We use this matrix identity which very Interestingly actually naturally arise in the Newton's method for finding the inverse square root.",
                    "label": 1
                },
                {
                    "sent": "And also we have seen this similar formula yesterday in the keynote talk.",
                    "label": 0
                },
                {
                    "sent": "But here we stick with multiplication.",
                    "label": 0
                },
                {
                    "sent": "Because we need to return when it will return a separate factor instead of 1 inverse operator.",
                    "label": 0
                },
                {
                    "sent": "And also this is already in the symmetric form here.",
                    "label": 0
                },
                {
                    "sent": "Notice this middle term is actually a has better condition number then I'm original MSX because the spectral radius of X is bounded by one, so we treated as a new.",
                    "label": 0
                },
                {
                    "sent": "I'm SX one and proceed with the new X, so I assume XD will become practically there.",
                    "label": 0
                },
                {
                    "sent": "Oh and we can recover the inverse vector by tracing this process backward.",
                    "label": 0
                },
                {
                    "sent": "Here at the company.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Computational challenges that as we as we learned yesterday is the XD will become dense very quickly and so we need to specify it.",
                    "label": 0
                },
                {
                    "sent": "However, the existing Special special specification algorithm does not directly apply here, because writing down this middle term alone is too expensive.",
                    "label": 0
                },
                {
                    "sent": "What we need is a specification algorithm that can directly specify a dense object that is represented in polynomial of sparse matrices, and punishment have solved for degree two polynomial, which which is a similar special case.",
                    "label": 0
                },
                {
                    "sent": "Here we are aiming at more general matrix polynomials.",
                    "label": 0
                },
                {
                    "sent": "Here we can see that this middle term is actually one instance of a family of matrix polynomial, which we call them random walk matrix polynomial an X.",
                    "label": 1
                },
                {
                    "sent": "Here is closely closely connected with the transition matrix of the natural random walk on the graph, and this polynomial is just a convex combination of the random walk with different number of steps and we by studying this connection we have proposed algorithm that can efficiently sample of matrix polynomials in this family with.",
                    "label": 1
                },
                {
                    "sent": "Here linear work and polylogarithmic depth, and we believe that this result will enable many more efficient numerical routines and we have more results and so please came by our poster.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}