{
    "id": "afth7o535y4aawqvah4eptgwa4665n3a",
    "title": "Stable and Accurate Feature Selection",
    "info": {
        "author": [
            "Zehra Cataltepe, Istanbul Technical University"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Feature Selection"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_cataltepe_safs/",
    "segmentation": [
        [
            "OK, so the table of can't."
        ],
        [
            "So I'm going to talk about first.",
            "I'm going to give it, you know, two slide summary of what we are doing because this is where I'm going to get most of our attention anyway.",
            "And then if you keep your attention, I'm going to talk about stability measure that we are going to be using in this work, then I'm going to talk about minimum redundancy, maximum relevance, which is the work of Christine and his colleagues do feature selection that we are going to be using and which we have used in a number of other studies and found to be quite successful.",
            "Then MMR has two different criteria to select features in my DNA molecule.",
            "I'm going to talk about the stability and accuracy of those two different criteria and will also include introduce an adage criteria, mids, Alpha which seems to give us some.",
            "Balance some tradeoff between stability and accuracy.",
            "Then I'll talk about experimental results on eight different datasets and can."
        ],
        [
            "Put the papers.",
            "So we all want our feature selection selection methods to be accurate.",
            "You know whether we want to have better accuracy than what we have or we want to have less features and still the accuracy that we are having.",
            "We do want to get rid of noisy, irrelevant or redundant features and have a good accuracy.",
            "There is another way to look at feature selection.",
            "We can also ask how stable the feature selection algorithm is and by stability we mean if there are small changes in the identity of training samples.",
            "Do the features selected change a lot or not?",
            "And this is especially important if you have a large number of features and you do want to know which features you should be acquiring.",
            "Or which features you should be naming as important.",
            "For example, in microdata analysis, which features are important for?"
        ],
        [
            "Biomarker identification and what we do in this work is we introduce a method to evaluate stability, feature selection method as the number of training samples decrease and we compare this table 10 accuracy of MI dynamic variance of MMR method and we do this spot theoretically and experimentally.",
            "We introduced them ideal for Metroid which gives us a tradeoff between relevance and redundancy and then for different datasets.",
            "We show that mids is.",
            "More stable than immature and then be sure that different values of Alpha should be chosen.",
            "If you want better stability and accuracy."
        ],
        [
            "So I have this is the.",
            "Somebody that has been work on stability measures before kulo SIS evaluated different stability measures, different sensibilities of different feature selection methods, and he showed that although some feature selection methods have the similar accuracies, they have huge variations in their stabilities.",
            "That's what we also see in our datasets, and there are two work by.",
            "On work by says ET al.",
            "2008 another one you Dingo Scalzo in 2008 and 2009 they talk about and sample feature selection and group based feature selection where they try to use ensambles the power of ensembles to come up with better features, subsets and there's a PhD thesis.",
            "I just recently encountered by critic in 2008 where he also introduces stability.",
            "Measured and.",
            "You measure stability vendors, random perturbations."
        ],
        [
            "Training data.",
            "So how do you measure stability?",
            "So, so the way we measure stability is we think of 1 feature selection algorithm which produced us a subset of features which we call our one another feature selection algorithm produced us and their set of features are too and we say we measure the stability of a feature selection algorithm by comparing these two subsets.",
            "If these two subsets are two different from each other, than we say that the algorithm is not so stable.",
            "If not, we say that algorithm is stable.",
            "So how do you measure the difference to measure the difference, we treat these two sets of features using a bipartite graph, and in the bipartite graph the edges have weights.",
            "The weights show the similarity between features.",
            "And we want to find a maximal matching.",
            "On this bipartite graph, by maximum matching, I mean you could be matching these two features or these two features in the maximum matching value match the features you want to make sure that the sum of those edges are maximized and there are algorithms dating from 1950s that had been used to find maximum matching maximum matchings."
        ],
        [
            "Alright.",
            "For the maximum."
        ],
        [
            "Watching the need of a.",
            "To assess the the similarities between between features, we could say no.",
            "If you select the same feature in two subsets, your weight is 1, otherwise your weight is 0, But most features are correlated with each other.",
            "So you really want a non non bind."
        ],
        [
            "We measured and for that we use symmetric uncertainity.",
            "So the similarity within the maximum matching is the sum of the average of the weights of the.",
            "Of the features that you have matched and the weight between 2 features is defined as the symmetric uncertainty between features, where we measure the symmetric uncertainty as the information gain divided by the sum of the entropy."
        ],
        [
            "And there are other stability measures.",
            "I'm not going to go into this.",
            "This is also work that we want to follow up with.",
            "By the way, if there's anybody in the audience who is among the among the authors of the papers I'm citing, I would really love to meet with them after the talk.",
            "Or maybe will ask me easy question."
        ],
        [
            "Suzanne will meet that they also alright, so coming tomorrow, March before I go through this, we were really fascinated by Emily March for two reasons.",
            "Everybody has had been doing feature selection and everybody's, I think familiar with wrap around filter type methods.",
            "Wrapper methods are good.",
            "They give you very accurate results, but they're hard to implement 'cause you need to train a classifier through the features or into the features, train the classifier again.",
            "So so if you have a.",
            "Because file which is hard to train that it takes out of time, the filter methods like PCA for example.",
            "They are far fast, but they don't look at your labels.",
            "They don't look at the relevance of features so so they treat as if the labels don't exist and they may perform badly because of that.",
            "What am I ever does is it looks at the labels it assesses the relevance of features, but not by means of training a classifier, but by just measuring the mutual information between features and labels.",
            "So you can think of it as you know, I'm not reaching your classifier, but I'm just measuring how similar or how related two columns in my data set, namely the labels and features are and to assess the similarity between features.",
            "It again uses similar.",
            "Mutually in formation between 2 features.",
            "So I made a MoD, has relevance and redundancy, so given a feature subset.",
            "A22 quantities are defined on the subset.",
            "One of them is the redundancy, so the redundancy is the sum of the mutual information between each feature pairs.",
            "The average of those average of the average mutual information between each feature pair.",
            "And the relevance is.",
            "For each feature in my set, I measured the mutual information between the feature and the label, and I take the average.",
            "So if I have lots of.",
            "Relevant features this will be maximized if I have a features which are not related to be with each other.",
            "That one will be maximized."
        ],
        [
            "OK, so we want to have a feature subset which has as non redundant as possible and as relevant as possible.",
            "So we want to maximize the relevance and we want to minimize redundancy at the same time.",
            "How can we achieve that?",
            "I'm sure there are many many ways, but what have been done in MMR is they have they have used the mutual information quotient which is the.",
            "The ratio of relevance and divide by the redundancy and Michelin formation difference, which is the difference between them.",
            "So if you maximize the MEQ or if you maximize them ID then you are maximizing relevance and minimizing redundancy at the same time.",
            "So so actually over the Skype conversation with Li we decided why don't we add a.",
            "Term here, which lets US trade relevance and redundancy.",
            "Our whole goal was actually stability.",
            "We were hoping that if we if we put some parameter on relevant standard and see maybe we could be controlling stability also at the same time."
        ],
        [
            "OK, am I the is more stable than in my queue?",
            "That is, there have been work comparing MIDS and I'm actually in terms of accuracy, but in terms of stability, what we have seen here is mids is more stable than MIQ and theoretically it's easy to see for me for why they are stable.",
            "This assumes that we have measures of relevance V and redundancy W. Of course, we measure them over a finite subset.",
            "So let's say that there is the actual relevance an actual redundancy which we cannot measure, because if we had infinite amount of data we could measure, but we only have a subset of size N, so the VN that we measure is, let's say, V plus epsilon and WN that we measure is W plus Delta.",
            "And we assume with a grain of salt that maybe people from the audience who may object that.",
            "And I would like to listen to that those objections.",
            "But right now we assume that those two error terms are distributed normally with certain mean and variance term which depends on the number of samples.",
            "So the way is of MIDNMIQVN.",
            "Minus three VWN or VNB and divide by WN the variance of those terms is a direct indication of the stability of the feature selection criteria, because if you have more variance than it means you are going to be selecting features here and there.",
            "If you have less variance, it means you are going to be selecting features consistently.",
            "And then be able eat the various.",
            "This is good old match.",
            "I mean like college one or something like that.",
            "So the variance of mids is the variance of the two terms in it added.",
            "So it is two Sigma squared.",
            "But when we look at look at the variance of MIQ we couldn't evaluate it.",
            "And the reason why we could not evaluate it is 'cause if since you since you have a quotient at the denominator, if you have a term which is.",
            "Any non negligible mass around 0.",
            "Then you have the kosher component which means your ratio has.",
            "It has a mean and variance undefined, I mean and and the second moment is infinite so so not only you know, not only can we say it's more or less and we can't really even say it's more or less because we cannot actually evaluate it.",
            "OK, a friend of mine asked me what happens if the moment is not around 0, but we have."
        ],
        [
            "Address that one yet so.",
            "So we decided to actually just take bootstrap samples and evaluate the VMW.",
            "Didn't send redundancy and MIQ and mids on a data set.",
            "Here you see as a number of features in our subset changes over different boot steps are plans 50 actually Bootstrap.",
            "Once you see the.",
            "The relevance in dark blue redundancy in red and the MI two and MID in.",
            "Is it green?",
            "OK, I think.",
            "OK, great, I see so.",
            "So as you see, especially when, especially here when are W has some mess around 0 or am IQ is having really a huge variance.",
            "But when no W keeps having non negligible mass here and there, but in general the error bar on MIQ is much less than the error bar on mids OK."
        ],
        [
            "Alright, so so we we wanted to do this evaluation on a huge number of eight.",
            "8 is actually, I guess with enough number of datasets we got some datasets from UCI.",
            "We have the audio Journal data set which actually I talked about on Monday.",
            "It's the channel Texas data set and use of my student.",
            "My pages student obtain some 50 features.",
            "On that and.",
            "And these are the datasets that we are going to be using.",
            "As you see, we have.",
            "We have a variation in number of instances Anna variation in the number of features we have.",
            "We have large number of features as well as some small number of features in our."
        ],
        [
            "Because it's.",
            "Experimental setup how did we?",
            "How did you measure the stability and accuracy of our of our feature selection methods?",
            "So what we wanted to see was was the following.",
            "Up to now what people have done."
        ],
        [
            "It is.",
            "They said let's take a data set.",
            "I hope you guys everybody can see this from the back.",
            "I hope so.",
            "So what they said?",
            "Well, let's take a data set you know, let's produce the test test sets and training sets out of the datasets.",
            "And then they said on each training set.",
            "Let's take some bootstrap samples and then for each bootstrap sample, let's evaluate whether the features that we selected are stable or not OK.",
            "When you take the bootstrap samples as everybody knows, you take usually on the average you take about 67% of your data, so you are actually getting a fraction of your original data, but it's around 0.6% ratio is 0.6.",
            "But we wanted to see was what happens to my stability.",
            "Are stability as you have smaller and smaller datasets, so bootstrap may correspond to somewhere around here is 0.6.",
            "We want to see.",
            "What happens if I'm using only 10% of my training data?",
            "25% of my data or my whole training data and we measure we measure the stability between the features selected from this subset versus the feature selected from this subset.",
            "OK, so of course as you have less unless feed samples you expect your stability to decrease.",
            "But how does it decrease for different methods?",
            "I'll skip the this part.",
            "It's in the paper, but I think."
        ],
        [
            "I gave the basic idea, so we first evaluated MID&MIQ in terms of in terms of stability for different ratios of feature subsets and as the number of features changes.",
            "Ah, my dear.",
            "Has a. I'm sorry this is MI two MI Q has less stability than mids.",
            "OK and here we see the results for different ratios.",
            "0.1 zero point 25 Zero point 5 and 0.775.",
            "So MIDS is consistently more stable than MIT."
        ],
        [
            "And we went ahead and evaluated the stability and accuracy now of.",
            "Different EM ideas for different values of Alpha.",
            "So mids Alpha equals 0.5 corresponds to our good old MIT mids and MIT is actually not.",
            "It's not my institution anyway, so am ID equals one or 0.0 are the extreme cases and a market is shown in yellow.",
            "Again as you see MIQ is not performing well.",
            "Mids is performing better, but which Alpha value is better?",
            "And we didn't see much difference."
        ],
        [
            "In terms of the accuracy values.",
            "And this is the result for enter data set."
        ],
        [
            "Same conclusion.",
            "So so am I.",
            "This is more stable than MEQ.",
            "Accuses are similar, which value of Alpha gives us better results, so mids is better, but which which mids Alpha is better.",
            "So to solve that problem for each number of safety features we computed the mean rank of each method.",
            "So whoever is at the top, it means it's the best method.",
            "The first method, second or third method for each number of selected features and we just sum the rank and took the average so?"
        ],
        [
            "That way, if I curve if a method square is always at the top, is going to get rank one, but my thoughts will get up and go up and down as the number of features change so."
        ],
        [
            "So we wanted to get an average idea of who was."
        ],
        [
            "More stable and who was more accurate?",
            "And this shows the stability and accuracy of mids for different.",
            "Ratios in my in the in the sub samples in the training set and for different values of alphas.",
            "As you see, the smaller values means higher rank, so similar values means better perform.",
            "Ask version one data set Alpha equals 0.25 gave us better results if you were to use MIDS and originally for me would be getting these."
        ],
        [
            "Slots so we wanted to look at the stability and accuracy of all our datasets and we wanted to see whether there is a certain value of Alpha which seemed to be good for each data set an and we think that the different datasets.",
            "Perform better for different values of Alpha.",
            "So Foreiners fair 1.44 + 1 in general for for three of our datasets, Sonar Parkinson Mask version 1.25 resulted."
        ],
        [
            "Better stability an actual."
        ],
        [
            "Better accuracy and for the other datasets, Alpha values greater than 0.5 resulted in better."
        ],
        [
            "Stability and accuracy, and for most of the datasets there is a value of Alpha which resulted in the best stability and accuracy.",
            "So, so how do we decide on?",
            "You know how to trade off accuracy and stability for a data set is something that we are thinking about and we have some clues about.",
            "But we have."
        ],
        [
            "Yet concluded on that and conclusions, we divide the Metro to redistribute of a feature selection method as the size of the data set changes.",
            "So you are really forcing your feature selection limit if you just remember to its limits.",
            "Really, you are giving it very little data, and as it has very little data, you are seeing how it performs.",
            "And we showed EM ID is more stable than MIQ.",
            "We suggested an improvement on mids and for each data set there is a different Alpha.",
            "These are.",
            "These are different pieces of future work that we are planning to do.",
            "One of them is based on the characteristics of each data set based on the relevance and Daniel's values computed at each iteration.",
            "Different values of life and may be relevant.",
            "We have some indications on that, but we haven't yet concluded.",
            "We want to enable it to stability of others.",
            "Different feature selection methods and we want to compare the stability measures suggested by other authors because they use different measures.",
            "Really, I mean you don't have to use metric uncertainty.",
            "You can use Pearson correlation.",
            "You can use other measures of similarity and you can use other ways of."
        ],
        [
            "Stability definition.",
            "And that's it.",
            "I would like to thank you for listening.",
            "I would like to thank to be tagged for a scholarship to my student account to protect his like Turkish NSF.",
            "For those who don't know, and for a biometrics project for supporting me to my other two students.",
            "Petition you for support in many, many different ways, and you can email your questions or suggestions to me or to lay."
        ],
        [
            "Your account is doing his military duty, so I can't even reach him right now, so if you could let me know.",
            "And these are my quarters.",
            "And.",
            "These are my group.",
            "I have to advertise.",
            "I should actually put the Turkey picture that would have been much better, but I couldn't find anyone.",
            "Alright, any questions?",
            "Thank you so much."
        ],
        [
            "Yes please.",
            "Sorry.",
            "The measure I may be misunderstanding was doing.",
            "I think it's very attractive way of looking at feature selection, and as I understand it, if you do feature selection and one subset of data and then doing another subset of data good overlap between the feature sets and feel good about that, yes.",
            "Information theoretic measure.",
            "Yes yes.",
            "But in a situation with highly correlated features and say feature A1 is correlated with A2 and B1 is correlated with B2.",
            "A1B1 and A2B2 two different feature subsets?",
            "Overlap, but because of the information theoretic scoring that you're doing that they have a good stability score, is that what you want, is it?",
            "That's probably what is not happening, because if a if A1A2 and B1B2 are highly correlated, it's highly unlikely that they are going to be selected because the feature selection algorithms, like MMR, will usually either select a one or select a true either select P1 or select P2.",
            "And A and your stability measure, yes, would be high if you had that scenario, but you would probably not not have that scenario, but but I I understand your concern.",
            "I understand your concern, but you know, we, I guess we assume that we wouldn't be having correlated features.",
            "We have not taken that into account in our measures, but I would be.",
            "I would be happy to think about how to have to include it.",
            "This way using a simpler over overlap measure of stability and everything else would still work, but that makes sense.",
            "And overlap.",
            "I'm done, I'm coming so I have found out in the regional.",
            "01 OK, OK. And I I would.",
            "This correlation based or information based similarity of the features will give you higher estimated.",
            "Yeah, I'm actually there are two different questions here.",
            "Actually that question actually I'm working on that with you answer with my advisor, but I'll talk about that with your question like the way I understand it, but you are asking is if I have two correlated features here on two correlated features here, and if I select those two.",
            "My method will say you have a stable method and I think it will say you have a stable method and you don't want that.",
            "I don't know how to overcome that right now like as I speak I may know how to know how to do it in half an hour, but I don't know it right now.",
            "I would like to talk about what you are talking about.",
            "The mutual information, especially if you have small number of samples, is known to over estimate the.",
            "I mean it is known to be overestimated.",
            "So if you have a small number of samples you have a you have.",
            "Or estimation of mutual information.",
            "You can use Pearson correlation or you can use others nonparametric methods.",
            "That wasn't your question.",
            "I'm doing my next paper Cup.",
            "Mike I did OK good alright.",
            "Yes.",
            "People that.",
            "The main is Paris is not looking for it once like features, but many different things that are relevant in different because it is quite different aspects of the underlying problem and I see I see.",
            "If you look at your algorithm said they are not good for.",
            "Yes.",
            "Yes, yes.",
            "I mean I I haven't.",
            "I'm not doing that.",
            "But if you want to do something like that I would go.",
            "I would look at two different things.",
            "I would look at multi task learning talks that have been given and I actually like joy.",
            "Dips is Jody here.",
            "He didn't come to my talk.",
            "OK I would look at joydeep cautious clusterin samples where he suggests ways to.",
            "To combine different clusterings so so you would probably have different features selected and you want to combine them somehow for your task, but I would look into multi task learning.",
            "From what I understand.",
            "I see I mean you would obtain and with my method for each task, you would obtain a different feature set because you will have different labels for each task.",
            "But we could talk off and discussing yes.",
            "Thank you so much.",
            "It's an honor to be here, thank you.",
            "I said"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the table of can't.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about first.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give it, you know, two slide summary of what we are doing because this is where I'm going to get most of our attention anyway.",
                    "label": 0
                },
                {
                    "sent": "And then if you keep your attention, I'm going to talk about stability measure that we are going to be using in this work, then I'm going to talk about minimum redundancy, maximum relevance, which is the work of Christine and his colleagues do feature selection that we are going to be using and which we have used in a number of other studies and found to be quite successful.",
                    "label": 0
                },
                {
                    "sent": "Then MMR has two different criteria to select features in my DNA molecule.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about the stability and accuracy of those two different criteria and will also include introduce an adage criteria, mids, Alpha which seems to give us some.",
                    "label": 1
                },
                {
                    "sent": "Balance some tradeoff between stability and accuracy.",
                    "label": 0
                },
                {
                    "sent": "Then I'll talk about experimental results on eight different datasets and can.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Put the papers.",
                    "label": 0
                },
                {
                    "sent": "So we all want our feature selection selection methods to be accurate.",
                    "label": 1
                },
                {
                    "sent": "You know whether we want to have better accuracy than what we have or we want to have less features and still the accuracy that we are having.",
                    "label": 0
                },
                {
                    "sent": "We do want to get rid of noisy, irrelevant or redundant features and have a good accuracy.",
                    "label": 1
                },
                {
                    "sent": "There is another way to look at feature selection.",
                    "label": 0
                },
                {
                    "sent": "We can also ask how stable the feature selection algorithm is and by stability we mean if there are small changes in the identity of training samples.",
                    "label": 1
                },
                {
                    "sent": "Do the features selected change a lot or not?",
                    "label": 0
                },
                {
                    "sent": "And this is especially important if you have a large number of features and you do want to know which features you should be acquiring.",
                    "label": 0
                },
                {
                    "sent": "Or which features you should be naming as important.",
                    "label": 1
                },
                {
                    "sent": "For example, in microdata analysis, which features are important for?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Biomarker identification and what we do in this work is we introduce a method to evaluate stability, feature selection method as the number of training samples decrease and we compare this table 10 accuracy of MI dynamic variance of MMR method and we do this spot theoretically and experimentally.",
                    "label": 1
                },
                {
                    "sent": "We introduced them ideal for Metroid which gives us a tradeoff between relevance and redundancy and then for different datasets.",
                    "label": 0
                },
                {
                    "sent": "We show that mids is.",
                    "label": 1
                },
                {
                    "sent": "More stable than immature and then be sure that different values of Alpha should be chosen.",
                    "label": 0
                },
                {
                    "sent": "If you want better stability and accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I have this is the.",
                    "label": 0
                },
                {
                    "sent": "Somebody that has been work on stability measures before kulo SIS evaluated different stability measures, different sensibilities of different feature selection methods, and he showed that although some feature selection methods have the similar accuracies, they have huge variations in their stabilities.",
                    "label": 1
                },
                {
                    "sent": "That's what we also see in our datasets, and there are two work by.",
                    "label": 0
                },
                {
                    "sent": "On work by says ET al.",
                    "label": 0
                },
                {
                    "sent": "2008 another one you Dingo Scalzo in 2008 and 2009 they talk about and sample feature selection and group based feature selection where they try to use ensambles the power of ensembles to come up with better features, subsets and there's a PhD thesis.",
                    "label": 0
                },
                {
                    "sent": "I just recently encountered by critic in 2008 where he also introduces stability.",
                    "label": 0
                },
                {
                    "sent": "Measured and.",
                    "label": 1
                },
                {
                    "sent": "You measure stability vendors, random perturbations.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Training data.",
                    "label": 0
                },
                {
                    "sent": "So how do you measure stability?",
                    "label": 0
                },
                {
                    "sent": "So, so the way we measure stability is we think of 1 feature selection algorithm which produced us a subset of features which we call our one another feature selection algorithm produced us and their set of features are too and we say we measure the stability of a feature selection algorithm by comparing these two subsets.",
                    "label": 1
                },
                {
                    "sent": "If these two subsets are two different from each other, than we say that the algorithm is not so stable.",
                    "label": 1
                },
                {
                    "sent": "If not, we say that algorithm is stable.",
                    "label": 0
                },
                {
                    "sent": "So how do you measure the difference to measure the difference, we treat these two sets of features using a bipartite graph, and in the bipartite graph the edges have weights.",
                    "label": 1
                },
                {
                    "sent": "The weights show the similarity between features.",
                    "label": 0
                },
                {
                    "sent": "And we want to find a maximal matching.",
                    "label": 0
                },
                {
                    "sent": "On this bipartite graph, by maximum matching, I mean you could be matching these two features or these two features in the maximum matching value match the features you want to make sure that the sum of those edges are maximized and there are algorithms dating from 1950s that had been used to find maximum matching maximum matchings.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "For the maximum.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Watching the need of a.",
                    "label": 0
                },
                {
                    "sent": "To assess the the similarities between between features, we could say no.",
                    "label": 0
                },
                {
                    "sent": "If you select the same feature in two subsets, your weight is 1, otherwise your weight is 0, But most features are correlated with each other.",
                    "label": 0
                },
                {
                    "sent": "So you really want a non non bind.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We measured and for that we use symmetric uncertainity.",
                    "label": 1
                },
                {
                    "sent": "So the similarity within the maximum matching is the sum of the average of the weights of the.",
                    "label": 1
                },
                {
                    "sent": "Of the features that you have matched and the weight between 2 features is defined as the symmetric uncertainty between features, where we measure the symmetric uncertainty as the information gain divided by the sum of the entropy.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there are other stability measures.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into this.",
                    "label": 0
                },
                {
                    "sent": "This is also work that we want to follow up with.",
                    "label": 0
                },
                {
                    "sent": "By the way, if there's anybody in the audience who is among the among the authors of the papers I'm citing, I would really love to meet with them after the talk.",
                    "label": 0
                },
                {
                    "sent": "Or maybe will ask me easy question.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Suzanne will meet that they also alright, so coming tomorrow, March before I go through this, we were really fascinated by Emily March for two reasons.",
                    "label": 0
                },
                {
                    "sent": "Everybody has had been doing feature selection and everybody's, I think familiar with wrap around filter type methods.",
                    "label": 1
                },
                {
                    "sent": "Wrapper methods are good.",
                    "label": 0
                },
                {
                    "sent": "They give you very accurate results, but they're hard to implement 'cause you need to train a classifier through the features or into the features, train the classifier again.",
                    "label": 0
                },
                {
                    "sent": "So so if you have a.",
                    "label": 0
                },
                {
                    "sent": "Because file which is hard to train that it takes out of time, the filter methods like PCA for example.",
                    "label": 0
                },
                {
                    "sent": "They are far fast, but they don't look at your labels.",
                    "label": 0
                },
                {
                    "sent": "They don't look at the relevance of features so so they treat as if the labels don't exist and they may perform badly because of that.",
                    "label": 0
                },
                {
                    "sent": "What am I ever does is it looks at the labels it assesses the relevance of features, but not by means of training a classifier, but by just measuring the mutual information between features and labels.",
                    "label": 0
                },
                {
                    "sent": "So you can think of it as you know, I'm not reaching your classifier, but I'm just measuring how similar or how related two columns in my data set, namely the labels and features are and to assess the similarity between features.",
                    "label": 0
                },
                {
                    "sent": "It again uses similar.",
                    "label": 0
                },
                {
                    "sent": "Mutually in formation between 2 features.",
                    "label": 0
                },
                {
                    "sent": "So I made a MoD, has relevance and redundancy, so given a feature subset.",
                    "label": 1
                },
                {
                    "sent": "A22 quantities are defined on the subset.",
                    "label": 0
                },
                {
                    "sent": "One of them is the redundancy, so the redundancy is the sum of the mutual information between each feature pairs.",
                    "label": 1
                },
                {
                    "sent": "The average of those average of the average mutual information between each feature pair.",
                    "label": 0
                },
                {
                    "sent": "And the relevance is.",
                    "label": 0
                },
                {
                    "sent": "For each feature in my set, I measured the mutual information between the feature and the label, and I take the average.",
                    "label": 0
                },
                {
                    "sent": "So if I have lots of.",
                    "label": 0
                },
                {
                    "sent": "Relevant features this will be maximized if I have a features which are not related to be with each other.",
                    "label": 0
                },
                {
                    "sent": "That one will be maximized.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we want to have a feature subset which has as non redundant as possible and as relevant as possible.",
                    "label": 0
                },
                {
                    "sent": "So we want to maximize the relevance and we want to minimize redundancy at the same time.",
                    "label": 0
                },
                {
                    "sent": "How can we achieve that?",
                    "label": 0
                },
                {
                    "sent": "I'm sure there are many many ways, but what have been done in MMR is they have they have used the mutual information quotient which is the.",
                    "label": 0
                },
                {
                    "sent": "The ratio of relevance and divide by the redundancy and Michelin formation difference, which is the difference between them.",
                    "label": 0
                },
                {
                    "sent": "So if you maximize the MEQ or if you maximize them ID then you are maximizing relevance and minimizing redundancy at the same time.",
                    "label": 0
                },
                {
                    "sent": "So so actually over the Skype conversation with Li we decided why don't we add a.",
                    "label": 0
                },
                {
                    "sent": "Term here, which lets US trade relevance and redundancy.",
                    "label": 0
                },
                {
                    "sent": "Our whole goal was actually stability.",
                    "label": 0
                },
                {
                    "sent": "We were hoping that if we if we put some parameter on relevant standard and see maybe we could be controlling stability also at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, am I the is more stable than in my queue?",
                    "label": 0
                },
                {
                    "sent": "That is, there have been work comparing MIDS and I'm actually in terms of accuracy, but in terms of stability, what we have seen here is mids is more stable than MIQ and theoretically it's easy to see for me for why they are stable.",
                    "label": 0
                },
                {
                    "sent": "This assumes that we have measures of relevance V and redundancy W. Of course, we measure them over a finite subset.",
                    "label": 0
                },
                {
                    "sent": "So let's say that there is the actual relevance an actual redundancy which we cannot measure, because if we had infinite amount of data we could measure, but we only have a subset of size N, so the VN that we measure is, let's say, V plus epsilon and WN that we measure is W plus Delta.",
                    "label": 0
                },
                {
                    "sent": "And we assume with a grain of salt that maybe people from the audience who may object that.",
                    "label": 0
                },
                {
                    "sent": "And I would like to listen to that those objections.",
                    "label": 0
                },
                {
                    "sent": "But right now we assume that those two error terms are distributed normally with certain mean and variance term which depends on the number of samples.",
                    "label": 0
                },
                {
                    "sent": "So the way is of MIDNMIQVN.",
                    "label": 0
                },
                {
                    "sent": "Minus three VWN or VNB and divide by WN the variance of those terms is a direct indication of the stability of the feature selection criteria, because if you have more variance than it means you are going to be selecting features here and there.",
                    "label": 1
                },
                {
                    "sent": "If you have less variance, it means you are going to be selecting features consistently.",
                    "label": 0
                },
                {
                    "sent": "And then be able eat the various.",
                    "label": 0
                },
                {
                    "sent": "This is good old match.",
                    "label": 0
                },
                {
                    "sent": "I mean like college one or something like that.",
                    "label": 0
                },
                {
                    "sent": "So the variance of mids is the variance of the two terms in it added.",
                    "label": 0
                },
                {
                    "sent": "So it is two Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "But when we look at look at the variance of MIQ we couldn't evaluate it.",
                    "label": 0
                },
                {
                    "sent": "And the reason why we could not evaluate it is 'cause if since you since you have a quotient at the denominator, if you have a term which is.",
                    "label": 1
                },
                {
                    "sent": "Any non negligible mass around 0.",
                    "label": 1
                },
                {
                    "sent": "Then you have the kosher component which means your ratio has.",
                    "label": 1
                },
                {
                    "sent": "It has a mean and variance undefined, I mean and and the second moment is infinite so so not only you know, not only can we say it's more or less and we can't really even say it's more or less because we cannot actually evaluate it.",
                    "label": 0
                },
                {
                    "sent": "OK, a friend of mine asked me what happens if the moment is not around 0, but we have.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Address that one yet so.",
                    "label": 0
                },
                {
                    "sent": "So we decided to actually just take bootstrap samples and evaluate the VMW.",
                    "label": 0
                },
                {
                    "sent": "Didn't send redundancy and MIQ and mids on a data set.",
                    "label": 0
                },
                {
                    "sent": "Here you see as a number of features in our subset changes over different boot steps are plans 50 actually Bootstrap.",
                    "label": 0
                },
                {
                    "sent": "Once you see the.",
                    "label": 0
                },
                {
                    "sent": "The relevance in dark blue redundancy in red and the MI two and MID in.",
                    "label": 0
                },
                {
                    "sent": "Is it green?",
                    "label": 0
                },
                {
                    "sent": "OK, I think.",
                    "label": 0
                },
                {
                    "sent": "OK, great, I see so.",
                    "label": 0
                },
                {
                    "sent": "So as you see, especially when, especially here when are W has some mess around 0 or am IQ is having really a huge variance.",
                    "label": 0
                },
                {
                    "sent": "But when no W keeps having non negligible mass here and there, but in general the error bar on MIQ is much less than the error bar on mids OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so so we we wanted to do this evaluation on a huge number of eight.",
                    "label": 0
                },
                {
                    "sent": "8 is actually, I guess with enough number of datasets we got some datasets from UCI.",
                    "label": 0
                },
                {
                    "sent": "We have the audio Journal data set which actually I talked about on Monday.",
                    "label": 0
                },
                {
                    "sent": "It's the channel Texas data set and use of my student.",
                    "label": 0
                },
                {
                    "sent": "My pages student obtain some 50 features.",
                    "label": 0
                },
                {
                    "sent": "On that and.",
                    "label": 0
                },
                {
                    "sent": "And these are the datasets that we are going to be using.",
                    "label": 0
                },
                {
                    "sent": "As you see, we have.",
                    "label": 0
                },
                {
                    "sent": "We have a variation in number of instances Anna variation in the number of features we have.",
                    "label": 0
                },
                {
                    "sent": "We have large number of features as well as some small number of features in our.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because it's.",
                    "label": 0
                },
                {
                    "sent": "Experimental setup how did we?",
                    "label": 1
                },
                {
                    "sent": "How did you measure the stability and accuracy of our of our feature selection methods?",
                    "label": 1
                },
                {
                    "sent": "So what we wanted to see was was the following.",
                    "label": 0
                },
                {
                    "sent": "Up to now what people have done.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "They said let's take a data set.",
                    "label": 0
                },
                {
                    "sent": "I hope you guys everybody can see this from the back.",
                    "label": 0
                },
                {
                    "sent": "I hope so.",
                    "label": 0
                },
                {
                    "sent": "So what they said?",
                    "label": 0
                },
                {
                    "sent": "Well, let's take a data set you know, let's produce the test test sets and training sets out of the datasets.",
                    "label": 0
                },
                {
                    "sent": "And then they said on each training set.",
                    "label": 0
                },
                {
                    "sent": "Let's take some bootstrap samples and then for each bootstrap sample, let's evaluate whether the features that we selected are stable or not OK.",
                    "label": 0
                },
                {
                    "sent": "When you take the bootstrap samples as everybody knows, you take usually on the average you take about 67% of your data, so you are actually getting a fraction of your original data, but it's around 0.6% ratio is 0.6.",
                    "label": 0
                },
                {
                    "sent": "But we wanted to see was what happens to my stability.",
                    "label": 0
                },
                {
                    "sent": "Are stability as you have smaller and smaller datasets, so bootstrap may correspond to somewhere around here is 0.6.",
                    "label": 0
                },
                {
                    "sent": "We want to see.",
                    "label": 0
                },
                {
                    "sent": "What happens if I'm using only 10% of my training data?",
                    "label": 0
                },
                {
                    "sent": "25% of my data or my whole training data and we measure we measure the stability between the features selected from this subset versus the feature selected from this subset.",
                    "label": 0
                },
                {
                    "sent": "OK, so of course as you have less unless feed samples you expect your stability to decrease.",
                    "label": 0
                },
                {
                    "sent": "But how does it decrease for different methods?",
                    "label": 0
                },
                {
                    "sent": "I'll skip the this part.",
                    "label": 0
                },
                {
                    "sent": "It's in the paper, but I think.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I gave the basic idea, so we first evaluated MID&MIQ in terms of in terms of stability for different ratios of feature subsets and as the number of features changes.",
                    "label": 0
                },
                {
                    "sent": "Ah, my dear.",
                    "label": 0
                },
                {
                    "sent": "Has a. I'm sorry this is MI two MI Q has less stability than mids.",
                    "label": 0
                },
                {
                    "sent": "OK and here we see the results for different ratios.",
                    "label": 0
                },
                {
                    "sent": "0.1 zero point 25 Zero point 5 and 0.775.",
                    "label": 0
                },
                {
                    "sent": "So MIDS is consistently more stable than MIT.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we went ahead and evaluated the stability and accuracy now of.",
                    "label": 0
                },
                {
                    "sent": "Different EM ideas for different values of Alpha.",
                    "label": 0
                },
                {
                    "sent": "So mids Alpha equals 0.5 corresponds to our good old MIT mids and MIT is actually not.",
                    "label": 0
                },
                {
                    "sent": "It's not my institution anyway, so am ID equals one or 0.0 are the extreme cases and a market is shown in yellow.",
                    "label": 0
                },
                {
                    "sent": "Again as you see MIQ is not performing well.",
                    "label": 0
                },
                {
                    "sent": "Mids is performing better, but which Alpha value is better?",
                    "label": 0
                },
                {
                    "sent": "And we didn't see much difference.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of the accuracy values.",
                    "label": 0
                },
                {
                    "sent": "And this is the result for enter data set.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Same conclusion.",
                    "label": 0
                },
                {
                    "sent": "So so am I.",
                    "label": 0
                },
                {
                    "sent": "This is more stable than MEQ.",
                    "label": 1
                },
                {
                    "sent": "Accuses are similar, which value of Alpha gives us better results, so mids is better, but which which mids Alpha is better.",
                    "label": 0
                },
                {
                    "sent": "So to solve that problem for each number of safety features we computed the mean rank of each method.",
                    "label": 1
                },
                {
                    "sent": "So whoever is at the top, it means it's the best method.",
                    "label": 0
                },
                {
                    "sent": "The first method, second or third method for each number of selected features and we just sum the rank and took the average so?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That way, if I curve if a method square is always at the top, is going to get rank one, but my thoughts will get up and go up and down as the number of features change so.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we wanted to get an average idea of who was.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More stable and who was more accurate?",
                    "label": 0
                },
                {
                    "sent": "And this shows the stability and accuracy of mids for different.",
                    "label": 0
                },
                {
                    "sent": "Ratios in my in the in the sub samples in the training set and for different values of alphas.",
                    "label": 0
                },
                {
                    "sent": "As you see, the smaller values means higher rank, so similar values means better perform.",
                    "label": 0
                },
                {
                    "sent": "Ask version one data set Alpha equals 0.25 gave us better results if you were to use MIDS and originally for me would be getting these.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slots so we wanted to look at the stability and accuracy of all our datasets and we wanted to see whether there is a certain value of Alpha which seemed to be good for each data set an and we think that the different datasets.",
                    "label": 0
                },
                {
                    "sent": "Perform better for different values of Alpha.",
                    "label": 0
                },
                {
                    "sent": "So Foreiners fair 1.44 + 1 in general for for three of our datasets, Sonar Parkinson Mask version 1.25 resulted.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better stability an actual.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better accuracy and for the other datasets, Alpha values greater than 0.5 resulted in better.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stability and accuracy, and for most of the datasets there is a value of Alpha which resulted in the best stability and accuracy.",
                    "label": 0
                },
                {
                    "sent": "So, so how do we decide on?",
                    "label": 0
                },
                {
                    "sent": "You know how to trade off accuracy and stability for a data set is something that we are thinking about and we have some clues about.",
                    "label": 0
                },
                {
                    "sent": "But we have.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yet concluded on that and conclusions, we divide the Metro to redistribute of a feature selection method as the size of the data set changes.",
                    "label": 1
                },
                {
                    "sent": "So you are really forcing your feature selection limit if you just remember to its limits.",
                    "label": 0
                },
                {
                    "sent": "Really, you are giving it very little data, and as it has very little data, you are seeing how it performs.",
                    "label": 1
                },
                {
                    "sent": "And we showed EM ID is more stable than MIQ.",
                    "label": 0
                },
                {
                    "sent": "We suggested an improvement on mids and for each data set there is a different Alpha.",
                    "label": 1
                },
                {
                    "sent": "These are.",
                    "label": 0
                },
                {
                    "sent": "These are different pieces of future work that we are planning to do.",
                    "label": 1
                },
                {
                    "sent": "One of them is based on the characteristics of each data set based on the relevance and Daniel's values computed at each iteration.",
                    "label": 0
                },
                {
                    "sent": "Different values of life and may be relevant.",
                    "label": 1
                },
                {
                    "sent": "We have some indications on that, but we haven't yet concluded.",
                    "label": 0
                },
                {
                    "sent": "We want to enable it to stability of others.",
                    "label": 0
                },
                {
                    "sent": "Different feature selection methods and we want to compare the stability measures suggested by other authors because they use different measures.",
                    "label": 0
                },
                {
                    "sent": "Really, I mean you don't have to use metric uncertainty.",
                    "label": 0
                },
                {
                    "sent": "You can use Pearson correlation.",
                    "label": 0
                },
                {
                    "sent": "You can use other measures of similarity and you can use other ways of.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stability definition.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "I would like to thank you for listening.",
                    "label": 1
                },
                {
                    "sent": "I would like to thank to be tagged for a scholarship to my student account to protect his like Turkish NSF.",
                    "label": 0
                },
                {
                    "sent": "For those who don't know, and for a biometrics project for supporting me to my other two students.",
                    "label": 1
                },
                {
                    "sent": "Petition you for support in many, many different ways, and you can email your questions or suggestions to me or to lay.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your account is doing his military duty, so I can't even reach him right now, so if you could let me know.",
                    "label": 0
                },
                {
                    "sent": "And these are my quarters.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "These are my group.",
                    "label": 0
                },
                {
                    "sent": "I have to advertise.",
                    "label": 0
                },
                {
                    "sent": "I should actually put the Turkey picture that would have been much better, but I couldn't find anyone.",
                    "label": 0
                },
                {
                    "sent": "Alright, any questions?",
                    "label": 0
                },
                {
                    "sent": "Thank you so much.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes please.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "The measure I may be misunderstanding was doing.",
                    "label": 0
                },
                {
                    "sent": "I think it's very attractive way of looking at feature selection, and as I understand it, if you do feature selection and one subset of data and then doing another subset of data good overlap between the feature sets and feel good about that, yes.",
                    "label": 0
                },
                {
                    "sent": "Information theoretic measure.",
                    "label": 0
                },
                {
                    "sent": "Yes yes.",
                    "label": 0
                },
                {
                    "sent": "But in a situation with highly correlated features and say feature A1 is correlated with A2 and B1 is correlated with B2.",
                    "label": 0
                },
                {
                    "sent": "A1B1 and A2B2 two different feature subsets?",
                    "label": 0
                },
                {
                    "sent": "Overlap, but because of the information theoretic scoring that you're doing that they have a good stability score, is that what you want, is it?",
                    "label": 0
                },
                {
                    "sent": "That's probably what is not happening, because if a if A1A2 and B1B2 are highly correlated, it's highly unlikely that they are going to be selected because the feature selection algorithms, like MMR, will usually either select a one or select a true either select P1 or select P2.",
                    "label": 0
                },
                {
                    "sent": "And A and your stability measure, yes, would be high if you had that scenario, but you would probably not not have that scenario, but but I I understand your concern.",
                    "label": 0
                },
                {
                    "sent": "I understand your concern, but you know, we, I guess we assume that we wouldn't be having correlated features.",
                    "label": 0
                },
                {
                    "sent": "We have not taken that into account in our measures, but I would be.",
                    "label": 0
                },
                {
                    "sent": "I would be happy to think about how to have to include it.",
                    "label": 0
                },
                {
                    "sent": "This way using a simpler over overlap measure of stability and everything else would still work, but that makes sense.",
                    "label": 0
                },
                {
                    "sent": "And overlap.",
                    "label": 0
                },
                {
                    "sent": "I'm done, I'm coming so I have found out in the regional.",
                    "label": 0
                },
                {
                    "sent": "01 OK, OK. And I I would.",
                    "label": 0
                },
                {
                    "sent": "This correlation based or information based similarity of the features will give you higher estimated.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm actually there are two different questions here.",
                    "label": 0
                },
                {
                    "sent": "Actually that question actually I'm working on that with you answer with my advisor, but I'll talk about that with your question like the way I understand it, but you are asking is if I have two correlated features here on two correlated features here, and if I select those two.",
                    "label": 0
                },
                {
                    "sent": "My method will say you have a stable method and I think it will say you have a stable method and you don't want that.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to overcome that right now like as I speak I may know how to know how to do it in half an hour, but I don't know it right now.",
                    "label": 0
                },
                {
                    "sent": "I would like to talk about what you are talking about.",
                    "label": 0
                },
                {
                    "sent": "The mutual information, especially if you have small number of samples, is known to over estimate the.",
                    "label": 0
                },
                {
                    "sent": "I mean it is known to be overestimated.",
                    "label": 0
                },
                {
                    "sent": "So if you have a small number of samples you have a you have.",
                    "label": 0
                },
                {
                    "sent": "Or estimation of mutual information.",
                    "label": 0
                },
                {
                    "sent": "You can use Pearson correlation or you can use others nonparametric methods.",
                    "label": 0
                },
                {
                    "sent": "That wasn't your question.",
                    "label": 0
                },
                {
                    "sent": "I'm doing my next paper Cup.",
                    "label": 0
                },
                {
                    "sent": "Mike I did OK good alright.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "People that.",
                    "label": 0
                },
                {
                    "sent": "The main is Paris is not looking for it once like features, but many different things that are relevant in different because it is quite different aspects of the underlying problem and I see I see.",
                    "label": 0
                },
                {
                    "sent": "If you look at your algorithm said they are not good for.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "I mean I I haven't.",
                    "label": 0
                },
                {
                    "sent": "I'm not doing that.",
                    "label": 0
                },
                {
                    "sent": "But if you want to do something like that I would go.",
                    "label": 0
                },
                {
                    "sent": "I would look at two different things.",
                    "label": 0
                },
                {
                    "sent": "I would look at multi task learning talks that have been given and I actually like joy.",
                    "label": 0
                },
                {
                    "sent": "Dips is Jody here.",
                    "label": 0
                },
                {
                    "sent": "He didn't come to my talk.",
                    "label": 0
                },
                {
                    "sent": "OK I would look at joydeep cautious clusterin samples where he suggests ways to.",
                    "label": 0
                },
                {
                    "sent": "To combine different clusterings so so you would probably have different features selected and you want to combine them somehow for your task, but I would look into multi task learning.",
                    "label": 0
                },
                {
                    "sent": "From what I understand.",
                    "label": 0
                },
                {
                    "sent": "I see I mean you would obtain and with my method for each task, you would obtain a different feature set because you will have different labels for each task.",
                    "label": 0
                },
                {
                    "sent": "But we could talk off and discussing yes.",
                    "label": 0
                },
                {
                    "sent": "Thank you so much.",
                    "label": 0
                },
                {
                    "sent": "It's an honor to be here, thank you.",
                    "label": 0
                },
                {
                    "sent": "I said",
                    "label": 0
                }
            ]
        }
    }
}