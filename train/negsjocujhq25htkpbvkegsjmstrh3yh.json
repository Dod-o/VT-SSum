{
    "id": "negsjocujhq25htkpbvkegsjmstrh3yh",
    "title": "The Geometry of Losses",
    "info": {
        "author": [
            "Robert C. Williamson, Australian National University"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_williamson_geometry/",
    "segmentation": [
        [
            "Machine learning is always done for a purpose, and I will argue that this means that loss functions."
        ],
        [
            "Matter, even if you think that all you're doing is getting information from your data.",
            "Ultimately, you're going to use the information.",
            "Think, use utility and there will be a loss function.",
            "So loss functions are central to machine learning.",
            "Now I'm only going to talk about loss functions for a problem where the labels come from the discrete set one up to N and your job is to estimate a probability.",
            "Of course you can have more general problems than that.",
            "Now I would claim that machine learning only pays lip service to losses, see flip.",
            "And why is that?",
            "Because normally you look at pretty well.",
            "Any paper in machine learning when they talk about a loss function, it's just that function ill, right?",
            "You say, here's the laussel.",
            "Now we get on and do the interesting stuff."
        ],
        [
            "You might assume that the function is convex or Lipschitz, but beyond that you won't see much, so there's no structure to the loss functions.",
            "There's no principles about how you go about designing them.",
            "There's no guidance to a practitioner about what you should know which loss function you should use, and there's no real understanding of the implications of the choice of the loss function, so it's a bit of a character, sure, but I'm making my point.",
            "There is however a theory of loss functions.",
            "Which I'll be."
        ],
        [
            "Only talk about now, so remind you the loss on talking about.",
            "It's a real valued function that is in curd.",
            "When you predict with probability P and event I occurs now it's very convenient instead to view that loss in this equivalent way.",
            "Right?",
            "All I've done there is to stack up those partial functions into a vector.",
            "So now an from now on I will think of the loss L as a map from the simplex.",
            "The in dimensional simplex.",
            "To a vector of positive real numbers.",
            "That's what a loss does.",
            "OK, it's a heck of a lot easier to think about functions of 1 argument then of two arguments.",
            "And I will use this notion of proper loss is a loss is proper if when you draw the labels from some underlying distribution, then predicting with that same distribution minimizes the loss.",
            "OK, so that's written there in symbols.",
            "That's what a proper loss means.",
            "Write an improper loft.",
            "Loss would be Daft, right?",
            "I mean you would be.",
            "You'd be penalized for getting things right.",
            "So I'll introduce a little bit of terminology now that you talk about the loss set.",
            "Search the image of the simplex under the loss, and I believe it was larger introduced this idea of the Super prediction set, so you'll see."
        ],
        [
            "On this graph here.",
            "So the black curve see here.",
            "This is the last set.",
            "It's the image here.",
            "I've used a more general prediction space, but think of that as a simplex.",
            "The axes here other partial losses.",
            "So in this case N = 2 and all of the pictures will be for N = 2.",
            "But the theory holds more generally, and the Super prediction said is this yellow set here, which goes off to Infinity.",
            "Alright, I can't draw it off to Infinity, but you think of this as a set going off to Infinity, and it's the fact that every proper loss induces a convex.",
            "Super prediction set.",
            "Furthermore, you can argue that if you have a loss whose super prediction set is not convex, then it's inadmissible in that you would never want to use such a loss because you could always do better by using a loss that was the convex Hull of it.",
            "I'm not going to prove these facts, but just leave me.",
            "Furthermore, if you have a loss with a super prediction set that is convex, you can always re parameterize it so that it is a proper loss.",
            "Combine those three facts together and you can claim that there's nothing to lose by only considering convex super prediction sets.",
            "OK. Got a used car for sale too.",
            "Believe me right but this is true."
        ],
        [
            "And it allows us to now just restrict ourselves to these convex super prediction sets.",
            "So the goal of the talk is I want to understand loss is better and I want to just take a simple change of perspective.",
            "What if we start with that set and go back to the losses right?",
            "Sounds like it's trivial, but it turns out there's some interesting things that comes out of that.",
            "It allows you to build an algebra of losses, that's novel.",
            "It allows you to develop a new thing, which I've called an inverse loss, and I don't.",
            "I don't see that you would have come up with that idea without coming from this geometrical perspective.",
            "So I use some."
        ],
        [
            "Basic ideas in convex analysis are not going to teach you this stuff here.",
            "The key point I'm going to rapidly go through here is that many of the notions that are used in convex analysis.",
            "There's always a concave version of them as well.",
            "But to start with we use Minkowski sums, the definitions there we talk about the recession cone of a set.",
            "You see this set here, C, The recession cone.",
            "This 0 + 3.",
            "That's the set of directions from the origin that you can keep going off to Infinity and stay within the set.",
            "OK."
        ],
        [
            "You have the sub differential for convex functions.",
            "Of course there's a super differential.",
            "For concave functions.",
            "You note the notation.",
            "There's a little V on top, it's for the convex case it's a hat, it's a concave case.",
            "It's just convenient to do it this way because the natural objects that you get for losses end up being concave, right?",
            "I mean if one could talk about gains instead of losses, you could stick with the convex situation."
        ],
        [
            "These sets so the Super prediction sets look like the one at the bottom, and this terminology is introduced by Rockefeller.",
            "This is a set of positive type, right?",
            "So what's that mean?",
            "It's recession cone is the positive.",
            "Often you can head off any direction to the North East and stay in the set, and the origin is not within the set.",
            "And there's also the complementary thing which is of negative type.",
            "It's a little odd that it's asymmetric like this, right?",
            "But you'll see why that is right.",
            "The origins, not in the set here.",
            "It is here the key type that we will work with for losses is the sets of positive type."
        ],
        [
            "It's well known that convex sets are characterized by their support functions.",
            "This is this definition of the support function, the normal definition, and that support function is a convex function.",
            "There's an equivalent definition you could use written there, which is the concave support functions.",
            "This will also characterize the sets, but this is a concave function and this is the natural one to work with, so these concave support functions are closed there, one homogeneous.",
            "I'll give you the definition later if you don't know what that is and the concave support function is indeed concave and the key.",
            "A key thing is that there's a one to one relationship.",
            "If you know this, support funk."
        ],
        [
            "And you know the convex set and these support functions, it's about supporting hyperplanes, so you would have all met this before.",
            "There is this notion of the support set right?",
            "The support set is simply the point in the set of interest which is supported by the supporting hyperplane.",
            "So that's shown in the picture there.",
            "So keep that in mind.",
            "You might not have.",
            "Come across the name for that before, but the idea of it is is obvious, and the interesting fact that I've is important for the rest of the talk is that it turns out that the support set is.",
            "Here is the Super differential of the concave support function.",
            "The support set is the Super differential of the concave support function, so it's universally true."
        ],
        [
            "The last bit of machinery is pohlers engages, so engaged is like a norm.",
            "So if you have any set like this, these are called star shaped sets.",
            "But obviously if it's convex it's going to work.",
            "It holds more generally the gauge measures how far out you can go till you leave the set right.",
            "So think of and think of a norm ball, right?",
            "What the norm does.",
            "Normals are usually the norm is 1 and you transcribe the locus.",
            "There's a concave gauge, right?",
            "So this is perhaps less well known, right?",
            "You won't find many papers in the literature on this, but it's a similar idea.",
            "Here's my set C and I can describe the set in terms of how far I can go along those Rays until I hit it right?",
            "So that's another way of capturing what the set does."
        ],
        [
            "Once you've got these sets, there's a fundamental operation called the polar right, so the polar can be described in this fashion.",
            "This is the normal convex case, so note this notation.",
            "It's got this little circle, and there's a V in it that reminds you it's a convex case.",
            "This is the sublevel set at level one of the support function, so all of these operations are very simple.",
            "This is a standard thing I will give you some examples of these polls later on.",
            "And a key property of these poles is that the gauge of a set is equal to the support function of the polar, and likewise the gauge of the polar is the support function of the set.",
            "This is all classical."
        ],
        [
            "And that works also for the concave case.",
            "Alright, you see, the only difference between these two slides is that there's hats versus these, right?",
            "So that's important gauges and pollers, so gauges and support functions via apology reality."
        ],
        [
            "And there is equivalently an operation on gauges, so you can take the gauge of a set.",
            "You can do an operation, so that's now a function.",
            "So there's this operation you can do on that function, and it has the property that if you do that operation on the function, so there's an overload of the notation here on the left hand side, you're applying an operation on a function taking the polar of the gauge, and that is the gauge of the polar.",
            "So this is all kind of classical stuff scattered around the place, but you'll see why I will use that for shortly, and there's a concave version.",
            "It's exactly the same, it's become soups.",
            "These become hats."
        ],
        [
            "So that's the machinery that's used and.",
            "The answer to the first question, how do you construct the loss from the Super prediction set is trivial, right?",
            "You simply take the support set that is the loss and these things end up being the right type.",
            "This is a support function, the subgradient.",
            "This will be a vector, and so you get the loss which is a vector valued function.",
            "And it's automatically proper, so this is a Canonical way to construct a proper loss.",
            "You'll see some examples of this shortly."
        ],
        [
            "But first, some implications of it.",
            "So normally the way you parameterized proper losses in terms of the Bayes risk.",
            "So that's a function from the simplex to the real numbers.",
            "In this case, it turns out that the Bayes risk is automatically defined on the positive off, and so here these things are probabilities.",
            "Now you've got a thing that's like an entropy, but rather than feeding it a probability, you're feeding it a positive vector.",
            "There's no normalization.",
            "However, automatically because it's a support function, it's one homogeneous, so that means if you scale the argument, the function is scaled linearly.",
            "And there's a classical theorem that if you take the derivative of such a function, then you get a zero homogeneous function.",
            "This is easy to see and a consequence of that is that the losses are zero homogeneous.",
            "Now what that means is that if you apply the loss to a vector, it doesn't matter how long the vector is, it's just the direction that matters.",
            "So there's no real mystery in this.",
            "I mean, you know if you know that it's a probability, you can always normalize it, but this just comes for free, but thinking of it this way.",
            "You can, you can conceive of what the loss vector is doing is it's taking a probability and it's distorting it.",
            "And that then begs the question, well, if you can start with the probability, which is a vector and then get a loss, can you go back the other way?",
            "And that's what I'm going to talk about."
        ],
        [
            "Bully.",
            "So there's a.",
            "This is famous algorithm, the aggregating algorithm, and one of the things you need to do there is.",
            "You have multiple predictions, so these two red dots are loss vectors.",
            "You get a pseudo prediction, which is some.",
            "Combine some combination of those predictions and then what you'd like to get is an actual prediction.",
            "So this is graphically you have this blue point.",
            "You can see there and you would like to find a point on the curve that is to the South.",
            "West, that's.",
            "Logicals is finding a substitution function."
        ],
        [
            "So I will show you that the inverse loss does that.",
            "So what do you mean by an inverse loss?",
            "So you know you might think?",
            "Well, if X is equal to some LV you might like there to be a function L inverse such that this holds true, but you can't do this literally."
        ],
        [
            "There's a key technical result that I uncovered which is you can think of this as the analogue to the classical result about.",
            "The.",
            "Duality between inverses and Fenchel jewels.",
            "But this is for Paula Jules.",
            "So don't worry if you can't parse that, but I've given you the citation there.",
            "The crucial conclusion."
        ],
        [
            "That you get to with only a moderate amount of work is as follows.",
            "If you start with a loss that's defined by set S, you then take the polar of that set S. That will be another convex set.",
            "Then define the loss from that polar.",
            "That gives you the inverse loss.",
            "So I'll demonstrate that with an example on the next slide.",
            "First, it's just a couple of points.",
            "The inverse is only up to a positive scaling, so you're not guaranteed this thing in blue.",
            "Right, that's what you might like, right?",
            "L inverse of LD should equal D. You do not get that what you do get is this right?",
            "So this is actually an example of a thing called the drawers and pseudo."
        ],
        [
            "Inverse, so you think of the pseudo inverse of the matrix.",
            "This is an abstraction of that, so it's like an inverse, but it's not really to pseudoinverse.",
            "So not only losses, distorted probabilities, but."
        ],
        [
            "Abilities are distorted losses, so here's an example.",
            "So suppose you're given a point D here, right?",
            "So this is, Yep, this is a pseudo prediction, and what you would like is to find this point X.",
            "So how can you go and do that?",
            "Well, you have this point D. Here you you see this horizontal hyperplane here that's orthogonal to D. If you evaluate.",
            "The inverse loss at D becausw the inverse losses proper.",
            "That's why you've got this supporting hyperplane here.",
            "That will give you a value point Y.",
            "Right or actually might give you this point S, But the key now it will give you the point why and any scaling of that will do.",
            "This gives you, of course, another hyperplane which is orthogonal to it.",
            "And if you evaluate the desired loss.",
            "With respect to this vector S again because of properness you get to that point X. Alright, so it seems a little bizarre, but all that I'm relying upon here is the definition of properness prop properness.",
            "Remember the definition of properness I gave Properness tells you that these supporting hyperplanes.",
            "Sorry, Properness tells you that the if I evaluate a loss at a point S then the value of the loss will be this point X, which is the support point of the set.",
            "That's the crucial geometry that's going on OK."
        ],
        [
            "This is not a new idea.",
            "This is where I found the idea right.",
            "So it's 60 years old in economics and it's in production, production economics and they call it polar duality.",
            "And when I saw that picture, I realized this is what I was after, so it's nothing new under the sun."
        ],
        [
            "So there are some examples in the paper.",
            "It turns out that the LP family, so this is a little weird.",
            "These are concave gauges, so normally you think of P norms between, you know, with pgri being from one to Infinity.",
            "Here, they're in this weird set, right?",
            "But trust me, this works and you get this.",
            "Standard jewel property truncated here for some reason, but this is gamma P polar.",
            "This is gamma Q so that's just like the normal."
        ],
        [
            "Ality this is an example with those LP losses, right?",
            "So you can.",
            "You probably need a few minutes to understand these pictures, but if you just follow that through and you appreciate the properness gives you, ya'll."
        ],
        [
            "See that these are the inverse losses.",
            "Once you've represented losses in terms of convex bodies, you can then think of, well, how can I would combine convex bodies and what does that imply about losses?",
            "So there's lots of operations on convex bodies that preserve convexity.",
            "The ones in red are examples, so this chap Seger developed a general family of operations on convex bodies that preserved convexity, and that's why this is blue notation here, because you know, he parameterized it in a particular way.",
            "He used these sets C. Which are induced by the standard LP norms in the paper?",
            "I've generalized that to a an arbitrary set C. So now you have a binary operation."
        ],
        [
            "On two in dimensional convex bodies parameterized by third set C and the third set, C is another convex body, but it lives in two dimensions and I'm not going to try and explain how the."
        ],
        [
            "Formulas or work?",
            "Let me just.",
            "There is an equivalent formula on.",
            "Functions so you can have an operation on sets.",
            "There is an analogous operation on functions which allows you to get."
        ],
        [
            "A result that looks like this.",
            "So let me interpret this for you.",
            "Here we are talking about the concave support function of a combination of two convex sets, and this operation is parameterized by a set.",
            "See on the right hand side I've got a concave support function.",
            "Another operation which is analogous to this one on between the two concave support functions.",
            "So this gives you a very interesting algebra of convex bodies parameterized by C. And you can do that for.",
            "There's two different classes of operations that they called the direct and inverse sum.",
            "And this is my penultimate slide, the what I thought was one of the previous results was that you can also express the polar of this.",
            "So here you've got two convex bodies.",
            "You take their polar, the polar of their sum and you just observe that over here you've got the complementary operation parameterized by the polar.",
            "So I still haven't understood all the implications of this yet.",
            "But the interesting thing is you've got this whole algebra that you can do on these losses.",
            "Right, so you've now got a structure, which is what I complained that you didn't have before."
        ],
        [
            "So the conclusion you can indeed parameterized losses by the convex bodies and the loss is simply the subgradient of the support function.",
            "They automatically zero homogeneous.",
            "It gives you this strange notion which may well be useful of an inverse loss.",
            "You can calculate these inverse losses exactly in some cases, not in all cases.",
            "Another very interesting case is thing called the Cobb Douglas production function.",
            "It ends up being self inverse in a bizarre way.",
            "And you get this new algebra of losses induced by sets.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Machine learning is always done for a purpose, and I will argue that this means that loss functions.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Matter, even if you think that all you're doing is getting information from your data.",
                    "label": 1
                },
                {
                    "sent": "Ultimately, you're going to use the information.",
                    "label": 1
                },
                {
                    "sent": "Think, use utility and there will be a loss function.",
                    "label": 0
                },
                {
                    "sent": "So loss functions are central to machine learning.",
                    "label": 1
                },
                {
                    "sent": "Now I'm only going to talk about loss functions for a problem where the labels come from the discrete set one up to N and your job is to estimate a probability.",
                    "label": 0
                },
                {
                    "sent": "Of course you can have more general problems than that.",
                    "label": 0
                },
                {
                    "sent": "Now I would claim that machine learning only pays lip service to losses, see flip.",
                    "label": 0
                },
                {
                    "sent": "And why is that?",
                    "label": 0
                },
                {
                    "sent": "Because normally you look at pretty well.",
                    "label": 0
                },
                {
                    "sent": "Any paper in machine learning when they talk about a loss function, it's just that function ill, right?",
                    "label": 0
                },
                {
                    "sent": "You say, here's the laussel.",
                    "label": 0
                },
                {
                    "sent": "Now we get on and do the interesting stuff.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You might assume that the function is convex or Lipschitz, but beyond that you won't see much, so there's no structure to the loss functions.",
                    "label": 1
                },
                {
                    "sent": "There's no principles about how you go about designing them.",
                    "label": 0
                },
                {
                    "sent": "There's no guidance to a practitioner about what you should know which loss function you should use, and there's no real understanding of the implications of the choice of the loss function, so it's a bit of a character, sure, but I'm making my point.",
                    "label": 0
                },
                {
                    "sent": "There is however a theory of loss functions.",
                    "label": 0
                },
                {
                    "sent": "Which I'll be.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Only talk about now, so remind you the loss on talking about.",
                    "label": 0
                },
                {
                    "sent": "It's a real valued function that is in curd.",
                    "label": 0
                },
                {
                    "sent": "When you predict with probability P and event I occurs now it's very convenient instead to view that loss in this equivalent way.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "All I've done there is to stack up those partial functions into a vector.",
                    "label": 0
                },
                {
                    "sent": "So now an from now on I will think of the loss L as a map from the simplex.",
                    "label": 0
                },
                {
                    "sent": "The in dimensional simplex.",
                    "label": 0
                },
                {
                    "sent": "To a vector of positive real numbers.",
                    "label": 0
                },
                {
                    "sent": "That's what a loss does.",
                    "label": 0
                },
                {
                    "sent": "OK, it's a heck of a lot easier to think about functions of 1 argument then of two arguments.",
                    "label": 0
                },
                {
                    "sent": "And I will use this notion of proper loss is a loss is proper if when you draw the labels from some underlying distribution, then predicting with that same distribution minimizes the loss.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's written there in symbols.",
                    "label": 0
                },
                {
                    "sent": "That's what a proper loss means.",
                    "label": 0
                },
                {
                    "sent": "Write an improper loft.",
                    "label": 0
                },
                {
                    "sent": "Loss would be Daft, right?",
                    "label": 0
                },
                {
                    "sent": "I mean you would be.",
                    "label": 0
                },
                {
                    "sent": "You'd be penalized for getting things right.",
                    "label": 0
                },
                {
                    "sent": "So I'll introduce a little bit of terminology now that you talk about the loss set.",
                    "label": 0
                },
                {
                    "sent": "Search the image of the simplex under the loss, and I believe it was larger introduced this idea of the Super prediction set, so you'll see.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On this graph here.",
                    "label": 0
                },
                {
                    "sent": "So the black curve see here.",
                    "label": 0
                },
                {
                    "sent": "This is the last set.",
                    "label": 0
                },
                {
                    "sent": "It's the image here.",
                    "label": 0
                },
                {
                    "sent": "I've used a more general prediction space, but think of that as a simplex.",
                    "label": 0
                },
                {
                    "sent": "The axes here other partial losses.",
                    "label": 0
                },
                {
                    "sent": "So in this case N = 2 and all of the pictures will be for N = 2.",
                    "label": 0
                },
                {
                    "sent": "But the theory holds more generally, and the Super prediction said is this yellow set here, which goes off to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Alright, I can't draw it off to Infinity, but you think of this as a set going off to Infinity, and it's the fact that every proper loss induces a convex.",
                    "label": 1
                },
                {
                    "sent": "Super prediction set.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, you can argue that if you have a loss whose super prediction set is not convex, then it's inadmissible in that you would never want to use such a loss because you could always do better by using a loss that was the convex Hull of it.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to prove these facts, but just leave me.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, if you have a loss with a super prediction set that is convex, you can always re parameterize it so that it is a proper loss.",
                    "label": 0
                },
                {
                    "sent": "Combine those three facts together and you can claim that there's nothing to lose by only considering convex super prediction sets.",
                    "label": 0
                },
                {
                    "sent": "OK. Got a used car for sale too.",
                    "label": 0
                },
                {
                    "sent": "Believe me right but this is true.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it allows us to now just restrict ourselves to these convex super prediction sets.",
                    "label": 0
                },
                {
                    "sent": "So the goal of the talk is I want to understand loss is better and I want to just take a simple change of perspective.",
                    "label": 0
                },
                {
                    "sent": "What if we start with that set and go back to the losses right?",
                    "label": 0
                },
                {
                    "sent": "Sounds like it's trivial, but it turns out there's some interesting things that comes out of that.",
                    "label": 0
                },
                {
                    "sent": "It allows you to build an algebra of losses, that's novel.",
                    "label": 1
                },
                {
                    "sent": "It allows you to develop a new thing, which I've called an inverse loss, and I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't see that you would have come up with that idea without coming from this geometrical perspective.",
                    "label": 0
                },
                {
                    "sent": "So I use some.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basic ideas in convex analysis are not going to teach you this stuff here.",
                    "label": 0
                },
                {
                    "sent": "The key point I'm going to rapidly go through here is that many of the notions that are used in convex analysis.",
                    "label": 0
                },
                {
                    "sent": "There's always a concave version of them as well.",
                    "label": 0
                },
                {
                    "sent": "But to start with we use Minkowski sums, the definitions there we talk about the recession cone of a set.",
                    "label": 0
                },
                {
                    "sent": "You see this set here, C, The recession cone.",
                    "label": 1
                },
                {
                    "sent": "This 0 + 3.",
                    "label": 0
                },
                {
                    "sent": "That's the set of directions from the origin that you can keep going off to Infinity and stay within the set.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have the sub differential for convex functions.",
                    "label": 0
                },
                {
                    "sent": "Of course there's a super differential.",
                    "label": 0
                },
                {
                    "sent": "For concave functions.",
                    "label": 0
                },
                {
                    "sent": "You note the notation.",
                    "label": 0
                },
                {
                    "sent": "There's a little V on top, it's for the convex case it's a hat, it's a concave case.",
                    "label": 0
                },
                {
                    "sent": "It's just convenient to do it this way because the natural objects that you get for losses end up being concave, right?",
                    "label": 0
                },
                {
                    "sent": "I mean if one could talk about gains instead of losses, you could stick with the convex situation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These sets so the Super prediction sets look like the one at the bottom, and this terminology is introduced by Rockefeller.",
                    "label": 0
                },
                {
                    "sent": "This is a set of positive type, right?",
                    "label": 1
                },
                {
                    "sent": "So what's that mean?",
                    "label": 0
                },
                {
                    "sent": "It's recession cone is the positive.",
                    "label": 0
                },
                {
                    "sent": "Often you can head off any direction to the North East and stay in the set, and the origin is not within the set.",
                    "label": 1
                },
                {
                    "sent": "And there's also the complementary thing which is of negative type.",
                    "label": 0
                },
                {
                    "sent": "It's a little odd that it's asymmetric like this, right?",
                    "label": 0
                },
                {
                    "sent": "But you'll see why that is right.",
                    "label": 1
                },
                {
                    "sent": "The origins, not in the set here.",
                    "label": 0
                },
                {
                    "sent": "It is here the key type that we will work with for losses is the sets of positive type.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's well known that convex sets are characterized by their support functions.",
                    "label": 1
                },
                {
                    "sent": "This is this definition of the support function, the normal definition, and that support function is a convex function.",
                    "label": 1
                },
                {
                    "sent": "There's an equivalent definition you could use written there, which is the concave support functions.",
                    "label": 1
                },
                {
                    "sent": "This will also characterize the sets, but this is a concave function and this is the natural one to work with, so these concave support functions are closed there, one homogeneous.",
                    "label": 0
                },
                {
                    "sent": "I'll give you the definition later if you don't know what that is and the concave support function is indeed concave and the key.",
                    "label": 1
                },
                {
                    "sent": "A key thing is that there's a one to one relationship.",
                    "label": 0
                },
                {
                    "sent": "If you know this, support funk.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you know the convex set and these support functions, it's about supporting hyperplanes, so you would have all met this before.",
                    "label": 0
                },
                {
                    "sent": "There is this notion of the support set right?",
                    "label": 0
                },
                {
                    "sent": "The support set is simply the point in the set of interest which is supported by the supporting hyperplane.",
                    "label": 1
                },
                {
                    "sent": "So that's shown in the picture there.",
                    "label": 0
                },
                {
                    "sent": "So keep that in mind.",
                    "label": 0
                },
                {
                    "sent": "You might not have.",
                    "label": 0
                },
                {
                    "sent": "Come across the name for that before, but the idea of it is is obvious, and the interesting fact that I've is important for the rest of the talk is that it turns out that the support set is.",
                    "label": 1
                },
                {
                    "sent": "Here is the Super differential of the concave support function.",
                    "label": 0
                },
                {
                    "sent": "The support set is the Super differential of the concave support function, so it's universally true.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last bit of machinery is pohlers engages, so engaged is like a norm.",
                    "label": 0
                },
                {
                    "sent": "So if you have any set like this, these are called star shaped sets.",
                    "label": 0
                },
                {
                    "sent": "But obviously if it's convex it's going to work.",
                    "label": 0
                },
                {
                    "sent": "It holds more generally the gauge measures how far out you can go till you leave the set right.",
                    "label": 0
                },
                {
                    "sent": "So think of and think of a norm ball, right?",
                    "label": 0
                },
                {
                    "sent": "What the norm does.",
                    "label": 0
                },
                {
                    "sent": "Normals are usually the norm is 1 and you transcribe the locus.",
                    "label": 0
                },
                {
                    "sent": "There's a concave gauge, right?",
                    "label": 0
                },
                {
                    "sent": "So this is perhaps less well known, right?",
                    "label": 0
                },
                {
                    "sent": "You won't find many papers in the literature on this, but it's a similar idea.",
                    "label": 0
                },
                {
                    "sent": "Here's my set C and I can describe the set in terms of how far I can go along those Rays until I hit it right?",
                    "label": 0
                },
                {
                    "sent": "So that's another way of capturing what the set does.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once you've got these sets, there's a fundamental operation called the polar right, so the polar can be described in this fashion.",
                    "label": 0
                },
                {
                    "sent": "This is the normal convex case, so note this notation.",
                    "label": 0
                },
                {
                    "sent": "It's got this little circle, and there's a V in it that reminds you it's a convex case.",
                    "label": 0
                },
                {
                    "sent": "This is the sublevel set at level one of the support function, so all of these operations are very simple.",
                    "label": 0
                },
                {
                    "sent": "This is a standard thing I will give you some examples of these polls later on.",
                    "label": 0
                },
                {
                    "sent": "And a key property of these poles is that the gauge of a set is equal to the support function of the polar, and likewise the gauge of the polar is the support function of the set.",
                    "label": 0
                },
                {
                    "sent": "This is all classical.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that works also for the concave case.",
                    "label": 0
                },
                {
                    "sent": "Alright, you see, the only difference between these two slides is that there's hats versus these, right?",
                    "label": 0
                },
                {
                    "sent": "So that's important gauges and pollers, so gauges and support functions via apology reality.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there is equivalently an operation on gauges, so you can take the gauge of a set.",
                    "label": 0
                },
                {
                    "sent": "You can do an operation, so that's now a function.",
                    "label": 0
                },
                {
                    "sent": "So there's this operation you can do on that function, and it has the property that if you do that operation on the function, so there's an overload of the notation here on the left hand side, you're applying an operation on a function taking the polar of the gauge, and that is the gauge of the polar.",
                    "label": 0
                },
                {
                    "sent": "So this is all kind of classical stuff scattered around the place, but you'll see why I will use that for shortly, and there's a concave version.",
                    "label": 0
                },
                {
                    "sent": "It's exactly the same, it's become soups.",
                    "label": 0
                },
                {
                    "sent": "These become hats.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the machinery that's used and.",
                    "label": 0
                },
                {
                    "sent": "The answer to the first question, how do you construct the loss from the Super prediction set is trivial, right?",
                    "label": 0
                },
                {
                    "sent": "You simply take the support set that is the loss and these things end up being the right type.",
                    "label": 0
                },
                {
                    "sent": "This is a support function, the subgradient.",
                    "label": 1
                },
                {
                    "sent": "This will be a vector, and so you get the loss which is a vector valued function.",
                    "label": 0
                },
                {
                    "sent": "And it's automatically proper, so this is a Canonical way to construct a proper loss.",
                    "label": 1
                },
                {
                    "sent": "You'll see some examples of this shortly.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But first, some implications of it.",
                    "label": 1
                },
                {
                    "sent": "So normally the way you parameterized proper losses in terms of the Bayes risk.",
                    "label": 1
                },
                {
                    "sent": "So that's a function from the simplex to the real numbers.",
                    "label": 0
                },
                {
                    "sent": "In this case, it turns out that the Bayes risk is automatically defined on the positive off, and so here these things are probabilities.",
                    "label": 0
                },
                {
                    "sent": "Now you've got a thing that's like an entropy, but rather than feeding it a probability, you're feeding it a positive vector.",
                    "label": 0
                },
                {
                    "sent": "There's no normalization.",
                    "label": 0
                },
                {
                    "sent": "However, automatically because it's a support function, it's one homogeneous, so that means if you scale the argument, the function is scaled linearly.",
                    "label": 1
                },
                {
                    "sent": "And there's a classical theorem that if you take the derivative of such a function, then you get a zero homogeneous function.",
                    "label": 0
                },
                {
                    "sent": "This is easy to see and a consequence of that is that the losses are zero homogeneous.",
                    "label": 0
                },
                {
                    "sent": "Now what that means is that if you apply the loss to a vector, it doesn't matter how long the vector is, it's just the direction that matters.",
                    "label": 0
                },
                {
                    "sent": "So there's no real mystery in this.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know if you know that it's a probability, you can always normalize it, but this just comes for free, but thinking of it this way.",
                    "label": 0
                },
                {
                    "sent": "You can, you can conceive of what the loss vector is doing is it's taking a probability and it's distorting it.",
                    "label": 0
                },
                {
                    "sent": "And that then begs the question, well, if you can start with the probability, which is a vector and then get a loss, can you go back the other way?",
                    "label": 0
                },
                {
                    "sent": "And that's what I'm going to talk about.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bully.",
                    "label": 0
                },
                {
                    "sent": "So there's a.",
                    "label": 0
                },
                {
                    "sent": "This is famous algorithm, the aggregating algorithm, and one of the things you need to do there is.",
                    "label": 0
                },
                {
                    "sent": "You have multiple predictions, so these two red dots are loss vectors.",
                    "label": 1
                },
                {
                    "sent": "You get a pseudo prediction, which is some.",
                    "label": 1
                },
                {
                    "sent": "Combine some combination of those predictions and then what you'd like to get is an actual prediction.",
                    "label": 0
                },
                {
                    "sent": "So this is graphically you have this blue point.",
                    "label": 0
                },
                {
                    "sent": "You can see there and you would like to find a point on the curve that is to the South.",
                    "label": 0
                },
                {
                    "sent": "West, that's.",
                    "label": 0
                },
                {
                    "sent": "Logicals is finding a substitution function.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will show you that the inverse loss does that.",
                    "label": 0
                },
                {
                    "sent": "So what do you mean by an inverse loss?",
                    "label": 0
                },
                {
                    "sent": "So you know you might think?",
                    "label": 0
                },
                {
                    "sent": "Well, if X is equal to some LV you might like there to be a function L inverse such that this holds true, but you can't do this literally.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a key technical result that I uncovered which is you can think of this as the analogue to the classical result about.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Duality between inverses and Fenchel jewels.",
                    "label": 0
                },
                {
                    "sent": "But this is for Paula Jules.",
                    "label": 0
                },
                {
                    "sent": "So don't worry if you can't parse that, but I've given you the citation there.",
                    "label": 0
                },
                {
                    "sent": "The crucial conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That you get to with only a moderate amount of work is as follows.",
                    "label": 0
                },
                {
                    "sent": "If you start with a loss that's defined by set S, you then take the polar of that set S. That will be another convex set.",
                    "label": 0
                },
                {
                    "sent": "Then define the loss from that polar.",
                    "label": 0
                },
                {
                    "sent": "That gives you the inverse loss.",
                    "label": 1
                },
                {
                    "sent": "So I'll demonstrate that with an example on the next slide.",
                    "label": 0
                },
                {
                    "sent": "First, it's just a couple of points.",
                    "label": 0
                },
                {
                    "sent": "The inverse is only up to a positive scaling, so you're not guaranteed this thing in blue.",
                    "label": 0
                },
                {
                    "sent": "Right, that's what you might like, right?",
                    "label": 0
                },
                {
                    "sent": "L inverse of LD should equal D. You do not get that what you do get is this right?",
                    "label": 0
                },
                {
                    "sent": "So this is actually an example of a thing called the drawers and pseudo.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inverse, so you think of the pseudo inverse of the matrix.",
                    "label": 0
                },
                {
                    "sent": "This is an abstraction of that, so it's like an inverse, but it's not really to pseudoinverse.",
                    "label": 0
                },
                {
                    "sent": "So not only losses, distorted probabilities, but.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Abilities are distorted losses, so here's an example.",
                    "label": 0
                },
                {
                    "sent": "So suppose you're given a point D here, right?",
                    "label": 1
                },
                {
                    "sent": "So this is, Yep, this is a pseudo prediction, and what you would like is to find this point X.",
                    "label": 0
                },
                {
                    "sent": "So how can you go and do that?",
                    "label": 0
                },
                {
                    "sent": "Well, you have this point D. Here you you see this horizontal hyperplane here that's orthogonal to D. If you evaluate.",
                    "label": 0
                },
                {
                    "sent": "The inverse loss at D becausw the inverse losses proper.",
                    "label": 0
                },
                {
                    "sent": "That's why you've got this supporting hyperplane here.",
                    "label": 0
                },
                {
                    "sent": "That will give you a value point Y.",
                    "label": 0
                },
                {
                    "sent": "Right or actually might give you this point S, But the key now it will give you the point why and any scaling of that will do.",
                    "label": 0
                },
                {
                    "sent": "This gives you, of course, another hyperplane which is orthogonal to it.",
                    "label": 0
                },
                {
                    "sent": "And if you evaluate the desired loss.",
                    "label": 1
                },
                {
                    "sent": "With respect to this vector S again because of properness you get to that point X. Alright, so it seems a little bizarre, but all that I'm relying upon here is the definition of properness prop properness.",
                    "label": 0
                },
                {
                    "sent": "Remember the definition of properness I gave Properness tells you that these supporting hyperplanes.",
                    "label": 0
                },
                {
                    "sent": "Sorry, Properness tells you that the if I evaluate a loss at a point S then the value of the loss will be this point X, which is the support point of the set.",
                    "label": 1
                },
                {
                    "sent": "That's the crucial geometry that's going on OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is not a new idea.",
                    "label": 0
                },
                {
                    "sent": "This is where I found the idea right.",
                    "label": 0
                },
                {
                    "sent": "So it's 60 years old in economics and it's in production, production economics and they call it polar duality.",
                    "label": 0
                },
                {
                    "sent": "And when I saw that picture, I realized this is what I was after, so it's nothing new under the sun.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are some examples in the paper.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the LP family, so this is a little weird.",
                    "label": 0
                },
                {
                    "sent": "These are concave gauges, so normally you think of P norms between, you know, with pgri being from one to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Here, they're in this weird set, right?",
                    "label": 0
                },
                {
                    "sent": "But trust me, this works and you get this.",
                    "label": 0
                },
                {
                    "sent": "Standard jewel property truncated here for some reason, but this is gamma P polar.",
                    "label": 0
                },
                {
                    "sent": "This is gamma Q so that's just like the normal.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ality this is an example with those LP losses, right?",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "You probably need a few minutes to understand these pictures, but if you just follow that through and you appreciate the properness gives you, ya'll.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See that these are the inverse losses.",
                    "label": 0
                },
                {
                    "sent": "Once you've represented losses in terms of convex bodies, you can then think of, well, how can I would combine convex bodies and what does that imply about losses?",
                    "label": 1
                },
                {
                    "sent": "So there's lots of operations on convex bodies that preserve convexity.",
                    "label": 0
                },
                {
                    "sent": "The ones in red are examples, so this chap Seger developed a general family of operations on convex bodies that preserved convexity, and that's why this is blue notation here, because you know, he parameterized it in a particular way.",
                    "label": 0
                },
                {
                    "sent": "He used these sets C. Which are induced by the standard LP norms in the paper?",
                    "label": 0
                },
                {
                    "sent": "I've generalized that to a an arbitrary set C. So now you have a binary operation.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On two in dimensional convex bodies parameterized by third set C and the third set, C is another convex body, but it lives in two dimensions and I'm not going to try and explain how the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formulas or work?",
                    "label": 0
                },
                {
                    "sent": "Let me just.",
                    "label": 0
                },
                {
                    "sent": "There is an equivalent formula on.",
                    "label": 0
                },
                {
                    "sent": "Functions so you can have an operation on sets.",
                    "label": 0
                },
                {
                    "sent": "There is an analogous operation on functions which allows you to get.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A result that looks like this.",
                    "label": 0
                },
                {
                    "sent": "So let me interpret this for you.",
                    "label": 0
                },
                {
                    "sent": "Here we are talking about the concave support function of a combination of two convex sets, and this operation is parameterized by a set.",
                    "label": 0
                },
                {
                    "sent": "See on the right hand side I've got a concave support function.",
                    "label": 0
                },
                {
                    "sent": "Another operation which is analogous to this one on between the two concave support functions.",
                    "label": 0
                },
                {
                    "sent": "So this gives you a very interesting algebra of convex bodies parameterized by C. And you can do that for.",
                    "label": 0
                },
                {
                    "sent": "There's two different classes of operations that they called the direct and inverse sum.",
                    "label": 0
                },
                {
                    "sent": "And this is my penultimate slide, the what I thought was one of the previous results was that you can also express the polar of this.",
                    "label": 0
                },
                {
                    "sent": "So here you've got two convex bodies.",
                    "label": 0
                },
                {
                    "sent": "You take their polar, the polar of their sum and you just observe that over here you've got the complementary operation parameterized by the polar.",
                    "label": 0
                },
                {
                    "sent": "So I still haven't understood all the implications of this yet.",
                    "label": 0
                },
                {
                    "sent": "But the interesting thing is you've got this whole algebra that you can do on these losses.",
                    "label": 0
                },
                {
                    "sent": "Right, so you've now got a structure, which is what I complained that you didn't have before.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the conclusion you can indeed parameterized losses by the convex bodies and the loss is simply the subgradient of the support function.",
                    "label": 1
                },
                {
                    "sent": "They automatically zero homogeneous.",
                    "label": 1
                },
                {
                    "sent": "It gives you this strange notion which may well be useful of an inverse loss.",
                    "label": 1
                },
                {
                    "sent": "You can calculate these inverse losses exactly in some cases, not in all cases.",
                    "label": 0
                },
                {
                    "sent": "Another very interesting case is thing called the Cobb Douglas production function.",
                    "label": 1
                },
                {
                    "sent": "It ends up being self inverse in a bizarre way.",
                    "label": 0
                },
                {
                    "sent": "And you get this new algebra of losses induced by sets.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}