{
    "id": "wns6jjb4kl2crp6lnxhnnxyaye5jvwgu",
    "title": "Improved Nystrom Low-Rank Approximation and Error Analysis",
    "info": {
        "author": [
            "Kai Zhang, Department of Computer and Information Sciences, Temple University"
        ],
        "published": "Aug. 5, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Algorithms and Data Structures",
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/icml08_zhang_inl/",
    "segmentation": [
        [
            "Good afternoon.",
            "Public, my presentation is improved Nystrom.",
            "Lower approximation and error analysis joint work with Iverson and James Clark."
        ],
        [
            "Here is the outline.",
            "The central problem we're trying to solve in this paper is the lower approximation of kernel matrix is we first point out is white applications in machine learning and then we will give a brief review on existing.",
            "Fast approximation techniques.",
            "And our focus would be the sampling based method and in particular Nystrom method, which is the well known technique for approximating large scale angle systems and low rank kernel matrix approximation.",
            "However, despite this popularity analysis on the key step of choosing the landmark points in the Nystrom method is relatively limited, so in the second part we are going to provide an important observation on the Nystrom lower approximation and then derive an error bound of the error bound and this error analysis suggests a surprisingly simple and effective sampling scheme for the Nystrom method and we are going to.",
            "Evaluate this scheme compared with a number of states with other approaches using some supervised and unsupervised learning tasks here.",
            "And the last part is concluding remarks."
        ],
        [
            "OK, I'm given an end by anchor no matrix low rank proximation aims at decomposing K into.",
            "Into GG prime, where G is of very low column rank and approximation quality could be measured by the difference between K&G prime.",
            "Theoretically the angle value decomposition could provide the optimal solution for this loan proximation problem under both the spectral norm and problems problems more, but the complexity is CU.",
            "An efficient alternative have to be pursued."
        ],
        [
            "Lauren Proximation could be quite useful in the following two problem.",
            "The first is matrix angle value decomposition.",
            "If we have a lower approximation available and then the top top angle values and vectors of the kernel matrix K. Could be an could be obtained very efficiently by using by decomposing a much smaller matrix G prime G which is only N by N rather than N by N. Similarly for matrix inversion.",
            "Linear systems of this kind.",
            "We could also use the lower approximation plus the Woodbury formula, which also reduces complexity to come from CU2M squared North."
        ],
        [
            "A number of important machine learning approaches could actually benefit from the lower approximation.",
            "For example, kernel PCA, LDA, spectral clustering, Laplacian angle map, normalized cut.",
            "These are based on angular decomposition of the kernel matrix and also Gaussian process.",
            "This grass VM those are based on the solving linear systems or matrix inversion."
        ],
        [
            "There are many approaches, efficient approaches for solving.",
            "Lauren Proximation one is a greedy approaches.",
            "For example, the incomplete kaliski decomposition, sparse greedy kernel methods, and greedy spectral embedding.",
            "Basically greedy approaches try to incrementally choose samples to maximize improvement on the objective for each step.",
            "Besides greedy approaches.",
            "We could also have an random sampling which is used in standard Nystrom method and its variant using probability example in approaches.",
            "OK."
        ],
        [
            "Here we give a very brief introduction on the Nystrom method.",
            "Basically it was used to solve the integral equation of this client here.",
            "It still needs a little pointers thing, doesn't work.",
            "OK, so given a data X we are trying to solve integral equations here.",
            "KXY is a PSD kernel 5 Y is the engine functions, Lambda is angle values and P is the data distribution.",
            "There are three steps in the Nystrom method.",
            "First, we're trying to approximate approximate the.",
            "I left integral using the empirical average based on a set of landmark points at Z that is supposed to be the same distribution as P. Second step is to choose this variable X.",
            "In this equation and which results in a very small angle.",
            "After solving this morannon system, we could extrapolate this small angle vectors to overall data by using Nystrom extension formula.",
            "The third step."
        ],
        [
            "A very nice property of nice for method is that it can be considered equivalently as approximating the kernel matrix.",
            "As follows, suppose we are given a kernel matrix K which is N by N. Then actually we can randomly choose M columns.",
            "Which is actually the blue columns here, corresponding to the randomly chosen data points and the yellow part is is prime.",
            "And then we define the W, the kernel matrix on landmark points, which is also the intersection between between the blue columns and yellow rolls here.",
            "Then the Nystrom method is actually implicitly approximating K by using East Dublin Verseny prime, which is surprisingly simple.",
            "Hey."
        ],
        [
            "Next, we are going to propose the improvement."
        ],
        [
            "Our method.",
            "The approximation quality of Nystrom method could be measured by the difference between the original kernel matrix and approximated despite all those, nicer method has been applied with success on many important machine learning problems.",
            "The error analysis is still quite limited.",
            "This is because the matrix is here.",
            "K&W are all of different sizes and also there seems to like a simple data structure for us to utilize.",
            "Recently there is a probabilistic error bound.",
            "Derived for.",
            "Nystrom method however, the sampling probabilities depends on the norm of the rows and columns of the kernel matrix, which requires N square.",
            "Our complexity, so it's quite expensive and empirically probabilistic sampling is even worse than the random sample."
        ],
        [
            "OK, so next we are going to point out a very important intuitions on the Nystrom method.",
            "Our vision is as follows.",
            "Given a data X and landmarks at Z, the Nystrom method can actually.",
            "Accurately approximate, OK, thank you.",
            "The nicer method can actually accurately approximate the kernel entry K of XI and XJ.",
            "If there exist 2 landmark points that ZP&VQ that overlap with XI and XJ respectively, here we will give intuition illustration here.",
            "The blue circles are data points and the Red Cross is our landmark points.",
            "Suppose X3 and X4 overlap with Z2, N Z3 respectively.",
            "Then we rewrite the nice from low rank proximation formula here."
        ],
        [
            "It was the same version as this."
        ],
        [
            "So if we want to compute, we want to examine the reconstructed entry between KX3 and X4.",
            "Basically, it was this.",
            "Roll vector, which is similarity between X3 and at landmark points and the inverse of the smaller kernel matrix and this column vector, which is actually similarity between X, X4 and landmark points.",
            "OK, due to the two overlapping here, this role vectors actually similar identical to this one and this column back to identical to this one, which is easy an then by using this trivial equality with regard to matrix inverse we could show.",
            "The reconstructed entry of KX3 and X4 is actually equal to KV-2 and V3.",
            "Which is actually very simple reconstruction by the matrix inversion.",
            "Since on V23 overlap with X three X4, we could accurately reconstruct this case case 3 four.",
            "So this is, we believe the most important intuition reflected in nice room approximation."
        ],
        [
            "So this suggests an intuitive intuitive criteria for sampling.",
            "How do we choose Landmark once that Z is to choose Z to overlap with the data X as much as possible?",
            "Suppose the data X aggregates into M clusters an imagine an ideal case where each cluster shrinks to a one point.",
            "In this case, very naturally, we could actually choosing the cluster centers.",
            "The only position for each cluster as the landmarks at V and in this case the approximation error is zero according to our operations.",
            "However, in practice.",
            "Each class is actually quite diffused, and now the problem is how do we still need to choose cluster centers and how do we cluster the data?"
        ],
        [
            "To answer this other problems, we will provide our analysis.",
            "First we will show a very simple case, similar case.",
            "With this partition, the data acts into M clusters.",
            "For example, here are three classes according to the landmark points at the crossing point.",
            "Image we suppose at this moment every class has the same number of points big T. And for every value of small.",
            "From 1 two big T we perform sampling as follows.",
            "We choose one point from each cluster and then denote the.",
            "Subset of samples chosen at time T as XIT, for example.",
            "Here we have XI one XI, two as I3.",
            "The three circles are chosen in the 1st.",
            "Time step and three squares.",
            "Second step and three.",
            "Diamond third step OK.",
            "So now we define the partial approximation error as the error induced on a block.",
            "Because we had because the data set acts could be imagined as the union of X, one X2 to XT which are disjoint to each other and we will.",
            "Define departure approximation error as this one.",
            "Basically it was a partial version of the Nystrom method because we only consider this blog of the kernel matrix."
        ],
        [
            "It can be shown that the.",
            "Partial approximation error is bounded as follows here.",
            "M is the number of clusters, or them of landmark points.",
            "CCXK is the kernel dependent constant and.",
            "This EI.",
            "I I is the partial approximation error that means."
        ],
        [
            "If we are.",
            "Represent each point in in it using an.",
            "Even close this latter point the condensation error.",
            "And then we are actually sum over all the blocks here."
        ],
        [
            "And we ultimately have this complete error bound.",
            "OK, we can see that several factors here are almost fixed or examples Big T is the largest number of points for each cluster, M is number of clusters, C is a constant, W is the kernel matrix, which is difficult to analyze here.",
            "So we just leave it apart and a very important factor at our disposal is E. The condensation error of coding every single point XI with the closest.",
            "Landmark One VVJK and this error bound actually applies to linear polynomial.",
            "RBF kernel."
        ],
        [
            "OK, so we can observe that the key factor.",
            "Key factors nice from low round proximation is actually encoding powers of the landmark points in representation representing the data.",
            "On the other hand, the timings algorithm is known to find local optimum of the condensation errors, so combining these two factors are very natural.",
            "Scheme is to use the claiming centers as the landmark points for the Nystrom method.",
            "Strictly speaking, this is not the kind of sampling because it is out of sample.",
            "The chosen landmark points are cluster centers, which might not necessarily overlap with the data.",
            "OK, the K means algorithm is has linear complexity in time and space and there are several fast versions utilizing the geometrical reasoning and empirically outperformed all known Nystrom methods as well as greedy approaches."
        ],
        [
            "Before we evaluate our algorithm.",
            "We provide an illustration on to showing the correctness of our error analysis.",
            "Here we are plus the relations between quantization error E which is induced by representing each data point with the closest landmark points.",
            "This line is low rank approximation of the nicer method.",
            "We use three scheme probabilistic sampling, random sampling and K means came incentive.",
            "OK, we can see that the lower the quantization error, the lower the loram proximation error.",
            "There is a strong correlation between between these two values, so this actually coincides well with our error bound."
        ],
        [
            "OK experiment.",
            "Our setup is as follows.",
            "We use some benchmark data from the best VM.",
            "I mean, Ethan, USPS on all task is to one is matrix low rank approximation.",
            "We numerically approximate the kernel matrix and see how well it performs in terms of numerical approximation error and kernel PCA and least squares VN.",
            "We compare with number of approaches.",
            "First is a random sampling, the standardized method, second probability, exampling 3rd is a greedy approach.",
            "The incomplete policy conversation.",
            "And the last one is ending up the conversation with which we approve it at SVD and provides optimal solutions."
        ],
        [
            "OK, here are some eight data from the VM.",
            "First, don't be a surprised by the surprisingly small numbers of data size here compared with the previous few talks.",
            "This is because in this experiment, we're trying to actually recall the low rank approximation error by different approaches in order to have optimal solutions, we have to perform complete.",
            "Position on our PC so the maximum data we could afford to handle is about 400 four 1500, but it doesn't mean our algorithm could not be applied to much larger data.",
            "Here we because the value decomposition and incomplete release decomposition are deterministic, which has, which has only a fixed values for Nystrom method with random sampling, probabilistic sampling and K means because they are non deterministic in nature.",
            "So we gradually increase the number of.",
            "Number of landmark points from 1% to 10% of the data size and then we.",
            "Repeat 20 times for each value of N and then we will call the average performance for angularity conversation we simply use the top an angle vectors for reconstruction and then recall the approximation errors so we can see.",
            "Our algorithm is red curve with squares.",
            "Seems to be only inferior to the optimal undervalued accommodation.",
            "Next, come the random sampling and probability sampling and incomplete release decomposition seems to.",
            "Giving large errors on several data.",
            "However, for some data whose spectrum decays rapidly to zero, which means its rank is very low, then the incomplete release decomposition also quickly drops.",
            "The error drops.",
            "Here we use RBF kernels."
        ],
        [
            "Also test the.",
            "Linear kernels.",
            "Note that for linear kernels, the rank of the kernel matrix equals the number of features or even lower.",
            "So in this case most of the algorithm actually will quickly.",
            "I reduce errors when the number of landmark points increases because it is already reaches the rank.",
            "However, averagely our algorithm still.",
            "Quite competitive to others."
        ],
        [
            "Here is the polynomial kernel of degree three.",
            "We can see that because it's nonlinear kernel, the cases are quite similar to the RBF kernels, and our algorithm is only inferior to the optimal one.",
            "OK."
        ],
        [
            "The second experiment is kernel PCA.",
            "What we do here is we first.",
            "We first perform standard kernel PC on the complete kernel matrix and obtain the complete the ground truth embedding on the top three principle directions, and then we.",
            "Using this for approaches.",
            "Logan Paul summation techniques to obtain the approximate embeddings.",
            "We align approximate embeddings with the ground truth embedding and record the alignment errors.",
            "Sometimes average, and then we can see that.",
            "We also recall the time.",
            "In terms of approximation, numerical numerical approximation errors, our algorithm averagely is 1 old one or two orders of magnitude slower than other three approaches and in terms of time consumptions.",
            "Of course, the random sampling is the.",
            "Most efficient approach.",
            "Our algorithm is about several times slower than random approaches and also could be slower than other approaches.",
            "However, this is deemed as worthy because the approximation quality is much higher, and the factor here is only 5 or 10 times.",
            "Another another thing to know that we only use the most naive implementation of K means by using the kind of fast versions we could expect to have much.",
            "Higher efficiency."
        ],
        [
            "The last experiment is a disgrace.",
            "The end.",
            "I remember that for least square SVM.",
            "The key step is to solve the inverse inverse of the kernel matrix passing plus a jittering term.",
            "So here we perform standard least squares VM on USPS digits, which is relatively difficult to differentiate.",
            "With SVD, our approach and Nystrom as a random sampling and probably exampling greedy approaches.",
            "Basically all these approaches perform lower approximation on the kernel matrix, so they could have an approximate solution for least squares VM.",
            "Here is a classification errors from the best to the worst is red, green, blue and black so we can see that if we choose number of landmark points to be 5% of the data size.",
            "SVD seems to give averagely the best performance, because it's a more denied.",
            "The kernel matrix then come the least squares VM and then our approach is the other three approaches already performed.",
            "Much worse this case."
        ],
        [
            "We also increased the number of landmark points from 5% to 10% of the data, and in this case things change is slightly the best, is still SVD and then comes our approach because we have used 10% of the data size as the landmark points and then comes the original lease squares VM and then other approaches in this case and I stress is also working well.",
            "Because in this case the number of random landmark points has already been able to solve this problem problem in terms of difficulty.",
            "OK."
        ],
        [
            "OK, now."
        ],
        [
            "Come to our conclusion.",
            "Actually, our conclusion is quite simple.",
            "We show that the nice film lower approximation.",
            "Depends crucially on the encoding powers of the landmark points in terms of representing the data.",
            "So in this case K means would be deemed as a very suitable sampling scheme for Nystrom method and it is simple, easy to implement.",
            "The whole algorithm could be done using about 10 lines of Matlab calls.",
            "Future work.",
            "Although we have provided a generalized error bound for different kernels, including RBF and linear and polynomial, in practice we find that the different kernels actually seem to favor the sampling schemes preliminary.",
            "Our experience is that for RBF kernel, because it's based on the distance between pairwise points, K means work is working pretty well.",
            "However, for linear kernels.",
            "Where the norm of the data points actually.",
            "Is a factor here, so it might seems to favor some variance of K means, which takes into account the norms of data points.",
            "So this is One Direction.",
            "Second direction is.",
            "We want we will try try to extend our approach to more generalize the kernel matrix is where we don't have a closed form kernel functions.",
            "And.",
            "To go one even one step further, we.",
            "Also want to use in our approach to scale up or speed up the singular value decomposition of a general data.",
            "Actually, preliminary results have shown that by using only square complexity, because SVD usually can be considered cubic, we can actually extend our approach to SVD and obtain good scaling behaviors as well as accuracy.",
            "Last thing is we also want to combine this little approximation scheme with side information, say the label information.",
            "In this case we could afford to use much lower lot more points in order to perform.",
            "A certain classification task with specified accuracy.",
            "OK."
        ],
        [
            "This is the end, thank you.",
            "You questions.",
            "So you mentioned that your method was.",
            "I don't know about an order of magnitude slower than said in Nystrom method, but that it was more accurate.",
            "Did you try to compare for the same amount of computation?",
            "The accuracy of your method versus using Nystrom because?",
            "Clearly."
        ],
        [
            "Using Nystrom then you could take many more columns, right?",
            "OK, so that is very practical.",
            "Consideration means using the same computations whether you could compute accurate solutions and nice romantic.",
            "We haven't tried this because we basically just."
        ],
        [
            "The very efficient version K means we could make our algorithm also quite efficient."
        ],
        [
            "Also, you can see that actually the here the time consumption is seems not to be a very big difference."
        ],
        [
            "So here you could also think it is effective as using same number, same number of landmark points in this criteria using only.",
            "And if you consider the number of data points is 1/4 of the algorithm using the same number of landmark points, then our approach is only inferior to SVD.",
            "But definitely will also perform kind of more meaningful comparisons between.",
            "Using the same amount of computational time to see whether they also give more accurate solution."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon.",
                    "label": 0
                },
                {
                    "sent": "Public, my presentation is improved Nystrom.",
                    "label": 1
                },
                {
                    "sent": "Lower approximation and error analysis joint work with Iverson and James Clark.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is the outline.",
                    "label": 0
                },
                {
                    "sent": "The central problem we're trying to solve in this paper is the lower approximation of kernel matrix is we first point out is white applications in machine learning and then we will give a brief review on existing.",
                    "label": 0
                },
                {
                    "sent": "Fast approximation techniques.",
                    "label": 0
                },
                {
                    "sent": "And our focus would be the sampling based method and in particular Nystrom method, which is the well known technique for approximating large scale angle systems and low rank kernel matrix approximation.",
                    "label": 1
                },
                {
                    "sent": "However, despite this popularity analysis on the key step of choosing the landmark points in the Nystrom method is relatively limited, so in the second part we are going to provide an important observation on the Nystrom lower approximation and then derive an error bound of the error bound and this error analysis suggests a surprisingly simple and effective sampling scheme for the Nystrom method and we are going to.",
                    "label": 0
                },
                {
                    "sent": "Evaluate this scheme compared with a number of states with other approaches using some supervised and unsupervised learning tasks here.",
                    "label": 0
                },
                {
                    "sent": "And the last part is concluding remarks.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I'm given an end by anchor no matrix low rank proximation aims at decomposing K into.",
                    "label": 1
                },
                {
                    "sent": "Into GG prime, where G is of very low column rank and approximation quality could be measured by the difference between K&G prime.",
                    "label": 0
                },
                {
                    "sent": "Theoretically the angle value decomposition could provide the optimal solution for this loan proximation problem under both the spectral norm and problems problems more, but the complexity is CU.",
                    "label": 0
                },
                {
                    "sent": "An efficient alternative have to be pursued.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lauren Proximation could be quite useful in the following two problem.",
                    "label": 0
                },
                {
                    "sent": "The first is matrix angle value decomposition.",
                    "label": 0
                },
                {
                    "sent": "If we have a lower approximation available and then the top top angle values and vectors of the kernel matrix K. Could be an could be obtained very efficiently by using by decomposing a much smaller matrix G prime G which is only N by N rather than N by N. Similarly for matrix inversion.",
                    "label": 0
                },
                {
                    "sent": "Linear systems of this kind.",
                    "label": 0
                },
                {
                    "sent": "We could also use the lower approximation plus the Woodbury formula, which also reduces complexity to come from CU2M squared North.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A number of important machine learning approaches could actually benefit from the lower approximation.",
                    "label": 0
                },
                {
                    "sent": "For example, kernel PCA, LDA, spectral clustering, Laplacian angle map, normalized cut.",
                    "label": 1
                },
                {
                    "sent": "These are based on angular decomposition of the kernel matrix and also Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "This grass VM those are based on the solving linear systems or matrix inversion.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are many approaches, efficient approaches for solving.",
                    "label": 0
                },
                {
                    "sent": "Lauren Proximation one is a greedy approaches.",
                    "label": 1
                },
                {
                    "sent": "For example, the incomplete kaliski decomposition, sparse greedy kernel methods, and greedy spectral embedding.",
                    "label": 1
                },
                {
                    "sent": "Basically greedy approaches try to incrementally choose samples to maximize improvement on the objective for each step.",
                    "label": 1
                },
                {
                    "sent": "Besides greedy approaches.",
                    "label": 0
                },
                {
                    "sent": "We could also have an random sampling which is used in standard Nystrom method and its variant using probability example in approaches.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we give a very brief introduction on the Nystrom method.",
                    "label": 0
                },
                {
                    "sent": "Basically it was used to solve the integral equation of this client here.",
                    "label": 0
                },
                {
                    "sent": "It still needs a little pointers thing, doesn't work.",
                    "label": 0
                },
                {
                    "sent": "OK, so given a data X we are trying to solve integral equations here.",
                    "label": 0
                },
                {
                    "sent": "KXY is a PSD kernel 5 Y is the engine functions, Lambda is angle values and P is the data distribution.",
                    "label": 0
                },
                {
                    "sent": "There are three steps in the Nystrom method.",
                    "label": 1
                },
                {
                    "sent": "First, we're trying to approximate approximate the.",
                    "label": 0
                },
                {
                    "sent": "I left integral using the empirical average based on a set of landmark points at Z that is supposed to be the same distribution as P. Second step is to choose this variable X.",
                    "label": 0
                },
                {
                    "sent": "In this equation and which results in a very small angle.",
                    "label": 0
                },
                {
                    "sent": "After solving this morannon system, we could extrapolate this small angle vectors to overall data by using Nystrom extension formula.",
                    "label": 0
                },
                {
                    "sent": "The third step.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A very nice property of nice for method is that it can be considered equivalently as approximating the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "As follows, suppose we are given a kernel matrix K which is N by N. Then actually we can randomly choose M columns.",
                    "label": 1
                },
                {
                    "sent": "Which is actually the blue columns here, corresponding to the randomly chosen data points and the yellow part is is prime.",
                    "label": 0
                },
                {
                    "sent": "And then we define the W, the kernel matrix on landmark points, which is also the intersection between between the blue columns and yellow rolls here.",
                    "label": 0
                },
                {
                    "sent": "Then the Nystrom method is actually implicitly approximating K by using East Dublin Verseny prime, which is surprisingly simple.",
                    "label": 0
                },
                {
                    "sent": "Hey.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, we are going to propose the improvement.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our method.",
                    "label": 0
                },
                {
                    "sent": "The approximation quality of Nystrom method could be measured by the difference between the original kernel matrix and approximated despite all those, nicer method has been applied with success on many important machine learning problems.",
                    "label": 0
                },
                {
                    "sent": "The error analysis is still quite limited.",
                    "label": 1
                },
                {
                    "sent": "This is because the matrix is here.",
                    "label": 1
                },
                {
                    "sent": "K&W are all of different sizes and also there seems to like a simple data structure for us to utilize.",
                    "label": 1
                },
                {
                    "sent": "Recently there is a probabilistic error bound.",
                    "label": 0
                },
                {
                    "sent": "Derived for.",
                    "label": 0
                },
                {
                    "sent": "Nystrom method however, the sampling probabilities depends on the norm of the rows and columns of the kernel matrix, which requires N square.",
                    "label": 1
                },
                {
                    "sent": "Our complexity, so it's quite expensive and empirically probabilistic sampling is even worse than the random sample.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so next we are going to point out a very important intuitions on the Nystrom method.",
                    "label": 0
                },
                {
                    "sent": "Our vision is as follows.",
                    "label": 0
                },
                {
                    "sent": "Given a data X and landmarks at Z, the Nystrom method can actually.",
                    "label": 1
                },
                {
                    "sent": "Accurately approximate, OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "The nicer method can actually accurately approximate the kernel entry K of XI and XJ.",
                    "label": 0
                },
                {
                    "sent": "If there exist 2 landmark points that ZP&VQ that overlap with XI and XJ respectively, here we will give intuition illustration here.",
                    "label": 0
                },
                {
                    "sent": "The blue circles are data points and the Red Cross is our landmark points.",
                    "label": 0
                },
                {
                    "sent": "Suppose X3 and X4 overlap with Z2, N Z3 respectively.",
                    "label": 0
                },
                {
                    "sent": "Then we rewrite the nice from low rank proximation formula here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It was the same version as this.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we want to compute, we want to examine the reconstructed entry between KX3 and X4.",
                    "label": 0
                },
                {
                    "sent": "Basically, it was this.",
                    "label": 0
                },
                {
                    "sent": "Roll vector, which is similarity between X3 and at landmark points and the inverse of the smaller kernel matrix and this column vector, which is actually similarity between X, X4 and landmark points.",
                    "label": 0
                },
                {
                    "sent": "OK, due to the two overlapping here, this role vectors actually similar identical to this one and this column back to identical to this one, which is easy an then by using this trivial equality with regard to matrix inverse we could show.",
                    "label": 0
                },
                {
                    "sent": "The reconstructed entry of KX3 and X4 is actually equal to KV-2 and V3.",
                    "label": 0
                },
                {
                    "sent": "Which is actually very simple reconstruction by the matrix inversion.",
                    "label": 0
                },
                {
                    "sent": "Since on V23 overlap with X three X4, we could accurately reconstruct this case case 3 four.",
                    "label": 0
                },
                {
                    "sent": "So this is, we believe the most important intuition reflected in nice room approximation.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this suggests an intuitive intuitive criteria for sampling.",
                    "label": 0
                },
                {
                    "sent": "How do we choose Landmark once that Z is to choose Z to overlap with the data X as much as possible?",
                    "label": 1
                },
                {
                    "sent": "Suppose the data X aggregates into M clusters an imagine an ideal case where each cluster shrinks to a one point.",
                    "label": 1
                },
                {
                    "sent": "In this case, very naturally, we could actually choosing the cluster centers.",
                    "label": 0
                },
                {
                    "sent": "The only position for each cluster as the landmarks at V and in this case the approximation error is zero according to our operations.",
                    "label": 0
                },
                {
                    "sent": "However, in practice.",
                    "label": 0
                },
                {
                    "sent": "Each class is actually quite diffused, and now the problem is how do we still need to choose cluster centers and how do we cluster the data?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To answer this other problems, we will provide our analysis.",
                    "label": 0
                },
                {
                    "sent": "First we will show a very simple case, similar case.",
                    "label": 0
                },
                {
                    "sent": "With this partition, the data acts into M clusters.",
                    "label": 1
                },
                {
                    "sent": "For example, here are three classes according to the landmark points at the crossing point.",
                    "label": 0
                },
                {
                    "sent": "Image we suppose at this moment every class has the same number of points big T. And for every value of small.",
                    "label": 0
                },
                {
                    "sent": "From 1 two big T we perform sampling as follows.",
                    "label": 0
                },
                {
                    "sent": "We choose one point from each cluster and then denote the.",
                    "label": 0
                },
                {
                    "sent": "Subset of samples chosen at time T as XIT, for example.",
                    "label": 0
                },
                {
                    "sent": "Here we have XI one XI, two as I3.",
                    "label": 0
                },
                {
                    "sent": "The three circles are chosen in the 1st.",
                    "label": 0
                },
                {
                    "sent": "Time step and three squares.",
                    "label": 0
                },
                {
                    "sent": "Second step and three.",
                    "label": 0
                },
                {
                    "sent": "Diamond third step OK.",
                    "label": 0
                },
                {
                    "sent": "So now we define the partial approximation error as the error induced on a block.",
                    "label": 0
                },
                {
                    "sent": "Because we had because the data set acts could be imagined as the union of X, one X2 to XT which are disjoint to each other and we will.",
                    "label": 0
                },
                {
                    "sent": "Define departure approximation error as this one.",
                    "label": 0
                },
                {
                    "sent": "Basically it was a partial version of the Nystrom method because we only consider this blog of the kernel matrix.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It can be shown that the.",
                    "label": 0
                },
                {
                    "sent": "Partial approximation error is bounded as follows here.",
                    "label": 1
                },
                {
                    "sent": "M is the number of clusters, or them of landmark points.",
                    "label": 1
                },
                {
                    "sent": "CCXK is the kernel dependent constant and.",
                    "label": 0
                },
                {
                    "sent": "This EI.",
                    "label": 1
                },
                {
                    "sent": "I I is the partial approximation error that means.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we are.",
                    "label": 0
                },
                {
                    "sent": "Represent each point in in it using an.",
                    "label": 0
                },
                {
                    "sent": "Even close this latter point the condensation error.",
                    "label": 0
                },
                {
                    "sent": "And then we are actually sum over all the blocks here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we ultimately have this complete error bound.",
                    "label": 0
                },
                {
                    "sent": "OK, we can see that several factors here are almost fixed or examples Big T is the largest number of points for each cluster, M is number of clusters, C is a constant, W is the kernel matrix, which is difficult to analyze here.",
                    "label": 0
                },
                {
                    "sent": "So we just leave it apart and a very important factor at our disposal is E. The condensation error of coding every single point XI with the closest.",
                    "label": 1
                },
                {
                    "sent": "Landmark One VVJK and this error bound actually applies to linear polynomial.",
                    "label": 0
                },
                {
                    "sent": "RBF kernel.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we can observe that the key factor.",
                    "label": 1
                },
                {
                    "sent": "Key factors nice from low round proximation is actually encoding powers of the landmark points in representation representing the data.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the timings algorithm is known to find local optimum of the condensation errors, so combining these two factors are very natural.",
                    "label": 1
                },
                {
                    "sent": "Scheme is to use the claiming centers as the landmark points for the Nystrom method.",
                    "label": 0
                },
                {
                    "sent": "Strictly speaking, this is not the kind of sampling because it is out of sample.",
                    "label": 0
                },
                {
                    "sent": "The chosen landmark points are cluster centers, which might not necessarily overlap with the data.",
                    "label": 0
                },
                {
                    "sent": "OK, the K means algorithm is has linear complexity in time and space and there are several fast versions utilizing the geometrical reasoning and empirically outperformed all known Nystrom methods as well as greedy approaches.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before we evaluate our algorithm.",
                    "label": 0
                },
                {
                    "sent": "We provide an illustration on to showing the correctness of our error analysis.",
                    "label": 0
                },
                {
                    "sent": "Here we are plus the relations between quantization error E which is induced by representing each data point with the closest landmark points.",
                    "label": 1
                },
                {
                    "sent": "This line is low rank approximation of the nicer method.",
                    "label": 1
                },
                {
                    "sent": "We use three scheme probabilistic sampling, random sampling and K means came incentive.",
                    "label": 1
                },
                {
                    "sent": "OK, we can see that the lower the quantization error, the lower the loram proximation error.",
                    "label": 0
                },
                {
                    "sent": "There is a strong correlation between between these two values, so this actually coincides well with our error bound.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK experiment.",
                    "label": 0
                },
                {
                    "sent": "Our setup is as follows.",
                    "label": 0
                },
                {
                    "sent": "We use some benchmark data from the best VM.",
                    "label": 0
                },
                {
                    "sent": "I mean, Ethan, USPS on all task is to one is matrix low rank approximation.",
                    "label": 1
                },
                {
                    "sent": "We numerically approximate the kernel matrix and see how well it performs in terms of numerical approximation error and kernel PCA and least squares VN.",
                    "label": 0
                },
                {
                    "sent": "We compare with number of approaches.",
                    "label": 0
                },
                {
                    "sent": "First is a random sampling, the standardized method, second probability, exampling 3rd is a greedy approach.",
                    "label": 0
                },
                {
                    "sent": "The incomplete policy conversation.",
                    "label": 0
                },
                {
                    "sent": "And the last one is ending up the conversation with which we approve it at SVD and provides optimal solutions.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here are some eight data from the VM.",
                    "label": 0
                },
                {
                    "sent": "First, don't be a surprised by the surprisingly small numbers of data size here compared with the previous few talks.",
                    "label": 0
                },
                {
                    "sent": "This is because in this experiment, we're trying to actually recall the low rank approximation error by different approaches in order to have optimal solutions, we have to perform complete.",
                    "label": 0
                },
                {
                    "sent": "Position on our PC so the maximum data we could afford to handle is about 400 four 1500, but it doesn't mean our algorithm could not be applied to much larger data.",
                    "label": 0
                },
                {
                    "sent": "Here we because the value decomposition and incomplete release decomposition are deterministic, which has, which has only a fixed values for Nystrom method with random sampling, probabilistic sampling and K means because they are non deterministic in nature.",
                    "label": 0
                },
                {
                    "sent": "So we gradually increase the number of.",
                    "label": 0
                },
                {
                    "sent": "Number of landmark points from 1% to 10% of the data size and then we.",
                    "label": 0
                },
                {
                    "sent": "Repeat 20 times for each value of N and then we will call the average performance for angularity conversation we simply use the top an angle vectors for reconstruction and then recall the approximation errors so we can see.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm is red curve with squares.",
                    "label": 0
                },
                {
                    "sent": "Seems to be only inferior to the optimal undervalued accommodation.",
                    "label": 0
                },
                {
                    "sent": "Next, come the random sampling and probability sampling and incomplete release decomposition seems to.",
                    "label": 0
                },
                {
                    "sent": "Giving large errors on several data.",
                    "label": 0
                },
                {
                    "sent": "However, for some data whose spectrum decays rapidly to zero, which means its rank is very low, then the incomplete release decomposition also quickly drops.",
                    "label": 0
                },
                {
                    "sent": "The error drops.",
                    "label": 0
                },
                {
                    "sent": "Here we use RBF kernels.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also test the.",
                    "label": 0
                },
                {
                    "sent": "Linear kernels.",
                    "label": 0
                },
                {
                    "sent": "Note that for linear kernels, the rank of the kernel matrix equals the number of features or even lower.",
                    "label": 0
                },
                {
                    "sent": "So in this case most of the algorithm actually will quickly.",
                    "label": 0
                },
                {
                    "sent": "I reduce errors when the number of landmark points increases because it is already reaches the rank.",
                    "label": 0
                },
                {
                    "sent": "However, averagely our algorithm still.",
                    "label": 0
                },
                {
                    "sent": "Quite competitive to others.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the polynomial kernel of degree three.",
                    "label": 0
                },
                {
                    "sent": "We can see that because it's nonlinear kernel, the cases are quite similar to the RBF kernels, and our algorithm is only inferior to the optimal one.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second experiment is kernel PCA.",
                    "label": 0
                },
                {
                    "sent": "What we do here is we first.",
                    "label": 0
                },
                {
                    "sent": "We first perform standard kernel PC on the complete kernel matrix and obtain the complete the ground truth embedding on the top three principle directions, and then we.",
                    "label": 0
                },
                {
                    "sent": "Using this for approaches.",
                    "label": 0
                },
                {
                    "sent": "Logan Paul summation techniques to obtain the approximate embeddings.",
                    "label": 0
                },
                {
                    "sent": "We align approximate embeddings with the ground truth embedding and record the alignment errors.",
                    "label": 0
                },
                {
                    "sent": "Sometimes average, and then we can see that.",
                    "label": 0
                },
                {
                    "sent": "We also recall the time.",
                    "label": 0
                },
                {
                    "sent": "In terms of approximation, numerical numerical approximation errors, our algorithm averagely is 1 old one or two orders of magnitude slower than other three approaches and in terms of time consumptions.",
                    "label": 0
                },
                {
                    "sent": "Of course, the random sampling is the.",
                    "label": 0
                },
                {
                    "sent": "Most efficient approach.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm is about several times slower than random approaches and also could be slower than other approaches.",
                    "label": 0
                },
                {
                    "sent": "However, this is deemed as worthy because the approximation quality is much higher, and the factor here is only 5 or 10 times.",
                    "label": 0
                },
                {
                    "sent": "Another another thing to know that we only use the most naive implementation of K means by using the kind of fast versions we could expect to have much.",
                    "label": 0
                },
                {
                    "sent": "Higher efficiency.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last experiment is a disgrace.",
                    "label": 0
                },
                {
                    "sent": "The end.",
                    "label": 0
                },
                {
                    "sent": "I remember that for least square SVM.",
                    "label": 0
                },
                {
                    "sent": "The key step is to solve the inverse inverse of the kernel matrix passing plus a jittering term.",
                    "label": 0
                },
                {
                    "sent": "So here we perform standard least squares VM on USPS digits, which is relatively difficult to differentiate.",
                    "label": 0
                },
                {
                    "sent": "With SVD, our approach and Nystrom as a random sampling and probably exampling greedy approaches.",
                    "label": 0
                },
                {
                    "sent": "Basically all these approaches perform lower approximation on the kernel matrix, so they could have an approximate solution for least squares VM.",
                    "label": 0
                },
                {
                    "sent": "Here is a classification errors from the best to the worst is red, green, blue and black so we can see that if we choose number of landmark points to be 5% of the data size.",
                    "label": 0
                },
                {
                    "sent": "SVD seems to give averagely the best performance, because it's a more denied.",
                    "label": 0
                },
                {
                    "sent": "The kernel matrix then come the least squares VM and then our approach is the other three approaches already performed.",
                    "label": 0
                },
                {
                    "sent": "Much worse this case.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also increased the number of landmark points from 5% to 10% of the data, and in this case things change is slightly the best, is still SVD and then comes our approach because we have used 10% of the data size as the landmark points and then comes the original lease squares VM and then other approaches in this case and I stress is also working well.",
                    "label": 0
                },
                {
                    "sent": "Because in this case the number of random landmark points has already been able to solve this problem problem in terms of difficulty.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Come to our conclusion.",
                    "label": 0
                },
                {
                    "sent": "Actually, our conclusion is quite simple.",
                    "label": 0
                },
                {
                    "sent": "We show that the nice film lower approximation.",
                    "label": 0
                },
                {
                    "sent": "Depends crucially on the encoding powers of the landmark points in terms of representing the data.",
                    "label": 0
                },
                {
                    "sent": "So in this case K means would be deemed as a very suitable sampling scheme for Nystrom method and it is simple, easy to implement.",
                    "label": 0
                },
                {
                    "sent": "The whole algorithm could be done using about 10 lines of Matlab calls.",
                    "label": 0
                },
                {
                    "sent": "Future work.",
                    "label": 0
                },
                {
                    "sent": "Although we have provided a generalized error bound for different kernels, including RBF and linear and polynomial, in practice we find that the different kernels actually seem to favor the sampling schemes preliminary.",
                    "label": 0
                },
                {
                    "sent": "Our experience is that for RBF kernel, because it's based on the distance between pairwise points, K means work is working pretty well.",
                    "label": 0
                },
                {
                    "sent": "However, for linear kernels.",
                    "label": 0
                },
                {
                    "sent": "Where the norm of the data points actually.",
                    "label": 0
                },
                {
                    "sent": "Is a factor here, so it might seems to favor some variance of K means, which takes into account the norms of data points.",
                    "label": 0
                },
                {
                    "sent": "So this is One Direction.",
                    "label": 0
                },
                {
                    "sent": "Second direction is.",
                    "label": 0
                },
                {
                    "sent": "We want we will try try to extend our approach to more generalize the kernel matrix is where we don't have a closed form kernel functions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "To go one even one step further, we.",
                    "label": 0
                },
                {
                    "sent": "Also want to use in our approach to scale up or speed up the singular value decomposition of a general data.",
                    "label": 0
                },
                {
                    "sent": "Actually, preliminary results have shown that by using only square complexity, because SVD usually can be considered cubic, we can actually extend our approach to SVD and obtain good scaling behaviors as well as accuracy.",
                    "label": 0
                },
                {
                    "sent": "Last thing is we also want to combine this little approximation scheme with side information, say the label information.",
                    "label": 0
                },
                {
                    "sent": "In this case we could afford to use much lower lot more points in order to perform.",
                    "label": 0
                },
                {
                    "sent": "A certain classification task with specified accuracy.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the end, thank you.",
                    "label": 0
                },
                {
                    "sent": "You questions.",
                    "label": 0
                },
                {
                    "sent": "So you mentioned that your method was.",
                    "label": 0
                },
                {
                    "sent": "I don't know about an order of magnitude slower than said in Nystrom method, but that it was more accurate.",
                    "label": 0
                },
                {
                    "sent": "Did you try to compare for the same amount of computation?",
                    "label": 0
                },
                {
                    "sent": "The accuracy of your method versus using Nystrom because?",
                    "label": 0
                },
                {
                    "sent": "Clearly.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using Nystrom then you could take many more columns, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so that is very practical.",
                    "label": 0
                },
                {
                    "sent": "Consideration means using the same computations whether you could compute accurate solutions and nice romantic.",
                    "label": 0
                },
                {
                    "sent": "We haven't tried this because we basically just.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The very efficient version K means we could make our algorithm also quite efficient.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, you can see that actually the here the time consumption is seems not to be a very big difference.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here you could also think it is effective as using same number, same number of landmark points in this criteria using only.",
                    "label": 0
                },
                {
                    "sent": "And if you consider the number of data points is 1/4 of the algorithm using the same number of landmark points, then our approach is only inferior to SVD.",
                    "label": 0
                },
                {
                    "sent": "But definitely will also perform kind of more meaningful comparisons between.",
                    "label": 0
                },
                {
                    "sent": "Using the same amount of computational time to see whether they also give more accurate solution.",
                    "label": 0
                }
            ]
        }
    }
}