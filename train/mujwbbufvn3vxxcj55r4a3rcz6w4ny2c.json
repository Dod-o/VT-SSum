{
    "id": "mujwbbufvn3vxxcj55r4a3rcz6w4ny2c",
    "title": "Introduction to Machine Learning",
    "info": {
        "author": [
            "Katherine A. Heller, Department of Statistical Science, Duke University"
        ],
        "published": "Oct. 11, 2018",
        "recorded": "July 2018",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/DLRLsummerschool2018_heller_machine_learning/",
    "segmentation": [
        [
            "Thanks very much Graham.",
            "Yeah, so I'm here to talk to you today about just sort of like the basics that you're going to need going into other talks in the summer school, I guess before I get started I should also kind of give a shout out to see far who is very kindly supporting me.",
            "Being here with my kids, which is not something that you often find particularly.",
            "Where I'm from, so I really appreciate that.",
            "OK, so an introduction to machine learning."
        ],
        [
            "So there are a bunch of things that so I guess.",
            "I guess I should stop here and say this is going to get a little technical pretty quickly.",
            "If you're expecting really technical, it's not going to get really technical at all, but it's going to get into sort of like the details of what we're what we're doing in machine learning pretty quickly.",
            "And so you guys have the background to go into the rest of the talks and kind of understand what's going on.",
            "So I'm going to be talking about the different kinds of machine learning, so a lot of you guys are here for the.",
            "Reinforcement learning workshop.",
            "As well as this one, and so I'll touch briefly on what makes reinforcement learning different from other kinds of machine learning, I'm going to discuss linear regression, which is sort of one of the most basic techniques, and one of the ones that you're going to have to understand in order to understand neural networks regularization.",
            "Why we actually care about Regularising things?",
            "Bayesian methods, which are largely what I work on.",
            "Logistic regression again, coming off of linear regression.",
            "Add something again.",
            "You're going to need to know to understand neural networks, and then I'm going to answer sort of that.",
            "That kind of concludes the amount of material I really need to cover as like basic material to go into the rest of the rest of the course, but I wanted to talk for a little bit about why it is that we do this right, because we all do machine learning.",
            "Machine learning is, I guess getting to be more and more trendy everyday and like what are the kinds of applications?",
            "Or or areas in which we think that we can impact society with what we do.",
            "And so I'm going to.",
            "I'm going to talk a little bit about that.",
            "And then I'm going to talk a little bit about what I do personally, being mostly applications to health care and why I find that to be motivating.",
            "OK, so let me start with."
        ],
        [
            "What are the different kinds of machine learning?",
            "So machine learning is typically broken down into three different categories.",
            "It's not.",
            "It's not a perfect breakdown, but you'll hear people talk about supervised learning or reinforcement learning or unsupervised learning.",
            "And most of the work that I've done personally has fallen under unsupervised learning.",
            "But I think a lot of the work that you're going to be hearing about here is supervised learning.",
            "And until the Reinforcement Learning Workshop where you're going to hear about reinforcement learning.",
            "So the idea is that supervised."
        ],
        [
            "Running takes in some set of inputs.",
            "Those inputs could be.",
            "Any set of numbers they could be vital signs of a patient in the hospital and Maps them to an output URL label right?",
            "They could be that could be like are you septic or not septic?",
            "Something that you'll see probably really commonly is images right?",
            "So it could be like image features X one through XD and it could be mapped to an output like.",
            "Is this a cat or not a cat?",
            "OK, so our goal in supervised learning is to be able to find a good function F which does this mapping for us.",
            "So what can we use to take in these features of, say, like our CAT image and be able to say whether or not this is a cap?",
            "So whatever the function is, we'd like it to be able to map our inputs to the correct label all of the time.",
            "We'd like it to be able to correctly say this is a cat image.",
            "This is not a cat image 100% correctly every single time, including on new examples, right?",
            "We can't do that in practice, like we're never going to get everything perfectly correctly, and So what we really need is some kind of metric or some kind of way of telling us how close we are to this ideal.",
            "How close are we to being able to say it's to say, OK, I have this function which takes some set of features and some or some inputs and Maps it to some level cat or not cat.",
            "So this involves choosing some kind of loss function.",
            "You'll probably hear about loss functions quite a bit, and the loss function could be something like.",
            "What are the number of times we get it wrong, right?",
            "Like what are the number of times we say cat when it's not a cat, or how close we are to the correct label?",
            "So if we're looking at some things that the label doesn't have to be binary 01, it could be something like I'm trying to predict what your height is.",
            "So how close did I get right?",
            "How far away am I?",
            "And then we want to minimize the loss using the data set that we have, right?",
            "So we're looking at doing something like training.",
            "We get some set of labels that we know, and then we say, OK, you know what are what.",
            "What is the function that we can learn which will minimize the amount of loss between being able to predict the labels that we know like cat or not cat from images that we already have in our data set.",
            "And then we try to minimize minimize the distance.",
            "Between basically like the perfect or the ideal function and the function that we're learning."
        ],
        [
            "So here are some examples of supervised learning.",
            "Again, so object detection.",
            "So here we're looking at, say, classifying images as sheep or cows.",
            "I actually did see this once I'm to talk, so I'm not making it up completely.",
            "What I work on again is mostly healthcare application, so we do this kind of supervised learning when we're trying to do something like predicting surgical complications which doesn't have nearly as nice images but but you are working in a regime that is not necessarily just like pictures all the time.",
            "So, for example, the kinds of inputs that we have might be what kind of surgery did the person have?",
            "What is the patient's age?",
            "What are the other kinds of illnesses?",
            "The patient suffering from?",
            "What's their DNR status?",
            "What surgeon is performing performing the surgery, or how like, what's their nutritional status?",
            "Are they healthy?",
            "Have they have they lost a lot of weight?",
            "That kind of thing?",
            "And from all of these things we're trying to do something like predict a particular complication that they might have, or they might not have after having the surgery, right?",
            "So are they likely to have a bleeding complication of breathing complication or neurological complication or some kind of?",
            "Infection right?",
            "So we look at at sort of predicting zero or one for each of those."
        ],
        [
            "OK, so that's basically an overview of supervised learning reinforcement learning.",
            "Reinforcement learning is learning what actions to take based on our reward signal, and again, this is going to get covered, probably like in the reinforcement Learning Workshop in a few days so.",
            "I don't have this here, but like one of the one of the reasons that we look at reinforcement learning is to be able to say OK, we want to know in a hospital setting what kind of actions or interventions we should take in order to sort of like lead to a situation in which the patient is healthiest, the healthiest they possibly can be.",
            "But a lot of where you see reinforcement learning in the literature is again things like playing games, and I'll show you that in a second, so you have some kind of state.",
            "Usually there's a state of the world that you're in, so if you think of, like, say, like the game of go, you could be in a particular situation where the board looks a particular way.",
            "So then you evaluate your chances of getting to another state by taking a particular action by making a particular move right.",
            "Like so you're going to have a board that changes to look another particular way.",
            "What are the chances given that I make a particular move right, and so in the end, or sometimes along the way, or reward signal is received.",
            "So if you win the game you get some reward.",
            "If you lose the game you get some kind of negative reward.",
            "Or you don't get a reward and we want to be able to take actions or make moves right in our game of go that lead to getting the most reward possible in expectation.",
            "So we learn a set of actions or set of moves to make in a say in the game.",
            "Or a policy they call this a policy which leads to maximizing the reward that we expect at the end of the day.",
            "OK, so reinforcement learning all about learning what actions you might take in order to maximize your expected reward."
        ],
        [
            "OK, and so examples of reinforcement learning that you see a lot are things like the game of go right?",
            "So there's the Alphago paper or movie where where they used reinforcement learning to have an automatic automated way of deciding on moves in the game of go.",
            "They've also used it for doing things like trying to play Atari games.",
            "Well, I've used it myself in order to try to decide what medical interventions.",
            "Are most effective in a hospital setting again, unlike the previous thing which I talked to you about, I think doing this is even a step farther for the hospital, right?",
            "In that they can kind of get behind the idea that you have an automated system which their physicians see, which might predict that a patient has or doesn't have something.",
            "But when you start recommending ways to treat them through like reinforcement learning, they currently sketch out a little bit more about that, so I think that's a little farther off on the horizon.",
            "But I have certainly used it.",
            "Oh sorry, I have certainly used it in order to try to try to an automated way, decide what medical interventions would be most effective for a particular patient.",
            "So here the state is the health of the patient, like vital science, right?",
            "In a way that we can measure it.",
            "The action that we take is a particular medical intervention, right?",
            "So it might be administering Ivy fluids or antibiotics, and then the reward comes from like did the patient liver not?",
            "Did the patient live plus 10?"
        ],
        [
            "OK, and last and last we have unsupervised learning.",
            "So in unsupervised learning and again most of the work that I've done actually falls into this category.",
            "An unsupervised learning.",
            "You don't have any labels, right?",
            "So there's no training in the sense that, like you have an image and you take the image features and you try to use this to predict, is this a sheep?",
            "Or is this a cow, right?",
            "You're just looking to try to find like interesting and informative patterns in your data.",
            "So one of the one of the things that I've really worked on and.",
            "Is the most common maybe example of this is like clustering data.",
            "So you want to find groups of things that are going on in your data and this could come about for many many reasons.",
            "One of the things that I do again in in a medical setting is say we have some mobile app data.",
            "The mobile app data comes from patients with neurological disease.",
            "Can we find subpopulations within that data, right?",
            "Can we find subpopulations?",
            "Let's say react to a medication differently.",
            "Can we find some populations that are having a different disease course and that's not a supervised kind of problem?",
            "We don't have actual labels on that, but we can look in an unsupervised way at the structure of our data and try to make an assessment."
        ],
        [
            "OK, so clustering clustering is the act of grouping objects or data points together based on common features.",
            "So clustering is natural or grouping things together is natural.",
            "You could group together athletes by what kind of sport they play.",
            "You could group together people by what gang there in you could group together.",
            "I like lots of different things.",
            "OK, so on this slide I have an example of a few objects right?",
            "So I could choose to group the fruit objects together into one group and the animal objects together to another group, right?",
            "But you know why is it that I'm interested in making this grouping first half right?"
        ],
        [
            "And the reason why I'm interested in learning about this grouping is because it helps me do some kind of prediction for like a new object, right?",
            "So if I see the new object which is given there in the middle right?",
            "I want to.",
            "I might want to predict some things about it.",
            "Like does it breathe water?",
            "It sells like that kind of thing and I can actually use this kind of grouping or clustering to make to make predictions about what it is that this new object does from extrapolating from the other Members.",
            "Members of its group without actually having to go ahead and measure things like is it breathing or what's it, cell type like.",
            "There aren't any inherently correct clustering, and this is something that you're going to find a lot of an unsupervised learning.",
            "You're not necessarily in a situation where you get like the right solution, only only clusterings that are right for a particular problem, right?",
            "So if we look at the objects that I'm showing you right here, one of the things one of the ways that we might be interested in grouping them together is not by fruit versus animals, but by things that hang on trees and things that don't write, and we would get.",
            "A different, a different grouping, or a different clustering if that's what we were interested in."
        ],
        [
            "OK, so like I said another example of unsupervised learning is learning different hidden factors that make patient populations.",
            "Patient population is different hospital so OK, so this is more like.",
            "Sometimes we're confronted with situations where we say, like have a national national national data set, right?",
            "So first surgical complications.",
            "We have a national database of, like all of the different people who went through surgery and all of the participating hospitals and what their complications were.",
            "And we want to be able to do things like figure out how is it that I can take that national level data and use it to make the best predictions possible at my own local hospital right at Duke University Hospital for me.",
            "This isn't this isn't exactly given to us by our national database because Duke University Hospital as a single hospital is going to have a bit of a different patient population than the nation as a whole, right?",
            "You could have different kinds of hospitals.",
            "Some of these are community hospitals where they are seeing a different a very different subset of the population, and so one of the things that I've worked on is is being able to leverage this large national database in order to.",
            "Learn these hidden factors and make them make better predictions locali.",
            "And another thing that that I talked to you guys about already is I've also used clustering to look at people who suffer from particular neurological disease or the clustering of mobile app data to look at people who suffer from a particular neurological disease.",
            "So like, why are you tired?",
            "It might be because of your disease.",
            "It might be because of your medication medication that you're on.",
            "It might be because you're depressed, or it might be because you just haven't gotten good sleep over the last few nights, right?",
            "And these are all very different kinds of explanations.",
            "And what the what kind of category you as an individual individual follow into is going to help lead to different treatment decisions.",
            "And so we really want to understand that."
        ],
        [
            "OK, so that pretty much covers like the basics of like what supervised learning is what unsupervised learning is and what reinforcement learning is an what.",
            "What regimes or paradigms they might be able.",
            "They may be able to be useful to us going forward, but now sort of like more pragmatically, how is it that we might go about trying to solve a supervised learning problem or solve an unsupervised learning problem?",
            "OK, so the first thing that we start out with is a data set.",
            "Right, so you have some kind of set of data.",
            "This data involves a whole bunch of features.",
            "They could be features of an image.",
            "They could be features of a patient who's in the hospital.",
            "If we're doing supervised learning, which is the 1st place we're going to, we're going to go right.",
            "We have some kind of additional column in our data matrix, which is the label, right?",
            "Like?",
            "Is this patient septic or not septic?",
            "Is this image an image of a chypre account?",
            "So if the labels are discrete, like what I was just talking about, this is a good place to do what's called classification.",
            "So can we classify this image is being a sheep image versus can we classify this image is being a cow image if the labels are continuous we can do something called the regression which is say exactly where in this continuous spectrum do we think that we lie given?",
            "The input that we're seeing.",
            "Um?",
            "We might have more complex labels and somebody will probably talk about this with you guys later on.",
            "This is called structured prediction."
        ],
        [
            "OK, so let's go back to supervised learning for a second, so this is really just bringing up this slide again.",
            "So OK, so our goal in supervised learning is to take some set of inputs, like some some features of the patient in the hospital or some features of an image and make some kind of prediction, say like let's say a classification or or or regression prediction.",
            "We want to predict say like what this person's pulse is going to be, right like something that's a little bit more continuous and that we're probably not going to do classification for.",
            "How is it that we do this?",
            "How is it that we find the appropriate function that Maps are that Maps our input space to our label space?",
            "What does the function look?"
        ],
        [
            "Like right?",
            "And so the set of functions that we can use in order to do that.",
            "Do this comes from some kind of hypothesis class.",
            "Hypothesis class is really the terminology that people use often when describing this, so the hypothesis classes like the space of all possible functions that we're considering.",
            "And so they can include certain function groups or function types so the functions can be like linear functions.",
            "So say we're considering all possible linear functions, or all possible polynomial functions, right?",
            "So the scatter plot at the bottom right shows the relationship between US high school graduation and the percentage of people living in poverty.",
            "What kind of hypothesis class might be appropriate?",
            "Let me ask you guys what kind of hypothesis costs might be appropriate.",
            "Yeah, linear OK so so we might be able to fit this scatterplot.",
            "Would say like a line, right?",
            "There might be a linear relationship between US high school graduation rates and the percentage of people living in poverty."
        ],
        [
            "OK, so we can like look at all of the different lines that we might be able to fit to this data.",
            "How do we know which one is correct?",
            "OK, so which which of the four lines do you guys think looks better?",
            "Yeah, so so right.",
            "So all of these all of these functions, all of these linear functions are in our hypothesis class, right?",
            "They're all linear functions and we want to be able to like learn the best one, and the best one means the one that fits to the data and tells us the most about what the data looks like.",
            "OK, so how do we determine that?"
        ],
        [
            "So this gets into fitting a line, right?",
            "So if we have this hypothesis space that are these linear functions, the function itself can be described by an equation of the form y = W zero plus W1X, right?",
            "So we're in two dimensions here.",
            "In general, if we are in more than two dimensions, we can have y = W zero plus W1X1 plus W2X2 plus blah blah blah on.",
            "To however many dimensions we have right, we can sum from zero from zero to DWIXI or just say W transpose X.",
            "So here WXR G + 1 length vectors W are often called.",
            "The the weights you can think of them as parameters of the model right there.",
            "The things that we want to learn right?",
            "So they really define what are linear function looks like, whether it's like.",
            "Air B or C. Your D right like these are going to have to do with the WS.",
            "And that's why we care about learning the best ones possible.",
            "OK, so so then this brings up the question of like how do we measure the quality of the lines fit?",
            "Or how do we choose the best W right?",
            "Since we've we've determined that W is the thing that we really care about.",
            "W learning W is what's going to help us learn that this is line a that line A is the best line.",
            "Anne."
        ],
        [
            "So the way that we can think about it is by saying OK there are for any particular line that we learn, there are residuals or there's a difference between the data that we actually observe and that particular line.",
            "And if we're learning the best line possible, what we really want to do is like learn the line.",
            "That's going to minimize those residuals right?",
            "So another way of saying this is like we have some data set.",
            "The line that's going to be the best.",
            "That's going to look the best to us.",
            "Is the line that minimizes, like the distance, that each data point is off the line, right?",
            "Whether it's positive or negative, right?",
            "Like we really want to try to minimize that difference and so we can learn those parameters.",
            "We can learn those W's in order to do this."
        ],
        [
            "OK, so here's here's an example of that right?",
            "Like you can see where these are, since these are each dot is a state in the United States or Washington DC, right?",
            "And so you can look at where each dot wise with respect to the line that we've we've learned.",
            "We can look at the distance between that particular dot that applauded on the where the Y axis is the percentage of people who live in poverty, and the X axis is the percentage of people who graduated from high school.",
            "And we can compute the distance between that data point and the line that we're learning, right?",
            "So for Washington DC, that's 5.44.",
            "For Rhode Island, that's negative 4.16 right?",
            "And this is what we mean when we talk about residuals just the distance that the data is off of the line up for learning.",
            "OK, so how good or bad it are is our line."
        ],
        [
            "We want the line that had the smallest residuals possible.",
            "We want the line that's really going to minimize this distance.",
            "And the way that we get that is, we want to minimize the sum of squared residuals.",
            "We want to minimize that distance people on a final wine with the weights that minimizes that distance summed over each data point, right?",
            "And this is called least squares.",
            "So why do we do least squares in this linear Russian setting in order to learn the weights W. The reasons why we use least squares or that you see people talking about least squares.",
            "So First off, it's really commonly used.",
            "People people talk about it a lot.",
            "Square is a nicer function than the absolute value function, right?",
            "So if you think about the distance between DC and that line being 5.4 four, we could just keep it as that.",
            "We have that negative 4.16.",
            "We don't actually want to negative, right?",
            "We want to say the distance from that data point.",
            "The Rhode Island data point to the line is 4.16, right?",
            "So we could just absolute value everything, right?",
            "But it's really much nicer to work with the square function, and I think other people will get into this more than me.",
            "It's really much nicer to work with the square function, which is nice and continuous than it is to work with an absolute value function.",
            "And in many applications, for many applications that we're going to consider having twice the residual or having the data point like twice as far off the line is more than twice as bad, right?",
            "Like having that extra distances becomes really really, really bad very quickly, and so having this sum of squares distance having this least squares kind of paradigm is going to be appropriate when we care when we care about the distance, the distance that did the point lies from the line a lot.",
            "Where farther away is much, much worse.",
            "OK, so how do we use minimizing the square?"
        ],
        [
            "Residuals define W, right?",
            "So we've got this concept of the residuals.",
            "We've got this concept that the data point lies some distance from the line.",
            "How do we use that to find our weights?",
            "So really, what we want to do, we talked about like the best fit line, the weights that we want are the ones that are going to give us the line that minimizes the residuals.",
            "So we just do argument over W the sum of our residuals, right?",
            "So we say we want to minimize what are residuals are with respect to the W that we find, right?",
            "So in this way, in this way we find the W that we're looking for.",
            "OK, so this is computed sort of like.",
            "The way that we would compute minimizing anything the way that you were kind of taught in high school or in college math, right?",
            "We compute this by taking the gradient with respect to W, and by setting this this function equal to 0.",
            "And you're going to learn a lot.",
            "I think there's going to be a lot of this summer school which is devoted to ways of taking the gradient with respect to W. Setting it equal to 0 and solving right, and so in this particular case, in this linear regression case, we can get a solution from doing this in closed form, right?",
            "So it turns out that WW the is given by that closed form equation.",
            "There W = X transpose XX, transpose Y.",
            "But that's often not the case.",
            "You often can't get W in closed form, and so a lot of people are going to talk to you about various kinds of optimization methods that you might use when you can't actually find W in closed form.",
            "So that's where this is kind of all heading."
        ],
        [
            "But in our example, we can, and so we learned that for the line of interest that the percentage of people living in poverty is equal to 64.68 -- .6 two times the percentage of high school graduates, right?",
            "So we can learn our weight parameters for the line that we're interested in for our particular data set."
        ],
        [
            "OK, so then the next question is how do we know that a line is right, right?",
            "So we looked at that particular data set at that particular scatter plot and we were like that looks linear.",
            "It looks like something a line would fit.",
            "Well, we can't always visualize the data set that we're working with.",
            "We, you know, we don't always know by visualizing it even right like what what works best.",
            "And So what are the pluses and minuses of using a line if we shouldn't be using a line or using something more complicated or whatever.",
            "And so really hear the danger is that we can underfit or overfit to our data, right?",
            "So we have some particular data set.",
            "I stole this I think.",
            "Maybe like from online I think maybe it was from measuring originally I don't know.",
            "But but here's here's some example of like a data set.",
            "Where we can learn some kind of function?",
            "The function is given in blue, right and so you see with the blue function over on the left hand side.",
            "We're learning a line to fit something that's probably not linear, right?",
            "In this case, we're underfitting.",
            "We're finding a function that's too simple, and so this is going to lead to hide bias, right?",
            "So another way of saying this is that we're going to have large training error.",
            "Right, so the you know the best fit line that we have is the line that shown, but we're going to have high residuals.",
            "We can have the function that shown in the middle which looks like just right and then we can have the function that we see on."
        ],
        [
            "Right hand side which is like very quickly and we call this high variance.",
            "So if you think about variance from your math class, probably back in high school it means that it bounces around a lot and so one of the problems that we really face in machine learning is that if we fit a function that's too complex to our data and it bounces around too much, we're going to get really low training.",
            "You're right, like it's going to be able to fit our training data set really, really well.",
            "But we're going to have really high test error.",
            "It's not going to generalize that well to new data.",
            "It's like just bouncing around way too much.",
            "And so we want to be able to avoid that.",
            "We want to be able to avoid that concept of overfitting using some kind of function that's too complex, and we want to be able to avoid underfitting, which is using a function that is too simple and be able to find this function.",
            "That's kind of just right just right for our data."
        ],
        [
            "OK, so as we get more and more data we can consider more and more complex hypothesis because we have more and more evidence about what that function looks like.",
            "But I guess the question is what happens when we consider functions that have a lot of parameters compared to our data set size.",
            "What happens if we consider functions which are two weekly for our data?",
            "Endlessly this too.",
            "Just a discussion of maximum likelihood, right?",
            "So how are we going about finding the parameters that we are for our function?",
            "And the way that we've gone about finding parameters so far in the case of linear regression, how do we find those WS?",
            "How do we find out?",
            "Line is something called a maximum likelihood of approach, right?",
            "So this looks at the probability of our data, given that the model is maximized with respect to the parameters, right?",
            "So we're looking at basically trying to say we want it to be maximally likely that.",
            "That particular line is responsible for our data that.",
            "Is probably not going to make contact with you because generative models is a complicated concept, but we want to make sure that we're maximizing the probability of the data given the model parameters with respect to the model parameters.",
            "OK. Um?",
            "This can be done in the same way that we just talked about.",
            "We take gradients with respect to our model parameters data, so I've kind of overloaded model parameters here.",
            "So our model primers data are just the W's that I was talking about in the linear regression case.",
            "So here you can equate the two if you're thinking about linear regression you have some set of weights.",
            "Those weights are parameters we want to learn those weights when we take gradients, we set it equal to 0 and we solve.",
            "If in general we have something of the form, the probability of our data given some set of model parameters, like the weights in our linear regression, we can do the exact same thing.",
            "We can take gradients with respect to Theta, set equal to 0 and solve.",
            "So in fact, our least squares solution that we talked about for linear regression can be seen as maximizing the likelihood of a Gaussian distribution with mean W transpose X, right?",
            "So that's to say if those residuals are distributed as a Gaussian off of that line, this is what we get when we maximize the likelihood of our data."
        ],
        [
            "OK, but the problem is that if we have this approach, if we use this approach, this maximum likelihood approach, we run the risk of overfitting.",
            "If we have too many parameters.",
            "So you could see that if we had a parameter for every data point in our data set, we would be able to find a function which fit our data set perfectly right?",
            "So we have some let me go back here.",
            "We have some data set like in this example.",
            "We can fit a squiggly line.",
            "If we have enough parameters that let's this line squiggle enough, then we can hit every single data point exactly, right?",
            "And so.",
            "That's not necessarily something we want to do.",
            "'cause as we talked about before, this is something which is going to lead to really low training error, but it's not going to generalize well and we want to learn a function that's going to generalize well.",
            "So instead of just looking at doing this maximum likelihood kind of learning of these parameters, one of the things that we do is that we could change the function that we're learning in order to penalize having too many parameters right.",
            "So we want to penalize.",
            "We want to maximize their training.",
            "Our training minimize our training error, right?",
            "But we also want to penalize having lots of parameters, right?",
            "So we want to penalize this kind of situation over on the right where we have a very quickly line with lots of parameters, right?",
            "And say OK, I think you should actually use the minimum number of parameters possible.",
            "OK, and we do this by adding in a term right?",
            "So we have our same loss function.",
            "We do the same kind of like minimizing of the residuals that we did before, but we can say OK if our hypothesis class is going to allow us to have like very complicated functions, we're going to penalize those functions.",
            "So every time like we add in a new parameter to the line that we're learning, we're going to like up the penalty so that we don't actually, so that we were saying we don't actually want to find that.",
            "Function we only want to find that function that we really need the extra parameter.",
            "OK, so the way that we do this is by taking again, taking our original loss, which was the distance between the data points in the line that we were learning and adding in some kind of regularization term.",
            "And here this regularization term is given by Lambda which is called the regularization coefficient.",
            "Basically it's some.",
            "Some scalar parameter which allows us to control the tradeoff between how well we want to fit our data and how simple we want the function that we're learning to be, and then some kind of regularization term which penalizes having a more complex function.",
            "And that's our of data, right?",
            "So the more parameters we have, the bigger that regularization term is going to be.",
            "If we have a Lambda which is reasonably high, right like that?",
            "That's going to add a lot to the function that we're trying to minimize, and so we're not going to want to go there.",
            "OK, so there are lots of different.",
            "So what does this are look like?",
            "Which is this regularization term look like?",
            "There are lots of different choices for our.",
            "There are lots of different ways we could say OK, we're going to penalize having more parameters.",
            "And we can talk really briefly about different choices for are really common choices that you're going."
        ],
        [
            "The pop up again and again our L wanted L2 penalties.",
            "OK, and so we call this L1 or L2 regularization an L1L2 regularization penalizes complexity of the functions in different ways.",
            "So one just trade up, penalizes the number of parameters used.",
            "An L2 penalizes the number of parameters squared, right?",
            "So in our linear regression setting, this would be what you see on the left.",
            "In terms of the minimization that y -- W transpose X ^2.",
            "That's our original residual function from our linear regression that we minimize to learn what the W weights were.",
            "But then we have this extra term Lambda times, and then for the L2 penalty W squared and for the L1 penalty just W, right?",
            "And we call this some kind of regularization right?",
            "So it's really penalizing penalizing when we're we're seeing those more complicated functions, right?",
            "It's just in slightly different ways, so this is again like a squared versus like an awkward thing.",
            "And so in a probabilistic setting, these regularizers correspond to what we call different kind of priors.",
            "And I'll talk about what that means in a second.",
            "But basically it corresponds to in the L2K suggesting prior in the L1 case Laplacian prior on the weights.",
            "Um?",
            "L1 regularization also leads to something called shrinkage.",
            "So when you're when you're hearing about shrinkage shrinkage priors, sometimes sparsity priors.",
            "You'll hear about one regularization.",
            "So basically this means that this is something which is going to like seeing the weights go close to zero.",
            "It's going to sort of like push what we're learning to a situation in which the weights get smaller."
        ],
        [
            "Color or become zero?",
            "OK, so.",
            "That that discussion about priors.",
            "The L1 prior, the L1 regularizer being equivalent to a little possum prior, and L2 regularizer being a couple into a Gaussian prior brings us to Bayes rule, right?",
            "Like what is this prior term that I mentioned?",
            "So Bayesian methods are used a lot and phase rule is used a lot and so trying to trying to find the WS which maximize the regularization equations that I gave.",
            "On the previous slides, is equivalent to doing something called map estimation, right?",
            "It's equivalent to using this kind of Bayes rule and doing some kind of like maximizing of maximizing of the parameters Theta."
        ],
        [
            "OK. We don't have to maximize.",
            "We don't have to optimize the parameters, and I'll talk to you in a second about like what else we might do instead of optimizing the parameters because I've spent most of my career not optimizing parameters and most of what you're going to learn about here is optimizing parameters so.",
            "I thought that I just tell you really briefly about about alternatives.",
            "OK, so, but the first thing you have to know is Bayes rule.",
            "OK, so Bayes rule basically provides us with a coherent framework for reasoning about our beliefs in the face of uncertainty.",
            "So we have the probability of the data given the model parameters.",
            "That's what we've been talking about.",
            "Like this whole time, right?",
            "Like This is why we talked about like the sum of squared residuals in that particular that potentially.",
            "Corresponding to like a Gaussian model, right?",
            "So that probability of the data given the model parameters could be a Gaussian and then we have some kind of prior on the model parameters.",
            "So we talked about different kinds of regularizers we talked about having like a Gaussian regularizer, oral possing regularizer and we talked about how that corresponded to the kind of prior that we're using, and so that prior P of Theta would go there.",
            "And then we have some kind of normalizer.",
            "Which then gives us P of Theta given D, that's what's on the left hand side, which is our posterior.",
            "OK, so that intuitive idea behind this is that RPF data our prior captures our prior beliefs about the state of the world.",
            "So apriori we might think that that we have a Gaussian which looks a particular way and then we see more evidence coming in through our likelihood function.",
            "The probability of.",
            "Of our data given given our model parameters, gives us the probability of our observations of the observations that we're seeing in our data set.",
            "Given that we're in a particular state data, or given that we have a particular prior over our model parameters, and then we can use this to compute P of Theta given D, which is our posterior or update updated beliefs having observed the data that we did about the state of the world data.",
            "Given our observations deep and so this gets into something that like I talk about a lot, I don't want to get into this here, but this gets into something that I talk about a lot 'cause I'm going to stats Department, so I end up teaching a lot of stats classes and one of the things that we look at a lot.",
            "It's that's way too much is like hypothesis testing, and often like that's kind of set up so that we say, OK, you know.",
            "What do I think PP.",
            "Of the given Theta look looks like and how do I deal with that just on its own an when we look at a maximum likelihood kind of setting were again kind of looking at this P of D given Theta on its own, but that's often not the quantity that we're really interested in were not often interested in the probability of our data given our model parameters.",
            "Usually what we're actually interested in is the probability of our model parameters given our data.",
            "Our data is something that we observe.",
            "It is fixed.",
            "It is given to us by the world and then we want to know what line is it that we choose, given that this is what we're seeing.",
            "So really the thing that varies the thing that's the random variable that we're interested in is this data.",
            "Are the parameters like of this line right?",
            "And looking at a Bayesian setting, it's got like a lot of advantages, but one of the advantages is that it treats that it treats that Theta those sets of weights as the random variable and the data as sort of given.",
            "OK, another another.",
            "Another bonus is that you avoid the overfitting problem or you in the same way that regularization avoids the overfitting problem.",
            "I talked about these different kinds of regularizers like L1 regularization or L2 regularization corresponding to particular priors, right?",
            "Those priors fit in very nicely in the Bayesian setting.",
            "You can see them right here, and so once you incorporate these regularizers.",
            "You're giving a little bit more slack to allowing allowing your training area to go up in exchange for having a similar function.",
            "There are a lot of other kinds of kinds of benefits that I'm not going into too much here that you get from a Bayesian paradigm, and people will talk more to you about this later, but like you get some kind of sense of uncertainty, like how confident am I in the parameters that I learned for my linear function, etc."
        ],
        [
            "OK, and so this is the slide where I really deviate from from the setting of maximizing.",
            "Finding them the maximally likely parameters even with regularization.",
            "So in most of my work I don't actually optimize, I compute something called a marginal likelihood.",
            "When I go to do clustering and it's so instead of taking gradients and setting them equal setting of functions equal to 0.",
            "I compute something called a marginal likelihood, and the marginal likelihood is defined using the integral that you see, and so the downside is this is an integral and so that kind of sucks computationally, but the upside is that you're averaging now over a lot of different parameters.",
            "Instead of choosing just one.",
            "And so there are lots of benefits to this so.",
            "So I when I when I go to do something like compare one or two clusters, I can compute the marginal likelihood so I can compute the marginal likelihood.",
            "OK, so the marginal likelihood is the probability of my data given my model.",
            "So here in this example plot I have a bunch of data points.",
            "I'm going to say those data points came from a Gaussian and so I'm going to compare the hypothesis that they came from all from one Gaussian versus they came from two different Gaussians.",
            "Want guessing for the purple dots?",
            "An want guessing for the for the blue dots, and this is a movie.",
            "It's a little ridiculous to call this a movie, but you'll see what happens when when they separate and we can compare the likelihood that the data came from one Gaussian versus that they came from 2 Gaussian clusters, right?",
            "OK, so the marginal likelihood is the probability of the data given the model.",
            "The model is a Gaussian, so I'm going to integrate the probability of the data given some fixed but unknown model parameters Theta times the prior on the model parameters.",
            "So here the model parameters for Gaussian would be the Gaussians mean and covariance.",
            "Right, and so I've got some kind of prior idea of what the meaning covariance should look like, right?",
            "So this is the interpretation that all of the data points.",
            "This gives the interpretation that all of the data points in our data set were generated from the same model with unknown parameters Theta.",
            "And I am now integrating over what I think all what I think those parameters data might be.",
            "Instead of optimizing and find the maximally likely likely one.",
            "OK, so I can use this concept of a marginal likelihood.",
            "To compare the one versus 2 cluster model and I'll show you I'm actually marked OK, so I'll show you hopefully.",
            "Here we go.",
            "OK, so hopefully you can see as the as the purple datapoints pull apart from the blue data points.",
            "The probability of the two cluster model goes up very nicely, so that again when they're apart, the probability of having two clusters is very high.",
            "So I guess the point of showing you guys that video is to say OK. You know if I was in a strictly extract marginal likelihood, kind of.",
            "I was in a strict maximum likelihood kind of setting the two cluster model would always be better, right?",
            "It would always say you know, in the worst case, kind of what we're going to do is we're going to shut off one of the clusters, right?",
            "But I'm always better off having that second cluster than not having that second cluster, and so you're really seeing this concept of regularization.",
            "And here and marginalization over the parameters where we have, we can compare the probability of the one cluster versus 2 cluster model.",
            "And we're finding the right kind of thing, or the thing that makes sense to us, kind of.",
            "Eyeballing the data set as the data pulls apart."
        ],
        [
            "OK, so this leads to a bit of a discussion about Bayesian Occam's Razor, so we might have a data set.",
            "The data set might look like I just made up the data set, kind of in the plot above, right?",
            "Like their objects, they have a certain size under certain color they might.",
            "We might be able to plot them like I laid out, and so the question then becomes like is this data set best modeled with one cluster within one cluster model with the cluster model or with the three cluster model?",
            "And you know how do we go about figuring that out?",
            "And so we can use the kind of marginal likelihood setting for for trying to determine the correct number of clusters that I just that I just showed you.",
            "OK, so when I compute a marginal likelihood, remember, I'm computing the probability of the data given the model automatically get an Occam's razor kind of affect where I say OK. Models that are too simple are not going to model the data set that I care about, or the data set that I have well and models that are too complex model the data set that I have well, but they model lots of different datasets well so they have their probability mass.",
            "So the probability of the data given the model it has to integrate to one right?",
            "Or it has to sum to one over all possible datasets, and so there's only a limited amount of probability mass.",
            "So if you're a function.",
            "They can model lots of different datasets.",
            "Well, you're spreading your probability mass.",
            "Then over all of the different datasets, whereas if you don't model the data set well, you might have a lot of probability mass on the datasets that you do model well.",
            "But if that doesn't capture the one that we're interested in, we're still not going to give high probability to our particular data set, so we don't want something that's true simple.",
            "We don't want like a line for a curve data set, right?",
            "That doesn't give high probability mass to our curve data set.",
            "But we also don't want something too complex, something that wiggles around all the time, right?",
            "Because that thing that that function that wiggles around all the time.",
            "We got lots of different ways to model lots of different datasets really well.",
            "We want something that's just write something that just wiggles enough to capture our data set well without being able to capture lots and lots of datasets well."
        ],
        [
            "OK, so this leaves maybe I'll just briefly mention nonparametric models, right?",
            "So they're like nonparametric Bayesian models.",
            "They're not parametric models in general.",
            "OK, so coming thinking of better clustering kind of situation.",
            "How do we know which which clustering models to compare?",
            "So we looked at comparing say like one versus 2 clusters or one versus 2 versus 3 clusters.",
            "But what if, like we have a data set which we don't even know how to visualize?",
            "Maybe it has like 150 clusters.",
            "Do we compare all of those different things?",
            "Do we compare 1234 probable wildly up 250?",
            "Doing a large number of these model comparisons is really costly and so non pennridge bays in general provides flexible priors over clustering models and allows us to infer the right number of clusters for data set.",
            "But nonparametric models in general allows that allows for this concept that we have an infinite set of parameters and we allow the number of parameters to grow as our data set size grows so you can remember maybe like way back I said.",
            "Like you know, having more complex functions is OK as we have more and more data right?",
            "So this will allow our complexity to rise as our amount of data rises.",
            "OK, so parametric models assume that there's some finite set of parameters or clusters that captures everything there is to know about the data.",
            "The complexity of the model is bounded, so we if we have three clusters, right?",
            "It doesn't matter how much data we see.",
            "You're only ever going to have three clusters, right?",
            "Our complexity is bounded, whereas if we have a nonparametric model right, we allow the number of parameters to grow as our amount of data grows, we can say OK as we see more data coming in.",
            "Maybe the number of clusters is going to go from 3:00 to 5:00 to 7:00, right?",
            "And maybe that's OK because we're getting more and more data and more and more evidence."
        ],
        [
            "OK.",
            "So that's my little spiel on Bayesian methods.",
            "I hope you guys enjoyed it.",
            "We're going to talk about now about slightly more sophisticated regression models, and we're going to do this again, like in prep for transitioning from linear regression.",
            "So we talked a lot about fitting a line to two things like generalized linear models and logistic regression.",
            "Because you're going to see that this is really an analogy for a neuron in a neural network, and people will go on to talk to you about neural networks, but you're really going to have to kind of have the basics of this before.",
            "You understand how neural networks or.",
            "So in the same way that we looked at linear regression, we can look at other regression models that allow us to learn nonlinear functions right?",
            "And so I think one of the big benefits of of neural networks has been that it allows us to learn nonlinear functions.",
            "We don't want to just be stuck in sort of like this linear paradigm, so in particular I'm going to talk about generalized linear models and logistic regression.",
            "OK, we can do the same thing that we've done in the past, which is screw my marginal likelihood integration situation.",
            "We're going to maximize the parameters we can.",
            "We can take gradients with respect to our parameters set, set our loss function equal to 0.",
            "The problem is that as we move to more complicated functions like generalized linear models or logistic regression, this solution, the solution for the weights, the solution for our parameters is not available to us in closed form and we've got to do something else like gradient descent.",
            "So again, you're going to learn more about different kinds of optimization techniques from future speakers."
        ],
        [
            "OK, so they're running example here.",
            "Is going to be this Donner party example.",
            "I should say that these are the slides that I sometimes teach off of and so I've stolen these innocence from my Department.",
            "OK, so.",
            "And I'll just read this to you really, briefly.",
            "So in 1846, the Donner and Reed families left Springfield, IL for California by covered wagon in July the Donner Party, as as it became known, reached Fort Bridger, Wyoming.",
            "There its leaders decided to attempt a new and untested route to the Sacramento Valley, having reached its full size of 87 people and 20 wagons.",
            "The party was delayed by a difficult crossing of the Wah Sache Range, and again in the crossing of.",
            "The desert West of the Great Salt Lake.",
            "The group became stranded in the Eastern Sierra Nevada mountains when the region was hit by heavy snows in late October.",
            "By the time the last survivor was rescued on April 21st, eighteen 4740 of the 87 members had died from famine.",
            "An exposure to extreme cold.",
            "OK, so."
        ],
        [
            "This leads to this is a very sad situation for them, but a very nice data set for us.",
            "And and so here we have 4545 members of the Donner Party.",
            "We have them listed by age, by sex and buys status mortality status, whether they died or survived.",
            "OK, so their age.",
            "So for person one.",
            "So you see, the first row is person one.",
            "Their age is 23 years old, their sexes, male and their statuses that they died before they were rescued.",
            "Person Two was 40 years old and female and survived etc etc.",
            "OK."
        ],
        [
            "So we can look at their status, their their mortality versus their gender, right?",
            "So we can look at the people who died and what gender they are, whether they died or survived.",
            "So of the males.",
            "20 of them died and 10 of them survived of the females 5:00 AM died and 10 of them survived.",
            "We can also look at things like mortality status versus each.",
            "You know, and and basically like what you're seeing in different ways in these pots is that you were more likely to die if you were male and you are more likely to die if you were older.",
            "Probably not a huge surprise.",
            "But we want to be able to do things like like, say, OK, both Agent."
        ],
        [
            "Under have an effect on somebody survival, but how is it that we can explore this relationship so we can set died to zero dive?",
            "Is R0 and survive to one.",
            "This isn't something that we can really fit with a linear model.",
            "We need something something more, something that's going to be able to tell us based on somebody's age and based on somebody's gender with the likelihood is that they they survived or died.",
            "OK, so one way to think about this is we can think about this is like a coin flip we can think about treating whether somebody survived or died as successes or failures that arise from like a Bernoulli trial, or like a coin flip where the probability of success or lookaheads is given by some kind of transformation of the linear model that we already learned about.",
            "So we already learned about this nice linear model and we just want to say OK, how do we take that and transform it to something which is going to give us?",
            "A likelihood of the person dying 0 or the person surviving one."
        ],
        [
            "OK, and so this leads to something called generalized linear models.",
            "So GLM's are very general ways of addressing these kinds of problems and logistic regression, which is the thing that we're particularly interested in, is just one kind of generalized linear model.",
            "And all she all challens have three things.",
            "The first thing is a probability distribution describing the outcome variable.",
            "So here the outcome is, did the person liver die right?",
            "And the probability distribution that we talked about that being was a Brimley right like liver die?",
            "It's like a coin flip kind of trial.",
            "They have a linear model like the linear model that we discussed, right?",
            "Y is equal to W 0 plus W1X.",
            "And then there's some link function and the link function relates the linear model to the outcome variable.",
            "Alright, and we call this one function G right?",
            "You might hear about it later on, 'cause it's like an activation function."
        ],
        [
            "OK, so logistic regression is a particular kind of GLM that's used to model binary categorical variable.",
            "So here a binary categorical variable variable is is the right thing to go after rate, like whether somebody lived or died is a binary variable.",
            "Using numerical and categorical predictors, right?",
            "So the things that were going off of like age or gender.",
            "Sometimes they can be numerical, right?",
            "Like ages, pretty numerical or categorical like are they male or female?",
            "So here we assume that a binary distribution produced the outcome variable, and so we want to model P the probability of a success for given set of predictors.",
            "So we want to be able to look at with their age what's their gender and then predict the probability of the liver die so.",
            "To specify the logistic model, we need to establish a link function that's going to connect yrr outcome liver die.",
            "Sorry yrr are linear function which which is some kind of waited some kind of weighted linear combination of their age and their age, their gender to P, the probability that they live or die.",
            "OK, so there are a lot of different options for this and you guys are going to see probably some more options for this, But the option that we use here is the logistic function.",
            "So the logistic function is log of P / 1 -- P for P between zero and one right so?",
            "Again, like the probability of seeing header tail.",
            "OK."
        ],
        [
            "Um?",
            "OK, so the logistic function takes a value between zero and one and Maps it to a value between negative Infinity and Infinity.",
            "So if we think about we want to be able to take this linear function and map it to between zero and one.",
            "This is kind of going the wrong way around right?",
            "And so we want to go the other way around.",
            "We want to take some value between negative Infinity Infinity and map it to some value between zero and one right?",
            "So we want to take the results of our linear are linear equation that we've learned write an, map it to some probability that they survived.",
            "So we use the inverse of the logistic function, so this is 1 / 1 plus Exp of negative extra Exp of X / 1 + X P of X is the inverse of the logistic function.",
            "Um?",
            "But right, and so that goes the way the way around that we that we want that allows us to take this linear equation and say what's the probability of life or death?",
            "OK, and so this can be OK. And then this interpretation.",
            "This can be useful for interpreting the model.",
            "Because they can just also just tell us what are the odds of success rate you can think of what you get coming out of this as being a probability right or probability of living."
        ],
        [
            "OK.",
            "So the.",
            "The three GLM criteria right are things that we talked about in terms of like here.",
            "Let's go back.",
            "We need a probability distribution describing the outcome variable.",
            "We need a linear model and we need a link function.",
            "Right, so here are probability distribution.",
            "Is the bernoulli.",
            "Our linear model is a linear combination of, say like age and gender of a weighted combination of age and gender and our link function is the logistic function.",
            "So from that we can get our probability of whether we think a particular person lives or dies, right?",
            "So if we look at the inverse of the logistic we get P coming out, P is the probability of a 1 right?",
            "And so it's just Exp of that our linear equation are linear weighting of age and gender over one plus Exp of our linear weighting of age and gender."
        ],
        [
            "OK, and so our logistic function looks like this, right?",
            "So we are no longer fitting align to our data.",
            "We are allowing this to look like some kind of curve which asymptotes out at one on one end an ASMR totes out at 0 on the other, and we can learn different logistic curves or logistic functions for different kinds of populations or different kinds of settings, right?",
            "So again, we can like learn different parameters, where the parameters.",
            "Are they like the weights of of our linear model?",
            "Um?",
            "OK, and so the logistic functions that you see here you have one for men and one for women, and you can notice that like the one for women out lies about the one for men because it was more likely that they would survive, right?",
            "So what's plotted here?",
            "The little triangles are the women and the open circles are then right.",
            "So this is the logistic function that we get coming out.",
            "OK, so I think that the."
        ],
        [
            "Was sort of all of the material that is sort of like we need to cover in order for you guys to really understand the rest of what's going on in the rest of the talks in the rest of the summer school, so hopefully you can jump, gotten something out of something that I've talked about and you can jump off of that into some of the other people's talks.",
            "So in summary, we looked at different kinds of machine learning rate, supervised learning, unsupervised learning reinforcement learning.",
            "We looked at different kinds of linear models and how to fit them today to when our models are too simple or too complex for the particular data set that we have overfitting a particular complicated function with a lot of parameters to a data set, which really requires a simpler function in order to generalize well.",
            "How we might prevent ourselves from doing that different kinds of regularization which allow us to prefer simpler functions even though more complicated functions are reducing our training error.",
            "More Bayesian techniques which are another way of viewing regularization and marginal likelihoods where we're integrating instead of maximizing, integrating over instead of maximizing our parameters, and then Lastly generalized linear models and logistic regression.",
            "How do we take that line that we learned and transform it into something like a binary classifier or something that's going to give us?",
            "Probability of death or living like in various kinds of situations, and that is through the logistic function.",
            "And that's what we learned about in logistic regression.",
            "Now we're no longer doing linear regression.",
            "We're allowing our regression curve to be nonlinear.",
            "And again, like I said, that's going to form the basis for neural networks that you guys are going to learn about coming up."
        ],
        [
            "OK, so I guess I have a few minutes of.",
            "How am I doing on time in my overtime overtime?",
            "OK, so I'll try to just I had at the NFU sides about kind of like the work that I did 'cause that's I mean.",
            "First off that's easiest for me to put in right?",
            "'cause this might work but also like it provides hopefully some motivation and to the kinds of reasons that you would be interested in learning about this kind of methodology and that we hope to take it and use it within the health care system in order to try to improve people's health and so I'll skip talking about that in a lot of detail.",
            "But I do want to.",
            "Leave you kind of like with a little bit of motivation about where things are headed, moving, moving forward so.",
            "So what?",
            "I talked to you about and what I presented you with a really just the basics and you guys are going to get a lot more details in the summer school as more people as more people go through their courses.",
            "The important thing to always keep in mind is that the primary purpose of all of the methodology that we talk about is really to be able to develop useful models for real data to be able to say OK, I'm going to enter into this real world setting.",
            "I'm going to take into it some of the knowledge.",
            "Some of the things that we know how to do and somehow make something better.",
            "Whether it's like to be able to have driverless cars that work better or to be able to improve the health of patients or make better treatment decisions.",
            "Right?",
            "That's kind of like our goal at the end of the day.",
            "So there are lots of really cool.",
            "Um?",
            "Things that are being done in the machine learning community and lots of these use more sophisticated methods than what I talked about in this talk.",
            "This is really again just the beginnings, but.",
            "I again we're interested in sort of having this kind of real world impact at the end of the day, whether it's through this simple models that I presented here or from the more sophisticated models that we've built up off of this."
        ],
        [
            "OK, so again, like the places that people are applying this to that it includes like self driving cars and involves sort of the stability of drones or robots it includes.",
            "Can we make product recommendations that are tailored for individuals right?",
            "Things that you were actually more interested in buying?",
            "Can we make criminal justice recommendations so one of the things that a lot of my friends are really interested in are things like algorithms.",
            "That get used to make a decision about whether somebody should be let out on parole or not, and so we do have automated algorithmic ways of trying to do this now, but they have all kinds of biases associated with them, and we're trying to eliminate, eliminate those the best extent possible.",
            "And also healthcare predictions is again an area that I work in a lot, right?",
            "Like might somebody be at risk for having developed sepsis in their time in the hospital?",
            "What does the course of their kind of kidney disease look like?",
            "What's their chance of having a complication coming out of surgery?",
            "What kind of multiple sclerosis?",
            "There's a particular patient have?",
            "These are all questions that I work on sort of on a daily basis."
        ],
        [
            "And then I should say it.",
            "Still, there's a lot that we don't know, and a lot that we.",
            "Really need the help of everybody is sitting here and trying to figure out, right so?",
            "Like questions that get bandied about a lot between me and my colleagues and things that we really don't have a good understanding of.",
            "So it's unquestionable that they're at least certain settings in which we've seen really substantial gains by using deep neural networks.",
            "Where do these game games come from, exactly?",
            "Right?",
            "Like how much of this is like we have access to more data.",
            "How much of this is the efficiency with which we can get through it?",
            "How much of it is the non linearity that I talked about?",
            "Like when we move from linear to logistic regression?",
            "How is regularization working right?",
            "Like there's clearly some kind of regularization that's necessary within within the learning of deep neural networks, and you often see this regularization come in.",
            "So we talked a little bit about regularization.",
            "You often see this regularization come in through things like early stopping or dropout methods that you'll learn about further on in this class, right?",
            "And so this is one way of increasing the amount of regularization going on.",
            "What exactly is going on here?",
            "What's it doing?",
            "Why do we see it giving us benefits?",
            "And then like how can we speed things up right?",
            "As we have more and more data and still have principle to purchase to what we're doing?",
            "OK.",
            "So thank you very much."
        ],
        [
            "Thank you for talking you go back to the regularization section.",
            "No, that's like.",
            "About the L1L2 Well, the one about the minimizing the.",
            "Maybe the next slide.",
            "Yeah this one.",
            "So here said that.",
            "Elwyn and L2 are penalizing the number of parameters square, is it?",
            "But I guess it's more like minimizing the the values of the parameters.",
            "Is that the better way to put it?",
            "Because it's really by setting the values to zero that we're checking.",
            "Yeah, you're you're right, and in this particular case, but I'm like intuitively right.",
            "Like you want to learn, sort of like the the.",
            "The least complicated function possible, and if you do something like you shrink somebody's weight to 0 right, you're kind of taking that parameter out of Adam consideration, right?",
            "And you're learning sort of like a less complicated thanks.",
            "Just wanted a clarification.",
            "So many people apply perturbations.",
            "For example like dropout to get some predictive uncertainty of deep learning, and many people thought like this is the way to integrate Bayesian methods, but still learning.",
            "So what's your opinion about this direction?",
            "About, sorry about that.",
            "Yeah about the direction of like integrating theory in Bayesian approach using kind of perturbation drop out.",
            "Like yeah no.",
            "I mean I think so so I'm not sure I'm familiar with exactly what you're referring to, but I think this whole kind of like direction of trying to integrate Bayesian and deep neural networks is really important right?",
            "Because both of those things they each have strengths that the other doesn't sort of like historically, right, so?",
            "So things like how might we?",
            "There are a lot of things that.",
            "Come up where people are like.",
            "I'm not sure how we do transfer learning or or start new categories or like.",
            "Or have like colder director perception or do attention in a principled manner or whatever in a deep neural networks setting.",
            "And you can do a lot of this using using Bayesian statistical methods, right?",
            "And so in fact.",
            "So there were a bunch of us on it on a gram on about combining, so actually combining it just started combining Bayesian statistical methods and deep neural networks, and it was interesting because we get this and we're looking at it.",
            "We're reading through it about all of the problems that deep neural networks have in terms of like learning how to deal with new data coming in and.",
            "Increasing transfer learning is starting new categories and stuff like that, and we're looking at we're like.",
            "This is totally not a problem like any Bayesian statistical method has, right?",
            "Like all of all of these things right?",
            "Like the problem businesses just call methods have is like their slow.",
            "You've gotta integrate.",
            "You've got it.",
            "You've got to evaluate integrals all the time, and things like that which are not problems that be that deep.",
            "Neural networks have right?",
            "So so it's like they each have their sets of strength.",
            "And I think that there's going to be a lot to gain by combining those types of strengths and putting them together.",
            "Hi, thanks for the lecture regarding the integral you just mentioned and.",
            "Combining basean and neural networks.",
            "For the marginal likelihood, can that be used in your example you gave the number of clusters in a model could be used for say the number of layers in a deep neural network.",
            "Something like that.",
            "Yeah, where are you like?",
            "I can't find you I'm over here.",
            "Feels really awkward to like, not know who you're talking about talking to.",
            "OK, I'm sorry.",
            "Can you repeat what you said?",
            "Yeah, so regarding the marginal likelihood, I actually have a technical question on that too.",
            "If you could go to that slide with yeah.",
            "Yeah, this one.",
            "So under the integral you have probability of Theta given M. Is that the prior probability.",
            "Yes, that is the probability.",
            "So here M just represents our model.",
            "So in this case it would be like a Gaussian model and then theater the model parameters so they would be like the Gaussian mean and covariance.",
            "So you gave the example of using this to decide how many clusters you should use in a clustering algorithm.",
            "Could you use it for how many layers to use in a deep neural network?",
            "Something like that?",
            "Yeah.",
            "Maybe I feel like I'm not the right person to ask about exactly how you would go about doing that, but like.",
            "It seems like something that might be helpful.",
            "I mean like I think I think the problem would be that, like right?",
            "So you have like some prior distribution.",
            "I could see you have some prior like thinking about it on the fly, but like I could see that you have some prior distribution over a number of layers and you want to integrate across all possible kind of like neural network structures.",
            "And and their powders.",
            "And I think that makes a lot of sense in general.",
            "I'm a big fan of marginalization.",
            "I think that would be like potentially really intractably slow.",
            "But yeah, it's something not, not.",
            "I mean sort of like the secret is like I don't.",
            "I don't actually do a lot of like deep neural networks, and so I've got.",
            "I've got.",
            "I've got one or two right?",
            "And so like, yeah, you might.",
            "You might be best off like also asking somebody who does do deep neural networks for an answer that question.",
            "Thank you.",
            "I also want to say to Catherine, we deliberately do this.",
            "We bring in people that don't do a lot of deep learning.",
            "Sometimes they give the intro lecture 'cause we want to get different perspectives.",
            "So I want to thank her for being really brave and getting on the stage and feeling your deep learning questions.",
            "So thank you.",
            "An one thing as you know, one great thing about this community is there's a lot of resources online for self study and learning, so there's courses out there.",
            "There's there's tax, there's different tutorials and so forth.",
            "So for someone say that knows about some deep learning and want to get more into the Bayesian statistics side, or even sort of Bayesian learning, is there anything that you would recommend as like a way to pick up more material and do it self study?",
            "Yeah, I'm not going to have anything like really super useful to say here, but I mean, it's just like there are some classic books and statistics, right?",
            "Like the Bayesian data analysis book and things like that that are worth looking into your take some statistics classes.",
            "I would say.",
            "I mean Juke is a very.",
            "Bias place in that, like Duke, is incredibly busy and right, like Duke, is the Bayesian Mecca of the United States or maybe even North America an is.",
            "It's probably why they existed Department could kind of stomach hiring me because they're like they're not really like a statistician near like a computer scientist, but you're busy in an absolute way.",
            "I guess maybe you're OK.",
            "So my training was in the Kingdom and people tend to be more Bayesian there.",
            "I actually lived like 2 blocks away from the tomb of Thomas Bayes for awhile.",
            "But yeah, I mean certainly finding like a Bayesian class.",
            "I mean, so this is my way of saying, like even if you take a statistics class depending on where you are like they might not have like the same Bayesian mantra that like where I was trained and where I am currently has.",
            "But if you look for.",
            "Sort of like getting together.",
            "Your background is just shapes and then trying to specifically aim to take Bayesian classes.",
            "That's probably incredibly helpful."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks very much Graham.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I'm here to talk to you today about just sort of like the basics that you're going to need going into other talks in the summer school, I guess before I get started I should also kind of give a shout out to see far who is very kindly supporting me.",
                    "label": 0
                },
                {
                    "sent": "Being here with my kids, which is not something that you often find particularly.",
                    "label": 0
                },
                {
                    "sent": "Where I'm from, so I really appreciate that.",
                    "label": 0
                },
                {
                    "sent": "OK, so an introduction to machine learning.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are a bunch of things that so I guess.",
                    "label": 0
                },
                {
                    "sent": "I guess I should stop here and say this is going to get a little technical pretty quickly.",
                    "label": 0
                },
                {
                    "sent": "If you're expecting really technical, it's not going to get really technical at all, but it's going to get into sort of like the details of what we're what we're doing in machine learning pretty quickly.",
                    "label": 0
                },
                {
                    "sent": "And so you guys have the background to go into the rest of the talks and kind of understand what's going on.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to be talking about the different kinds of machine learning, so a lot of you guys are here for the.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning workshop.",
                    "label": 0
                },
                {
                    "sent": "As well as this one, and so I'll touch briefly on what makes reinforcement learning different from other kinds of machine learning, I'm going to discuss linear regression, which is sort of one of the most basic techniques, and one of the ones that you're going to have to understand in order to understand neural networks regularization.",
                    "label": 0
                },
                {
                    "sent": "Why we actually care about Regularising things?",
                    "label": 1
                },
                {
                    "sent": "Bayesian methods, which are largely what I work on.",
                    "label": 0
                },
                {
                    "sent": "Logistic regression again, coming off of linear regression.",
                    "label": 1
                },
                {
                    "sent": "Add something again.",
                    "label": 0
                },
                {
                    "sent": "You're going to need to know to understand neural networks, and then I'm going to answer sort of that.",
                    "label": 0
                },
                {
                    "sent": "That kind of concludes the amount of material I really need to cover as like basic material to go into the rest of the rest of the course, but I wanted to talk for a little bit about why it is that we do this right, because we all do machine learning.",
                    "label": 0
                },
                {
                    "sent": "Machine learning is, I guess getting to be more and more trendy everyday and like what are the kinds of applications?",
                    "label": 0
                },
                {
                    "sent": "Or or areas in which we think that we can impact society with what we do.",
                    "label": 0
                },
                {
                    "sent": "And so I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk a little bit about that.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to talk a little bit about what I do personally, being mostly applications to health care and why I find that to be motivating.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me start with.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What are the different kinds of machine learning?",
                    "label": 1
                },
                {
                    "sent": "So machine learning is typically broken down into three different categories.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not a perfect breakdown, but you'll hear people talk about supervised learning or reinforcement learning or unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "And most of the work that I've done personally has fallen under unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "But I think a lot of the work that you're going to be hearing about here is supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And until the Reinforcement Learning Workshop where you're going to hear about reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that supervised.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Running takes in some set of inputs.",
                    "label": 1
                },
                {
                    "sent": "Those inputs could be.",
                    "label": 0
                },
                {
                    "sent": "Any set of numbers they could be vital signs of a patient in the hospital and Maps them to an output URL label right?",
                    "label": 0
                },
                {
                    "sent": "They could be that could be like are you septic or not septic?",
                    "label": 0
                },
                {
                    "sent": "Something that you'll see probably really commonly is images right?",
                    "label": 0
                },
                {
                    "sent": "So it could be like image features X one through XD and it could be mapped to an output like.",
                    "label": 0
                },
                {
                    "sent": "Is this a cat or not a cat?",
                    "label": 0
                },
                {
                    "sent": "OK, so our goal in supervised learning is to be able to find a good function F which does this mapping for us.",
                    "label": 1
                },
                {
                    "sent": "So what can we use to take in these features of, say, like our CAT image and be able to say whether or not this is a cap?",
                    "label": 0
                },
                {
                    "sent": "So whatever the function is, we'd like it to be able to map our inputs to the correct label all of the time.",
                    "label": 1
                },
                {
                    "sent": "We'd like it to be able to correctly say this is a cat image.",
                    "label": 0
                },
                {
                    "sent": "This is not a cat image 100% correctly every single time, including on new examples, right?",
                    "label": 0
                },
                {
                    "sent": "We can't do that in practice, like we're never going to get everything perfectly correctly, and So what we really need is some kind of metric or some kind of way of telling us how close we are to this ideal.",
                    "label": 0
                },
                {
                    "sent": "How close are we to being able to say it's to say, OK, I have this function which takes some set of features and some or some inputs and Maps it to some level cat or not cat.",
                    "label": 0
                },
                {
                    "sent": "So this involves choosing some kind of loss function.",
                    "label": 1
                },
                {
                    "sent": "You'll probably hear about loss functions quite a bit, and the loss function could be something like.",
                    "label": 0
                },
                {
                    "sent": "What are the number of times we get it wrong, right?",
                    "label": 0
                },
                {
                    "sent": "Like what are the number of times we say cat when it's not a cat, or how close we are to the correct label?",
                    "label": 0
                },
                {
                    "sent": "So if we're looking at some things that the label doesn't have to be binary 01, it could be something like I'm trying to predict what your height is.",
                    "label": 0
                },
                {
                    "sent": "So how close did I get right?",
                    "label": 1
                },
                {
                    "sent": "How far away am I?",
                    "label": 0
                },
                {
                    "sent": "And then we want to minimize the loss using the data set that we have, right?",
                    "label": 0
                },
                {
                    "sent": "So we're looking at doing something like training.",
                    "label": 0
                },
                {
                    "sent": "We get some set of labels that we know, and then we say, OK, you know what are what.",
                    "label": 0
                },
                {
                    "sent": "What is the function that we can learn which will minimize the amount of loss between being able to predict the labels that we know like cat or not cat from images that we already have in our data set.",
                    "label": 0
                },
                {
                    "sent": "And then we try to minimize minimize the distance.",
                    "label": 0
                },
                {
                    "sent": "Between basically like the perfect or the ideal function and the function that we're learning.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are some examples of supervised learning.",
                    "label": 1
                },
                {
                    "sent": "Again, so object detection.",
                    "label": 0
                },
                {
                    "sent": "So here we're looking at, say, classifying images as sheep or cows.",
                    "label": 0
                },
                {
                    "sent": "I actually did see this once I'm to talk, so I'm not making it up completely.",
                    "label": 0
                },
                {
                    "sent": "What I work on again is mostly healthcare application, so we do this kind of supervised learning when we're trying to do something like predicting surgical complications which doesn't have nearly as nice images but but you are working in a regime that is not necessarily just like pictures all the time.",
                    "label": 0
                },
                {
                    "sent": "So, for example, the kinds of inputs that we have might be what kind of surgery did the person have?",
                    "label": 0
                },
                {
                    "sent": "What is the patient's age?",
                    "label": 0
                },
                {
                    "sent": "What are the other kinds of illnesses?",
                    "label": 0
                },
                {
                    "sent": "The patient suffering from?",
                    "label": 0
                },
                {
                    "sent": "What's their DNR status?",
                    "label": 0
                },
                {
                    "sent": "What surgeon is performing performing the surgery, or how like, what's their nutritional status?",
                    "label": 0
                },
                {
                    "sent": "Are they healthy?",
                    "label": 0
                },
                {
                    "sent": "Have they have they lost a lot of weight?",
                    "label": 0
                },
                {
                    "sent": "That kind of thing?",
                    "label": 0
                },
                {
                    "sent": "And from all of these things we're trying to do something like predict a particular complication that they might have, or they might not have after having the surgery, right?",
                    "label": 1
                },
                {
                    "sent": "So are they likely to have a bleeding complication of breathing complication or neurological complication or some kind of?",
                    "label": 0
                },
                {
                    "sent": "Infection right?",
                    "label": 0
                },
                {
                    "sent": "So we look at at sort of predicting zero or one for each of those.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's basically an overview of supervised learning reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning is learning what actions to take based on our reward signal, and again, this is going to get covered, probably like in the reinforcement Learning Workshop in a few days so.",
                    "label": 1
                },
                {
                    "sent": "I don't have this here, but like one of the one of the reasons that we look at reinforcement learning is to be able to say OK, we want to know in a hospital setting what kind of actions or interventions we should take in order to sort of like lead to a situation in which the patient is healthiest, the healthiest they possibly can be.",
                    "label": 0
                },
                {
                    "sent": "But a lot of where you see reinforcement learning in the literature is again things like playing games, and I'll show you that in a second, so you have some kind of state.",
                    "label": 0
                },
                {
                    "sent": "Usually there's a state of the world that you're in, so if you think of, like, say, like the game of go, you could be in a particular situation where the board looks a particular way.",
                    "label": 0
                },
                {
                    "sent": "So then you evaluate your chances of getting to another state by taking a particular action by making a particular move right.",
                    "label": 0
                },
                {
                    "sent": "Like so you're going to have a board that changes to look another particular way.",
                    "label": 0
                },
                {
                    "sent": "What are the chances given that I make a particular move right, and so in the end, or sometimes along the way, or reward signal is received.",
                    "label": 1
                },
                {
                    "sent": "So if you win the game you get some reward.",
                    "label": 0
                },
                {
                    "sent": "If you lose the game you get some kind of negative reward.",
                    "label": 1
                },
                {
                    "sent": "Or you don't get a reward and we want to be able to take actions or make moves right in our game of go that lead to getting the most reward possible in expectation.",
                    "label": 1
                },
                {
                    "sent": "So we learn a set of actions or set of moves to make in a say in the game.",
                    "label": 0
                },
                {
                    "sent": "Or a policy they call this a policy which leads to maximizing the reward that we expect at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "OK, so reinforcement learning all about learning what actions you might take in order to maximize your expected reward.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so examples of reinforcement learning that you see a lot are things like the game of go right?",
                    "label": 1
                },
                {
                    "sent": "So there's the Alphago paper or movie where where they used reinforcement learning to have an automatic automated way of deciding on moves in the game of go.",
                    "label": 0
                },
                {
                    "sent": "They've also used it for doing things like trying to play Atari games.",
                    "label": 0
                },
                {
                    "sent": "Well, I've used it myself in order to try to decide what medical interventions.",
                    "label": 0
                },
                {
                    "sent": "Are most effective in a hospital setting again, unlike the previous thing which I talked to you about, I think doing this is even a step farther for the hospital, right?",
                    "label": 0
                },
                {
                    "sent": "In that they can kind of get behind the idea that you have an automated system which their physicians see, which might predict that a patient has or doesn't have something.",
                    "label": 0
                },
                {
                    "sent": "But when you start recommending ways to treat them through like reinforcement learning, they currently sketch out a little bit more about that, so I think that's a little farther off on the horizon.",
                    "label": 0
                },
                {
                    "sent": "But I have certainly used it.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, I have certainly used it in order to try to try to an automated way, decide what medical interventions would be most effective for a particular patient.",
                    "label": 0
                },
                {
                    "sent": "So here the state is the health of the patient, like vital science, right?",
                    "label": 1
                },
                {
                    "sent": "In a way that we can measure it.",
                    "label": 0
                },
                {
                    "sent": "The action that we take is a particular medical intervention, right?",
                    "label": 0
                },
                {
                    "sent": "So it might be administering Ivy fluids or antibiotics, and then the reward comes from like did the patient liver not?",
                    "label": 0
                },
                {
                    "sent": "Did the patient live plus 10?",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and last and last we have unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So in unsupervised learning and again most of the work that I've done actually falls into this category.",
                    "label": 0
                },
                {
                    "sent": "An unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "You don't have any labels, right?",
                    "label": 0
                },
                {
                    "sent": "So there's no training in the sense that, like you have an image and you take the image features and you try to use this to predict, is this a sheep?",
                    "label": 0
                },
                {
                    "sent": "Or is this a cow, right?",
                    "label": 0
                },
                {
                    "sent": "You're just looking to try to find like interesting and informative patterns in your data.",
                    "label": 1
                },
                {
                    "sent": "So one of the one of the things that I've really worked on and.",
                    "label": 0
                },
                {
                    "sent": "Is the most common maybe example of this is like clustering data.",
                    "label": 0
                },
                {
                    "sent": "So you want to find groups of things that are going on in your data and this could come about for many many reasons.",
                    "label": 0
                },
                {
                    "sent": "One of the things that I do again in in a medical setting is say we have some mobile app data.",
                    "label": 0
                },
                {
                    "sent": "The mobile app data comes from patients with neurological disease.",
                    "label": 0
                },
                {
                    "sent": "Can we find subpopulations within that data, right?",
                    "label": 0
                },
                {
                    "sent": "Can we find subpopulations?",
                    "label": 0
                },
                {
                    "sent": "Let's say react to a medication differently.",
                    "label": 0
                },
                {
                    "sent": "Can we find some populations that are having a different disease course and that's not a supervised kind of problem?",
                    "label": 0
                },
                {
                    "sent": "We don't have actual labels on that, but we can look in an unsupervised way at the structure of our data and try to make an assessment.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so clustering clustering is the act of grouping objects or data points together based on common features.",
                    "label": 1
                },
                {
                    "sent": "So clustering is natural or grouping things together is natural.",
                    "label": 0
                },
                {
                    "sent": "You could group together athletes by what kind of sport they play.",
                    "label": 0
                },
                {
                    "sent": "You could group together people by what gang there in you could group together.",
                    "label": 0
                },
                {
                    "sent": "I like lots of different things.",
                    "label": 0
                },
                {
                    "sent": "OK, so on this slide I have an example of a few objects right?",
                    "label": 0
                },
                {
                    "sent": "So I could choose to group the fruit objects together into one group and the animal objects together to another group, right?",
                    "label": 0
                },
                {
                    "sent": "But you know why is it that I'm interested in making this grouping first half right?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the reason why I'm interested in learning about this grouping is because it helps me do some kind of prediction for like a new object, right?",
                    "label": 0
                },
                {
                    "sent": "So if I see the new object which is given there in the middle right?",
                    "label": 0
                },
                {
                    "sent": "I want to.",
                    "label": 0
                },
                {
                    "sent": "I might want to predict some things about it.",
                    "label": 0
                },
                {
                    "sent": "Like does it breathe water?",
                    "label": 0
                },
                {
                    "sent": "It sells like that kind of thing and I can actually use this kind of grouping or clustering to make to make predictions about what it is that this new object does from extrapolating from the other Members.",
                    "label": 0
                },
                {
                    "sent": "Members of its group without actually having to go ahead and measure things like is it breathing or what's it, cell type like.",
                    "label": 0
                },
                {
                    "sent": "There aren't any inherently correct clustering, and this is something that you're going to find a lot of an unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "You're not necessarily in a situation where you get like the right solution, only only clusterings that are right for a particular problem, right?",
                    "label": 1
                },
                {
                    "sent": "So if we look at the objects that I'm showing you right here, one of the things one of the ways that we might be interested in grouping them together is not by fruit versus animals, but by things that hang on trees and things that don't write, and we would get.",
                    "label": 0
                },
                {
                    "sent": "A different, a different grouping, or a different clustering if that's what we were interested in.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so like I said another example of unsupervised learning is learning different hidden factors that make patient populations.",
                    "label": 1
                },
                {
                    "sent": "Patient population is different hospital so OK, so this is more like.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we're confronted with situations where we say, like have a national national national data set, right?",
                    "label": 0
                },
                {
                    "sent": "So first surgical complications.",
                    "label": 0
                },
                {
                    "sent": "We have a national database of, like all of the different people who went through surgery and all of the participating hospitals and what their complications were.",
                    "label": 0
                },
                {
                    "sent": "And we want to be able to do things like figure out how is it that I can take that national level data and use it to make the best predictions possible at my own local hospital right at Duke University Hospital for me.",
                    "label": 0
                },
                {
                    "sent": "This isn't this isn't exactly given to us by our national database because Duke University Hospital as a single hospital is going to have a bit of a different patient population than the nation as a whole, right?",
                    "label": 0
                },
                {
                    "sent": "You could have different kinds of hospitals.",
                    "label": 0
                },
                {
                    "sent": "Some of these are community hospitals where they are seeing a different a very different subset of the population, and so one of the things that I've worked on is is being able to leverage this large national database in order to.",
                    "label": 0
                },
                {
                    "sent": "Learn these hidden factors and make them make better predictions locali.",
                    "label": 0
                },
                {
                    "sent": "And another thing that that I talked to you guys about already is I've also used clustering to look at people who suffer from particular neurological disease or the clustering of mobile app data to look at people who suffer from a particular neurological disease.",
                    "label": 1
                },
                {
                    "sent": "So like, why are you tired?",
                    "label": 0
                },
                {
                    "sent": "It might be because of your disease.",
                    "label": 0
                },
                {
                    "sent": "It might be because of your medication medication that you're on.",
                    "label": 0
                },
                {
                    "sent": "It might be because you're depressed, or it might be because you just haven't gotten good sleep over the last few nights, right?",
                    "label": 0
                },
                {
                    "sent": "And these are all very different kinds of explanations.",
                    "label": 0
                },
                {
                    "sent": "And what the what kind of category you as an individual individual follow into is going to help lead to different treatment decisions.",
                    "label": 0
                },
                {
                    "sent": "And so we really want to understand that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that pretty much covers like the basics of like what supervised learning is what unsupervised learning is and what reinforcement learning is an what.",
                    "label": 0
                },
                {
                    "sent": "What regimes or paradigms they might be able.",
                    "label": 0
                },
                {
                    "sent": "They may be able to be useful to us going forward, but now sort of like more pragmatically, how is it that we might go about trying to solve a supervised learning problem or solve an unsupervised learning problem?",
                    "label": 0
                },
                {
                    "sent": "OK, so the first thing that we start out with is a data set.",
                    "label": 0
                },
                {
                    "sent": "Right, so you have some kind of set of data.",
                    "label": 0
                },
                {
                    "sent": "This data involves a whole bunch of features.",
                    "label": 1
                },
                {
                    "sent": "They could be features of an image.",
                    "label": 0
                },
                {
                    "sent": "They could be features of a patient who's in the hospital.",
                    "label": 1
                },
                {
                    "sent": "If we're doing supervised learning, which is the 1st place we're going to, we're going to go right.",
                    "label": 0
                },
                {
                    "sent": "We have some kind of additional column in our data matrix, which is the label, right?",
                    "label": 1
                },
                {
                    "sent": "Like?",
                    "label": 0
                },
                {
                    "sent": "Is this patient septic or not septic?",
                    "label": 0
                },
                {
                    "sent": "Is this image an image of a chypre account?",
                    "label": 0
                },
                {
                    "sent": "So if the labels are discrete, like what I was just talking about, this is a good place to do what's called classification.",
                    "label": 0
                },
                {
                    "sent": "So can we classify this image is being a sheep image versus can we classify this image is being a cow image if the labels are continuous we can do something called the regression which is say exactly where in this continuous spectrum do we think that we lie given?",
                    "label": 0
                },
                {
                    "sent": "The input that we're seeing.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "We might have more complex labels and somebody will probably talk about this with you guys later on.",
                    "label": 0
                },
                {
                    "sent": "This is called structured prediction.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's go back to supervised learning for a second, so this is really just bringing up this slide again.",
                    "label": 1
                },
                {
                    "sent": "So OK, so our goal in supervised learning is to take some set of inputs, like some some features of the patient in the hospital or some features of an image and make some kind of prediction, say like let's say a classification or or or regression prediction.",
                    "label": 1
                },
                {
                    "sent": "We want to predict say like what this person's pulse is going to be, right like something that's a little bit more continuous and that we're probably not going to do classification for.",
                    "label": 0
                },
                {
                    "sent": "How is it that we do this?",
                    "label": 0
                },
                {
                    "sent": "How is it that we find the appropriate function that Maps are that Maps our input space to our label space?",
                    "label": 0
                },
                {
                    "sent": "What does the function look?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like right?",
                    "label": 0
                },
                {
                    "sent": "And so the set of functions that we can use in order to do that.",
                    "label": 0
                },
                {
                    "sent": "Do this comes from some kind of hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "Hypothesis class is really the terminology that people use often when describing this, so the hypothesis classes like the space of all possible functions that we're considering.",
                    "label": 0
                },
                {
                    "sent": "And so they can include certain function groups or function types so the functions can be like linear functions.",
                    "label": 0
                },
                {
                    "sent": "So say we're considering all possible linear functions, or all possible polynomial functions, right?",
                    "label": 0
                },
                {
                    "sent": "So the scatter plot at the bottom right shows the relationship between US high school graduation and the percentage of people living in poverty.",
                    "label": 1
                },
                {
                    "sent": "What kind of hypothesis class might be appropriate?",
                    "label": 1
                },
                {
                    "sent": "Let me ask you guys what kind of hypothesis costs might be appropriate.",
                    "label": 0
                },
                {
                    "sent": "Yeah, linear OK so so we might be able to fit this scatterplot.",
                    "label": 0
                },
                {
                    "sent": "Would say like a line, right?",
                    "label": 0
                },
                {
                    "sent": "There might be a linear relationship between US high school graduation rates and the percentage of people living in poverty.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we can like look at all of the different lines that we might be able to fit to this data.",
                    "label": 0
                },
                {
                    "sent": "How do we know which one is correct?",
                    "label": 0
                },
                {
                    "sent": "OK, so which which of the four lines do you guys think looks better?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so right.",
                    "label": 0
                },
                {
                    "sent": "So all of these all of these functions, all of these linear functions are in our hypothesis class, right?",
                    "label": 0
                },
                {
                    "sent": "They're all linear functions and we want to be able to like learn the best one, and the best one means the one that fits to the data and tells us the most about what the data looks like.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we determine that?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this gets into fitting a line, right?",
                    "label": 1
                },
                {
                    "sent": "So if we have this hypothesis space that are these linear functions, the function itself can be described by an equation of the form y = W zero plus W1X, right?",
                    "label": 1
                },
                {
                    "sent": "So we're in two dimensions here.",
                    "label": 0
                },
                {
                    "sent": "In general, if we are in more than two dimensions, we can have y = W zero plus W1X1 plus W2X2 plus blah blah blah on.",
                    "label": 0
                },
                {
                    "sent": "To however many dimensions we have right, we can sum from zero from zero to DWIXI or just say W transpose X.",
                    "label": 0
                },
                {
                    "sent": "So here WXR G + 1 length vectors W are often called.",
                    "label": 0
                },
                {
                    "sent": "The the weights you can think of them as parameters of the model right there.",
                    "label": 0
                },
                {
                    "sent": "The things that we want to learn right?",
                    "label": 1
                },
                {
                    "sent": "So they really define what are linear function looks like, whether it's like.",
                    "label": 0
                },
                {
                    "sent": "Air B or C. Your D right like these are going to have to do with the WS.",
                    "label": 0
                },
                {
                    "sent": "And that's why we care about learning the best ones possible.",
                    "label": 0
                },
                {
                    "sent": "OK, so so then this brings up the question of like how do we measure the quality of the lines fit?",
                    "label": 1
                },
                {
                    "sent": "Or how do we choose the best W right?",
                    "label": 0
                },
                {
                    "sent": "Since we've we've determined that W is the thing that we really care about.",
                    "label": 0
                },
                {
                    "sent": "W learning W is what's going to help us learn that this is line a that line A is the best line.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the way that we can think about it is by saying OK there are for any particular line that we learn, there are residuals or there's a difference between the data that we actually observe and that particular line.",
                    "label": 1
                },
                {
                    "sent": "And if we're learning the best line possible, what we really want to do is like learn the line.",
                    "label": 0
                },
                {
                    "sent": "That's going to minimize those residuals right?",
                    "label": 0
                },
                {
                    "sent": "So another way of saying this is like we have some data set.",
                    "label": 0
                },
                {
                    "sent": "The line that's going to be the best.",
                    "label": 0
                },
                {
                    "sent": "That's going to look the best to us.",
                    "label": 0
                },
                {
                    "sent": "Is the line that minimizes, like the distance, that each data point is off the line, right?",
                    "label": 0
                },
                {
                    "sent": "Whether it's positive or negative, right?",
                    "label": 0
                },
                {
                    "sent": "Like we really want to try to minimize that difference and so we can learn those parameters.",
                    "label": 0
                },
                {
                    "sent": "We can learn those W's in order to do this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's here's an example of that right?",
                    "label": 0
                },
                {
                    "sent": "Like you can see where these are, since these are each dot is a state in the United States or Washington DC, right?",
                    "label": 0
                },
                {
                    "sent": "And so you can look at where each dot wise with respect to the line that we've we've learned.",
                    "label": 0
                },
                {
                    "sent": "We can look at the distance between that particular dot that applauded on the where the Y axis is the percentage of people who live in poverty, and the X axis is the percentage of people who graduated from high school.",
                    "label": 0
                },
                {
                    "sent": "And we can compute the distance between that data point and the line that we're learning, right?",
                    "label": 0
                },
                {
                    "sent": "So for Washington DC, that's 5.44.",
                    "label": 0
                },
                {
                    "sent": "For Rhode Island, that's negative 4.16 right?",
                    "label": 0
                },
                {
                    "sent": "And this is what we mean when we talk about residuals just the distance that the data is off of the line up for learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so how good or bad it are is our line.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We want the line that had the smallest residuals possible.",
                    "label": 0
                },
                {
                    "sent": "We want the line that's really going to minimize this distance.",
                    "label": 0
                },
                {
                    "sent": "And the way that we get that is, we want to minimize the sum of squared residuals.",
                    "label": 1
                },
                {
                    "sent": "We want to minimize that distance people on a final wine with the weights that minimizes that distance summed over each data point, right?",
                    "label": 0
                },
                {
                    "sent": "And this is called least squares.",
                    "label": 0
                },
                {
                    "sent": "So why do we do least squares in this linear Russian setting in order to learn the weights W. The reasons why we use least squares or that you see people talking about least squares.",
                    "label": 0
                },
                {
                    "sent": "So First off, it's really commonly used.",
                    "label": 0
                },
                {
                    "sent": "People people talk about it a lot.",
                    "label": 0
                },
                {
                    "sent": "Square is a nicer function than the absolute value function, right?",
                    "label": 1
                },
                {
                    "sent": "So if you think about the distance between DC and that line being 5.4 four, we could just keep it as that.",
                    "label": 0
                },
                {
                    "sent": "We have that negative 4.16.",
                    "label": 0
                },
                {
                    "sent": "We don't actually want to negative, right?",
                    "label": 0
                },
                {
                    "sent": "We want to say the distance from that data point.",
                    "label": 0
                },
                {
                    "sent": "The Rhode Island data point to the line is 4.16, right?",
                    "label": 0
                },
                {
                    "sent": "So we could just absolute value everything, right?",
                    "label": 0
                },
                {
                    "sent": "But it's really much nicer to work with the square function, and I think other people will get into this more than me.",
                    "label": 0
                },
                {
                    "sent": "It's really much nicer to work with the square function, which is nice and continuous than it is to work with an absolute value function.",
                    "label": 0
                },
                {
                    "sent": "And in many applications, for many applications that we're going to consider having twice the residual or having the data point like twice as far off the line is more than twice as bad, right?",
                    "label": 1
                },
                {
                    "sent": "Like having that extra distances becomes really really, really bad very quickly, and so having this sum of squares distance having this least squares kind of paradigm is going to be appropriate when we care when we care about the distance, the distance that did the point lies from the line a lot.",
                    "label": 0
                },
                {
                    "sent": "Where farther away is much, much worse.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we use minimizing the square?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Residuals define W, right?",
                    "label": 0
                },
                {
                    "sent": "So we've got this concept of the residuals.",
                    "label": 0
                },
                {
                    "sent": "We've got this concept that the data point lies some distance from the line.",
                    "label": 0
                },
                {
                    "sent": "How do we use that to find our weights?",
                    "label": 1
                },
                {
                    "sent": "So really, what we want to do, we talked about like the best fit line, the weights that we want are the ones that are going to give us the line that minimizes the residuals.",
                    "label": 0
                },
                {
                    "sent": "So we just do argument over W the sum of our residuals, right?",
                    "label": 0
                },
                {
                    "sent": "So we say we want to minimize what are residuals are with respect to the W that we find, right?",
                    "label": 0
                },
                {
                    "sent": "So in this way, in this way we find the W that we're looking for.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is computed sort of like.",
                    "label": 0
                },
                {
                    "sent": "The way that we would compute minimizing anything the way that you were kind of taught in high school or in college math, right?",
                    "label": 0
                },
                {
                    "sent": "We compute this by taking the gradient with respect to W, and by setting this this function equal to 0.",
                    "label": 1
                },
                {
                    "sent": "And you're going to learn a lot.",
                    "label": 0
                },
                {
                    "sent": "I think there's going to be a lot of this summer school which is devoted to ways of taking the gradient with respect to W. Setting it equal to 0 and solving right, and so in this particular case, in this linear regression case, we can get a solution from doing this in closed form, right?",
                    "label": 0
                },
                {
                    "sent": "So it turns out that WW the is given by that closed form equation.",
                    "label": 0
                },
                {
                    "sent": "There W = X transpose XX, transpose Y.",
                    "label": 0
                },
                {
                    "sent": "But that's often not the case.",
                    "label": 0
                },
                {
                    "sent": "You often can't get W in closed form, and so a lot of people are going to talk to you about various kinds of optimization methods that you might use when you can't actually find W in closed form.",
                    "label": 0
                },
                {
                    "sent": "So that's where this is kind of all heading.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in our example, we can, and so we learned that for the line of interest that the percentage of people living in poverty is equal to 64.68 -- .6 two times the percentage of high school graduates, right?",
                    "label": 0
                },
                {
                    "sent": "So we can learn our weight parameters for the line that we're interested in for our particular data set.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so then the next question is how do we know that a line is right, right?",
                    "label": 1
                },
                {
                    "sent": "So we looked at that particular data set at that particular scatter plot and we were like that looks linear.",
                    "label": 0
                },
                {
                    "sent": "It looks like something a line would fit.",
                    "label": 0
                },
                {
                    "sent": "Well, we can't always visualize the data set that we're working with.",
                    "label": 0
                },
                {
                    "sent": "We, you know, we don't always know by visualizing it even right like what what works best.",
                    "label": 0
                },
                {
                    "sent": "And So what are the pluses and minuses of using a line if we shouldn't be using a line or using something more complicated or whatever.",
                    "label": 0
                },
                {
                    "sent": "And so really hear the danger is that we can underfit or overfit to our data, right?",
                    "label": 1
                },
                {
                    "sent": "So we have some particular data set.",
                    "label": 0
                },
                {
                    "sent": "I stole this I think.",
                    "label": 0
                },
                {
                    "sent": "Maybe like from online I think maybe it was from measuring originally I don't know.",
                    "label": 0
                },
                {
                    "sent": "But but here's here's some example of like a data set.",
                    "label": 0
                },
                {
                    "sent": "Where we can learn some kind of function?",
                    "label": 0
                },
                {
                    "sent": "The function is given in blue, right and so you see with the blue function over on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "We're learning a line to fit something that's probably not linear, right?",
                    "label": 0
                },
                {
                    "sent": "In this case, we're underfitting.",
                    "label": 0
                },
                {
                    "sent": "We're finding a function that's too simple, and so this is going to lead to hide bias, right?",
                    "label": 0
                },
                {
                    "sent": "So another way of saying this is that we're going to have large training error.",
                    "label": 0
                },
                {
                    "sent": "Right, so the you know the best fit line that we have is the line that shown, but we're going to have high residuals.",
                    "label": 0
                },
                {
                    "sent": "We can have the function that shown in the middle which looks like just right and then we can have the function that we see on.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right hand side which is like very quickly and we call this high variance.",
                    "label": 0
                },
                {
                    "sent": "So if you think about variance from your math class, probably back in high school it means that it bounces around a lot and so one of the problems that we really face in machine learning is that if we fit a function that's too complex to our data and it bounces around too much, we're going to get really low training.",
                    "label": 1
                },
                {
                    "sent": "You're right, like it's going to be able to fit our training data set really, really well.",
                    "label": 0
                },
                {
                    "sent": "But we're going to have really high test error.",
                    "label": 0
                },
                {
                    "sent": "It's not going to generalize that well to new data.",
                    "label": 0
                },
                {
                    "sent": "It's like just bouncing around way too much.",
                    "label": 0
                },
                {
                    "sent": "And so we want to be able to avoid that.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to avoid that concept of overfitting using some kind of function that's too complex, and we want to be able to avoid underfitting, which is using a function that is too simple and be able to find this function.",
                    "label": 1
                },
                {
                    "sent": "That's kind of just right just right for our data.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so as we get more and more data we can consider more and more complex hypothesis because we have more and more evidence about what that function looks like.",
                    "label": 0
                },
                {
                    "sent": "But I guess the question is what happens when we consider functions that have a lot of parameters compared to our data set size.",
                    "label": 1
                },
                {
                    "sent": "What happens if we consider functions which are two weekly for our data?",
                    "label": 0
                },
                {
                    "sent": "Endlessly this too.",
                    "label": 0
                },
                {
                    "sent": "Just a discussion of maximum likelihood, right?",
                    "label": 0
                },
                {
                    "sent": "So how are we going about finding the parameters that we are for our function?",
                    "label": 0
                },
                {
                    "sent": "And the way that we've gone about finding parameters so far in the case of linear regression, how do we find those WS?",
                    "label": 0
                },
                {
                    "sent": "How do we find out?",
                    "label": 0
                },
                {
                    "sent": "Line is something called a maximum likelihood of approach, right?",
                    "label": 0
                },
                {
                    "sent": "So this looks at the probability of our data, given that the model is maximized with respect to the parameters, right?",
                    "label": 1
                },
                {
                    "sent": "So we're looking at basically trying to say we want it to be maximally likely that.",
                    "label": 0
                },
                {
                    "sent": "That particular line is responsible for our data that.",
                    "label": 0
                },
                {
                    "sent": "Is probably not going to make contact with you because generative models is a complicated concept, but we want to make sure that we're maximizing the probability of the data given the model parameters with respect to the model parameters.",
                    "label": 1
                },
                {
                    "sent": "OK. Um?",
                    "label": 0
                },
                {
                    "sent": "This can be done in the same way that we just talked about.",
                    "label": 0
                },
                {
                    "sent": "We take gradients with respect to our model parameters data, so I've kind of overloaded model parameters here.",
                    "label": 0
                },
                {
                    "sent": "So our model primers data are just the W's that I was talking about in the linear regression case.",
                    "label": 0
                },
                {
                    "sent": "So here you can equate the two if you're thinking about linear regression you have some set of weights.",
                    "label": 0
                },
                {
                    "sent": "Those weights are parameters we want to learn those weights when we take gradients, we set it equal to 0 and we solve.",
                    "label": 0
                },
                {
                    "sent": "If in general we have something of the form, the probability of our data given some set of model parameters, like the weights in our linear regression, we can do the exact same thing.",
                    "label": 0
                },
                {
                    "sent": "We can take gradients with respect to Theta, set equal to 0 and solve.",
                    "label": 0
                },
                {
                    "sent": "So in fact, our least squares solution that we talked about for linear regression can be seen as maximizing the likelihood of a Gaussian distribution with mean W transpose X, right?",
                    "label": 1
                },
                {
                    "sent": "So that's to say if those residuals are distributed as a Gaussian off of that line, this is what we get when we maximize the likelihood of our data.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, but the problem is that if we have this approach, if we use this approach, this maximum likelihood approach, we run the risk of overfitting.",
                    "label": 1
                },
                {
                    "sent": "If we have too many parameters.",
                    "label": 0
                },
                {
                    "sent": "So you could see that if we had a parameter for every data point in our data set, we would be able to find a function which fit our data set perfectly right?",
                    "label": 0
                },
                {
                    "sent": "So we have some let me go back here.",
                    "label": 1
                },
                {
                    "sent": "We have some data set like in this example.",
                    "label": 0
                },
                {
                    "sent": "We can fit a squiggly line.",
                    "label": 0
                },
                {
                    "sent": "If we have enough parameters that let's this line squiggle enough, then we can hit every single data point exactly, right?",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "That's not necessarily something we want to do.",
                    "label": 0
                },
                {
                    "sent": "'cause as we talked about before, this is something which is going to lead to really low training error, but it's not going to generalize well and we want to learn a function that's going to generalize well.",
                    "label": 0
                },
                {
                    "sent": "So instead of just looking at doing this maximum likelihood kind of learning of these parameters, one of the things that we do is that we could change the function that we're learning in order to penalize having too many parameters right.",
                    "label": 0
                },
                {
                    "sent": "So we want to penalize.",
                    "label": 0
                },
                {
                    "sent": "We want to maximize their training.",
                    "label": 0
                },
                {
                    "sent": "Our training minimize our training error, right?",
                    "label": 0
                },
                {
                    "sent": "But we also want to penalize having lots of parameters, right?",
                    "label": 0
                },
                {
                    "sent": "So we want to penalize this kind of situation over on the right where we have a very quickly line with lots of parameters, right?",
                    "label": 0
                },
                {
                    "sent": "And say OK, I think you should actually use the minimum number of parameters possible.",
                    "label": 0
                },
                {
                    "sent": "OK, and we do this by adding in a term right?",
                    "label": 0
                },
                {
                    "sent": "So we have our same loss function.",
                    "label": 0
                },
                {
                    "sent": "We do the same kind of like minimizing of the residuals that we did before, but we can say OK if our hypothesis class is going to allow us to have like very complicated functions, we're going to penalize those functions.",
                    "label": 0
                },
                {
                    "sent": "So every time like we add in a new parameter to the line that we're learning, we're going to like up the penalty so that we don't actually, so that we were saying we don't actually want to find that.",
                    "label": 0
                },
                {
                    "sent": "Function we only want to find that function that we really need the extra parameter.",
                    "label": 0
                },
                {
                    "sent": "OK, so the way that we do this is by taking again, taking our original loss, which was the distance between the data points in the line that we were learning and adding in some kind of regularization term.",
                    "label": 0
                },
                {
                    "sent": "And here this regularization term is given by Lambda which is called the regularization coefficient.",
                    "label": 0
                },
                {
                    "sent": "Basically it's some.",
                    "label": 0
                },
                {
                    "sent": "Some scalar parameter which allows us to control the tradeoff between how well we want to fit our data and how simple we want the function that we're learning to be, and then some kind of regularization term which penalizes having a more complex function.",
                    "label": 0
                },
                {
                    "sent": "And that's our of data, right?",
                    "label": 0
                },
                {
                    "sent": "So the more parameters we have, the bigger that regularization term is going to be.",
                    "label": 0
                },
                {
                    "sent": "If we have a Lambda which is reasonably high, right like that?",
                    "label": 0
                },
                {
                    "sent": "That's going to add a lot to the function that we're trying to minimize, and so we're not going to want to go there.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are lots of different.",
                    "label": 0
                },
                {
                    "sent": "So what does this are look like?",
                    "label": 0
                },
                {
                    "sent": "Which is this regularization term look like?",
                    "label": 0
                },
                {
                    "sent": "There are lots of different choices for our.",
                    "label": 0
                },
                {
                    "sent": "There are lots of different ways we could say OK, we're going to penalize having more parameters.",
                    "label": 0
                },
                {
                    "sent": "And we can talk really briefly about different choices for are really common choices that you're going.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The pop up again and again our L wanted L2 penalties.",
                    "label": 0
                },
                {
                    "sent": "OK, and so we call this L1 or L2 regularization an L1L2 regularization penalizes complexity of the functions in different ways.",
                    "label": 0
                },
                {
                    "sent": "So one just trade up, penalizes the number of parameters used.",
                    "label": 1
                },
                {
                    "sent": "An L2 penalizes the number of parameters squared, right?",
                    "label": 1
                },
                {
                    "sent": "So in our linear regression setting, this would be what you see on the left.",
                    "label": 1
                },
                {
                    "sent": "In terms of the minimization that y -- W transpose X ^2.",
                    "label": 0
                },
                {
                    "sent": "That's our original residual function from our linear regression that we minimize to learn what the W weights were.",
                    "label": 0
                },
                {
                    "sent": "But then we have this extra term Lambda times, and then for the L2 penalty W squared and for the L1 penalty just W, right?",
                    "label": 0
                },
                {
                    "sent": "And we call this some kind of regularization right?",
                    "label": 0
                },
                {
                    "sent": "So it's really penalizing penalizing when we're we're seeing those more complicated functions, right?",
                    "label": 0
                },
                {
                    "sent": "It's just in slightly different ways, so this is again like a squared versus like an awkward thing.",
                    "label": 0
                },
                {
                    "sent": "And so in a probabilistic setting, these regularizers correspond to what we call different kind of priors.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk about what that means in a second.",
                    "label": 1
                },
                {
                    "sent": "But basically it corresponds to in the L2K suggesting prior in the L1 case Laplacian prior on the weights.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "L1 regularization also leads to something called shrinkage.",
                    "label": 0
                },
                {
                    "sent": "So when you're when you're hearing about shrinkage shrinkage priors, sometimes sparsity priors.",
                    "label": 0
                },
                {
                    "sent": "You'll hear about one regularization.",
                    "label": 0
                },
                {
                    "sent": "So basically this means that this is something which is going to like seeing the weights go close to zero.",
                    "label": 0
                },
                {
                    "sent": "It's going to sort of like push what we're learning to a situation in which the weights get smaller.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Color or become zero?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "That that discussion about priors.",
                    "label": 0
                },
                {
                    "sent": "The L1 prior, the L1 regularizer being equivalent to a little possum prior, and L2 regularizer being a couple into a Gaussian prior brings us to Bayes rule, right?",
                    "label": 0
                },
                {
                    "sent": "Like what is this prior term that I mentioned?",
                    "label": 0
                },
                {
                    "sent": "So Bayesian methods are used a lot and phase rule is used a lot and so trying to trying to find the WS which maximize the regularization equations that I gave.",
                    "label": 0
                },
                {
                    "sent": "On the previous slides, is equivalent to doing something called map estimation, right?",
                    "label": 0
                },
                {
                    "sent": "It's equivalent to using this kind of Bayes rule and doing some kind of like maximizing of maximizing of the parameters Theta.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. We don't have to maximize.",
                    "label": 0
                },
                {
                    "sent": "We don't have to optimize the parameters, and I'll talk to you in a second about like what else we might do instead of optimizing the parameters because I've spent most of my career not optimizing parameters and most of what you're going to learn about here is optimizing parameters so.",
                    "label": 0
                },
                {
                    "sent": "I thought that I just tell you really briefly about about alternatives.",
                    "label": 0
                },
                {
                    "sent": "OK, so, but the first thing you have to know is Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "OK, so Bayes rule basically provides us with a coherent framework for reasoning about our beliefs in the face of uncertainty.",
                    "label": 1
                },
                {
                    "sent": "So we have the probability of the data given the model parameters.",
                    "label": 0
                },
                {
                    "sent": "That's what we've been talking about.",
                    "label": 0
                },
                {
                    "sent": "Like this whole time, right?",
                    "label": 0
                },
                {
                    "sent": "Like This is why we talked about like the sum of squared residuals in that particular that potentially.",
                    "label": 0
                },
                {
                    "sent": "Corresponding to like a Gaussian model, right?",
                    "label": 0
                },
                {
                    "sent": "So that probability of the data given the model parameters could be a Gaussian and then we have some kind of prior on the model parameters.",
                    "label": 0
                },
                {
                    "sent": "So we talked about different kinds of regularizers we talked about having like a Gaussian regularizer, oral possing regularizer and we talked about how that corresponded to the kind of prior that we're using, and so that prior P of Theta would go there.",
                    "label": 0
                },
                {
                    "sent": "And then we have some kind of normalizer.",
                    "label": 0
                },
                {
                    "sent": "Which then gives us P of Theta given D, that's what's on the left hand side, which is our posterior.",
                    "label": 1
                },
                {
                    "sent": "OK, so that intuitive idea behind this is that RPF data our prior captures our prior beliefs about the state of the world.",
                    "label": 0
                },
                {
                    "sent": "So apriori we might think that that we have a Gaussian which looks a particular way and then we see more evidence coming in through our likelihood function.",
                    "label": 0
                },
                {
                    "sent": "The probability of.",
                    "label": 0
                },
                {
                    "sent": "Of our data given given our model parameters, gives us the probability of our observations of the observations that we're seeing in our data set.",
                    "label": 0
                },
                {
                    "sent": "Given that we're in a particular state data, or given that we have a particular prior over our model parameters, and then we can use this to compute P of Theta given D, which is our posterior or update updated beliefs having observed the data that we did about the state of the world data.",
                    "label": 0
                },
                {
                    "sent": "Given our observations deep and so this gets into something that like I talk about a lot, I don't want to get into this here, but this gets into something that I talk about a lot 'cause I'm going to stats Department, so I end up teaching a lot of stats classes and one of the things that we look at a lot.",
                    "label": 0
                },
                {
                    "sent": "It's that's way too much is like hypothesis testing, and often like that's kind of set up so that we say, OK, you know.",
                    "label": 0
                },
                {
                    "sent": "What do I think PP.",
                    "label": 0
                },
                {
                    "sent": "Of the given Theta look looks like and how do I deal with that just on its own an when we look at a maximum likelihood kind of setting were again kind of looking at this P of D given Theta on its own, but that's often not the quantity that we're really interested in were not often interested in the probability of our data given our model parameters.",
                    "label": 0
                },
                {
                    "sent": "Usually what we're actually interested in is the probability of our model parameters given our data.",
                    "label": 0
                },
                {
                    "sent": "Our data is something that we observe.",
                    "label": 0
                },
                {
                    "sent": "It is fixed.",
                    "label": 0
                },
                {
                    "sent": "It is given to us by the world and then we want to know what line is it that we choose, given that this is what we're seeing.",
                    "label": 0
                },
                {
                    "sent": "So really the thing that varies the thing that's the random variable that we're interested in is this data.",
                    "label": 0
                },
                {
                    "sent": "Are the parameters like of this line right?",
                    "label": 0
                },
                {
                    "sent": "And looking at a Bayesian setting, it's got like a lot of advantages, but one of the advantages is that it treats that it treats that Theta those sets of weights as the random variable and the data as sort of given.",
                    "label": 0
                },
                {
                    "sent": "OK, another another.",
                    "label": 0
                },
                {
                    "sent": "Another bonus is that you avoid the overfitting problem or you in the same way that regularization avoids the overfitting problem.",
                    "label": 0
                },
                {
                    "sent": "I talked about these different kinds of regularizers like L1 regularization or L2 regularization corresponding to particular priors, right?",
                    "label": 0
                },
                {
                    "sent": "Those priors fit in very nicely in the Bayesian setting.",
                    "label": 0
                },
                {
                    "sent": "You can see them right here, and so once you incorporate these regularizers.",
                    "label": 0
                },
                {
                    "sent": "You're giving a little bit more slack to allowing allowing your training area to go up in exchange for having a similar function.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of other kinds of kinds of benefits that I'm not going into too much here that you get from a Bayesian paradigm, and people will talk more to you about this later, but like you get some kind of sense of uncertainty, like how confident am I in the parameters that I learned for my linear function, etc.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so this is the slide where I really deviate from from the setting of maximizing.",
                    "label": 0
                },
                {
                    "sent": "Finding them the maximally likely parameters even with regularization.",
                    "label": 0
                },
                {
                    "sent": "So in most of my work I don't actually optimize, I compute something called a marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "When I go to do clustering and it's so instead of taking gradients and setting them equal setting of functions equal to 0.",
                    "label": 0
                },
                {
                    "sent": "I compute something called a marginal likelihood, and the marginal likelihood is defined using the integral that you see, and so the downside is this is an integral and so that kind of sucks computationally, but the upside is that you're averaging now over a lot of different parameters.",
                    "label": 0
                },
                {
                    "sent": "Instead of choosing just one.",
                    "label": 0
                },
                {
                    "sent": "And so there are lots of benefits to this so.",
                    "label": 0
                },
                {
                    "sent": "So I when I when I go to do something like compare one or two clusters, I can compute the marginal likelihood so I can compute the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "OK, so the marginal likelihood is the probability of my data given my model.",
                    "label": 1
                },
                {
                    "sent": "So here in this example plot I have a bunch of data points.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say those data points came from a Gaussian and so I'm going to compare the hypothesis that they came from all from one Gaussian versus they came from two different Gaussians.",
                    "label": 0
                },
                {
                    "sent": "Want guessing for the purple dots?",
                    "label": 0
                },
                {
                    "sent": "An want guessing for the for the blue dots, and this is a movie.",
                    "label": 0
                },
                {
                    "sent": "It's a little ridiculous to call this a movie, but you'll see what happens when when they separate and we can compare the likelihood that the data came from one Gaussian versus that they came from 2 Gaussian clusters, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so the marginal likelihood is the probability of the data given the model.",
                    "label": 0
                },
                {
                    "sent": "The model is a Gaussian, so I'm going to integrate the probability of the data given some fixed but unknown model parameters Theta times the prior on the model parameters.",
                    "label": 0
                },
                {
                    "sent": "So here the model parameters for Gaussian would be the Gaussians mean and covariance.",
                    "label": 0
                },
                {
                    "sent": "Right, and so I've got some kind of prior idea of what the meaning covariance should look like, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the interpretation that all of the data points.",
                    "label": 0
                },
                {
                    "sent": "This gives the interpretation that all of the data points in our data set were generated from the same model with unknown parameters Theta.",
                    "label": 1
                },
                {
                    "sent": "And I am now integrating over what I think all what I think those parameters data might be.",
                    "label": 0
                },
                {
                    "sent": "Instead of optimizing and find the maximally likely likely one.",
                    "label": 0
                },
                {
                    "sent": "OK, so I can use this concept of a marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "To compare the one versus 2 cluster model and I'll show you I'm actually marked OK, so I'll show you hopefully.",
                    "label": 0
                },
                {
                    "sent": "Here we go.",
                    "label": 0
                },
                {
                    "sent": "OK, so hopefully you can see as the as the purple datapoints pull apart from the blue data points.",
                    "label": 0
                },
                {
                    "sent": "The probability of the two cluster model goes up very nicely, so that again when they're apart, the probability of having two clusters is very high.",
                    "label": 0
                },
                {
                    "sent": "So I guess the point of showing you guys that video is to say OK. You know if I was in a strictly extract marginal likelihood, kind of.",
                    "label": 0
                },
                {
                    "sent": "I was in a strict maximum likelihood kind of setting the two cluster model would always be better, right?",
                    "label": 0
                },
                {
                    "sent": "It would always say you know, in the worst case, kind of what we're going to do is we're going to shut off one of the clusters, right?",
                    "label": 0
                },
                {
                    "sent": "But I'm always better off having that second cluster than not having that second cluster, and so you're really seeing this concept of regularization.",
                    "label": 0
                },
                {
                    "sent": "And here and marginalization over the parameters where we have, we can compare the probability of the one cluster versus 2 cluster model.",
                    "label": 0
                },
                {
                    "sent": "And we're finding the right kind of thing, or the thing that makes sense to us, kind of.",
                    "label": 0
                },
                {
                    "sent": "Eyeballing the data set as the data pulls apart.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this leads to a bit of a discussion about Bayesian Occam's Razor, so we might have a data set.",
                    "label": 0
                },
                {
                    "sent": "The data set might look like I just made up the data set, kind of in the plot above, right?",
                    "label": 0
                },
                {
                    "sent": "Like their objects, they have a certain size under certain color they might.",
                    "label": 0
                },
                {
                    "sent": "We might be able to plot them like I laid out, and so the question then becomes like is this data set best modeled with one cluster within one cluster model with the cluster model or with the three cluster model?",
                    "label": 1
                },
                {
                    "sent": "And you know how do we go about figuring that out?",
                    "label": 0
                },
                {
                    "sent": "And so we can use the kind of marginal likelihood setting for for trying to determine the correct number of clusters that I just that I just showed you.",
                    "label": 0
                },
                {
                    "sent": "OK, so when I compute a marginal likelihood, remember, I'm computing the probability of the data given the model automatically get an Occam's razor kind of affect where I say OK. Models that are too simple are not going to model the data set that I care about, or the data set that I have well and models that are too complex model the data set that I have well, but they model lots of different datasets well so they have their probability mass.",
                    "label": 0
                },
                {
                    "sent": "So the probability of the data given the model it has to integrate to one right?",
                    "label": 0
                },
                {
                    "sent": "Or it has to sum to one over all possible datasets, and so there's only a limited amount of probability mass.",
                    "label": 0
                },
                {
                    "sent": "So if you're a function.",
                    "label": 0
                },
                {
                    "sent": "They can model lots of different datasets.",
                    "label": 0
                },
                {
                    "sent": "Well, you're spreading your probability mass.",
                    "label": 0
                },
                {
                    "sent": "Then over all of the different datasets, whereas if you don't model the data set well, you might have a lot of probability mass on the datasets that you do model well.",
                    "label": 0
                },
                {
                    "sent": "But if that doesn't capture the one that we're interested in, we're still not going to give high probability to our particular data set, so we don't want something that's true simple.",
                    "label": 0
                },
                {
                    "sent": "We don't want like a line for a curve data set, right?",
                    "label": 0
                },
                {
                    "sent": "That doesn't give high probability mass to our curve data set.",
                    "label": 0
                },
                {
                    "sent": "But we also don't want something too complex, something that wiggles around all the time, right?",
                    "label": 0
                },
                {
                    "sent": "Because that thing that that function that wiggles around all the time.",
                    "label": 0
                },
                {
                    "sent": "We got lots of different ways to model lots of different datasets really well.",
                    "label": 0
                },
                {
                    "sent": "We want something that's just write something that just wiggles enough to capture our data set well without being able to capture lots and lots of datasets well.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this leaves maybe I'll just briefly mention nonparametric models, right?",
                    "label": 0
                },
                {
                    "sent": "So they're like nonparametric Bayesian models.",
                    "label": 0
                },
                {
                    "sent": "They're not parametric models in general.",
                    "label": 0
                },
                {
                    "sent": "OK, so coming thinking of better clustering kind of situation.",
                    "label": 0
                },
                {
                    "sent": "How do we know which which clustering models to compare?",
                    "label": 1
                },
                {
                    "sent": "So we looked at comparing say like one versus 2 clusters or one versus 2 versus 3 clusters.",
                    "label": 0
                },
                {
                    "sent": "But what if, like we have a data set which we don't even know how to visualize?",
                    "label": 0
                },
                {
                    "sent": "Maybe it has like 150 clusters.",
                    "label": 0
                },
                {
                    "sent": "Do we compare all of those different things?",
                    "label": 0
                },
                {
                    "sent": "Do we compare 1234 probable wildly up 250?",
                    "label": 1
                },
                {
                    "sent": "Doing a large number of these model comparisons is really costly and so non pennridge bays in general provides flexible priors over clustering models and allows us to infer the right number of clusters for data set.",
                    "label": 0
                },
                {
                    "sent": "But nonparametric models in general allows that allows for this concept that we have an infinite set of parameters and we allow the number of parameters to grow as our data set size grows so you can remember maybe like way back I said.",
                    "label": 0
                },
                {
                    "sent": "Like you know, having more complex functions is OK as we have more and more data right?",
                    "label": 1
                },
                {
                    "sent": "So this will allow our complexity to rise as our amount of data rises.",
                    "label": 0
                },
                {
                    "sent": "OK, so parametric models assume that there's some finite set of parameters or clusters that captures everything there is to know about the data.",
                    "label": 1
                },
                {
                    "sent": "The complexity of the model is bounded, so we if we have three clusters, right?",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter how much data we see.",
                    "label": 0
                },
                {
                    "sent": "You're only ever going to have three clusters, right?",
                    "label": 0
                },
                {
                    "sent": "Our complexity is bounded, whereas if we have a nonparametric model right, we allow the number of parameters to grow as our amount of data grows, we can say OK as we see more data coming in.",
                    "label": 0
                },
                {
                    "sent": "Maybe the number of clusters is going to go from 3:00 to 5:00 to 7:00, right?",
                    "label": 0
                },
                {
                    "sent": "And maybe that's OK because we're getting more and more data and more and more evidence.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's my little spiel on Bayesian methods.",
                    "label": 0
                },
                {
                    "sent": "I hope you guys enjoyed it.",
                    "label": 0
                },
                {
                    "sent": "We're going to talk about now about slightly more sophisticated regression models, and we're going to do this again, like in prep for transitioning from linear regression.",
                    "label": 0
                },
                {
                    "sent": "So we talked a lot about fitting a line to two things like generalized linear models and logistic regression.",
                    "label": 0
                },
                {
                    "sent": "Because you're going to see that this is really an analogy for a neuron in a neural network, and people will go on to talk to you about neural networks, but you're really going to have to kind of have the basics of this before.",
                    "label": 0
                },
                {
                    "sent": "You understand how neural networks or.",
                    "label": 0
                },
                {
                    "sent": "So in the same way that we looked at linear regression, we can look at other regression models that allow us to learn nonlinear functions right?",
                    "label": 1
                },
                {
                    "sent": "And so I think one of the big benefits of of neural networks has been that it allows us to learn nonlinear functions.",
                    "label": 0
                },
                {
                    "sent": "We don't want to just be stuck in sort of like this linear paradigm, so in particular I'm going to talk about generalized linear models and logistic regression.",
                    "label": 0
                },
                {
                    "sent": "OK, we can do the same thing that we've done in the past, which is screw my marginal likelihood integration situation.",
                    "label": 0
                },
                {
                    "sent": "We're going to maximize the parameters we can.",
                    "label": 0
                },
                {
                    "sent": "We can take gradients with respect to our parameters set, set our loss function equal to 0.",
                    "label": 0
                },
                {
                    "sent": "The problem is that as we move to more complicated functions like generalized linear models or logistic regression, this solution, the solution for the weights, the solution for our parameters is not available to us in closed form and we've got to do something else like gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So again, you're going to learn more about different kinds of optimization techniques from future speakers.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so they're running example here.",
                    "label": 0
                },
                {
                    "sent": "Is going to be this Donner party example.",
                    "label": 0
                },
                {
                    "sent": "I should say that these are the slides that I sometimes teach off of and so I've stolen these innocence from my Department.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "And I'll just read this to you really, briefly.",
                    "label": 0
                },
                {
                    "sent": "So in 1846, the Donner and Reed families left Springfield, IL for California by covered wagon in July the Donner Party, as as it became known, reached Fort Bridger, Wyoming.",
                    "label": 0
                },
                {
                    "sent": "There its leaders decided to attempt a new and untested route to the Sacramento Valley, having reached its full size of 87 people and 20 wagons.",
                    "label": 0
                },
                {
                    "sent": "The party was delayed by a difficult crossing of the Wah Sache Range, and again in the crossing of.",
                    "label": 0
                },
                {
                    "sent": "The desert West of the Great Salt Lake.",
                    "label": 0
                },
                {
                    "sent": "The group became stranded in the Eastern Sierra Nevada mountains when the region was hit by heavy snows in late October.",
                    "label": 0
                },
                {
                    "sent": "By the time the last survivor was rescued on April 21st, eighteen 4740 of the 87 members had died from famine.",
                    "label": 0
                },
                {
                    "sent": "An exposure to extreme cold.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This leads to this is a very sad situation for them, but a very nice data set for us.",
                    "label": 0
                },
                {
                    "sent": "And and so here we have 4545 members of the Donner Party.",
                    "label": 0
                },
                {
                    "sent": "We have them listed by age, by sex and buys status mortality status, whether they died or survived.",
                    "label": 0
                },
                {
                    "sent": "OK, so their age.",
                    "label": 0
                },
                {
                    "sent": "So for person one.",
                    "label": 0
                },
                {
                    "sent": "So you see, the first row is person one.",
                    "label": 0
                },
                {
                    "sent": "Their age is 23 years old, their sexes, male and their statuses that they died before they were rescued.",
                    "label": 0
                },
                {
                    "sent": "Person Two was 40 years old and female and survived etc etc.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can look at their status, their their mortality versus their gender, right?",
                    "label": 0
                },
                {
                    "sent": "So we can look at the people who died and what gender they are, whether they died or survived.",
                    "label": 0
                },
                {
                    "sent": "So of the males.",
                    "label": 0
                },
                {
                    "sent": "20 of them died and 10 of them survived of the females 5:00 AM died and 10 of them survived.",
                    "label": 0
                },
                {
                    "sent": "We can also look at things like mortality status versus each.",
                    "label": 0
                },
                {
                    "sent": "You know, and and basically like what you're seeing in different ways in these pots is that you were more likely to die if you were male and you are more likely to die if you were older.",
                    "label": 0
                },
                {
                    "sent": "Probably not a huge surprise.",
                    "label": 0
                },
                {
                    "sent": "But we want to be able to do things like like, say, OK, both Agent.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Under have an effect on somebody survival, but how is it that we can explore this relationship so we can set died to zero dive?",
                    "label": 0
                },
                {
                    "sent": "Is R0 and survive to one.",
                    "label": 0
                },
                {
                    "sent": "This isn't something that we can really fit with a linear model.",
                    "label": 0
                },
                {
                    "sent": "We need something something more, something that's going to be able to tell us based on somebody's age and based on somebody's gender with the likelihood is that they they survived or died.",
                    "label": 0
                },
                {
                    "sent": "OK, so one way to think about this is we can think about this is like a coin flip we can think about treating whether somebody survived or died as successes or failures that arise from like a Bernoulli trial, or like a coin flip where the probability of success or lookaheads is given by some kind of transformation of the linear model that we already learned about.",
                    "label": 0
                },
                {
                    "sent": "So we already learned about this nice linear model and we just want to say OK, how do we take that and transform it to something which is going to give us?",
                    "label": 0
                },
                {
                    "sent": "A likelihood of the person dying 0 or the person surviving one.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so this leads to something called generalized linear models.",
                    "label": 0
                },
                {
                    "sent": "So GLM's are very general ways of addressing these kinds of problems and logistic regression, which is the thing that we're particularly interested in, is just one kind of generalized linear model.",
                    "label": 1
                },
                {
                    "sent": "And all she all challens have three things.",
                    "label": 0
                },
                {
                    "sent": "The first thing is a probability distribution describing the outcome variable.",
                    "label": 1
                },
                {
                    "sent": "So here the outcome is, did the person liver die right?",
                    "label": 0
                },
                {
                    "sent": "And the probability distribution that we talked about that being was a Brimley right like liver die?",
                    "label": 0
                },
                {
                    "sent": "It's like a coin flip kind of trial.",
                    "label": 0
                },
                {
                    "sent": "They have a linear model like the linear model that we discussed, right?",
                    "label": 0
                },
                {
                    "sent": "Y is equal to W 0 plus W1X.",
                    "label": 0
                },
                {
                    "sent": "And then there's some link function and the link function relates the linear model to the outcome variable.",
                    "label": 1
                },
                {
                    "sent": "Alright, and we call this one function G right?",
                    "label": 0
                },
                {
                    "sent": "You might hear about it later on, 'cause it's like an activation function.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so logistic regression is a particular kind of GLM that's used to model binary categorical variable.",
                    "label": 0
                },
                {
                    "sent": "So here a binary categorical variable variable is is the right thing to go after rate, like whether somebody lived or died is a binary variable.",
                    "label": 0
                },
                {
                    "sent": "Using numerical and categorical predictors, right?",
                    "label": 0
                },
                {
                    "sent": "So the things that were going off of like age or gender.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they can be numerical, right?",
                    "label": 0
                },
                {
                    "sent": "Like ages, pretty numerical or categorical like are they male or female?",
                    "label": 0
                },
                {
                    "sent": "So here we assume that a binary distribution produced the outcome variable, and so we want to model P the probability of a success for given set of predictors.",
                    "label": 0
                },
                {
                    "sent": "So we want to be able to look at with their age what's their gender and then predict the probability of the liver die so.",
                    "label": 0
                },
                {
                    "sent": "To specify the logistic model, we need to establish a link function that's going to connect yrr outcome liver die.",
                    "label": 0
                },
                {
                    "sent": "Sorry yrr are linear function which which is some kind of waited some kind of weighted linear combination of their age and their age, their gender to P, the probability that they live or die.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are a lot of different options for this and you guys are going to see probably some more options for this, But the option that we use here is the logistic function.",
                    "label": 0
                },
                {
                    "sent": "So the logistic function is log of P / 1 -- P for P between zero and one right so?",
                    "label": 0
                },
                {
                    "sent": "Again, like the probability of seeing header tail.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so the logistic function takes a value between zero and one and Maps it to a value between negative Infinity and Infinity.",
                    "label": 0
                },
                {
                    "sent": "So if we think about we want to be able to take this linear function and map it to between zero and one.",
                    "label": 0
                },
                {
                    "sent": "This is kind of going the wrong way around right?",
                    "label": 0
                },
                {
                    "sent": "And so we want to go the other way around.",
                    "label": 0
                },
                {
                    "sent": "We want to take some value between negative Infinity Infinity and map it to some value between zero and one right?",
                    "label": 0
                },
                {
                    "sent": "So we want to take the results of our linear are linear equation that we've learned write an, map it to some probability that they survived.",
                    "label": 0
                },
                {
                    "sent": "So we use the inverse of the logistic function, so this is 1 / 1 plus Exp of negative extra Exp of X / 1 + X P of X is the inverse of the logistic function.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But right, and so that goes the way the way around that we that we want that allows us to take this linear equation and say what's the probability of life or death?",
                    "label": 0
                },
                {
                    "sent": "OK, and so this can be OK. And then this interpretation.",
                    "label": 0
                },
                {
                    "sent": "This can be useful for interpreting the model.",
                    "label": 0
                },
                {
                    "sent": "Because they can just also just tell us what are the odds of success rate you can think of what you get coming out of this as being a probability right or probability of living.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The three GLM criteria right are things that we talked about in terms of like here.",
                    "label": 1
                },
                {
                    "sent": "Let's go back.",
                    "label": 0
                },
                {
                    "sent": "We need a probability distribution describing the outcome variable.",
                    "label": 0
                },
                {
                    "sent": "We need a linear model and we need a link function.",
                    "label": 0
                },
                {
                    "sent": "Right, so here are probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Is the bernoulli.",
                    "label": 0
                },
                {
                    "sent": "Our linear model is a linear combination of, say like age and gender of a weighted combination of age and gender and our link function is the logistic function.",
                    "label": 0
                },
                {
                    "sent": "So from that we can get our probability of whether we think a particular person lives or dies, right?",
                    "label": 0
                },
                {
                    "sent": "So if we look at the inverse of the logistic we get P coming out, P is the probability of a 1 right?",
                    "label": 0
                },
                {
                    "sent": "And so it's just Exp of that our linear equation are linear weighting of age and gender over one plus Exp of our linear weighting of age and gender.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so our logistic function looks like this, right?",
                    "label": 0
                },
                {
                    "sent": "So we are no longer fitting align to our data.",
                    "label": 0
                },
                {
                    "sent": "We are allowing this to look like some kind of curve which asymptotes out at one on one end an ASMR totes out at 0 on the other, and we can learn different logistic curves or logistic functions for different kinds of populations or different kinds of settings, right?",
                    "label": 0
                },
                {
                    "sent": "So again, we can like learn different parameters, where the parameters.",
                    "label": 0
                },
                {
                    "sent": "Are they like the weights of of our linear model?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, and so the logistic functions that you see here you have one for men and one for women, and you can notice that like the one for women out lies about the one for men because it was more likely that they would survive, right?",
                    "label": 0
                },
                {
                    "sent": "So what's plotted here?",
                    "label": 0
                },
                {
                    "sent": "The little triangles are the women and the open circles are then right.",
                    "label": 0
                },
                {
                    "sent": "So this is the logistic function that we get coming out.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think that the.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Was sort of all of the material that is sort of like we need to cover in order for you guys to really understand the rest of what's going on in the rest of the talks in the rest of the summer school, so hopefully you can jump, gotten something out of something that I've talked about and you can jump off of that into some of the other people's talks.",
                    "label": 0
                },
                {
                    "sent": "So in summary, we looked at different kinds of machine learning rate, supervised learning, unsupervised learning reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "We looked at different kinds of linear models and how to fit them today to when our models are too simple or too complex for the particular data set that we have overfitting a particular complicated function with a lot of parameters to a data set, which really requires a simpler function in order to generalize well.",
                    "label": 1
                },
                {
                    "sent": "How we might prevent ourselves from doing that different kinds of regularization which allow us to prefer simpler functions even though more complicated functions are reducing our training error.",
                    "label": 0
                },
                {
                    "sent": "More Bayesian techniques which are another way of viewing regularization and marginal likelihoods where we're integrating instead of maximizing, integrating over instead of maximizing our parameters, and then Lastly generalized linear models and logistic regression.",
                    "label": 0
                },
                {
                    "sent": "How do we take that line that we learned and transform it into something like a binary classifier or something that's going to give us?",
                    "label": 0
                },
                {
                    "sent": "Probability of death or living like in various kinds of situations, and that is through the logistic function.",
                    "label": 0
                },
                {
                    "sent": "And that's what we learned about in logistic regression.",
                    "label": 0
                },
                {
                    "sent": "Now we're no longer doing linear regression.",
                    "label": 0
                },
                {
                    "sent": "We're allowing our regression curve to be nonlinear.",
                    "label": 0
                },
                {
                    "sent": "And again, like I said, that's going to form the basis for neural networks that you guys are going to learn about coming up.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I guess I have a few minutes of.",
                    "label": 0
                },
                {
                    "sent": "How am I doing on time in my overtime overtime?",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll try to just I had at the NFU sides about kind of like the work that I did 'cause that's I mean.",
                    "label": 0
                },
                {
                    "sent": "First off that's easiest for me to put in right?",
                    "label": 0
                },
                {
                    "sent": "'cause this might work but also like it provides hopefully some motivation and to the kinds of reasons that you would be interested in learning about this kind of methodology and that we hope to take it and use it within the health care system in order to try to improve people's health and so I'll skip talking about that in a lot of detail.",
                    "label": 0
                },
                {
                    "sent": "But I do want to.",
                    "label": 0
                },
                {
                    "sent": "Leave you kind of like with a little bit of motivation about where things are headed, moving, moving forward so.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "I talked to you about and what I presented you with a really just the basics and you guys are going to get a lot more details in the summer school as more people as more people go through their courses.",
                    "label": 0
                },
                {
                    "sent": "The important thing to always keep in mind is that the primary purpose of all of the methodology that we talk about is really to be able to develop useful models for real data to be able to say OK, I'm going to enter into this real world setting.",
                    "label": 1
                },
                {
                    "sent": "I'm going to take into it some of the knowledge.",
                    "label": 0
                },
                {
                    "sent": "Some of the things that we know how to do and somehow make something better.",
                    "label": 0
                },
                {
                    "sent": "Whether it's like to be able to have driverless cars that work better or to be able to improve the health of patients or make better treatment decisions.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "That's kind of like our goal at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "So there are lots of really cool.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "Things that are being done in the machine learning community and lots of these use more sophisticated methods than what I talked about in this talk.",
                    "label": 1
                },
                {
                    "sent": "This is really again just the beginnings, but.",
                    "label": 0
                },
                {
                    "sent": "I again we're interested in sort of having this kind of real world impact at the end of the day, whether it's through this simple models that I presented here or from the more sophisticated models that we've built up off of this.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so again, like the places that people are applying this to that it includes like self driving cars and involves sort of the stability of drones or robots it includes.",
                    "label": 1
                },
                {
                    "sent": "Can we make product recommendations that are tailored for individuals right?",
                    "label": 0
                },
                {
                    "sent": "Things that you were actually more interested in buying?",
                    "label": 0
                },
                {
                    "sent": "Can we make criminal justice recommendations so one of the things that a lot of my friends are really interested in are things like algorithms.",
                    "label": 1
                },
                {
                    "sent": "That get used to make a decision about whether somebody should be let out on parole or not, and so we do have automated algorithmic ways of trying to do this now, but they have all kinds of biases associated with them, and we're trying to eliminate, eliminate those the best extent possible.",
                    "label": 0
                },
                {
                    "sent": "And also healthcare predictions is again an area that I work in a lot, right?",
                    "label": 0
                },
                {
                    "sent": "Like might somebody be at risk for having developed sepsis in their time in the hospital?",
                    "label": 0
                },
                {
                    "sent": "What does the course of their kind of kidney disease look like?",
                    "label": 0
                },
                {
                    "sent": "What's their chance of having a complication coming out of surgery?",
                    "label": 0
                },
                {
                    "sent": "What kind of multiple sclerosis?",
                    "label": 0
                },
                {
                    "sent": "There's a particular patient have?",
                    "label": 0
                },
                {
                    "sent": "These are all questions that I work on sort of on a daily basis.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then I should say it.",
                    "label": 0
                },
                {
                    "sent": "Still, there's a lot that we don't know, and a lot that we.",
                    "label": 1
                },
                {
                    "sent": "Really need the help of everybody is sitting here and trying to figure out, right so?",
                    "label": 0
                },
                {
                    "sent": "Like questions that get bandied about a lot between me and my colleagues and things that we really don't have a good understanding of.",
                    "label": 0
                },
                {
                    "sent": "So it's unquestionable that they're at least certain settings in which we've seen really substantial gains by using deep neural networks.",
                    "label": 0
                },
                {
                    "sent": "Where do these game games come from, exactly?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Like how much of this is like we have access to more data.",
                    "label": 0
                },
                {
                    "sent": "How much of this is the efficiency with which we can get through it?",
                    "label": 0
                },
                {
                    "sent": "How much of it is the non linearity that I talked about?",
                    "label": 0
                },
                {
                    "sent": "Like when we move from linear to logistic regression?",
                    "label": 1
                },
                {
                    "sent": "How is regularization working right?",
                    "label": 0
                },
                {
                    "sent": "Like there's clearly some kind of regularization that's necessary within within the learning of deep neural networks, and you often see this regularization come in.",
                    "label": 0
                },
                {
                    "sent": "So we talked a little bit about regularization.",
                    "label": 0
                },
                {
                    "sent": "You often see this regularization come in through things like early stopping or dropout methods that you'll learn about further on in this class, right?",
                    "label": 0
                },
                {
                    "sent": "And so this is one way of increasing the amount of regularization going on.",
                    "label": 0
                },
                {
                    "sent": "What exactly is going on here?",
                    "label": 0
                },
                {
                    "sent": "What's it doing?",
                    "label": 0
                },
                {
                    "sent": "Why do we see it giving us benefits?",
                    "label": 1
                },
                {
                    "sent": "And then like how can we speed things up right?",
                    "label": 0
                },
                {
                    "sent": "As we have more and more data and still have principle to purchase to what we're doing?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for talking you go back to the regularization section.",
                    "label": 0
                },
                {
                    "sent": "No, that's like.",
                    "label": 0
                },
                {
                    "sent": "About the L1L2 Well, the one about the minimizing the.",
                    "label": 0
                },
                {
                    "sent": "Maybe the next slide.",
                    "label": 0
                },
                {
                    "sent": "Yeah this one.",
                    "label": 0
                },
                {
                    "sent": "So here said that.",
                    "label": 0
                },
                {
                    "sent": "Elwyn and L2 are penalizing the number of parameters square, is it?",
                    "label": 0
                },
                {
                    "sent": "But I guess it's more like minimizing the the values of the parameters.",
                    "label": 0
                },
                {
                    "sent": "Is that the better way to put it?",
                    "label": 0
                },
                {
                    "sent": "Because it's really by setting the values to zero that we're checking.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you're you're right, and in this particular case, but I'm like intuitively right.",
                    "label": 0
                },
                {
                    "sent": "Like you want to learn, sort of like the the.",
                    "label": 0
                },
                {
                    "sent": "The least complicated function possible, and if you do something like you shrink somebody's weight to 0 right, you're kind of taking that parameter out of Adam consideration, right?",
                    "label": 0
                },
                {
                    "sent": "And you're learning sort of like a less complicated thanks.",
                    "label": 0
                },
                {
                    "sent": "Just wanted a clarification.",
                    "label": 0
                },
                {
                    "sent": "So many people apply perturbations.",
                    "label": 0
                },
                {
                    "sent": "For example like dropout to get some predictive uncertainty of deep learning, and many people thought like this is the way to integrate Bayesian methods, but still learning.",
                    "label": 0
                },
                {
                    "sent": "So what's your opinion about this direction?",
                    "label": 0
                },
                {
                    "sent": "About, sorry about that.",
                    "label": 0
                },
                {
                    "sent": "Yeah about the direction of like integrating theory in Bayesian approach using kind of perturbation drop out.",
                    "label": 0
                },
                {
                    "sent": "Like yeah no.",
                    "label": 0
                },
                {
                    "sent": "I mean I think so so I'm not sure I'm familiar with exactly what you're referring to, but I think this whole kind of like direction of trying to integrate Bayesian and deep neural networks is really important right?",
                    "label": 0
                },
                {
                    "sent": "Because both of those things they each have strengths that the other doesn't sort of like historically, right, so?",
                    "label": 0
                },
                {
                    "sent": "So things like how might we?",
                    "label": 0
                },
                {
                    "sent": "There are a lot of things that.",
                    "label": 0
                },
                {
                    "sent": "Come up where people are like.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure how we do transfer learning or or start new categories or like.",
                    "label": 0
                },
                {
                    "sent": "Or have like colder director perception or do attention in a principled manner or whatever in a deep neural networks setting.",
                    "label": 0
                },
                {
                    "sent": "And you can do a lot of this using using Bayesian statistical methods, right?",
                    "label": 0
                },
                {
                    "sent": "And so in fact.",
                    "label": 0
                },
                {
                    "sent": "So there were a bunch of us on it on a gram on about combining, so actually combining it just started combining Bayesian statistical methods and deep neural networks, and it was interesting because we get this and we're looking at it.",
                    "label": 0
                },
                {
                    "sent": "We're reading through it about all of the problems that deep neural networks have in terms of like learning how to deal with new data coming in and.",
                    "label": 0
                },
                {
                    "sent": "Increasing transfer learning is starting new categories and stuff like that, and we're looking at we're like.",
                    "label": 0
                },
                {
                    "sent": "This is totally not a problem like any Bayesian statistical method has, right?",
                    "label": 0
                },
                {
                    "sent": "Like all of all of these things right?",
                    "label": 0
                },
                {
                    "sent": "Like the problem businesses just call methods have is like their slow.",
                    "label": 0
                },
                {
                    "sent": "You've gotta integrate.",
                    "label": 0
                },
                {
                    "sent": "You've got it.",
                    "label": 0
                },
                {
                    "sent": "You've got to evaluate integrals all the time, and things like that which are not problems that be that deep.",
                    "label": 0
                },
                {
                    "sent": "Neural networks have right?",
                    "label": 0
                },
                {
                    "sent": "So so it's like they each have their sets of strength.",
                    "label": 0
                },
                {
                    "sent": "And I think that there's going to be a lot to gain by combining those types of strengths and putting them together.",
                    "label": 0
                },
                {
                    "sent": "Hi, thanks for the lecture regarding the integral you just mentioned and.",
                    "label": 0
                },
                {
                    "sent": "Combining basean and neural networks.",
                    "label": 0
                },
                {
                    "sent": "For the marginal likelihood, can that be used in your example you gave the number of clusters in a model could be used for say the number of layers in a deep neural network.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, where are you like?",
                    "label": 0
                },
                {
                    "sent": "I can't find you I'm over here.",
                    "label": 0
                },
                {
                    "sent": "Feels really awkward to like, not know who you're talking about talking to.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Can you repeat what you said?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so regarding the marginal likelihood, I actually have a technical question on that too.",
                    "label": 0
                },
                {
                    "sent": "If you could go to that slide with yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this one.",
                    "label": 0
                },
                {
                    "sent": "So under the integral you have probability of Theta given M. Is that the prior probability.",
                    "label": 0
                },
                {
                    "sent": "Yes, that is the probability.",
                    "label": 0
                },
                {
                    "sent": "So here M just represents our model.",
                    "label": 0
                },
                {
                    "sent": "So in this case it would be like a Gaussian model and then theater the model parameters so they would be like the Gaussian mean and covariance.",
                    "label": 0
                },
                {
                    "sent": "So you gave the example of using this to decide how many clusters you should use in a clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "Could you use it for how many layers to use in a deep neural network?",
                    "label": 0
                },
                {
                    "sent": "Something like that?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Maybe I feel like I'm not the right person to ask about exactly how you would go about doing that, but like.",
                    "label": 0
                },
                {
                    "sent": "It seems like something that might be helpful.",
                    "label": 0
                },
                {
                    "sent": "I mean like I think I think the problem would be that, like right?",
                    "label": 0
                },
                {
                    "sent": "So you have like some prior distribution.",
                    "label": 0
                },
                {
                    "sent": "I could see you have some prior like thinking about it on the fly, but like I could see that you have some prior distribution over a number of layers and you want to integrate across all possible kind of like neural network structures.",
                    "label": 0
                },
                {
                    "sent": "And and their powders.",
                    "label": 0
                },
                {
                    "sent": "And I think that makes a lot of sense in general.",
                    "label": 0
                },
                {
                    "sent": "I'm a big fan of marginalization.",
                    "label": 0
                },
                {
                    "sent": "I think that would be like potentially really intractably slow.",
                    "label": 0
                },
                {
                    "sent": "But yeah, it's something not, not.",
                    "label": 0
                },
                {
                    "sent": "I mean sort of like the secret is like I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't actually do a lot of like deep neural networks, and so I've got.",
                    "label": 0
                },
                {
                    "sent": "I've got.",
                    "label": 0
                },
                {
                    "sent": "I've got one or two right?",
                    "label": 0
                },
                {
                    "sent": "And so like, yeah, you might.",
                    "label": 0
                },
                {
                    "sent": "You might be best off like also asking somebody who does do deep neural networks for an answer that question.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "I also want to say to Catherine, we deliberately do this.",
                    "label": 0
                },
                {
                    "sent": "We bring in people that don't do a lot of deep learning.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they give the intro lecture 'cause we want to get different perspectives.",
                    "label": 0
                },
                {
                    "sent": "So I want to thank her for being really brave and getting on the stage and feeling your deep learning questions.",
                    "label": 0
                },
                {
                    "sent": "So thank you.",
                    "label": 0
                },
                {
                    "sent": "An one thing as you know, one great thing about this community is there's a lot of resources online for self study and learning, so there's courses out there.",
                    "label": 0
                },
                {
                    "sent": "There's there's tax, there's different tutorials and so forth.",
                    "label": 0
                },
                {
                    "sent": "So for someone say that knows about some deep learning and want to get more into the Bayesian statistics side, or even sort of Bayesian learning, is there anything that you would recommend as like a way to pick up more material and do it self study?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm not going to have anything like really super useful to say here, but I mean, it's just like there are some classic books and statistics, right?",
                    "label": 0
                },
                {
                    "sent": "Like the Bayesian data analysis book and things like that that are worth looking into your take some statistics classes.",
                    "label": 0
                },
                {
                    "sent": "I would say.",
                    "label": 0
                },
                {
                    "sent": "I mean Juke is a very.",
                    "label": 0
                },
                {
                    "sent": "Bias place in that, like Duke, is incredibly busy and right, like Duke, is the Bayesian Mecca of the United States or maybe even North America an is.",
                    "label": 0
                },
                {
                    "sent": "It's probably why they existed Department could kind of stomach hiring me because they're like they're not really like a statistician near like a computer scientist, but you're busy in an absolute way.",
                    "label": 0
                },
                {
                    "sent": "I guess maybe you're OK.",
                    "label": 0
                },
                {
                    "sent": "So my training was in the Kingdom and people tend to be more Bayesian there.",
                    "label": 0
                },
                {
                    "sent": "I actually lived like 2 blocks away from the tomb of Thomas Bayes for awhile.",
                    "label": 0
                },
                {
                    "sent": "But yeah, I mean certainly finding like a Bayesian class.",
                    "label": 0
                },
                {
                    "sent": "I mean, so this is my way of saying, like even if you take a statistics class depending on where you are like they might not have like the same Bayesian mantra that like where I was trained and where I am currently has.",
                    "label": 0
                },
                {
                    "sent": "But if you look for.",
                    "label": 0
                },
                {
                    "sent": "Sort of like getting together.",
                    "label": 0
                },
                {
                    "sent": "Your background is just shapes and then trying to specifically aim to take Bayesian classes.",
                    "label": 0
                },
                {
                    "sent": "That's probably incredibly helpful.",
                    "label": 0
                }
            ]
        }
    }
}