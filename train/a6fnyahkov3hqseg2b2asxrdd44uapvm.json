{
    "id": "a6fnyahkov3hqseg2b2asxrdd44uapvm",
    "title": "Local Minima Free Parameterized Appearance Models",
    "info": {
        "author": [
            "Minh Hoai Nguyen, Robotics Institute, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Jan. 15, 2009",
        "recorded": "October 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/cmulls08_nguyen_lmf/",
    "segmentation": [
        [
            "OK.",
            "I think we should get started so my name in and today I'm going to talk about local minimum free parameterized appear in models and the joint work with my advisor, Fernando Dilatory.",
            "So I have given this talk before and I saw a lot of you actually was my audience before, so you might have come here for the food I guess.",
            "If you already have the food and you want to leave that file.",
            "OK, so.",
            "Let me start with the problems that we're trying to solve with image alignment, so."
        ],
        [
            "In alignment, you are given an image and you want to act like with the model and the model could be something very simple for the like the template.",
            "By solving for correspondences.",
            "The model can be something much more so sophisticated, like the active appearance model here you don't have a single template, but you have like a model for the appearance and shape variation."
        ],
        [
            "Now image alignment is very fundamental problem in computer vision and it has been addressed by many many people and in a broad sense you can characterize it in three major groups like you have the direct method where thing operate in the pixel domain you have the feature based method.",
            "When you solve for correspondence it using more discriminative distinctive features like the Ann, you have the model based method like the generative or discriminative models.",
            "So, so image alignment is an optimization problem.",
            "For example, this is optimization problem for template alignment.",
            "Here."
        ],
        [
            "D is that the input image?",
            "The draft is the reference template.",
            "And what you want to do, you want to apply the image with respect to the template.",
            "And actually, the set of pixels that you want to apply.",
            "Suppose like you start from this position, what you want to do is you want to rotate, translate and scale this one such that it fit with this template.",
            "Right now.",
            "The function F is the function that specified the type geometric transformations that you can move.",
            "This set of pixel.",
            "Here, for example, it might say hey, you can only translate it, or you can only rotate it, so F it is functions that tell you what kind of transformation you can apply on that set of pixel action.",
            "And PP is the most in paramaters that you.",
            "How much do translate?",
            "How much to rotate, and how much to scale?",
            "So this set of.",
            "Picture right here.",
            "So what you want to do so in this equation everything effects except for the most important matters that what you want to figure out how much to rotate, scale and translate up this to apply with the template.",
            "I hope it's clear, so the optimization here we have the search space.",
            "The space here is that the parameterization of the most in parameter P Ain, the second component of the optimization problem is the energy function in this case is that the L2 norm.",
            "And the third important component is the search strategy.",
            "You have the you have the space, you have the energy function and the subject is.",
            "It tells you how to search for the best optimal Parramatta and the search jet is.",
            "It could be something like exhaustive search.",
            "You can do the sliding window approach.",
            "You can do branch and bound.",
            "But these two methods are typically not efficient enough for high dimensional subspace.",
            "So the gradient based method is usually the method of choice, and it's the most popular method so far.",
            "Now a lot of research in this direction has been focusing on coming up with a better search strategy, but the point that we want to make in this talk is, hey, the energy funds in at least at equally important at the search strategy now.",
            "So instead of coming up with new strategy, what we're going to do is come up with a better energy function, better cost function for image alignment.",
            "And what we're going to do is we're going to assume that the subject is the cratin based optimization.",
            "So that's what we do.",
            "Learn the energy function for better for better image alignment."
        ],
        [
            "Now before I talk about how you can learn the cost function for image alignment, let me talk about several issues of gradient based optimization.",
            "So here I have an image.",
            "And I have a template.",
            "What I'm going to do is take the tablet, move it around, and compute the error surface.",
            "Here you go.",
            "And this is the error surface.",
            "By moving this template around and compute the errors at every location.",
            "And he showed the.",
            "The contour plot of this error surface and the black dot at several location of the local minima, for example.",
            "This local minima.",
            "Correspond to this red box here.",
            "This one correct onto the Scion box.",
            "And blue box.",
            "Then there's many other local minima.",
            "So the first problem of gradient based optimization is there too many local minimums.",
            "For example, suppose we start from.",
            "You start from here.",
            "You said you might convert to the location of the rest square, but if we start somewhere over here, then probably you're not going to convert to the desired location.",
            "Probably at the restaurant, right?",
            "So that the first issue of running by optimization.",
            "Now.",
            "Can you guess which one it actually correspond to?",
            "The global minimum is that the red 1 blue or the science?",
            "Actually none of them.",
            "So one that correspond to the local minimum at the global minimum.",
            "Actually the white one down here we actually correspond to this point of the error surface and this point of this point Contour plot.",
            "At this point, the error surface it will add itself, but it Ruth Ann.",
            "And so I should mention the second issue of the cost funds in the hay.",
            "Even what you are searching for the global minimum that not correspond to what you really want to.",
            "Even worse, in many cases the none of the global minimum actually correspond to the desired location."
        ],
        [
            "Alright, so we're going to address these two issues to learn a better cost function.",
            "No, the problem of local minima has been pointed out by many people where you have like a kind of bumpy function and there's no local minimum expected place.",
            "What we need to do is to learn a better energy function where you have the local minimum expected play and the function quasi concave.",
            "So when you do, the gradient descent is going to convert to that location.",
            "Now.",
            "The way we get to learn this core function E by enforcing the local minimum at and only at the right places.",
            "And similar to the spirit of our ideas, that says recently there's some work on this direction.",
            "For example, women at all proposed to learning like only one dimensional function to better fit the active shape model.",
            "An go at all learner discriminative tracker using boosted rank constraint.",
            "But these two methods are different from ours, are in more generative, and we directly learn the multi dimensional cost function to better for image alignment."
        ],
        [
            "OK, so again, this is the energy function for the template alignment that I shown earlier.",
            "So this turn, now that this is the best day of the quadratic cost function.",
            "So if you expand this one, you can see that it is the quadratic term where you have a quadratic term here and you have a linear term for.",
            "In the case of template alignment, A is the identity matrix and B is that the negative of the reference template."
        ],
        [
            "What we're going to do?",
            "We're going to learn Ace, and be the paramenter right up the quadratic code function to better fit for better image alignment."
        ],
        [
            "OK, so before I talk about the actual learning algorithm, let me talk about the training data for the training data.",
            "We have the image we have the template and we also assume that we know the groundtruth position."
        ],
        [
            "OK, and we not only will have only one pair image, the training image and the template, we might want to learn a common cost function across multiple training images.",
            "Here we want to learn across functions that robust to like changing illumination, changing, expressing, or even across multiple subjects.",
            "And here is the notation that we're going to use.",
            "We're going to denote the training images at die at image I at die, the reference template, and the ground truth."
        ],
        [
            "OK, so the first desire property of the cost function is a local minimum at the expected play at the play of ground truth.",
            "A necessary condition for this is the gradient at that play to vanish.",
            "And requiring the gradient vector to vanish is equivalent to requiring the L2 norm of the gradient to be 0."
        ],
        [
            "Now for the second desired properties, suppose you doing great in this end.",
            "At every step you walk along the against the direction of gradient, the 1:00 to bring you closer to the ground truth position, right?",
            "So one is the direction of gradient descent is somewhat agree with the optimal working direction.",
            "Here P is a graduate position, P is the way you add, and this is the direction the optimal working direction we walk along that direction is the fastest way to get to the ground truth.",
            "Now the blue vector is the gradient of this cost function at this point, and the red one is the direction of gradient descent against the gradient direction, right?",
            "So in this case you can see that it bring bring closer to graduate position.",
            "However, if you look at this point, this is the gradient direction of gradient decent.",
            "If you start from this point, probably not.",
            "Not going to convert to the crowd should position.",
            "So what we want a II desired property is the direction of gradient descent.",
            "Agree with the optimal working direction."
        ],
        [
            "And what we're going to do is we can do in folks that the angle between the two vectors, the direction optimal walking direction and the direction of gradient descent is smaller than 90 degrees.",
            "For example, here it's going to converge here, but in this case it's going to diverse.",
            "You're not going to convert this position.",
            "And if you translate mathematically, translate it to it lcisd.",
            "The DOT product between the two vectors, the gradient descent direction and the optimal direction is greater than 0.",
            "It positive.",
            "Now we're going to enforce this constraint on a certain neighborhoods surrounding the ground should position we don't want to consider something tools close to the graduate position because we consider everything that like convert to this local neighborhood, it's good enough.",
            "And we don't want to like consider info the constraint for all the space that we have because like in typical Lee in for example, in alignment attracting a lot of the time, you can assume that the one frame to the next frames or object doesn't move that much.",
            "OK. OK, so the learning problem we have.",
            "We talk about two issues of the gradient based optimization and then we come up with two desired properties to address those two issues.",
            "The first one."
        ],
        [
            "The gradient vanish at the desired Groucho, producing the desired location.",
            "The second one is.",
            "The direction of gradient descent agree with the optimal working direction.",
            "So what we're going to do is to learn A&B, the parameter of the cost function to satisfy these two constraints.",
            "The problem is.",
            "They might not any A&B that simultaneously satisfy both of these constraints across all the training images.",
            "So what do we do?",
            "With that relaxed problem, so instead of requiring having a hash constant, what we want to do is to minimize instead.",
            "And for the second constraint, 1st instead of requiring it to be positive, we're going to require to greater sum than some margin.",
            "And then.",
            "Introducing Slack variable you allow the constraint to be violated, but penalized for the slackness."
        ],
        [
            "Oh great, now this.",
            "This is the formulation.",
            "This is the framework.",
            "This is optimization problem that we're going to use to learn A&B.",
            "The parameter of the code function.",
            "So how can we optimize for this?",
            "Great is the convex optimization problem and in machine learning if you can solve it convex submodular.",
            "You done, you can publish paper, right?",
            "Like I did OK, but.",
            "One problem, one issue here you have to deal with infinite number of constraint.",
            "And the way we deal with it is to maintain a subset of most violated constraint.",
            "And we iteratively update that."
        ],
        [
            "OK. Let's move to the Special K. OK, the word template alignment.",
            "So this is a cost function for template alignment that I showed at least twice before.",
            "And it is equivalent to insult, you know, you take it, you expand it.",
            "It's the same with this one, right?",
            "No big deal.",
            "For the word template alignment, all you need to do is put the diagonal metric in the middle.",
            "What you mean is, hey, I'm going to give a different word for this in pixel.",
            "Not every pixel should get like the same where there's no reason why the optimal right does it fit into the quadratic or function.",
            "Of course it does.",
            "Here is that the dynamic W&B is the multiplication of dynamic W and the reference template with the negative side in front right?",
            "So we have a new parameterization for A&B, So what we need to do is learn the wet W. Such that it works for template alignment."
        ],
        [
            "So yeah, I'm showing the experiment with the Gaussian.",
            "We have the template and the image an.",
            "This is the 3D representation of the template and the image.",
            "You can think of like the intensity of the template like 4 apply Gaussian."
        ],
        [
            "What we need to do?",
            "We take the template.",
            "We move it around.",
            "And compute the errors of it, and this is the error surface that we get.",
            "This is not good.",
            "It's got a great work around the local minimum here so.",
            "Now, after learning the word template, this is the error surface that we get.",
            "And this is the set of words that we learned."
        ],
        [
            "OK, so now that move on to something more like a much more sophisticated model like the active appearance models.",
            "Is the active appear in model?",
            "You have the image.",
            "What we want to do is kind of.",
            "You want to apply the lower the facial landmark to the desired location.",
            "The correct location that we want to do.",
            "You can see that for the template it very cannot deal with a nonrigid motion.",
            "For example, if a person is talking, then the template alignment doesn't work right?",
            "So we need something more."
        ],
        [
            "Better for the OK.",
            "So this is the energy function for template alignment, not the active appearance model.",
            "But for active appearance model P actually the combination for a 5 panel maturan shape coefficient.",
            "And the chef you have a diminshed plus some.",
            "Variations of the shared variation.",
            "So P is a combination of like translation, rotation, scale an.",
            "This one is like hey is the nonrigid motion that you need to optimize over 2.",
            "And the second different you no longer have only one reference template.",
            "You actually have.",
            "The whole subspace for the appearing variation.",
            "So these appear in variation using training data.",
            "The idea is the appearing of your image can be the linear combination of the thing in your training data.",
            "Right so.",
            "So so usually you and the basic for sale variation are trained during the training data and in the optimization you need to optimize P and also see further appearance.",
            "That's why you minimize overseas here too.",
            "So from the point of view of the optimization problem, this one doesn't doesn't change that much.",
            "Moving from template alignment to active appearance model, not much different.",
            "It's just an optimization problem.",
            "OK, so this is."
        ],
        [
            "Energy function for active appearance model.",
            "When you minimize this function.",
            "And does it fit into the kind of quadratic cost function?",
            "Yes it does, because a is that the difference between the identity matrix and U transpose is the basis for the appearance variation and B is the zero vector.",
            "Now, So what we need to do is learn a word basic for active appearance model.",
            "One of the things here, if you look at the coefficient for edge of this term here it equals zero or one.",
            "But there's no reason why it optimal what we're going to do is put the wet in front of it, and we're going to learn the wait for it.",
            "Now note that the K here and the bigger here are different.",
            "OK, here, usually much smaller than K here and you obtain using the PCI how much the component of the appearance you want to preserve.",
            "Like 90% K pretty big 80% case smaller here if we throw in all the and then we're going to learn the word for edge of individual components."
        ],
        [
            "So the experiment we have with the multiple database, we have 337 subjects at different expressions, fire, different expression faces, associates that quickly.",
            "68 hand labeled landmarks we use.",
            "As for the ground truth and for testing we randomly perturb it and to see how well it converts to the graduate position.",
            "And so this is the."
        ],
        [
            "Is out of the way.",
            "PCI basis for image alignment.",
            "If we look from left to right, the climate problem become harder because we perturb the perturbation amount, become bigger and bigger.",
            "Now the in the first row is the initial, so.",
            "Errors, you can see it getting bigger and bigger.",
            "And the next three row are the result of different three different PCI model with 90% energy, 80% and 70%.",
            "And the last one is the result of our method.",
            "Now that forget about the last row for the moment.",
            "If you look at the three PCA model, if you can see that for this more participation, PCA reserving 90% of energy, actually that's the best.",
            "However, at the alignment problem become harder, PCI preserving lower amount of energy.",
            "Actually, that's much better.",
            "The reason for it is if you present more energy for the appearance variation, you have more local minima.",
            "How so?",
            "If you know about like Kenny Edge detector detectors, that your tradeoff between.",
            "Localization and detection.",
            "Here we have the same energy.",
            "We have the tradeoff between having a lot of small number of local minima and having the local minima at the desired players expected place.",
            "So our method, what it does is to learn the optimum tradeoff between those two desired criteria."
        ],
        [
            "OK, so this last slide wake up OK, so.",
            "So in this talk I talk about learning the optimal metric I envy of the cost function.",
            "For image alignment.",
            "We demonstrated for the tablet alignment and active appearance models.",
            "And the way we learn the cost function by enforcing that local minima occur at an early at the right place and the learning problem is convex quadratic formulation and the type of message.",
            "And the only thing that I want you to remember from this talk today it probably we should spend less time on the search strategy and spend more time on what we are searching for and with this."
        ],
        [
            "When to convert and thank you.",
            "Yes they do."
        ],
        [
            "I'm not sure I completely understood what you did to solve this.",
            "Is it just a separate Oracle?",
            "Something.",
            "So there are two ways like we start with something that you.",
            "Random sampling of the thing of the neighborhood, and then we do a system for several iteration.",
            "We after we get the new costs and we run it, we fight the like the constraint where it violated and it not crowd.",
            "Should we add it to the set of constraints?",
            "So is this something you can?",
            "I mean is this guys give you the right answer or is this?",
            "You came up with it.",
            "Well, first of all, for the constraint generation, it only work if you guarantee to find the global optimum right at every step.",
            "But in this case it doesn't really find the global optimum every step, so it keeps the heuristic instead of the theoretical.",
            "Justification for it, yeah.",
            "Yes.",
            "Or do you need them?",
            "Well so A&B.",
            "I envy the Parramatta cost function right.",
            "Usually very big metrican we cannot really directly learn it, so usually what I did quote to repair a matter I to make it a smaller number of parameters so we can learn.",
            "And the nice thing about A&B is the cost function is linear in terms of A&B.",
            "That's why the learning problem become quadratic in terms of I and B and is a convict problem.",
            "Yeah.",
            "Any other questions?",
            "Yes, you when you're comparing at the PCA for the parent follow were you able to throw some of the components out you had so you can create the speed of the.",
            "Yeah, so so that's a good point.",
            "Like the more component you have, the slower it is, right?",
            "Yeah, so typically we used more number.",
            "Like if you preserve a 70% of the appearance is only like 10 component or something like that.",
            "So it very fast at that.",
            "But there's a trade off for that.",
            "Yeah, but yeah, a lot of them are zero, yes?",
            "Yes, John, so there are some other convex formulations of metric learning.",
            "This compares to.",
            "Convert formulation of metric learning so so learning Mahalanobis distance matrix?",
            "I think there are some convex formulations of that Empire work well, so there's a lot of play convex formulation for the learning problem, but as far as I know there's none of them actually do for the image alignment problem.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I think we should get started so my name in and today I'm going to talk about local minimum free parameterized appear in models and the joint work with my advisor, Fernando Dilatory.",
                    "label": 0
                },
                {
                    "sent": "So I have given this talk before and I saw a lot of you actually was my audience before, so you might have come here for the food I guess.",
                    "label": 0
                },
                {
                    "sent": "If you already have the food and you want to leave that file.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Let me start with the problems that we're trying to solve with image alignment, so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In alignment, you are given an image and you want to act like with the model and the model could be something very simple for the like the template.",
                    "label": 0
                },
                {
                    "sent": "By solving for correspondences.",
                    "label": 0
                },
                {
                    "sent": "The model can be something much more so sophisticated, like the active appearance model here you don't have a single template, but you have like a model for the appearance and shape variation.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now image alignment is very fundamental problem in computer vision and it has been addressed by many many people and in a broad sense you can characterize it in three major groups like you have the direct method where thing operate in the pixel domain you have the feature based method.",
                    "label": 0
                },
                {
                    "sent": "When you solve for correspondence it using more discriminative distinctive features like the Ann, you have the model based method like the generative or discriminative models.",
                    "label": 0
                },
                {
                    "sent": "So, so image alignment is an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "For example, this is optimization problem for template alignment.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "D is that the input image?",
                    "label": 0
                },
                {
                    "sent": "The draft is the reference template.",
                    "label": 0
                },
                {
                    "sent": "And what you want to do, you want to apply the image with respect to the template.",
                    "label": 0
                },
                {
                    "sent": "And actually, the set of pixels that you want to apply.",
                    "label": 0
                },
                {
                    "sent": "Suppose like you start from this position, what you want to do is you want to rotate, translate and scale this one such that it fit with this template.",
                    "label": 0
                },
                {
                    "sent": "Right now.",
                    "label": 0
                },
                {
                    "sent": "The function F is the function that specified the type geometric transformations that you can move.",
                    "label": 0
                },
                {
                    "sent": "This set of pixel.",
                    "label": 0
                },
                {
                    "sent": "Here, for example, it might say hey, you can only translate it, or you can only rotate it, so F it is functions that tell you what kind of transformation you can apply on that set of pixel action.",
                    "label": 0
                },
                {
                    "sent": "And PP is the most in paramaters that you.",
                    "label": 0
                },
                {
                    "sent": "How much do translate?",
                    "label": 0
                },
                {
                    "sent": "How much to rotate, and how much to scale?",
                    "label": 0
                },
                {
                    "sent": "So this set of.",
                    "label": 0
                },
                {
                    "sent": "Picture right here.",
                    "label": 0
                },
                {
                    "sent": "So what you want to do so in this equation everything effects except for the most important matters that what you want to figure out how much to rotate, scale and translate up this to apply with the template.",
                    "label": 0
                },
                {
                    "sent": "I hope it's clear, so the optimization here we have the search space.",
                    "label": 0
                },
                {
                    "sent": "The space here is that the parameterization of the most in parameter P Ain, the second component of the optimization problem is the energy function in this case is that the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "And the third important component is the search strategy.",
                    "label": 0
                },
                {
                    "sent": "You have the you have the space, you have the energy function and the subject is.",
                    "label": 0
                },
                {
                    "sent": "It tells you how to search for the best optimal Parramatta and the search jet is.",
                    "label": 0
                },
                {
                    "sent": "It could be something like exhaustive search.",
                    "label": 0
                },
                {
                    "sent": "You can do the sliding window approach.",
                    "label": 0
                },
                {
                    "sent": "You can do branch and bound.",
                    "label": 0
                },
                {
                    "sent": "But these two methods are typically not efficient enough for high dimensional subspace.",
                    "label": 0
                },
                {
                    "sent": "So the gradient based method is usually the method of choice, and it's the most popular method so far.",
                    "label": 0
                },
                {
                    "sent": "Now a lot of research in this direction has been focusing on coming up with a better search strategy, but the point that we want to make in this talk is, hey, the energy funds in at least at equally important at the search strategy now.",
                    "label": 0
                },
                {
                    "sent": "So instead of coming up with new strategy, what we're going to do is come up with a better energy function, better cost function for image alignment.",
                    "label": 0
                },
                {
                    "sent": "And what we're going to do is we're going to assume that the subject is the cratin based optimization.",
                    "label": 0
                },
                {
                    "sent": "So that's what we do.",
                    "label": 0
                },
                {
                    "sent": "Learn the energy function for better for better image alignment.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now before I talk about how you can learn the cost function for image alignment, let me talk about several issues of gradient based optimization.",
                    "label": 0
                },
                {
                    "sent": "So here I have an image.",
                    "label": 0
                },
                {
                    "sent": "And I have a template.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to do is take the tablet, move it around, and compute the error surface.",
                    "label": 0
                },
                {
                    "sent": "Here you go.",
                    "label": 0
                },
                {
                    "sent": "And this is the error surface.",
                    "label": 0
                },
                {
                    "sent": "By moving this template around and compute the errors at every location.",
                    "label": 0
                },
                {
                    "sent": "And he showed the.",
                    "label": 0
                },
                {
                    "sent": "The contour plot of this error surface and the black dot at several location of the local minima, for example.",
                    "label": 0
                },
                {
                    "sent": "This local minima.",
                    "label": 0
                },
                {
                    "sent": "Correspond to this red box here.",
                    "label": 0
                },
                {
                    "sent": "This one correct onto the Scion box.",
                    "label": 0
                },
                {
                    "sent": "And blue box.",
                    "label": 0
                },
                {
                    "sent": "Then there's many other local minima.",
                    "label": 0
                },
                {
                    "sent": "So the first problem of gradient based optimization is there too many local minimums.",
                    "label": 0
                },
                {
                    "sent": "For example, suppose we start from.",
                    "label": 0
                },
                {
                    "sent": "You start from here.",
                    "label": 0
                },
                {
                    "sent": "You said you might convert to the location of the rest square, but if we start somewhere over here, then probably you're not going to convert to the desired location.",
                    "label": 0
                },
                {
                    "sent": "Probably at the restaurant, right?",
                    "label": 0
                },
                {
                    "sent": "So that the first issue of running by optimization.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Can you guess which one it actually correspond to?",
                    "label": 0
                },
                {
                    "sent": "The global minimum is that the red 1 blue or the science?",
                    "label": 0
                },
                {
                    "sent": "Actually none of them.",
                    "label": 0
                },
                {
                    "sent": "So one that correspond to the local minimum at the global minimum.",
                    "label": 0
                },
                {
                    "sent": "Actually the white one down here we actually correspond to this point of the error surface and this point of this point Contour plot.",
                    "label": 0
                },
                {
                    "sent": "At this point, the error surface it will add itself, but it Ruth Ann.",
                    "label": 0
                },
                {
                    "sent": "And so I should mention the second issue of the cost funds in the hay.",
                    "label": 0
                },
                {
                    "sent": "Even what you are searching for the global minimum that not correspond to what you really want to.",
                    "label": 0
                },
                {
                    "sent": "Even worse, in many cases the none of the global minimum actually correspond to the desired location.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so we're going to address these two issues to learn a better cost function.",
                    "label": 0
                },
                {
                    "sent": "No, the problem of local minima has been pointed out by many people where you have like a kind of bumpy function and there's no local minimum expected place.",
                    "label": 0
                },
                {
                    "sent": "What we need to do is to learn a better energy function where you have the local minimum expected play and the function quasi concave.",
                    "label": 0
                },
                {
                    "sent": "So when you do, the gradient descent is going to convert to that location.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The way we get to learn this core function E by enforcing the local minimum at and only at the right places.",
                    "label": 0
                },
                {
                    "sent": "And similar to the spirit of our ideas, that says recently there's some work on this direction.",
                    "label": 0
                },
                {
                    "sent": "For example, women at all proposed to learning like only one dimensional function to better fit the active shape model.",
                    "label": 0
                },
                {
                    "sent": "An go at all learner discriminative tracker using boosted rank constraint.",
                    "label": 0
                },
                {
                    "sent": "But these two methods are different from ours, are in more generative, and we directly learn the multi dimensional cost function to better for image alignment.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so again, this is the energy function for the template alignment that I shown earlier.",
                    "label": 0
                },
                {
                    "sent": "So this turn, now that this is the best day of the quadratic cost function.",
                    "label": 0
                },
                {
                    "sent": "So if you expand this one, you can see that it is the quadratic term where you have a quadratic term here and you have a linear term for.",
                    "label": 0
                },
                {
                    "sent": "In the case of template alignment, A is the identity matrix and B is that the negative of the reference template.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we're going to do?",
                    "label": 0
                },
                {
                    "sent": "We're going to learn Ace, and be the paramenter right up the quadratic code function to better fit for better image alignment.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so before I talk about the actual learning algorithm, let me talk about the training data for the training data.",
                    "label": 0
                },
                {
                    "sent": "We have the image we have the template and we also assume that we know the groundtruth position.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and we not only will have only one pair image, the training image and the template, we might want to learn a common cost function across multiple training images.",
                    "label": 0
                },
                {
                    "sent": "Here we want to learn across functions that robust to like changing illumination, changing, expressing, or even across multiple subjects.",
                    "label": 0
                },
                {
                    "sent": "And here is the notation that we're going to use.",
                    "label": 0
                },
                {
                    "sent": "We're going to denote the training images at die at image I at die, the reference template, and the ground truth.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the first desire property of the cost function is a local minimum at the expected play at the play of ground truth.",
                    "label": 0
                },
                {
                    "sent": "A necessary condition for this is the gradient at that play to vanish.",
                    "label": 0
                },
                {
                    "sent": "And requiring the gradient vector to vanish is equivalent to requiring the L2 norm of the gradient to be 0.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now for the second desired properties, suppose you doing great in this end.",
                    "label": 0
                },
                {
                    "sent": "At every step you walk along the against the direction of gradient, the 1:00 to bring you closer to the ground truth position, right?",
                    "label": 0
                },
                {
                    "sent": "So one is the direction of gradient descent is somewhat agree with the optimal working direction.",
                    "label": 0
                },
                {
                    "sent": "Here P is a graduate position, P is the way you add, and this is the direction the optimal working direction we walk along that direction is the fastest way to get to the ground truth.",
                    "label": 0
                },
                {
                    "sent": "Now the blue vector is the gradient of this cost function at this point, and the red one is the direction of gradient descent against the gradient direction, right?",
                    "label": 0
                },
                {
                    "sent": "So in this case you can see that it bring bring closer to graduate position.",
                    "label": 0
                },
                {
                    "sent": "However, if you look at this point, this is the gradient direction of gradient decent.",
                    "label": 0
                },
                {
                    "sent": "If you start from this point, probably not.",
                    "label": 0
                },
                {
                    "sent": "Not going to convert to the crowd should position.",
                    "label": 0
                },
                {
                    "sent": "So what we want a II desired property is the direction of gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Agree with the optimal working direction.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we're going to do is we can do in folks that the angle between the two vectors, the direction optimal walking direction and the direction of gradient descent is smaller than 90 degrees.",
                    "label": 0
                },
                {
                    "sent": "For example, here it's going to converge here, but in this case it's going to diverse.",
                    "label": 0
                },
                {
                    "sent": "You're not going to convert this position.",
                    "label": 0
                },
                {
                    "sent": "And if you translate mathematically, translate it to it lcisd.",
                    "label": 0
                },
                {
                    "sent": "The DOT product between the two vectors, the gradient descent direction and the optimal direction is greater than 0.",
                    "label": 1
                },
                {
                    "sent": "It positive.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to enforce this constraint on a certain neighborhoods surrounding the ground should position we don't want to consider something tools close to the graduate position because we consider everything that like convert to this local neighborhood, it's good enough.",
                    "label": 0
                },
                {
                    "sent": "And we don't want to like consider info the constraint for all the space that we have because like in typical Lee in for example, in alignment attracting a lot of the time, you can assume that the one frame to the next frames or object doesn't move that much.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so the learning problem we have.",
                    "label": 0
                },
                {
                    "sent": "We talk about two issues of the gradient based optimization and then we come up with two desired properties to address those two issues.",
                    "label": 0
                },
                {
                    "sent": "The first one.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The gradient vanish at the desired Groucho, producing the desired location.",
                    "label": 0
                },
                {
                    "sent": "The second one is.",
                    "label": 0
                },
                {
                    "sent": "The direction of gradient descent agree with the optimal working direction.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is to learn A&B, the parameter of the cost function to satisfy these two constraints.",
                    "label": 0
                },
                {
                    "sent": "The problem is.",
                    "label": 0
                },
                {
                    "sent": "They might not any A&B that simultaneously satisfy both of these constraints across all the training images.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "With that relaxed problem, so instead of requiring having a hash constant, what we want to do is to minimize instead.",
                    "label": 0
                },
                {
                    "sent": "And for the second constraint, 1st instead of requiring it to be positive, we're going to require to greater sum than some margin.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Introducing Slack variable you allow the constraint to be violated, but penalized for the slackness.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh great, now this.",
                    "label": 0
                },
                {
                    "sent": "This is the formulation.",
                    "label": 0
                },
                {
                    "sent": "This is the framework.",
                    "label": 0
                },
                {
                    "sent": "This is optimization problem that we're going to use to learn A&B.",
                    "label": 0
                },
                {
                    "sent": "The parameter of the code function.",
                    "label": 0
                },
                {
                    "sent": "So how can we optimize for this?",
                    "label": 0
                },
                {
                    "sent": "Great is the convex optimization problem and in machine learning if you can solve it convex submodular.",
                    "label": 1
                },
                {
                    "sent": "You done, you can publish paper, right?",
                    "label": 0
                },
                {
                    "sent": "Like I did OK, but.",
                    "label": 0
                },
                {
                    "sent": "One problem, one issue here you have to deal with infinite number of constraint.",
                    "label": 1
                },
                {
                    "sent": "And the way we deal with it is to maintain a subset of most violated constraint.",
                    "label": 1
                },
                {
                    "sent": "And we iteratively update that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Let's move to the Special K. OK, the word template alignment.",
                    "label": 0
                },
                {
                    "sent": "So this is a cost function for template alignment that I showed at least twice before.",
                    "label": 0
                },
                {
                    "sent": "And it is equivalent to insult, you know, you take it, you expand it.",
                    "label": 0
                },
                {
                    "sent": "It's the same with this one, right?",
                    "label": 0
                },
                {
                    "sent": "No big deal.",
                    "label": 0
                },
                {
                    "sent": "For the word template alignment, all you need to do is put the diagonal metric in the middle.",
                    "label": 0
                },
                {
                    "sent": "What you mean is, hey, I'm going to give a different word for this in pixel.",
                    "label": 0
                },
                {
                    "sent": "Not every pixel should get like the same where there's no reason why the optimal right does it fit into the quadratic or function.",
                    "label": 0
                },
                {
                    "sent": "Of course it does.",
                    "label": 0
                },
                {
                    "sent": "Here is that the dynamic W&B is the multiplication of dynamic W and the reference template with the negative side in front right?",
                    "label": 0
                },
                {
                    "sent": "So we have a new parameterization for A&B, So what we need to do is learn the wet W. Such that it works for template alignment.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, I'm showing the experiment with the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "We have the template and the image an.",
                    "label": 0
                },
                {
                    "sent": "This is the 3D representation of the template and the image.",
                    "label": 0
                },
                {
                    "sent": "You can think of like the intensity of the template like 4 apply Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we need to do?",
                    "label": 0
                },
                {
                    "sent": "We take the template.",
                    "label": 0
                },
                {
                    "sent": "We move it around.",
                    "label": 0
                },
                {
                    "sent": "And compute the errors of it, and this is the error surface that we get.",
                    "label": 0
                },
                {
                    "sent": "This is not good.",
                    "label": 0
                },
                {
                    "sent": "It's got a great work around the local minimum here so.",
                    "label": 0
                },
                {
                    "sent": "Now, after learning the word template, this is the error surface that we get.",
                    "label": 0
                },
                {
                    "sent": "And this is the set of words that we learned.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now that move on to something more like a much more sophisticated model like the active appearance models.",
                    "label": 1
                },
                {
                    "sent": "Is the active appear in model?",
                    "label": 0
                },
                {
                    "sent": "You have the image.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is kind of.",
                    "label": 0
                },
                {
                    "sent": "You want to apply the lower the facial landmark to the desired location.",
                    "label": 0
                },
                {
                    "sent": "The correct location that we want to do.",
                    "label": 0
                },
                {
                    "sent": "You can see that for the template it very cannot deal with a nonrigid motion.",
                    "label": 0
                },
                {
                    "sent": "For example, if a person is talking, then the template alignment doesn't work right?",
                    "label": 0
                },
                {
                    "sent": "So we need something more.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Better for the OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the energy function for template alignment, not the active appearance model.",
                    "label": 1
                },
                {
                    "sent": "But for active appearance model P actually the combination for a 5 panel maturan shape coefficient.",
                    "label": 0
                },
                {
                    "sent": "And the chef you have a diminshed plus some.",
                    "label": 0
                },
                {
                    "sent": "Variations of the shared variation.",
                    "label": 0
                },
                {
                    "sent": "So P is a combination of like translation, rotation, scale an.",
                    "label": 0
                },
                {
                    "sent": "This one is like hey is the nonrigid motion that you need to optimize over 2.",
                    "label": 0
                },
                {
                    "sent": "And the second different you no longer have only one reference template.",
                    "label": 0
                },
                {
                    "sent": "You actually have.",
                    "label": 0
                },
                {
                    "sent": "The whole subspace for the appearing variation.",
                    "label": 0
                },
                {
                    "sent": "So these appear in variation using training data.",
                    "label": 0
                },
                {
                    "sent": "The idea is the appearing of your image can be the linear combination of the thing in your training data.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "So so usually you and the basic for sale variation are trained during the training data and in the optimization you need to optimize P and also see further appearance.",
                    "label": 0
                },
                {
                    "sent": "That's why you minimize overseas here too.",
                    "label": 0
                },
                {
                    "sent": "So from the point of view of the optimization problem, this one doesn't doesn't change that much.",
                    "label": 0
                },
                {
                    "sent": "Moving from template alignment to active appearance model, not much different.",
                    "label": 0
                },
                {
                    "sent": "It's just an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Energy function for active appearance model.",
                    "label": 0
                },
                {
                    "sent": "When you minimize this function.",
                    "label": 0
                },
                {
                    "sent": "And does it fit into the kind of quadratic cost function?",
                    "label": 0
                },
                {
                    "sent": "Yes it does, because a is that the difference between the identity matrix and U transpose is the basis for the appearance variation and B is the zero vector.",
                    "label": 0
                },
                {
                    "sent": "Now, So what we need to do is learn a word basic for active appearance model.",
                    "label": 0
                },
                {
                    "sent": "One of the things here, if you look at the coefficient for edge of this term here it equals zero or one.",
                    "label": 0
                },
                {
                    "sent": "But there's no reason why it optimal what we're going to do is put the wet in front of it, and we're going to learn the wait for it.",
                    "label": 0
                },
                {
                    "sent": "Now note that the K here and the bigger here are different.",
                    "label": 0
                },
                {
                    "sent": "OK, here, usually much smaller than K here and you obtain using the PCI how much the component of the appearance you want to preserve.",
                    "label": 0
                },
                {
                    "sent": "Like 90% K pretty big 80% case smaller here if we throw in all the and then we're going to learn the word for edge of individual components.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the experiment we have with the multiple database, we have 337 subjects at different expressions, fire, different expression faces, associates that quickly.",
                    "label": 0
                },
                {
                    "sent": "68 hand labeled landmarks we use.",
                    "label": 0
                },
                {
                    "sent": "As for the ground truth and for testing we randomly perturb it and to see how well it converts to the graduate position.",
                    "label": 0
                },
                {
                    "sent": "And so this is the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is out of the way.",
                    "label": 0
                },
                {
                    "sent": "PCI basis for image alignment.",
                    "label": 0
                },
                {
                    "sent": "If we look from left to right, the climate problem become harder because we perturb the perturbation amount, become bigger and bigger.",
                    "label": 0
                },
                {
                    "sent": "Now the in the first row is the initial, so.",
                    "label": 0
                },
                {
                    "sent": "Errors, you can see it getting bigger and bigger.",
                    "label": 0
                },
                {
                    "sent": "And the next three row are the result of different three different PCI model with 90% energy, 80% and 70%.",
                    "label": 0
                },
                {
                    "sent": "And the last one is the result of our method.",
                    "label": 0
                },
                {
                    "sent": "Now that forget about the last row for the moment.",
                    "label": 0
                },
                {
                    "sent": "If you look at the three PCA model, if you can see that for this more participation, PCA reserving 90% of energy, actually that's the best.",
                    "label": 0
                },
                {
                    "sent": "However, at the alignment problem become harder, PCI preserving lower amount of energy.",
                    "label": 0
                },
                {
                    "sent": "Actually, that's much better.",
                    "label": 0
                },
                {
                    "sent": "The reason for it is if you present more energy for the appearance variation, you have more local minima.",
                    "label": 0
                },
                {
                    "sent": "How so?",
                    "label": 0
                },
                {
                    "sent": "If you know about like Kenny Edge detector detectors, that your tradeoff between.",
                    "label": 0
                },
                {
                    "sent": "Localization and detection.",
                    "label": 0
                },
                {
                    "sent": "Here we have the same energy.",
                    "label": 0
                },
                {
                    "sent": "We have the tradeoff between having a lot of small number of local minima and having the local minima at the desired players expected place.",
                    "label": 0
                },
                {
                    "sent": "So our method, what it does is to learn the optimum tradeoff between those two desired criteria.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this last slide wake up OK, so.",
                    "label": 0
                },
                {
                    "sent": "So in this talk I talk about learning the optimal metric I envy of the cost function.",
                    "label": 0
                },
                {
                    "sent": "For image alignment.",
                    "label": 0
                },
                {
                    "sent": "We demonstrated for the tablet alignment and active appearance models.",
                    "label": 0
                },
                {
                    "sent": "And the way we learn the cost function by enforcing that local minima occur at an early at the right place and the learning problem is convex quadratic formulation and the type of message.",
                    "label": 0
                },
                {
                    "sent": "And the only thing that I want you to remember from this talk today it probably we should spend less time on the search strategy and spend more time on what we are searching for and with this.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When to convert and thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes they do.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not sure I completely understood what you did to solve this.",
                    "label": 0
                },
                {
                    "sent": "Is it just a separate Oracle?",
                    "label": 0
                },
                {
                    "sent": "Something.",
                    "label": 0
                },
                {
                    "sent": "So there are two ways like we start with something that you.",
                    "label": 0
                },
                {
                    "sent": "Random sampling of the thing of the neighborhood, and then we do a system for several iteration.",
                    "label": 0
                },
                {
                    "sent": "We after we get the new costs and we run it, we fight the like the constraint where it violated and it not crowd.",
                    "label": 0
                },
                {
                    "sent": "Should we add it to the set of constraints?",
                    "label": 0
                },
                {
                    "sent": "So is this something you can?",
                    "label": 0
                },
                {
                    "sent": "I mean is this guys give you the right answer or is this?",
                    "label": 0
                },
                {
                    "sent": "You came up with it.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, for the constraint generation, it only work if you guarantee to find the global optimum right at every step.",
                    "label": 0
                },
                {
                    "sent": "But in this case it doesn't really find the global optimum every step, so it keeps the heuristic instead of the theoretical.",
                    "label": 0
                },
                {
                    "sent": "Justification for it, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Or do you need them?",
                    "label": 0
                },
                {
                    "sent": "Well so A&B.",
                    "label": 0
                },
                {
                    "sent": "I envy the Parramatta cost function right.",
                    "label": 0
                },
                {
                    "sent": "Usually very big metrican we cannot really directly learn it, so usually what I did quote to repair a matter I to make it a smaller number of parameters so we can learn.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing about A&B is the cost function is linear in terms of A&B.",
                    "label": 0
                },
                {
                    "sent": "That's why the learning problem become quadratic in terms of I and B and is a convict problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Yes, you when you're comparing at the PCA for the parent follow were you able to throw some of the components out you had so you can create the speed of the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so that's a good point.",
                    "label": 0
                },
                {
                    "sent": "Like the more component you have, the slower it is, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so typically we used more number.",
                    "label": 0
                },
                {
                    "sent": "Like if you preserve a 70% of the appearance is only like 10 component or something like that.",
                    "label": 0
                },
                {
                    "sent": "So it very fast at that.",
                    "label": 0
                },
                {
                    "sent": "But there's a trade off for that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but yeah, a lot of them are zero, yes?",
                    "label": 0
                },
                {
                    "sent": "Yes, John, so there are some other convex formulations of metric learning.",
                    "label": 0
                },
                {
                    "sent": "This compares to.",
                    "label": 0
                },
                {
                    "sent": "Convert formulation of metric learning so so learning Mahalanobis distance matrix?",
                    "label": 0
                },
                {
                    "sent": "I think there are some convex formulations of that Empire work well, so there's a lot of play convex formulation for the learning problem, but as far as I know there's none of them actually do for the image alignment problem.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}