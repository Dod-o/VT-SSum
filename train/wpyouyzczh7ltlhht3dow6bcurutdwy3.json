{
    "id": "wpyouyzczh7ltlhht3dow6bcurutdwy3",
    "title": "Incorporating Prior Knowledge into NLP with Markov Logic",
    "info": {
        "author": [
            "Pedro Domingos, Dept. of Computer Science & Engineering, University of Washington"
        ],
        "published": "Aug. 11, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Natural Language Processing",
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/icml08_domingos_ipk/",
    "segmentation": [
        [
            "Alright, thank you all for being here.",
            "Thanks for inviting me.",
            "I'm actually quite happy to be at this workshop because this is really a topic that I'm very much interested in and you know, as I will talk a little bit about in this talk, we've actually started to apply Markov logic to NLP because we think it's a great match, so.",
            "So the title my talk is incorporating prior knowledge into NLP with Markov logic.",
            "This is work that I've done at UW with these people.",
            "Stanley Kok, Daniel Loud- Poon met, Richardson Prox, England, Mark Sumner, and Julie Wang."
        ],
        [
            "Here's a brief overview.",
            "I will begin with a little bit of motivation and background, and then I will do an intro to Markov logic.",
            "Could talk about some of our latest greatest influence in learning algorithms, and then exemplify how we can apply these techniques to incorporate prior knowledge into NLP in a way that I think is a very flexible and powerful.",
            "And then I will conclude with a little."
        ],
        [
            "Of discussion.",
            "So here's the motivation.",
            "Language and knowledge.",
            "I really like the chicken and the egg.",
            "Because to understand language you need knowledge.",
            "You need knowledge to resolve parsing ambiguities.",
            "There's all these problems in and out, you know, like recently in the in the textual entailment domain, one of the winning teams did in an error analysis and see what is the biggest thing that we're missing to do better 'cause nobody is doing that well, it's knowledge.",
            "So without knowledge of the world and linguistic knowledge, you can only get so far in NLP, so you need knowledge for language.",
            "On the other hand, you need language to acquire knowledge.",
            "Encoding knowledge manually.",
            "You know using you know expert system techniques is just too slow, too expensive, not worth.",
            "It's never going to happen.",
            "The only way it's ever going to happen is if you acquire that knowledge from text.",
            "So we're in this interesting situation that if we have the knowledge, we would know how to do NLP and if we knew how to do an LP then we have the knowledge, right?",
            "So how do we solve this problem?",
            "There's only one way that I know solving problems like this and that's to bootstrap.",
            "The way you Bootstrap in this case is you start with a small hand coded knowledge base.",
            "The smallest you can get away with.",
            "And then you use that to help process some text that's hopefully not too hard, or at least go to a bunch of texts and get something out of it and then and then you take that and add it to your knowledge base.",
            "And now you have a better knowledge base.",
            "Still not perfect, but better and hopefully with this knowledge base you cannot process some more text than maybe some harder text.",
            "And now you keep going.",
            "Now think of this as a kind of em.",
            "At a high level, right?",
            "If in each step of this cycle you get more knowledge than you had before, and better ability to process language than you had before, and you keep going long enough, then you potentially will do amazing things at an acceptable cost, right?",
            "So this is the dream."
        ],
        [
            "If you will, another question is, if we're going to make this happen, what do we need?",
            "And we think those are somewhat different from what people have worried about in the past because the knowledge that we're going to be using if it comes from text is not going to be clean.",
            "It's going to be.",
            "It's not going to be like logical knowledge, it's going to be very noisy.",
            "It's going to be very incomplete.",
            "It's going to have all sorts of problems, but we still have to be able to work with it.",
            "And the inference must allow for this, you know, like standard logical inference is just not going to cut it right?",
            "It's that would break.",
            "We wouldn't even get off the ground.",
            "And the other thing is that the NLP has to be opened up to the knowledge.",
            "We can just have a black box parser that spits out some stuff that then gets put through some logical form extractor and hope that things work.",
            "That way we need to put the hooks of the knowledge into every step of the NLP such that it gets used if it's useful.",
            "If it's not useful, it doesn't right?",
            "So we need to open up all these NLP components and.",
            "Put in the knowledge so we need ways to do that.",
            "And in general, I would say that we need to perform joint inference between the NLP and the knowledge representation.",
            "Pipelines are bad because there's already somebody has mentioned the errors accumulate and you can't do anything about them.",
            "What we want to do is joint inference across all the stages of NLP and all that on all the stages of knowledge representation.",
            "Or at least we want to allow for that.",
            "So in order to make these things happen, we need a common representation language for both the knowledge and the NLP parts.",
            "We need the language that's that allows us to do all of this together, and that language has to be at least as rich as first order logic, otherwise it won't handle many of the things that we want to be able to say and reason about or that we need in order to do the NLP.",
            "And it obviously also has to be probabilistic in order to deal with all that noisy and noisy and incomplete information.",
            "And then you not only need a language like this, but you need efficient learning and inference algorithms to go with it.",
            "Or you know you just have something that's interesting but purely theoretical.",
            "So what I'm going to describe here is, you know Markov logic and its algorithms, which we've developed in these last few years, and which I would argue are a very promising candidate to do this.",
            "So I will talk about the representation, the algorithms and some of the things that we've done with it so far.",
            "And of course you know these are still early days, but I think."
        ],
        [
            "Things look pretty exciting.",
            "Here's a one slide summary of Markov logic.",
            "The syntax of the language is extremely simple.",
            "It's just first order formulas with weights and the semantics is the following.",
            "We treat these first order formulas as templates for features of Markov networks.",
            "And the weights of the formulas become the weights of the features that were generated from them.",
            "OK, so it looks like logic, but you know deep down it's a Markov network.",
            "And now we're going to do inference over this by lifted belief propagation, which is a combination of belief propagation.",
            "With, you know, the lifted inference that you can do in logic things like resolution and so forth, and then learning well for which learning we're going to use voted Perceptron type algorithm and for learning formulas we can revising formulas.",
            "Also, we can use inductive logic programming techniques which I won't cover here, but we've also been working on those.",
            "And then you can apply this to all sorts of things.",
            "The one example that will go into in some detail here, as time allows information extraction."
        ],
        [
            "I will also mention a few others, so let me start by just briefly touching on a couple of pieces of background that probably are already known to most of you, but."
        ],
        [
            "Just to make sure that we're on the same page and we have the terminology down and so forth.",
            "So Markov networks are undirected graphical models.",
            "There's a node for each variable.",
            "And the connection between nodes represents a direct dependence and vice versa.",
            "So for example, cough is conditionally independent of smoking given cancer and asthma OK, and so this is.",
            "This tells you what the conditional independences are in the domain.",
            "The form of the model is a product of so called potential functions over the clicks.",
            "So here there's a potential function over this click and the potential function over this click.",
            "Here's an example of a potential function over here.",
            "So there's four states.",
            "Assuming the variables are Boolean, these values are.",
            "Any non negative real numbers and then the probability of a state is just the product of the values of the potentials divided by the normalization function OK?",
            "Now this is not a very efficient which represent these models because the number of parameters that you need goes up exponentially within with the size of a clique.",
            "So if you want to have large clicks, this doesn't cut it."
        ],
        [
            "What does cut it, however, is to use a log linear model representation.",
            "Which is as follows.",
            "Instead of a product of potentials, what we're going to have is a weighted sum of features.",
            "Exponentiated right and you can convert from one to the other.",
            "I could have one feature for every state of a clique and this be, you know, the log of the value potential function, so I can."
        ],
        [
            "Certainly convert."
        ],
        [
            "From this form to this form, but the thing that I have now gained is that I could have a very large click with, you know, 1000 nodes.",
            "We initan 2 to the thousand states you find, know what the 10 important features are.",
            "I just need to learn 10 parameters.",
            "Or you know deal with 10 parameters so suddenly life."
        ],
        [
            "Would be much better.",
            "So for example this potential function here.",
            "If you look at it, the only thing that it's saying is that it's unlikely that you smoke and don't have cancer right?",
            "Because smoking cause."
        ],
        [
            "Cancer and we can actually just represent it with this one feature, which is 1 if you don't smoke or have cancer and 0 otherwise, and you know it."
        ],
        [
            "You give it a weight of 1.5.",
            "It realizes this potential function.",
            "Of course, in this case is this is not very impressive, but with large clicks this can make all the difference.",
            "And this is a."
        ],
        [
            "That we're going to be exploiting a very thorough."
        ],
        [
            "In what we do.",
            "So just a little bit on 1st order logic.",
            "So first order logic formulas are built up out of four types of symbols.",
            "Constants like Anna representing objects in the domain, variables like X functions like mother of X, and predicates representing relations like friends XY or properties of objects, and then they get combined with logical connectives like end or quantifiers and so forth.",
            "We call a grounding.",
            "This is going to be important for us.",
            "The result, the grounding of a formula or or a predicate is the result of replacing all variables by constants.",
            "So for example, if you have friends, XY is on your predicates and Ann and Bob is 2 of your constants friends, Anna.",
            "Bob is one such grounding.",
            "OK, and in even friends Anna Bob is just a Boolean variable, right?",
            "It's true or false.",
            "It's true if Ann and Bob are friends then it's false.",
            "If they're not friends and we're going to call a world or model or interpretation and assignment of truth values to all ground predicates, right?",
            "I replace all constants into all predicates.",
            "I have a very large number of Boolean variables.",
            "This is my world and the thing that we're going to be interested in here is probability distributions over these.",
            "In essence, this is what we're going to be talking about."
        ],
        [
            "Through all of this."
        ],
        [
            "So now we can introduce Markov logic.",
            "Let me start with intuition and then I will give a more precise definition than an example.",
            "Here's intuition.",
            "Illogical knowledge base.",
            "You can think of it as a set of hard constraints on the set of possible worlds.",
            "If you say that if you smoke, you have cancer, and there's even one person in your world that smokes and doesn't have cancer, then the world is impossible.",
            "And this is what makes logic so brittle.",
            "Alright, well, how about we make it a little more, you know, flexible by softening the constraints.",
            "Meaning when a world violates of formula, it just becomes less probable, not impossible, and we're going to give you formula await that reflects how strong of a constraint it is.",
            "If I really believe in this constraint, I give it a high weight and my world plays a big penalty for violating it, and then the probability of the world is going to be a log linear model of the form that we just saw.",
            "The probability is just the exponentiate some of the weights of the formulas that are world satisfies.",
            "So the more formulas of world satisfies the more like."
        ],
        [
            "It is OK.",
            "Here's the more precise definition.",
            "We're going to call a Markov logic network, or MLN, a set of pairs FW, where F is a standard formula in first order logic and W is a real number, positive or negative.",
            "That's the syntax.",
            "The semantics is as follows, together with a set of constants representing the objects in your domain, and MLN defines as a Markov network will call it a ground Markov network to distinguish it from the 1st order, one as follows.",
            "It's going to have one node for each grounding of each predicate in the MLN.",
            "And it's going to have one feature for each grounding of each formula in the MLN with the corresponding weight, OK?"
        ],
        [
            "Here's an example that hopefully will make things concrete.",
            "Here are two statements in English smoking causes cancer.",
            "Friends have similar smoking habits, both true.",
            "Anne."
        ],
        [
            "You know, if you've taken a 101, you know how to translate this into logic.",
            "You can say for every X smokes X implies cancer X, and for every XY friends, XY implies smokes X equivalent to smokes.",
            "Why right?",
            "You can do this with your eyes closed, but there's a problem."
        ],
        [
            "Which is that these two statements?"
        ],
        [
            "Were true, and these two are false.",
            "Not not, not.",
            "Not everybody who smokes gets cancer and not all pairs of friends have the same smoking habits right now with Markov."
        ],
        [
            "Logic, we can attach weights to these and turn them back into what they should have been all along, which is statistical statements about the world and this statement has a higher weight because it's a stronger regularity.",
            "OK, so now this is my MLN.",
            "What is this is actually saying about the world?",
            "What's the probability?"
        ],
        [
            "Well, let's take a very simple world with just two people in it.",
            "Ann and Bob.",
            "Now we're going to have a node in our Markov network."
        ],
        [
            "For every grounding of every predicate, that means cancer.",
            "Anna smokes and."
        ],
        [
            "The same for Bob Friends in the Bob Friends Bob, Anna friends, Anna Anna friends Bob Bob.",
            "OK, this is just a bunch of Boolean variables.",
            "And now we're going to have links, meaning direct dependencies between predicates that appear together in some formula."
        ],
        [
            "OK, so for example, there's going to be an arc between canceran and smokes Anna.",
            "Same for Bob and this formula here has three."
        ],
        [
            "Predicates in it, so there's going to be a 3 way click here, OK?",
            "So now here's my Markov network, right, I have."
        ],
        [
            "I have my model structure.",
            "The probability distribution is just one over Z times the exponential of the sum over all formulas of the weight of the Formula Times the number of true groundings of the formula in the data.",
            "So now what's going to happen is that the more smokers in my domain that have cancer, the more likely the world is going to be, and that's what I want.",
            "I don't want the world to be indistinguishably impossible, whether one smoker doesn't get cancer or 100.",
            "And as we shall see, this is going to make a lot of things work for us.",
            "That wouldn't if we just stuck to the strict.",
            "A logical interpretation.",
            "Now, in Markov logic we can handle functions, existential quantifiers, infinite continuous domains.",
            "I won't talk about that here, 'cause it's not that important, but."
        ],
        [
            "In the papers.",
            "A couple of things that are interesting to note are, first of all, what is the relation of Markov logic to the kinds of statistical models that people use in NLP?",
            "Well, this is one of the nice things is that they are almost all of them very simple.",
            "Special cases of Markov logic.",
            "Meaning if you know how to do things in Markov logic, you know how to do Markov networks, Markov, random fields, which are really the same thing, vision networks, log, linear models, exponential models, maximum entropy models, give distributions, Wilson machines, logistic regression, HMM, CRF etc etc.",
            "So it's very easy to do these things in Markov logic, and the power that Markov logic adds, though, is that in Markov logic it's very easy to make your objects not be independent.",
            "Most statistical models assume that your objects are independent, or there's a very simple dependency like a linear chain.",
            "In Markov logic, you can make the pin them make them dependent anyway that you want just by defining relations that involve them, like for example with the friends XY relation I can make weather Anna smokes depend on where the Bob smokes."
        ],
        [
            "And on the logic side, it's probably you know, intuitive that first order logic is the limit that you get when you have infinite weights, and indeed we have a theorem that says you can answer every entailment Queen 1st order logic by computing conditional probability synonym where the width could be Infinity.",
            "More Interestingly, when the weights are finite, what happens is that if your knowledge base is satisfiable, meaning I'm able to make all the formulas to at the same time, then those satisfying states are the modes of the distribution.",
            "So the world is that first order logic likes there still in there, even in my probabilistic version they are the modes of the peaks of the probability.",
            "Even better, though, in first order logic, if you have a contradiction than anything follows from it and pigs fly and you're doomed in Markov logic if we have a contradiction.",
            "It's not a problem.",
            "You weigh the evidence on both sides, and you say what the probability of this statement is, and now we can deal with noisy knowledge, contradictory knowledge, knowledge from 10 different sources that I don't have to integrate very carefully.",
            "Things actually scale."
        ],
        [
            "OK, so that's the representation.",
            "Let me say a little bit about the inference and the learning."
        ],
        [
            "So the goal, in inferences to compute probabilities or find the MLP state and one way that we can do that, is with belief propagation.",
            "The nice thing about belief propagation is that it subsumes a lot of algorithms using NLP like you know, Viterbi various kinds of dynamic programming, etc.",
            "And the basic idea and belief propagation or one way to look at it, is that we're going to form a bipartite network with the variables on one side and the features on the other, and the variables in Markov logic are ground atoms like smokes, Anna, and the features are ground formulas like smoke San implies.",
            "Cancer Anna and then what we do is we repeat until convergence this loop where the nodes send messages to their features and the future send messages back to their nodes and in essence what these messages represent are the approximate marginals at that point."
        ],
        [
            "The computation, so here's a graphical illustration of this.",
            "So here are my notes there.",
            "Atoms, like, for example, smokes Anna right?",
            "It's just a Boolean variable, and these rectangles represent formulas like smoke, sannon friends Anna Bob implies smokes Bob, right?",
            "And so.",
            "And I have the connections between variables in the formula that they appear."
        ],
        [
            "And right now what I'm going to do is I send these messages."
        ],
        [
            "The notes to the formulas and then I send these messages back from the formulas to the nodes and I keep going right until I converge.",
            "And this is this is in."
        ],
        [
            "That's what belief propagation does, but there's a problem which is for our purposes, this is going to be far too slow.",
            "The reason this is going to be hard to slow is that on a network with 20 or 30 nodes, sure, but we're going to be dealing very easily here with numbers that have millions and billions of nodes in the applications that we've done so far.",
            "If you have 1000 objects and you have transitivity, well, there's a building features right there.",
            "OK, so how do we do belief propagation on the network with billions?",
            "Or who knows how many nodes?",
            "The way we're going to do this is by so like using the kind of lifted inference that you can do in logic where you group together the atoms in formulas that pass the same messages into one super node or super feature, and then we only do inference on that on that lifted network, and this lifted network could be, or there's an orders of magnitude smaller than the full network, depending on how much evidence you have and so forth."
        ],
        [
            "So again, here's a graphical illustration of this.",
            "If you look at the atoms and how they are related to the formula."
        ],
        [
            "'cause basically what happens is that this Atom forms a group by itself because it talks with that formula.",
            "These two talk with those three so I can group them together.",
            "Same with these two right?",
            "So now what I have is these groups of atoms and formulas that I can treat is 1.",
            "OK, I'm glossing over the details, will see a little bit more shortly.",
            "This is just intuition, and now because I only have in essence this one supernode, and this one super feature although."
        ],
        [
            "Messages can actually just be replaced by one OK, and now I do much less work in each iteration than I did before.",
            "Literally tend to The Who knows how much less work.",
            "In the limit, I can do infinitely less work because I can reason over infinite domains with in a finite number of steps.",
            "And the messages that I have to send are very similar to what I had before.",
            "I just need these constants that have to do with counting how many messages your sending, right?",
            "If this matches is standing in for 10 messages, this thing that needs to get raised to the 10th power, but pretty much once I have this lifted network belief propagation is the same thing that happened before."
        ],
        [
            "Except much more efficient.",
            "So here's the idea.",
            "The way this is going to work is that first we formed the lifted network composed of super nodes and super features.",
            "A supernode is a set of ground atoms that all send and receive exactly the same messages throughout belief propagation, and the Super features the same thing, except it's for ground clauses.",
            "OK, so I formed this lifted network and now I run belief propagation on the lifted network and we can prove that this will give you the exact same results as ground BP.",
            "And we can also prove that we can find the minimal lifted network and there's only one.",
            "And as I said, the time in memory, not just the time savings, but the memory savings from this can be huge things that wouldn't fit in memory now."
        ],
        [
            "So how do you form the lifted network again at a high level?",
            "It's extremely simple.",
            "Basically, start by forming initial super nodes as follows.",
            "For each predicate, I'm going to have one supernode containing all the true groundings, once appointed, containing all the false groundings, and one super note containing all the unknown groundings.",
            "And for some predicates you might have only unknown groundings.",
            "And now I've formed super features by doing joints of the supernodes that participate in them.",
            "Right, and so now I have a set of super features and now I form a new generation of super nodes by projecting the Super features down to the predicates that appear in them.",
            "This is really just a sequence of database operations do joins to form super features and then the projections to form super nodes and then a super knowledge is going to be the groundings of the predicate with the same number of projections from each super feature, because those are the ones that are going to get exactly the same messages.",
            "So in essence, what we're doing and then and then I repeat.",
            "So in essence, what this algorithm is doing is simulating belief propagation, keeping track of which nodes become distinguishable.",
            "So the only things I ever separate are the ones that the evidence sanctions separating and everything else I reason about is 1, because you know anything else is away."
        ],
        [
            "East so that's inference."
        ],
        [
            "Let's see a little bit about learning.",
            "So the data for learning is a relational database, an if all you have is something like text, right?",
            "What you need to do is to turn your text into relational database.",
            "This can be done by having like a token predicate that says I have the predicate John between positions one and two, and so forth.",
            "Or you know any other representation that is convenient.",
            "You can represent the parse tree easily as a set of atoms and so forth, and here I'm going to make the closed world assumption to make life easy.",
            "Meaning everything that's not in the database is assumed false.",
            "If you don't want to make that assumption in some applications, we don't.",
            "Then you need EM versions of the thing that I'm going to think that I'm going to talk about here and then we have them.",
            "So now there's two learning tests.",
            "As usual, there is learning the parameters, i.e.",
            "The weights of the MLN, and for that we have a voted Perceptron algorithm among others, that I will briefly describe and for learning structure either from scratch or by revising the formulas that you got in one way or another we can use inductive logic programming techniques."
        ],
        [
            "So wait, learning the goal is to maximize the conditional likelihood of your query atoms, giving your evidence once.",
            "Here's the derivative of the log conditional likelihood with respect to the to await.",
            "It's very intuitive.",
            "It's just as in a Markov random field.",
            "We now have all these dependencies, but if you do the math, the form for the likelihood is still exactly the same, except that now we have this very rich language in which the specify parameter time, right?",
            "This is like an MRF with a lot of intricate parameter time, and so the derivative is just.",
            "The number of groundings of the clause corresponding to that weight in the data.",
            "The two number minus the expected number according to the model.",
            "So if your model is predicting that the clause is true less often than it is the way it needs to go up, and vice versa, and when they're all line up, we've reached our global optimum.",
            "It's a convex problem, and we're done."
        ],
        [
            "OK, so now how do we do this?",
            "Well, one algorithm that is, you know, widely used in NLP was originally proposed by Michael Collins for training.",
            "Hmm, discriminatively is the voted Perceptron.",
            "And the whole perception is sort of like a structured generalization of the perceptron.",
            "It assumes that the network is a linear chain, and then what it does is the following.",
            "You initialize your weights to zero and then you compute your most likely Y given X using Viterbi and then you just do the difference of the counts in that solution and in the data multiply by a learning rate, which could even be one and you know add to the previous wait right?",
            "And this is a lot like the perceptron.",
            "The other big difference you know besides being structures that you don't return the final wait, see return their average.",
            "And this is both empirically much better and theoretically you can.",
            "You know you can show that this gives better generalization.",
            "So now what we would like to have is a learning algorithm for millions, which sounds like a much more difficult proposition, but."
        ],
        [
            "In fact, it's not because the only thing we need to do is to take the voted Perceptron and Plug in lifted BP where before we had with Kirby and this could be Max product or some product BP depending on what you prefer, but then everything works as before, except that now you can have an arbitrary network, you know the evidence can be anywhere.",
            "The structure of the graph can be anything that you want and you can have all this parameter time, but at some level the algorithm is."
        ],
        [
            "The same.",
            "OK, so."
        ],
        [
            "Let me mention.",
            "Just to make this concrete and see how we can apply it in NLP one application.",
            "Here's a simple example of information extraction.",
            "I have a set of citations from the."
        ],
        [
            "Paper and I want to segment it, meaning I want to see you know where are the author strings, the titles, the venues."
        ],
        [
            "And then I want to do the Inter resolution.",
            "Meaning you know, I want to figure out that Tripoli 06 is the same as the proceedings of the 21st National Conference on Artificial Intelligence.",
            "Notice this is hard because the strings are completely different."
        ],
        [
            "And then once I've done this, I also want to figure out that these two records are the same.",
            "In this you know, these two are also the same.",
            "OK, so this is just a simple example of information extract."
        ],
        [
            "And the state of the art today is to basically do this as follows.",
            "For segmentation, use something like an HMM or CRF to assign each took into a field, author, author, author, title, title, venue, and then for the Inter Resolution user classifier, like Syllogistic regression, to predict for each pair of fields or citations whether they are the same.",
            "And then you do some kind of transitive closure step.",
            "We face the same as B&B is the same as C, then is the same as C. Now to implement this today using a language like you know Java or C++ or whatever it's going to run.",
            "You probably 10s of thousands of lines of code and you know it'll probably take you weeks to do that whole thing and debug it and whatnot.",
            "In Markov logic, you can do this whole thing in just seven formulas that fit on a slide.",
            "OK, this is sort of like the game that you get you have now this ability to put in knowledge very compactly, much more complicated than if you were using logic and trying to put in all the exceptions.",
            "And then you let the learning do the hard work for you and the probabilistic inference you know deal with the."
        ],
        [
            "Nothing under specification and so forth, so here's an example.",
            "He's specifically one network that you can use for this."
        ],
        [
            "Here, here are the types that you need to have.",
            "You actually don't need to declare them, but you know just for clarity I am going to declare them here.",
            "You have tokens, you have fields, say author, title, venue, etc.",
            "Citations and positions within a site."
        ],
        [
            "Nation.",
            "And now your evidence predicate is just token that says this token, say Parag appears in position one of citation C1, OK?"
        ],
        [
            "Is my evidence and I'm going to have my query predicates in field.",
            "Is the predicate that does the segmentation.",
            "It says that this position in this citation belongs to this field.",
            "Like for example the first citation is part of the author field and these two do the Inter resolution.",
            "This one says that the author field in these two citations is the same and this one says that these two citations are the same."
        ],
        [
            "OK, so here's the MLN.",
            "These for."
        ],
        [
            "Three formulas basically implement an HMM.",
            "What is CRF?",
            "Actually?",
            "Because we train discriminatively, but you know, you can do either.",
            "And then these four do the Inter resolution.",
            "So this formula here you read it as follows.",
            "All the variables that are not quantified are implicitly a universally quantified, as in.",
            "You know logic, programming and so forth.",
            "This little plus means that I want to learn a separate wait for every instance of the formula with every combination of these variables.",
            "So what this formula is saying is that if I have a certain token in a certain position, then that position is in a certain field and I'm going to have a wait for every token.",
            "Field pair, which means that this is the observation matrix of the HMM.",
            "There's literally a one to one correspondence.",
            "Between this and the observation matrix of an HMM."
        ],
        [
            "And this here is the transition matrix.",
            "It says that if I'm in a certain field now that I'm in a certain field in the next position, in fact the only thing that we need for this purpose is to capture the fact that you tend to stay in the same field.",
            "But in general you could do another."
        ],
        [
            "Hmm, this way and this one is actually something that you don't need to do a regular hmm, but we are going to do it here this way, which is I'm going to allow things to not belong to any field.",
            "I don't need another field, I'm just going to allow you know all of the field predicates to be false, but then I do.",
            "I need to add this formula.",
            "It says you can't be in two fields at once, so this says that you know if you are in 1 field then you're not in the other.",
            "So this is an HMM, it does the segmentation."
        ],
        [
            "And and now these formulas here do the Inter resolution.",
            "This formula here is basically predicting weather 2 fields are the same from the fact that they have the same tokens in the same positions, right?",
            "So this is very precisely logistic regression where this is the predicted variable and these are the predictor variables.",
            "And you could also think of it as just doing a similarity computation, right?",
            "It's deciding whether to fields are the same by how many tokens they have in common.",
            "And of course some tokens.",
            "Matter more than others, right?",
            "'cause they're more informative and that's why we're going to have a different weight for every token Enfield pair."
        ],
        [
            "OK. Now this formula is very short, but does a lot of work.",
            "It says that if 2 feet if the fields in a citation are the same, then the citation is the same, right?",
            "If the author is the same, then the citation is the same.",
            "If the field is the same, then the citation is the same.",
            "Notice these things are not valid as logical formulas, but estatistica regularity's.",
            "They're great, right?",
            "If the authors are the same, these papers are now much more likely to be the same then if the authors weren't the same.",
            "OK. And actually this also works the other way if the citation is the same, then the authors.",
            "And the you know, titles in the venues are also the same.",
            "So I can actually infer that because you know, these authors are very similar and the title is the same.",
            "This citation is the same, and now because of that, I now know that triple Year 2000 and the 20th Conference on Artificial Intelligence are the same.",
            "Right, I don't actually need to encode that this actually gets figured out by the inference, and now this propagates right?",
            "So you could have."
        ],
        [
            "Very long chains of inference, and now these two things are just the transitive closure.",
            "For example, you know if citation C is the same as the prime and see Primus MSE prime, then sees the same as the prime prime.",
            "Notice the following a few years ago, McCallum and Welner had a very highly cited paper in NIPS where the only thing that they did was to take a CRF and add transit tivity to do Inter resolution, and you know it was a lot of work for this.",
            "They had to design algorithms for the inference for the learning and whatnot in Markov logic.",
            "You can accomplish that just by adding this formula, right?",
            "It's one more line.",
            "And the job is done OK. Because we have, you know, the powerful learning and inference algorithms and."
        ],
        [
            "Would that actually make this possible now?",
            "This is actually not a bad interior solution model as it is.",
            "But this here is a pretty weak segmentation model, and the reason is that this is this is, you know, good at propagating author and good at propagating title, but not very good at designing where the boundary is right.",
            "Now what people do when they don't do this with CRF, since they come up with rules for predicting the start and end of a field like this, is what the whole area of reproduction is about.",
            "But combining the two is not is nontrivial.",
            "Well here, here it actually is trivial.",
            "For example, we know as you know, then Roth mentioned before that this is a small example of how you win.",
            "You can include your knowledge easily into an MLN.",
            "We know that in citations, the field boundaries usually occur at periods.",
            "OK, so now what I can do is that I qualify this rule that saying that if a position isn't in the field then the next position in the same field.",
            "That propagation only happens if the token is not a period.",
            "Having a period breaks the propagation.",
            "Anyway, just like I added and this makes a huge, we're going to struggle.",
            "This makes a huge difference to the quality of the result.",
            "And the same way that I can now just like another things like, which is also, you know it's a weaker predictor, but it's still useful, right?",
            "So I can put these things in easily."
        ],
        [
            "So here's what happens on Quora.",
            "You know fairly standard data set for these things.",
            "This is this.",
            "So this is precision and recall you'd like to be up here.",
            "This is what happens when you just use the tokens right?",
            "Just you know, basically, logistic regression.",
            "Here's what happens when you bring in the sequence information.",
            "It's actually somewhat of a mixed bag because at high recall the precision goes down.",
            "But not only is what happens when you just modify your rule by putting in this.",
            "Right at this point, this is no longer in HMM, but we don't care.",
            "We have the algorithms to deal with it, you know?",
            "Now you're getting pretty up close to this corner, and if you now add the comma, you get even a little bit better.",
            "So even with just a simple MLN we get results that are pretty close to the state of the art.",
            "Now, of course we can keep going and try to write down more of our knowledge.",
            "You know the kinds of things that then was talking about.",
            "You know, citations tend to start with an author, for example, and you don't.",
            "You know you don't have more than one instance of this."
        ],
        [
            "In field and so forth.",
            "And we've done that.",
            "We get an MLN with about 40 rules in it.",
            "You know, this is on the web.",
            "You can look at it.",
            "You can understand it.",
            "And this MLN gives the best results to date on sites here in core beating models like CRF based models and others that took a long time to develop and a lot of people working on them and gazillions of features.",
            "These models actually don't have that many parameters.",
            "They have the right parameters because you specify the structure by writing the formulas by specifying your knowledge.",
            "So this is one thing that we've done.",
            "Let me mention a couple of others.",
            "Here's a more recent result.",
            "We've developed an approach.",
            "The approach that we saw here was supervised.",
            "We've also developed an approach to unsupervised coreference resolution using an EM like version of our algorithms and we get the best results to date on machinas.",
            "For those of you familiar with the Higgin client paper, they got 70% F. One we get 80.",
            "This is a huge jump.",
            "In fact, this is such a huge jump that we actually do better than most supervised approaches on these on these testbeds.",
            "Another thing that we've done over somewhat different flavor, but just to give you an idea of what can be done, is we've also done extracting whole ontologies from data.",
            "From text data, here's what we've done.",
            "We took text Runner, which is a system that runs over, you know, millions of web pages and extracts tuples by basically doing some heuristic parsing to see where the objects in their relations are.",
            "But then of course it's extremely noisy.",
            "But basically we take from them 2 million tuples.",
            "This is not.",
            "This is a fairly large scale thing, and the first thing that we do is you know the coreference resolution and you know we write out the box.",
            "We beat the system that they have called resolve for doing this, but then we don't need to stop there.",
            "We can do the ontology induction.",
            "And So what we do is, in essence, we're doing clustering of these symbols again.",
            "The MLN that we're using for this is very simple.",
            "It has literally half a dozen rules, but it does things like it discovers the concept of Planet of Country of City.",
            "It, of course, doesn't name them that.",
            "But if you look at the classes that it's forming that actually very high quality clusters, and you should compare them, for example with Word Net, where they overlap, we get high quality results, wouldn't it?",
            "Has some things that we don't because you know this is web text.",
            "This also has a ton of concepts that were not doesn't, but now you can put the two together.",
            "And you actually have a much richer ontology than."
        ],
        [
            "So this is where we're at today.",
            "So before I conclude, let me just mention where we want to go next."
        ],
        [
            "In fact, where we're going now.",
            "So the thing that we're doing now is we have now extracted this ontology from text and we have structured learning algorithms IOP type algorithms.",
            "We're going to apply them to this and extract rules.",
            "And remember, these are probabilistic rules.",
            "These are, you know this is an MLN, and now once we've done that, we can now ask queries.",
            "And the system potentially can answer questions that are not answered in anyone given fact it can actually do chaining.",
            "Right so we you know, we now that we from the ontology, we can induce knowledge and now we can start to answer questions.",
            "We would also like to apply Markov logic to other NLP tasks like for example parsing.",
            "It's very straightforward to encode, you know PCF GSM lens and we've done this for toy grammars.",
            "Of course doing this for real grammars is something that's not trivial, but we're going to try to do that.",
            "And then we want to connect all the pieces, do the joint inference, and then close the loop, feed the knowledge back into the NLP.",
            "Let the knowledge help with the parsing.",
            "Let the knowledge that I've acquired by reading some texts for example, help me decide that you know you know.",
            "In John 8 the pizza with a fork, you know the fork attaches to 8 and then ate the pizza with pepperoni.",
            "It attaches to the pizza right?",
            "Because one is a utensil and the other one is edible, right?",
            "That's the kind of thing that we can do when we close the loop between these two things so."
        ],
        [
            "Is where we're going to go.",
            "Let me summarize.",
            "Language and knowledge are a chicken and egg problem.",
            "You need knowledge to understand language using language to acquire knowledge.",
            "So the solution is to bootstrap, you know, start with a little bit of knowledge.",
            "Read some text with that.",
            "Have more knowledge.",
            "Read more text.",
            "Keep going Markov logic Network provides a language and algorithms for doing this.",
            "The language is very simple.",
            "It's weight 1st order formulas that get compiled into Markov networks.",
            "We saw an inference algorithm for this lifted with propagation.",
            "Anna learning algorithm voted perceptron.",
            "We've had several successes to date.",
            "With this like you know we have the state of the art results in a number of problems.",
            "I mentioned some of them.",
            "All of this, all of the algorithms that I've described in a whole bunch of others are available in this open source software called Alchemy, available at this URL, and you can download it, play with it.",
            "You can use it to solve your NLP test by encoding your knowledge or your hypothesis and trying things out without having to worry about the learning in the inference.",
            "Or if you have a better learning and inference algorithm, you can plug it into outcome using the existing API's and then immediately works with everything else and.",
            "With other things that people might have in the future, and our goal is, you know, we actually have a website.",
            "Our goal is to actually start a repository where people can contribute millions so you can use somebody's in lens, solve a problem better using.",
            "Maybe those are some lens and build up this body of knowledge right?",
            "We want to have a corpus of knowledge, not just the corpus of raw text.",
            "And now when somebody wants to solve an LP problem, they can draw on that corpus and you know they can modify it, refine it, contribute their new MLN and hopefully in this way we will over the years get something that's.",
            "Very sophisticated and able to do things that haven't been possible before.",
            "Thank you.",
            "Questions.",
            "So of course, these books trucks are increasing, but just as sort of a self feeding mechanism.",
            "So we can either yeah converge or like rapidly if you're just getting wrong knowledge that then.",
            "Wrong translation right?",
            "Exactly?",
            "So what we need here is something like the analog of the EM theorem that says that you get better in each iteration as opposed to getting worse.",
            "And I think you know we haven't really started to go through this loop a lot of times, but I think one of the keys is to make sure that.",
            "We were able to filter out the bad knowledge and keeping the good knowledge and one way that we've been doing this so far is that when we do this ontology induction.",
            "In essence, what we're doing is like we're keeping the things that are even indirectly consistent and throwing out the things are inconsistent.",
            "So, for example, if you're doing this from the web and you know the majority opinion on the web directly and indirectly via chains of influences that you know Elvis killed Kennedy, then we will believe that Elvis killed Kennedy, right?",
            "But you know, we can't do better than the sources.",
            "And we'll see how things turn out, but you know, that's the plan, yeah?",
            "So your formula for transitivity is same field.",
            "Right?",
            "Just an axiom."
        ],
        [
            "It's just it's just a template, so it could learn that same field actually means you know using the citation closure or or something like that, right?",
            "So this.",
            "True fact about same field in same citation, no no.",
            "So in this case we're doing supervised learning and we're not inventing predicates, So what happens is that I have a database where these things are labeled.",
            "The one thing that I'm going to learn for this is not the meaning of same field, because that doesn't change is the weight of this rule.",
            "I could decide to ignore this rule, but if I if I if this rule has a positive weight, it really means that you know it's transitivity.",
            "Run the waiting.",
            "Oh so good point.",
            "You could decide that this rule is a hard rule and cannot be violated because transitivity is definitionally true, and in fact we tried that out and you know it's totally up to you and you know it's not.",
            "It's OK to do that, but you actually get better results by allowing a finite way to be learned by this.",
            "And the reason for that is that your data is poor.",
            "We don't have a real model of what's going on, and so in some cases not for cluster recall, but for pairwise recall.",
            "You're actually better off, you know, allowing this to be violated, but if you don't like that, just make that a hard rule.",
            "Right so.",
            "Equality is not necessarily transitive.",
            "No, I would remember we're doing discriminative learning and the model that we have.",
            "The world is oversimplified, so sometimes it's better to violate things that are true.",
            "But if you don't want to violate them, it's not that big of a difference.",
            "You can just leave it like this if you remember the you know, then let's talk this morning.",
            "He talked about having hard constraints and having soft constraints.",
            "One thing that I didn't mention is that in Markov logic or in alchemy you can the way you make a constraint harder just by putting a full stop at the end, as in Prolog.",
            "So if you want to make transitivity hard rule.",
            "You just put a full stop here in the full stop here and then the way it doesn't get learn.",
            "And then what happens is that all the learning and parameter learning happened within the box of these constraints.",
            "They only happen in things that satisfy that.",
            "When we start bootstrapping, the number of rules is going to keep increasing.",
            "The number of variables actually not necessarily.",
            "What's probably going to happen is that the number of rules is going to converge to something and not move beyond that right?",
            "There are these people have done this sort of like mass collaboration projects where everybody can contribute knowledge over the web and one of the things that they found is that this is incredibly wasteful because everybody is always contributing the same knowledge.",
            "So in fact.",
            "That is almost the same rule, but just slightly different looking and Oh no, no.",
            "But here's The thing is that this isn't logic.",
            "This is Markov logic and the beauty of Markov logic is that you don't need those 50 different.",
            "This was actually our experience.",
            "We built these models with 50 rules and we discovered that only five were necessary because there's a lot of richness in the way the weights interact.",
            "You actually get the right conclusions alot of the time, even if you don't have exactly the right formula.",
            "So you get like this huge reduction in the number of actual rules that you need.",
            "There's a lot of redundancy, and this model won't learn redundant rules right, because they won't pay for themselves.",
            "There's the usual complexity, penalty, price, whatnot.",
            "So unless a rule is really allowing you to reach conclusions that you couldn't before or you know to be more precise, changing the probability landscape as opposed to just shifting weight around between equivalent things, we won't learn it.",
            "But you know this, we will see what happens, right?",
            "This is what we've seen so far.",
            "Is that very, very simple models actually give you like the one I just showed you actually give you most of what you need.",
            "I mean, if you're reading loads of text, you might find loading different rules about loads of objects, but they're all the same object class and you have to make something.",
            "This is exactly right, but this is what we do.",
            "In fact, the first thing that we do is we resolve all those objects to figure out that it's the same object and the way we do this is.",
            "Well it now OK, so remember you're reading text if what the text is about them is the same, then as far as the text is concerned they are the same.",
            "If it's like, well if they're slightly different then I will have an ontology with the two kinds of objects.",
            "And I will also want to have the generalization of them right, so if it really is the case that my world is large, then I want my ontology to grow.",
            "Absolutely, but in.",
            "Indeed, and in that case I want to have the extra bowl.",
            "OK, any other question maybe sure.",
            "It probably requires some effort to get off to be fast enough.",
            "No kidding.",
            "That's what we've been working on is we make this fast so you don't have to.",
            "So in terms of alchemy.",
            "How much knowledge is required in order to get?",
            "Good results in to get it to run fast enough for casual user, do you really understand absolutely right?",
            "This is the key question, so here's what our experience of the people using this has been so far right and we have a mailing list or people can send feedback and ask questions.",
            "Is getting getting alchemy working?",
            "Is actually very quick if you know first order logic you can write formulas and really in one afternoon you can learn how to use Alchemy Now how long it takes you to get alchemy to do what you want depends on you and the domain.",
            "It could be difficult.",
            "You usually have to go through this process where.",
            "You try putting things in, taking them out, see how fast they are, see whether they give the right results, right?",
            "So there's the usual refinement process that always has to happen, except that now you're doing through this explicit and understandable interface of the rules.",
            "But you know this is no magic pen is here, right?",
            "We've put these things together, but it doesn't work any better than individual components, right?",
            "Weight learning is not something that you can treat completely as a back black box or belief propagation, so you know we inherit the pluses and minuses of those things.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, thank you all for being here.",
                    "label": 0
                },
                {
                    "sent": "Thanks for inviting me.",
                    "label": 0
                },
                {
                    "sent": "I'm actually quite happy to be at this workshop because this is really a topic that I'm very much interested in and you know, as I will talk a little bit about in this talk, we've actually started to apply Markov logic to NLP because we think it's a great match, so.",
                    "label": 0
                },
                {
                    "sent": "So the title my talk is incorporating prior knowledge into NLP with Markov logic.",
                    "label": 1
                },
                {
                    "sent": "This is work that I've done at UW with these people.",
                    "label": 0
                },
                {
                    "sent": "Stanley Kok, Daniel Loud- Poon met, Richardson Prox, England, Mark Sumner, and Julie Wang.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's a brief overview.",
                    "label": 0
                },
                {
                    "sent": "I will begin with a little bit of motivation and background, and then I will do an intro to Markov logic.",
                    "label": 1
                },
                {
                    "sent": "Could talk about some of our latest greatest influence in learning algorithms, and then exemplify how we can apply these techniques to incorporate prior knowledge into NLP in a way that I think is a very flexible and powerful.",
                    "label": 0
                },
                {
                    "sent": "And then I will conclude with a little.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of discussion.",
                    "label": 0
                },
                {
                    "sent": "So here's the motivation.",
                    "label": 0
                },
                {
                    "sent": "Language and knowledge.",
                    "label": 0
                },
                {
                    "sent": "I really like the chicken and the egg.",
                    "label": 1
                },
                {
                    "sent": "Because to understand language you need knowledge.",
                    "label": 1
                },
                {
                    "sent": "You need knowledge to resolve parsing ambiguities.",
                    "label": 0
                },
                {
                    "sent": "There's all these problems in and out, you know, like recently in the in the textual entailment domain, one of the winning teams did in an error analysis and see what is the biggest thing that we're missing to do better 'cause nobody is doing that well, it's knowledge.",
                    "label": 0
                },
                {
                    "sent": "So without knowledge of the world and linguistic knowledge, you can only get so far in NLP, so you need knowledge for language.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, you need language to acquire knowledge.",
                    "label": 0
                },
                {
                    "sent": "Encoding knowledge manually.",
                    "label": 0
                },
                {
                    "sent": "You know using you know expert system techniques is just too slow, too expensive, not worth.",
                    "label": 0
                },
                {
                    "sent": "It's never going to happen.",
                    "label": 0
                },
                {
                    "sent": "The only way it's ever going to happen is if you acquire that knowledge from text.",
                    "label": 0
                },
                {
                    "sent": "So we're in this interesting situation that if we have the knowledge, we would know how to do NLP and if we knew how to do an LP then we have the knowledge, right?",
                    "label": 0
                },
                {
                    "sent": "So how do we solve this problem?",
                    "label": 0
                },
                {
                    "sent": "There's only one way that I know solving problems like this and that's to bootstrap.",
                    "label": 0
                },
                {
                    "sent": "The way you Bootstrap in this case is you start with a small hand coded knowledge base.",
                    "label": 0
                },
                {
                    "sent": "The smallest you can get away with.",
                    "label": 1
                },
                {
                    "sent": "And then you use that to help process some text that's hopefully not too hard, or at least go to a bunch of texts and get something out of it and then and then you take that and add it to your knowledge base.",
                    "label": 0
                },
                {
                    "sent": "And now you have a better knowledge base.",
                    "label": 1
                },
                {
                    "sent": "Still not perfect, but better and hopefully with this knowledge base you cannot process some more text than maybe some harder text.",
                    "label": 0
                },
                {
                    "sent": "And now you keep going.",
                    "label": 0
                },
                {
                    "sent": "Now think of this as a kind of em.",
                    "label": 0
                },
                {
                    "sent": "At a high level, right?",
                    "label": 0
                },
                {
                    "sent": "If in each step of this cycle you get more knowledge than you had before, and better ability to process language than you had before, and you keep going long enough, then you potentially will do amazing things at an acceptable cost, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the dream.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you will, another question is, if we're going to make this happen, what do we need?",
                    "label": 0
                },
                {
                    "sent": "And we think those are somewhat different from what people have worried about in the past because the knowledge that we're going to be using if it comes from text is not going to be clean.",
                    "label": 0
                },
                {
                    "sent": "It's going to be.",
                    "label": 0
                },
                {
                    "sent": "It's not going to be like logical knowledge, it's going to be very noisy.",
                    "label": 1
                },
                {
                    "sent": "It's going to be very incomplete.",
                    "label": 0
                },
                {
                    "sent": "It's going to have all sorts of problems, but we still have to be able to work with it.",
                    "label": 0
                },
                {
                    "sent": "And the inference must allow for this, you know, like standard logical inference is just not going to cut it right?",
                    "label": 1
                },
                {
                    "sent": "It's that would break.",
                    "label": 0
                },
                {
                    "sent": "We wouldn't even get off the ground.",
                    "label": 1
                },
                {
                    "sent": "And the other thing is that the NLP has to be opened up to the knowledge.",
                    "label": 0
                },
                {
                    "sent": "We can just have a black box parser that spits out some stuff that then gets put through some logical form extractor and hope that things work.",
                    "label": 0
                },
                {
                    "sent": "That way we need to put the hooks of the knowledge into every step of the NLP such that it gets used if it's useful.",
                    "label": 0
                },
                {
                    "sent": "If it's not useful, it doesn't right?",
                    "label": 0
                },
                {
                    "sent": "So we need to open up all these NLP components and.",
                    "label": 1
                },
                {
                    "sent": "Put in the knowledge so we need ways to do that.",
                    "label": 1
                },
                {
                    "sent": "And in general, I would say that we need to perform joint inference between the NLP and the knowledge representation.",
                    "label": 0
                },
                {
                    "sent": "Pipelines are bad because there's already somebody has mentioned the errors accumulate and you can't do anything about them.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is joint inference across all the stages of NLP and all that on all the stages of knowledge representation.",
                    "label": 0
                },
                {
                    "sent": "Or at least we want to allow for that.",
                    "label": 0
                },
                {
                    "sent": "So in order to make these things happen, we need a common representation language for both the knowledge and the NLP parts.",
                    "label": 1
                },
                {
                    "sent": "We need the language that's that allows us to do all of this together, and that language has to be at least as rich as first order logic, otherwise it won't handle many of the things that we want to be able to say and reason about or that we need in order to do the NLP.",
                    "label": 0
                },
                {
                    "sent": "And it obviously also has to be probabilistic in order to deal with all that noisy and noisy and incomplete information.",
                    "label": 0
                },
                {
                    "sent": "And then you not only need a language like this, but you need efficient learning and inference algorithms to go with it.",
                    "label": 0
                },
                {
                    "sent": "Or you know you just have something that's interesting but purely theoretical.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to describe here is, you know Markov logic and its algorithms, which we've developed in these last few years, and which I would argue are a very promising candidate to do this.",
                    "label": 0
                },
                {
                    "sent": "So I will talk about the representation, the algorithms and some of the things that we've done with it so far.",
                    "label": 0
                },
                {
                    "sent": "And of course you know these are still early days, but I think.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things look pretty exciting.",
                    "label": 0
                },
                {
                    "sent": "Here's a one slide summary of Markov logic.",
                    "label": 0
                },
                {
                    "sent": "The syntax of the language is extremely simple.",
                    "label": 0
                },
                {
                    "sent": "It's just first order formulas with weights and the semantics is the following.",
                    "label": 0
                },
                {
                    "sent": "We treat these first order formulas as templates for features of Markov networks.",
                    "label": 0
                },
                {
                    "sent": "And the weights of the formulas become the weights of the features that were generated from them.",
                    "label": 0
                },
                {
                    "sent": "OK, so it looks like logic, but you know deep down it's a Markov network.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to do inference over this by lifted belief propagation, which is a combination of belief propagation.",
                    "label": 0
                },
                {
                    "sent": "With, you know, the lifted inference that you can do in logic things like resolution and so forth, and then learning well for which learning we're going to use voted Perceptron type algorithm and for learning formulas we can revising formulas.",
                    "label": 0
                },
                {
                    "sent": "Also, we can use inductive logic programming techniques which I won't cover here, but we've also been working on those.",
                    "label": 0
                },
                {
                    "sent": "And then you can apply this to all sorts of things.",
                    "label": 0
                },
                {
                    "sent": "The one example that will go into in some detail here, as time allows information extraction.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will also mention a few others, so let me start by just briefly touching on a couple of pieces of background that probably are already known to most of you, but.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to make sure that we're on the same page and we have the terminology down and so forth.",
                    "label": 0
                },
                {
                    "sent": "So Markov networks are undirected graphical models.",
                    "label": 1
                },
                {
                    "sent": "There's a node for each variable.",
                    "label": 0
                },
                {
                    "sent": "And the connection between nodes represents a direct dependence and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So for example, cough is conditionally independent of smoking given cancer and asthma OK, and so this is.",
                    "label": 0
                },
                {
                    "sent": "This tells you what the conditional independences are in the domain.",
                    "label": 0
                },
                {
                    "sent": "The form of the model is a product of so called potential functions over the clicks.",
                    "label": 0
                },
                {
                    "sent": "So here there's a potential function over this click and the potential function over this click.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of a potential function over here.",
                    "label": 0
                },
                {
                    "sent": "So there's four states.",
                    "label": 0
                },
                {
                    "sent": "Assuming the variables are Boolean, these values are.",
                    "label": 0
                },
                {
                    "sent": "Any non negative real numbers and then the probability of a state is just the product of the values of the potentials divided by the normalization function OK?",
                    "label": 0
                },
                {
                    "sent": "Now this is not a very efficient which represent these models because the number of parameters that you need goes up exponentially within with the size of a clique.",
                    "label": 0
                },
                {
                    "sent": "So if you want to have large clicks, this doesn't cut it.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What does cut it, however, is to use a log linear model representation.",
                    "label": 0
                },
                {
                    "sent": "Which is as follows.",
                    "label": 0
                },
                {
                    "sent": "Instead of a product of potentials, what we're going to have is a weighted sum of features.",
                    "label": 0
                },
                {
                    "sent": "Exponentiated right and you can convert from one to the other.",
                    "label": 0
                },
                {
                    "sent": "I could have one feature for every state of a clique and this be, you know, the log of the value potential function, so I can.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Certainly convert.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From this form to this form, but the thing that I have now gained is that I could have a very large click with, you know, 1000 nodes.",
                    "label": 0
                },
                {
                    "sent": "We initan 2 to the thousand states you find, know what the 10 important features are.",
                    "label": 0
                },
                {
                    "sent": "I just need to learn 10 parameters.",
                    "label": 0
                },
                {
                    "sent": "Or you know deal with 10 parameters so suddenly life.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would be much better.",
                    "label": 0
                },
                {
                    "sent": "So for example this potential function here.",
                    "label": 0
                },
                {
                    "sent": "If you look at it, the only thing that it's saying is that it's unlikely that you smoke and don't have cancer right?",
                    "label": 0
                },
                {
                    "sent": "Because smoking cause.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cancer and we can actually just represent it with this one feature, which is 1 if you don't smoke or have cancer and 0 otherwise, and you know it.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You give it a weight of 1.5.",
                    "label": 0
                },
                {
                    "sent": "It realizes this potential function.",
                    "label": 0
                },
                {
                    "sent": "Of course, in this case is this is not very impressive, but with large clicks this can make all the difference.",
                    "label": 0
                },
                {
                    "sent": "And this is a.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we're going to be exploiting a very thorough.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In what we do.",
                    "label": 0
                },
                {
                    "sent": "So just a little bit on 1st order logic.",
                    "label": 0
                },
                {
                    "sent": "So first order logic formulas are built up out of four types of symbols.",
                    "label": 0
                },
                {
                    "sent": "Constants like Anna representing objects in the domain, variables like X functions like mother of X, and predicates representing relations like friends XY or properties of objects, and then they get combined with logical connectives like end or quantifiers and so forth.",
                    "label": 0
                },
                {
                    "sent": "We call a grounding.",
                    "label": 0
                },
                {
                    "sent": "This is going to be important for us.",
                    "label": 0
                },
                {
                    "sent": "The result, the grounding of a formula or or a predicate is the result of replacing all variables by constants.",
                    "label": 1
                },
                {
                    "sent": "So for example, if you have friends, XY is on your predicates and Ann and Bob is 2 of your constants friends, Anna.",
                    "label": 0
                },
                {
                    "sent": "Bob is one such grounding.",
                    "label": 1
                },
                {
                    "sent": "OK, and in even friends Anna Bob is just a Boolean variable, right?",
                    "label": 0
                },
                {
                    "sent": "It's true or false.",
                    "label": 0
                },
                {
                    "sent": "It's true if Ann and Bob are friends then it's false.",
                    "label": 0
                },
                {
                    "sent": "If they're not friends and we're going to call a world or model or interpretation and assignment of truth values to all ground predicates, right?",
                    "label": 1
                },
                {
                    "sent": "I replace all constants into all predicates.",
                    "label": 0
                },
                {
                    "sent": "I have a very large number of Boolean variables.",
                    "label": 0
                },
                {
                    "sent": "This is my world and the thing that we're going to be interested in here is probability distributions over these.",
                    "label": 0
                },
                {
                    "sent": "In essence, this is what we're going to be talking about.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through all of this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we can introduce Markov logic.",
                    "label": 0
                },
                {
                    "sent": "Let me start with intuition and then I will give a more precise definition than an example.",
                    "label": 0
                },
                {
                    "sent": "Here's intuition.",
                    "label": 0
                },
                {
                    "sent": "Illogical knowledge base.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as a set of hard constraints on the set of possible worlds.",
                    "label": 1
                },
                {
                    "sent": "If you say that if you smoke, you have cancer, and there's even one person in your world that smokes and doesn't have cancer, then the world is impossible.",
                    "label": 0
                },
                {
                    "sent": "And this is what makes logic so brittle.",
                    "label": 0
                },
                {
                    "sent": "Alright, well, how about we make it a little more, you know, flexible by softening the constraints.",
                    "label": 1
                },
                {
                    "sent": "Meaning when a world violates of formula, it just becomes less probable, not impossible, and we're going to give you formula await that reflects how strong of a constraint it is.",
                    "label": 0
                },
                {
                    "sent": "If I really believe in this constraint, I give it a high weight and my world plays a big penalty for violating it, and then the probability of the world is going to be a log linear model of the form that we just saw.",
                    "label": 0
                },
                {
                    "sent": "The probability is just the exponentiate some of the weights of the formulas that are world satisfies.",
                    "label": 0
                },
                {
                    "sent": "So the more formulas of world satisfies the more like.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is OK.",
                    "label": 0
                },
                {
                    "sent": "Here's the more precise definition.",
                    "label": 0
                },
                {
                    "sent": "We're going to call a Markov logic network, or MLN, a set of pairs FW, where F is a standard formula in first order logic and W is a real number, positive or negative.",
                    "label": 1
                },
                {
                    "sent": "That's the syntax.",
                    "label": 0
                },
                {
                    "sent": "The semantics is as follows, together with a set of constants representing the objects in your domain, and MLN defines as a Markov network will call it a ground Markov network to distinguish it from the 1st order, one as follows.",
                    "label": 0
                },
                {
                    "sent": "It's going to have one node for each grounding of each predicate in the MLN.",
                    "label": 1
                },
                {
                    "sent": "And it's going to have one feature for each grounding of each formula in the MLN with the corresponding weight, OK?",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example that hopefully will make things concrete.",
                    "label": 0
                },
                {
                    "sent": "Here are two statements in English smoking causes cancer.",
                    "label": 0
                },
                {
                    "sent": "Friends have similar smoking habits, both true.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know, if you've taken a 101, you know how to translate this into logic.",
                    "label": 0
                },
                {
                    "sent": "You can say for every X smokes X implies cancer X, and for every XY friends, XY implies smokes X equivalent to smokes.",
                    "label": 1
                },
                {
                    "sent": "Why right?",
                    "label": 0
                },
                {
                    "sent": "You can do this with your eyes closed, but there's a problem.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is that these two statements?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Were true, and these two are false.",
                    "label": 0
                },
                {
                    "sent": "Not not, not.",
                    "label": 0
                },
                {
                    "sent": "Not everybody who smokes gets cancer and not all pairs of friends have the same smoking habits right now with Markov.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Logic, we can attach weights to these and turn them back into what they should have been all along, which is statistical statements about the world and this statement has a higher weight because it's a stronger regularity.",
                    "label": 0
                },
                {
                    "sent": "OK, so now this is my MLN.",
                    "label": 0
                },
                {
                    "sent": "What is this is actually saying about the world?",
                    "label": 0
                },
                {
                    "sent": "What's the probability?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, let's take a very simple world with just two people in it.",
                    "label": 0
                },
                {
                    "sent": "Ann and Bob.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to have a node in our Markov network.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For every grounding of every predicate, that means cancer.",
                    "label": 0
                },
                {
                    "sent": "Anna smokes and.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same for Bob Friends in the Bob Friends Bob, Anna friends, Anna Anna friends Bob Bob.",
                    "label": 0
                },
                {
                    "sent": "OK, this is just a bunch of Boolean variables.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to have links, meaning direct dependencies between predicates that appear together in some formula.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so for example, there's going to be an arc between canceran and smokes Anna.",
                    "label": 0
                },
                {
                    "sent": "Same for Bob and this formula here has three.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Predicates in it, so there's going to be a 3 way click here, OK?",
                    "label": 0
                },
                {
                    "sent": "So now here's my Markov network, right, I have.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have my model structure.",
                    "label": 0
                },
                {
                    "sent": "The probability distribution is just one over Z times the exponential of the sum over all formulas of the weight of the Formula Times the number of true groundings of the formula in the data.",
                    "label": 1
                },
                {
                    "sent": "So now what's going to happen is that the more smokers in my domain that have cancer, the more likely the world is going to be, and that's what I want.",
                    "label": 0
                },
                {
                    "sent": "I don't want the world to be indistinguishably impossible, whether one smoker doesn't get cancer or 100.",
                    "label": 0
                },
                {
                    "sent": "And as we shall see, this is going to make a lot of things work for us.",
                    "label": 0
                },
                {
                    "sent": "That wouldn't if we just stuck to the strict.",
                    "label": 0
                },
                {
                    "sent": "A logical interpretation.",
                    "label": 0
                },
                {
                    "sent": "Now, in Markov logic we can handle functions, existential quantifiers, infinite continuous domains.",
                    "label": 1
                },
                {
                    "sent": "I won't talk about that here, 'cause it's not that important, but.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the papers.",
                    "label": 0
                },
                {
                    "sent": "A couple of things that are interesting to note are, first of all, what is the relation of Markov logic to the kinds of statistical models that people use in NLP?",
                    "label": 0
                },
                {
                    "sent": "Well, this is one of the nice things is that they are almost all of them very simple.",
                    "label": 0
                },
                {
                    "sent": "Special cases of Markov logic.",
                    "label": 0
                },
                {
                    "sent": "Meaning if you know how to do things in Markov logic, you know how to do Markov networks, Markov, random fields, which are really the same thing, vision networks, log, linear models, exponential models, maximum entropy models, give distributions, Wilson machines, logistic regression, HMM, CRF etc etc.",
                    "label": 1
                },
                {
                    "sent": "So it's very easy to do these things in Markov logic, and the power that Markov logic adds, though, is that in Markov logic it's very easy to make your objects not be independent.",
                    "label": 0
                },
                {
                    "sent": "Most statistical models assume that your objects are independent, or there's a very simple dependency like a linear chain.",
                    "label": 0
                },
                {
                    "sent": "In Markov logic, you can make the pin them make them dependent anyway that you want just by defining relations that involve them, like for example with the friends XY relation I can make weather Anna smokes depend on where the Bob smokes.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And on the logic side, it's probably you know, intuitive that first order logic is the limit that you get when you have infinite weights, and indeed we have a theorem that says you can answer every entailment Queen 1st order logic by computing conditional probability synonym where the width could be Infinity.",
                    "label": 0
                },
                {
                    "sent": "More Interestingly, when the weights are finite, what happens is that if your knowledge base is satisfiable, meaning I'm able to make all the formulas to at the same time, then those satisfying states are the modes of the distribution.",
                    "label": 0
                },
                {
                    "sent": "So the world is that first order logic likes there still in there, even in my probabilistic version they are the modes of the peaks of the probability.",
                    "label": 0
                },
                {
                    "sent": "Even better, though, in first order logic, if you have a contradiction than anything follows from it and pigs fly and you're doomed in Markov logic if we have a contradiction.",
                    "label": 0
                },
                {
                    "sent": "It's not a problem.",
                    "label": 0
                },
                {
                    "sent": "You weigh the evidence on both sides, and you say what the probability of this statement is, and now we can deal with noisy knowledge, contradictory knowledge, knowledge from 10 different sources that I don't have to integrate very carefully.",
                    "label": 0
                },
                {
                    "sent": "Things actually scale.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's the representation.",
                    "label": 0
                },
                {
                    "sent": "Let me say a little bit about the inference and the learning.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the goal, in inferences to compute probabilities or find the MLP state and one way that we can do that, is with belief propagation.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about belief propagation is that it subsumes a lot of algorithms using NLP like you know, Viterbi various kinds of dynamic programming, etc.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea and belief propagation or one way to look at it, is that we're going to form a bipartite network with the variables on one side and the features on the other, and the variables in Markov logic are ground atoms like smokes, Anna, and the features are ground formulas like smoke San implies.",
                    "label": 0
                },
                {
                    "sent": "Cancer Anna and then what we do is we repeat until convergence this loop where the nodes send messages to their features and the future send messages back to their nodes and in essence what these messages represent are the approximate marginals at that point.",
                    "label": 1
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The computation, so here's a graphical illustration of this.",
                    "label": 0
                },
                {
                    "sent": "So here are my notes there.",
                    "label": 0
                },
                {
                    "sent": "Atoms, like, for example, smokes Anna right?",
                    "label": 0
                },
                {
                    "sent": "It's just a Boolean variable, and these rectangles represent formulas like smoke, sannon friends Anna Bob implies smokes Bob, right?",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "And I have the connections between variables in the formula that they appear.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And right now what I'm going to do is I send these messages.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The notes to the formulas and then I send these messages back from the formulas to the nodes and I keep going right until I converge.",
                    "label": 0
                },
                {
                    "sent": "And this is this is in.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's what belief propagation does, but there's a problem which is for our purposes, this is going to be far too slow.",
                    "label": 1
                },
                {
                    "sent": "The reason this is going to be hard to slow is that on a network with 20 or 30 nodes, sure, but we're going to be dealing very easily here with numbers that have millions and billions of nodes in the applications that we've done so far.",
                    "label": 0
                },
                {
                    "sent": "If you have 1000 objects and you have transitivity, well, there's a building features right there.",
                    "label": 1
                },
                {
                    "sent": "OK, so how do we do belief propagation on the network with billions?",
                    "label": 0
                },
                {
                    "sent": "Or who knows how many nodes?",
                    "label": 0
                },
                {
                    "sent": "The way we're going to do this is by so like using the kind of lifted inference that you can do in logic where you group together the atoms in formulas that pass the same messages into one super node or super feature, and then we only do inference on that on that lifted network, and this lifted network could be, or there's an orders of magnitude smaller than the full network, depending on how much evidence you have and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, here's a graphical illustration of this.",
                    "label": 0
                },
                {
                    "sent": "If you look at the atoms and how they are related to the formula.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "'cause basically what happens is that this Atom forms a group by itself because it talks with that formula.",
                    "label": 0
                },
                {
                    "sent": "These two talk with those three so I can group them together.",
                    "label": 0
                },
                {
                    "sent": "Same with these two right?",
                    "label": 0
                },
                {
                    "sent": "So now what I have is these groups of atoms and formulas that I can treat is 1.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm glossing over the details, will see a little bit more shortly.",
                    "label": 0
                },
                {
                    "sent": "This is just intuition, and now because I only have in essence this one supernode, and this one super feature although.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Messages can actually just be replaced by one OK, and now I do much less work in each iteration than I did before.",
                    "label": 0
                },
                {
                    "sent": "Literally tend to The Who knows how much less work.",
                    "label": 0
                },
                {
                    "sent": "In the limit, I can do infinitely less work because I can reason over infinite domains with in a finite number of steps.",
                    "label": 0
                },
                {
                    "sent": "And the messages that I have to send are very similar to what I had before.",
                    "label": 0
                },
                {
                    "sent": "I just need these constants that have to do with counting how many messages your sending, right?",
                    "label": 0
                },
                {
                    "sent": "If this matches is standing in for 10 messages, this thing that needs to get raised to the 10th power, but pretty much once I have this lifted network belief propagation is the same thing that happened before.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Except much more efficient.",
                    "label": 0
                },
                {
                    "sent": "So here's the idea.",
                    "label": 0
                },
                {
                    "sent": "The way this is going to work is that first we formed the lifted network composed of super nodes and super features.",
                    "label": 0
                },
                {
                    "sent": "A supernode is a set of ground atoms that all send and receive exactly the same messages throughout belief propagation, and the Super features the same thing, except it's for ground clauses.",
                    "label": 1
                },
                {
                    "sent": "OK, so I formed this lifted network and now I run belief propagation on the lifted network and we can prove that this will give you the exact same results as ground BP.",
                    "label": 0
                },
                {
                    "sent": "And we can also prove that we can find the minimal lifted network and there's only one.",
                    "label": 0
                },
                {
                    "sent": "And as I said, the time in memory, not just the time savings, but the memory savings from this can be huge things that wouldn't fit in memory now.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do you form the lifted network again at a high level?",
                    "label": 1
                },
                {
                    "sent": "It's extremely simple.",
                    "label": 0
                },
                {
                    "sent": "Basically, start by forming initial super nodes as follows.",
                    "label": 0
                },
                {
                    "sent": "For each predicate, I'm going to have one supernode containing all the true groundings, once appointed, containing all the false groundings, and one super note containing all the unknown groundings.",
                    "label": 0
                },
                {
                    "sent": "And for some predicates you might have only unknown groundings.",
                    "label": 0
                },
                {
                    "sent": "And now I've formed super features by doing joints of the supernodes that participate in them.",
                    "label": 0
                },
                {
                    "sent": "Right, and so now I have a set of super features and now I form a new generation of super nodes by projecting the Super features down to the predicates that appear in them.",
                    "label": 0
                },
                {
                    "sent": "This is really just a sequence of database operations do joins to form super features and then the projections to form super nodes and then a super knowledge is going to be the groundings of the predicate with the same number of projections from each super feature, because those are the ones that are going to get exactly the same messages.",
                    "label": 1
                },
                {
                    "sent": "So in essence, what we're doing and then and then I repeat.",
                    "label": 0
                },
                {
                    "sent": "So in essence, what this algorithm is doing is simulating belief propagation, keeping track of which nodes become distinguishable.",
                    "label": 0
                },
                {
                    "sent": "So the only things I ever separate are the ones that the evidence sanctions separating and everything else I reason about is 1, because you know anything else is away.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "East so that's inference.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's see a little bit about learning.",
                    "label": 0
                },
                {
                    "sent": "So the data for learning is a relational database, an if all you have is something like text, right?",
                    "label": 1
                },
                {
                    "sent": "What you need to do is to turn your text into relational database.",
                    "label": 0
                },
                {
                    "sent": "This can be done by having like a token predicate that says I have the predicate John between positions one and two, and so forth.",
                    "label": 0
                },
                {
                    "sent": "Or you know any other representation that is convenient.",
                    "label": 0
                },
                {
                    "sent": "You can represent the parse tree easily as a set of atoms and so forth, and here I'm going to make the closed world assumption to make life easy.",
                    "label": 0
                },
                {
                    "sent": "Meaning everything that's not in the database is assumed false.",
                    "label": 0
                },
                {
                    "sent": "If you don't want to make that assumption in some applications, we don't.",
                    "label": 0
                },
                {
                    "sent": "Then you need EM versions of the thing that I'm going to think that I'm going to talk about here and then we have them.",
                    "label": 0
                },
                {
                    "sent": "So now there's two learning tests.",
                    "label": 0
                },
                {
                    "sent": "As usual, there is learning the parameters, i.e.",
                    "label": 1
                },
                {
                    "sent": "The weights of the MLN, and for that we have a voted Perceptron algorithm among others, that I will briefly describe and for learning structure either from scratch or by revising the formulas that you got in one way or another we can use inductive logic programming techniques.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So wait, learning the goal is to maximize the conditional likelihood of your query atoms, giving your evidence once.",
                    "label": 1
                },
                {
                    "sent": "Here's the derivative of the log conditional likelihood with respect to the to await.",
                    "label": 0
                },
                {
                    "sent": "It's very intuitive.",
                    "label": 0
                },
                {
                    "sent": "It's just as in a Markov random field.",
                    "label": 0
                },
                {
                    "sent": "We now have all these dependencies, but if you do the math, the form for the likelihood is still exactly the same, except that now we have this very rich language in which the specify parameter time, right?",
                    "label": 0
                },
                {
                    "sent": "This is like an MRF with a lot of intricate parameter time, and so the derivative is just.",
                    "label": 1
                },
                {
                    "sent": "The number of groundings of the clause corresponding to that weight in the data.",
                    "label": 1
                },
                {
                    "sent": "The two number minus the expected number according to the model.",
                    "label": 0
                },
                {
                    "sent": "So if your model is predicting that the clause is true less often than it is the way it needs to go up, and vice versa, and when they're all line up, we've reached our global optimum.",
                    "label": 0
                },
                {
                    "sent": "It's a convex problem, and we're done.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now how do we do this?",
                    "label": 0
                },
                {
                    "sent": "Well, one algorithm that is, you know, widely used in NLP was originally proposed by Michael Collins for training.",
                    "label": 1
                },
                {
                    "sent": "Hmm, discriminatively is the voted Perceptron.",
                    "label": 0
                },
                {
                    "sent": "And the whole perception is sort of like a structured generalization of the perceptron.",
                    "label": 0
                },
                {
                    "sent": "It assumes that the network is a linear chain, and then what it does is the following.",
                    "label": 1
                },
                {
                    "sent": "You initialize your weights to zero and then you compute your most likely Y given X using Viterbi and then you just do the difference of the counts in that solution and in the data multiply by a learning rate, which could even be one and you know add to the previous wait right?",
                    "label": 0
                },
                {
                    "sent": "And this is a lot like the perceptron.",
                    "label": 0
                },
                {
                    "sent": "The other big difference you know besides being structures that you don't return the final wait, see return their average.",
                    "label": 0
                },
                {
                    "sent": "And this is both empirically much better and theoretically you can.",
                    "label": 0
                },
                {
                    "sent": "You know you can show that this gives better generalization.",
                    "label": 0
                },
                {
                    "sent": "So now what we would like to have is a learning algorithm for millions, which sounds like a much more difficult proposition, but.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, it's not because the only thing we need to do is to take the voted Perceptron and Plug in lifted BP where before we had with Kirby and this could be Max product or some product BP depending on what you prefer, but then everything works as before, except that now you can have an arbitrary network, you know the evidence can be anywhere.",
                    "label": 0
                },
                {
                    "sent": "The structure of the graph can be anything that you want and you can have all this parameter time, but at some level the algorithm is.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me mention.",
                    "label": 0
                },
                {
                    "sent": "Just to make this concrete and see how we can apply it in NLP one application.",
                    "label": 0
                },
                {
                    "sent": "Here's a simple example of information extraction.",
                    "label": 0
                },
                {
                    "sent": "I have a set of citations from the.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paper and I want to segment it, meaning I want to see you know where are the author strings, the titles, the venues.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then I want to do the Inter resolution.",
                    "label": 0
                },
                {
                    "sent": "Meaning you know, I want to figure out that Tripoli 06 is the same as the proceedings of the 21st National Conference on Artificial Intelligence.",
                    "label": 1
                },
                {
                    "sent": "Notice this is hard because the strings are completely different.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then once I've done this, I also want to figure out that these two records are the same.",
                    "label": 0
                },
                {
                    "sent": "In this you know, these two are also the same.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just a simple example of information extract.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the state of the art today is to basically do this as follows.",
                    "label": 0
                },
                {
                    "sent": "For segmentation, use something like an HMM or CRF to assign each took into a field, author, author, author, title, title, venue, and then for the Inter Resolution user classifier, like Syllogistic regression, to predict for each pair of fields or citations whether they are the same.",
                    "label": 1
                },
                {
                    "sent": "And then you do some kind of transitive closure step.",
                    "label": 0
                },
                {
                    "sent": "We face the same as B&B is the same as C, then is the same as C. Now to implement this today using a language like you know Java or C++ or whatever it's going to run.",
                    "label": 0
                },
                {
                    "sent": "You probably 10s of thousands of lines of code and you know it'll probably take you weeks to do that whole thing and debug it and whatnot.",
                    "label": 0
                },
                {
                    "sent": "In Markov logic, you can do this whole thing in just seven formulas that fit on a slide.",
                    "label": 0
                },
                {
                    "sent": "OK, this is sort of like the game that you get you have now this ability to put in knowledge very compactly, much more complicated than if you were using logic and trying to put in all the exceptions.",
                    "label": 0
                },
                {
                    "sent": "And then you let the learning do the hard work for you and the probabilistic inference you know deal with the.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nothing under specification and so forth, so here's an example.",
                    "label": 0
                },
                {
                    "sent": "He's specifically one network that you can use for this.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here, here are the types that you need to have.",
                    "label": 0
                },
                {
                    "sent": "You actually don't need to declare them, but you know just for clarity I am going to declare them here.",
                    "label": 0
                },
                {
                    "sent": "You have tokens, you have fields, say author, title, venue, etc.",
                    "label": 1
                },
                {
                    "sent": "Citations and positions within a site.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation.",
                    "label": 0
                },
                {
                    "sent": "And now your evidence predicate is just token that says this token, say Parag appears in position one of citation C1, OK?",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is my evidence and I'm going to have my query predicates in field.",
                    "label": 0
                },
                {
                    "sent": "Is the predicate that does the segmentation.",
                    "label": 0
                },
                {
                    "sent": "It says that this position in this citation belongs to this field.",
                    "label": 0
                },
                {
                    "sent": "Like for example the first citation is part of the author field and these two do the Inter resolution.",
                    "label": 0
                },
                {
                    "sent": "This one says that the author field in these two citations is the same and this one says that these two citations are the same.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's the MLN.",
                    "label": 0
                },
                {
                    "sent": "These for.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three formulas basically implement an HMM.",
                    "label": 0
                },
                {
                    "sent": "What is CRF?",
                    "label": 0
                },
                {
                    "sent": "Actually?",
                    "label": 0
                },
                {
                    "sent": "Because we train discriminatively, but you know, you can do either.",
                    "label": 0
                },
                {
                    "sent": "And then these four do the Inter resolution.",
                    "label": 0
                },
                {
                    "sent": "So this formula here you read it as follows.",
                    "label": 0
                },
                {
                    "sent": "All the variables that are not quantified are implicitly a universally quantified, as in.",
                    "label": 0
                },
                {
                    "sent": "You know logic, programming and so forth.",
                    "label": 0
                },
                {
                    "sent": "This little plus means that I want to learn a separate wait for every instance of the formula with every combination of these variables.",
                    "label": 0
                },
                {
                    "sent": "So what this formula is saying is that if I have a certain token in a certain position, then that position is in a certain field and I'm going to have a wait for every token.",
                    "label": 0
                },
                {
                    "sent": "Field pair, which means that this is the observation matrix of the HMM.",
                    "label": 0
                },
                {
                    "sent": "There's literally a one to one correspondence.",
                    "label": 0
                },
                {
                    "sent": "Between this and the observation matrix of an HMM.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this here is the transition matrix.",
                    "label": 0
                },
                {
                    "sent": "It says that if I'm in a certain field now that I'm in a certain field in the next position, in fact the only thing that we need for this purpose is to capture the fact that you tend to stay in the same field.",
                    "label": 0
                },
                {
                    "sent": "But in general you could do another.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hmm, this way and this one is actually something that you don't need to do a regular hmm, but we are going to do it here this way, which is I'm going to allow things to not belong to any field.",
                    "label": 0
                },
                {
                    "sent": "I don't need another field, I'm just going to allow you know all of the field predicates to be false, but then I do.",
                    "label": 0
                },
                {
                    "sent": "I need to add this formula.",
                    "label": 0
                },
                {
                    "sent": "It says you can't be in two fields at once, so this says that you know if you are in 1 field then you're not in the other.",
                    "label": 0
                },
                {
                    "sent": "So this is an HMM, it does the segmentation.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And and now these formulas here do the Inter resolution.",
                    "label": 0
                },
                {
                    "sent": "This formula here is basically predicting weather 2 fields are the same from the fact that they have the same tokens in the same positions, right?",
                    "label": 0
                },
                {
                    "sent": "So this is very precisely logistic regression where this is the predicted variable and these are the predictor variables.",
                    "label": 0
                },
                {
                    "sent": "And you could also think of it as just doing a similarity computation, right?",
                    "label": 0
                },
                {
                    "sent": "It's deciding whether to fields are the same by how many tokens they have in common.",
                    "label": 0
                },
                {
                    "sent": "And of course some tokens.",
                    "label": 0
                },
                {
                    "sent": "Matter more than others, right?",
                    "label": 0
                },
                {
                    "sent": "'cause they're more informative and that's why we're going to have a different weight for every token Enfield pair.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Now this formula is very short, but does a lot of work.",
                    "label": 0
                },
                {
                    "sent": "It says that if 2 feet if the fields in a citation are the same, then the citation is the same, right?",
                    "label": 0
                },
                {
                    "sent": "If the author is the same, then the citation is the same.",
                    "label": 0
                },
                {
                    "sent": "If the field is the same, then the citation is the same.",
                    "label": 0
                },
                {
                    "sent": "Notice these things are not valid as logical formulas, but estatistica regularity's.",
                    "label": 0
                },
                {
                    "sent": "They're great, right?",
                    "label": 0
                },
                {
                    "sent": "If the authors are the same, these papers are now much more likely to be the same then if the authors weren't the same.",
                    "label": 0
                },
                {
                    "sent": "OK. And actually this also works the other way if the citation is the same, then the authors.",
                    "label": 0
                },
                {
                    "sent": "And the you know, titles in the venues are also the same.",
                    "label": 0
                },
                {
                    "sent": "So I can actually infer that because you know, these authors are very similar and the title is the same.",
                    "label": 0
                },
                {
                    "sent": "This citation is the same, and now because of that, I now know that triple Year 2000 and the 20th Conference on Artificial Intelligence are the same.",
                    "label": 0
                },
                {
                    "sent": "Right, I don't actually need to encode that this actually gets figured out by the inference, and now this propagates right?",
                    "label": 0
                },
                {
                    "sent": "So you could have.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very long chains of inference, and now these two things are just the transitive closure.",
                    "label": 0
                },
                {
                    "sent": "For example, you know if citation C is the same as the prime and see Primus MSE prime, then sees the same as the prime prime.",
                    "label": 0
                },
                {
                    "sent": "Notice the following a few years ago, McCallum and Welner had a very highly cited paper in NIPS where the only thing that they did was to take a CRF and add transit tivity to do Inter resolution, and you know it was a lot of work for this.",
                    "label": 0
                },
                {
                    "sent": "They had to design algorithms for the inference for the learning and whatnot in Markov logic.",
                    "label": 0
                },
                {
                    "sent": "You can accomplish that just by adding this formula, right?",
                    "label": 0
                },
                {
                    "sent": "It's one more line.",
                    "label": 0
                },
                {
                    "sent": "And the job is done OK. Because we have, you know, the powerful learning and inference algorithms and.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would that actually make this possible now?",
                    "label": 0
                },
                {
                    "sent": "This is actually not a bad interior solution model as it is.",
                    "label": 0
                },
                {
                    "sent": "But this here is a pretty weak segmentation model, and the reason is that this is this is, you know, good at propagating author and good at propagating title, but not very good at designing where the boundary is right.",
                    "label": 0
                },
                {
                    "sent": "Now what people do when they don't do this with CRF, since they come up with rules for predicting the start and end of a field like this, is what the whole area of reproduction is about.",
                    "label": 0
                },
                {
                    "sent": "But combining the two is not is nontrivial.",
                    "label": 0
                },
                {
                    "sent": "Well here, here it actually is trivial.",
                    "label": 0
                },
                {
                    "sent": "For example, we know as you know, then Roth mentioned before that this is a small example of how you win.",
                    "label": 0
                },
                {
                    "sent": "You can include your knowledge easily into an MLN.",
                    "label": 0
                },
                {
                    "sent": "We know that in citations, the field boundaries usually occur at periods.",
                    "label": 0
                },
                {
                    "sent": "OK, so now what I can do is that I qualify this rule that saying that if a position isn't in the field then the next position in the same field.",
                    "label": 0
                },
                {
                    "sent": "That propagation only happens if the token is not a period.",
                    "label": 0
                },
                {
                    "sent": "Having a period breaks the propagation.",
                    "label": 0
                },
                {
                    "sent": "Anyway, just like I added and this makes a huge, we're going to struggle.",
                    "label": 0
                },
                {
                    "sent": "This makes a huge difference to the quality of the result.",
                    "label": 0
                },
                {
                    "sent": "And the same way that I can now just like another things like, which is also, you know it's a weaker predictor, but it's still useful, right?",
                    "label": 0
                },
                {
                    "sent": "So I can put these things in easily.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's what happens on Quora.",
                    "label": 0
                },
                {
                    "sent": "You know fairly standard data set for these things.",
                    "label": 0
                },
                {
                    "sent": "This is this.",
                    "label": 0
                },
                {
                    "sent": "So this is precision and recall you'd like to be up here.",
                    "label": 0
                },
                {
                    "sent": "This is what happens when you just use the tokens right?",
                    "label": 0
                },
                {
                    "sent": "Just you know, basically, logistic regression.",
                    "label": 0
                },
                {
                    "sent": "Here's what happens when you bring in the sequence information.",
                    "label": 0
                },
                {
                    "sent": "It's actually somewhat of a mixed bag because at high recall the precision goes down.",
                    "label": 0
                },
                {
                    "sent": "But not only is what happens when you just modify your rule by putting in this.",
                    "label": 0
                },
                {
                    "sent": "Right at this point, this is no longer in HMM, but we don't care.",
                    "label": 0
                },
                {
                    "sent": "We have the algorithms to deal with it, you know?",
                    "label": 0
                },
                {
                    "sent": "Now you're getting pretty up close to this corner, and if you now add the comma, you get even a little bit better.",
                    "label": 0
                },
                {
                    "sent": "So even with just a simple MLN we get results that are pretty close to the state of the art.",
                    "label": 0
                },
                {
                    "sent": "Now, of course we can keep going and try to write down more of our knowledge.",
                    "label": 0
                },
                {
                    "sent": "You know the kinds of things that then was talking about.",
                    "label": 0
                },
                {
                    "sent": "You know, citations tend to start with an author, for example, and you don't.",
                    "label": 0
                },
                {
                    "sent": "You know you don't have more than one instance of this.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In field and so forth.",
                    "label": 0
                },
                {
                    "sent": "And we've done that.",
                    "label": 0
                },
                {
                    "sent": "We get an MLN with about 40 rules in it.",
                    "label": 0
                },
                {
                    "sent": "You know, this is on the web.",
                    "label": 0
                },
                {
                    "sent": "You can look at it.",
                    "label": 0
                },
                {
                    "sent": "You can understand it.",
                    "label": 0
                },
                {
                    "sent": "And this MLN gives the best results to date on sites here in core beating models like CRF based models and others that took a long time to develop and a lot of people working on them and gazillions of features.",
                    "label": 0
                },
                {
                    "sent": "These models actually don't have that many parameters.",
                    "label": 0
                },
                {
                    "sent": "They have the right parameters because you specify the structure by writing the formulas by specifying your knowledge.",
                    "label": 0
                },
                {
                    "sent": "So this is one thing that we've done.",
                    "label": 0
                },
                {
                    "sent": "Let me mention a couple of others.",
                    "label": 0
                },
                {
                    "sent": "Here's a more recent result.",
                    "label": 0
                },
                {
                    "sent": "We've developed an approach.",
                    "label": 0
                },
                {
                    "sent": "The approach that we saw here was supervised.",
                    "label": 0
                },
                {
                    "sent": "We've also developed an approach to unsupervised coreference resolution using an EM like version of our algorithms and we get the best results to date on machinas.",
                    "label": 1
                },
                {
                    "sent": "For those of you familiar with the Higgin client paper, they got 70% F. One we get 80.",
                    "label": 0
                },
                {
                    "sent": "This is a huge jump.",
                    "label": 0
                },
                {
                    "sent": "In fact, this is such a huge jump that we actually do better than most supervised approaches on these on these testbeds.",
                    "label": 0
                },
                {
                    "sent": "Another thing that we've done over somewhat different flavor, but just to give you an idea of what can be done, is we've also done extracting whole ontologies from data.",
                    "label": 1
                },
                {
                    "sent": "From text data, here's what we've done.",
                    "label": 0
                },
                {
                    "sent": "We took text Runner, which is a system that runs over, you know, millions of web pages and extracts tuples by basically doing some heuristic parsing to see where the objects in their relations are.",
                    "label": 0
                },
                {
                    "sent": "But then of course it's extremely noisy.",
                    "label": 1
                },
                {
                    "sent": "But basically we take from them 2 million tuples.",
                    "label": 0
                },
                {
                    "sent": "This is not.",
                    "label": 0
                },
                {
                    "sent": "This is a fairly large scale thing, and the first thing that we do is you know the coreference resolution and you know we write out the box.",
                    "label": 1
                },
                {
                    "sent": "We beat the system that they have called resolve for doing this, but then we don't need to stop there.",
                    "label": 0
                },
                {
                    "sent": "We can do the ontology induction.",
                    "label": 0
                },
                {
                    "sent": "And So what we do is, in essence, we're doing clustering of these symbols again.",
                    "label": 0
                },
                {
                    "sent": "The MLN that we're using for this is very simple.",
                    "label": 0
                },
                {
                    "sent": "It has literally half a dozen rules, but it does things like it discovers the concept of Planet of Country of City.",
                    "label": 0
                },
                {
                    "sent": "It, of course, doesn't name them that.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the classes that it's forming that actually very high quality clusters, and you should compare them, for example with Word Net, where they overlap, we get high quality results, wouldn't it?",
                    "label": 0
                },
                {
                    "sent": "Has some things that we don't because you know this is web text.",
                    "label": 0
                },
                {
                    "sent": "This also has a ton of concepts that were not doesn't, but now you can put the two together.",
                    "label": 0
                },
                {
                    "sent": "And you actually have a much richer ontology than.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is where we're at today.",
                    "label": 0
                },
                {
                    "sent": "So before I conclude, let me just mention where we want to go next.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In fact, where we're going now.",
                    "label": 0
                },
                {
                    "sent": "So the thing that we're doing now is we have now extracted this ontology from text and we have structured learning algorithms IOP type algorithms.",
                    "label": 0
                },
                {
                    "sent": "We're going to apply them to this and extract rules.",
                    "label": 0
                },
                {
                    "sent": "And remember, these are probabilistic rules.",
                    "label": 0
                },
                {
                    "sent": "These are, you know this is an MLN, and now once we've done that, we can now ask queries.",
                    "label": 0
                },
                {
                    "sent": "And the system potentially can answer questions that are not answered in anyone given fact it can actually do chaining.",
                    "label": 0
                },
                {
                    "sent": "Right so we you know, we now that we from the ontology, we can induce knowledge and now we can start to answer questions.",
                    "label": 0
                },
                {
                    "sent": "We would also like to apply Markov logic to other NLP tasks like for example parsing.",
                    "label": 1
                },
                {
                    "sent": "It's very straightforward to encode, you know PCF GSM lens and we've done this for toy grammars.",
                    "label": 0
                },
                {
                    "sent": "Of course doing this for real grammars is something that's not trivial, but we're going to try to do that.",
                    "label": 0
                },
                {
                    "sent": "And then we want to connect all the pieces, do the joint inference, and then close the loop, feed the knowledge back into the NLP.",
                    "label": 0
                },
                {
                    "sent": "Let the knowledge help with the parsing.",
                    "label": 0
                },
                {
                    "sent": "Let the knowledge that I've acquired by reading some texts for example, help me decide that you know you know.",
                    "label": 0
                },
                {
                    "sent": "In John 8 the pizza with a fork, you know the fork attaches to 8 and then ate the pizza with pepperoni.",
                    "label": 0
                },
                {
                    "sent": "It attaches to the pizza right?",
                    "label": 0
                },
                {
                    "sent": "Because one is a utensil and the other one is edible, right?",
                    "label": 0
                },
                {
                    "sent": "That's the kind of thing that we can do when we close the loop between these two things so.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is where we're going to go.",
                    "label": 0
                },
                {
                    "sent": "Let me summarize.",
                    "label": 0
                },
                {
                    "sent": "Language and knowledge are a chicken and egg problem.",
                    "label": 1
                },
                {
                    "sent": "You need knowledge to understand language using language to acquire knowledge.",
                    "label": 0
                },
                {
                    "sent": "So the solution is to bootstrap, you know, start with a little bit of knowledge.",
                    "label": 0
                },
                {
                    "sent": "Read some text with that.",
                    "label": 0
                },
                {
                    "sent": "Have more knowledge.",
                    "label": 0
                },
                {
                    "sent": "Read more text.",
                    "label": 1
                },
                {
                    "sent": "Keep going Markov logic Network provides a language and algorithms for doing this.",
                    "label": 0
                },
                {
                    "sent": "The language is very simple.",
                    "label": 0
                },
                {
                    "sent": "It's weight 1st order formulas that get compiled into Markov networks.",
                    "label": 0
                },
                {
                    "sent": "We saw an inference algorithm for this lifted with propagation.",
                    "label": 1
                },
                {
                    "sent": "Anna learning algorithm voted perceptron.",
                    "label": 1
                },
                {
                    "sent": "We've had several successes to date.",
                    "label": 0
                },
                {
                    "sent": "With this like you know we have the state of the art results in a number of problems.",
                    "label": 0
                },
                {
                    "sent": "I mentioned some of them.",
                    "label": 0
                },
                {
                    "sent": "All of this, all of the algorithms that I've described in a whole bunch of others are available in this open source software called Alchemy, available at this URL, and you can download it, play with it.",
                    "label": 0
                },
                {
                    "sent": "You can use it to solve your NLP test by encoding your knowledge or your hypothesis and trying things out without having to worry about the learning in the inference.",
                    "label": 0
                },
                {
                    "sent": "Or if you have a better learning and inference algorithm, you can plug it into outcome using the existing API's and then immediately works with everything else and.",
                    "label": 0
                },
                {
                    "sent": "With other things that people might have in the future, and our goal is, you know, we actually have a website.",
                    "label": 0
                },
                {
                    "sent": "Our goal is to actually start a repository where people can contribute millions so you can use somebody's in lens, solve a problem better using.",
                    "label": 0
                },
                {
                    "sent": "Maybe those are some lens and build up this body of knowledge right?",
                    "label": 0
                },
                {
                    "sent": "We want to have a corpus of knowledge, not just the corpus of raw text.",
                    "label": 0
                },
                {
                    "sent": "And now when somebody wants to solve an LP problem, they can draw on that corpus and you know they can modify it, refine it, contribute their new MLN and hopefully in this way we will over the years get something that's.",
                    "label": 0
                },
                {
                    "sent": "Very sophisticated and able to do things that haven't been possible before.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "So of course, these books trucks are increasing, but just as sort of a self feeding mechanism.",
                    "label": 0
                },
                {
                    "sent": "So we can either yeah converge or like rapidly if you're just getting wrong knowledge that then.",
                    "label": 0
                },
                {
                    "sent": "Wrong translation right?",
                    "label": 0
                },
                {
                    "sent": "Exactly?",
                    "label": 0
                },
                {
                    "sent": "So what we need here is something like the analog of the EM theorem that says that you get better in each iteration as opposed to getting worse.",
                    "label": 0
                },
                {
                    "sent": "And I think you know we haven't really started to go through this loop a lot of times, but I think one of the keys is to make sure that.",
                    "label": 0
                },
                {
                    "sent": "We were able to filter out the bad knowledge and keeping the good knowledge and one way that we've been doing this so far is that when we do this ontology induction.",
                    "label": 0
                },
                {
                    "sent": "In essence, what we're doing is like we're keeping the things that are even indirectly consistent and throwing out the things are inconsistent.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if you're doing this from the web and you know the majority opinion on the web directly and indirectly via chains of influences that you know Elvis killed Kennedy, then we will believe that Elvis killed Kennedy, right?",
                    "label": 0
                },
                {
                    "sent": "But you know, we can't do better than the sources.",
                    "label": 0
                },
                {
                    "sent": "And we'll see how things turn out, but you know, that's the plan, yeah?",
                    "label": 0
                },
                {
                    "sent": "So your formula for transitivity is same field.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Just an axiom.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's just it's just a template, so it could learn that same field actually means you know using the citation closure or or something like that, right?",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "True fact about same field in same citation, no no.",
                    "label": 0
                },
                {
                    "sent": "So in this case we're doing supervised learning and we're not inventing predicates, So what happens is that I have a database where these things are labeled.",
                    "label": 0
                },
                {
                    "sent": "The one thing that I'm going to learn for this is not the meaning of same field, because that doesn't change is the weight of this rule.",
                    "label": 0
                },
                {
                    "sent": "I could decide to ignore this rule, but if I if I if this rule has a positive weight, it really means that you know it's transitivity.",
                    "label": 0
                },
                {
                    "sent": "Run the waiting.",
                    "label": 0
                },
                {
                    "sent": "Oh so good point.",
                    "label": 0
                },
                {
                    "sent": "You could decide that this rule is a hard rule and cannot be violated because transitivity is definitionally true, and in fact we tried that out and you know it's totally up to you and you know it's not.",
                    "label": 0
                },
                {
                    "sent": "It's OK to do that, but you actually get better results by allowing a finite way to be learned by this.",
                    "label": 0
                },
                {
                    "sent": "And the reason for that is that your data is poor.",
                    "label": 0
                },
                {
                    "sent": "We don't have a real model of what's going on, and so in some cases not for cluster recall, but for pairwise recall.",
                    "label": 0
                },
                {
                    "sent": "You're actually better off, you know, allowing this to be violated, but if you don't like that, just make that a hard rule.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Equality is not necessarily transitive.",
                    "label": 0
                },
                {
                    "sent": "No, I would remember we're doing discriminative learning and the model that we have.",
                    "label": 0
                },
                {
                    "sent": "The world is oversimplified, so sometimes it's better to violate things that are true.",
                    "label": 0
                },
                {
                    "sent": "But if you don't want to violate them, it's not that big of a difference.",
                    "label": 0
                },
                {
                    "sent": "You can just leave it like this if you remember the you know, then let's talk this morning.",
                    "label": 0
                },
                {
                    "sent": "He talked about having hard constraints and having soft constraints.",
                    "label": 0
                },
                {
                    "sent": "One thing that I didn't mention is that in Markov logic or in alchemy you can the way you make a constraint harder just by putting a full stop at the end, as in Prolog.",
                    "label": 0
                },
                {
                    "sent": "So if you want to make transitivity hard rule.",
                    "label": 0
                },
                {
                    "sent": "You just put a full stop here in the full stop here and then the way it doesn't get learn.",
                    "label": 0
                },
                {
                    "sent": "And then what happens is that all the learning and parameter learning happened within the box of these constraints.",
                    "label": 0
                },
                {
                    "sent": "They only happen in things that satisfy that.",
                    "label": 0
                },
                {
                    "sent": "When we start bootstrapping, the number of rules is going to keep increasing.",
                    "label": 0
                },
                {
                    "sent": "The number of variables actually not necessarily.",
                    "label": 0
                },
                {
                    "sent": "What's probably going to happen is that the number of rules is going to converge to something and not move beyond that right?",
                    "label": 0
                },
                {
                    "sent": "There are these people have done this sort of like mass collaboration projects where everybody can contribute knowledge over the web and one of the things that they found is that this is incredibly wasteful because everybody is always contributing the same knowledge.",
                    "label": 0
                },
                {
                    "sent": "So in fact.",
                    "label": 0
                },
                {
                    "sent": "That is almost the same rule, but just slightly different looking and Oh no, no.",
                    "label": 0
                },
                {
                    "sent": "But here's The thing is that this isn't logic.",
                    "label": 0
                },
                {
                    "sent": "This is Markov logic and the beauty of Markov logic is that you don't need those 50 different.",
                    "label": 0
                },
                {
                    "sent": "This was actually our experience.",
                    "label": 0
                },
                {
                    "sent": "We built these models with 50 rules and we discovered that only five were necessary because there's a lot of richness in the way the weights interact.",
                    "label": 0
                },
                {
                    "sent": "You actually get the right conclusions alot of the time, even if you don't have exactly the right formula.",
                    "label": 0
                },
                {
                    "sent": "So you get like this huge reduction in the number of actual rules that you need.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of redundancy, and this model won't learn redundant rules right, because they won't pay for themselves.",
                    "label": 0
                },
                {
                    "sent": "There's the usual complexity, penalty, price, whatnot.",
                    "label": 0
                },
                {
                    "sent": "So unless a rule is really allowing you to reach conclusions that you couldn't before or you know to be more precise, changing the probability landscape as opposed to just shifting weight around between equivalent things, we won't learn it.",
                    "label": 0
                },
                {
                    "sent": "But you know this, we will see what happens, right?",
                    "label": 0
                },
                {
                    "sent": "This is what we've seen so far.",
                    "label": 0
                },
                {
                    "sent": "Is that very, very simple models actually give you like the one I just showed you actually give you most of what you need.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you're reading loads of text, you might find loading different rules about loads of objects, but they're all the same object class and you have to make something.",
                    "label": 0
                },
                {
                    "sent": "This is exactly right, but this is what we do.",
                    "label": 0
                },
                {
                    "sent": "In fact, the first thing that we do is we resolve all those objects to figure out that it's the same object and the way we do this is.",
                    "label": 0
                },
                {
                    "sent": "Well it now OK, so remember you're reading text if what the text is about them is the same, then as far as the text is concerned they are the same.",
                    "label": 0
                },
                {
                    "sent": "If it's like, well if they're slightly different then I will have an ontology with the two kinds of objects.",
                    "label": 0
                },
                {
                    "sent": "And I will also want to have the generalization of them right, so if it really is the case that my world is large, then I want my ontology to grow.",
                    "label": 0
                },
                {
                    "sent": "Absolutely, but in.",
                    "label": 0
                },
                {
                    "sent": "Indeed, and in that case I want to have the extra bowl.",
                    "label": 0
                },
                {
                    "sent": "OK, any other question maybe sure.",
                    "label": 0
                },
                {
                    "sent": "It probably requires some effort to get off to be fast enough.",
                    "label": 0
                },
                {
                    "sent": "No kidding.",
                    "label": 0
                },
                {
                    "sent": "That's what we've been working on is we make this fast so you don't have to.",
                    "label": 0
                },
                {
                    "sent": "So in terms of alchemy.",
                    "label": 0
                },
                {
                    "sent": "How much knowledge is required in order to get?",
                    "label": 0
                },
                {
                    "sent": "Good results in to get it to run fast enough for casual user, do you really understand absolutely right?",
                    "label": 0
                },
                {
                    "sent": "This is the key question, so here's what our experience of the people using this has been so far right and we have a mailing list or people can send feedback and ask questions.",
                    "label": 0
                },
                {
                    "sent": "Is getting getting alchemy working?",
                    "label": 0
                },
                {
                    "sent": "Is actually very quick if you know first order logic you can write formulas and really in one afternoon you can learn how to use Alchemy Now how long it takes you to get alchemy to do what you want depends on you and the domain.",
                    "label": 0
                },
                {
                    "sent": "It could be difficult.",
                    "label": 0
                },
                {
                    "sent": "You usually have to go through this process where.",
                    "label": 0
                },
                {
                    "sent": "You try putting things in, taking them out, see how fast they are, see whether they give the right results, right?",
                    "label": 0
                },
                {
                    "sent": "So there's the usual refinement process that always has to happen, except that now you're doing through this explicit and understandable interface of the rules.",
                    "label": 0
                },
                {
                    "sent": "But you know this is no magic pen is here, right?",
                    "label": 0
                },
                {
                    "sent": "We've put these things together, but it doesn't work any better than individual components, right?",
                    "label": 0
                },
                {
                    "sent": "Weight learning is not something that you can treat completely as a back black box or belief propagation, so you know we inherit the pluses and minuses of those things.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}