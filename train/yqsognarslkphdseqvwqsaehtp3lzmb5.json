{
    "id": "yqsognarslkphdseqvwqsaehtp3lzmb5",
    "title": "Structure Preserving Embedding",
    "info": {
        "author": [
            "Blake Shaw, Department of Computer Science, Columbia University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/icml09_shaw_spe/",
    "segmentation": [
        [
            "Thanks, Julian.",
            "I'm a Blake Shaw and today I'll be presenting structure preserving, embedding joint work with Tony Jebara at Columbia University.",
            "Maybe I'll wait for people to get a seat."
        ],
        [
            "Structure preserving embedding SP is a graph embedding algorithm.",
            "Meaning given as input a symmetric binary adjacency matrix representing a graph of nodes and edges.",
            "SP will produce as output.",
            "Point coordinates for each node in the graph in some D dimensional space.",
            "There are many different objectives for assigning point coordinates to nodes.",
            "For example, one objective is to draw node."
        ],
        [
            "In a 2D plane such that no edges ever cross this is, this is the objective for planar graphs.",
            "Another objective is approximating NP hard sparsest cut problem.",
            "So they're solving this combinatorial problem using graph embedding.",
            "Our focus is visualization and compression.",
            "There are many interesting real world datasets and synthetic datasets that consist only of binary or actions between entities such as links between websites, friendships in a social network, etc.",
            "We want to be able to visualize these interesting network datasets."
        ],
        [
            "Let's consider some algorithms that accomplish this.",
            "Many graph embedding techniques are based off of spring embedding.",
            "So first the nodes are randomly placed in the 2D plane.",
            "Nodes which have edges between them have Springs between them and their use Hooke's law to compute the forces an you simulate this physical system until it reaches some equilibrium, some local optimum.",
            "There is spectral embedding which is basically running PCA on the adjacency matrix.",
            "You know decomposed adjacency matrix with an SVD and use the eigenvectors with the highest eigenvalues as the coordinates.",
            "Similarly Laplacian Eigen Maps also you does the spectral decomposition but does it on the graph Laplacian instead of on the adjacency matrix.",
            "And here you use the eigenvectors with the smallest non zero eigenvalues for the coordinates for the nodes.",
            "SPD is similar."
        ],
        [
            "To these other spectral methods.",
            "But instead of applying a spectral decomposition to the adjacency matrix or the graph Laplacian, we apply it to a matrix K which we learn via semidefinite program.",
            "So this K that we learn has very specific properties.",
            "One we have linear constraints in our SDP that preserve the topology of the input graph.",
            "In this kernel K that we're learning.",
            "And also the objective function of our SDP guarantees that K is low rank and close to the spectral embedding solution.",
            "So once we've found K again, we do a spectral decomposition and we use the eigenvectors with the largest eigenvalues as the coordinates for the nodes.",
            "This technique is very similar to maximum variance in folding and minimum volume embedding where we couple performing an SDP with an SVD.",
            "To give a low dimensional embedding, let's see how these algorithms."
        ],
        [
            "So on a simple toy example here we see the Mobius ladder graph.",
            "This is a classical graph.",
            "The embedding should look something like the Mobius Band.",
            "Well, we see that spectral embedding collapses some of the nodes on top of each other, and Furthermore from the eigen spectrum we see that there are six possible dimensions to choose from when we're trying to choose coordinates for the nodes in this graph.",
            "The spring embedding on the left looks like a good diagram of what this graph should look like, although I wouldn't say that it's really topologically correct, because any nearest neighbor type algorithm would connect nodes along the red dotted lines shown here and not along the blue lines.",
            "So the topological property that this strip has only one side is lossed in this embedding, and then the spring embedding on the right shows a poor local optimum.",
            "Really, we want to algorithm, which is convex.",
            "SBE for doing these graph embeddings?",
            "SBE embedding has four dominant dimensions.",
            "As we can see from the Eigen spectrum, two of them are more dominant than the others and we see that it accurately captures the twist of the Mobius Band."
        ],
        [
            "So now that I've motivated the problem, let's dig into the SB algorithm, starting with our objective for our semidefinite program, which ensures that K is low rank and thus we get a low dimensional graph embedding."
        ],
        [
            "So low rank K corresponds to low dimensional embedding.",
            "That's because when we choose our eigenvectors for our embedding, the ones with non zero eigenvalues we keep.",
            "So any ones we want as many eigenvectors with zero eigenvalues.",
            "So we can drop them.",
            "So here's the objective for the semidefinite program for structure preserving embedding we show in the paper in the poster that this objective function produces a low rank version.",
            "Of spectral embedding.",
            "So here we're trying to learn a matrix K, and there's some common constraints that K is positive semidefinite that K is bounded from growing, growing, and that K is centered.",
            "So come see the poster tomorrow night if you want to more details about this objective function and how we guarantee it to produce a low rank version of spectral embedding."
        ],
        [
            "So that's the objective function of RSVP.",
            "Now let's talk about the constraints on SDP.",
            "Both are linear.",
            "We start with this intuition that we want reversible graph embeddings.",
            "We really want the matrix K to sort of inherit the global topological properties of the input adjacency matrix.",
            "So we want to guarantee the connectivity algorithm G of K such as K nearest neighbors or be matching maximum weight spanning tree, etc should be able to recover the edges from the coordinates of the embedding such that the output of that connectivity algorithm is exactly the input adjacency matrix.",
            "Giving some edges we can run embedding to get points, but the key intuition here is that we want now given points to be able to run a connectivity algorithm and get back the exact initial edges.",
            "That would say that the points the embedding is topologically correct and it respects the original edges in the input adjacency matrix.",
            "So for a variety of connectivity."
        ],
        [
            "Rythms, Kenyeres neighbors be matching maximum weight spanning tree epsilon, neighborhood graphs.",
            "We can guarantee this property with linear constraints on K. Let's take a look at some examples.",
            "So if the connectivity algorithm is K nearest neighbors where each point is connected to the K, other points which are the closest we have.",
            "These linear constraints on K stating that the distance between unconnected nodes has to be greater than the largest connected distance.",
            "Here distance is just a linear function of KR embedding.",
            "Similarly for epsilon neighborhoods, each point is connected to all their points, which lie within a radius of epsilon.",
            "This again corresponds to these simple linear constraints on K that guarantee that when we run this algorithm on our embedding, we get back exactly the edges we used to construct that embedding.",
            "So what's going on here?",
            "Is these constraints are preventing, for example, the blue nodes from invading the neighborhood of the red node and thus changing the adjacency matrix that would be formed from running the connectivity algorithm on the embedding."
        ],
        [
            "So here is the SP algorithm when the connectivity algorithm is K nearest neighbors or epsilon balls etc.",
            "It's one SDP where we're maximizing the cost function that I showed you earlier with these simple linear constraints on K. Once we learn K, we apply an SVD decay and we use the top eigenvectors for the coordinates.",
            "Let's talk about some other possibilities for the connectivity algorithm.",
            "If it's be matching or maximum weight spanning tree, one of these maximum weight sub graph methods, can we come up with linear constraints on K that Similarly make this guarantee that we want so be matching tries to assign neighbors for each node be neighbors such that it maximizes some weight in a weight matrix?",
            "Here we're going to find weights just as a linear function of K negated distances.",
            "Maximum spanning tree similarly tries to maximize weight, but guarantees that the adjacency matrix formed forms a tree.",
            "These algorithms have have sort of stricter constraints than K nearest neighbors.",
            "They're trying to enforce this global property that the adjacency matrix is a tree or has exactly this many neighbors, so."
        ],
        [
            "So the constraints on K take this form.",
            "There's possibly an exponential number of these constraints stating that one configuration beats sort of all other configurations.",
            "Here a~ is all other adjacency matrices belonging to some class like be matchings or trees.",
            "But Fortunately for us, we can use a cutting plane technique to avoid enumeration, similar to the structured prediction SVM work.",
            "So we iterate an SDP, adding the worst violating constraint of this form at each iteration.",
            "Let's see what that looks."
        ],
        [
            "So again, we're given as input an adjacency matrix connectivity algorithm, which is a maximum weight sub graph method such as be matching or maximum spanning tree.",
            "Now we're going to 1st solve the SDP without any structure preserving constraints.",
            "This is going to give us back the rank one spectral embedding solution, and then we're going to find the most violated constraint.",
            "And we're going to add that to RSVP, and we're going to iterate and we're going to stop iterating when the weight of the worst violator comes close to the optimal weight.",
            "Let's talk about how SP is implemented.",
            "It's implemented in."
        ],
        [
            "Matlab using CSDP&SDPLR.",
            "It has complexity similar to a lot of other SCPS.",
            "For dimensionality reduction, there are many inactive constraints, so working set methods yield good speedups, and in particular."
        ],
        [
            "SDP LR takes advantage of our low rank objective and that also offers a speedup.",
            "We've run SP on graphs with up to 1000 nodes.",
            "Now that I've talked about the details of the SP algorithm, let me show you some experimental results."
        ],
        [
            "Here are some classical graphs we saw the Mobius ladder, and here's the Tesseract or the hypercube."
        ],
        [
            "Here's the Selman, Swartz Mark and the Balaban 10 cage.",
            "As we see, spectral embedding will often find many possible dimensions to use for embedding.",
            "That's what we can see from the eigen spectrum next to each graph.",
            "Where SB on the other hand, gives us compact embeddings, only a few possible dimensions for visualization.",
            "Here are some."
        ],
        [
            "Organic molecules, the true physical 3D embedding, is shown on the left.",
            "This is what the molecule actually looks like.",
            "Given only information about which nodes connect to which others.",
            "You know can we sort of come close to this real true 3D embedding in a 2D embedding that only has the connectivity information to work with?",
            "Spectral embedding seems to clump together nodes in strange ways doesn't really seem to give us a good informative embedding that looks like the 3D embedding Laplacian eigen Maps will normalize applied Maps.",
            "They don't behave as well either where structure preserving embedding seems to give a good embedding.",
            "That's a good diagram of what these molecules actually look like."
        ],
        [
            "Here is 981 political blogs and the link structure between all of them.",
            "The red is the conservative, the blue is the liberal, and the reconstruction error is shown next to the eigen spectrums for each.",
            "So this reconstruction error is the number of edges that.",
            "That are different from the initial link structure.",
            "If you were to run a connectivity algorithm in this case be matching on just the two D embedding, and so you can see that we have the lowest reconstruction error and our embedding is 2 dimensional."
        ],
        [
            "Structure preserving embedding is similar to many of these manifold learning based methods for dimensionality reduction, such as locally linear embedding, maximum variance in folding, minimum volume embedding.",
            "These methods preserve pairwise distances between data points.",
            "However, adding topology preserving constraints to these existing algorithms yields more accurate embeddings.",
            "Prevents prevents collapsing of the underlying manifold.",
            "Let me give you an example.",
            "So here are some original data running nvu or mve on this data set will collapse the diamond in the middle of the barbell where adding these topology preserving constraints will prevent the distance between these two points from shrinking too much to the point where it would change the underlying connectivity and therefore adding these topology preserving constraints more accurately.",
            "Represents this data set."
        ],
        [
            "So the maximum variance unfolding objective.",
            "Tries to unfold the data subject to constraints that preserve local distance is defined by a graph on the data.",
            "Similarly, minimum volume bedding tries to maximize variance in the target D and target D dimensions.",
            "For nearest neighbor graphs, all we have to do is add these linear constraints to this optimization and we can preserve topology as well as simply local distances.",
            "For the maximum weight sub graph methods, we can iteratively add this constraint corresponding to the worst violator at each iteration.",
            "And similarly, we can preserve topology.",
            "So."
        ],
        [
            "Here's an example of what I mean.",
            "We ran a one nearest neighbor classifier on a variety of UCI datasets.",
            "They were all embedded with these dimensionality reduction techniques, and we're going to compare using.",
            "Compare the accuracy of this classifier on the 2 dimensional embeddings versus using all the original features.",
            "We see that MVP plus SP yields the most accurate low dimensional embedding across the board.",
            "An in three cases op digits and honest, honest, here and and.",
            "Wine, is it or no and E. Coli.",
            "We actually get a higher accuracy than in the all dimensional case.",
            "That means that using 2 features per data point, those two features that come back from Envy plus SP you get higher K nearest neighbor or one nearest neighbor accuracy then using all the original features for each data point.",
            "Um?",
            "So, So what can we take back in this?"
        ],
        [
            "We can see that SB finds low dimensional representations of graphs that implicitly preserve topology.",
            "And also that preserving local distances is insufficient for faithfully embedding graphs in low dimensional space.",
            "We need to preserve graph topology as well.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks, Julian.",
                    "label": 0
                },
                {
                    "sent": "I'm a Blake Shaw and today I'll be presenting structure preserving, embedding joint work with Tony Jebara at Columbia University.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll wait for people to get a seat.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Structure preserving embedding SP is a graph embedding algorithm.",
                    "label": 1
                },
                {
                    "sent": "Meaning given as input a symmetric binary adjacency matrix representing a graph of nodes and edges.",
                    "label": 0
                },
                {
                    "sent": "SP will produce as output.",
                    "label": 1
                },
                {
                    "sent": "Point coordinates for each node in the graph in some D dimensional space.",
                    "label": 0
                },
                {
                    "sent": "There are many different objectives for assigning point coordinates to nodes.",
                    "label": 0
                },
                {
                    "sent": "For example, one objective is to draw node.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a 2D plane such that no edges ever cross this is, this is the objective for planar graphs.",
                    "label": 0
                },
                {
                    "sent": "Another objective is approximating NP hard sparsest cut problem.",
                    "label": 1
                },
                {
                    "sent": "So they're solving this combinatorial problem using graph embedding.",
                    "label": 0
                },
                {
                    "sent": "Our focus is visualization and compression.",
                    "label": 1
                },
                {
                    "sent": "There are many interesting real world datasets and synthetic datasets that consist only of binary or actions between entities such as links between websites, friendships in a social network, etc.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to visualize these interesting network datasets.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's consider some algorithms that accomplish this.",
                    "label": 0
                },
                {
                    "sent": "Many graph embedding techniques are based off of spring embedding.",
                    "label": 0
                },
                {
                    "sent": "So first the nodes are randomly placed in the 2D plane.",
                    "label": 0
                },
                {
                    "sent": "Nodes which have edges between them have Springs between them and their use Hooke's law to compute the forces an you simulate this physical system until it reaches some equilibrium, some local optimum.",
                    "label": 0
                },
                {
                    "sent": "There is spectral embedding which is basically running PCA on the adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "You know decomposed adjacency matrix with an SVD and use the eigenvectors with the highest eigenvalues as the coordinates.",
                    "label": 1
                },
                {
                    "sent": "Similarly Laplacian Eigen Maps also you does the spectral decomposition but does it on the graph Laplacian instead of on the adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "And here you use the eigenvectors with the smallest non zero eigenvalues for the coordinates for the nodes.",
                    "label": 0
                },
                {
                    "sent": "SPD is similar.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To these other spectral methods.",
                    "label": 0
                },
                {
                    "sent": "But instead of applying a spectral decomposition to the adjacency matrix or the graph Laplacian, we apply it to a matrix K which we learn via semidefinite program.",
                    "label": 0
                },
                {
                    "sent": "So this K that we learn has very specific properties.",
                    "label": 0
                },
                {
                    "sent": "One we have linear constraints in our SDP that preserve the topology of the input graph.",
                    "label": 1
                },
                {
                    "sent": "In this kernel K that we're learning.",
                    "label": 1
                },
                {
                    "sent": "And also the objective function of our SDP guarantees that K is low rank and close to the spectral embedding solution.",
                    "label": 0
                },
                {
                    "sent": "So once we've found K again, we do a spectral decomposition and we use the eigenvectors with the largest eigenvalues as the coordinates for the nodes.",
                    "label": 0
                },
                {
                    "sent": "This technique is very similar to maximum variance in folding and minimum volume embedding where we couple performing an SDP with an SVD.",
                    "label": 0
                },
                {
                    "sent": "To give a low dimensional embedding, let's see how these algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So on a simple toy example here we see the Mobius ladder graph.",
                    "label": 0
                },
                {
                    "sent": "This is a classical graph.",
                    "label": 0
                },
                {
                    "sent": "The embedding should look something like the Mobius Band.",
                    "label": 0
                },
                {
                    "sent": "Well, we see that spectral embedding collapses some of the nodes on top of each other, and Furthermore from the eigen spectrum we see that there are six possible dimensions to choose from when we're trying to choose coordinates for the nodes in this graph.",
                    "label": 0
                },
                {
                    "sent": "The spring embedding on the left looks like a good diagram of what this graph should look like, although I wouldn't say that it's really topologically correct, because any nearest neighbor type algorithm would connect nodes along the red dotted lines shown here and not along the blue lines.",
                    "label": 0
                },
                {
                    "sent": "So the topological property that this strip has only one side is lossed in this embedding, and then the spring embedding on the right shows a poor local optimum.",
                    "label": 0
                },
                {
                    "sent": "Really, we want to algorithm, which is convex.",
                    "label": 0
                },
                {
                    "sent": "SBE for doing these graph embeddings?",
                    "label": 0
                },
                {
                    "sent": "SBE embedding has four dominant dimensions.",
                    "label": 0
                },
                {
                    "sent": "As we can see from the Eigen spectrum, two of them are more dominant than the others and we see that it accurately captures the twist of the Mobius Band.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now that I've motivated the problem, let's dig into the SB algorithm, starting with our objective for our semidefinite program, which ensures that K is low rank and thus we get a low dimensional graph embedding.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So low rank K corresponds to low dimensional embedding.",
                    "label": 1
                },
                {
                    "sent": "That's because when we choose our eigenvectors for our embedding, the ones with non zero eigenvalues we keep.",
                    "label": 0
                },
                {
                    "sent": "So any ones we want as many eigenvectors with zero eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So we can drop them.",
                    "label": 0
                },
                {
                    "sent": "So here's the objective for the semidefinite program for structure preserving embedding we show in the paper in the poster that this objective function produces a low rank version.",
                    "label": 1
                },
                {
                    "sent": "Of spectral embedding.",
                    "label": 0
                },
                {
                    "sent": "So here we're trying to learn a matrix K, and there's some common constraints that K is positive semidefinite that K is bounded from growing, growing, and that K is centered.",
                    "label": 1
                },
                {
                    "sent": "So come see the poster tomorrow night if you want to more details about this objective function and how we guarantee it to produce a low rank version of spectral embedding.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the objective function of RSVP.",
                    "label": 0
                },
                {
                    "sent": "Now let's talk about the constraints on SDP.",
                    "label": 0
                },
                {
                    "sent": "Both are linear.",
                    "label": 0
                },
                {
                    "sent": "We start with this intuition that we want reversible graph embeddings.",
                    "label": 0
                },
                {
                    "sent": "We really want the matrix K to sort of inherit the global topological properties of the input adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "So we want to guarantee the connectivity algorithm G of K such as K nearest neighbors or be matching maximum weight spanning tree, etc should be able to recover the edges from the coordinates of the embedding such that the output of that connectivity algorithm is exactly the input adjacency matrix.",
                    "label": 1
                },
                {
                    "sent": "Giving some edges we can run embedding to get points, but the key intuition here is that we want now given points to be able to run a connectivity algorithm and get back the exact initial edges.",
                    "label": 0
                },
                {
                    "sent": "That would say that the points the embedding is topologically correct and it respects the original edges in the input adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "So for a variety of connectivity.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rythms, Kenyeres neighbors be matching maximum weight spanning tree epsilon, neighborhood graphs.",
                    "label": 0
                },
                {
                    "sent": "We can guarantee this property with linear constraints on K. Let's take a look at some examples.",
                    "label": 0
                },
                {
                    "sent": "So if the connectivity algorithm is K nearest neighbors where each point is connected to the K, other points which are the closest we have.",
                    "label": 0
                },
                {
                    "sent": "These linear constraints on K stating that the distance between unconnected nodes has to be greater than the largest connected distance.",
                    "label": 0
                },
                {
                    "sent": "Here distance is just a linear function of KR embedding.",
                    "label": 1
                },
                {
                    "sent": "Similarly for epsilon neighborhoods, each point is connected to all their points, which lie within a radius of epsilon.",
                    "label": 0
                },
                {
                    "sent": "This again corresponds to these simple linear constraints on K that guarantee that when we run this algorithm on our embedding, we get back exactly the edges we used to construct that embedding.",
                    "label": 0
                },
                {
                    "sent": "So what's going on here?",
                    "label": 0
                },
                {
                    "sent": "Is these constraints are preventing, for example, the blue nodes from invading the neighborhood of the red node and thus changing the adjacency matrix that would be formed from running the connectivity algorithm on the embedding.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the SP algorithm when the connectivity algorithm is K nearest neighbors or epsilon balls etc.",
                    "label": 0
                },
                {
                    "sent": "It's one SDP where we're maximizing the cost function that I showed you earlier with these simple linear constraints on K. Once we learn K, we apply an SVD decay and we use the top eigenvectors for the coordinates.",
                    "label": 0
                },
                {
                    "sent": "Let's talk about some other possibilities for the connectivity algorithm.",
                    "label": 0
                },
                {
                    "sent": "If it's be matching or maximum weight spanning tree, one of these maximum weight sub graph methods, can we come up with linear constraints on K that Similarly make this guarantee that we want so be matching tries to assign neighbors for each node be neighbors such that it maximizes some weight in a weight matrix?",
                    "label": 0
                },
                {
                    "sent": "Here we're going to find weights just as a linear function of K negated distances.",
                    "label": 0
                },
                {
                    "sent": "Maximum spanning tree similarly tries to maximize weight, but guarantees that the adjacency matrix formed forms a tree.",
                    "label": 0
                },
                {
                    "sent": "These algorithms have have sort of stricter constraints than K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "They're trying to enforce this global property that the adjacency matrix is a tree or has exactly this many neighbors, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the constraints on K take this form.",
                    "label": 1
                },
                {
                    "sent": "There's possibly an exponential number of these constraints stating that one configuration beats sort of all other configurations.",
                    "label": 0
                },
                {
                    "sent": "Here a~ is all other adjacency matrices belonging to some class like be matchings or trees.",
                    "label": 0
                },
                {
                    "sent": "But Fortunately for us, we can use a cutting plane technique to avoid enumeration, similar to the structured prediction SVM work.",
                    "label": 1
                },
                {
                    "sent": "So we iterate an SDP, adding the worst violating constraint of this form at each iteration.",
                    "label": 1
                },
                {
                    "sent": "Let's see what that looks.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, we're given as input an adjacency matrix connectivity algorithm, which is a maximum weight sub graph method such as be matching or maximum spanning tree.",
                    "label": 1
                },
                {
                    "sent": "Now we're going to 1st solve the SDP without any structure preserving constraints.",
                    "label": 1
                },
                {
                    "sent": "This is going to give us back the rank one spectral embedding solution, and then we're going to find the most violated constraint.",
                    "label": 1
                },
                {
                    "sent": "And we're going to add that to RSVP, and we're going to iterate and we're going to stop iterating when the weight of the worst violator comes close to the optimal weight.",
                    "label": 0
                },
                {
                    "sent": "Let's talk about how SP is implemented.",
                    "label": 0
                },
                {
                    "sent": "It's implemented in.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Matlab using CSDP&SDPLR.",
                    "label": 0
                },
                {
                    "sent": "It has complexity similar to a lot of other SCPS.",
                    "label": 1
                },
                {
                    "sent": "For dimensionality reduction, there are many inactive constraints, so working set methods yield good speedups, and in particular.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "SDP LR takes advantage of our low rank objective and that also offers a speedup.",
                    "label": 0
                },
                {
                    "sent": "We've run SP on graphs with up to 1000 nodes.",
                    "label": 0
                },
                {
                    "sent": "Now that I've talked about the details of the SP algorithm, let me show you some experimental results.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are some classical graphs we saw the Mobius ladder, and here's the Tesseract or the hypercube.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the Selman, Swartz Mark and the Balaban 10 cage.",
                    "label": 0
                },
                {
                    "sent": "As we see, spectral embedding will often find many possible dimensions to use for embedding.",
                    "label": 1
                },
                {
                    "sent": "That's what we can see from the eigen spectrum next to each graph.",
                    "label": 0
                },
                {
                    "sent": "Where SB on the other hand, gives us compact embeddings, only a few possible dimensions for visualization.",
                    "label": 0
                },
                {
                    "sent": "Here are some.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Organic molecules, the true physical 3D embedding, is shown on the left.",
                    "label": 0
                },
                {
                    "sent": "This is what the molecule actually looks like.",
                    "label": 0
                },
                {
                    "sent": "Given only information about which nodes connect to which others.",
                    "label": 0
                },
                {
                    "sent": "You know can we sort of come close to this real true 3D embedding in a 2D embedding that only has the connectivity information to work with?",
                    "label": 0
                },
                {
                    "sent": "Spectral embedding seems to clump together nodes in strange ways doesn't really seem to give us a good informative embedding that looks like the 3D embedding Laplacian eigen Maps will normalize applied Maps.",
                    "label": 0
                },
                {
                    "sent": "They don't behave as well either where structure preserving embedding seems to give a good embedding.",
                    "label": 1
                },
                {
                    "sent": "That's a good diagram of what these molecules actually look like.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is 981 political blogs and the link structure between all of them.",
                    "label": 1
                },
                {
                    "sent": "The red is the conservative, the blue is the liberal, and the reconstruction error is shown next to the eigen spectrums for each.",
                    "label": 0
                },
                {
                    "sent": "So this reconstruction error is the number of edges that.",
                    "label": 0
                },
                {
                    "sent": "That are different from the initial link structure.",
                    "label": 0
                },
                {
                    "sent": "If you were to run a connectivity algorithm in this case be matching on just the two D embedding, and so you can see that we have the lowest reconstruction error and our embedding is 2 dimensional.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Structure preserving embedding is similar to many of these manifold learning based methods for dimensionality reduction, such as locally linear embedding, maximum variance in folding, minimum volume embedding.",
                    "label": 1
                },
                {
                    "sent": "These methods preserve pairwise distances between data points.",
                    "label": 1
                },
                {
                    "sent": "However, adding topology preserving constraints to these existing algorithms yields more accurate embeddings.",
                    "label": 0
                },
                {
                    "sent": "Prevents prevents collapsing of the underlying manifold.",
                    "label": 0
                },
                {
                    "sent": "Let me give you an example.",
                    "label": 0
                },
                {
                    "sent": "So here are some original data running nvu or mve on this data set will collapse the diamond in the middle of the barbell where adding these topology preserving constraints will prevent the distance between these two points from shrinking too much to the point where it would change the underlying connectivity and therefore adding these topology preserving constraints more accurately.",
                    "label": 0
                },
                {
                    "sent": "Represents this data set.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the maximum variance unfolding objective.",
                    "label": 0
                },
                {
                    "sent": "Tries to unfold the data subject to constraints that preserve local distance is defined by a graph on the data.",
                    "label": 0
                },
                {
                    "sent": "Similarly, minimum volume bedding tries to maximize variance in the target D and target D dimensions.",
                    "label": 0
                },
                {
                    "sent": "For nearest neighbor graphs, all we have to do is add these linear constraints to this optimization and we can preserve topology as well as simply local distances.",
                    "label": 0
                },
                {
                    "sent": "For the maximum weight sub graph methods, we can iteratively add this constraint corresponding to the worst violator at each iteration.",
                    "label": 0
                },
                {
                    "sent": "And similarly, we can preserve topology.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's an example of what I mean.",
                    "label": 0
                },
                {
                    "sent": "We ran a one nearest neighbor classifier on a variety of UCI datasets.",
                    "label": 1
                },
                {
                    "sent": "They were all embedded with these dimensionality reduction techniques, and we're going to compare using.",
                    "label": 1
                },
                {
                    "sent": "Compare the accuracy of this classifier on the 2 dimensional embeddings versus using all the original features.",
                    "label": 0
                },
                {
                    "sent": "We see that MVP plus SP yields the most accurate low dimensional embedding across the board.",
                    "label": 0
                },
                {
                    "sent": "An in three cases op digits and honest, honest, here and and.",
                    "label": 0
                },
                {
                    "sent": "Wine, is it or no and E. Coli.",
                    "label": 0
                },
                {
                    "sent": "We actually get a higher accuracy than in the all dimensional case.",
                    "label": 0
                },
                {
                    "sent": "That means that using 2 features per data point, those two features that come back from Envy plus SP you get higher K nearest neighbor or one nearest neighbor accuracy then using all the original features for each data point.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So, So what can we take back in this?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can see that SB finds low dimensional representations of graphs that implicitly preserve topology.",
                    "label": 1
                },
                {
                    "sent": "And also that preserving local distances is insufficient for faithfully embedding graphs in low dimensional space.",
                    "label": 1
                },
                {
                    "sent": "We need to preserve graph topology as well.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}