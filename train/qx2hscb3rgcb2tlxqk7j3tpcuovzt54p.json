{
    "id": "qx2hscb3rgcb2tlxqk7j3tpcuovzt54p",
    "title": "Web Based Probabilistic Textual Entailment",
    "info": {
        "author": [
            "Oren Glickman, Bar-Ilan University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "April 2005",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/pcw05_glickman_wbpte/",
    "segmentation": [
        [
            "OK, yeah so.",
            "We did our own submission.",
            "To the challenge.",
            "Anne and.",
            "We tried to.",
            "Focus on.",
            "Developing probabilistic.",
            "OK. And developing a probabilistic framework.",
            "For entertainment and."
        ],
        [
            "To understand the probabilistic framework, let's start with the classical definition of what entailment is.",
            "If you read semantics, formal semantics books at least one.",
            "Way of defining entitlement is.",
            "A text implies an apotheosis if.",
            "In any world in which.",
            "The text is 2 isn't necessarily.",
            "That also hypothesis is true.",
            "OK, no matter what the circumstances or they use possible world.",
            "The world's and semantics.",
            "In the end, no matter what the.",
            "Context in which the text was stated.",
            "If it is true, then necessarily they just told then you have entailment.",
            "The problem."
        ],
        [
            "Examples, for example, one of the examples this following example from the.",
            "Dataset.",
            "Take a few seconds to read it and sort of process it in your mind and you tend to agree that.",
            "Definitely.",
            "There isn't a moment here, and it was annotated as two.",
            "But if you try to prove it logically, there's nothing here logical.",
            "That could.",
            "You could infer necessarily that age is true.",
            "We tend to agree that it's too, but it's not necessarily.",
            "I could think about circumstances in which the text would be true and age would not.",
            "Even though these circumstances are not probably so.",
            "How do we define probabilistic textual entailment?",
            "OK, and we need to define a probability space.",
            "Here are events, and this was actually a much more difficult task than what I."
        ],
        [
            "Expected there.",
            "And then.",
            "I will.",
            "It took us quite awhile to come up with something, so what's the rationale?",
            "Here is that in this case.",
            "T does not necessarily entail age, but it does add some substantial information about the correctness of age.",
            "We believe that if somebody stated T. You would only stay T in cases in which H is true.",
            "OK.",
            "Under the probability that somebody would state T. And H in.",
            "In that circumstance, in which H2 is either 0 or close to 0.",
            "Oh hey."
        ],
        [
            "Which means.",
            "It's for us, yeah.",
            "Anne.",
            "OK, so.",
            "What is our probability or probability probability space here?",
            "So.",
            "We need to see what our.",
            "Are variables about events or what others that probabilities above and the first thing we did is we separated the text from my pastor.",
            "Since we say texts are text OK and it's.",
            "An innumerable you can list all different types of text in the world, and it's a numerable set, OK, and hypothesis that something slightly different because hypothesis are only things that you can assign a tour file, so we're considering as I pass this only text which are grammatical sentential, and you can say either yes or no to them.",
            "OK and meaningless texts.",
            "Is not.",
            "It would not be useful as hypothesis.",
            "So my processors are actually textual propositions.",
            "And then.",
            "We call something as a possible world, which I'm not sure if it's the exact relation to formal semantics, but a possible world is a two assignment from hypothesis to tour funds.",
            "201 OK. And.",
            "Then we have.",
            "Big W, which is the set of any kind of mappings of tools assignments to.",
            "My processes.",
            "Anne."
        ],
        [
            "And then we came up with this following generative model.",
            "OK, we assume.",
            "That we have a language show source that produces text and when it produces a text, there is a hidden.",
            "Possible wall the hidden tools assignment so when the.",
            "When a source generates a text, we don't know what's true or not, but.",
            "It knows.",
            "For every single proposition, for everything, will textual.",
            "Statement if that statement is true or not within the context of the text.",
            "OK. And.",
            "We simply assume that overall the probability space of.",
            "T and that there is some kind of probability distribution over the tecsun possible worlds.",
            "And the source, you know taxes are coin according to this probability space and generates a document and ahead and possible which we do not observe.",
            "Anne."
        ],
        [
            "If we assume this kind of generative model, then we can talk about probabilities and then.",
            "We could say something, for example, what's the probability that 4?",
            "Assume hypothesis what's the prior probability know regardless of what was the text that was generated.",
            "What is the probability that it is assigned one?",
            "For example, if H is there, sentence Paris is the capital of funds.",
            "We can calculate.",
            "What is the probability that actually Paris is the capital of funds?",
            "It is true in the context of a text, regardless of text.",
            "It's not one because there's person Texas as well, right?",
            "So?",
            "Might be 0.9 zero point 95.",
            "I don't know.",
            "And but we can also look at the conditional probability given the some text.",
            "For example, I checked into.",
            "I flew into France and checked into the hotel in Paris.",
            "In that case, we believe that.",
            "If a source generated that XD.",
            "There will be a very high probability that it will assign a true value of 1 to the sentence.",
            "For example, the test is the capital of France, OK?",
            "And.",
            "So once we have these probabilities.",
            "Ah."
        ],
        [
            "So we define textual entailment for actually probabilistic textual entailment as follows.",
            "We say that a text entails and hypothesis.",
            "If.",
            "There.",
            "Conditional probability is greater than the pile.",
            "That means that.",
            "Knowing that the source that that do we have a son text can handle this or somebody generated the text.",
            "It increases our belief.",
            "That I positives is actually true.",
            "Overall, that prior that we believe that this hypothesis would be true in general, OK?",
            "If so, we say that.",
            "It increases probability and we have probabilistic entailment.",
            "OK, we can try to quantify it or look at the mutual information or something like that."
        ],
        [
            "So.",
            "This is a definition.",
            "Well now I'm going to go a few.",
            "100 miles down and try to see how we can model this abstract notion.",
            "And start from the.",
            "Scratch bottom of lexical models.",
            "And we define actually a sort of.",
            "Sub task of entailment or a lexical variant of entailment.",
            "In which we call lexical entailment.",
            "And we say that a text entails in my classes if every individual term in ages entailed from T. OK, but they don't necessary the different tones within dye passes don't necessarily hold the right relations needed to be entailed from the text.",
            "For example, if we have here the text in the process we have.",
            "Robert.",
            "More wild or whoever it is and we have the Queen of Highland and we have is but the text.",
            "Which is we would say it's literally lexically entailed from the text, meaning that every lexical item could be inferred from the text.",
            "But it's not as a whole, is not entailed from the text.",
            "And."
        ],
        [
            "Following Rhonda Ann.",
            "So we try to model this kind of lexical entailment.",
            "Based on current statistics from documents.",
            "And.",
            "In order to do so, we follow the following assumptions.",
            "First assumption is sort of an independent of truth assignments.",
            "We assume that this or that the two assignments was different lexical components of an hypothesis are independent of each other, so you can breakdown the age into different terms.",
            "Assuming sort of a bag of world model and simply multiply the different probabilities.",
            "And then.",
            "So.",
            "The problem is that both agenti I'll text and we want to break it down into smaller things that we can count probabilities on.",
            "We can condition and the whole thing, so we broke down the age.",
            "Now we need to breakdown the T1.",
            "Possible way to do it is to assume sort of an alignment that that most of the probability mass over here comes from a specific.",
            "Element within the text.",
            "Which is sort of an underlying alignment between the text and hypothesis, and then you take a maximum.",
            "We developed other models as well, but this is sort of this.",
            "It is simplest one.",
            "So.",
            "The probability here the basic underlying probability is what's the probability that at some tone.",
            "Is 2 given that?",
            "The test contains a different term.",
            "And.",
            "Once you have these two together, then."
        ],
        [
            "We have basically.",
            "Always that we need all we need to do is calculate these.",
            "Lexical probabilities.",
            "And so when we go down to.",
            "Developing a concrete model we.",
            "Assume that there are sources.",
            "The sources generated the web and.",
            "It generates it as sort of a distribution.",
            "Bility distribution over text that are available and web.",
            "And the problem is that we don't.",
            "That that also assignments are hidden and we don't see them, so we don't know.",
            "And how to count exactly?",
            "These kinds of truth probabilities, so we do another.",
            "Very cool to something that at home is 2 in a document only if and if and only if it appears in a document and then this other.",
            "Belt that balance down never doubted nails down to boils down to Cohen's probabilities.",
            "Watch the probability that assume that if a document has a son terminal, have another term OK. And we collect them from Assault Engine."
        ],
        [
            "Am OK, so our submission is as follows.",
            "We tokenize the words in the text and I processes will remove stop words an we collect the current statistics for every single pair of words from the text and hypothesis from wheel Delta Vista because they did not block us if we over exhaustively.",
            "Hit then an.",
            "And then.",
            "The problem was defined as a classification problem, so we needed to put some kind of threshold and give confidences.",
            "So we used this following.",
            "Conditional probability as our confidence.",
            "We tuned and the development set some kind of threshold and it's confidence, and if it's negative if it's below the confidence then we take.",
            "The probability that it's not total, which obviously they are not scaled together, but.",
            "And that's the most intensive, sensible insensible saying to do an and.",
            "Run all tests so."
        ],
        [
            "And these are the results, as you've seen in the first talk.",
            "And it actually.",
            "Achieved a very high accuracy.",
            "Relative to other submissions an.",
            "It did reasonably well in the confidence weighted score as well.",
            "Anne.",
            "But"
        ],
        [
            "You know you can look at numbers, but then when you try to look underneath the hood and save what it actually did.",
            "So this is just one example.",
            "They looked for each word here.",
            "What was the world that it actually was aligned or that had the highest conditional probability Co occurrence probability and you can see.",
            "Some of them are good, you know it gets dependent Japanese voter and vote.",
            "I didn't do any kind of stemming or morphological analysis or any any kind of thing like that and it finds these connections, but then it gets all kinds of strange things like turn out into half and percent into less, which.",
            "Kind of dubious.",
            "Threshold firewood, yeah, that's that's a good point.",
            "Then you know you always think about it after you run your submission and and looking into this.",
            "And then you say OK, Oh yeah, and this is what we should do.",
            "And actually, actually.",
            "What I did try to take is the analytical threshold and that is if it's higher than.",
            "In the prior, but because from Google the pyre for half is so loaded, any other world would raise the probability.",
            "So actually you would want some kind of.",
            "Threat or not just.",
            "Yeah, yeah, I mean.",
            "OK. No, yes it is the point and."
        ],
        [
            "OK, so.",
            "Fumarole sort of analysis of stuff.",
            "I tried to look at the precision recall curve and.",
            "I guess you can figure out what this means.",
            "This is that.",
            "And the local recoil meaning under high.",
            "Confidence.",
            "The precision is low.",
            "Once again, how you world overlap correlates to the negative examples.",
            "And but even though this sort of handicapped and the system overall manages to bend up to, you know, precision of.",
            "Close to 0.6.",
            "Anne."
        ],
        [
            "Another kind of.",
            "God, I tried to do is.",
            "I want to see if the probabilities help that are maybe you know running without probabilities and just taking.",
            "01 In case the world appeals or not.",
            "If a word appears in a world and everyone they pass it.",
            "If it appears in the text.",
            "Yeah, then it's one if not 0.",
            "Anne.",
            "And you can see that.",
            "If it does get better than the baseline, OK, so it does do better than not using probabilities or not using some kind of fuzzy.",
            "And mechanism of looking at words which do not appear in the hypothesis."
        ],
        [
            "OK, so.",
            "Let's conclude an.",
            "So we define a probabilistic setting.",
            "An which we hope would catch on and or develop forward that so that we can talk about the the probabilistic nature of this fuzzy entailment.",
            "We proposed.",
            "Definition for probabilistic or fuzzy in payment and.",
            "We also demonstrated a concrete model which uses word Co occurrence statistiques.",
            "And is based under proposed.",
            "A probabilistic setting.",
            "Am and.",
            "The system did not that bad after all, so.",
            "So maybe this is more like general question, but let's say having all these different approaches.",
            "These different approaches produce different errors on the data set, or more less than what you mean different approaches that different tasks.",
            "Approaches which we hear in different presentations OK.",
            "Different errors, so how much the error rates between the?",
            "In that space, which is obviously pretty different philosophically as well, structural lines before.",
            "I'm not sure.",
            "If the mistakes which assistant made were they on the same examples as, let's say, but not us.",
            "Let's do let me take questions about this talk and then I'll show you a slide I prepared last night because this is sort of a general.",
            "I tried to.",
            "I can't do any sort of experiment.",
            "And what do the systems agree upon or don't agree upon and doing a vote voting to see if system diffuse voting?",
            "Overall systems, how would it do?",
            "And so on.",
            "Within each task, force cases are 15.",
            "Yes it is.",
            "Comment, I guess your approach would be better if you would take into account also some context not so you are now comparing asking other reason this case.",
            "Char.",
            "Even with that, but since you know some of the Contacts from the address from the, that's so if you limit this search.",
            "Yeah.",
            "For example, many antonyms.",
            "Oh yeah.",
            "Distribution with similarity and has the same quark if not walls.",
            "Tend to be more similar than than Corcorans.",
            "OK, winter and summer don't necessarily core core, but they are definitely similar in most.",
            "And.",
            "I mean, there are many ways of maybe trying to improve, but we wanted to social service distribution with similarity is not probabilities and I wanted to so that you know, you can take us on probability and put it into this framework.",
            "And if I wanted to, you know, focus on trying to tweak the best system.",
            "Then you know you can use any kind of weights and then tune them and so on.",
            "Ronnie what happened to the possible words on the way?",
            "They vanished.",
            "Anne.",
            "Reasoning about possible words.",
            "And when the world passable walls is a scary, but it's not as scary as it actually, if you think.",
            "What it is OK and now if you would want to try to actually model.",
            "Look at the different possible worlds.",
            "It might be complicated, but if you think of it as just interpretations.",
            "Anne and.",
            "And so ignore have complete the set of all different kinds of different interpretation.",
            "But just think about it as in a given text.",
            "Some things could be true, could be assigned tools that are not do not appear there and then try to model.",
            "This specifically.",
            "And we did some work time to use EM and these kinds of stuff to model specifically these phenomena.",
            "So so.",
            "They didn't.",
            "They don't exactly vanish.",
            "They could that possible worlds OK?",
            "What is the age of?",
            "And the sentence doesn't.",
            "The sentence meaning.",
            "Only one world.",
            "Yeah, you mean it doesn't hold in.",
            "Yeah.",
            "You mean, for example, that I pass?",
            "Is the person in the capital of France, but yet it's not entailed from the text.",
            "I.",
            "Only checking.",
            "Because the way we.",
            "So nothing always holds.",
            "Dealing with.",
            "Applications are you using question, answering notes, notes from some knowledge base, but from texts what you're looking for is can I find a test from which I can infer a track you kind of playing again H. You know might be true or false in some real world, but when I impose ages as an H in a texture Internet setting, I'm saying suppose I don't know whether H is true or not.",
            "H might be true, but may also be false.",
            "I don't know.",
            "I don't have this knowledge now I get a text and I want to know.",
            "The truth of age from this particular test, so that would be in it in a.",
            "The conditional probability of having.",
            "Yes.",
            "What I'm saying is it is cold.",
            "Both.",
            "Assuming that age itself.",
            "Maybe?",
            "H. Is always lower than one.",
            "Assume that I mean not the probability of the prior is apparent over possible circumstances, right?",
            "The distribution is set over whenever effects to generate is generated.",
            "A possible worlds or sequence.",
            "Is generated.",
            "Assume that there might be some event of the generation of texts with which age mate maybe false.",
            "Let's say assumption of age of not knowing whether it is true or not.",
            "So you always assume that age could have been forced.",
            "Now if synced increases this probability.",
            "Suppose maybe ages prior would be 1 minus epsilon OK, but then given T if T increases problems 1 minus half an epsilon then you would say that if he doesn't change, is your probability relative to the prior.",
            "I would say no.",
            "OK, that's a setting actually doing something which we believe fits the whole idea of trying to infer something from text, because some people do question answering.",
            "You notify just putting the answer from some database, but that's not really the text.",
            "The inference from text came right in infants with text game is that you must get from the text subdivision of information about the truth.",
            "Properties.",
            "It's not clear what the definition of being false is well.",
            "Language.",
            "General.",
            "George.",
            "The question is.",
            "For example, because there would be many, many documents says that is forcing your case.",
            "So in general I think that you really have to be careful about the kind of tax exam.",
            "For example, how many negations you have in your in your in your domain.",
            "Most of your methods so not know your your specific and general very simple methods would work very well.",
            "If you don't negations.",
            "For example, get lots of negations, they don't work.",
            "Because if you just.",
            "Frequency for example.",
            "So in general it's a very general question.",
            "Distinction between the distichs setting the first part, or install versus a concrete model.",
            "So in the probabilistic setting, we do require that the text would increase the probability of the text would involve a negation.",
            "If let's you decrease the problem, therefore did not intend to test my contained indication of affecting some extension of this clinician.",
            "OK, so a good model may still take all the negations with account.",
            "Setting is does the texted some information that increases the likelihood of age to be true?",
            "That's that's where we, you know we put.",
            "Even though it's simple.",
            "Actually, that's a very delicate point.",
            "And that's where we want to put this point and applying statistical empty, you know we want to know what this translation is, a source which we believe that this is seems to capture the public setting that people intend when they're making inferences based on text.",
            "Like maybe we're wrong.",
            "And then.",
            "Kind of studies.",
            "Want to have available.",
            "Which erases or stock quotes.",
            "Actually, maybe some of this is actually reasons for quitting.",
            "Give some teaser for the next exactly.",
            "Discussing all these issues you know is open up this early with.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, yeah so.",
                    "label": 0
                },
                {
                    "sent": "We did our own submission.",
                    "label": 0
                },
                {
                    "sent": "To the challenge.",
                    "label": 0
                },
                {
                    "sent": "Anne and.",
                    "label": 0
                },
                {
                    "sent": "We tried to.",
                    "label": 0
                },
                {
                    "sent": "Focus on.",
                    "label": 0
                },
                {
                    "sent": "Developing probabilistic.",
                    "label": 0
                },
                {
                    "sent": "OK. And developing a probabilistic framework.",
                    "label": 0
                },
                {
                    "sent": "For entertainment and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To understand the probabilistic framework, let's start with the classical definition of what entailment is.",
                    "label": 0
                },
                {
                    "sent": "If you read semantics, formal semantics books at least one.",
                    "label": 0
                },
                {
                    "sent": "Way of defining entitlement is.",
                    "label": 0
                },
                {
                    "sent": "A text implies an apotheosis if.",
                    "label": 1
                },
                {
                    "sent": "In any world in which.",
                    "label": 1
                },
                {
                    "sent": "The text is 2 isn't necessarily.",
                    "label": 1
                },
                {
                    "sent": "That also hypothesis is true.",
                    "label": 0
                },
                {
                    "sent": "OK, no matter what the circumstances or they use possible world.",
                    "label": 0
                },
                {
                    "sent": "The world's and semantics.",
                    "label": 0
                },
                {
                    "sent": "In the end, no matter what the.",
                    "label": 0
                },
                {
                    "sent": "Context in which the text was stated.",
                    "label": 0
                },
                {
                    "sent": "If it is true, then necessarily they just told then you have entailment.",
                    "label": 0
                },
                {
                    "sent": "The problem.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples, for example, one of the examples this following example from the.",
                    "label": 0
                },
                {
                    "sent": "Dataset.",
                    "label": 0
                },
                {
                    "sent": "Take a few seconds to read it and sort of process it in your mind and you tend to agree that.",
                    "label": 0
                },
                {
                    "sent": "Definitely.",
                    "label": 0
                },
                {
                    "sent": "There isn't a moment here, and it was annotated as two.",
                    "label": 0
                },
                {
                    "sent": "But if you try to prove it logically, there's nothing here logical.",
                    "label": 0
                },
                {
                    "sent": "That could.",
                    "label": 0
                },
                {
                    "sent": "You could infer necessarily that age is true.",
                    "label": 0
                },
                {
                    "sent": "We tend to agree that it's too, but it's not necessarily.",
                    "label": 0
                },
                {
                    "sent": "I could think about circumstances in which the text would be true and age would not.",
                    "label": 0
                },
                {
                    "sent": "Even though these circumstances are not probably so.",
                    "label": 0
                },
                {
                    "sent": "How do we define probabilistic textual entailment?",
                    "label": 0
                },
                {
                    "sent": "OK, and we need to define a probability space.",
                    "label": 0
                },
                {
                    "sent": "Here are events, and this was actually a much more difficult task than what I.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Expected there.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "I will.",
                    "label": 0
                },
                {
                    "sent": "It took us quite awhile to come up with something, so what's the rationale?",
                    "label": 0
                },
                {
                    "sent": "Here is that in this case.",
                    "label": 0
                },
                {
                    "sent": "T does not necessarily entail age, but it does add some substantial information about the correctness of age.",
                    "label": 1
                },
                {
                    "sent": "We believe that if somebody stated T. You would only stay T in cases in which H is true.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Under the probability that somebody would state T. And H in.",
                    "label": 0
                },
                {
                    "sent": "In that circumstance, in which H2 is either 0 or close to 0.",
                    "label": 0
                },
                {
                    "sent": "Oh hey.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which means.",
                    "label": 0
                },
                {
                    "sent": "It's for us, yeah.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What is our probability or probability probability space here?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We need to see what our.",
                    "label": 0
                },
                {
                    "sent": "Are variables about events or what others that probabilities above and the first thing we did is we separated the text from my pastor.",
                    "label": 0
                },
                {
                    "sent": "Since we say texts are text OK and it's.",
                    "label": 0
                },
                {
                    "sent": "An innumerable you can list all different types of text in the world, and it's a numerable set, OK, and hypothesis that something slightly different because hypothesis are only things that you can assign a tour file, so we're considering as I pass this only text which are grammatical sentential, and you can say either yes or no to them.",
                    "label": 0
                },
                {
                    "sent": "OK and meaningless texts.",
                    "label": 0
                },
                {
                    "sent": "Is not.",
                    "label": 0
                },
                {
                    "sent": "It would not be useful as hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So my processors are actually textual propositions.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "We call something as a possible world, which I'm not sure if it's the exact relation to formal semantics, but a possible world is a two assignment from hypothesis to tour funds.",
                    "label": 1
                },
                {
                    "sent": "201 OK. And.",
                    "label": 0
                },
                {
                    "sent": "Then we have.",
                    "label": 0
                },
                {
                    "sent": "Big W, which is the set of any kind of mappings of tools assignments to.",
                    "label": 1
                },
                {
                    "sent": "My processes.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we came up with this following generative model.",
                    "label": 0
                },
                {
                    "sent": "OK, we assume.",
                    "label": 0
                },
                {
                    "sent": "That we have a language show source that produces text and when it produces a text, there is a hidden.",
                    "label": 1
                },
                {
                    "sent": "Possible wall the hidden tools assignment so when the.",
                    "label": 0
                },
                {
                    "sent": "When a source generates a text, we don't know what's true or not, but.",
                    "label": 0
                },
                {
                    "sent": "It knows.",
                    "label": 0
                },
                {
                    "sent": "For every single proposition, for everything, will textual.",
                    "label": 0
                },
                {
                    "sent": "Statement if that statement is true or not within the context of the text.",
                    "label": 0
                },
                {
                    "sent": "OK. And.",
                    "label": 0
                },
                {
                    "sent": "We simply assume that overall the probability space of.",
                    "label": 0
                },
                {
                    "sent": "T and that there is some kind of probability distribution over the tecsun possible worlds.",
                    "label": 1
                },
                {
                    "sent": "And the source, you know taxes are coin according to this probability space and generates a document and ahead and possible which we do not observe.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we assume this kind of generative model, then we can talk about probabilities and then.",
                    "label": 0
                },
                {
                    "sent": "We could say something, for example, what's the probability that 4?",
                    "label": 0
                },
                {
                    "sent": "Assume hypothesis what's the prior probability know regardless of what was the text that was generated.",
                    "label": 0
                },
                {
                    "sent": "What is the probability that it is assigned one?",
                    "label": 1
                },
                {
                    "sent": "For example, if H is there, sentence Paris is the capital of funds.",
                    "label": 0
                },
                {
                    "sent": "We can calculate.",
                    "label": 0
                },
                {
                    "sent": "What is the probability that actually Paris is the capital of funds?",
                    "label": 0
                },
                {
                    "sent": "It is true in the context of a text, regardless of text.",
                    "label": 0
                },
                {
                    "sent": "It's not one because there's person Texas as well, right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "Might be 0.9 zero point 95.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "And but we can also look at the conditional probability given the some text.",
                    "label": 0
                },
                {
                    "sent": "For example, I checked into.",
                    "label": 0
                },
                {
                    "sent": "I flew into France and checked into the hotel in Paris.",
                    "label": 0
                },
                {
                    "sent": "In that case, we believe that.",
                    "label": 0
                },
                {
                    "sent": "If a source generated that XD.",
                    "label": 0
                },
                {
                    "sent": "There will be a very high probability that it will assign a true value of 1 to the sentence.",
                    "label": 1
                },
                {
                    "sent": "For example, the test is the capital of France, OK?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So once we have these probabilities.",
                    "label": 0
                },
                {
                    "sent": "Ah.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we define textual entailment for actually probabilistic textual entailment as follows.",
                    "label": 1
                },
                {
                    "sent": "We say that a text entails and hypothesis.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "Conditional probability is greater than the pile.",
                    "label": 0
                },
                {
                    "sent": "That means that.",
                    "label": 0
                },
                {
                    "sent": "Knowing that the source that that do we have a son text can handle this or somebody generated the text.",
                    "label": 0
                },
                {
                    "sent": "It increases our belief.",
                    "label": 0
                },
                {
                    "sent": "That I positives is actually true.",
                    "label": 0
                },
                {
                    "sent": "Overall, that prior that we believe that this hypothesis would be true in general, OK?",
                    "label": 0
                },
                {
                    "sent": "If so, we say that.",
                    "label": 0
                },
                {
                    "sent": "It increases probability and we have probabilistic entailment.",
                    "label": 0
                },
                {
                    "sent": "OK, we can try to quantify it or look at the mutual information or something like that.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is a definition.",
                    "label": 0
                },
                {
                    "sent": "Well now I'm going to go a few.",
                    "label": 0
                },
                {
                    "sent": "100 miles down and try to see how we can model this abstract notion.",
                    "label": 0
                },
                {
                    "sent": "And start from the.",
                    "label": 0
                },
                {
                    "sent": "Scratch bottom of lexical models.",
                    "label": 0
                },
                {
                    "sent": "And we define actually a sort of.",
                    "label": 0
                },
                {
                    "sent": "Sub task of entailment or a lexical variant of entailment.",
                    "label": 0
                },
                {
                    "sent": "In which we call lexical entailment.",
                    "label": 0
                },
                {
                    "sent": "And we say that a text entails in my classes if every individual term in ages entailed from T. OK, but they don't necessary the different tones within dye passes don't necessarily hold the right relations needed to be entailed from the text.",
                    "label": 1
                },
                {
                    "sent": "For example, if we have here the text in the process we have.",
                    "label": 0
                },
                {
                    "sent": "Robert.",
                    "label": 0
                },
                {
                    "sent": "More wild or whoever it is and we have the Queen of Highland and we have is but the text.",
                    "label": 0
                },
                {
                    "sent": "Which is we would say it's literally lexically entailed from the text, meaning that every lexical item could be inferred from the text.",
                    "label": 0
                },
                {
                    "sent": "But it's not as a whole, is not entailed from the text.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Following Rhonda Ann.",
                    "label": 0
                },
                {
                    "sent": "So we try to model this kind of lexical entailment.",
                    "label": 0
                },
                {
                    "sent": "Based on current statistics from documents.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In order to do so, we follow the following assumptions.",
                    "label": 0
                },
                {
                    "sent": "First assumption is sort of an independent of truth assignments.",
                    "label": 0
                },
                {
                    "sent": "We assume that this or that the two assignments was different lexical components of an hypothesis are independent of each other, so you can breakdown the age into different terms.",
                    "label": 0
                },
                {
                    "sent": "Assuming sort of a bag of world model and simply multiply the different probabilities.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The problem is that both agenti I'll text and we want to break it down into smaller things that we can count probabilities on.",
                    "label": 0
                },
                {
                    "sent": "We can condition and the whole thing, so we broke down the age.",
                    "label": 0
                },
                {
                    "sent": "Now we need to breakdown the T1.",
                    "label": 0
                },
                {
                    "sent": "Possible way to do it is to assume sort of an alignment that that most of the probability mass over here comes from a specific.",
                    "label": 0
                },
                {
                    "sent": "Element within the text.",
                    "label": 0
                },
                {
                    "sent": "Which is sort of an underlying alignment between the text and hypothesis, and then you take a maximum.",
                    "label": 0
                },
                {
                    "sent": "We developed other models as well, but this is sort of this.",
                    "label": 0
                },
                {
                    "sent": "It is simplest one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The probability here the basic underlying probability is what's the probability that at some tone.",
                    "label": 0
                },
                {
                    "sent": "Is 2 given that?",
                    "label": 0
                },
                {
                    "sent": "The test contains a different term.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Once you have these two together, then.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have basically.",
                    "label": 0
                },
                {
                    "sent": "Always that we need all we need to do is calculate these.",
                    "label": 0
                },
                {
                    "sent": "Lexical probabilities.",
                    "label": 0
                },
                {
                    "sent": "And so when we go down to.",
                    "label": 0
                },
                {
                    "sent": "Developing a concrete model we.",
                    "label": 0
                },
                {
                    "sent": "Assume that there are sources.",
                    "label": 0
                },
                {
                    "sent": "The sources generated the web and.",
                    "label": 1
                },
                {
                    "sent": "It generates it as sort of a distribution.",
                    "label": 0
                },
                {
                    "sent": "Bility distribution over text that are available and web.",
                    "label": 0
                },
                {
                    "sent": "And the problem is that we don't.",
                    "label": 0
                },
                {
                    "sent": "That that also assignments are hidden and we don't see them, so we don't know.",
                    "label": 0
                },
                {
                    "sent": "And how to count exactly?",
                    "label": 0
                },
                {
                    "sent": "These kinds of truth probabilities, so we do another.",
                    "label": 1
                },
                {
                    "sent": "Very cool to something that at home is 2 in a document only if and if and only if it appears in a document and then this other.",
                    "label": 0
                },
                {
                    "sent": "Belt that balance down never doubted nails down to boils down to Cohen's probabilities.",
                    "label": 0
                },
                {
                    "sent": "Watch the probability that assume that if a document has a son terminal, have another term OK. And we collect them from Assault Engine.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Am OK, so our submission is as follows.",
                    "label": 0
                },
                {
                    "sent": "We tokenize the words in the text and I processes will remove stop words an we collect the current statistics for every single pair of words from the text and hypothesis from wheel Delta Vista because they did not block us if we over exhaustively.",
                    "label": 1
                },
                {
                    "sent": "Hit then an.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "The problem was defined as a classification problem, so we needed to put some kind of threshold and give confidences.",
                    "label": 0
                },
                {
                    "sent": "So we used this following.",
                    "label": 0
                },
                {
                    "sent": "Conditional probability as our confidence.",
                    "label": 0
                },
                {
                    "sent": "We tuned and the development set some kind of threshold and it's confidence, and if it's negative if it's below the confidence then we take.",
                    "label": 0
                },
                {
                    "sent": "The probability that it's not total, which obviously they are not scaled together, but.",
                    "label": 0
                },
                {
                    "sent": "And that's the most intensive, sensible insensible saying to do an and.",
                    "label": 0
                },
                {
                    "sent": "Run all tests so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are the results, as you've seen in the first talk.",
                    "label": 0
                },
                {
                    "sent": "And it actually.",
                    "label": 0
                },
                {
                    "sent": "Achieved a very high accuracy.",
                    "label": 0
                },
                {
                    "sent": "Relative to other submissions an.",
                    "label": 0
                },
                {
                    "sent": "It did reasonably well in the confidence weighted score as well.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know you can look at numbers, but then when you try to look underneath the hood and save what it actually did.",
                    "label": 0
                },
                {
                    "sent": "So this is just one example.",
                    "label": 0
                },
                {
                    "sent": "They looked for each word here.",
                    "label": 0
                },
                {
                    "sent": "What was the world that it actually was aligned or that had the highest conditional probability Co occurrence probability and you can see.",
                    "label": 0
                },
                {
                    "sent": "Some of them are good, you know it gets dependent Japanese voter and vote.",
                    "label": 1
                },
                {
                    "sent": "I didn't do any kind of stemming or morphological analysis or any any kind of thing like that and it finds these connections, but then it gets all kinds of strange things like turn out into half and percent into less, which.",
                    "label": 0
                },
                {
                    "sent": "Kind of dubious.",
                    "label": 0
                },
                {
                    "sent": "Threshold firewood, yeah, that's that's a good point.",
                    "label": 0
                },
                {
                    "sent": "Then you know you always think about it after you run your submission and and looking into this.",
                    "label": 0
                },
                {
                    "sent": "And then you say OK, Oh yeah, and this is what we should do.",
                    "label": 0
                },
                {
                    "sent": "And actually, actually.",
                    "label": 0
                },
                {
                    "sent": "What I did try to take is the analytical threshold and that is if it's higher than.",
                    "label": 0
                },
                {
                    "sent": "In the prior, but because from Google the pyre for half is so loaded, any other world would raise the probability.",
                    "label": 0
                },
                {
                    "sent": "So actually you would want some kind of.",
                    "label": 0
                },
                {
                    "sent": "Threat or not just.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, I mean.",
                    "label": 0
                },
                {
                    "sent": "OK. No, yes it is the point and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Fumarole sort of analysis of stuff.",
                    "label": 0
                },
                {
                    "sent": "I tried to look at the precision recall curve and.",
                    "label": 0
                },
                {
                    "sent": "I guess you can figure out what this means.",
                    "label": 0
                },
                {
                    "sent": "This is that.",
                    "label": 0
                },
                {
                    "sent": "And the local recoil meaning under high.",
                    "label": 0
                },
                {
                    "sent": "Confidence.",
                    "label": 0
                },
                {
                    "sent": "The precision is low.",
                    "label": 0
                },
                {
                    "sent": "Once again, how you world overlap correlates to the negative examples.",
                    "label": 0
                },
                {
                    "sent": "And but even though this sort of handicapped and the system overall manages to bend up to, you know, precision of.",
                    "label": 0
                },
                {
                    "sent": "Close to 0.6.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another kind of.",
                    "label": 0
                },
                {
                    "sent": "God, I tried to do is.",
                    "label": 0
                },
                {
                    "sent": "I want to see if the probabilities help that are maybe you know running without probabilities and just taking.",
                    "label": 0
                },
                {
                    "sent": "01 In case the world appeals or not.",
                    "label": 0
                },
                {
                    "sent": "If a word appears in a world and everyone they pass it.",
                    "label": 0
                },
                {
                    "sent": "If it appears in the text.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then it's one if not 0.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And you can see that.",
                    "label": 0
                },
                {
                    "sent": "If it does get better than the baseline, OK, so it does do better than not using probabilities or not using some kind of fuzzy.",
                    "label": 0
                },
                {
                    "sent": "And mechanism of looking at words which do not appear in the hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Let's conclude an.",
                    "label": 0
                },
                {
                    "sent": "So we define a probabilistic setting.",
                    "label": 0
                },
                {
                    "sent": "An which we hope would catch on and or develop forward that so that we can talk about the the probabilistic nature of this fuzzy entailment.",
                    "label": 0
                },
                {
                    "sent": "We proposed.",
                    "label": 0
                },
                {
                    "sent": "Definition for probabilistic or fuzzy in payment and.",
                    "label": 0
                },
                {
                    "sent": "We also demonstrated a concrete model which uses word Co occurrence statistiques.",
                    "label": 0
                },
                {
                    "sent": "And is based under proposed.",
                    "label": 0
                },
                {
                    "sent": "A probabilistic setting.",
                    "label": 0
                },
                {
                    "sent": "Am and.",
                    "label": 0
                },
                {
                    "sent": "The system did not that bad after all, so.",
                    "label": 0
                },
                {
                    "sent": "So maybe this is more like general question, but let's say having all these different approaches.",
                    "label": 0
                },
                {
                    "sent": "These different approaches produce different errors on the data set, or more less than what you mean different approaches that different tasks.",
                    "label": 0
                },
                {
                    "sent": "Approaches which we hear in different presentations OK.",
                    "label": 0
                },
                {
                    "sent": "Different errors, so how much the error rates between the?",
                    "label": 0
                },
                {
                    "sent": "In that space, which is obviously pretty different philosophically as well, structural lines before.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "If the mistakes which assistant made were they on the same examples as, let's say, but not us.",
                    "label": 0
                },
                {
                    "sent": "Let's do let me take questions about this talk and then I'll show you a slide I prepared last night because this is sort of a general.",
                    "label": 0
                },
                {
                    "sent": "I tried to.",
                    "label": 0
                },
                {
                    "sent": "I can't do any sort of experiment.",
                    "label": 0
                },
                {
                    "sent": "And what do the systems agree upon or don't agree upon and doing a vote voting to see if system diffuse voting?",
                    "label": 0
                },
                {
                    "sent": "Overall systems, how would it do?",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "Within each task, force cases are 15.",
                    "label": 0
                },
                {
                    "sent": "Yes it is.",
                    "label": 0
                },
                {
                    "sent": "Comment, I guess your approach would be better if you would take into account also some context not so you are now comparing asking other reason this case.",
                    "label": 0
                },
                {
                    "sent": "Char.",
                    "label": 0
                },
                {
                    "sent": "Even with that, but since you know some of the Contacts from the address from the, that's so if you limit this search.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "For example, many antonyms.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "Distribution with similarity and has the same quark if not walls.",
                    "label": 0
                },
                {
                    "sent": "Tend to be more similar than than Corcorans.",
                    "label": 0
                },
                {
                    "sent": "OK, winter and summer don't necessarily core core, but they are definitely similar in most.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I mean, there are many ways of maybe trying to improve, but we wanted to social service distribution with similarity is not probabilities and I wanted to so that you know, you can take us on probability and put it into this framework.",
                    "label": 0
                },
                {
                    "sent": "And if I wanted to, you know, focus on trying to tweak the best system.",
                    "label": 0
                },
                {
                    "sent": "Then you know you can use any kind of weights and then tune them and so on.",
                    "label": 0
                },
                {
                    "sent": "Ronnie what happened to the possible words on the way?",
                    "label": 0
                },
                {
                    "sent": "They vanished.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Reasoning about possible words.",
                    "label": 0
                },
                {
                    "sent": "And when the world passable walls is a scary, but it's not as scary as it actually, if you think.",
                    "label": 0
                },
                {
                    "sent": "What it is OK and now if you would want to try to actually model.",
                    "label": 0
                },
                {
                    "sent": "Look at the different possible worlds.",
                    "label": 0
                },
                {
                    "sent": "It might be complicated, but if you think of it as just interpretations.",
                    "label": 0
                },
                {
                    "sent": "Anne and.",
                    "label": 0
                },
                {
                    "sent": "And so ignore have complete the set of all different kinds of different interpretation.",
                    "label": 0
                },
                {
                    "sent": "But just think about it as in a given text.",
                    "label": 0
                },
                {
                    "sent": "Some things could be true, could be assigned tools that are not do not appear there and then try to model.",
                    "label": 0
                },
                {
                    "sent": "This specifically.",
                    "label": 0
                },
                {
                    "sent": "And we did some work time to use EM and these kinds of stuff to model specifically these phenomena.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "They didn't.",
                    "label": 0
                },
                {
                    "sent": "They don't exactly vanish.",
                    "label": 0
                },
                {
                    "sent": "They could that possible worlds OK?",
                    "label": 0
                },
                {
                    "sent": "What is the age of?",
                    "label": 0
                },
                {
                    "sent": "And the sentence doesn't.",
                    "label": 0
                },
                {
                    "sent": "The sentence meaning.",
                    "label": 0
                },
                {
                    "sent": "Only one world.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you mean it doesn't hold in.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You mean, for example, that I pass?",
                    "label": 0
                },
                {
                    "sent": "Is the person in the capital of France, but yet it's not entailed from the text.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Only checking.",
                    "label": 0
                },
                {
                    "sent": "Because the way we.",
                    "label": 0
                },
                {
                    "sent": "So nothing always holds.",
                    "label": 0
                },
                {
                    "sent": "Dealing with.",
                    "label": 0
                },
                {
                    "sent": "Applications are you using question, answering notes, notes from some knowledge base, but from texts what you're looking for is can I find a test from which I can infer a track you kind of playing again H. You know might be true or false in some real world, but when I impose ages as an H in a texture Internet setting, I'm saying suppose I don't know whether H is true or not.",
                    "label": 0
                },
                {
                    "sent": "H might be true, but may also be false.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't have this knowledge now I get a text and I want to know.",
                    "label": 0
                },
                {
                    "sent": "The truth of age from this particular test, so that would be in it in a.",
                    "label": 0
                },
                {
                    "sent": "The conditional probability of having.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "What I'm saying is it is cold.",
                    "label": 0
                },
                {
                    "sent": "Both.",
                    "label": 0
                },
                {
                    "sent": "Assuming that age itself.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "H. Is always lower than one.",
                    "label": 0
                },
                {
                    "sent": "Assume that I mean not the probability of the prior is apparent over possible circumstances, right?",
                    "label": 0
                },
                {
                    "sent": "The distribution is set over whenever effects to generate is generated.",
                    "label": 0
                },
                {
                    "sent": "A possible worlds or sequence.",
                    "label": 1
                },
                {
                    "sent": "Is generated.",
                    "label": 0
                },
                {
                    "sent": "Assume that there might be some event of the generation of texts with which age mate maybe false.",
                    "label": 0
                },
                {
                    "sent": "Let's say assumption of age of not knowing whether it is true or not.",
                    "label": 0
                },
                {
                    "sent": "So you always assume that age could have been forced.",
                    "label": 0
                },
                {
                    "sent": "Now if synced increases this probability.",
                    "label": 0
                },
                {
                    "sent": "Suppose maybe ages prior would be 1 minus epsilon OK, but then given T if T increases problems 1 minus half an epsilon then you would say that if he doesn't change, is your probability relative to the prior.",
                    "label": 0
                },
                {
                    "sent": "I would say no.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a setting actually doing something which we believe fits the whole idea of trying to infer something from text, because some people do question answering.",
                    "label": 0
                },
                {
                    "sent": "You notify just putting the answer from some database, but that's not really the text.",
                    "label": 0
                },
                {
                    "sent": "The inference from text came right in infants with text game is that you must get from the text subdivision of information about the truth.",
                    "label": 0
                },
                {
                    "sent": "Properties.",
                    "label": 0
                },
                {
                    "sent": "It's not clear what the definition of being false is well.",
                    "label": 0
                },
                {
                    "sent": "Language.",
                    "label": 0
                },
                {
                    "sent": "General.",
                    "label": 0
                },
                {
                    "sent": "George.",
                    "label": 0
                },
                {
                    "sent": "The question is.",
                    "label": 0
                },
                {
                    "sent": "For example, because there would be many, many documents says that is forcing your case.",
                    "label": 0
                },
                {
                    "sent": "So in general I think that you really have to be careful about the kind of tax exam.",
                    "label": 0
                },
                {
                    "sent": "For example, how many negations you have in your in your in your domain.",
                    "label": 0
                },
                {
                    "sent": "Most of your methods so not know your your specific and general very simple methods would work very well.",
                    "label": 0
                },
                {
                    "sent": "If you don't negations.",
                    "label": 0
                },
                {
                    "sent": "For example, get lots of negations, they don't work.",
                    "label": 0
                },
                {
                    "sent": "Because if you just.",
                    "label": 0
                },
                {
                    "sent": "Frequency for example.",
                    "label": 0
                },
                {
                    "sent": "So in general it's a very general question.",
                    "label": 0
                },
                {
                    "sent": "Distinction between the distichs setting the first part, or install versus a concrete model.",
                    "label": 1
                },
                {
                    "sent": "So in the probabilistic setting, we do require that the text would increase the probability of the text would involve a negation.",
                    "label": 0
                },
                {
                    "sent": "If let's you decrease the problem, therefore did not intend to test my contained indication of affecting some extension of this clinician.",
                    "label": 0
                },
                {
                    "sent": "OK, so a good model may still take all the negations with account.",
                    "label": 1
                },
                {
                    "sent": "Setting is does the texted some information that increases the likelihood of age to be true?",
                    "label": 0
                },
                {
                    "sent": "That's that's where we, you know we put.",
                    "label": 0
                },
                {
                    "sent": "Even though it's simple.",
                    "label": 0
                },
                {
                    "sent": "Actually, that's a very delicate point.",
                    "label": 0
                },
                {
                    "sent": "And that's where we want to put this point and applying statistical empty, you know we want to know what this translation is, a source which we believe that this is seems to capture the public setting that people intend when they're making inferences based on text.",
                    "label": 0
                },
                {
                    "sent": "Like maybe we're wrong.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Kind of studies.",
                    "label": 0
                },
                {
                    "sent": "Want to have available.",
                    "label": 0
                },
                {
                    "sent": "Which erases or stock quotes.",
                    "label": 0
                },
                {
                    "sent": "Actually, maybe some of this is actually reasons for quitting.",
                    "label": 0
                },
                {
                    "sent": "Give some teaser for the next exactly.",
                    "label": 0
                },
                {
                    "sent": "Discussing all these issues you know is open up this early with.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}