{
    "id": "kir4ssu7n67yfmcyztqvr5zvbhkztogv",
    "title": "Deep-er Kernels",
    "info": {
        "author": [
            "John Shawe-Taylor, Centre for Computational Statistics and Machine Learning, University College London"
        ],
        "published": "Aug. 26, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Compressed Sensing",
            "Top->Computer Science->Machine Learning->Regularization"
        ]
    },
    "url": "http://videolectures.net/roks2013_shawe_taylor_kernels/",
    "segmentation": [
        [
            "So just some background to the title."
        ],
        [
            "So deep learning has, as I think we've all seen, emerged.",
            "Or rather re emerged as a sort of very important research topic with considerable commercial value.",
            "You know, with the company Hintons Company being sold to Google and so on, and large interest in this learning from a number of important companies who are using machine learning with increasing vigor."
        ],
        [
            "Delete deep belief networks and related approaches have sort of lead.",
            "This charge are pushing this agenda of deep learning as something that's vital and important.",
            "In contrast."
        ],
        [
            "Custom kernels are sometimes referred to as shallow.",
            "Which obviously you know feels a little.",
            "Not quite the thing we like to be called and.",
            "Particularly, it said by deep people, so the aim of this talk."
        ],
        [
            "Kiss to discuss what I understand people to mean by deep learning and describe a number of ways in which kernel learning has been made deeper, and I'm not going to claim it.",
            "It's truly deep, but what I will try to convince you is that, at least when it goes deeper, it knows it keeps some control over what it's trying to do.",
            "It tries to analyze the steps that it's taking so that in a sense it's deeper on a firmer footing."
        ],
        [
            "So why is Colonel learning referred to as shallow?",
            "By the way, if you have any questions, please feel free to interrupt and ask.",
            "Well, the reason is that I mean you might say well wait a minute kernels, learn nonlinear functions in the input space, so that would appear to implement some inflexible deep learning system.",
            "However."
        ],
        [
            "What they actually implement, as we all know, is a linear function in a kernel defined feature space.",
            "So what you have is a fixed mapping which is chosen through the choice of kernel to some feature space which you don't actually explicitly work with.",
            "But is there and then in that feature space you learn this weight vector W, which represents a linear mapping in that feature space.",
            "Now again, you don't actually learn W explicitly, you learn it implicitly through the dual representation.",
            "But as far as the actual implementation, sorry actual functionality is concerned.",
            "This is a single layer learning process.",
            "You're only learning in this output layer and that's the reason that people call it shallow OK."
        ],
        [
            "And this is contrasted with deep learning, where parameters spread across several layers, typically with nonlinear transfer functions in between.",
            "However, there are various sort of flavors of deep learning, and very often you will find that the learning of the deeper layers is often unsupervised, so you'll sort of do an unsupervised stage of learning some multilayer network, and then perhaps if you want to do classification, you'll fix those deeper layers and you will learn some.",
            "Linear function perhaps taking inputs from all of the different layers of the deep network.",
            "So in some sense we're effectively pre learning a representation we can think of the deep learning as pre learning representation which would be analogous to learning the kernel, which I think we've heard of before.",
            "So let's explore that just a little bit.",
            "So what happens in practice?"
        ],
        [
            "So in practice, what I mean in practice for kernels you know when people apply kernel methods, what do they actually do?",
            "Well, typically they perform some learning of the kernel, so they might fix some hyperparameters via some heuristic that they look at the data and find the distance between the the nearest points with different classifications, and that is chosen as a Sigma.",
            "Possibly they might use cross validation to adapt the hyperparameters, including Sigma.",
            "Um, or possibly more hyperparameters associated with the kernel in order to to perform assess the performance in the left out parts of the cross validation, maybe for classification or regression.",
            "So."
        ],
        [
            "In some respects.",
            "This practice is actually at odds with one of the key kind of strengths, or, if you like selling points of kernel methods, which is that the based on some rigorous statistical analysis and generalization bounds.",
            "The problem is that if you take your standard generalization bounds, they actually fall apart.",
            "If you allow the training data to affect the choice of feature space.",
            "So if you the generalization bounds are reliant on the fact that you fixed the feature space and then you see the training data, and then you do the learning and then you have a bound.",
            "If you see the data first, then choose your feature space.",
            "And then do you learning.",
            "Then the standard bans don't actually apply, so in some respects practice is at odds with one of the key strengths of.",
            "Kernel methods, one of its suppose it's selling points.",
            "If you use all of the data, including the learning of the testator, then even test set bounds will be invalidated.",
            "If we use the testing data in the representation learning phase.",
            "So you know we do have to be very careful with the kind of assertions that we make once we start adapting the feature space based on the training and particularly on the testing data."
        ],
        [
            "So however, what actually also happens in practice that practitioners work with the development of the methods and the development of specialist kernels in a way that is mimicking deep learning.",
            "Although it's encoding deep prior knowledge.",
            "And the learning is some sort of trial and error of different researchers trying different things.",
            "And then at the end they come up with this sort of agreed best practice and an example of that would be the histograms of Patch cluster presence used in an object detection system.",
            "So there you create a clustering of the patches in a whole collection of images.",
            "You then take a new image you see which clusters are represented.",
            "By patches in that new image you create a histogram over those cluster centers that are represented and that is the feature vector that you use in your classification, and that's a very effective method, but clearly it is a very sophisticated development that's happened through a number of researchers gaining insight and understanding the dynamics and the statistics of these image images that are being considered in this object detection system.",
            "So.",
            "You know, I think deep learning is happening there, but it's what we would like to see is deep learning happening automatically in some automatic automatic automized process?"
        ],
        [
            "OK, So what I want to do in this talk is present a number of promising directions that sort of ticks.",
            "Some of these boxes that I've been highlighting in this introduction, so I'd like."
        ],
        [
            "To learn a kernel representation, possibly tuned to the main learning task.",
            "I would like."
        ],
        [
            "Take it to potentially provide some analysis of the resulting system that supports the choice.",
            "The design choices and bounds, the performance of the resulting system."
        ],
        [
            "And I'd like to look for empirical evidence that supports the approach on real world data.",
            "So what I'm looking for is something that mimics some sort of deep learning, but comes perhaps in a more measured and controlled way, and for which we have some sort of performance bound or or estimation."
        ],
        [
            "So the the different contributions will perhaps appear a little disjointed, but I hope that a convincing and coherent story."
        ],
        [
            "Emerged that deeper learning of kernels is alive and kicking.",
            "OK, so that's that's what I would hope to do.",
            "OK, so I'm going to start."
        ],
        [
            "With what I believe is the first bound that addresses the complete system of learning the representation and then learning a classifier or aggressor.",
            "Maybe I'm wrong, but I think this is the first bound that ties those two together, and it's based on a matching pursuit approach to generating a feature space.",
            "So what does matching pursuit do?",
            "It greedily chooses training examples that determine directions in the feature space that are well suited to some task and then deflate.",
            "So this can happen in the kernel defined feature space.",
            "So you can do it to choose some subspace effectively of the.",
            "Of the feature space defined by perhaps a very general kernel."
        ],
        [
            "So the analysis that I'm going to refer to combines this idea of sparse reconstruction.",
            "So the fact that you can reconstruct your subspace that you work in from the training examples from a sparse set of training examples.",
            "So the way that the bounds work is to imagine that you had at priority chosen the indices of the examples.",
            "So it's going to be index I one I2I3 up to IK.",
            "That are going to be actually determining my feature space Now, assuming I've done that, then actually the definition of the feature space is in some way fixed.",
            "Apriori it's fixed because we know the indices that are going to be used.",
            "We don't actually know what the examples are going to be, but the indices that we're going to use a fixed and so then we can apply a standard bound and then what we need to do is to count the possible number of ways and use a union bound are with all the ways we could have choose chosen those K examples to fix the feature space.",
            "So that's the trick that's used here.",
            "It's a sort of standard sparse sparse learning and trick that you can use and.",
            "Essentially, since the Union band comes in through the number of applications of the Union, bound comes in through the under the log, you get a log of the number of whatever it is N choose K where M choose K, where M is the number of training examples.",
            "K is the dimension of the space, and that's actually K log N, so you actually end up with a reasonably good bound and then within that feature space you can apply.",
            "The usual bound for some low dimensional learning of dimension K. If K was the size of that feature space."
        ],
        [
            "So you can apply this for all sorts of different learning problems.",
            "You can do a sparse PCA classification, regression, Canonical correlation analysis, and they all come with bounds.",
            "So I I'm putting in the sort of references to the papers where this work appeared, so that I think I've included all of the people who are Co authors as I go through.",
            "The only ones that I haven't I mentioned at the on the 1st slide where Dimitri assess the NESCAC kiss and.",
            "Tell me a Riddle mirror.",
            "Fernandez Reyes, but I hope I'm otherwise they will appear as we go.",
            "So just to give you an example of how the bound pans out and looks in practice, I'm going to show you."
        ],
        [
            "A plot that shows the reconstruction error projected onto one of bound for one dimensional projections of CCA.",
            "Using this sparse method based on the level of sparsity used to generate the space.",
            "So if we go as we increase the size of the space, you can see the bound has this interesting specific form, but actually surprisingly appears to match the true test error so that the level of these bands does appear to be something that's useful in practice.",
            "So this can be used as I said to generate."
        ],
        [
            "Feature spaces for a number of different problems.",
            "And this is just giving an IV."
        ],
        [
            "Sample for one of them.",
            "OK say stop me, I'm that's the end of that example.",
            "Just to give you a flavor of something.",
            "But please do interrupt if you wanted to."
        ],
        [
            "Ask.",
            "OK, so I'm going to move on now to methods that derive kernels from probabilistic models.",
            "This is a very natural thing I think to consider.",
            "If we're thinking about learning a representation natural way to do that is to model the data in some way, create some probabilistic model of the data, and then derive a kernel from that probabilistic model."
        ],
        [
            "So there are two main methods that can be used to define kernels from probabilistic models.",
            "The first is where we have a model class and we learn some distribution over the model class, some waiting at the different models as it were, and then we create the kernel, which is basically treating that waiting as a way of weighting the actual features of each model for the example.",
            "Which where the feature is given as the probability of the example in that model.",
            "So what this will say is you know two points X&Z are similar if they have similar probabilities in the in the most.",
            "Emphasize models, so learning here can happen.",
            "In order to, you can adjust this waiting through some learning process so you can learn this.",
            "This waiting and then that will affect the representation that you effectively create."
        ],
        [
            "This is sometimes known as the marginalization kernel.",
            "I'm actually not going to talk about this one.",
            "I'm going to concentrate on this second method, which is the Fisher kernel and that works for the case where the model is determined by a real parameter vector.",
            "So you have a model that is defined in terms of some vector of real real numbers and the way it's done is by taking the what's called a.",
            "So it's the log probability of a data point in the model and take the derivative of that with respect to the parameters.",
            "So the is called the score vector and the score vector has the same dimension as the dimension of the parameters of the probabilistic model and each parameter in the score vector corresponds to the derivative of the log probability of the example in the model.",
            "So I'll just give you an example so you get a flavor of.",
            "How it works?"
        ],
        [
            "And hopefully you get it.",
            "I mean, I want to do this 'cause I I'd like to give you a feel for what this is trying to do, 'cause it's a little strange initially I think maybe you're familiar with this already, so if so please bear with me.",
            "But if not, I found this quite difficult sort of construction to understand what's really going on here.",
            "So just to give you an example, here's a very simple probabilistic model which is just a 1 dimensional Gaussian.",
            "Has two parameters, a variance parameter and mean, so that is the the two parameters that are going to be considered to be adjusted and with respect to which will take derivatives."
        ],
        [
            "And the Fisher score vector I've just referred to is the derivative.",
            "The log likelihood with respect to the.",
            "The parameters, so here's the log likelihood.",
            "We're just going to take the derivative of this with respect to mu and respect to Sigma, and we're going to create."
        ],
        [
            "This score vector, which has this form.",
            "OK, and now if I take a particular choice of the parameters, remember the score vector is going to be determined once we know what the parameters of the model are.",
            "Once we've set the parameters, so we can choose use training data to set them if we wish."
        ],
        [
            "So if we just for example take the mean to be 0 and Sigma to be one, then we end up with."
        ],
        [
            "This embedding and I should say no."
        ],
        [
            "But the X, the original input appears linearly in the first component, so you can think of the first component.",
            "In this case, is exactly X, communal to zero, and Sigma nought squared is 1 S. The input is directly mapped onto the 1st."
        ],
        [
            "Component that's this component across here and effectively we've been given an extra dimension which is."
        ],
        [
            "This second dimension here and so."
        ],
        [
            "But we can now do.",
            "For instance, in this model is separate away from.",
            "We can learn, say with a linear classifier in this 2 dimensional space, an interval in the input space which we couldn't do with a linear function in the input space.",
            "So well, how would learning affect this representation?",
            "Well, we could imagine that the mean of the distribution would be moved.",
            "So if the mean was 1.5, we would shift this whole thing to the right.",
            "And again, if the data was very concentrated, then the standard deviation could make this a much more.",
            "So what we're going to get is this is going to be sort of fitted over the data in a way that it's going to maximally help us discriminate.",
            "Using these two different parts.",
            "So we're going to be able to find regions in the data that are tuned to the data if we learn."
        ],
        [
            "These parameters mu Nordson signal from the data."
        ],
        [
            "And notice that it becomes very sensitive to things that are quite improbable.",
            "So in this case, remember we were talking about a situation where the mean was zero and the standard deviation was one, so the data is going to be pretty concentrated in this region.",
            "The feature vector becomes very pronounced if we're moving away from that point of common probability, and this corresponds to the fact that you're looking at the derivative of the parameters with respect to the model.",
            "If the data looks very unlikely in the model, then when we take that derivative, it's going to be a larger value."
        ],
        [
            "OK, so I'm going to give you a couple of sort of more real world examples of the Fisher kernel just to give you a sense of how again this has been used to generate what I think are interesting deeper representations for particular data that has been considered in practice.",
            "So if we consider a Markov model of generating text conditioned on the previous N characters, so we're going to have we're seeing a stream of characters we look at end characters and we look at the probability.",
            "Based on the fact that we've seen a certain set of characters here of what the next character would be, and we can create a Markov model of that type.",
            "If we take the uniform distribution."
        ],
        [
            "For the probabilities of the next character.",
            "So we just say they're all equally probable, independently of what this sequence was.",
            "Then the.",
            "The Fisher kernel that we get from that from that model is actually the string, correct?",
            "If you remember, is counting the number of occurrences of sub sequences, in this case of length N + 1 because it's the end characters of the prefix and the extra character that we actually see, which index the probability score for that you know the likelihood of that character happening.",
            "So basically what happens is we get account of a feature corresponding to that.",
            "N + 1 dimensional string.",
            "So for every possible N + 1 dimensional string we have a feature and the feature score is going to be just the counts of the number of occurrences of that string in our document.",
            "So this is just the string kernel, so the string kernel can be viewed as a Fisher kernel over this particular model, and now that gives us the possibility of adapting the string kernel.",
            "Learning the string kernel adapted to some class of functions.",
            "We can for instance set those probabilities of the next character, the N + 1 character based on some training.",
            "And that will give emphasis, perhaps to certain unlikely.",
            "Again, that the emphasis will be on unlikely sequences, and that might be analogous to the TF IDF.",
            "The TF IDF weighting for words if you remember, emphasizes improbable words, unusual words, because it tends to be.",
            "They tend to be useful for classification, so again, it's kind of learning to pick out useful features based on some learning process.",
            "From in this case a probabilistic model."
        ],
        [
            "We can also extend this to a finite state automaton, or you know other possibilities of where we have variable length Markov.",
            "Trees, and so.",
            "The idea here is that some sequences you know are much more informative than others, and you know if you seek you, you know what's coming next.",
            "It's going to be Uber almost for certain, but if you see some sequences you need longer pre sequences.",
            "So you need that an really to be a variable rather than fixed in the finite state automaton case.",
            "You can do that.",
            "You know we get."
        ],
        [
            "Bolts that are reasonably competitive with TF.",
            "IDF Bank of words on Reuters and some improvements on average position.",
            "This was some time ago that we did this, but you know, it's an example of where you actually learn a representation based on a corpus.",
            "In this case of text and use that to generate a kernel in a semi principle way.",
            "Now what I what I had in the previous case with the matching pursuit, was this bound that tide together the representation that you learned in the performance?",
            "In this case we do not have that.",
            "You'd have to have the.",
            "The probabilistic model learned from a separate corpus of data and then applied on your training data.",
            "If you wanted to have your standard generalization bounds work.",
            "OK.",
            "So I think that I will mention one other just without a slide, which the example I gave earlier with the histogram of patches you can actually get the similar results using a Fisher kernel over Gaussian mixture model of those patches.",
            "So you take a Gaussian mixture model of the patches, take the Fisher kernel on top of that with restricting the variance to be diagonal.",
            "I think this is the usual.",
            "In order to make it more manageable.",
            "And what you you, you get performance, that is is very compareable, if not better than than the cluster histograms that I described before.",
            "So this is again a method that Maps somehow creates a representation that emphasizes unusual features in an image.",
            "So I think this is my intuition about why that's a good method.",
            "If you think about your Gaussian mixture model of the patches, what it's trying to do is model the sort of typical background statistics of images.",
            "An when you're doing object detection, what you're looking for is sort of something that sticks out that is characteristic of that object that you're trying to detect.",
            "So if for instance, it's a bicycle, it might have some spoke features that don't appear in most images, and so those will not be represented very well in the Gaussian mixture model, and so they will appear.",
            "If you like it as I did it."
        ],
        [
            "In that diagram there will be something that will appear out here will get hugely emphasized, so there will be a strong feature associated with that.",
            "That Patch that has that spoke in it and in a particular way relative to the other patches an so sorry the other other centers of the Gaussian mixture model and so now a classifier can easily use that in order to generate a classifier.",
            "That's very good at detecting save bicycles.",
            "So I think you know, again, you know you."
        ],
        [
            "These are methods that leverage a lot of knowledge, but they also do learn a representation from data and it's, I think, fair to say it's it's it's.",
            "Deep is too strong, but deeper.",
            "OK."
        ],
        [
            "So I'm not going to move on to look at multiple kernel learning, and the reason I want to talk about this is that I think here we're getting also the learning of the representation wrapped into the learning of whatever we're trying to do.",
            "The classifier or the regres or whatever it is we're trying to learn.",
            "So in the previous examples we would do a phase where we would learn the representation.",
            "We then take that and we apply it in the classifier learning stage.",
            "Now in multiple kernel learning.",
            "These two are tide together, So what is multiple kernel learning?",
            "Well we we use this class of kernels where we have a base set of kernels which I'm going to assume for the moment is finite, but it's also being considered the case where this is an infinite set and I'll talk a little bit about that later.",
            "But imagine just a finite set of kernels and we're going to imagine that we're going to look at all the kernels we can create by taking convex combinations of this base set of kernels.",
            "So we're going to take positive.",
            "Combinations where the sum of the factors is equal to 1, and then we're going to.",
            "Choose both visenti these weightings and the weight vector in the corresponding feature space in order to optimize the typical SVM training criterion.",
            "That is tradeoff between the margin and the slack variables that measure somehow the misclassified or not marginally classified points.",
            "And the interesting thing is that this is actually a convex problem, so in fact.",
            "Despite the fact that we've got the Zeds to optimize and the weight vectors, and they somehow seem to interact, this actually remains convex.",
            "And Furthermore, we can bound the performance of the corresponding.",
            "Classifier, let's say for the sake of argument, let's fix it on a classifier."
        ],
        [
            "And this is using one of them.",
            "Techniques used for bounding generalization known as Rademacher complexity.",
            "So I'll define what Rademacher complexity is in on the next slide.",
            "But there's one key result in murder market complexity that says that some measure of complexity.",
            "Well, just go to the next slide so you can see this is the measure of."
        ],
        [
            "Flexity for a function class that is known as Rademacher complexity and what it does is it takes an expectation over random noise of plus minus one.",
            "This Sigma is an M dimensional vector equal to the size of the training set with random plus minus ones with equal probability in and what it then tries for each.",
            "Each value of that vector it tries to align the function.",
            "With that random noise so it tries to find the function that has best correlation on the training data.",
            "With that random noise.",
            "So in a sense the argument is if you can align with high expectation with random noise, then there's very good chance you'll overfit and it's a very sort of intuitive idea here, but it's been turned into a bound.",
            "It's very again natural."
        ],
        [
            "And it just appears as sort of additive between the true and the empirical.",
            "This is like the empirical losses is the true loss that we're interested in, and we have this sort of additive factor with the margin.",
            "In fact, coming in here of the Rademacher complexity.",
            "But what we've used in addition here is that when you move to a convex Hull of a set of functions.",
            "You pay no price in the Rademacher complexity, so this is a very interesting property of Rademacher complexity that.",
            "Is being used for bounding boosting algorithms, so boosting typically takes you know one norm combination of a set of weak learners and so you get this sort of convex Hull class of functions where you take your base learners and then you take the convex combinations of those, create a much richer class, and yet you pay no price in the panel in the complexity.",
            "So it's a very kind of interesting and you know I would argue sort of deep insight into why.",
            "Boosting is such a good method, but here we're using it if you like to boost the.",
            "The Colonel, so we're taking a number of kernel function spaces.",
            "Each of these FTS is the set of functions that you can realize with the with Colonel Capiti with Norm one bounded weight vectors.",
            "So these are these functions that we can realize, and we're taking the union of those now.",
            "Actually, we're going to use the convex Hull of that Union, but I'm saying you know the convex Hull comes at no extra cost, so actually we can bound this in terms of this Union.",
            "OK.",
            "So that's you know, really nice."
        ],
        [
            "Property and we can actually."
        ],
        [
            "We need this bound on this union thing and there is a bit of a."
        ],
        [
            "Problem because the soup here is over the union of different function classes and for each Sigma we're going to have a different function that's going to be realized.",
            "So we essentially we need to swap these two, and it's pretty pretty messy."
        ],
        [
            "But there's a trick, and the trick is that we."
        ],
        [
            "Can with high probability replace the Rademacher complexity with a measurement on a single Sigma?",
            "So this is a single single randomly generated Sigma with high probability.",
            "The we just estimate the Rademacher complexity with that single Sigma and will be within some small factor of the true Rademacher complexity.",
            "And we can do that for the Rademacher complexity of the whole function class and for the individual.",
            "You know each kernel function class so that trick allows us to."
        ],
        [
            "Plugged together this sequence of inequality is where we start with the full function class we go to that particular Sigma.",
            "We know the soup over that, but now we can split this into the Max and the soup over the individual function classes.",
            "The soup over the individual function classes can now be replaced with an extra cost back to the Rademacher complexity that's going the other way here.",
            "And so we end up with the bound in terms of the Max to the random act complexities of the individual function classes plus this sort of relatively benign term where the number of kernels enters under the logarithm here, so it has a very weak dependence on the number of kernels that we actually use, provided that they are all bounded by some reasonable, individually bounded by the reasonable random."
        ],
        [
            "Complexity and if we take, for instance, the standard bound for the Rademacher complexity of kernel defined class, it's just the should be a square root.",
            "Sorry it's the square root of the trace of the kernel for that class, and so if you took, say, Gaussian kernel, this would just be square root of M, you get two over gamma square root of M, so this is a very benign bound and this enters here so."
        ],
        [
            "Wharton take home messages that this gives a log arhythmic or additive dependence on the number of kernels.",
            "So what it's saying is actually.",
            "I mean, I think people have been a bit tentative when they apply multiple kernel learning.",
            "Maybe 10 kernels.",
            "OK 12, so it's very kind of conservative and what this bound is saying.",
            "No, no, you can really go for the jugular.",
            "You can throw in million kernels.",
            "It's not going to provide you gotta thousand training examples no problem.",
            "Provided a game that races are all bounded reasonably that races don't in some way explode, yes.",
            "Exactly, this complexity is is benign is weak.",
            "So the main thing is going to be the margin and the slack variables like the error term if you like.",
            "So this is again just work with Zach Hussein."
        ],
        [
            "So this I think has been.",
            "I mean these guys did this before we did that result, so I'm not claiming that they took our advice and did it.",
            "But I think this is a good example where you know post festum we've justified the good performance that they they got.",
            "So this is Vivaldi ET al.",
            "Applied multiple kernel learning to the Pascal Visual Objects Challenge VOC 2007 data and they used it very large number of kernels through them in just press the button and.",
            "They got improvements over the winners of the challenge in 17 out of the twenty categories.",
            "You know, I think that's already pretty impressive.",
            "And then in half the categories they got an increase in average precision of over 25%.",
            "That's a huge hike in performance, so this is really been quite an influential paper.",
            "I think this is the actual paper itself, so it's a CPR in 2009.",
            "So you know, I think this is telling us that actually we can learn representations from the data as we learn the classifier that we're actually interested in, and we can be much more generous than we perhaps had thought in throwing in potential kernels that we might use.",
            "In the way that these guys have done that there.",
            "OK.",
            "I'm going to talk now."
        ],
        [
            "A little bit about how one might learn those kernels and show.",
            "I mean, these guys have also extended said that here.",
            "But yes, I haven't.",
            "So they've also scaled to affectively millions of kernels.",
            "They've actually.",
            "This was not in this paper, particularly in a later paper.",
            "They've also looked at scaling and I just wanted to give you an intuition about how that.",
            "Why is that?",
            "How's that possible and how do you actually go about scaling?",
            "Because if you think about millions of kernels and you've got thousands of data points, you got huge numbers of.",
            "Just storage to take care of computing these kernels and so on so.",
            "This is sort of relating it back to something to a version of boosting known as linear programming, boosting, and the way this works is it actually generates a sequence of linear programs well.",
            "Effectively I should start with.",
            "Sorry I should backtrack a little bit.",
            "What you do is you replace the two norm regularization of the SVM with the one norm regularization.",
            "So that's you just take the hinge loss.",
            "The usual thing you have replaced the two norm with the one norm and you end up with a linear program.",
            "Now you can actually solve that linear the jewel of that linear program by an iterative method, and that is known as column generation.",
            "In linear programming parlance.",
            "So what you do is you start with a.",
            "Uniform distribution over the examples.",
            "This is going to be analogous to the distribution you have when you do boosting, but these are just the dual variables of the linear program.",
            "In fact, you look for the J star that maximizes this, which is exactly the criterion you would use when you were doing boosting in, say, Adaboost is just the one that in that waiting maximizes the correlation with the target and.",
            "You then add that to your set of columns that you work with, and you solve a linear program.",
            "There are 16 linear program which behaves as if these are the only weak learners that you're allowed to use.",
            "That gives you a new vector U of weights over the examples which sum to one or distribution of the examples under threshold beta and then you go back.",
            "So you cycle round and you generate these extra columns.",
            "And you have this nice guarantee, convergence and stopping stopping soft stopping criterion, which is basically given by this inequality here, which tells you whether the how much the new weak learner will can contribute at most.",
            "So it gives an upper bound through some method.",
            "So what I'm saying is, you know this is actually a method that you can start with a small number of kernels, do your training potentially.",
            "We haven't quite.",
            "We don't quite know how to.",
            "Maximize this quantity over a kernel space.",
            "This is just for a finite set here, but I'll show you in a minute.",
            "Maximize it in small set of kernels, add in the kernel one new kernel.",
            "Solve for that small set of kernels.",
            "Get a criterion for incorporating a new set of kernels, and so on, and so you actually iterate through this large set of kernels, solving your problem only with a small subset as you go.",
            "Anne."
        ],
        [
            "And this is the way you can actually find which is the weak learner within one kernel defined future space.",
            "We need to pick this maximum and we can actually just write down what it.",
            "What is the value of the maximum.",
            "And indeed it turns out that the weight vector that realizes this maximum.",
            "I mean the trick is simply to take the sum inside the inner product and so you can get this soup with the W. And we know that the W that realizes this is the unit vector that's parallel to this.",
            "Vector here and so you can actually write down."
        ],
        [
            "The W in this form, and it has a dual representation, so you can do it in a kernel defined feature space and so you can for each."
        ],
        [
            "Kernel workout what the value of that kernel could be, and then you pick the one for which this value is largest, so you can actually iterate through.",
            "So you."
        ],
        [
            "You use the linear programming approach to to implement multiple kernel learning and but more."
        ],
        [
            "Generally, I think we can start to see this you vector as some kind of signal that you can use to refine other representations, and this is being used for say learning the Gaussian width parameter.",
            "Until Nigeria did that, I think I think it was hungry, asserting ICU.",
            "Yeah, OK."
        ],
        [
            "So I'm going to give another example which is for a Fisher kernel.",
            "So back to Fisher kernels there, parameterized by a probabilistic model.",
            "Can we use this?"
        ],
        [
            "You signal of which is effectively telling us how to tune our weak learner or our feature space in such a way that we maximize this correlation is the."
        ],
        [
            "Critical thing we're interested in here so we can imagine that the you know this is being now used to adapt some representation of the kernel here in order to maximize that correlation.",
            "And we can do that for a Fisher kernel.",
            "So the signal you is used to."
        ],
        [
            "Optimize the kernel by adjusting the parameters of the model.",
            "So we've used Hmm's for modeling time series data and use this to adapt the in order to forecast foreign exchange rates."
        ],
        [
            "With some encouraging results, I wouldn't say this is Mega, but it's so.",
            "Here's a couple of papers with Martin Sewell and Tristan Fletcher that are looking at this kind of approach, so again, just wanted to give you an example of where we're tying in learning relatively complex representations that are tuned to a particular learning task.",
            "OK so my final example I've got so I should 5 minutes.",
            "OK great is to look at nonlinear feature selection so."
        ],
        [
            "Here we've got an interesting relationship between kernel target alignment.",
            "So when I mean nonlinear feature selection, imagine using Gaussian kernel.",
            "You have an input feature vector and what you're interested in is sub selecting a set of those features to your Gaussian kernel in order to improve the representation for some learning task.",
            "So typically you might have a situation where only you're expecting only a small number of those features to be relevant, but you know that you're going to have a nonlinear function of those features.",
            "Let's say might be snips.",
            "In some you know genetic example where you know a combination complex combination of snips, but you're expecting a small number of them to be influencing some phenotype.",
            "OK, so how can we?",
            "There's an interesting measure here that relates the kernel target alignment of a kernel with the ability of that kernel to correlate with with an output signal.",
            "So this maximizes the correlation with an output signal.",
            "And we find that equal to the square root of the kernel, target and alignment."
        ],
        [
            "So we it suggests defining the contribution of a feature.",
            "The problem here is you have to specify the kernel.",
            "Now how are you going to find the right kernel that introduces the feature in the right way?",
            "So we solve that problem by defining the contribution as the expected alignment that we get when we include the feature compared to the expected alignment we get for the same size.",
            "Or rather, this is going to be 1, one feature bigger than this one.",
            "When we don't include the feature so effectively, we're going to consider all sets of features that don't include the feature I of a fixed size, and then consider the same set, but with I included.",
            "In other words, there's a direct one to one correspondence between these sets, where S will have the extra feature I and S prime will have that feature removed, so we can see that each time we do this, we're seeing what in a particular context I can contribute to the alignment and hence too.",
            "The representation or correlation with the output signal.",
            "And the size of these typically might be half of the features or something like that.",
            "Half of the number of features that we have.",
            "So he is just an."
        ],
        [
            "Example to show how it works in practice, so this is a toy example where we have 200 features.",
            "The function is just the X or function of the first 2 features.",
            "OK, so those are the only ones that have a genuine correlation with the target, but of course individually they have no correlation.",
            "Cousin X or function, so it's only the two together that can be detected and so this is the.",
            "The set of contribution estimated from a sample, so it's obviously a noisy estimate and what we do is we order these contributions and then color the bottom 25% and so this is the first coloring II culling, third culling and the forthcoming and as you remove the irrelevant features, these relevant features start to become more prominent, and soon they dominate the other features completely.",
            "So you can see that we have this method that effectively removes irrelevant features and allows the relevant features to emerge.",
            "And there are."
        ],
        [
            "Some properties that we can prove under certain assumptions that firstly, irrelevant feat."
        ],
        [
            "Is make a negative contribution."
        ],
        [
            "The chances of a relevant feature being in the bottom quarter of this prank contributions on a sufficiently large random sample is arbitrary small, so by choosing the sample big enough we can make that chance of throwing away irrelevant feature arbitrarily small."
        ],
        [
            "So when we call the 25% we don't lose our good features."
        ],
        [
            "And we also have the possibility of locking in features that appear in the top 25% consistently.",
            "We haven't actually used that in this example."
        ],
        [
            "Yeah, we've just colored features, but it just hopefully illustrates what's happening.",
            "So here are some results with artificial data where we are compare."
        ],
        [
            "During this run cell, which is the algorithm and talking about with various standard methods and we certainly appear to be performing in the same benchmark, in some cases we perform a lot better, so you know some will fail on.",
            "So stability selection will fail for this nonlinear Western, but RFE does quite well, but we do slightly better in that case, and so on, and the number of features we get seems seems to be a more reliable as well.",
            "This X or problem.",
            "Again, stability selection fails.",
            "Fosik also fails on this one.",
            "RFE does quite well basic.",
            "This is a forward.",
            "This is a backwards, does reasonably well, but overall where the more consistent method."
        ],
        [
            "And then on real world data we also get some, you know, recently good performance.",
            "I can go back if people want to do it."
        ],
        [
            "But the perhaps the final example that I think is worth mentioning.",
            "We applied this to the Deep learning challenge.",
            "The black Box Learning challenge, and."
        ],
        [
            "But we have the initial sparse filtering staff due to G count it Al.",
            "So just one preprocessing layer."
        ],
        [
            "Form the culling steps that I just described and then you."
        ],
        [
            "This LP boost MKL method to combine the corresponding kernels, one kernel for each culling.",
            "If you look at the data."
        ],
        [
            "And we came out third so that it's not bad.",
            "I thought for something that wasn't deep, you know.",
            "So we scored .685 versus the winning score of .702 in accuracy.",
            "Sorry I think yeah.",
            "So in summer."
        ],
        [
            "Learning deep representations is, I think, important for the analysis of real world data."
        ],
        [
            "Many kernel practicians are using deep learning, but typically in relatively ad hoc."
        ],
        [
            "Anna attempts at more.",
            "Principled methods have been rewarded with considerable success."
        ],
        [
            "And there is already a range of theoretical results relating deeper learning kernel methods that place the approach approaches on a firmer footing I believe.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just some background to the title.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So deep learning has, as I think we've all seen, emerged.",
                    "label": 1
                },
                {
                    "sent": "Or rather re emerged as a sort of very important research topic with considerable commercial value.",
                    "label": 0
                },
                {
                    "sent": "You know, with the company Hintons Company being sold to Google and so on, and large interest in this learning from a number of important companies who are using machine learning with increasing vigor.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Delete deep belief networks and related approaches have sort of lead.",
                    "label": 1
                },
                {
                    "sent": "This charge are pushing this agenda of deep learning as something that's vital and important.",
                    "label": 0
                },
                {
                    "sent": "In contrast.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Custom kernels are sometimes referred to as shallow.",
                    "label": 1
                },
                {
                    "sent": "Which obviously you know feels a little.",
                    "label": 0
                },
                {
                    "sent": "Not quite the thing we like to be called and.",
                    "label": 1
                },
                {
                    "sent": "Particularly, it said by deep people, so the aim of this talk.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kiss to discuss what I understand people to mean by deep learning and describe a number of ways in which kernel learning has been made deeper, and I'm not going to claim it.",
                    "label": 1
                },
                {
                    "sent": "It's truly deep, but what I will try to convince you is that, at least when it goes deeper, it knows it keeps some control over what it's trying to do.",
                    "label": 0
                },
                {
                    "sent": "It tries to analyze the steps that it's taking so that in a sense it's deeper on a firmer footing.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why is Colonel learning referred to as shallow?",
                    "label": 0
                },
                {
                    "sent": "By the way, if you have any questions, please feel free to interrupt and ask.",
                    "label": 0
                },
                {
                    "sent": "Well, the reason is that I mean you might say well wait a minute kernels, learn nonlinear functions in the input space, so that would appear to implement some inflexible deep learning system.",
                    "label": 1
                },
                {
                    "sent": "However.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What they actually implement, as we all know, is a linear function in a kernel defined feature space.",
                    "label": 1
                },
                {
                    "sent": "So what you have is a fixed mapping which is chosen through the choice of kernel to some feature space which you don't actually explicitly work with.",
                    "label": 0
                },
                {
                    "sent": "But is there and then in that feature space you learn this weight vector W, which represents a linear mapping in that feature space.",
                    "label": 0
                },
                {
                    "sent": "Now again, you don't actually learn W explicitly, you learn it implicitly through the dual representation.",
                    "label": 0
                },
                {
                    "sent": "But as far as the actual implementation, sorry actual functionality is concerned.",
                    "label": 1
                },
                {
                    "sent": "This is a single layer learning process.",
                    "label": 0
                },
                {
                    "sent": "You're only learning in this output layer and that's the reason that people call it shallow OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is contrasted with deep learning, where parameters spread across several layers, typically with nonlinear transfer functions in between.",
                    "label": 1
                },
                {
                    "sent": "However, there are various sort of flavors of deep learning, and very often you will find that the learning of the deeper layers is often unsupervised, so you'll sort of do an unsupervised stage of learning some multilayer network, and then perhaps if you want to do classification, you'll fix those deeper layers and you will learn some.",
                    "label": 0
                },
                {
                    "sent": "Linear function perhaps taking inputs from all of the different layers of the deep network.",
                    "label": 1
                },
                {
                    "sent": "So in some sense we're effectively pre learning a representation we can think of the deep learning as pre learning representation which would be analogous to learning the kernel, which I think we've heard of before.",
                    "label": 0
                },
                {
                    "sent": "So let's explore that just a little bit.",
                    "label": 0
                },
                {
                    "sent": "So what happens in practice?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in practice, what I mean in practice for kernels you know when people apply kernel methods, what do they actually do?",
                    "label": 1
                },
                {
                    "sent": "Well, typically they perform some learning of the kernel, so they might fix some hyperparameters via some heuristic that they look at the data and find the distance between the the nearest points with different classifications, and that is chosen as a Sigma.",
                    "label": 1
                },
                {
                    "sent": "Possibly they might use cross validation to adapt the hyperparameters, including Sigma.",
                    "label": 0
                },
                {
                    "sent": "Um, or possibly more hyperparameters associated with the kernel in order to to perform assess the performance in the left out parts of the cross validation, maybe for classification or regression.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In some respects.",
                    "label": 0
                },
                {
                    "sent": "This practice is actually at odds with one of the key kind of strengths, or, if you like selling points of kernel methods, which is that the based on some rigorous statistical analysis and generalization bounds.",
                    "label": 0
                },
                {
                    "sent": "The problem is that if you take your standard generalization bounds, they actually fall apart.",
                    "label": 0
                },
                {
                    "sent": "If you allow the training data to affect the choice of feature space.",
                    "label": 0
                },
                {
                    "sent": "So if you the generalization bounds are reliant on the fact that you fixed the feature space and then you see the training data, and then you do the learning and then you have a bound.",
                    "label": 0
                },
                {
                    "sent": "If you see the data first, then choose your feature space.",
                    "label": 0
                },
                {
                    "sent": "And then do you learning.",
                    "label": 0
                },
                {
                    "sent": "Then the standard bans don't actually apply, so in some respects practice is at odds with one of the key strengths of.",
                    "label": 0
                },
                {
                    "sent": "Kernel methods, one of its suppose it's selling points.",
                    "label": 0
                },
                {
                    "sent": "If you use all of the data, including the learning of the testator, then even test set bounds will be invalidated.",
                    "label": 1
                },
                {
                    "sent": "If we use the testing data in the representation learning phase.",
                    "label": 0
                },
                {
                    "sent": "So you know we do have to be very careful with the kind of assertions that we make once we start adapting the feature space based on the training and particularly on the testing data.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So however, what actually also happens in practice that practitioners work with the development of the methods and the development of specialist kernels in a way that is mimicking deep learning.",
                    "label": 0
                },
                {
                    "sent": "Although it's encoding deep prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "And the learning is some sort of trial and error of different researchers trying different things.",
                    "label": 0
                },
                {
                    "sent": "And then at the end they come up with this sort of agreed best practice and an example of that would be the histograms of Patch cluster presence used in an object detection system.",
                    "label": 1
                },
                {
                    "sent": "So there you create a clustering of the patches in a whole collection of images.",
                    "label": 0
                },
                {
                    "sent": "You then take a new image you see which clusters are represented.",
                    "label": 0
                },
                {
                    "sent": "By patches in that new image you create a histogram over those cluster centers that are represented and that is the feature vector that you use in your classification, and that's a very effective method, but clearly it is a very sophisticated development that's happened through a number of researchers gaining insight and understanding the dynamics and the statistics of these image images that are being considered in this object detection system.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You know, I think deep learning is happening there, but it's what we would like to see is deep learning happening automatically in some automatic automatic automized process?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what I want to do in this talk is present a number of promising directions that sort of ticks.",
                    "label": 0
                },
                {
                    "sent": "Some of these boxes that I've been highlighting in this introduction, so I'd like.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To learn a kernel representation, possibly tuned to the main learning task.",
                    "label": 0
                },
                {
                    "sent": "I would like.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take it to potentially provide some analysis of the resulting system that supports the choice.",
                    "label": 0
                },
                {
                    "sent": "The design choices and bounds, the performance of the resulting system.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'd like to look for empirical evidence that supports the approach on real world data.",
                    "label": 0
                },
                {
                    "sent": "So what I'm looking for is something that mimics some sort of deep learning, but comes perhaps in a more measured and controlled way, and for which we have some sort of performance bound or or estimation.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the the different contributions will perhaps appear a little disjointed, but I hope that a convincing and coherent story.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Emerged that deeper learning of kernels is alive and kicking.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's that's what I would hope to do.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to start.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With what I believe is the first bound that addresses the complete system of learning the representation and then learning a classifier or aggressor.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'm wrong, but I think this is the first bound that ties those two together, and it's based on a matching pursuit approach to generating a feature space.",
                    "label": 0
                },
                {
                    "sent": "So what does matching pursuit do?",
                    "label": 1
                },
                {
                    "sent": "It greedily chooses training examples that determine directions in the feature space that are well suited to some task and then deflate.",
                    "label": 1
                },
                {
                    "sent": "So this can happen in the kernel defined feature space.",
                    "label": 0
                },
                {
                    "sent": "So you can do it to choose some subspace effectively of the.",
                    "label": 0
                },
                {
                    "sent": "Of the feature space defined by perhaps a very general kernel.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the analysis that I'm going to refer to combines this idea of sparse reconstruction.",
                    "label": 0
                },
                {
                    "sent": "So the fact that you can reconstruct your subspace that you work in from the training examples from a sparse set of training examples.",
                    "label": 0
                },
                {
                    "sent": "So the way that the bounds work is to imagine that you had at priority chosen the indices of the examples.",
                    "label": 0
                },
                {
                    "sent": "So it's going to be index I one I2I3 up to IK.",
                    "label": 0
                },
                {
                    "sent": "That are going to be actually determining my feature space Now, assuming I've done that, then actually the definition of the feature space is in some way fixed.",
                    "label": 1
                },
                {
                    "sent": "Apriori it's fixed because we know the indices that are going to be used.",
                    "label": 0
                },
                {
                    "sent": "We don't actually know what the examples are going to be, but the indices that we're going to use a fixed and so then we can apply a standard bound and then what we need to do is to count the possible number of ways and use a union bound are with all the ways we could have choose chosen those K examples to fix the feature space.",
                    "label": 0
                },
                {
                    "sent": "So that's the trick that's used here.",
                    "label": 0
                },
                {
                    "sent": "It's a sort of standard sparse sparse learning and trick that you can use and.",
                    "label": 0
                },
                {
                    "sent": "Essentially, since the Union band comes in through the number of applications of the Union, bound comes in through the under the log, you get a log of the number of whatever it is N choose K where M choose K, where M is the number of training examples.",
                    "label": 0
                },
                {
                    "sent": "K is the dimension of the space, and that's actually K log N, so you actually end up with a reasonably good bound and then within that feature space you can apply.",
                    "label": 0
                },
                {
                    "sent": "The usual bound for some low dimensional learning of dimension K. If K was the size of that feature space.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you can apply this for all sorts of different learning problems.",
                    "label": 0
                },
                {
                    "sent": "You can do a sparse PCA classification, regression, Canonical correlation analysis, and they all come with bounds.",
                    "label": 1
                },
                {
                    "sent": "So I I'm putting in the sort of references to the papers where this work appeared, so that I think I've included all of the people who are Co authors as I go through.",
                    "label": 0
                },
                {
                    "sent": "The only ones that I haven't I mentioned at the on the 1st slide where Dimitri assess the NESCAC kiss and.",
                    "label": 0
                },
                {
                    "sent": "Tell me a Riddle mirror.",
                    "label": 0
                },
                {
                    "sent": "Fernandez Reyes, but I hope I'm otherwise they will appear as we go.",
                    "label": 0
                },
                {
                    "sent": "So just to give you an example of how the bound pans out and looks in practice, I'm going to show you.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A plot that shows the reconstruction error projected onto one of bound for one dimensional projections of CCA.",
                    "label": 0
                },
                {
                    "sent": "Using this sparse method based on the level of sparsity used to generate the space.",
                    "label": 1
                },
                {
                    "sent": "So if we go as we increase the size of the space, you can see the bound has this interesting specific form, but actually surprisingly appears to match the true test error so that the level of these bands does appear to be something that's useful in practice.",
                    "label": 0
                },
                {
                    "sent": "So this can be used as I said to generate.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Feature spaces for a number of different problems.",
                    "label": 0
                },
                {
                    "sent": "And this is just giving an IV.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample for one of them.",
                    "label": 0
                },
                {
                    "sent": "OK say stop me, I'm that's the end of that example.",
                    "label": 0
                },
                {
                    "sent": "Just to give you a flavor of something.",
                    "label": 0
                },
                {
                    "sent": "But please do interrupt if you wanted to.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ask.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to move on now to methods that derive kernels from probabilistic models.",
                    "label": 1
                },
                {
                    "sent": "This is a very natural thing I think to consider.",
                    "label": 0
                },
                {
                    "sent": "If we're thinking about learning a representation natural way to do that is to model the data in some way, create some probabilistic model of the data, and then derive a kernel from that probabilistic model.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are two main methods that can be used to define kernels from probabilistic models.",
                    "label": 1
                },
                {
                    "sent": "The first is where we have a model class and we learn some distribution over the model class, some waiting at the different models as it were, and then we create the kernel, which is basically treating that waiting as a way of weighting the actual features of each model for the example.",
                    "label": 1
                },
                {
                    "sent": "Which where the feature is given as the probability of the example in that model.",
                    "label": 0
                },
                {
                    "sent": "So what this will say is you know two points X&Z are similar if they have similar probabilities in the in the most.",
                    "label": 0
                },
                {
                    "sent": "Emphasize models, so learning here can happen.",
                    "label": 0
                },
                {
                    "sent": "In order to, you can adjust this waiting through some learning process so you can learn this.",
                    "label": 0
                },
                {
                    "sent": "This waiting and then that will affect the representation that you effectively create.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is sometimes known as the marginalization kernel.",
                    "label": 1
                },
                {
                    "sent": "I'm actually not going to talk about this one.",
                    "label": 0
                },
                {
                    "sent": "I'm going to concentrate on this second method, which is the Fisher kernel and that works for the case where the model is determined by a real parameter vector.",
                    "label": 1
                },
                {
                    "sent": "So you have a model that is defined in terms of some vector of real real numbers and the way it's done is by taking the what's called a.",
                    "label": 0
                },
                {
                    "sent": "So it's the log probability of a data point in the model and take the derivative of that with respect to the parameters.",
                    "label": 0
                },
                {
                    "sent": "So the is called the score vector and the score vector has the same dimension as the dimension of the parameters of the probabilistic model and each parameter in the score vector corresponds to the derivative of the log probability of the example in the model.",
                    "label": 0
                },
                {
                    "sent": "So I'll just give you an example so you get a flavor of.",
                    "label": 0
                },
                {
                    "sent": "How it works?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And hopefully you get it.",
                    "label": 0
                },
                {
                    "sent": "I mean, I want to do this 'cause I I'd like to give you a feel for what this is trying to do, 'cause it's a little strange initially I think maybe you're familiar with this already, so if so please bear with me.",
                    "label": 0
                },
                {
                    "sent": "But if not, I found this quite difficult sort of construction to understand what's really going on here.",
                    "label": 0
                },
                {
                    "sent": "So just to give you an example, here's a very simple probabilistic model which is just a 1 dimensional Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Has two parameters, a variance parameter and mean, so that is the the two parameters that are going to be considered to be adjusted and with respect to which will take derivatives.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the Fisher score vector I've just referred to is the derivative.",
                    "label": 1
                },
                {
                    "sent": "The log likelihood with respect to the.",
                    "label": 1
                },
                {
                    "sent": "The parameters, so here's the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "We're just going to take the derivative of this with respect to mu and respect to Sigma, and we're going to create.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This score vector, which has this form.",
                    "label": 0
                },
                {
                    "sent": "OK, and now if I take a particular choice of the parameters, remember the score vector is going to be determined once we know what the parameters of the model are.",
                    "label": 1
                },
                {
                    "sent": "Once we've set the parameters, so we can choose use training data to set them if we wish.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we just for example take the mean to be 0 and Sigma to be one, then we end up with.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This embedding and I should say no.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the X, the original input appears linearly in the first component, so you can think of the first component.",
                    "label": 0
                },
                {
                    "sent": "In this case, is exactly X, communal to zero, and Sigma nought squared is 1 S. The input is directly mapped onto the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Component that's this component across here and effectively we've been given an extra dimension which is.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This second dimension here and so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we can now do.",
                    "label": 0
                },
                {
                    "sent": "For instance, in this model is separate away from.",
                    "label": 0
                },
                {
                    "sent": "We can learn, say with a linear classifier in this 2 dimensional space, an interval in the input space which we couldn't do with a linear function in the input space.",
                    "label": 0
                },
                {
                    "sent": "So well, how would learning affect this representation?",
                    "label": 0
                },
                {
                    "sent": "Well, we could imagine that the mean of the distribution would be moved.",
                    "label": 0
                },
                {
                    "sent": "So if the mean was 1.5, we would shift this whole thing to the right.",
                    "label": 0
                },
                {
                    "sent": "And again, if the data was very concentrated, then the standard deviation could make this a much more.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to get is this is going to be sort of fitted over the data in a way that it's going to maximally help us discriminate.",
                    "label": 0
                },
                {
                    "sent": "Using these two different parts.",
                    "label": 0
                },
                {
                    "sent": "So we're going to be able to find regions in the data that are tuned to the data if we learn.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These parameters mu Nordson signal from the data.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And notice that it becomes very sensitive to things that are quite improbable.",
                    "label": 0
                },
                {
                    "sent": "So in this case, remember we were talking about a situation where the mean was zero and the standard deviation was one, so the data is going to be pretty concentrated in this region.",
                    "label": 0
                },
                {
                    "sent": "The feature vector becomes very pronounced if we're moving away from that point of common probability, and this corresponds to the fact that you're looking at the derivative of the parameters with respect to the model.",
                    "label": 0
                },
                {
                    "sent": "If the data looks very unlikely in the model, then when we take that derivative, it's going to be a larger value.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to give you a couple of sort of more real world examples of the Fisher kernel just to give you a sense of how again this has been used to generate what I think are interesting deeper representations for particular data that has been considered in practice.",
                    "label": 0
                },
                {
                    "sent": "So if we consider a Markov model of generating text conditioned on the previous N characters, so we're going to have we're seeing a stream of characters we look at end characters and we look at the probability.",
                    "label": 1
                },
                {
                    "sent": "Based on the fact that we've seen a certain set of characters here of what the next character would be, and we can create a Markov model of that type.",
                    "label": 0
                },
                {
                    "sent": "If we take the uniform distribution.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the probabilities of the next character.",
                    "label": 0
                },
                {
                    "sent": "So we just say they're all equally probable, independently of what this sequence was.",
                    "label": 0
                },
                {
                    "sent": "Then the.",
                    "label": 0
                },
                {
                    "sent": "The Fisher kernel that we get from that from that model is actually the string, correct?",
                    "label": 0
                },
                {
                    "sent": "If you remember, is counting the number of occurrences of sub sequences, in this case of length N + 1 because it's the end characters of the prefix and the extra character that we actually see, which index the probability score for that you know the likelihood of that character happening.",
                    "label": 0
                },
                {
                    "sent": "So basically what happens is we get account of a feature corresponding to that.",
                    "label": 0
                },
                {
                    "sent": "N + 1 dimensional string.",
                    "label": 0
                },
                {
                    "sent": "So for every possible N + 1 dimensional string we have a feature and the feature score is going to be just the counts of the number of occurrences of that string in our document.",
                    "label": 0
                },
                {
                    "sent": "So this is just the string kernel, so the string kernel can be viewed as a Fisher kernel over this particular model, and now that gives us the possibility of adapting the string kernel.",
                    "label": 0
                },
                {
                    "sent": "Learning the string kernel adapted to some class of functions.",
                    "label": 0
                },
                {
                    "sent": "We can for instance set those probabilities of the next character, the N + 1 character based on some training.",
                    "label": 0
                },
                {
                    "sent": "And that will give emphasis, perhaps to certain unlikely.",
                    "label": 0
                },
                {
                    "sent": "Again, that the emphasis will be on unlikely sequences, and that might be analogous to the TF IDF.",
                    "label": 0
                },
                {
                    "sent": "The TF IDF weighting for words if you remember, emphasizes improbable words, unusual words, because it tends to be.",
                    "label": 0
                },
                {
                    "sent": "They tend to be useful for classification, so again, it's kind of learning to pick out useful features based on some learning process.",
                    "label": 0
                },
                {
                    "sent": "From in this case a probabilistic model.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also extend this to a finite state automaton, or you know other possibilities of where we have variable length Markov.",
                    "label": 0
                },
                {
                    "sent": "Trees, and so.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that some sequences you know are much more informative than others, and you know if you seek you, you know what's coming next.",
                    "label": 0
                },
                {
                    "sent": "It's going to be Uber almost for certain, but if you see some sequences you need longer pre sequences.",
                    "label": 0
                },
                {
                    "sent": "So you need that an really to be a variable rather than fixed in the finite state automaton case.",
                    "label": 0
                },
                {
                    "sent": "You can do that.",
                    "label": 0
                },
                {
                    "sent": "You know we get.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bolts that are reasonably competitive with TF.",
                    "label": 0
                },
                {
                    "sent": "IDF Bank of words on Reuters and some improvements on average position.",
                    "label": 1
                },
                {
                    "sent": "This was some time ago that we did this, but you know, it's an example of where you actually learn a representation based on a corpus.",
                    "label": 1
                },
                {
                    "sent": "In this case of text and use that to generate a kernel in a semi principle way.",
                    "label": 0
                },
                {
                    "sent": "Now what I what I had in the previous case with the matching pursuit, was this bound that tide together the representation that you learned in the performance?",
                    "label": 0
                },
                {
                    "sent": "In this case we do not have that.",
                    "label": 0
                },
                {
                    "sent": "You'd have to have the.",
                    "label": 0
                },
                {
                    "sent": "The probabilistic model learned from a separate corpus of data and then applied on your training data.",
                    "label": 0
                },
                {
                    "sent": "If you wanted to have your standard generalization bounds work.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I think that I will mention one other just without a slide, which the example I gave earlier with the histogram of patches you can actually get the similar results using a Fisher kernel over Gaussian mixture model of those patches.",
                    "label": 0
                },
                {
                    "sent": "So you take a Gaussian mixture model of the patches, take the Fisher kernel on top of that with restricting the variance to be diagonal.",
                    "label": 0
                },
                {
                    "sent": "I think this is the usual.",
                    "label": 0
                },
                {
                    "sent": "In order to make it more manageable.",
                    "label": 0
                },
                {
                    "sent": "And what you you, you get performance, that is is very compareable, if not better than than the cluster histograms that I described before.",
                    "label": 0
                },
                {
                    "sent": "So this is again a method that Maps somehow creates a representation that emphasizes unusual features in an image.",
                    "label": 0
                },
                {
                    "sent": "So I think this is my intuition about why that's a good method.",
                    "label": 0
                },
                {
                    "sent": "If you think about your Gaussian mixture model of the patches, what it's trying to do is model the sort of typical background statistics of images.",
                    "label": 0
                },
                {
                    "sent": "An when you're doing object detection, what you're looking for is sort of something that sticks out that is characteristic of that object that you're trying to detect.",
                    "label": 0
                },
                {
                    "sent": "So if for instance, it's a bicycle, it might have some spoke features that don't appear in most images, and so those will not be represented very well in the Gaussian mixture model, and so they will appear.",
                    "label": 0
                },
                {
                    "sent": "If you like it as I did it.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In that diagram there will be something that will appear out here will get hugely emphasized, so there will be a strong feature associated with that.",
                    "label": 0
                },
                {
                    "sent": "That Patch that has that spoke in it and in a particular way relative to the other patches an so sorry the other other centers of the Gaussian mixture model and so now a classifier can easily use that in order to generate a classifier.",
                    "label": 0
                },
                {
                    "sent": "That's very good at detecting save bicycles.",
                    "label": 0
                },
                {
                    "sent": "So I think you know, again, you know you.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are methods that leverage a lot of knowledge, but they also do learn a representation from data and it's, I think, fair to say it's it's it's.",
                    "label": 0
                },
                {
                    "sent": "Deep is too strong, but deeper.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm not going to move on to look at multiple kernel learning, and the reason I want to talk about this is that I think here we're getting also the learning of the representation wrapped into the learning of whatever we're trying to do.",
                    "label": 0
                },
                {
                    "sent": "The classifier or the regres or whatever it is we're trying to learn.",
                    "label": 0
                },
                {
                    "sent": "So in the previous examples we would do a phase where we would learn the representation.",
                    "label": 0
                },
                {
                    "sent": "We then take that and we apply it in the classifier learning stage.",
                    "label": 0
                },
                {
                    "sent": "Now in multiple kernel learning.",
                    "label": 1
                },
                {
                    "sent": "These two are tide together, So what is multiple kernel learning?",
                    "label": 0
                },
                {
                    "sent": "Well we we use this class of kernels where we have a base set of kernels which I'm going to assume for the moment is finite, but it's also being considered the case where this is an infinite set and I'll talk a little bit about that later.",
                    "label": 0
                },
                {
                    "sent": "But imagine just a finite set of kernels and we're going to imagine that we're going to look at all the kernels we can create by taking convex combinations of this base set of kernels.",
                    "label": 0
                },
                {
                    "sent": "So we're going to take positive.",
                    "label": 1
                },
                {
                    "sent": "Combinations where the sum of the factors is equal to 1, and then we're going to.",
                    "label": 0
                },
                {
                    "sent": "Choose both visenti these weightings and the weight vector in the corresponding feature space in order to optimize the typical SVM training criterion.",
                    "label": 0
                },
                {
                    "sent": "That is tradeoff between the margin and the slack variables that measure somehow the misclassified or not marginally classified points.",
                    "label": 1
                },
                {
                    "sent": "And the interesting thing is that this is actually a convex problem, so in fact.",
                    "label": 0
                },
                {
                    "sent": "Despite the fact that we've got the Zeds to optimize and the weight vectors, and they somehow seem to interact, this actually remains convex.",
                    "label": 1
                },
                {
                    "sent": "And Furthermore, we can bound the performance of the corresponding.",
                    "label": 0
                },
                {
                    "sent": "Classifier, let's say for the sake of argument, let's fix it on a classifier.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is using one of them.",
                    "label": 0
                },
                {
                    "sent": "Techniques used for bounding generalization known as Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "So I'll define what Rademacher complexity is in on the next slide.",
                    "label": 0
                },
                {
                    "sent": "But there's one key result in murder market complexity that says that some measure of complexity.",
                    "label": 0
                },
                {
                    "sent": "Well, just go to the next slide so you can see this is the measure of.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Flexity for a function class that is known as Rademacher complexity and what it does is it takes an expectation over random noise of plus minus one.",
                    "label": 1
                },
                {
                    "sent": "This Sigma is an M dimensional vector equal to the size of the training set with random plus minus ones with equal probability in and what it then tries for each.",
                    "label": 0
                },
                {
                    "sent": "Each value of that vector it tries to align the function.",
                    "label": 0
                },
                {
                    "sent": "With that random noise so it tries to find the function that has best correlation on the training data.",
                    "label": 0
                },
                {
                    "sent": "With that random noise.",
                    "label": 0
                },
                {
                    "sent": "So in a sense the argument is if you can align with high expectation with random noise, then there's very good chance you'll overfit and it's a very sort of intuitive idea here, but it's been turned into a bound.",
                    "label": 0
                },
                {
                    "sent": "It's very again natural.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it just appears as sort of additive between the true and the empirical.",
                    "label": 0
                },
                {
                    "sent": "This is like the empirical losses is the true loss that we're interested in, and we have this sort of additive factor with the margin.",
                    "label": 0
                },
                {
                    "sent": "In fact, coming in here of the Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "But what we've used in addition here is that when you move to a convex Hull of a set of functions.",
                    "label": 0
                },
                {
                    "sent": "You pay no price in the Rademacher complexity, so this is a very interesting property of Rademacher complexity that.",
                    "label": 0
                },
                {
                    "sent": "Is being used for bounding boosting algorithms, so boosting typically takes you know one norm combination of a set of weak learners and so you get this sort of convex Hull class of functions where you take your base learners and then you take the convex combinations of those, create a much richer class, and yet you pay no price in the panel in the complexity.",
                    "label": 0
                },
                {
                    "sent": "So it's a very kind of interesting and you know I would argue sort of deep insight into why.",
                    "label": 0
                },
                {
                    "sent": "Boosting is such a good method, but here we're using it if you like to boost the.",
                    "label": 0
                },
                {
                    "sent": "The Colonel, so we're taking a number of kernel function spaces.",
                    "label": 0
                },
                {
                    "sent": "Each of these FTS is the set of functions that you can realize with the with Colonel Capiti with Norm one bounded weight vectors.",
                    "label": 0
                },
                {
                    "sent": "So these are these functions that we can realize, and we're taking the union of those now.",
                    "label": 0
                },
                {
                    "sent": "Actually, we're going to use the convex Hull of that Union, but I'm saying you know the convex Hull comes at no extra cost, so actually we can bound this in terms of this Union.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's you know, really nice.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Property and we can actually.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We need this bound on this union thing and there is a bit of a.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem because the soup here is over the union of different function classes and for each Sigma we're going to have a different function that's going to be realized.",
                    "label": 0
                },
                {
                    "sent": "So we essentially we need to swap these two, and it's pretty pretty messy.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there's a trick, and the trick is that we.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can with high probability replace the Rademacher complexity with a measurement on a single Sigma?",
                    "label": 0
                },
                {
                    "sent": "So this is a single single randomly generated Sigma with high probability.",
                    "label": 0
                },
                {
                    "sent": "The we just estimate the Rademacher complexity with that single Sigma and will be within some small factor of the true Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "And we can do that for the Rademacher complexity of the whole function class and for the individual.",
                    "label": 0
                },
                {
                    "sent": "You know each kernel function class so that trick allows us to.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plugged together this sequence of inequality is where we start with the full function class we go to that particular Sigma.",
                    "label": 0
                },
                {
                    "sent": "We know the soup over that, but now we can split this into the Max and the soup over the individual function classes.",
                    "label": 0
                },
                {
                    "sent": "The soup over the individual function classes can now be replaced with an extra cost back to the Rademacher complexity that's going the other way here.",
                    "label": 0
                },
                {
                    "sent": "And so we end up with the bound in terms of the Max to the random act complexities of the individual function classes plus this sort of relatively benign term where the number of kernels enters under the logarithm here, so it has a very weak dependence on the number of kernels that we actually use, provided that they are all bounded by some reasonable, individually bounded by the reasonable random.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Complexity and if we take, for instance, the standard bound for the Rademacher complexity of kernel defined class, it's just the should be a square root.",
                    "label": 0
                },
                {
                    "sent": "Sorry it's the square root of the trace of the kernel for that class, and so if you took, say, Gaussian kernel, this would just be square root of M, you get two over gamma square root of M, so this is a very benign bound and this enters here so.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wharton take home messages that this gives a log arhythmic or additive dependence on the number of kernels.",
                    "label": 1
                },
                {
                    "sent": "So what it's saying is actually.",
                    "label": 1
                },
                {
                    "sent": "I mean, I think people have been a bit tentative when they apply multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "Maybe 10 kernels.",
                    "label": 0
                },
                {
                    "sent": "OK 12, so it's very kind of conservative and what this bound is saying.",
                    "label": 0
                },
                {
                    "sent": "No, no, you can really go for the jugular.",
                    "label": 0
                },
                {
                    "sent": "You can throw in million kernels.",
                    "label": 0
                },
                {
                    "sent": "It's not going to provide you gotta thousand training examples no problem.",
                    "label": 0
                },
                {
                    "sent": "Provided a game that races are all bounded reasonably that races don't in some way explode, yes.",
                    "label": 0
                },
                {
                    "sent": "Exactly, this complexity is is benign is weak.",
                    "label": 0
                },
                {
                    "sent": "So the main thing is going to be the margin and the slack variables like the error term if you like.",
                    "label": 0
                },
                {
                    "sent": "So this is again just work with Zach Hussein.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this I think has been.",
                    "label": 0
                },
                {
                    "sent": "I mean these guys did this before we did that result, so I'm not claiming that they took our advice and did it.",
                    "label": 0
                },
                {
                    "sent": "But I think this is a good example where you know post festum we've justified the good performance that they they got.",
                    "label": 0
                },
                {
                    "sent": "So this is Vivaldi ET al.",
                    "label": 0
                },
                {
                    "sent": "Applied multiple kernel learning to the Pascal Visual Objects Challenge VOC 2007 data and they used it very large number of kernels through them in just press the button and.",
                    "label": 1
                },
                {
                    "sent": "They got improvements over the winners of the challenge in 17 out of the twenty categories.",
                    "label": 1
                },
                {
                    "sent": "You know, I think that's already pretty impressive.",
                    "label": 0
                },
                {
                    "sent": "And then in half the categories they got an increase in average precision of over 25%.",
                    "label": 0
                },
                {
                    "sent": "That's a huge hike in performance, so this is really been quite an influential paper.",
                    "label": 0
                },
                {
                    "sent": "I think this is the actual paper itself, so it's a CPR in 2009.",
                    "label": 0
                },
                {
                    "sent": "So you know, I think this is telling us that actually we can learn representations from the data as we learn the classifier that we're actually interested in, and we can be much more generous than we perhaps had thought in throwing in potential kernels that we might use.",
                    "label": 0
                },
                {
                    "sent": "In the way that these guys have done that there.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk now.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A little bit about how one might learn those kernels and show.",
                    "label": 0
                },
                {
                    "sent": "I mean, these guys have also extended said that here.",
                    "label": 0
                },
                {
                    "sent": "But yes, I haven't.",
                    "label": 0
                },
                {
                    "sent": "So they've also scaled to affectively millions of kernels.",
                    "label": 0
                },
                {
                    "sent": "They've actually.",
                    "label": 0
                },
                {
                    "sent": "This was not in this paper, particularly in a later paper.",
                    "label": 0
                },
                {
                    "sent": "They've also looked at scaling and I just wanted to give you an intuition about how that.",
                    "label": 1
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "How's that possible and how do you actually go about scaling?",
                    "label": 0
                },
                {
                    "sent": "Because if you think about millions of kernels and you've got thousands of data points, you got huge numbers of.",
                    "label": 0
                },
                {
                    "sent": "Just storage to take care of computing these kernels and so on so.",
                    "label": 0
                },
                {
                    "sent": "This is sort of relating it back to something to a version of boosting known as linear programming, boosting, and the way this works is it actually generates a sequence of linear programs well.",
                    "label": 1
                },
                {
                    "sent": "Effectively I should start with.",
                    "label": 0
                },
                {
                    "sent": "Sorry I should backtrack a little bit.",
                    "label": 0
                },
                {
                    "sent": "What you do is you replace the two norm regularization of the SVM with the one norm regularization.",
                    "label": 1
                },
                {
                    "sent": "So that's you just take the hinge loss.",
                    "label": 1
                },
                {
                    "sent": "The usual thing you have replaced the two norm with the one norm and you end up with a linear program.",
                    "label": 0
                },
                {
                    "sent": "Now you can actually solve that linear the jewel of that linear program by an iterative method, and that is known as column generation.",
                    "label": 1
                },
                {
                    "sent": "In linear programming parlance.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you start with a.",
                    "label": 0
                },
                {
                    "sent": "Uniform distribution over the examples.",
                    "label": 0
                },
                {
                    "sent": "This is going to be analogous to the distribution you have when you do boosting, but these are just the dual variables of the linear program.",
                    "label": 0
                },
                {
                    "sent": "In fact, you look for the J star that maximizes this, which is exactly the criterion you would use when you were doing boosting in, say, Adaboost is just the one that in that waiting maximizes the correlation with the target and.",
                    "label": 0
                },
                {
                    "sent": "You then add that to your set of columns that you work with, and you solve a linear program.",
                    "label": 0
                },
                {
                    "sent": "There are 16 linear program which behaves as if these are the only weak learners that you're allowed to use.",
                    "label": 0
                },
                {
                    "sent": "That gives you a new vector U of weights over the examples which sum to one or distribution of the examples under threshold beta and then you go back.",
                    "label": 0
                },
                {
                    "sent": "So you cycle round and you generate these extra columns.",
                    "label": 1
                },
                {
                    "sent": "And you have this nice guarantee, convergence and stopping stopping soft stopping criterion, which is basically given by this inequality here, which tells you whether the how much the new weak learner will can contribute at most.",
                    "label": 0
                },
                {
                    "sent": "So it gives an upper bound through some method.",
                    "label": 0
                },
                {
                    "sent": "So what I'm saying is, you know this is actually a method that you can start with a small number of kernels, do your training potentially.",
                    "label": 0
                },
                {
                    "sent": "We haven't quite.",
                    "label": 0
                },
                {
                    "sent": "We don't quite know how to.",
                    "label": 0
                },
                {
                    "sent": "Maximize this quantity over a kernel space.",
                    "label": 0
                },
                {
                    "sent": "This is just for a finite set here, but I'll show you in a minute.",
                    "label": 0
                },
                {
                    "sent": "Maximize it in small set of kernels, add in the kernel one new kernel.",
                    "label": 0
                },
                {
                    "sent": "Solve for that small set of kernels.",
                    "label": 0
                },
                {
                    "sent": "Get a criterion for incorporating a new set of kernels, and so on, and so you actually iterate through this large set of kernels, solving your problem only with a small subset as you go.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the way you can actually find which is the weak learner within one kernel defined future space.",
                    "label": 0
                },
                {
                    "sent": "We need to pick this maximum and we can actually just write down what it.",
                    "label": 0
                },
                {
                    "sent": "What is the value of the maximum.",
                    "label": 0
                },
                {
                    "sent": "And indeed it turns out that the weight vector that realizes this maximum.",
                    "label": 0
                },
                {
                    "sent": "I mean the trick is simply to take the sum inside the inner product and so you can get this soup with the W. And we know that the W that realizes this is the unit vector that's parallel to this.",
                    "label": 0
                },
                {
                    "sent": "Vector here and so you can actually write down.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The W in this form, and it has a dual representation, so you can do it in a kernel defined feature space and so you can for each.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kernel workout what the value of that kernel could be, and then you pick the one for which this value is largest, so you can actually iterate through.",
                    "label": 0
                },
                {
                    "sent": "So you.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You use the linear programming approach to to implement multiple kernel learning and but more.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Generally, I think we can start to see this you vector as some kind of signal that you can use to refine other representations, and this is being used for say learning the Gaussian width parameter.",
                    "label": 1
                },
                {
                    "sent": "Until Nigeria did that, I think I think it was hungry, asserting ICU.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to give another example which is for a Fisher kernel.",
                    "label": 0
                },
                {
                    "sent": "So back to Fisher kernels there, parameterized by a probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "Can we use this?",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You signal of which is effectively telling us how to tune our weak learner or our feature space in such a way that we maximize this correlation is the.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Critical thing we're interested in here so we can imagine that the you know this is being now used to adapt some representation of the kernel here in order to maximize that correlation.",
                    "label": 0
                },
                {
                    "sent": "And we can do that for a Fisher kernel.",
                    "label": 0
                },
                {
                    "sent": "So the signal you is used to.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimize the kernel by adjusting the parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "So we've used Hmm's for modeling time series data and use this to adapt the in order to forecast foreign exchange rates.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With some encouraging results, I wouldn't say this is Mega, but it's so.",
                    "label": 0
                },
                {
                    "sent": "Here's a couple of papers with Martin Sewell and Tristan Fletcher that are looking at this kind of approach, so again, just wanted to give you an example of where we're tying in learning relatively complex representations that are tuned to a particular learning task.",
                    "label": 0
                },
                {
                    "sent": "OK so my final example I've got so I should 5 minutes.",
                    "label": 0
                },
                {
                    "sent": "OK great is to look at nonlinear feature selection so.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we've got an interesting relationship between kernel target alignment.",
                    "label": 0
                },
                {
                    "sent": "So when I mean nonlinear feature selection, imagine using Gaussian kernel.",
                    "label": 1
                },
                {
                    "sent": "You have an input feature vector and what you're interested in is sub selecting a set of those features to your Gaussian kernel in order to improve the representation for some learning task.",
                    "label": 0
                },
                {
                    "sent": "So typically you might have a situation where only you're expecting only a small number of those features to be relevant, but you know that you're going to have a nonlinear function of those features.",
                    "label": 0
                },
                {
                    "sent": "Let's say might be snips.",
                    "label": 0
                },
                {
                    "sent": "In some you know genetic example where you know a combination complex combination of snips, but you're expecting a small number of them to be influencing some phenotype.",
                    "label": 0
                },
                {
                    "sent": "OK, so how can we?",
                    "label": 0
                },
                {
                    "sent": "There's an interesting measure here that relates the kernel target alignment of a kernel with the ability of that kernel to correlate with with an output signal.",
                    "label": 1
                },
                {
                    "sent": "So this maximizes the correlation with an output signal.",
                    "label": 0
                },
                {
                    "sent": "And we find that equal to the square root of the kernel, target and alignment.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we it suggests defining the contribution of a feature.",
                    "label": 1
                },
                {
                    "sent": "The problem here is you have to specify the kernel.",
                    "label": 0
                },
                {
                    "sent": "Now how are you going to find the right kernel that introduces the feature in the right way?",
                    "label": 0
                },
                {
                    "sent": "So we solve that problem by defining the contribution as the expected alignment that we get when we include the feature compared to the expected alignment we get for the same size.",
                    "label": 0
                },
                {
                    "sent": "Or rather, this is going to be 1, one feature bigger than this one.",
                    "label": 0
                },
                {
                    "sent": "When we don't include the feature so effectively, we're going to consider all sets of features that don't include the feature I of a fixed size, and then consider the same set, but with I included.",
                    "label": 0
                },
                {
                    "sent": "In other words, there's a direct one to one correspondence between these sets, where S will have the extra feature I and S prime will have that feature removed, so we can see that each time we do this, we're seeing what in a particular context I can contribute to the alignment and hence too.",
                    "label": 1
                },
                {
                    "sent": "The representation or correlation with the output signal.",
                    "label": 0
                },
                {
                    "sent": "And the size of these typically might be half of the features or something like that.",
                    "label": 0
                },
                {
                    "sent": "Half of the number of features that we have.",
                    "label": 0
                },
                {
                    "sent": "So he is just an.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example to show how it works in practice, so this is a toy example where we have 200 features.",
                    "label": 0
                },
                {
                    "sent": "The function is just the X or function of the first 2 features.",
                    "label": 0
                },
                {
                    "sent": "OK, so those are the only ones that have a genuine correlation with the target, but of course individually they have no correlation.",
                    "label": 0
                },
                {
                    "sent": "Cousin X or function, so it's only the two together that can be detected and so this is the.",
                    "label": 0
                },
                {
                    "sent": "The set of contribution estimated from a sample, so it's obviously a noisy estimate and what we do is we order these contributions and then color the bottom 25% and so this is the first coloring II culling, third culling and the forthcoming and as you remove the irrelevant features, these relevant features start to become more prominent, and soon they dominate the other features completely.",
                    "label": 0
                },
                {
                    "sent": "So you can see that we have this method that effectively removes irrelevant features and allows the relevant features to emerge.",
                    "label": 0
                },
                {
                    "sent": "And there are.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some properties that we can prove under certain assumptions that firstly, irrelevant feat.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is make a negative contribution.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The chances of a relevant feature being in the bottom quarter of this prank contributions on a sufficiently large random sample is arbitrary small, so by choosing the sample big enough we can make that chance of throwing away irrelevant feature arbitrarily small.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when we call the 25% we don't lose our good features.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also have the possibility of locking in features that appear in the top 25% consistently.",
                    "label": 0
                },
                {
                    "sent": "We haven't actually used that in this example.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, we've just colored features, but it just hopefully illustrates what's happening.",
                    "label": 0
                },
                {
                    "sent": "So here are some results with artificial data where we are compare.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "During this run cell, which is the algorithm and talking about with various standard methods and we certainly appear to be performing in the same benchmark, in some cases we perform a lot better, so you know some will fail on.",
                    "label": 0
                },
                {
                    "sent": "So stability selection will fail for this nonlinear Western, but RFE does quite well, but we do slightly better in that case, and so on, and the number of features we get seems seems to be a more reliable as well.",
                    "label": 0
                },
                {
                    "sent": "This X or problem.",
                    "label": 0
                },
                {
                    "sent": "Again, stability selection fails.",
                    "label": 0
                },
                {
                    "sent": "Fosik also fails on this one.",
                    "label": 0
                },
                {
                    "sent": "RFE does quite well basic.",
                    "label": 0
                },
                {
                    "sent": "This is a forward.",
                    "label": 0
                },
                {
                    "sent": "This is a backwards, does reasonably well, but overall where the more consistent method.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then on real world data we also get some, you know, recently good performance.",
                    "label": 0
                },
                {
                    "sent": "I can go back if people want to do it.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the perhaps the final example that I think is worth mentioning.",
                    "label": 0
                },
                {
                    "sent": "We applied this to the Deep learning challenge.",
                    "label": 0
                },
                {
                    "sent": "The black Box Learning challenge, and.",
                    "label": 1
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we have the initial sparse filtering staff due to G count it Al.",
                    "label": 0
                },
                {
                    "sent": "So just one preprocessing layer.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Form the culling steps that I just described and then you.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This LP boost MKL method to combine the corresponding kernels, one kernel for each culling.",
                    "label": 0
                },
                {
                    "sent": "If you look at the data.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we came out third so that it's not bad.",
                    "label": 0
                },
                {
                    "sent": "I thought for something that wasn't deep, you know.",
                    "label": 0
                },
                {
                    "sent": "So we scored .685 versus the winning score of .702 in accuracy.",
                    "label": 1
                },
                {
                    "sent": "Sorry I think yeah.",
                    "label": 0
                },
                {
                    "sent": "So in summer.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning deep representations is, I think, important for the analysis of real world data.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Many kernel practicians are using deep learning, but typically in relatively ad hoc.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anna attempts at more.",
                    "label": 0
                },
                {
                    "sent": "Principled methods have been rewarded with considerable success.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there is already a range of theoretical results relating deeper learning kernel methods that place the approach approaches on a firmer footing I believe.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}