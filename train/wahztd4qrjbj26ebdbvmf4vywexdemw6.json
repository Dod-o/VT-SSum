{
    "id": "wahztd4qrjbj26ebdbvmf4vywexdemw6",
    "title": "Efficient signal processing in random networks that generate variability: A comparison of internally generated and externally induced variability",
    "info": {
        "author": [
            "Sakyasingha Dasgupta, RIKEN Brain Science Institute"
        ],
        "published": "March 7, 2016",
        "recorded": "December 2015",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Physics->Statistical Physics"
        ]
    },
    "url": "http://videolectures.net/netadis2015_dasgupta_random_networks/",
    "segmentation": [
        [
            "I'm sucker hi, I'm from the Recon Brain Science Institute and this is joint work with dishonesty.",
            "Kalvin Kazuki Hara from the University of Tokyo, an arrow to Izumi from the Rican Princess Institute.",
            "So the main question that we were trying to answer here is using modeling to find out what is the source of cortical variability.",
            "And how that actually affects neural computational signal processing.",
            "And the reason this is interesting is because if you look at the steady state or the resting state of the brain, there is a large amount of variability there.",
            "However, it's already been seen from mine, and Sejnowski's work from 1995.",
            "The single neurons are extremely reliable."
        ],
        [
            "So the question is, where is that variability come from?",
            "An since that there is this variability?",
            "What kind of role that plays in computation the way we model this is basically we consider balanced random networks of quadratic integrated fire neurons, and it has already been known from a sample in ski and so much work that if you have this balance to excitation inhibition in random record networks basically depends on strong synaptic connections.",
            "You can have runaway chaos or basically chaotic dynamics.",
            "So what we consider is two different networks.",
            "One is this deterministic chaotic network.",
            "Essentially, the networks are exactly the same, the only difference is the type of couplings.",
            "If you have very strong coupling, you have chaotic dynamics, and on the other end of the spectrum you have with the stochastic network, which is basically very weak coupling between the neurons excitation and inhibition neurons, but it receives strong external noise, and that's the reason is stochastic.",
            "But if you look at these two networks and if you look at the spontaneous activity, there is virtually no difference and that's what you see in the lower.",
            "Graph here whether you look at the auto correlation structure of the cost correlation structure, it essentially looks the same, so no difference between the two networks.",
            "However, if you give a small perturbation to the network, there is strong difference in evoke responses.",
            "I haven't shown it for the interest of time."
        ],
        [
            "It will become clear in the next slide.",
            "So what we did is that using dynamic mean field theory.",
            "If you now characterize the dynamics of this QF networks, what you find that you can derive a single parameter family which is given by this Tau or dependent on G~ with which you can actually interpolate between the two networks.",
            "You can smoothly interpolate between a completely took stochastic network which has very weak coupling, but driven by extrinsic noise and on the other end of the spectrum you have this deterministically chaotic network strong coupling, but doesn't receive any external noise.",
            "Now, once again, if you look at the spontaneous activity of the whole family, you see that there is no difference.",
            "The auto correlation of the cross correlation structure will look exactly the same.",
            "However, now if you have a small perturbation to the network, they evoke responses are quite different across the family.",
            "So you see that when GTL is equal to zero, that is the case of the stochastic network completely driven by external noise, and that virtually has no change in the spiking probability.",
            "The similar thing you will observe if you look at the response magnitude of the network as well, but at the other end of the spectrum for the deterministically chaotic network, there's a large change in spiking probability.",
            "And if you look at the response magnitude, there is almost a monotonic increase, so the next."
        ],
        [
            "Immediate question was what happens in the presence of activity dependent plasticity?",
            "Would that so that does the neural type of neural variability affect learning and what we observe is that if you now have activity dependent plasticity in terms of any kind of heaven plus city in the inputs an access, you basically give some kind of nonlinear input to the network and you're trying to learn this input.",
            "What you find is that the deterministic chaotic network learns it extremely well, so that is observed by the signal to noise ratio.",
            "So you have a very high signal to noise ratio.",
            "For the deterministically chaotic network in comparison to the stochastic case, there is virtually no change.",
            "So clearly that was an indication that in the presence of internally generated noise, which is chaotic dynamics, you have very good decoding capabilities finally."
        ],
        [
            "We wanted to ask is that does that actually affect inference or Bayesian computation?",
            "And one idea has been that you need to cast City in order to probabilistic computation.",
            "But what we show here is that even a deterministic network as long as the internal dynamics is given by chaotic dynamics, you can actually sample from this internal dynamics to learn a posterior distribution, which in fact is based optimal.",
            "That is kind of counter intuitive.",
            "So what we did is that it's a very simple protocol you have.",
            "Multi sensory integration being done so you have two different signals being projected onto the network.",
            "No learning in the input synapses, but now the output synapses basically learn by a cost function which is dependent on the care divergent between the optimal posterior distribution and the one that is sampled from the network.",
            "And what we show is that you can actually sample from this internal distribution of the optimal posterior distribution, which happens to be the Bayes optimal for multi signal integration.",
            "And if you now compare the performance of the stochastic versus the deterministic network.",
            "You observe that the deterministically chaotic network performs really well as compared to the stochastic network, and so the prediction being actually you can use a chaotic network as Boltzmann machine, which is kind of counter intuitive.",
            "So people think that you need to cast it to have a Boltzmann machine or do probabilistic computation.",
            "But you can actually use a chaotic dynamics as a substrate for probabilistic computation.",
            "Answer The prediction being that if at all neural variability in the cortex is chaos, which has been shown by numerous other models, it is in fact a good neural computing systems.",
            "Is efficient signal processing.",
            "So if you have further questions than you want to look at the details, please come to my poster which is right there.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm sucker hi, I'm from the Recon Brain Science Institute and this is joint work with dishonesty.",
                    "label": 0
                },
                {
                    "sent": "Kalvin Kazuki Hara from the University of Tokyo, an arrow to Izumi from the Rican Princess Institute.",
                    "label": 0
                },
                {
                    "sent": "So the main question that we were trying to answer here is using modeling to find out what is the source of cortical variability.",
                    "label": 1
                },
                {
                    "sent": "And how that actually affects neural computational signal processing.",
                    "label": 0
                },
                {
                    "sent": "And the reason this is interesting is because if you look at the steady state or the resting state of the brain, there is a large amount of variability there.",
                    "label": 0
                },
                {
                    "sent": "However, it's already been seen from mine, and Sejnowski's work from 1995.",
                    "label": 0
                },
                {
                    "sent": "The single neurons are extremely reliable.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question is, where is that variability come from?",
                    "label": 0
                },
                {
                    "sent": "An since that there is this variability?",
                    "label": 0
                },
                {
                    "sent": "What kind of role that plays in computation the way we model this is basically we consider balanced random networks of quadratic integrated fire neurons, and it has already been known from a sample in ski and so much work that if you have this balance to excitation inhibition in random record networks basically depends on strong synaptic connections.",
                    "label": 0
                },
                {
                    "sent": "You can have runaway chaos or basically chaotic dynamics.",
                    "label": 0
                },
                {
                    "sent": "So what we consider is two different networks.",
                    "label": 0
                },
                {
                    "sent": "One is this deterministic chaotic network.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the networks are exactly the same, the only difference is the type of couplings.",
                    "label": 1
                },
                {
                    "sent": "If you have very strong coupling, you have chaotic dynamics, and on the other end of the spectrum you have with the stochastic network, which is basically very weak coupling between the neurons excitation and inhibition neurons, but it receives strong external noise, and that's the reason is stochastic.",
                    "label": 1
                },
                {
                    "sent": "But if you look at these two networks and if you look at the spontaneous activity, there is virtually no difference and that's what you see in the lower.",
                    "label": 0
                },
                {
                    "sent": "Graph here whether you look at the auto correlation structure of the cost correlation structure, it essentially looks the same, so no difference between the two networks.",
                    "label": 0
                },
                {
                    "sent": "However, if you give a small perturbation to the network, there is strong difference in evoke responses.",
                    "label": 0
                },
                {
                    "sent": "I haven't shown it for the interest of time.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It will become clear in the next slide.",
                    "label": 0
                },
                {
                    "sent": "So what we did is that using dynamic mean field theory.",
                    "label": 0
                },
                {
                    "sent": "If you now characterize the dynamics of this QF networks, what you find that you can derive a single parameter family which is given by this Tau or dependent on G~ with which you can actually interpolate between the two networks.",
                    "label": 0
                },
                {
                    "sent": "You can smoothly interpolate between a completely took stochastic network which has very weak coupling, but driven by extrinsic noise and on the other end of the spectrum you have this deterministically chaotic network strong coupling, but doesn't receive any external noise.",
                    "label": 0
                },
                {
                    "sent": "Now, once again, if you look at the spontaneous activity of the whole family, you see that there is no difference.",
                    "label": 1
                },
                {
                    "sent": "The auto correlation of the cross correlation structure will look exactly the same.",
                    "label": 1
                },
                {
                    "sent": "However, now if you have a small perturbation to the network, they evoke responses are quite different across the family.",
                    "label": 0
                },
                {
                    "sent": "So you see that when GTL is equal to zero, that is the case of the stochastic network completely driven by external noise, and that virtually has no change in the spiking probability.",
                    "label": 0
                },
                {
                    "sent": "The similar thing you will observe if you look at the response magnitude of the network as well, but at the other end of the spectrum for the deterministically chaotic network, there's a large change in spiking probability.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the response magnitude, there is almost a monotonic increase, so the next.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Immediate question was what happens in the presence of activity dependent plasticity?",
                    "label": 1
                },
                {
                    "sent": "Would that so that does the neural type of neural variability affect learning and what we observe is that if you now have activity dependent plasticity in terms of any kind of heaven plus city in the inputs an access, you basically give some kind of nonlinear input to the network and you're trying to learn this input.",
                    "label": 0
                },
                {
                    "sent": "What you find is that the deterministic chaotic network learns it extremely well, so that is observed by the signal to noise ratio.",
                    "label": 0
                },
                {
                    "sent": "So you have a very high signal to noise ratio.",
                    "label": 0
                },
                {
                    "sent": "For the deterministically chaotic network in comparison to the stochastic case, there is virtually no change.",
                    "label": 0
                },
                {
                    "sent": "So clearly that was an indication that in the presence of internally generated noise, which is chaotic dynamics, you have very good decoding capabilities finally.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We wanted to ask is that does that actually affect inference or Bayesian computation?",
                    "label": 0
                },
                {
                    "sent": "And one idea has been that you need to cast City in order to probabilistic computation.",
                    "label": 0
                },
                {
                    "sent": "But what we show here is that even a deterministic network as long as the internal dynamics is given by chaotic dynamics, you can actually sample from this internal dynamics to learn a posterior distribution, which in fact is based optimal.",
                    "label": 0
                },
                {
                    "sent": "That is kind of counter intuitive.",
                    "label": 0
                },
                {
                    "sent": "So what we did is that it's a very simple protocol you have.",
                    "label": 0
                },
                {
                    "sent": "Multi sensory integration being done so you have two different signals being projected onto the network.",
                    "label": 0
                },
                {
                    "sent": "No learning in the input synapses, but now the output synapses basically learn by a cost function which is dependent on the care divergent between the optimal posterior distribution and the one that is sampled from the network.",
                    "label": 0
                },
                {
                    "sent": "And what we show is that you can actually sample from this internal distribution of the optimal posterior distribution, which happens to be the Bayes optimal for multi signal integration.",
                    "label": 0
                },
                {
                    "sent": "And if you now compare the performance of the stochastic versus the deterministic network.",
                    "label": 0
                },
                {
                    "sent": "You observe that the deterministically chaotic network performs really well as compared to the stochastic network, and so the prediction being actually you can use a chaotic network as Boltzmann machine, which is kind of counter intuitive.",
                    "label": 0
                },
                {
                    "sent": "So people think that you need to cast it to have a Boltzmann machine or do probabilistic computation.",
                    "label": 0
                },
                {
                    "sent": "But you can actually use a chaotic dynamics as a substrate for probabilistic computation.",
                    "label": 1
                },
                {
                    "sent": "Answer The prediction being that if at all neural variability in the cortex is chaos, which has been shown by numerous other models, it is in fact a good neural computing systems.",
                    "label": 0
                },
                {
                    "sent": "Is efficient signal processing.",
                    "label": 0
                },
                {
                    "sent": "So if you have further questions than you want to look at the details, please come to my poster which is right there.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}