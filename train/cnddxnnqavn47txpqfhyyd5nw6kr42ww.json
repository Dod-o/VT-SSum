{
    "id": "cnddxnnqavn47txpqfhyyd5nw6kr42ww",
    "title": "Truthful Mechanisms for Multi Agent Self-Interested Correspondence Selection",
    "info": {
        "author": [
            "Terry Payne, University of Liverpool"
        ],
        "published": "Nov. 27, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2019_payne_correspondence_selection/",
    "segmentation": [
        [
            "My name Sir Terry Pain and I am actually here to present work on behalf of my student.",
            "Non she who couldn't actually make it.",
            "Today we've been looking at issues to do with finding alignments and specifically finding alignments within decentralized setting.",
            "My background actually is in semantic web services and the idea of bringing services together where are typically those services are trying to do some form of negotiation when they meet each."
        ],
        [
            "One service provided meets another service provider or consumer, so the outline of the talk I'm going to give today is to look at the notion of matchings.",
            "These are one to one or injective alignments.",
            "Assuming this stays stable.",
            "That you generate when you're forming ontology.",
            "So it's a classic problem with any form of ontology alignment system, but also it's an issue that we need to consider within the distributed alignment negotiation process.",
            "So we'll be looking at that scenario as well and show the fact that because you've actually got agents that can actually declare information to their peers, they can act strategically.",
            "They can potentially choose to misrepresent the information about those correspondences.",
            "When they're trying to find them in order to potentially gain some form of strategic advance advantage, and we explore this whole problem from a game theoretic perspective, so apologies for that, I'm going to be presenting this salient results from this work.",
            "All of the proofs can be found in the paper itself."
        ],
        [
            "So as I'm sure most of you are aware, of course we can use ontologies to model various domains for different systems.",
            "And of course, if you've got a scenario where there are multiple stakeholders producing data within a common domain, you typically find that there may be multiple ontologies that effectively are overlapping with respective domain, but the way they're encoded is somewhat different, so we need a mechanism that can find the mappings between the entities in these different.",
            "Ontologies, and that's what an alignment system Ascentia Lee does."
        ],
        [
            "And the way traditional many traditional mechanisms do this is by considering the entities in one ontology, the entities in the other ontology, computing some form of pairwise similarity between each of the entities of these two ontologists.",
            "That could be very trivially based on terminology.",
            "It could be based on the structure of the ontology.",
            "There are many different approaches that we could consider, and you potentially end up with a partial or completely connected by Python.",
            "Bipartite graph, and from that you then want to pick those edges that will allow you to form ideally or one to one mapping between each of the entities.",
            "Couple of caveats which I can talk about offline, but essentially that's what you're trying to do, so you're trying to form this matching."
        ],
        [
            "On these two.",
            "From this set of edges that you can form now there are a number of different approaches that you'll find if you look in the literature.",
            "Often people talk about finding a complete or an optimal solution.",
            "For example, one that's maximizing social welfare.",
            "Now what I mean by social welfare is if you assume that each edge has some form of weight, you can find different combinations of these edges and you want to find the one that results in the maximal sum of all of the edge weights.",
            "Good algorithms out there like the Hungarian, which will find with complexity and cubed.",
            "Alternatively because often N is big, so you want a faster approach.",
            "You might use a sub optimal approach to finding these edges.",
            "For example, you might think of ordering them and using a stable marriage algorithm from the classic, else sharply work of the 1960s.",
            "Or you might take a greedy approach.",
            "But Interestingly enough these will produce suboptimal solutions and the question is how suboptimal are they?",
            "They nearly optimal.",
            "Are they a far distance from optimal?",
            "They may be faster, but will they produce approximately similar alignments or completely different alignments?",
            "And the question is we don't necessary."
        ],
        [
            "Really, no.",
            "Here is an example of the illustrates the problem.",
            "I've now gotten incredibly simple scenario here where we've got two ontologies with two concepts in each ontology.",
            "First one has editor and author, the second has writer and contributor.",
            "And let's assume that we need an alignment between these two ontologies.",
            "Using some of these edges.",
            "Well, if you went through the optimal solution.",
            "Basically by looking at that diagram at the right, we know that the wait for editor to writer is 1.",
            "The weight for authors contributor is 1.",
            "Therefore, the maximal matching here would be those two edges E1 and E3, which should give us a total weight of two.",
            "That's the optimal, but have you notice?",
            "Actually, this doesn't include the best mapping.",
            "The best mapping is actually from author to writer, which is modelled here as one plus epsilon.",
            "So we just gotta value epsilon being an incredibly small value that just pushes it above 1.",
            "But if you take a greedy approach, that is the mapping that edge that will be selected from our approach.",
            "So as you can see, the if the worst case scenario here of possible solution is at least one plus epsilon compared to the optimal which is 2.",
            "So we now within at least a 50% bound.",
            "Of course, that's taking that view."
        ],
        [
            "Everything is centralized where you put your ontologies into a, say in Oracle or an alignment system that computes that the work we're doing is actually more looking at a decentralized scenario where an agent may know a number of different correspondences between this ontology and other ontologies out there.",
            "These may have been acquired from previous negotiations, so we're not talking to one shot game, we're talking evolution over many different engagements as it were.",
            "And because you've got a decentralized approach with two agents.",
            "When they come to declare their information, they can either be naive and simply just declare what they know, or they may be strategic.",
            "If they believe they could obtain some form of utility or benefit by actually misrepresenting their information, why would an agent actually do this?",
            "Well, it may well be that."
        ],
        [
            "An agent is trying to somehow impasse transactions, aligned its ontology so it can then transact with another service provider, and it gains some form of goods because you now have that Community communication channel working, and therefore it gains some monetary value.",
            "And there may be different monetary values you could get for it, depending on who you're transacting with.",
            "So let's assume for a second that we have these service providers service consumers that have this prior knowledge about.",
            "Possible correspondence is that they need to select from.",
            "Let's assume that each of those correspondences have some associated weight that could represent the efficacy.",
            "When it comes to transacting, it could be based on similarity or various reasons for having not wait, and we assume that these agents will be declaring some value, some bid, which might be the weight itself, or it might be a misrepresentation of the wait.",
            "We've been analyzing this from our game theory perspective, where we look at the mechanism.",
            "That will allow these to be combined in some way.",
            "So the aim here is to take these individual weights for these individual correspondence.",
            "Says put them together and find some form of solution.",
            "But analyze those properties so we can say something about the different types of solutions out there so.",
            "Ideally we'd like to get a an optimal solution, but will accept a sub optimal solution if we can find a mechanism that is potentially faster.",
            "And again, I have another example here where you can see how if you actually look at the concepts on one side to the other, an edge E1 would actually be calculated purely through an additive function.",
            "Did add 3 + 3 to get the total weight of that edge as six.",
            "For example on time.",
            "So in this case.",
            "We now have gain two solutions, the Optima."
        ],
        [
            "Solution here is going to be a value 23 where we have a 1 E 3 E 5 whereas an alternate solution could be that you find edges E2E four with a slightly lower total value, But it's certainly a plausible solution, and it's one that would have been found if we'd made use for example of a greedy algorithm.",
            "I just want to come back to that lying problem because I'm sure not."
        ],
        [
            "Probably say why would an agent lines in the reviews?",
            "Well, as I said, prior transactions May typically come up with better outcomes.",
            "For example, you might have trading agents that need to align the types of queries and types of goods they are providing and by providing, for example, a bookseller is providing secondhand books or information about certain topic.",
            "It might find that overtime.",
            "And is more successful in those types of trades, so of course it's looking at it from an egocentric perspective.",
            "It doesn't necessarily know what are the weights or what are the priorities of the other agent, so we might end up finding that one agent that's trying to misrepresent its weights in order to force a certain type of alignment may then result in the whole alignment negotiation process failing.",
            "So ideally what we'd like to do is find mechanisms that disincentivize agents from lying.",
            "We want them to be truth telling.",
            "So in other words, we want to show that a game theoretic mechanism will actually do worse if the agents bid strategically, the agents will do better if they are always truth telling and that's the."
        ],
        [
            "Active here now we know that there are mechanisms already out there that will support this within, for example, combinatorial auctions.",
            "There's the classic Vickrey Clarke Groves.",
            "Warranties that agents will always do better if they tell the truth in declaring their values of something and will actually do worse if they try to shade those values by either declaring them as higher or lower in the belief that they could personally do well.",
            "But remember, they're working in incomplete information, So what we want to analyze with this type of problem is it possible to have a faster algorithm that's not optimal, so we want to sub optimal solution?",
            "That is, approximately won't get the perfect solution, but it's still truthful for our problem.",
            "Could use VCG.",
            "It's incredibly complex.",
            "We want something that simple.",
            "And we've modeled this."
        ],
        [
            "By finding an analogy to the notion of having payments within game theory, so we assume that each agent will provide a profile corresponding to the weights for each of the correspondence is that it's interested in and."
        ],
        [
            "There will be then be a mechanism that has an allocation rolanda and allocation rule and the payment scheme.",
            "What this does is it looks at the bids from both the agents regarding the different correspondances and then decides on a solution and it.",
            "In theory it charges.",
            "It doesn't actually charged in theory charges.",
            "The agents for the cost of those correspondences that exist in the solution but then allocates the payment back.",
            "So what we really want in this scenario is to ensure that the agents are effectively utility 0, where utility is determined by the cost of a specific correspondence and the payment it gets back for that.",
            "If you've got a mechanism where an agent potentially could actually yield an additional value.",
            "Then it has a reason to be strategically.",
            "Calculating it can actually therefore misrepresent its values, because it can actually get an advantage from that."
        ],
        [
            "And what we want to do is with this mechanism, I don't maximize the social welfare if we can."
        ],
        [
            "So we've analyzed this theoretically, and we've got the following findings.",
            "So theorem one for the alignment problem that we're considering with this additive function.",
            "Any mechanism that does not adopt an optimal solution where agents declare their true valuations.",
            "It's either non truthful, so it doesn't support truth at all, or if it is truthful than non optimal solution has an approximation ratio of at least two.",
            "So in other words though, you may not get that.",
            "Optimal social welfare.",
            "It'll at least be within a factor of two, be within 50% to 100%, and we've got the proof for that.",
            "And basically we're saying if you have an optical mechanisms, it's either not truthful, but if it is truthful, it has this characteristic of being small of approximation factor smaller than two.",
            "A second fine."
        ],
        [
            "For the alignment problem with payment, any deterministic mechanism that doesn't adopt an optimal solution when the agents declare their true values to valuation, either that is non truthful or it's maximal in range.",
            "So just to clarify, maximum range of mechanism is maximum range.",
            "If there's a fixed subset of solutions that it could actually provide based on the set of input vectors, and actually it will guarantee guaranteed to return the one that maximizes the social welfare with respect to that.",
            "So we're looking at mechanisms that, given those, tries to maximize their.",
            "As social welfare.",
            "Finally, but the alignment problem with payment, the only truthful mechanisms are those that are maximal in range and with an approximation of at least two.",
            "So we've now characterized the problem from this perspective.",
            "We want to find ideally a solution.",
            "We could use a solution such as VCG, which is optimal, but it's computationally complex, so we want to find solutions that are non optimal but faster where agents can do no better than being truthful.",
            "Because their maximum range, but there are at least 50% of optimal, so they're not so bad."
        ],
        [
            "On a classic example of that would actually be a greedy algorithm.",
            "So what I've got here is a decentralized greedy algorithm which is actually based on the one published five Mel occurrence to consume it back in 2007, where what we do is we actually consider a set of bids from each of the two agents.",
            "We then for each of the correspondence is add the contribution of the left agent and the right agent to determine a cumulative value for each of those edges, and then we greedily go through and we find.",
            "Which edge has the highest combined value?",
            "OK, well add that to the matching and then remove any other edges that are adjacent to that.",
            "And we repeat that and repeat that to find a greedy solution.",
            "And we know it's computationally efficient.",
            "It's got complexity of an squared, for example.",
            "And we've taken that, and we've actually then analyzed it within a day."
        ],
        [
            "Centralized setting because we're interested in well, if you've got these agents, do they converge on trying to find any other solutions that could potentially benefit them?",
            "Or do they actually find the Nash equilibrium so to remind you and Nash equilibrium is where if agent one is aware that agent two has a certain strategy and based on Agent Sue strategy Agent 1 plays its strategy and Conversely Agent who will play that strategy because it's aware that agent one player strategy.",
            "And they can do no better than doing that, so we have almost the perfect scenario where the agents will not ever want to deviate from taking those strategies.",
            "And these strategies should be truth telling.",
            "And we've calculated from that we've analyzed that looking specifically, both at the price of anarchy and in the paper, we talk about the price of stability, and we've shown that.",
            "The price of anarchy, which again is the ratio but now for within the Nash equilibrium.",
            "How does the solution compare to the optimal solution?",
            "And we found that the price of anarchy of the first price greedy matching is exactly 4 based on two other theorem that we've actually proved.",
            "So we've now para."
        ],
        [
            "Authorize that so we know that within an ash perspective, in the decentralized view, you've got at least, well, you've got the performance compared to optimal of exactly 4.",
            "So to conclude this paper.",
            "Basically, we've been looking at matchings because it's central to any form of alignment system.",
            "We're trying to find these one one to one injective mappings in most cases, but we also need to do this within a decentralized approach where agents are coming together autonomously and trying to."
        ],
        [
            "Then somehow negotiate.",
            "But of course because it's decentralized, agents can be strategic.",
            "So we've looked at this whole problem from the classic game theory perspective, which is typically used when looking at how agents can be strategic, and we've shown that actually we have a number of properties.",
            "We know those properties that need to exist in order for our mechanism to be truthful, and therefore there's no need to be strategic.",
            "They just simply declare their values.",
            "We know that they can do no better than being honest.",
            "With the greedy additive algorithm, and we've looked at the certain characteristics when we're considering these being deployed within a full game and finding Nash equilibrium will actually therefore finish, thank you.",
            "Yeah, you have some question not necessarily related to ontology alignment there.",
            "First, it was a really well rendered talk.",
            "I really like the way you did it.",
            "I have a question because you have a very mechanistically way to set the prices that they will have to pay.",
            "But in real life, this is not necessarily the case, and that you can.",
            "How I don't remember the other way to say it in English, but you can get away with lying sometimes and so so did you think about any other way to set the price and so I can think about one which is basically that having not only the agent negotiating the alignment but actually using them and.",
            "And the price being the trust that the other agents have in this one.",
            "That is after after a while you know who's lying, why etc.",
            "So and then you've got very elaborate strategy because you've got to decide on what on what you will lie.",
            "What's more important.",
            "So that's an interesting question.",
            "In many ways, if you're going to factor that in, you could actually argue that that affects the weight of the correspondences themselves, because the weight of the correspondence is from some perspective could be based purely on the ontologies, or it could actually represent.",
            "As I said, the efficacy of using those correspondences within future alignments and part of the problem here is I know that I've got a set of correspondence as I could choose.",
            "I might have a certain preference for a certain set.",
            "And of course I could play that horse.",
            "Sorry, their value in order to say no, you're going to use these.",
            "I'm soldiering.",
            "You want this alignment, OK, you're turning around the same Archie there these other alignments that have value to you and as a consequence, when we calculate the full matching, it actually causes a matching to be computed.",
            "That is actually perhaps unusable or not necessarily conducive to why is there a bus transacting because these values have been inflated?",
            "So what we want is an alignment that actually, because of course I don't know what values younger cousin.",
            "Furthermore, it's hard to compute from a strategic perspective how your values could affect my manipulation, so that's why we're just trying to avoid these and your point about the fact that you could try to shape these values based on the information.",
            "I would argue we want to avoid that being used from a strategy and actually integrate that within the valuation function itself.",
            "Copy and take his own life.",
            "About social welfare yes, OK and social welfare.",
            "Basically nothing to do with groups.",
            "Note right, but in this context, when we talk about social welfare, we're talking about basically the value of all of the edges and optimizing that.",
            "But yes, it's not true.",
            "Back in so from where fair, actually?",
            "The social welfare, when the agent would maximize their game yesterday.",
            "Yes, but we're trying to optimize the joint social welfare rather than the individual, and in a way that's the crux of my earlier analysis and finding that actually trying to find that total social welfare just within alignments actually potentially forgets really good mappings, because it focuses on what is seen globally, but it doesn't focus on the local.",
            "Great so baby.",
            "We will continue this discussion.",
            "Any other question?",
            "Derek, did you experiment with real ontologies out there with you?",
            "Are you approach actually right?",
            "We actually have some preliminary work on this paper itself is theoretical, so it's all been through analysis and proof, but we've also done some preliminary work at starting with centralized approaches where we're looking at the different types of solution, and Interestingly enough, we are starting to identify that sub optimal solutions produce better alignments because of this localized view on the edges.",
            "When now conducting some larger scale experiments where we're looking at the type of Nash equilibrium we can find in the decentralized setting and then evaluating those solutions there.",
            "But we don't have results for that at the moment.",
            "Thanks, so we can conclude decision.",
            "Thanks all.",
            "And thanks to Terry.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name Sir Terry Pain and I am actually here to present work on behalf of my student.",
                    "label": 0
                },
                {
                    "sent": "Non she who couldn't actually make it.",
                    "label": 0
                },
                {
                    "sent": "Today we've been looking at issues to do with finding alignments and specifically finding alignments within decentralized setting.",
                    "label": 0
                },
                {
                    "sent": "My background actually is in semantic web services and the idea of bringing services together where are typically those services are trying to do some form of negotiation when they meet each.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One service provided meets another service provider or consumer, so the outline of the talk I'm going to give today is to look at the notion of matchings.",
                    "label": 1
                },
                {
                    "sent": "These are one to one or injective alignments.",
                    "label": 0
                },
                {
                    "sent": "Assuming this stays stable.",
                    "label": 0
                },
                {
                    "sent": "That you generate when you're forming ontology.",
                    "label": 0
                },
                {
                    "sent": "So it's a classic problem with any form of ontology alignment system, but also it's an issue that we need to consider within the distributed alignment negotiation process.",
                    "label": 0
                },
                {
                    "sent": "So we'll be looking at that scenario as well and show the fact that because you've actually got agents that can actually declare information to their peers, they can act strategically.",
                    "label": 0
                },
                {
                    "sent": "They can potentially choose to misrepresent the information about those correspondences.",
                    "label": 0
                },
                {
                    "sent": "When they're trying to find them in order to potentially gain some form of strategic advance advantage, and we explore this whole problem from a game theoretic perspective, so apologies for that, I'm going to be presenting this salient results from this work.",
                    "label": 1
                },
                {
                    "sent": "All of the proofs can be found in the paper itself.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I'm sure most of you are aware, of course we can use ontologies to model various domains for different systems.",
                    "label": 0
                },
                {
                    "sent": "And of course, if you've got a scenario where there are multiple stakeholders producing data within a common domain, you typically find that there may be multiple ontologies that effectively are overlapping with respective domain, but the way they're encoded is somewhat different, so we need a mechanism that can find the mappings between the entities in these different.",
                    "label": 0
                },
                {
                    "sent": "Ontologies, and that's what an alignment system Ascentia Lee does.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the way traditional many traditional mechanisms do this is by considering the entities in one ontology, the entities in the other ontology, computing some form of pairwise similarity between each of the entities of these two ontologists.",
                    "label": 0
                },
                {
                    "sent": "That could be very trivially based on terminology.",
                    "label": 0
                },
                {
                    "sent": "It could be based on the structure of the ontology.",
                    "label": 0
                },
                {
                    "sent": "There are many different approaches that we could consider, and you potentially end up with a partial or completely connected by Python.",
                    "label": 0
                },
                {
                    "sent": "Bipartite graph, and from that you then want to pick those edges that will allow you to form ideally or one to one mapping between each of the entities.",
                    "label": 1
                },
                {
                    "sent": "Couple of caveats which I can talk about offline, but essentially that's what you're trying to do, so you're trying to form this matching.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On these two.",
                    "label": 0
                },
                {
                    "sent": "From this set of edges that you can form now there are a number of different approaches that you'll find if you look in the literature.",
                    "label": 1
                },
                {
                    "sent": "Often people talk about finding a complete or an optimal solution.",
                    "label": 0
                },
                {
                    "sent": "For example, one that's maximizing social welfare.",
                    "label": 1
                },
                {
                    "sent": "Now what I mean by social welfare is if you assume that each edge has some form of weight, you can find different combinations of these edges and you want to find the one that results in the maximal sum of all of the edge weights.",
                    "label": 0
                },
                {
                    "sent": "Good algorithms out there like the Hungarian, which will find with complexity and cubed.",
                    "label": 0
                },
                {
                    "sent": "Alternatively because often N is big, so you want a faster approach.",
                    "label": 0
                },
                {
                    "sent": "You might use a sub optimal approach to finding these edges.",
                    "label": 0
                },
                {
                    "sent": "For example, you might think of ordering them and using a stable marriage algorithm from the classic, else sharply work of the 1960s.",
                    "label": 0
                },
                {
                    "sent": "Or you might take a greedy approach.",
                    "label": 0
                },
                {
                    "sent": "But Interestingly enough these will produce suboptimal solutions and the question is how suboptimal are they?",
                    "label": 1
                },
                {
                    "sent": "They nearly optimal.",
                    "label": 0
                },
                {
                    "sent": "Are they a far distance from optimal?",
                    "label": 0
                },
                {
                    "sent": "They may be faster, but will they produce approximately similar alignments or completely different alignments?",
                    "label": 0
                },
                {
                    "sent": "And the question is we don't necessary.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really, no.",
                    "label": 0
                },
                {
                    "sent": "Here is an example of the illustrates the problem.",
                    "label": 0
                },
                {
                    "sent": "I've now gotten incredibly simple scenario here where we've got two ontologies with two concepts in each ontology.",
                    "label": 1
                },
                {
                    "sent": "First one has editor and author, the second has writer and contributor.",
                    "label": 0
                },
                {
                    "sent": "And let's assume that we need an alignment between these two ontologies.",
                    "label": 0
                },
                {
                    "sent": "Using some of these edges.",
                    "label": 0
                },
                {
                    "sent": "Well, if you went through the optimal solution.",
                    "label": 1
                },
                {
                    "sent": "Basically by looking at that diagram at the right, we know that the wait for editor to writer is 1.",
                    "label": 0
                },
                {
                    "sent": "The weight for authors contributor is 1.",
                    "label": 0
                },
                {
                    "sent": "Therefore, the maximal matching here would be those two edges E1 and E3, which should give us a total weight of two.",
                    "label": 0
                },
                {
                    "sent": "That's the optimal, but have you notice?",
                    "label": 0
                },
                {
                    "sent": "Actually, this doesn't include the best mapping.",
                    "label": 1
                },
                {
                    "sent": "The best mapping is actually from author to writer, which is modelled here as one plus epsilon.",
                    "label": 0
                },
                {
                    "sent": "So we just gotta value epsilon being an incredibly small value that just pushes it above 1.",
                    "label": 0
                },
                {
                    "sent": "But if you take a greedy approach, that is the mapping that edge that will be selected from our approach.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, the if the worst case scenario here of possible solution is at least one plus epsilon compared to the optimal which is 2.",
                    "label": 0
                },
                {
                    "sent": "So we now within at least a 50% bound.",
                    "label": 0
                },
                {
                    "sent": "Of course, that's taking that view.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everything is centralized where you put your ontologies into a, say in Oracle or an alignment system that computes that the work we're doing is actually more looking at a decentralized scenario where an agent may know a number of different correspondences between this ontology and other ontologies out there.",
                    "label": 0
                },
                {
                    "sent": "These may have been acquired from previous negotiations, so we're not talking to one shot game, we're talking evolution over many different engagements as it were.",
                    "label": 0
                },
                {
                    "sent": "And because you've got a decentralized approach with two agents.",
                    "label": 0
                },
                {
                    "sent": "When they come to declare their information, they can either be naive and simply just declare what they know, or they may be strategic.",
                    "label": 0
                },
                {
                    "sent": "If they believe they could obtain some form of utility or benefit by actually misrepresenting their information, why would an agent actually do this?",
                    "label": 0
                },
                {
                    "sent": "Well, it may well be that.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An agent is trying to somehow impasse transactions, aligned its ontology so it can then transact with another service provider, and it gains some form of goods because you now have that Community communication channel working, and therefore it gains some monetary value.",
                    "label": 0
                },
                {
                    "sent": "And there may be different monetary values you could get for it, depending on who you're transacting with.",
                    "label": 1
                },
                {
                    "sent": "So let's assume for a second that we have these service providers service consumers that have this prior knowledge about.",
                    "label": 1
                },
                {
                    "sent": "Possible correspondence is that they need to select from.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that each of those correspondences have some associated weight that could represent the efficacy.",
                    "label": 0
                },
                {
                    "sent": "When it comes to transacting, it could be based on similarity or various reasons for having not wait, and we assume that these agents will be declaring some value, some bid, which might be the weight itself, or it might be a misrepresentation of the wait.",
                    "label": 1
                },
                {
                    "sent": "We've been analyzing this from our game theory perspective, where we look at the mechanism.",
                    "label": 0
                },
                {
                    "sent": "That will allow these to be combined in some way.",
                    "label": 0
                },
                {
                    "sent": "So the aim here is to take these individual weights for these individual correspondence.",
                    "label": 1
                },
                {
                    "sent": "Says put them together and find some form of solution.",
                    "label": 0
                },
                {
                    "sent": "But analyze those properties so we can say something about the different types of solutions out there so.",
                    "label": 0
                },
                {
                    "sent": "Ideally we'd like to get a an optimal solution, but will accept a sub optimal solution if we can find a mechanism that is potentially faster.",
                    "label": 0
                },
                {
                    "sent": "And again, I have another example here where you can see how if you actually look at the concepts on one side to the other, an edge E1 would actually be calculated purely through an additive function.",
                    "label": 0
                },
                {
                    "sent": "Did add 3 + 3 to get the total weight of that edge as six.",
                    "label": 0
                },
                {
                    "sent": "For example on time.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "We now have gain two solutions, the Optima.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solution here is going to be a value 23 where we have a 1 E 3 E 5 whereas an alternate solution could be that you find edges E2E four with a slightly lower total value, But it's certainly a plausible solution, and it's one that would have been found if we'd made use for example of a greedy algorithm.",
                    "label": 0
                },
                {
                    "sent": "I just want to come back to that lying problem because I'm sure not.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Probably say why would an agent lines in the reviews?",
                    "label": 1
                },
                {
                    "sent": "Well, as I said, prior transactions May typically come up with better outcomes.",
                    "label": 1
                },
                {
                    "sent": "For example, you might have trading agents that need to align the types of queries and types of goods they are providing and by providing, for example, a bookseller is providing secondhand books or information about certain topic.",
                    "label": 0
                },
                {
                    "sent": "It might find that overtime.",
                    "label": 0
                },
                {
                    "sent": "And is more successful in those types of trades, so of course it's looking at it from an egocentric perspective.",
                    "label": 0
                },
                {
                    "sent": "It doesn't necessarily know what are the weights or what are the priorities of the other agent, so we might end up finding that one agent that's trying to misrepresent its weights in order to force a certain type of alignment may then result in the whole alignment negotiation process failing.",
                    "label": 1
                },
                {
                    "sent": "So ideally what we'd like to do is find mechanisms that disincentivize agents from lying.",
                    "label": 0
                },
                {
                    "sent": "We want them to be truth telling.",
                    "label": 0
                },
                {
                    "sent": "So in other words, we want to show that a game theoretic mechanism will actually do worse if the agents bid strategically, the agents will do better if they are always truth telling and that's the.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Active here now we know that there are mechanisms already out there that will support this within, for example, combinatorial auctions.",
                    "label": 0
                },
                {
                    "sent": "There's the classic Vickrey Clarke Groves.",
                    "label": 0
                },
                {
                    "sent": "Warranties that agents will always do better if they tell the truth in declaring their values of something and will actually do worse if they try to shade those values by either declaring them as higher or lower in the belief that they could personally do well.",
                    "label": 0
                },
                {
                    "sent": "But remember, they're working in incomplete information, So what we want to analyze with this type of problem is it possible to have a faster algorithm that's not optimal, so we want to sub optimal solution?",
                    "label": 1
                },
                {
                    "sent": "That is, approximately won't get the perfect solution, but it's still truthful for our problem.",
                    "label": 0
                },
                {
                    "sent": "Could use VCG.",
                    "label": 0
                },
                {
                    "sent": "It's incredibly complex.",
                    "label": 0
                },
                {
                    "sent": "We want something that simple.",
                    "label": 0
                },
                {
                    "sent": "And we've modeled this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By finding an analogy to the notion of having payments within game theory, so we assume that each agent will provide a profile corresponding to the weights for each of the correspondence is that it's interested in and.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There will be then be a mechanism that has an allocation rolanda and allocation rule and the payment scheme.",
                    "label": 1
                },
                {
                    "sent": "What this does is it looks at the bids from both the agents regarding the different correspondances and then decides on a solution and it.",
                    "label": 0
                },
                {
                    "sent": "In theory it charges.",
                    "label": 0
                },
                {
                    "sent": "It doesn't actually charged in theory charges.",
                    "label": 1
                },
                {
                    "sent": "The agents for the cost of those correspondences that exist in the solution but then allocates the payment back.",
                    "label": 0
                },
                {
                    "sent": "So what we really want in this scenario is to ensure that the agents are effectively utility 0, where utility is determined by the cost of a specific correspondence and the payment it gets back for that.",
                    "label": 0
                },
                {
                    "sent": "If you've got a mechanism where an agent potentially could actually yield an additional value.",
                    "label": 0
                },
                {
                    "sent": "Then it has a reason to be strategically.",
                    "label": 0
                },
                {
                    "sent": "Calculating it can actually therefore misrepresent its values, because it can actually get an advantage from that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we want to do is with this mechanism, I don't maximize the social welfare if we can.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've analyzed this theoretically, and we've got the following findings.",
                    "label": 0
                },
                {
                    "sent": "So theorem one for the alignment problem that we're considering with this additive function.",
                    "label": 1
                },
                {
                    "sent": "Any mechanism that does not adopt an optimal solution where agents declare their true valuations.",
                    "label": 1
                },
                {
                    "sent": "It's either non truthful, so it doesn't support truth at all, or if it is truthful than non optimal solution has an approximation ratio of at least two.",
                    "label": 0
                },
                {
                    "sent": "So in other words though, you may not get that.",
                    "label": 0
                },
                {
                    "sent": "Optimal social welfare.",
                    "label": 1
                },
                {
                    "sent": "It'll at least be within a factor of two, be within 50% to 100%, and we've got the proof for that.",
                    "label": 0
                },
                {
                    "sent": "And basically we're saying if you have an optical mechanisms, it's either not truthful, but if it is truthful, it has this characteristic of being small of approximation factor smaller than two.",
                    "label": 0
                },
                {
                    "sent": "A second fine.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the alignment problem with payment, any deterministic mechanism that doesn't adopt an optimal solution when the agents declare their true values to valuation, either that is non truthful or it's maximal in range.",
                    "label": 1
                },
                {
                    "sent": "So just to clarify, maximum range of mechanism is maximum range.",
                    "label": 1
                },
                {
                    "sent": "If there's a fixed subset of solutions that it could actually provide based on the set of input vectors, and actually it will guarantee guaranteed to return the one that maximizes the social welfare with respect to that.",
                    "label": 0
                },
                {
                    "sent": "So we're looking at mechanisms that, given those, tries to maximize their.",
                    "label": 0
                },
                {
                    "sent": "As social welfare.",
                    "label": 0
                },
                {
                    "sent": "Finally, but the alignment problem with payment, the only truthful mechanisms are those that are maximal in range and with an approximation of at least two.",
                    "label": 0
                },
                {
                    "sent": "So we've now characterized the problem from this perspective.",
                    "label": 0
                },
                {
                    "sent": "We want to find ideally a solution.",
                    "label": 0
                },
                {
                    "sent": "We could use a solution such as VCG, which is optimal, but it's computationally complex, so we want to find solutions that are non optimal but faster where agents can do no better than being truthful.",
                    "label": 0
                },
                {
                    "sent": "Because their maximum range, but there are at least 50% of optimal, so they're not so bad.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On a classic example of that would actually be a greedy algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what I've got here is a decentralized greedy algorithm which is actually based on the one published five Mel occurrence to consume it back in 2007, where what we do is we actually consider a set of bids from each of the two agents.",
                    "label": 0
                },
                {
                    "sent": "We then for each of the correspondence is add the contribution of the left agent and the right agent to determine a cumulative value for each of those edges, and then we greedily go through and we find.",
                    "label": 0
                },
                {
                    "sent": "Which edge has the highest combined value?",
                    "label": 0
                },
                {
                    "sent": "OK, well add that to the matching and then remove any other edges that are adjacent to that.",
                    "label": 0
                },
                {
                    "sent": "And we repeat that and repeat that to find a greedy solution.",
                    "label": 0
                },
                {
                    "sent": "And we know it's computationally efficient.",
                    "label": 0
                },
                {
                    "sent": "It's got complexity of an squared, for example.",
                    "label": 0
                },
                {
                    "sent": "And we've taken that, and we've actually then analyzed it within a day.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Centralized setting because we're interested in well, if you've got these agents, do they converge on trying to find any other solutions that could potentially benefit them?",
                    "label": 0
                },
                {
                    "sent": "Or do they actually find the Nash equilibrium so to remind you and Nash equilibrium is where if agent one is aware that agent two has a certain strategy and based on Agent Sue strategy Agent 1 plays its strategy and Conversely Agent who will play that strategy because it's aware that agent one player strategy.",
                    "label": 0
                },
                {
                    "sent": "And they can do no better than doing that, so we have almost the perfect scenario where the agents will not ever want to deviate from taking those strategies.",
                    "label": 0
                },
                {
                    "sent": "And these strategies should be truth telling.",
                    "label": 0
                },
                {
                    "sent": "And we've calculated from that we've analyzed that looking specifically, both at the price of anarchy and in the paper, we talk about the price of stability, and we've shown that.",
                    "label": 0
                },
                {
                    "sent": "The price of anarchy, which again is the ratio but now for within the Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "How does the solution compare to the optimal solution?",
                    "label": 0
                },
                {
                    "sent": "And we found that the price of anarchy of the first price greedy matching is exactly 4 based on two other theorem that we've actually proved.",
                    "label": 1
                },
                {
                    "sent": "So we've now para.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Authorize that so we know that within an ash perspective, in the decentralized view, you've got at least, well, you've got the performance compared to optimal of exactly 4.",
                    "label": 0
                },
                {
                    "sent": "So to conclude this paper.",
                    "label": 0
                },
                {
                    "sent": "Basically, we've been looking at matchings because it's central to any form of alignment system.",
                    "label": 0
                },
                {
                    "sent": "We're trying to find these one one to one injective mappings in most cases, but we also need to do this within a decentralized approach where agents are coming together autonomously and trying to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then somehow negotiate.",
                    "label": 0
                },
                {
                    "sent": "But of course because it's decentralized, agents can be strategic.",
                    "label": 0
                },
                {
                    "sent": "So we've looked at this whole problem from the classic game theory perspective, which is typically used when looking at how agents can be strategic, and we've shown that actually we have a number of properties.",
                    "label": 0
                },
                {
                    "sent": "We know those properties that need to exist in order for our mechanism to be truthful, and therefore there's no need to be strategic.",
                    "label": 1
                },
                {
                    "sent": "They just simply declare their values.",
                    "label": 0
                },
                {
                    "sent": "We know that they can do no better than being honest.",
                    "label": 1
                },
                {
                    "sent": "With the greedy additive algorithm, and we've looked at the certain characteristics when we're considering these being deployed within a full game and finding Nash equilibrium will actually therefore finish, thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you have some question not necessarily related to ontology alignment there.",
                    "label": 0
                },
                {
                    "sent": "First, it was a really well rendered talk.",
                    "label": 0
                },
                {
                    "sent": "I really like the way you did it.",
                    "label": 0
                },
                {
                    "sent": "I have a question because you have a very mechanistically way to set the prices that they will have to pay.",
                    "label": 0
                },
                {
                    "sent": "But in real life, this is not necessarily the case, and that you can.",
                    "label": 0
                },
                {
                    "sent": "How I don't remember the other way to say it in English, but you can get away with lying sometimes and so so did you think about any other way to set the price and so I can think about one which is basically that having not only the agent negotiating the alignment but actually using them and.",
                    "label": 0
                },
                {
                    "sent": "And the price being the trust that the other agents have in this one.",
                    "label": 0
                },
                {
                    "sent": "That is after after a while you know who's lying, why etc.",
                    "label": 0
                },
                {
                    "sent": "So and then you've got very elaborate strategy because you've got to decide on what on what you will lie.",
                    "label": 0
                },
                {
                    "sent": "What's more important.",
                    "label": 0
                },
                {
                    "sent": "So that's an interesting question.",
                    "label": 0
                },
                {
                    "sent": "In many ways, if you're going to factor that in, you could actually argue that that affects the weight of the correspondences themselves, because the weight of the correspondence is from some perspective could be based purely on the ontologies, or it could actually represent.",
                    "label": 0
                },
                {
                    "sent": "As I said, the efficacy of using those correspondences within future alignments and part of the problem here is I know that I've got a set of correspondence as I could choose.",
                    "label": 0
                },
                {
                    "sent": "I might have a certain preference for a certain set.",
                    "label": 0
                },
                {
                    "sent": "And of course I could play that horse.",
                    "label": 0
                },
                {
                    "sent": "Sorry, their value in order to say no, you're going to use these.",
                    "label": 0
                },
                {
                    "sent": "I'm soldiering.",
                    "label": 0
                },
                {
                    "sent": "You want this alignment, OK, you're turning around the same Archie there these other alignments that have value to you and as a consequence, when we calculate the full matching, it actually causes a matching to be computed.",
                    "label": 0
                },
                {
                    "sent": "That is actually perhaps unusable or not necessarily conducive to why is there a bus transacting because these values have been inflated?",
                    "label": 0
                },
                {
                    "sent": "So what we want is an alignment that actually, because of course I don't know what values younger cousin.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, it's hard to compute from a strategic perspective how your values could affect my manipulation, so that's why we're just trying to avoid these and your point about the fact that you could try to shape these values based on the information.",
                    "label": 0
                },
                {
                    "sent": "I would argue we want to avoid that being used from a strategy and actually integrate that within the valuation function itself.",
                    "label": 0
                },
                {
                    "sent": "Copy and take his own life.",
                    "label": 0
                },
                {
                    "sent": "About social welfare yes, OK and social welfare.",
                    "label": 0
                },
                {
                    "sent": "Basically nothing to do with groups.",
                    "label": 0
                },
                {
                    "sent": "Note right, but in this context, when we talk about social welfare, we're talking about basically the value of all of the edges and optimizing that.",
                    "label": 0
                },
                {
                    "sent": "But yes, it's not true.",
                    "label": 0
                },
                {
                    "sent": "Back in so from where fair, actually?",
                    "label": 0
                },
                {
                    "sent": "The social welfare, when the agent would maximize their game yesterday.",
                    "label": 0
                },
                {
                    "sent": "Yes, but we're trying to optimize the joint social welfare rather than the individual, and in a way that's the crux of my earlier analysis and finding that actually trying to find that total social welfare just within alignments actually potentially forgets really good mappings, because it focuses on what is seen globally, but it doesn't focus on the local.",
                    "label": 0
                },
                {
                    "sent": "Great so baby.",
                    "label": 0
                },
                {
                    "sent": "We will continue this discussion.",
                    "label": 0
                },
                {
                    "sent": "Any other question?",
                    "label": 0
                },
                {
                    "sent": "Derek, did you experiment with real ontologies out there with you?",
                    "label": 0
                },
                {
                    "sent": "Are you approach actually right?",
                    "label": 0
                },
                {
                    "sent": "We actually have some preliminary work on this paper itself is theoretical, so it's all been through analysis and proof, but we've also done some preliminary work at starting with centralized approaches where we're looking at the different types of solution, and Interestingly enough, we are starting to identify that sub optimal solutions produce better alignments because of this localized view on the edges.",
                    "label": 0
                },
                {
                    "sent": "When now conducting some larger scale experiments where we're looking at the type of Nash equilibrium we can find in the decentralized setting and then evaluating those solutions there.",
                    "label": 0
                },
                {
                    "sent": "But we don't have results for that at the moment.",
                    "label": 0
                },
                {
                    "sent": "Thanks, so we can conclude decision.",
                    "label": 0
                },
                {
                    "sent": "Thanks all.",
                    "label": 0
                },
                {
                    "sent": "And thanks to Terry.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}