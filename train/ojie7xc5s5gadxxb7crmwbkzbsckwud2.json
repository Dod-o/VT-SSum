{
    "id": "ojie7xc5s5gadxxb7crmwbkzbsckwud2",
    "title": "A Corpus for Complex Question Answering over Knowledge Graphs",
    "info": {
        "author": [
            "Priyansh Trivedi, Institute of Computer Science, University of Bonn"
        ],
        "published": "Nov. 28, 2017",
        "recorded": "October 2017",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2017_trivedi_knowledge_graphs/",
    "segmentation": [
        [
            "Hi everyone, I'm French.",
            "I'm currently doing my Masters at the Uni One and I work in the Smart Data Analytics Lab under Professor Jens Lehmann where we made this question answering data set earlier."
        ],
        [
            "And I'm going to be talking about this data set and quickly go over question answering and.",
            "And we want to talk about what led to this creation of this data set.",
            "Why do we do it?",
            "Move on to some character characteristics of this data set and the process that we used to create it, and then finally talk about some discrete things that might be of relevance to this audience.",
            "So moving on."
        ],
        [
            "Question answering over knowledge graphs, which is the filter which this data set pertains, is, loosely speaking, a field in which one tries to convert a natural language question into a formal language wherein we could finally get the entity which the question would point to.",
            "For instance, who is the President of United States would ideally point to the entity Donald Trump in our knowledge base, and that's essentially what this field is about, and this field has seen tremendous improvements and advancements in the past five years.",
            "They've been techniques coming left, right, and center.",
            "And it's currently thriving and the state of the art is being beat every 3 four months.",
            "So it's quite exciting to work in this field at this moment.",
            "And that last point is the premise of our."
        ],
        [
            "Deviation so all of this.",
            "All of this work is based on our belief that challenges, when set in this field, can then be overcome, which I would also like to refer to as."
        ],
        [
            "I said procedure research.",
            "Guessing point in 2013 Web questions data Set was released over Freebase and right after that people started using very interesting very creative techniques to come up with the different solutions for the question answering problem and the state of the art.",
            "Right now standard 69% by the neural programming interpreter paper if I'm correct.",
            "Other example would be the quad or question answering over linked data challenge that.",
            "I'm sure a lot of people here would be familiar with, so this has been going on for the past eight years incrementally and over the years.",
            "This challenge, as the multilingual track of this challenge, has invited over 38 submissions and more, most of them being different and very interesting methods to solve the question.",
            "Answering problem.",
            "So that's our goal.",
            "That's that.",
            "That's what we want to do.",
            "We want to help people to challenge themselves to make a new.",
            "Question answering system and to improve the state of the art.",
            "Better to make this field more attuned to a practical usage and well, there's also the fact that over the period there are not a lot of large quick question answering datasets.",
            "So with the current size alot of machine learning techniques are thrown out of the window, and that's something that we want to rectify in the long run.",
            "And yes, the last."
        ],
        [
            "Could be that the traditional data set generation methods are gold standard data.",
            "Set generation methods are very time consuming and difficult to scale.",
            "Sam and."
        ],
        [
            "These are some of the very popular datasets over question answering, so there's web questions in 3917.",
            "Both of them came out in 2013, having 917 and 5800 questions respectively, and these datasets are over Freebase.",
            "The next two datasets, simple Questions, 30 million factoid questions, boast an impressive size, and they have been used by countless research papers, and because of their size, but these datasets have this thing that they're all simple questions.",
            "A simple questions in this context means the question which would pertain to one single triple in a knowledge base.",
            "For instance, who's the President of United States.",
            "But a complex question, in contrast, would be who is the wife of the President of the United States, which is a question that these datasets would not have.",
            "So solving this, why is it difficult?",
            "Problem is arguably simpler than solving complex questions, and the last in this table is a cold data set.",
            "As I said with 450 questions over DB pedia.",
            "And this is Elsie code.",
            "This is our data set with 5000 questions.",
            "Every question has its corresponding sparkle in the data set an A lot of these questions are complex and varied.",
            "And yes, it's over DB pedia.",
            "And that's my presentation is going to be about LC code."
        ],
        [
            "So at a glance else, God has complex questions.",
            "Every question has it Sparkle query in there a lot of questions are Boolean or aggregate based questions, so they'll be questions like is Donald Trump, the President of United States in there will be questions like how many presidents has there been in United States and so on and so forth.",
            "And it's a gold standard data set.",
            "So we created every question manually.",
            "An extensible so we could increase the data set, but both in size of the current mode of questions, as well as increase the complexity of the questions in the future, and I hope at the end of the presentation you will agree this is awesome.",
            "So yeah, moving on."
        ],
        [
            "How did we come up with this data set?",
            "So 5000?",
            "Question is Immanuel is quite a difficult amount to reach.",
            "And it takes a lot of perseverance and what we did in our process was to make the process a lot more easier and I'd like."
        ],
        [
            "You start that by creating a contrast between the traditional methods and then ours.",
            "So typically one would create or collect a bunch of natural language questions and then go about writing this sparkle manually.",
            "And this is what the."
        ],
        [
            "Process typically looks like, so your question name someone influenced by Gerald Tokine who won the Hugo Award.",
            "The answer is Stephen King, by the way.",
            "In case of DB Pedia and then one would go through the DB pedia manually annotate these entities and the predicates and then try to parse it and then create the corresponding sparkle queries.",
            "So this process is quite time consuming and difficult because the person doing that would need to understand that the PDF schema would need to understand sparkle and how does Sparkle work would since this conversion is from natural language.",
            "Formal language, so you can't afford typos you can't afford missing column or missing word or anything like that, because the formal language would not return an answer.",
            "And yes, of course he needs.",
            "Once it's done, the natural language, which in this case is English.",
            "So let me tweak things around here a bit, so let us assume."
        ],
        [
            "That we instead start with the sparkle query.",
            "We somehow get them and then we want to convert them to the natural language.",
            "In doing so, we would lose these prerequisite requisites, so one no longer needs to understand what the deal looks like just by looking at the sparkle query.",
            "One could transform that into the question.",
            "And yes, since the transformation is to natural language, so you can afford a bit of typos because you expect the system that will be using this data set to be resilient to some some small errors or missing space or something.",
            "So I'm going along this line."
        ],
        [
            "Let me just also assume that that Sparkle Query can automatically be converted into an intermediary.",
            "Text which looks like a question, but is typically grammatically incorrect, but this conversion is automatic and then from this automatic thing to the final question is quite an easy job.",
            "We just have to do a bit of grammar correction, so in this process we would lose this prerequisite as well that one needs to understand sparkle.",
            "It's a simple grammar correction step that would yield the final question, and that's basically what we do in our data set.",
            "We start with a bunch of sparkle queries.",
            "We automatically convert them to something which looks like this, and then we manually correct them to yield a final answer or a final."
        ],
        [
            "Russian um.",
            "But in doing so so."
        ],
        [
            "Obviously this allows us to scale up because we can afford errors and we don't need domain expertise and we could create a lot more questions in the very same time, and that's how we created our data set and but yeah, in doing so."
        ],
        [
            "So we took these two assumptions.",
            "That is, we automatically have a bunch of sparkle queries and then we could automatically convert them to that intermediary question and I'm going to go over these assumptions now.",
            "So to get a bunch of sparkle queries, we start with templates.",
            "We have 30 three sparkle."
        ],
        [
            "Templates that we wrote by hand, and so this is basically sparkle query with which has a triple butter instead of actual actual property properties and entities there.",
            "So if I were to visualize this, this is what this."
        ],
        [
            "Scribble pattern would look like you have a central entity with which is the answer that we don't know and there are two entities connected by two predicates in there.",
            "So then we populate this sparkle queries and we do so by collecting a bunch of entities."
        ],
        [
            "Eagle seed entities say we take the entity thinking and we say that this is going to be the answer to the questions I'm going to generate, and then we collect."
        ],
        [
            "Who hops up graph around this entity from DB Pedia?",
            "And in this sub graph we superimpose our sparkle."
        ],
        [
            "Purple pattern.",
            "And simply."
        ],
        [
            "Get this sparkle query so we have a bunch of seed entities and then we create generate the sub graph and then that's how we populate a sparkle queries an yeah.",
            "So this solves the first problem."
        ],
        [
            "And how do we actually convert them to the intermediary questions?",
            "Well, that's template template based as."
        ],
        [
            "So the idea is to reach from this query to this question who was influenced by Gerardo Keenan awarded the Hugo Award?",
            "Will not rooting for grammatical correctness.",
            "We just rooting for easily converting is easy conversion which need not be grammatically perfect.",
            "You can have some misplaced clause.",
            "You can have different tenses and everything so that process query pretty simple."
        ],
        [
            "As well, we have a question.",
            "Templates corresponding to every sparkle templates.",
            "We typically have four to five question templates in order to create more diverse data set.",
            "But yeah, these we created by hand as well, so using these templates we just Simply put in the entities and the predicates that were in the sparkle query Ann."
        ],
        [
            "Get this.",
            "So that's how we generate this intermediate questions and then the process is to convert them to the actual question.",
            "And that's pretty much our data generation process.",
            "As I do point out, the first thing was automatic an."
        ],
        [
            "This last thing is the manual work that has to be done for every question.",
            "So this manual work was done in a two step."
        ],
        [
            "This within the person would first convert them into a proper question.",
            "You would try to fix the clauses, make the grammar correct.",
            "Typically paraphrase some of the questions and the second one is the reviewer, which would see if there are some problems in the in.",
            "The correction would tweak some things around an in order to maintain a high quality of the data set.",
            "The second step is optional, but we did that because we were rooting for a high quality question data set in SQL."
        ],
        [
            "So this is what the pipeline looks like at a glance.",
            "We start with a bunch of answers and we create the sub graph.",
            "We create a valid SPARQL queries from the sub graph which we then finally convert to the question templates and then go through the two step process to finally get the question.",
            "So that's pretty much it.",
            "I'll finally be going over some small discreet."
        ],
        [
            "Topics that I think would be interesting for this community.",
            "So the foremost is the characteristics of the data set.",
            "So we have 5000 questions in our data set spread over 33 different sparkle templates out which only 18% are simple questions.",
            "Now that's a fact that I'd like to point out more, because all of the previously mentioned datasets, whether we called web questions, simple questions or the 30 million factoid questions, all of them have more than 50 to 60% questions, which have a single triple.",
            "So very simple questions.",
            "Whereas we have only 18%, so you'll encounter a very weird two triple or three people on questions, some of them being quite a challenge to solve by the current methods, so that I hope will set precedence for interesting techniques to come up in this field.",
            "Every question has approximately 12.29 words.",
            "It's the data set is based on the April 16 version of DB Pedia.",
            "An as of now it has over 150 downloads on the hosting site.",
            "So yeah."
        ],
        [
            "Moving on, there was a problem that we face that if we were to simple repose every possible triples on while creating this sparkle, we would end up with five 10,000 sparkle queries for a single sub graph.",
            "That would mean that all of our all of our questions would, let's say, would lead to the answer Stephen King.",
            "And that's not what we want obviously.",
            "So we use a bunch of filters in the sense that we would stochastically prune out alot of triples.",
            "We would use a white list of predicates too.",
            "Further, reduce the size to make it manageable by for the manual work.",
            "And so yeah, finally there are some limitations in it.",
            "The project is still in very nascent stage."
        ],
        [
            "As we don't have any literals in the questions, we don't have conditional aggregates.",
            "No unions, no optional queries, and no other scope questions.",
            "And."
        ],
        [
            "In the future, the most urgent thing that we're going to do is to create baselines over this data set, because we don't have any because of some reasons, namely the fact that there are a lot of complex questions, and most of the tools of question answering are based on more modern version of DB pedia, to which this data set currently does not adhere.",
            "And."
        ],
        [
            "So these are the papers that I preferred in this presentation.",
            "And yeah, so apart from these 5000 do have anymore."
        ],
        [
            "Questions.",
            "Repeat some questions OK one.",
            "One wonders with a one to try to go for questions which are representative of natural questions and they are sort of several reasons to think that these ones would not be.",
            "Yes, yes, representative natural questions in two ways.",
            "Here one is the temperature there.",
            "Well, the generated artificially based on limited data there templated also because the templated there's low inherent task complexity which would make learning to answer these particularly suited for your network techniques.",
            "So how we how I think this is very valuable work in driving the field?",
            "But how might we go towards a set of questions which were reasonable to produce an really representative of the sorts of questions that we should be aiming to be automated systems of the answers?",
            "So for the first part of the question, where the fact that these questions are not particularly representative of actual questions to be asked, what we did was that we created a bunch of, so the seed entities that we build.",
            "From which we build a sub.",
            "Graphs were taken from sources of very popular entities, and some of them are found on a particular Microsoft dump of entities which would adhere to the questions that they collected sometimes in the past.",
            "So that's how we try to make them more relevant.",
            "Be user predicate whitelist that we're also releasing with this data set, which would take only those predicates which would have human human values.",
            "So no metadata and no RDF type.",
            "No nothing like that.",
            "So this is quite a strong predicate whitelist, so that's how we do it.",
            "And the second question is of course important.",
            "How do we?",
            "Stop people from reverse engineering their templates to solve it.",
            "So what we did was that we would use a lot of synonyms for the surface form.",
            "We would twist the clauses around.",
            "We would create a lot of paraphrases to make the reverse engineering process more complex.",
            "So in your template and the examples you showed, your complex queries were pretty much two sets of triples, so.",
            "Um, in this database?",
            "Are there any complex queries that are more complex than just two, so we have a lot of queries which would have an RDF type filter.",
            "Apart from this 2 two triple thing we would have Boolean questions and we would have aggregates in the UI somewhere in there to make process more complex, but generally no.",
            "Generally we would have maximum two or three triple strong questions.",
            "You better work on DB Pedia.",
            "We work a lot with different domains and sectors where you do not have DB pedia.",
            "We have specific concepts.",
            "You only have text documents where you want to retrieve on ontologies from from there.",
            "Blast, it's very difficult to talk to domain experts to get these questions of the ground, so how would you tackle those type of domains with your approach?",
            "So if you go to our dumps you would find that the templates other question templates as well as partial templates are released there as a part of the data set, so you would typically have to create your own predicate whitelist for the for the domain knowledge base and your seed entity list and then run them through the same pipeline.",
            "So the I'll be honest, the project is not particularly one click right now, but it can be easily, easily done and we'd love to help you if you want to go along those lines.",
            "Right, so I would create this estimate very roughly, but it takes approximately 70 to 80 seconds.",
            "To paraphrase one question and another 1020 to review it, so 90 seconds per question Cross 2700 questions, we don't work on the current pedia version and you'd get the answer to that.",
            "Certain similar to their first question, which is that the data set seems to have a very strong bias.",
            "That one can reverse engineer.",
            "Yep, and so I guess I we did similar work and so to avoid the bias.",
            "What we did is we pull.",
            "We put the questions in Mechanical Turk and ask people to write 5 paraphrases of each question, and then we'd never use the Origonal statement right in the corpus.",
            "And so this would allow you to solve get so paraphrases that are what people would actually say, right, right?",
            "Yeah, machine so far would of course, and I mean like the talking thing.",
            "Nobody would say RRR dot talking.",
            "I mean, people would say Tolkien, right?",
            "Probably right now anyway.",
            "Consider doing something.",
            "Thank you, thank you.",
            "Speak again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi everyone, I'm French.",
                    "label": 0
                },
                {
                    "sent": "I'm currently doing my Masters at the Uni One and I work in the Smart Data Analytics Lab under Professor Jens Lehmann where we made this question answering data set earlier.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'm going to be talking about this data set and quickly go over question answering and.",
                    "label": 1
                },
                {
                    "sent": "And we want to talk about what led to this creation of this data set.",
                    "label": 0
                },
                {
                    "sent": "Why do we do it?",
                    "label": 0
                },
                {
                    "sent": "Move on to some character characteristics of this data set and the process that we used to create it, and then finally talk about some discrete things that might be of relevance to this audience.",
                    "label": 0
                },
                {
                    "sent": "So moving on.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Question answering over knowledge graphs, which is the filter which this data set pertains, is, loosely speaking, a field in which one tries to convert a natural language question into a formal language wherein we could finally get the entity which the question would point to.",
                    "label": 1
                },
                {
                    "sent": "For instance, who is the President of United States would ideally point to the entity Donald Trump in our knowledge base, and that's essentially what this field is about, and this field has seen tremendous improvements and advancements in the past five years.",
                    "label": 1
                },
                {
                    "sent": "They've been techniques coming left, right, and center.",
                    "label": 0
                },
                {
                    "sent": "And it's currently thriving and the state of the art is being beat every 3 four months.",
                    "label": 0
                },
                {
                    "sent": "So it's quite exciting to work in this field at this moment.",
                    "label": 0
                },
                {
                    "sent": "And that last point is the premise of our.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Deviation so all of this.",
                    "label": 0
                },
                {
                    "sent": "All of this work is based on our belief that challenges, when set in this field, can then be overcome, which I would also like to refer to as.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I said procedure research.",
                    "label": 0
                },
                {
                    "sent": "Guessing point in 2013 Web questions data Set was released over Freebase and right after that people started using very interesting very creative techniques to come up with the different solutions for the question answering problem and the state of the art.",
                    "label": 0
                },
                {
                    "sent": "Right now standard 69% by the neural programming interpreter paper if I'm correct.",
                    "label": 0
                },
                {
                    "sent": "Other example would be the quad or question answering over linked data challenge that.",
                    "label": 1
                },
                {
                    "sent": "I'm sure a lot of people here would be familiar with, so this has been going on for the past eight years incrementally and over the years.",
                    "label": 0
                },
                {
                    "sent": "This challenge, as the multilingual track of this challenge, has invited over 38 submissions and more, most of them being different and very interesting methods to solve the question.",
                    "label": 0
                },
                {
                    "sent": "Answering problem.",
                    "label": 0
                },
                {
                    "sent": "So that's our goal.",
                    "label": 0
                },
                {
                    "sent": "That's that.",
                    "label": 0
                },
                {
                    "sent": "That's what we want to do.",
                    "label": 0
                },
                {
                    "sent": "We want to help people to challenge themselves to make a new.",
                    "label": 0
                },
                {
                    "sent": "Question answering system and to improve the state of the art.",
                    "label": 1
                },
                {
                    "sent": "Better to make this field more attuned to a practical usage and well, there's also the fact that over the period there are not a lot of large quick question answering datasets.",
                    "label": 0
                },
                {
                    "sent": "So with the current size alot of machine learning techniques are thrown out of the window, and that's something that we want to rectify in the long run.",
                    "label": 0
                },
                {
                    "sent": "And yes, the last.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Could be that the traditional data set generation methods are gold standard data.",
                    "label": 0
                },
                {
                    "sent": "Set generation methods are very time consuming and difficult to scale.",
                    "label": 1
                },
                {
                    "sent": "Sam and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are some of the very popular datasets over question answering, so there's web questions in 3917.",
                    "label": 0
                },
                {
                    "sent": "Both of them came out in 2013, having 917 and 5800 questions respectively, and these datasets are over Freebase.",
                    "label": 0
                },
                {
                    "sent": "The next two datasets, simple Questions, 30 million factoid questions, boast an impressive size, and they have been used by countless research papers, and because of their size, but these datasets have this thing that they're all simple questions.",
                    "label": 0
                },
                {
                    "sent": "A simple questions in this context means the question which would pertain to one single triple in a knowledge base.",
                    "label": 0
                },
                {
                    "sent": "For instance, who's the President of United States.",
                    "label": 0
                },
                {
                    "sent": "But a complex question, in contrast, would be who is the wife of the President of the United States, which is a question that these datasets would not have.",
                    "label": 0
                },
                {
                    "sent": "So solving this, why is it difficult?",
                    "label": 0
                },
                {
                    "sent": "Problem is arguably simpler than solving complex questions, and the last in this table is a cold data set.",
                    "label": 0
                },
                {
                    "sent": "As I said with 450 questions over DB pedia.",
                    "label": 0
                },
                {
                    "sent": "And this is Elsie code.",
                    "label": 0
                },
                {
                    "sent": "This is our data set with 5000 questions.",
                    "label": 0
                },
                {
                    "sent": "Every question has its corresponding sparkle in the data set an A lot of these questions are complex and varied.",
                    "label": 0
                },
                {
                    "sent": "And yes, it's over DB pedia.",
                    "label": 0
                },
                {
                    "sent": "And that's my presentation is going to be about LC code.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So at a glance else, God has complex questions.",
                    "label": 1
                },
                {
                    "sent": "Every question has it Sparkle query in there a lot of questions are Boolean or aggregate based questions, so they'll be questions like is Donald Trump, the President of United States in there will be questions like how many presidents has there been in United States and so on and so forth.",
                    "label": 1
                },
                {
                    "sent": "And it's a gold standard data set.",
                    "label": 0
                },
                {
                    "sent": "So we created every question manually.",
                    "label": 0
                },
                {
                    "sent": "An extensible so we could increase the data set, but both in size of the current mode of questions, as well as increase the complexity of the questions in the future, and I hope at the end of the presentation you will agree this is awesome.",
                    "label": 0
                },
                {
                    "sent": "So yeah, moving on.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How did we come up with this data set?",
                    "label": 0
                },
                {
                    "sent": "So 5000?",
                    "label": 0
                },
                {
                    "sent": "Question is Immanuel is quite a difficult amount to reach.",
                    "label": 0
                },
                {
                    "sent": "And it takes a lot of perseverance and what we did in our process was to make the process a lot more easier and I'd like.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You start that by creating a contrast between the traditional methods and then ours.",
                    "label": 0
                },
                {
                    "sent": "So typically one would create or collect a bunch of natural language questions and then go about writing this sparkle manually.",
                    "label": 1
                },
                {
                    "sent": "And this is what the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Process typically looks like, so your question name someone influenced by Gerald Tokine who won the Hugo Award.",
                    "label": 1
                },
                {
                    "sent": "The answer is Stephen King, by the way.",
                    "label": 0
                },
                {
                    "sent": "In case of DB Pedia and then one would go through the DB pedia manually annotate these entities and the predicates and then try to parse it and then create the corresponding sparkle queries.",
                    "label": 0
                },
                {
                    "sent": "So this process is quite time consuming and difficult because the person doing that would need to understand that the PDF schema would need to understand sparkle and how does Sparkle work would since this conversion is from natural language.",
                    "label": 0
                },
                {
                    "sent": "Formal language, so you can't afford typos you can't afford missing column or missing word or anything like that, because the formal language would not return an answer.",
                    "label": 0
                },
                {
                    "sent": "And yes, of course he needs.",
                    "label": 0
                },
                {
                    "sent": "Once it's done, the natural language, which in this case is English.",
                    "label": 0
                },
                {
                    "sent": "So let me tweak things around here a bit, so let us assume.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we instead start with the sparkle query.",
                    "label": 0
                },
                {
                    "sent": "We somehow get them and then we want to convert them to the natural language.",
                    "label": 0
                },
                {
                    "sent": "In doing so, we would lose these prerequisite requisites, so one no longer needs to understand what the deal looks like just by looking at the sparkle query.",
                    "label": 0
                },
                {
                    "sent": "One could transform that into the question.",
                    "label": 0
                },
                {
                    "sent": "And yes, since the transformation is to natural language, so you can afford a bit of typos because you expect the system that will be using this data set to be resilient to some some small errors or missing space or something.",
                    "label": 0
                },
                {
                    "sent": "So I'm going along this line.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me just also assume that that Sparkle Query can automatically be converted into an intermediary.",
                    "label": 0
                },
                {
                    "sent": "Text which looks like a question, but is typically grammatically incorrect, but this conversion is automatic and then from this automatic thing to the final question is quite an easy job.",
                    "label": 0
                },
                {
                    "sent": "We just have to do a bit of grammar correction, so in this process we would lose this prerequisite as well that one needs to understand sparkle.",
                    "label": 0
                },
                {
                    "sent": "It's a simple grammar correction step that would yield the final question, and that's basically what we do in our data set.",
                    "label": 0
                },
                {
                    "sent": "We start with a bunch of sparkle queries.",
                    "label": 0
                },
                {
                    "sent": "We automatically convert them to something which looks like this, and then we manually correct them to yield a final answer or a final.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Russian um.",
                    "label": 0
                },
                {
                    "sent": "But in doing so so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Obviously this allows us to scale up because we can afford errors and we don't need domain expertise and we could create a lot more questions in the very same time, and that's how we created our data set and but yeah, in doing so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we took these two assumptions.",
                    "label": 0
                },
                {
                    "sent": "That is, we automatically have a bunch of sparkle queries and then we could automatically convert them to that intermediary question and I'm going to go over these assumptions now.",
                    "label": 0
                },
                {
                    "sent": "So to get a bunch of sparkle queries, we start with templates.",
                    "label": 0
                },
                {
                    "sent": "We have 30 three sparkle.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Templates that we wrote by hand, and so this is basically sparkle query with which has a triple butter instead of actual actual property properties and entities there.",
                    "label": 0
                },
                {
                    "sent": "So if I were to visualize this, this is what this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scribble pattern would look like you have a central entity with which is the answer that we don't know and there are two entities connected by two predicates in there.",
                    "label": 0
                },
                {
                    "sent": "So then we populate this sparkle queries and we do so by collecting a bunch of entities.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eagle seed entities say we take the entity thinking and we say that this is going to be the answer to the questions I'm going to generate, and then we collect.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who hops up graph around this entity from DB Pedia?",
                    "label": 0
                },
                {
                    "sent": "And in this sub graph we superimpose our sparkle.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Purple pattern.",
                    "label": 0
                },
                {
                    "sent": "And simply.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get this sparkle query so we have a bunch of seed entities and then we create generate the sub graph and then that's how we populate a sparkle queries an yeah.",
                    "label": 0
                },
                {
                    "sent": "So this solves the first problem.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And how do we actually convert them to the intermediary questions?",
                    "label": 0
                },
                {
                    "sent": "Well, that's template template based as.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea is to reach from this query to this question who was influenced by Gerardo Keenan awarded the Hugo Award?",
                    "label": 1
                },
                {
                    "sent": "Will not rooting for grammatical correctness.",
                    "label": 0
                },
                {
                    "sent": "We just rooting for easily converting is easy conversion which need not be grammatically perfect.",
                    "label": 0
                },
                {
                    "sent": "You can have some misplaced clause.",
                    "label": 0
                },
                {
                    "sent": "You can have different tenses and everything so that process query pretty simple.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As well, we have a question.",
                    "label": 0
                },
                {
                    "sent": "Templates corresponding to every sparkle templates.",
                    "label": 0
                },
                {
                    "sent": "We typically have four to five question templates in order to create more diverse data set.",
                    "label": 1
                },
                {
                    "sent": "But yeah, these we created by hand as well, so using these templates we just Simply put in the entities and the predicates that were in the sparkle query Ann.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get this.",
                    "label": 0
                },
                {
                    "sent": "So that's how we generate this intermediate questions and then the process is to convert them to the actual question.",
                    "label": 0
                },
                {
                    "sent": "And that's pretty much our data generation process.",
                    "label": 0
                },
                {
                    "sent": "As I do point out, the first thing was automatic an.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This last thing is the manual work that has to be done for every question.",
                    "label": 0
                },
                {
                    "sent": "So this manual work was done in a two step.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This within the person would first convert them into a proper question.",
                    "label": 0
                },
                {
                    "sent": "You would try to fix the clauses, make the grammar correct.",
                    "label": 1
                },
                {
                    "sent": "Typically paraphrase some of the questions and the second one is the reviewer, which would see if there are some problems in the in.",
                    "label": 0
                },
                {
                    "sent": "The correction would tweak some things around an in order to maintain a high quality of the data set.",
                    "label": 0
                },
                {
                    "sent": "The second step is optional, but we did that because we were rooting for a high quality question data set in SQL.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is what the pipeline looks like at a glance.",
                    "label": 0
                },
                {
                    "sent": "We start with a bunch of answers and we create the sub graph.",
                    "label": 0
                },
                {
                    "sent": "We create a valid SPARQL queries from the sub graph which we then finally convert to the question templates and then go through the two step process to finally get the question.",
                    "label": 1
                },
                {
                    "sent": "So that's pretty much it.",
                    "label": 0
                },
                {
                    "sent": "I'll finally be going over some small discreet.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Topics that I think would be interesting for this community.",
                    "label": 0
                },
                {
                    "sent": "So the foremost is the characteristics of the data set.",
                    "label": 0
                },
                {
                    "sent": "So we have 5000 questions in our data set spread over 33 different sparkle templates out which only 18% are simple questions.",
                    "label": 0
                },
                {
                    "sent": "Now that's a fact that I'd like to point out more, because all of the previously mentioned datasets, whether we called web questions, simple questions or the 30 million factoid questions, all of them have more than 50 to 60% questions, which have a single triple.",
                    "label": 0
                },
                {
                    "sent": "So very simple questions.",
                    "label": 0
                },
                {
                    "sent": "Whereas we have only 18%, so you'll encounter a very weird two triple or three people on questions, some of them being quite a challenge to solve by the current methods, so that I hope will set precedence for interesting techniques to come up in this field.",
                    "label": 0
                },
                {
                    "sent": "Every question has approximately 12.29 words.",
                    "label": 0
                },
                {
                    "sent": "It's the data set is based on the April 16 version of DB Pedia.",
                    "label": 0
                },
                {
                    "sent": "An as of now it has over 150 downloads on the hosting site.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Moving on, there was a problem that we face that if we were to simple repose every possible triples on while creating this sparkle, we would end up with five 10,000 sparkle queries for a single sub graph.",
                    "label": 0
                },
                {
                    "sent": "That would mean that all of our all of our questions would, let's say, would lead to the answer Stephen King.",
                    "label": 0
                },
                {
                    "sent": "And that's not what we want obviously.",
                    "label": 0
                },
                {
                    "sent": "So we use a bunch of filters in the sense that we would stochastically prune out alot of triples.",
                    "label": 0
                },
                {
                    "sent": "We would use a white list of predicates too.",
                    "label": 0
                },
                {
                    "sent": "Further, reduce the size to make it manageable by for the manual work.",
                    "label": 0
                },
                {
                    "sent": "And so yeah, finally there are some limitations in it.",
                    "label": 0
                },
                {
                    "sent": "The project is still in very nascent stage.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we don't have any literals in the questions, we don't have conditional aggregates.",
                    "label": 0
                },
                {
                    "sent": "No unions, no optional queries, and no other scope questions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the future, the most urgent thing that we're going to do is to create baselines over this data set, because we don't have any because of some reasons, namely the fact that there are a lot of complex questions, and most of the tools of question answering are based on more modern version of DB pedia, to which this data set currently does not adhere.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the papers that I preferred in this presentation.",
                    "label": 0
                },
                {
                    "sent": "And yeah, so apart from these 5000 do have anymore.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Repeat some questions OK one.",
                    "label": 0
                },
                {
                    "sent": "One wonders with a one to try to go for questions which are representative of natural questions and they are sort of several reasons to think that these ones would not be.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, representative natural questions in two ways.",
                    "label": 0
                },
                {
                    "sent": "Here one is the temperature there.",
                    "label": 0
                },
                {
                    "sent": "Well, the generated artificially based on limited data there templated also because the templated there's low inherent task complexity which would make learning to answer these particularly suited for your network techniques.",
                    "label": 0
                },
                {
                    "sent": "So how we how I think this is very valuable work in driving the field?",
                    "label": 0
                },
                {
                    "sent": "But how might we go towards a set of questions which were reasonable to produce an really representative of the sorts of questions that we should be aiming to be automated systems of the answers?",
                    "label": 0
                },
                {
                    "sent": "So for the first part of the question, where the fact that these questions are not particularly representative of actual questions to be asked, what we did was that we created a bunch of, so the seed entities that we build.",
                    "label": 0
                },
                {
                    "sent": "From which we build a sub.",
                    "label": 0
                },
                {
                    "sent": "Graphs were taken from sources of very popular entities, and some of them are found on a particular Microsoft dump of entities which would adhere to the questions that they collected sometimes in the past.",
                    "label": 0
                },
                {
                    "sent": "So that's how we try to make them more relevant.",
                    "label": 0
                },
                {
                    "sent": "Be user predicate whitelist that we're also releasing with this data set, which would take only those predicates which would have human human values.",
                    "label": 0
                },
                {
                    "sent": "So no metadata and no RDF type.",
                    "label": 0
                },
                {
                    "sent": "No nothing like that.",
                    "label": 0
                },
                {
                    "sent": "So this is quite a strong predicate whitelist, so that's how we do it.",
                    "label": 0
                },
                {
                    "sent": "And the second question is of course important.",
                    "label": 0
                },
                {
                    "sent": "How do we?",
                    "label": 0
                },
                {
                    "sent": "Stop people from reverse engineering their templates to solve it.",
                    "label": 0
                },
                {
                    "sent": "So what we did was that we would use a lot of synonyms for the surface form.",
                    "label": 0
                },
                {
                    "sent": "We would twist the clauses around.",
                    "label": 0
                },
                {
                    "sent": "We would create a lot of paraphrases to make the reverse engineering process more complex.",
                    "label": 0
                },
                {
                    "sent": "So in your template and the examples you showed, your complex queries were pretty much two sets of triples, so.",
                    "label": 0
                },
                {
                    "sent": "Um, in this database?",
                    "label": 0
                },
                {
                    "sent": "Are there any complex queries that are more complex than just two, so we have a lot of queries which would have an RDF type filter.",
                    "label": 0
                },
                {
                    "sent": "Apart from this 2 two triple thing we would have Boolean questions and we would have aggregates in the UI somewhere in there to make process more complex, but generally no.",
                    "label": 0
                },
                {
                    "sent": "Generally we would have maximum two or three triple strong questions.",
                    "label": 0
                },
                {
                    "sent": "You better work on DB Pedia.",
                    "label": 0
                },
                {
                    "sent": "We work a lot with different domains and sectors where you do not have DB pedia.",
                    "label": 0
                },
                {
                    "sent": "We have specific concepts.",
                    "label": 0
                },
                {
                    "sent": "You only have text documents where you want to retrieve on ontologies from from there.",
                    "label": 0
                },
                {
                    "sent": "Blast, it's very difficult to talk to domain experts to get these questions of the ground, so how would you tackle those type of domains with your approach?",
                    "label": 0
                },
                {
                    "sent": "So if you go to our dumps you would find that the templates other question templates as well as partial templates are released there as a part of the data set, so you would typically have to create your own predicate whitelist for the for the domain knowledge base and your seed entity list and then run them through the same pipeline.",
                    "label": 0
                },
                {
                    "sent": "So the I'll be honest, the project is not particularly one click right now, but it can be easily, easily done and we'd love to help you if you want to go along those lines.",
                    "label": 0
                },
                {
                    "sent": "Right, so I would create this estimate very roughly, but it takes approximately 70 to 80 seconds.",
                    "label": 0
                },
                {
                    "sent": "To paraphrase one question and another 1020 to review it, so 90 seconds per question Cross 2700 questions, we don't work on the current pedia version and you'd get the answer to that.",
                    "label": 0
                },
                {
                    "sent": "Certain similar to their first question, which is that the data set seems to have a very strong bias.",
                    "label": 0
                },
                {
                    "sent": "That one can reverse engineer.",
                    "label": 0
                },
                {
                    "sent": "Yep, and so I guess I we did similar work and so to avoid the bias.",
                    "label": 0
                },
                {
                    "sent": "What we did is we pull.",
                    "label": 0
                },
                {
                    "sent": "We put the questions in Mechanical Turk and ask people to write 5 paraphrases of each question, and then we'd never use the Origonal statement right in the corpus.",
                    "label": 0
                },
                {
                    "sent": "And so this would allow you to solve get so paraphrases that are what people would actually say, right, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, machine so far would of course, and I mean like the talking thing.",
                    "label": 0
                },
                {
                    "sent": "Nobody would say RRR dot talking.",
                    "label": 0
                },
                {
                    "sent": "I mean, people would say Tolkien, right?",
                    "label": 0
                },
                {
                    "sent": "Probably right now anyway.",
                    "label": 0
                },
                {
                    "sent": "Consider doing something.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you.",
                    "label": 0
                },
                {
                    "sent": "Speak again.",
                    "label": 0
                }
            ]
        }
    }
}