{
    "id": "s5mxmekf4atlu77sar3crh5tvunizh7w",
    "title": "Efficient high dimensional maximum entropy modeling via symmetric partition functions",
    "info": {
        "author": [
            "Paul Vernaza, Robotics Institute, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Data",
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/machine_vernaza_entropy_modeling/",
    "segmentation": [
        [
            "So I'm Paul for NASA.",
            "So basically the problem we want to solve here is we want to build probabilistic models of continuous high dimensional sequence data.",
            "OK, so for instance is can be motion capture data like illustrate on the left it could be data from computer vision such as we say we want to model the behavior of crowds or it can be sort of data from robotics is illustrated on the right here."
        ],
        [
            "OK, so the basic idea is we're going to take a maximum entropy approach to this problem, so we want to learn the maximum entropy distribution over paths that is consistent with some low dimensional features.",
            "OK, so to give you an illustration of what I mean.",
            "So suppose we have some true distribution which is illustrated by this kind of fuzzy cloud on left and we can sample.",
            "We have samples of the true distribution which are paths and these are training data.",
            "Now based on these paths.",
            "So you want to learn some approximate learn distribution which is illustrated by this.",
            "Lucky Cloud and we assume we can sample that distribution as well, and so the question is what constraints are we going to impose on to learn distribution or maximum entropy model?",
            "And this is the key step, so we're going to constrain our learn distribution such that when we sample pass from it here.",
            "And then we project the pass onto a low dimensional subspace.",
            "And we do the same thing for our training paths.",
            "And we compare these projections.",
            "Then those projections should appear similar.",
            "OK, so the intuition here is, let's say I give you two sets of 100 dimensional paths and I ask you OK, are these paths is drawn from the same distribution or not OK, I think most people what they would do intuitively is they would do something like project both sets of paths onto some common PCA basis and then judge based on the two appearances of the projections.",
            "Are they similar or not?",
            "If they are, then they're probably from the same distribution.",
            "If not, maybe not.",
            "OK, so so this is basically the problem.",
            "This is the intuition behind this, where if we're judging similarity based on features that are a function of some low dimensional projection."
        ],
        [
            "OK, and it turns out that if we assume features of this sort, that basically solves the intractability problem associated with the inference.",
            "For this case.",
            "OK, so normally if you know anything about Max int, the difficult part is always computing the partition function or the normalization factor for this distribution, and in this case there's a dynamic programming algorithm we can employ to basically valuation to compute the partition function, but unfortunately just do NHA evaluation.",
            "The complexity of that is going to scale exponentially.",
            "With the dimensionality of the state space right?",
            "So that definitely won't work if we have a 40 dimensional or 100 dimensional state space.",
            "Now it turns out if we make this assumption that the features are functions of a low dimensional projection, as I kind of described in the last slide.",
            "Then we get an exponential complexity reduction in dynamic programming and basically that arises because the partition function in this case is rotationally symmetric.",
            "OK, so basically we can do this altered form of dynamic programming, where we're constantly like rotate, exploiting the symmetry of the partition function in order to compute in a compressed form."
        ],
        [
            "And this is just a brief example of the kinds of inferences we can solve given the partition function.",
            "We can do things like.",
            "Obviously, if you give me a sequence, I can tell you what is the probability of observing that sequence, giving our model, or if you just give me a single frame of say, motion capture data, I can tell you what's the probability that a path generated by this model ever passed through that individual frame.",
            "So it's a very powerful framework for for solving these types of inferences.",
            "So if you're interested in hearing more details, please come by my talk.",
            "Or sorry my poster tomorrow afternoon.",
            "It's 51.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm Paul for NASA.",
                    "label": 0
                },
                {
                    "sent": "So basically the problem we want to solve here is we want to build probabilistic models of continuous high dimensional sequence data.",
                    "label": 0
                },
                {
                    "sent": "OK, so for instance is can be motion capture data like illustrate on the left it could be data from computer vision such as we say we want to model the behavior of crowds or it can be sort of data from robotics is illustrated on the right here.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the basic idea is we're going to take a maximum entropy approach to this problem, so we want to learn the maximum entropy distribution over paths that is consistent with some low dimensional features.",
                    "label": 1
                },
                {
                    "sent": "OK, so to give you an illustration of what I mean.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have some true distribution which is illustrated by this kind of fuzzy cloud on left and we can sample.",
                    "label": 0
                },
                {
                    "sent": "We have samples of the true distribution which are paths and these are training data.",
                    "label": 0
                },
                {
                    "sent": "Now based on these paths.",
                    "label": 0
                },
                {
                    "sent": "So you want to learn some approximate learn distribution which is illustrated by this.",
                    "label": 0
                },
                {
                    "sent": "Lucky Cloud and we assume we can sample that distribution as well, and so the question is what constraints are we going to impose on to learn distribution or maximum entropy model?",
                    "label": 0
                },
                {
                    "sent": "And this is the key step, so we're going to constrain our learn distribution such that when we sample pass from it here.",
                    "label": 0
                },
                {
                    "sent": "And then we project the pass onto a low dimensional subspace.",
                    "label": 0
                },
                {
                    "sent": "And we do the same thing for our training paths.",
                    "label": 0
                },
                {
                    "sent": "And we compare these projections.",
                    "label": 0
                },
                {
                    "sent": "Then those projections should appear similar.",
                    "label": 0
                },
                {
                    "sent": "OK, so the intuition here is, let's say I give you two sets of 100 dimensional paths and I ask you OK, are these paths is drawn from the same distribution or not OK, I think most people what they would do intuitively is they would do something like project both sets of paths onto some common PCA basis and then judge based on the two appearances of the projections.",
                    "label": 0
                },
                {
                    "sent": "Are they similar or not?",
                    "label": 0
                },
                {
                    "sent": "If they are, then they're probably from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "If not, maybe not.",
                    "label": 0
                },
                {
                    "sent": "OK, so so this is basically the problem.",
                    "label": 0
                },
                {
                    "sent": "This is the intuition behind this, where if we're judging similarity based on features that are a function of some low dimensional projection.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and it turns out that if we assume features of this sort, that basically solves the intractability problem associated with the inference.",
                    "label": 0
                },
                {
                    "sent": "For this case.",
                    "label": 0
                },
                {
                    "sent": "OK, so normally if you know anything about Max int, the difficult part is always computing the partition function or the normalization factor for this distribution, and in this case there's a dynamic programming algorithm we can employ to basically valuation to compute the partition function, but unfortunately just do NHA evaluation.",
                    "label": 0
                },
                {
                    "sent": "The complexity of that is going to scale exponentially.",
                    "label": 0
                },
                {
                    "sent": "With the dimensionality of the state space right?",
                    "label": 0
                },
                {
                    "sent": "So that definitely won't work if we have a 40 dimensional or 100 dimensional state space.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out if we make this assumption that the features are functions of a low dimensional projection, as I kind of described in the last slide.",
                    "label": 0
                },
                {
                    "sent": "Then we get an exponential complexity reduction in dynamic programming and basically that arises because the partition function in this case is rotationally symmetric.",
                    "label": 1
                },
                {
                    "sent": "OK, so basically we can do this altered form of dynamic programming, where we're constantly like rotate, exploiting the symmetry of the partition function in order to compute in a compressed form.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is just a brief example of the kinds of inferences we can solve given the partition function.",
                    "label": 0
                },
                {
                    "sent": "We can do things like.",
                    "label": 0
                },
                {
                    "sent": "Obviously, if you give me a sequence, I can tell you what is the probability of observing that sequence, giving our model, or if you just give me a single frame of say, motion capture data, I can tell you what's the probability that a path generated by this model ever passed through that individual frame.",
                    "label": 0
                },
                {
                    "sent": "So it's a very powerful framework for for solving these types of inferences.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in hearing more details, please come by my talk.",
                    "label": 0
                },
                {
                    "sent": "Or sorry my poster tomorrow afternoon.",
                    "label": 0
                },
                {
                    "sent": "It's 51.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}