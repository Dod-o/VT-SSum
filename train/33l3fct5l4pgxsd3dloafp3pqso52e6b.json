{
    "id": "33l3fct5l4pgxsd3dloafp3pqso52e6b",
    "title": "Mapping Natural Language to Description Logic",
    "info": {
        "author": [
            "Bikash Gyawali, Loria - Lorraine Research Laboratory in Computer Science and its Application"
        ],
        "published": "July 10, 2017",
        "recorded": "May 2017",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2017_gyawali_description_logic/",
    "segmentation": [
        [
            "Hi everyone, thanks for coming.",
            "My name is because Kelly and today I will talk about.",
            "I'll present her talk on how we worked on to produce Description logic exams for natural language text.",
            "So in this case is going to be sentences in English and it is a so I come from CNS Loria in France and it's a joint work with my colleagues Anastasia clear somewhere and volume.",
            "So in this talk.",
            "I will present the mainly this.",
            "I will present a reversible architecture in which we can go from natural language text to description, logic actions and once we have that then we can take the other way around so we can take the description, logic, action as the input end and get back the natural language sentences right."
        ],
        [
            "So just to clarify terms, in literature, in literature we usually use the term semantic parsing to represent.",
            "This is the first direction of work that is going from sentences in English to some formal representation.",
            "In our case, the formal representation is going to be description logic exams, so that is what we differ by semantic parsing and."
        ],
        [
            "Other way around is called generation, right so?",
            "So yeah, so I will present her.",
            "Our work, which does both of these things."
        ],
        [
            "So the context in which we did this research was in terms of a projector in our project.",
            "Partner was here with industry in France, an usually in organizing, so we usually they have a set of what they call the system installation design principle.",
            "So it's set of sentences written in plain text to describe the various parts that should interact with each other, or different entities that participate in an event for some.",
            "A specific task.",
            "So the SMTP texture in this in our use case was some sentences like like this where it says the pipes will be identified by labels here.",
            "So this is one of the specifications they have, so we have this in as the input and from this input we want to derive an output.",
            "In Descripcion logical format, right?",
            "So we want to get the axiom out of it.",
            "Yeah, so there is going to be the semantic parsing task and once we do that, we're going the other way around.",
            "That is the generation task and we will get back the original sentence."
        ],
        [
            "So why would we like to do this?",
            "So for the semantic parsing task, there are a couple of reasons.",
            "First is that instead of having text, if we can have a formal model, like if we can have an action, then it would help us to enrich an existing knowledge base.",
            "But also it would allow to eat once we have that formalism, then we can put it into some reasoning services and get some other.",
            "Benefits of the reasoning services right and the manual.",
            "The IF we approach this in a manual basis is going to be difficult because because of the time it takes and also the human expertise it takes and therefore we want to make it automatic.",
            "And this is also true because in organization the SDP or the text might be changed to update a new principles or whatever.",
            "And then we want to have.",
            "The updated extremes at the same time the other way around.",
            "The 1st Generation we are interested in it because again, the exams are easier for computer to process.",
            "Things like that, but it is difficult for human consumption.",
            "So if we have the generation engine, we can have readable explanation of what is going on with the exams.",
            "And as in our case we will show that the generation task we will use it either means to verify our passing meaning that.",
            "We will analyze how good our parsing was using the generation module.",
            "This will be clear in the in the coming slides."
        ],
        [
            "To outline the talk 1st, I'll present a brief overview of related systems related works in semantic parsing and generation.",
            "I will explain what our contributions are, whatever the new things that we did, then I will present a global overview of the approach.",
            "I will discuss the resources that we used and the techniques were utilized for our task.",
            "I will present the experimental data that we used.",
            "I will present the evaluation we did and we will see the results of them.",
            "And finally, I will talk.",
            "And this presentation with the conclusion."
        ],
        [
            "So for the semantic parsing task, a couple of infected many work have been done, but from our perspective.",
            "What we see is most of the tasks there are either targeting on the derivation of 1st order logic axioms from text, so this is not the case for us because we are targeting our elections or they require parallel data text corpus tool on such alignments from text to two extremes, right?",
            "But in our case we don't have the parallel data texture corpus, so it is not going to be the case 1st.",
            "And since we're talking about semantic parsing in terms of generating extremes to add to the ontology.",
            "Our work is also somewhat related to the ontology Learning learning task.",
            "And in the Ontology learning task are mainly we see that multiple works have been done, but they mainly work with identifying Toms in the in the sentence or identifying concepts or even identifying the hierarchy of concepts, identifying relations between concepts and so on.",
            "So if we look at the diagram here, most of the work they would they would reach until this level, right?",
            "But in our in this task we want to go one level up.",
            "So we want to take the whole sentence as the input and.",
            "The driver axiom that represents the entire meaning of the sentence.",
            "Uh, for?"
        ],
        [
            "Generation again similarly for the generation.",
            "Most of the work.",
            "Either they are require a large handwritten module of resources or they require parallel data text corpus tool on the alignments.",
            "And some walks are like the last one.",
            "Here are also describing.",
            "I'd also describing the generation of sentences from RDF triples, so this is not going to going to be the case for us because one second we don't have the parallel corpus.",
            "And also we want to generate from owl axioms rather than the RDF triples.",
            "So what are the explicit contributions that we make the so the first one is that we take the input at the.",
            "Sentence level and we generate the output at the owl axiom level for the semantic parsing task and the other way around for the generation task.",
            "So if we go back to this figure here like I said earlier, we are working at this level."
        ],
        [
            "Ann and we are going to use the generation module Azure.",
            "As a means of a.",
            "Measuring the accuracy of the semantic path.",
            "So this I will explain later on and once we have the exams from the semantic parsing parser engine, we're going to add those exams to the ontology and we're going to study the effect of it on the ontology loading task.",
            "And finally, we have developed a reversible semantic parser generator module which is modular, can take different grammars of different types, and it is also robust, meaning that it can escape over unknown words to give a partial parse.",
            "Or a partial generation."
        ],
        [
            "So the global approach that looks like this here we begin with the input SFTP, so this is the input text that we are going to process.",
            "We feel this to a semantic parser engine.",
            "The semantic parser engine is going to give a description logic representation of that sentence.",
            "Once we have this description logic exam so we can fill them to the ontology and thereby weakening this knowledge, we can add the output from our semantic parser to the ontology.",
            "But also we can take those actions and feed them to the generator.",
            "Anna asked it to it to give us the text describing the.",
            "Addiction, right?",
            "The motivation here is that we will compare this regenerated SMTP's with the original input side piece and we will use some kind of scoring which is the blue scoring two to identify how close they are and this this measure of closeness is going to be our basis on on determining how good the semantic parse output was.",
            "So it's a cyclic process.",
            "So let me begin with the here.",
            "I forgot to talk about the grammar and the lexicon.",
            "Both the semantic parser and generator.",
            "They take the grammar and the lexicalized the resources.",
            "An these resources are in common.",
            "In our case, the grammar was manually defined, and the lexicon was automatically extracted.",
            "So let me begin the explanation with the resources and then we can.",
            "We will see examples of how we do parsing and generation."
        ],
        [
            "So the grammar, like I said, was manual and we're using the feature based lexicalized tag representation for grammar.",
            "So what that means is that we're going to have the syntactic constructions in terms of represented in terms of trees.",
            "So for example, here we are saying that the word pipes is represented by a noun phrase, and similarly we have other trees in the grammar which describe different syntactic constructs is not only that we're also going to add."
        ],
        [
            "Semantics to the grammar.",
            "So if we look here we have the same grammar, but this time we have added the semantics.",
            "So this is that.",
            "Ah.",
            "The word pipes is NP, but also it has a semantics with the predicate argument structure.",
            "So we have the predicate called pipe and we have an argument which is the variable X in this case, and this is a constant that we assign a name to this structure.",
            "So L6 is, the is the constant that I can refer to the this predicate argument structure here, right?",
            "So in this case.",
            "So in this way we also put a semantics to all the grammar units, so all the trees in our grammar.",
            "And this this so.",
            "At this point we have both the syntax and semantics encoded in the grammar, right?",
            "So yeah, so this is what this makes our grammar in the lexicon."
        ],
        [
            "Now for passing indentation.",
            "Like I said earlier, we're going to use the grammar and the lexicalized resources.",
            "There are three many steps in.",
            "In doing that, first we're going to select some trees from the grammar, so the grammar can have several trees, but depending on the input we are going to use those six of those trees from the cameras are useful, and the basis of selecting them is to check whether they contain the same word as the as the words in the input sentence.",
            "This is true for parsing an or whether they contain the same semantics as as a semantics in the input.",
            "So this is the case for generation and in the second phase.",
            "Once we have selected the trees, we're going to combine all those.",
            "Trees that we selected and finally at the third phase the result of combination of all those trees is going to give us a the parse output or the generation output.",
            "Let's see an example because that will I think."
        ],
        [
            "Clear up things, so here I'm taking a small grammar, the one that I showed earlier.",
            "We have three different trees in this case and I'm trying to parse these two sentences.",
            "The first one is pipe, so not be used, so this is a negation and the other one is the kind of affirmative sentence.",
            "So 5 cell we use.",
            "Let's see for the first example.",
            "I just said during parsing we begin from the words in the in the input sentence and for each word in the input sentence we want to identify some trees in the grammar that mess the corresponding word right.",
            "So let's begin parsing for the first sentence here.",
            "As you can see the."
        ],
        [
            "Water pipes here.",
            "Message to the pipes here.",
            "So we're going to select this tree and therefore we have the semantics from coming from the tree at this.",
            "At this point, right?",
            "So it's at this point we have the semantics represented for the word pipes and similarly."
        ],
        [
            "We are going to match these words with the words contained in the in the tree here, so this tree is going to be selected and now this tree is going to be combined with the previously selected tree here and in doing so the variables are going to be unified.",
            "So what happens is that this variable Y is going to be unified with the with the with the constant.",
            "So it's going to be instantiated with this constant L6.",
            "Here this is true because this NP nor here is going to be substituted at this point.",
            "And therefore the unification ensures that whenever we get the the instantiation of this semantics here, the variable Y is replaced by the appropriate instantiation constant.",
            "So in this case it happens to be L6, right?",
            "So now we have parts for.",
            "Most of the words but one word remains there so."
        ],
        [
            "That I'm going to show now.",
            "So this order is going to select the last three here and again the unification takes place.",
            "The combination takes place and therefore we get this semantics as our parse output, right?",
            "Similarly, for the second sentence."
        ],
        [
            "The the word.",
            "We can see how these words aligned to the words in the tree, and in this case only these two trees are going to be used, so therefore we get this kind of semantics.",
            "So this was the true.",
            "This was the case for passing."
        ],
        [
            "This is by the way, what we call full parse, meaning that for each word in the input sentence we could find some tree in the grammar which content those words right so?",
            "But this is not always going to be the case.",
            "Let me present."
        ],
        [
            "For example, so this time I want to parse the third sentence.",
            "Pipes shall not be used, but assuming that."
        ],
        [
            "This tree is not in the grammar, right?",
            "So we want to pass the third sentence, but we don't have the tree corresponding to the word, not.",
            "So what happens here is that."
        ],
        [
            "We will just assume that the word not in the input sentence was as if it was not in the input at all, so we're going to skip over this word and then for rest of the words were going to select the trees.",
            "So."
        ],
        [
            "This means that again, we're going to make use of the first tree here and the secondary here, and therefore the the.",
            "The pass output that we get at this point is the same as the parse we got from the second example.",
            "It is as if the word not was not really present in the input, so these are examples of."
        ],
        [
            "What we call a partial parts in practice what we do is we try to find we try our best to find all possible methods, But if we can't find some then then we can skip over those words and.",
            "And try the try the parsing algorithm so this is what we call partial path."
        ],
        [
            "For generation to complete the example, so in generation we start from some input semantics and we want to generate output centers.",
            "So assuming that the input semantics is the one like the one shown over here, this time we're going to match the the.",
            "Units of semantics from the input, so to the."
        ],
        [
            "It's in the trees, so.",
            "In this case, we're going to align these semantics with the semantics over here."
        ],
        [
            "The semantics here to the semantics over here."
        ],
        [
            "And like on and finally we at this point we have selected all those trees and after the combination we get output sentence like pipe cell not be used for this.",
            "Input generation input, so this is how the parsing and generation works for us."
        ],
        [
            "Please note that whenever I presented an example here, the semantics I represented was in some kind of flat semantics, so so this was in order to be compatible with our existing system.",
            "What we do next is we define a set of transformation rules that take those flat semantics and give a description logical representation.",
            "So these are the set of all rules that we apply, so the last one might be easier to look.",
            "So this one is saying that if I have a literal.",
            "That expresses a predicate of true for an argument.",
            "So in the flat semantics, then I'm going to say that I have a concept.",
            "I have a concept for that in the description.",
            "Logic representation, right?",
            "So in the other examples that I discussed.",
            "So this thing here.",
            "For example, if we have this, we're going to build a concept called pipe.",
            "So the rest of the rules they work in a fairly easy manner.",
            "It is not very difficult to explore them, but of course I want to eat here.",
            "We don't have much time, but the important thing I want to highlight here is that we have different constructs.",
            "For example, we have the existential quantifier.",
            "We have the intersection, we have the Union and things like that.",
            "We have the negation and inverse and things like that.",
            "So yeah, so at this point we have.",
            "We have we had initiatives to evaluate our work."
        ],
        [
            "So again, coming to the evaluation first.",
            "First we have the data coming from the Airbus Industrie domain, and in this experiment we had 960 SMTP sentences.",
            "What we do is we try to identify.",
            "We tried to split them into two categories.",
            "We call them simple as IDP's and complexities.",
            "This was done manually.",
            "So the idea behind here is that we will consider.",
            "We will consider all the sentences that have a single clause as a simple SMTP and the rest of them are going to be called complexity so they can have more than one clauses or more than one relative relative clauses.",
            "For example in this example.",
            "So we did.",
            "This is written then.",
            "Now I will show the."
        ],
        [
            "Experimental results on these disputed data.",
            "We are going to.",
            "And do the analysis from 3 perspectives.",
            "First, we're going to identify the coverage and the robustness, so the coverage is like for how many of the total inputs did where we actually able to parse them.",
            "The robustness is the measure of the partial parses that I talked about earlier.",
            "So yeah, so there is going to be the first dimension.",
            "The second is about the correctness of the formula that we have obtained from the semantic parse output.",
            "This we analyzed in two directions.",
            "The first is the syntactic correctness and the 2nd is the semantic correctness.",
            "This I will present in the coming slides and finally, I like I said earlier, these axioms that we derive are going to be added to the ontology and we will study their effect in doing so.",
            "Lissa"
        ],
        [
            "Talk about the coverage.",
            "So we can see from from the figure here that.",
            "Far for both the simple SMTP center and the complexity piece, we have a high coverage ratio, meaning that for almost almost all of them we can.",
            "We can pass like we could not pass just for a few cases like 2.5% and for the other side we could parse.",
            "Here the.",
            "The failure comes from the fact that since the lexicon is automatically derived, sometimes the lexicon is attached to the wrong syntactic construct in the grammar, and whenever this happens, the wrongly selected trees.",
            "They cannot combine to each other and therefore we fail to do the parsing.",
            "The partial parses are high.",
            "This is because sometimes we might miss a single word, or it might be the case that we miss a complete clause, for example, or it might also be the case that.",
            "We missed a few words, right?",
            "So these are the cases of partial patches."
        ],
        [
            "Now.",
            "To identify the correctness of the of the parses that we had, we we.",
            "We can see that almost for almost all the passes that we we could get.",
            "So here what I did, what we did is we took the parse output and we ran them against our validation service, so that would tell us whether this is a syntactically correct all formula or not, and so it was the case that for 96% of whatever we could pass those were actually valid.",
            "Ideal formulas the few of them for the 4% of them it is.",
            "We analyzed why it was the case that our parse output was not correct and it happens to be that sometimes the sentences complex sentences actually can be quite very long.",
            "I think it is the longest one was like having 85 words or whatever and then our grammar is not not capable of capturing all those complex syntactic structures.",
            "So this means that.",
            "There is some room for improvement there.",
            "Now too."
        ],
        [
            "To evaluate semantic correctness, what we're going to do is we're going to take the.",
            "We're going to take the blue score like like I mentioned earlier, we're going to compare the original input with the with the reason rated sentences an.",
            "And I didn't even use the blue Score 2 to analyze things so fast.",
            "We categorized 3.",
            "Three different.",
            "Well, it's a range of blue scores the the one scoring less than 32%.",
            "We called them as belonging to the blue score, giving output the one that lie between 33% and 66%.",
            "They are going to be the medium blue scoring output and the rest is going to be in the high Blues output, right?",
            "So here we see that for all the full passes, so this is the start for the full path.",
            "All the full passes that we had whenever we try to resend it on the sentences from them.",
            "We got, we got the sentences in the high category, meaning that the reason little sentence was actually pretty close enough to the original input.",
            "So this signifies to us that our semantic parsing task is correctly capturing the semantics of the input sentence because we were able to resend it nearly to the exact match."
        ],
        [
            "But this is not true for."
        ],
        [
            "It would be different for partial cases.",
            "Here we can see that for the simple side piece, so the ones in pink here.",
            "So for the simple SMTP's most of them they belong to either medium blue score category or to the high risk category.",
            "A few of them they belong to a lowest called Blue category.",
            "This is because because of the inconsistency is coming from the from the lexicon.",
            "So even when the sentences were simple, maybe we didn't find a matching tree for one of each word, and then that significantly hampers the blue scoring.",
            "But on the for the complexity piece here.",
            "We we have a significant percentage of of them not giving a giving a very low blue score, so these are the cases I mentioned earlier like the sentence was really very long with so many clauses in them and maybe we missed when doing the partial on doing the partial passing.",
            "Maybe we missed one or two of the clauses completely right?",
            "So this might be the case for that.",
            "So again here."
        ],
        [
            "Have we have to think about how we can make the grammar?",
            "How we can extend the grammar in fact, to account for those cases?"
        ],
        [
            "Yeah, so now.",
            "So finally we have the actions.",
            "We're also going to add those exams to the ontology and here we have a few interesting observations.",
            "So what we did is from.",
            "We added the new concepts and relations to the ontology whenever they were not already presented there, and in doing so we ensure that such addition does not hamper the consistency and the satisfiability of the ontology.",
            "We see that we identified few many new classes.",
            "Lowers like 900.",
            "35 but also there were we could identify some of the existing classes of the ontology around 90 and a few new object properties that we could propose for the decent with ontology.",
            "And we also identified the subclass based on the subclass of action we could identify which which new concept can be the children are off the existing concept.",
            "Yeah, and so on an.",
            "And here we we see that for for 85% of what we could get from the parse output, we could add them to the ontology.",
            "There were a few redundancies and syntax errors.",
            "Anna in a very rare case we also got inconsistency.",
            "This was because the input FPS, sometime it contains contradictory sentence like one is the negation of the other and both the sentences are also in the input.",
            "So this helps us to identify the sources of.",
            "Error in the input section actually."
        ],
        [
            "To conclude.",
            "We built a modular framework for synchronizing text to formal representations.",
            "Anarchy feature of our approach is that this this whole framework is a reversible engine, and then we also used the benefit of reversibility to check for to analyze and evaluate our past results for the future work.",
            "We are trying to sort of run this architecture in cycle for multiple times and this would give us new predictions for the words in the extremes repeatedly and then we could use it for some kind of machine learning framework, but also the 11 concrete idea is to instead use deep learning techniques to launch source alignment, since now we can use these examples for supervision and then we can get rid of grammar and lexicon.",
            "Try to launch such alignments automatically using deep learning approaches.",
            "Thank you."
        ],
        [
            "OK, thank you.",
            "Very interesting and intriguing topic.",
            "I have a quick one question that that's probably my confusion.",
            "You said that you have a camera that you manually constructed, right?",
            "And you have a lexicon that was automatically constructed.",
            "So what data did you use to construct the lexicon?",
            "So for Lexicon we used chunking and regular expression kind of things.",
            "So we use the analytical framework for that.",
            "Yeah, but on what data?",
            "And So what happens is that we take the same input SNTP sandwi using analytical library.",
            "We can identify which of them are noun phrase or we serve them.",
            "We do the post office tagging.",
            "So this tells us which part of the sentence is a noun, which part is a verb and so on.",
            "And then we assume that the NP's are going to be the entities and the valves are going to be the rules OK and the manual construction grammar.",
            "What you consulted some data when you constructed the grammar or so the grammar was based on.",
            "Mineral analysis of the type of sentences that we have in the input and so to measure the syntactic.",
            "OK, so that comes to my question.",
            "So you created the grammar.",
            "Analyzing the sentences in the input words were those the same sensors that you then used to compute this course for evaluation.",
            "So we took a part of the input sentences to analyze and write a grammar for them.",
            "But of course we do not fine tune the grammar to each and every sentences of the of the input, but those sensors that you used to create the grammar were not in the valuation, they were in the evolution part of it was in the evaluation.",
            "Did you also evaluate only on the sensors that actually you haven't seen to haven't consulted to construct the ground because that would be.",
            "You know, informative about your generalization error.",
            "I think in the paper we have this information about the sampling of the how we did the sampling of the grammar, but right now I don't.",
            "I don't know.",
            "OK, OK, that's good.",
            "Yeah, then we'll probably have to look it up in the paper.",
            "So thanks so I have a question with regards to the complexity of the DL.",
            "This is generated on one of your slides.",
            "You show how you translate from the from the flat semantics.",
            "Yes to DL.",
            "Yes, true and there are some some, some some.",
            "Disjunctions here and also.",
            "Compliments.",
            "Yeah, yeah, so weird is used a lot for for generating DL and what can you do with with with the DL generated.",
            "From that, if it falls in in all full, for instance.",
            "Can you repeat the question please?",
            "Maybe make it a bit simple.",
            "It is difficult.",
            "You have D sentences.",
            "In natural language you generate logical formulas out of it and you want to do something.",
            "We did it right.",
            "Yes, OK. And DL explicitly tells you that you can use a subset of the constructors and everything will go fine because you are in a subset of the 1st order logic that is at least decidable for instance.",
            "Yes, but if she used all that you can use in all yes, then you fall into none undecidable.",
            "So we are not using the full power of our full representation, for example, so these are these are the these are.",
            "This is the list of the total constructs that we had.",
            "This is the total set of constructs we had, so we don't do anything more than this.",
            "We have time for one more question.",
            "OK, that's that's a different question.",
            "Have you considered so?",
            "This is this is a specific domain specific data that you're working with, and that's a very important you know to have methods for domain specific data about.",
            "Have you considered any baselines from the available systems for semantic parsing to just apply them on that data to sort of compare your system against something CCG based?",
            "Things that are available for download?",
            "Never?",
            "You looked at that.",
            "So during my presentation I made a very simplified presentation saying that we have the grammar with the lexicon at the same level, but in fact what we do is.",
            "This I didn't so earlier, but what we do is we have a set of a separation between the syntactic representation and the actual lexical information.",
            "So this lexical information are going to be instantiated by the.",
            "By the syntactic.",
            "Syntactic constructs at runtime, right?",
            "So this is what this is.",
            "What I'm trying to make tell you is that this allows for modularity, although we have not tried with CC, for example, what we can propose is that we can replace this architecture with the CC formalism, for example, and this part would still be relevant to the to their formalism, and then we can plug in that grammar architecture and it's still the framework would work.",
            "Yeah, let's think again.",
            "I was speaker and just proceed with the let's think again.",
            "Our pre skip speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi everyone, thanks for coming.",
                    "label": 0
                },
                {
                    "sent": "My name is because Kelly and today I will talk about.",
                    "label": 0
                },
                {
                    "sent": "I'll present her talk on how we worked on to produce Description logic exams for natural language text.",
                    "label": 1
                },
                {
                    "sent": "So in this case is going to be sentences in English and it is a so I come from CNS Loria in France and it's a joint work with my colleagues Anastasia clear somewhere and volume.",
                    "label": 0
                },
                {
                    "sent": "So in this talk.",
                    "label": 0
                },
                {
                    "sent": "I will present the mainly this.",
                    "label": 0
                },
                {
                    "sent": "I will present a reversible architecture in which we can go from natural language text to description, logic actions and once we have that then we can take the other way around so we can take the description, logic, action as the input end and get back the natural language sentences right.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to clarify terms, in literature, in literature we usually use the term semantic parsing to represent.",
                    "label": 0
                },
                {
                    "sent": "This is the first direction of work that is going from sentences in English to some formal representation.",
                    "label": 0
                },
                {
                    "sent": "In our case, the formal representation is going to be description logic exams, so that is what we differ by semantic parsing and.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other way around is called generation, right so?",
                    "label": 0
                },
                {
                    "sent": "So yeah, so I will present her.",
                    "label": 0
                },
                {
                    "sent": "Our work, which does both of these things.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the context in which we did this research was in terms of a projector in our project.",
                    "label": 0
                },
                {
                    "sent": "Partner was here with industry in France, an usually in organizing, so we usually they have a set of what they call the system installation design principle.",
                    "label": 1
                },
                {
                    "sent": "So it's set of sentences written in plain text to describe the various parts that should interact with each other, or different entities that participate in an event for some.",
                    "label": 0
                },
                {
                    "sent": "A specific task.",
                    "label": 1
                },
                {
                    "sent": "So the SMTP texture in this in our use case was some sentences like like this where it says the pipes will be identified by labels here.",
                    "label": 0
                },
                {
                    "sent": "So this is one of the specifications they have, so we have this in as the input and from this input we want to derive an output.",
                    "label": 0
                },
                {
                    "sent": "In Descripcion logical format, right?",
                    "label": 0
                },
                {
                    "sent": "So we want to get the axiom out of it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so there is going to be the semantic parsing task and once we do that, we're going the other way around.",
                    "label": 0
                },
                {
                    "sent": "That is the generation task and we will get back the original sentence.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why would we like to do this?",
                    "label": 0
                },
                {
                    "sent": "So for the semantic parsing task, there are a couple of reasons.",
                    "label": 1
                },
                {
                    "sent": "First is that instead of having text, if we can have a formal model, like if we can have an action, then it would help us to enrich an existing knowledge base.",
                    "label": 0
                },
                {
                    "sent": "But also it would allow to eat once we have that formalism, then we can put it into some reasoning services and get some other.",
                    "label": 0
                },
                {
                    "sent": "Benefits of the reasoning services right and the manual.",
                    "label": 0
                },
                {
                    "sent": "The IF we approach this in a manual basis is going to be difficult because because of the time it takes and also the human expertise it takes and therefore we want to make it automatic.",
                    "label": 0
                },
                {
                    "sent": "And this is also true because in organization the SDP or the text might be changed to update a new principles or whatever.",
                    "label": 0
                },
                {
                    "sent": "And then we want to have.",
                    "label": 0
                },
                {
                    "sent": "The updated extremes at the same time the other way around.",
                    "label": 0
                },
                {
                    "sent": "The 1st Generation we are interested in it because again, the exams are easier for computer to process.",
                    "label": 0
                },
                {
                    "sent": "Things like that, but it is difficult for human consumption.",
                    "label": 1
                },
                {
                    "sent": "So if we have the generation engine, we can have readable explanation of what is going on with the exams.",
                    "label": 0
                },
                {
                    "sent": "And as in our case we will show that the generation task we will use it either means to verify our passing meaning that.",
                    "label": 0
                },
                {
                    "sent": "We will analyze how good our parsing was using the generation module.",
                    "label": 0
                },
                {
                    "sent": "This will be clear in the in the coming slides.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To outline the talk 1st, I'll present a brief overview of related systems related works in semantic parsing and generation.",
                    "label": 0
                },
                {
                    "sent": "I will explain what our contributions are, whatever the new things that we did, then I will present a global overview of the approach.",
                    "label": 0
                },
                {
                    "sent": "I will discuss the resources that we used and the techniques were utilized for our task.",
                    "label": 0
                },
                {
                    "sent": "I will present the experimental data that we used.",
                    "label": 0
                },
                {
                    "sent": "I will present the evaluation we did and we will see the results of them.",
                    "label": 0
                },
                {
                    "sent": "And finally, I will talk.",
                    "label": 0
                },
                {
                    "sent": "And this presentation with the conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the semantic parsing task, a couple of infected many work have been done, but from our perspective.",
                    "label": 0
                },
                {
                    "sent": "What we see is most of the tasks there are either targeting on the derivation of 1st order logic axioms from text, so this is not the case for us because we are targeting our elections or they require parallel data text corpus tool on such alignments from text to two extremes, right?",
                    "label": 0
                },
                {
                    "sent": "But in our case we don't have the parallel data texture corpus, so it is not going to be the case 1st.",
                    "label": 0
                },
                {
                    "sent": "And since we're talking about semantic parsing in terms of generating extremes to add to the ontology.",
                    "label": 0
                },
                {
                    "sent": "Our work is also somewhat related to the ontology Learning learning task.",
                    "label": 0
                },
                {
                    "sent": "And in the Ontology learning task are mainly we see that multiple works have been done, but they mainly work with identifying Toms in the in the sentence or identifying concepts or even identifying the hierarchy of concepts, identifying relations between concepts and so on.",
                    "label": 0
                },
                {
                    "sent": "So if we look at the diagram here, most of the work they would they would reach until this level, right?",
                    "label": 0
                },
                {
                    "sent": "But in our in this task we want to go one level up.",
                    "label": 0
                },
                {
                    "sent": "So we want to take the whole sentence as the input and.",
                    "label": 0
                },
                {
                    "sent": "The driver axiom that represents the entire meaning of the sentence.",
                    "label": 0
                },
                {
                    "sent": "Uh, for?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generation again similarly for the generation.",
                    "label": 0
                },
                {
                    "sent": "Most of the work.",
                    "label": 0
                },
                {
                    "sent": "Either they are require a large handwritten module of resources or they require parallel data text corpus tool on the alignments.",
                    "label": 0
                },
                {
                    "sent": "And some walks are like the last one.",
                    "label": 0
                },
                {
                    "sent": "Here are also describing.",
                    "label": 0
                },
                {
                    "sent": "I'd also describing the generation of sentences from RDF triples, so this is not going to going to be the case for us because one second we don't have the parallel corpus.",
                    "label": 0
                },
                {
                    "sent": "And also we want to generate from owl axioms rather than the RDF triples.",
                    "label": 0
                },
                {
                    "sent": "So what are the explicit contributions that we make the so the first one is that we take the input at the.",
                    "label": 0
                },
                {
                    "sent": "Sentence level and we generate the output at the owl axiom level for the semantic parsing task and the other way around for the generation task.",
                    "label": 0
                },
                {
                    "sent": "So if we go back to this figure here like I said earlier, we are working at this level.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ann and we are going to use the generation module Azure.",
                    "label": 0
                },
                {
                    "sent": "As a means of a.",
                    "label": 0
                },
                {
                    "sent": "Measuring the accuracy of the semantic path.",
                    "label": 0
                },
                {
                    "sent": "So this I will explain later on and once we have the exams from the semantic parsing parser engine, we're going to add those exams to the ontology and we're going to study the effect of it on the ontology loading task.",
                    "label": 0
                },
                {
                    "sent": "And finally, we have developed a reversible semantic parser generator module which is modular, can take different grammars of different types, and it is also robust, meaning that it can escape over unknown words to give a partial parse.",
                    "label": 0
                },
                {
                    "sent": "Or a partial generation.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the global approach that looks like this here we begin with the input SFTP, so this is the input text that we are going to process.",
                    "label": 0
                },
                {
                    "sent": "We feel this to a semantic parser engine.",
                    "label": 0
                },
                {
                    "sent": "The semantic parser engine is going to give a description logic representation of that sentence.",
                    "label": 0
                },
                {
                    "sent": "Once we have this description logic exam so we can fill them to the ontology and thereby weakening this knowledge, we can add the output from our semantic parser to the ontology.",
                    "label": 0
                },
                {
                    "sent": "But also we can take those actions and feed them to the generator.",
                    "label": 0
                },
                {
                    "sent": "Anna asked it to it to give us the text describing the.",
                    "label": 0
                },
                {
                    "sent": "Addiction, right?",
                    "label": 0
                },
                {
                    "sent": "The motivation here is that we will compare this regenerated SMTP's with the original input side piece and we will use some kind of scoring which is the blue scoring two to identify how close they are and this this measure of closeness is going to be our basis on on determining how good the semantic parse output was.",
                    "label": 1
                },
                {
                    "sent": "So it's a cyclic process.",
                    "label": 0
                },
                {
                    "sent": "So let me begin with the here.",
                    "label": 0
                },
                {
                    "sent": "I forgot to talk about the grammar and the lexicon.",
                    "label": 0
                },
                {
                    "sent": "Both the semantic parser and generator.",
                    "label": 1
                },
                {
                    "sent": "They take the grammar and the lexicalized the resources.",
                    "label": 0
                },
                {
                    "sent": "An these resources are in common.",
                    "label": 0
                },
                {
                    "sent": "In our case, the grammar was manually defined, and the lexicon was automatically extracted.",
                    "label": 0
                },
                {
                    "sent": "So let me begin the explanation with the resources and then we can.",
                    "label": 0
                },
                {
                    "sent": "We will see examples of how we do parsing and generation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the grammar, like I said, was manual and we're using the feature based lexicalized tag representation for grammar.",
                    "label": 0
                },
                {
                    "sent": "So what that means is that we're going to have the syntactic constructions in terms of represented in terms of trees.",
                    "label": 0
                },
                {
                    "sent": "So for example, here we are saying that the word pipes is represented by a noun phrase, and similarly we have other trees in the grammar which describe different syntactic constructs is not only that we're also going to add.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Semantics to the grammar.",
                    "label": 0
                },
                {
                    "sent": "So if we look here we have the same grammar, but this time we have added the semantics.",
                    "label": 0
                },
                {
                    "sent": "So this is that.",
                    "label": 0
                },
                {
                    "sent": "Ah.",
                    "label": 0
                },
                {
                    "sent": "The word pipes is NP, but also it has a semantics with the predicate argument structure.",
                    "label": 0
                },
                {
                    "sent": "So we have the predicate called pipe and we have an argument which is the variable X in this case, and this is a constant that we assign a name to this structure.",
                    "label": 0
                },
                {
                    "sent": "So L6 is, the is the constant that I can refer to the this predicate argument structure here, right?",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "So in this way we also put a semantics to all the grammar units, so all the trees in our grammar.",
                    "label": 0
                },
                {
                    "sent": "And this this so.",
                    "label": 0
                },
                {
                    "sent": "At this point we have both the syntax and semantics encoded in the grammar, right?",
                    "label": 0
                },
                {
                    "sent": "So yeah, so this is what this makes our grammar in the lexicon.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now for passing indentation.",
                    "label": 0
                },
                {
                    "sent": "Like I said earlier, we're going to use the grammar and the lexicalized resources.",
                    "label": 0
                },
                {
                    "sent": "There are three many steps in.",
                    "label": 0
                },
                {
                    "sent": "In doing that, first we're going to select some trees from the grammar, so the grammar can have several trees, but depending on the input we are going to use those six of those trees from the cameras are useful, and the basis of selecting them is to check whether they contain the same word as the as the words in the input sentence.",
                    "label": 0
                },
                {
                    "sent": "This is true for parsing an or whether they contain the same semantics as as a semantics in the input.",
                    "label": 0
                },
                {
                    "sent": "So this is the case for generation and in the second phase.",
                    "label": 0
                },
                {
                    "sent": "Once we have selected the trees, we're going to combine all those.",
                    "label": 0
                },
                {
                    "sent": "Trees that we selected and finally at the third phase the result of combination of all those trees is going to give us a the parse output or the generation output.",
                    "label": 0
                },
                {
                    "sent": "Let's see an example because that will I think.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clear up things, so here I'm taking a small grammar, the one that I showed earlier.",
                    "label": 0
                },
                {
                    "sent": "We have three different trees in this case and I'm trying to parse these two sentences.",
                    "label": 0
                },
                {
                    "sent": "The first one is pipe, so not be used, so this is a negation and the other one is the kind of affirmative sentence.",
                    "label": 0
                },
                {
                    "sent": "So 5 cell we use.",
                    "label": 0
                },
                {
                    "sent": "Let's see for the first example.",
                    "label": 0
                },
                {
                    "sent": "I just said during parsing we begin from the words in the in the input sentence and for each word in the input sentence we want to identify some trees in the grammar that mess the corresponding word right.",
                    "label": 0
                },
                {
                    "sent": "So let's begin parsing for the first sentence here.",
                    "label": 0
                },
                {
                    "sent": "As you can see the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Water pipes here.",
                    "label": 0
                },
                {
                    "sent": "Message to the pipes here.",
                    "label": 0
                },
                {
                    "sent": "So we're going to select this tree and therefore we have the semantics from coming from the tree at this.",
                    "label": 0
                },
                {
                    "sent": "At this point, right?",
                    "label": 0
                },
                {
                    "sent": "So it's at this point we have the semantics represented for the word pipes and similarly.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are going to match these words with the words contained in the in the tree here, so this tree is going to be selected and now this tree is going to be combined with the previously selected tree here and in doing so the variables are going to be unified.",
                    "label": 0
                },
                {
                    "sent": "So what happens is that this variable Y is going to be unified with the with the with the constant.",
                    "label": 0
                },
                {
                    "sent": "So it's going to be instantiated with this constant L6.",
                    "label": 0
                },
                {
                    "sent": "Here this is true because this NP nor here is going to be substituted at this point.",
                    "label": 0
                },
                {
                    "sent": "And therefore the unification ensures that whenever we get the the instantiation of this semantics here, the variable Y is replaced by the appropriate instantiation constant.",
                    "label": 0
                },
                {
                    "sent": "So in this case it happens to be L6, right?",
                    "label": 0
                },
                {
                    "sent": "So now we have parts for.",
                    "label": 0
                },
                {
                    "sent": "Most of the words but one word remains there so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That I'm going to show now.",
                    "label": 0
                },
                {
                    "sent": "So this order is going to select the last three here and again the unification takes place.",
                    "label": 0
                },
                {
                    "sent": "The combination takes place and therefore we get this semantics as our parse output, right?",
                    "label": 0
                },
                {
                    "sent": "Similarly, for the second sentence.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The the word.",
                    "label": 0
                },
                {
                    "sent": "We can see how these words aligned to the words in the tree, and in this case only these two trees are going to be used, so therefore we get this kind of semantics.",
                    "label": 0
                },
                {
                    "sent": "So this was the true.",
                    "label": 0
                },
                {
                    "sent": "This was the case for passing.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is by the way, what we call full parse, meaning that for each word in the input sentence we could find some tree in the grammar which content those words right so?",
                    "label": 0
                },
                {
                    "sent": "But this is not always going to be the case.",
                    "label": 0
                },
                {
                    "sent": "Let me present.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, so this time I want to parse the third sentence.",
                    "label": 0
                },
                {
                    "sent": "Pipes shall not be used, but assuming that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This tree is not in the grammar, right?",
                    "label": 0
                },
                {
                    "sent": "So we want to pass the third sentence, but we don't have the tree corresponding to the word, not.",
                    "label": 0
                },
                {
                    "sent": "So what happens here is that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will just assume that the word not in the input sentence was as if it was not in the input at all, so we're going to skip over this word and then for rest of the words were going to select the trees.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This means that again, we're going to make use of the first tree here and the secondary here, and therefore the the.",
                    "label": 0
                },
                {
                    "sent": "The pass output that we get at this point is the same as the parse we got from the second example.",
                    "label": 0
                },
                {
                    "sent": "It is as if the word not was not really present in the input, so these are examples of.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we call a partial parts in practice what we do is we try to find we try our best to find all possible methods, But if we can't find some then then we can skip over those words and.",
                    "label": 0
                },
                {
                    "sent": "And try the try the parsing algorithm so this is what we call partial path.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For generation to complete the example, so in generation we start from some input semantics and we want to generate output centers.",
                    "label": 0
                },
                {
                    "sent": "So assuming that the input semantics is the one like the one shown over here, this time we're going to match the the.",
                    "label": 0
                },
                {
                    "sent": "Units of semantics from the input, so to the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's in the trees, so.",
                    "label": 0
                },
                {
                    "sent": "In this case, we're going to align these semantics with the semantics over here.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The semantics here to the semantics over here.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And like on and finally we at this point we have selected all those trees and after the combination we get output sentence like pipe cell not be used for this.",
                    "label": 0
                },
                {
                    "sent": "Input generation input, so this is how the parsing and generation works for us.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Please note that whenever I presented an example here, the semantics I represented was in some kind of flat semantics, so so this was in order to be compatible with our existing system.",
                    "label": 0
                },
                {
                    "sent": "What we do next is we define a set of transformation rules that take those flat semantics and give a description logical representation.",
                    "label": 0
                },
                {
                    "sent": "So these are the set of all rules that we apply, so the last one might be easier to look.",
                    "label": 0
                },
                {
                    "sent": "So this one is saying that if I have a literal.",
                    "label": 0
                },
                {
                    "sent": "That expresses a predicate of true for an argument.",
                    "label": 0
                },
                {
                    "sent": "So in the flat semantics, then I'm going to say that I have a concept.",
                    "label": 0
                },
                {
                    "sent": "I have a concept for that in the description.",
                    "label": 0
                },
                {
                    "sent": "Logic representation, right?",
                    "label": 0
                },
                {
                    "sent": "So in the other examples that I discussed.",
                    "label": 0
                },
                {
                    "sent": "So this thing here.",
                    "label": 0
                },
                {
                    "sent": "For example, if we have this, we're going to build a concept called pipe.",
                    "label": 0
                },
                {
                    "sent": "So the rest of the rules they work in a fairly easy manner.",
                    "label": 0
                },
                {
                    "sent": "It is not very difficult to explore them, but of course I want to eat here.",
                    "label": 0
                },
                {
                    "sent": "We don't have much time, but the important thing I want to highlight here is that we have different constructs.",
                    "label": 0
                },
                {
                    "sent": "For example, we have the existential quantifier.",
                    "label": 0
                },
                {
                    "sent": "We have the intersection, we have the Union and things like that.",
                    "label": 0
                },
                {
                    "sent": "We have the negation and inverse and things like that.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so at this point we have.",
                    "label": 0
                },
                {
                    "sent": "We have we had initiatives to evaluate our work.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, coming to the evaluation first.",
                    "label": 0
                },
                {
                    "sent": "First we have the data coming from the Airbus Industrie domain, and in this experiment we had 960 SMTP sentences.",
                    "label": 0
                },
                {
                    "sent": "What we do is we try to identify.",
                    "label": 0
                },
                {
                    "sent": "We tried to split them into two categories.",
                    "label": 0
                },
                {
                    "sent": "We call them simple as IDP's and complexities.",
                    "label": 0
                },
                {
                    "sent": "This was done manually.",
                    "label": 0
                },
                {
                    "sent": "So the idea behind here is that we will consider.",
                    "label": 0
                },
                {
                    "sent": "We will consider all the sentences that have a single clause as a simple SMTP and the rest of them are going to be called complexity so they can have more than one clauses or more than one relative relative clauses.",
                    "label": 0
                },
                {
                    "sent": "For example in this example.",
                    "label": 0
                },
                {
                    "sent": "So we did.",
                    "label": 0
                },
                {
                    "sent": "This is written then.",
                    "label": 0
                },
                {
                    "sent": "Now I will show the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experimental results on these disputed data.",
                    "label": 0
                },
                {
                    "sent": "We are going to.",
                    "label": 0
                },
                {
                    "sent": "And do the analysis from 3 perspectives.",
                    "label": 0
                },
                {
                    "sent": "First, we're going to identify the coverage and the robustness, so the coverage is like for how many of the total inputs did where we actually able to parse them.",
                    "label": 0
                },
                {
                    "sent": "The robustness is the measure of the partial parses that I talked about earlier.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so there is going to be the first dimension.",
                    "label": 0
                },
                {
                    "sent": "The second is about the correctness of the formula that we have obtained from the semantic parse output.",
                    "label": 0
                },
                {
                    "sent": "This we analyzed in two directions.",
                    "label": 0
                },
                {
                    "sent": "The first is the syntactic correctness and the 2nd is the semantic correctness.",
                    "label": 0
                },
                {
                    "sent": "This I will present in the coming slides and finally, I like I said earlier, these axioms that we derive are going to be added to the ontology and we will study their effect in doing so.",
                    "label": 0
                },
                {
                    "sent": "Lissa",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about the coverage.",
                    "label": 0
                },
                {
                    "sent": "So we can see from from the figure here that.",
                    "label": 0
                },
                {
                    "sent": "Far for both the simple SMTP center and the complexity piece, we have a high coverage ratio, meaning that for almost almost all of them we can.",
                    "label": 0
                },
                {
                    "sent": "We can pass like we could not pass just for a few cases like 2.5% and for the other side we could parse.",
                    "label": 0
                },
                {
                    "sent": "Here the.",
                    "label": 0
                },
                {
                    "sent": "The failure comes from the fact that since the lexicon is automatically derived, sometimes the lexicon is attached to the wrong syntactic construct in the grammar, and whenever this happens, the wrongly selected trees.",
                    "label": 0
                },
                {
                    "sent": "They cannot combine to each other and therefore we fail to do the parsing.",
                    "label": 0
                },
                {
                    "sent": "The partial parses are high.",
                    "label": 0
                },
                {
                    "sent": "This is because sometimes we might miss a single word, or it might be the case that we miss a complete clause, for example, or it might also be the case that.",
                    "label": 0
                },
                {
                    "sent": "We missed a few words, right?",
                    "label": 0
                },
                {
                    "sent": "So these are the cases of partial patches.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "To identify the correctness of the of the parses that we had, we we.",
                    "label": 0
                },
                {
                    "sent": "We can see that almost for almost all the passes that we we could get.",
                    "label": 0
                },
                {
                    "sent": "So here what I did, what we did is we took the parse output and we ran them against our validation service, so that would tell us whether this is a syntactically correct all formula or not, and so it was the case that for 96% of whatever we could pass those were actually valid.",
                    "label": 0
                },
                {
                    "sent": "Ideal formulas the few of them for the 4% of them it is.",
                    "label": 0
                },
                {
                    "sent": "We analyzed why it was the case that our parse output was not correct and it happens to be that sometimes the sentences complex sentences actually can be quite very long.",
                    "label": 0
                },
                {
                    "sent": "I think it is the longest one was like having 85 words or whatever and then our grammar is not not capable of capturing all those complex syntactic structures.",
                    "label": 0
                },
                {
                    "sent": "So this means that.",
                    "label": 0
                },
                {
                    "sent": "There is some room for improvement there.",
                    "label": 0
                },
                {
                    "sent": "Now too.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To evaluate semantic correctness, what we're going to do is we're going to take the.",
                    "label": 0
                },
                {
                    "sent": "We're going to take the blue score like like I mentioned earlier, we're going to compare the original input with the with the reason rated sentences an.",
                    "label": 0
                },
                {
                    "sent": "And I didn't even use the blue Score 2 to analyze things so fast.",
                    "label": 0
                },
                {
                    "sent": "We categorized 3.",
                    "label": 0
                },
                {
                    "sent": "Three different.",
                    "label": 0
                },
                {
                    "sent": "Well, it's a range of blue scores the the one scoring less than 32%.",
                    "label": 0
                },
                {
                    "sent": "We called them as belonging to the blue score, giving output the one that lie between 33% and 66%.",
                    "label": 0
                },
                {
                    "sent": "They are going to be the medium blue scoring output and the rest is going to be in the high Blues output, right?",
                    "label": 0
                },
                {
                    "sent": "So here we see that for all the full passes, so this is the start for the full path.",
                    "label": 0
                },
                {
                    "sent": "All the full passes that we had whenever we try to resend it on the sentences from them.",
                    "label": 0
                },
                {
                    "sent": "We got, we got the sentences in the high category, meaning that the reason little sentence was actually pretty close enough to the original input.",
                    "label": 0
                },
                {
                    "sent": "So this signifies to us that our semantic parsing task is correctly capturing the semantics of the input sentence because we were able to resend it nearly to the exact match.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this is not true for.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It would be different for partial cases.",
                    "label": 0
                },
                {
                    "sent": "Here we can see that for the simple side piece, so the ones in pink here.",
                    "label": 0
                },
                {
                    "sent": "So for the simple SMTP's most of them they belong to either medium blue score category or to the high risk category.",
                    "label": 0
                },
                {
                    "sent": "A few of them they belong to a lowest called Blue category.",
                    "label": 0
                },
                {
                    "sent": "This is because because of the inconsistency is coming from the from the lexicon.",
                    "label": 0
                },
                {
                    "sent": "So even when the sentences were simple, maybe we didn't find a matching tree for one of each word, and then that significantly hampers the blue scoring.",
                    "label": 0
                },
                {
                    "sent": "But on the for the complexity piece here.",
                    "label": 0
                },
                {
                    "sent": "We we have a significant percentage of of them not giving a giving a very low blue score, so these are the cases I mentioned earlier like the sentence was really very long with so many clauses in them and maybe we missed when doing the partial on doing the partial passing.",
                    "label": 0
                },
                {
                    "sent": "Maybe we missed one or two of the clauses completely right?",
                    "label": 0
                },
                {
                    "sent": "So this might be the case for that.",
                    "label": 0
                },
                {
                    "sent": "So again here.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have we have to think about how we can make the grammar?",
                    "label": 0
                },
                {
                    "sent": "How we can extend the grammar in fact, to account for those cases?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so now.",
                    "label": 0
                },
                {
                    "sent": "So finally we have the actions.",
                    "label": 0
                },
                {
                    "sent": "We're also going to add those exams to the ontology and here we have a few interesting observations.",
                    "label": 0
                },
                {
                    "sent": "So what we did is from.",
                    "label": 0
                },
                {
                    "sent": "We added the new concepts and relations to the ontology whenever they were not already presented there, and in doing so we ensure that such addition does not hamper the consistency and the satisfiability of the ontology.",
                    "label": 1
                },
                {
                    "sent": "We see that we identified few many new classes.",
                    "label": 0
                },
                {
                    "sent": "Lowers like 900.",
                    "label": 1
                },
                {
                    "sent": "35 but also there were we could identify some of the existing classes of the ontology around 90 and a few new object properties that we could propose for the decent with ontology.",
                    "label": 0
                },
                {
                    "sent": "And we also identified the subclass based on the subclass of action we could identify which which new concept can be the children are off the existing concept.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and so on an.",
                    "label": 1
                },
                {
                    "sent": "And here we we see that for for 85% of what we could get from the parse output, we could add them to the ontology.",
                    "label": 0
                },
                {
                    "sent": "There were a few redundancies and syntax errors.",
                    "label": 0
                },
                {
                    "sent": "Anna in a very rare case we also got inconsistency.",
                    "label": 0
                },
                {
                    "sent": "This was because the input FPS, sometime it contains contradictory sentence like one is the negation of the other and both the sentences are also in the input.",
                    "label": 1
                },
                {
                    "sent": "So this helps us to identify the sources of.",
                    "label": 0
                },
                {
                    "sent": "Error in the input section actually.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To conclude.",
                    "label": 0
                },
                {
                    "sent": "We built a modular framework for synchronizing text to formal representations.",
                    "label": 0
                },
                {
                    "sent": "Anarchy feature of our approach is that this this whole framework is a reversible engine, and then we also used the benefit of reversibility to check for to analyze and evaluate our past results for the future work.",
                    "label": 0
                },
                {
                    "sent": "We are trying to sort of run this architecture in cycle for multiple times and this would give us new predictions for the words in the extremes repeatedly and then we could use it for some kind of machine learning framework, but also the 11 concrete idea is to instead use deep learning techniques to launch source alignment, since now we can use these examples for supervision and then we can get rid of grammar and lexicon.",
                    "label": 0
                },
                {
                    "sent": "Try to launch such alignments automatically using deep learning approaches.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Very interesting and intriguing topic.",
                    "label": 0
                },
                {
                    "sent": "I have a quick one question that that's probably my confusion.",
                    "label": 0
                },
                {
                    "sent": "You said that you have a camera that you manually constructed, right?",
                    "label": 0
                },
                {
                    "sent": "And you have a lexicon that was automatically constructed.",
                    "label": 0
                },
                {
                    "sent": "So what data did you use to construct the lexicon?",
                    "label": 0
                },
                {
                    "sent": "So for Lexicon we used chunking and regular expression kind of things.",
                    "label": 0
                },
                {
                    "sent": "So we use the analytical framework for that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but on what data?",
                    "label": 0
                },
                {
                    "sent": "And So what happens is that we take the same input SNTP sandwi using analytical library.",
                    "label": 0
                },
                {
                    "sent": "We can identify which of them are noun phrase or we serve them.",
                    "label": 0
                },
                {
                    "sent": "We do the post office tagging.",
                    "label": 0
                },
                {
                    "sent": "So this tells us which part of the sentence is a noun, which part is a verb and so on.",
                    "label": 0
                },
                {
                    "sent": "And then we assume that the NP's are going to be the entities and the valves are going to be the rules OK and the manual construction grammar.",
                    "label": 0
                },
                {
                    "sent": "What you consulted some data when you constructed the grammar or so the grammar was based on.",
                    "label": 0
                },
                {
                    "sent": "Mineral analysis of the type of sentences that we have in the input and so to measure the syntactic.",
                    "label": 0
                },
                {
                    "sent": "OK, so that comes to my question.",
                    "label": 0
                },
                {
                    "sent": "So you created the grammar.",
                    "label": 0
                },
                {
                    "sent": "Analyzing the sentences in the input words were those the same sensors that you then used to compute this course for evaluation.",
                    "label": 0
                },
                {
                    "sent": "So we took a part of the input sentences to analyze and write a grammar for them.",
                    "label": 0
                },
                {
                    "sent": "But of course we do not fine tune the grammar to each and every sentences of the of the input, but those sensors that you used to create the grammar were not in the valuation, they were in the evolution part of it was in the evaluation.",
                    "label": 0
                },
                {
                    "sent": "Did you also evaluate only on the sensors that actually you haven't seen to haven't consulted to construct the ground because that would be.",
                    "label": 0
                },
                {
                    "sent": "You know, informative about your generalization error.",
                    "label": 0
                },
                {
                    "sent": "I think in the paper we have this information about the sampling of the how we did the sampling of the grammar, but right now I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, that's good.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then we'll probably have to look it up in the paper.",
                    "label": 0
                },
                {
                    "sent": "So thanks so I have a question with regards to the complexity of the DL.",
                    "label": 0
                },
                {
                    "sent": "This is generated on one of your slides.",
                    "label": 0
                },
                {
                    "sent": "You show how you translate from the from the flat semantics.",
                    "label": 0
                },
                {
                    "sent": "Yes to DL.",
                    "label": 0
                },
                {
                    "sent": "Yes, true and there are some some, some some.",
                    "label": 0
                },
                {
                    "sent": "Disjunctions here and also.",
                    "label": 0
                },
                {
                    "sent": "Compliments.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, so weird is used a lot for for generating DL and what can you do with with with the DL generated.",
                    "label": 0
                },
                {
                    "sent": "From that, if it falls in in all full, for instance.",
                    "label": 0
                },
                {
                    "sent": "Can you repeat the question please?",
                    "label": 0
                },
                {
                    "sent": "Maybe make it a bit simple.",
                    "label": 0
                },
                {
                    "sent": "It is difficult.",
                    "label": 0
                },
                {
                    "sent": "You have D sentences.",
                    "label": 0
                },
                {
                    "sent": "In natural language you generate logical formulas out of it and you want to do something.",
                    "label": 0
                },
                {
                    "sent": "We did it right.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK. And DL explicitly tells you that you can use a subset of the constructors and everything will go fine because you are in a subset of the 1st order logic that is at least decidable for instance.",
                    "label": 0
                },
                {
                    "sent": "Yes, but if she used all that you can use in all yes, then you fall into none undecidable.",
                    "label": 0
                },
                {
                    "sent": "So we are not using the full power of our full representation, for example, so these are these are the these are.",
                    "label": 0
                },
                {
                    "sent": "This is the list of the total constructs that we had.",
                    "label": 0
                },
                {
                    "sent": "This is the total set of constructs we had, so we don't do anything more than this.",
                    "label": 0
                },
                {
                    "sent": "We have time for one more question.",
                    "label": 0
                },
                {
                    "sent": "OK, that's that's a different question.",
                    "label": 0
                },
                {
                    "sent": "Have you considered so?",
                    "label": 0
                },
                {
                    "sent": "This is this is a specific domain specific data that you're working with, and that's a very important you know to have methods for domain specific data about.",
                    "label": 0
                },
                {
                    "sent": "Have you considered any baselines from the available systems for semantic parsing to just apply them on that data to sort of compare your system against something CCG based?",
                    "label": 0
                },
                {
                    "sent": "Things that are available for download?",
                    "label": 0
                },
                {
                    "sent": "Never?",
                    "label": 0
                },
                {
                    "sent": "You looked at that.",
                    "label": 0
                },
                {
                    "sent": "So during my presentation I made a very simplified presentation saying that we have the grammar with the lexicon at the same level, but in fact what we do is.",
                    "label": 0
                },
                {
                    "sent": "This I didn't so earlier, but what we do is we have a set of a separation between the syntactic representation and the actual lexical information.",
                    "label": 0
                },
                {
                    "sent": "So this lexical information are going to be instantiated by the.",
                    "label": 0
                },
                {
                    "sent": "By the syntactic.",
                    "label": 0
                },
                {
                    "sent": "Syntactic constructs at runtime, right?",
                    "label": 0
                },
                {
                    "sent": "So this is what this is.",
                    "label": 0
                },
                {
                    "sent": "What I'm trying to make tell you is that this allows for modularity, although we have not tried with CC, for example, what we can propose is that we can replace this architecture with the CC formalism, for example, and this part would still be relevant to the to their formalism, and then we can plug in that grammar architecture and it's still the framework would work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, let's think again.",
                    "label": 0
                },
                {
                    "sent": "I was speaker and just proceed with the let's think again.",
                    "label": 0
                },
                {
                    "sent": "Our pre skip speaker.",
                    "label": 0
                }
            ]
        }
    }
}