{
    "id": "loo2e5sibjedywpu3tc3nrgpxaazf36p",
    "title": "The Sample Complexity of Dictionary Learning",
    "info": {
        "author": [
            "Daniel Vainsencher, Computer Science Department, Technion - Israel Institute of Technology"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2011_vainsencher_dictionary/",
    "segmentation": [
        [
            "I'll be giving a talk about joint work with Seminole and fairly books thing from the technical also.",
            "And.",
            "The motivation for the talk is that there is a significant proliferation of media of all kinds, and currently there is a common approach that is."
        ],
        [
            "Somewhat uniform, and it might be better to do some learning.",
            "So for example, if we consider images, then the most successful or most successful standard into hopefully will be sometime very successful.",
            "Standard of JPEG.",
            "Use a particular basis to represent images and the idea here is that in these particular bases, most images that we know happen to have a sparse representation.",
            "Many of the coefficients are small, we can simply throw them away and it still looks good and the question is whether in fact, since there are many images that actually look alike, have common elements, all of those pictures from that.",
            "To the jungles of Bolivia.",
            "Actually sort of look the same in some sense.",
            "Can we make use of that in order to.",
            "Gain something so."
        ],
        [
            "What kinds of things might we gain?",
            "So the first application of course, is better compression for images from the trip, but there are also possibilities for relatively easy restoration of signals and also acquisition, by which I mean the area of compressed sensing.",
            "Also depend on having this good basis in which few coefficients are not."
        ],
        [
            "Sorry.",
            "And this does not apply only to images.",
            "In fact, voice signals can be could be considered the same way, e.g signals have consist of measurements of 64 different electrodes, which actually are probably connected in.",
            "There may be some useful basis for doing that and hyperspectral camera which at each pixel takes about hundreds, maybe 1000 of measurements at different frequencies as opposed to the three frequencies we are used to dealing with might also have useful.",
            "Bases, which in this case maybe are not as well known to us as wavelets that have been very well studied analytic."
        ],
        [
            "So let's consider this a bit more formally.",
            "I will take some time with this is it's not necessarily the most common setting we have here, so we have a set of samples signals in RN which are normalized to the sphere.",
            "We have N samples and they are distributed according to some unknown distribution so far."
        ],
        [
            "So good.",
            "And now.",
            "Instead of considering directly some classification loss, what we have is a distance between the best representation we can find, which is done by taking coefficient vector and some dictionary which has P elements on the unit sphere.",
            "And the signal that we're trying to represent, and we take the representation with minimal Euclidean distance and allowed representations coefficients, we allow our from some set which enforces some form of simplicity.",
            "So we will consider a hard sparsity constraint in the form of HK, which means that at most K coefficients are not zero and also a relaxed R Lambda concept of sparsity, where we bound."
        ],
        [
            "Point norm.",
            "And just to get an idea of what these mean, what we have here is a diagram diagram for each of them, of the precisely representable signals.",
            "Of course, in a very low dimensional setting that we can actually draw.",
            "So at the top we have the polytope that comes from the convex Hull of the dictionary elements which lie on a sphere, so everything inside the polytope can be represented precisely and everything outside will have some error, and in the bottom case what we see is the dictionary elements.",
            "In black, and if we assume here K is 2, then the signals we can represent precisely or exactly those great circles which are spanned by every two of those elements.",
            "And I might ask you to imagine on top of these sphere and the distance from each point on the sphere to these two bodies we see here give us exactly the meaning of HA for the appropriate sparsity constraint for a single dictionary.",
            "For every signal on the sphere."
        ],
        [
            "On top of that, something more standard.",
            "We may take a loss function.",
            "For example, we actually care about the quadratic distance or just the distance as it is."
        ],
        [
            "And our goal is to learn a particular dictionary that minimizes the expected loss on our unseen distribution."
        ],
        [
            "So just to fix our ideas in image restoration."
        ],
        [
            "And or in denoising, EG pretty common setting is for end to be 64 corresponding to either Patch of images or simply a measurement of all the electrodes.",
            "Dictionary size might be somewhat larger, and the sparsity coefficient would be significantly smaller than the dimension.",
            "So this is what we mean by the sparse representation.",
            "This is where compression for example would come from, and hundreds of thousands of examples are.",
            "Reasonable data set."
        ],
        [
            "So we considered already how to.",
            "Lossy compression comes about in the case of denoising, you might think that if images are sparse in this basis, however, the noise does not come from this basis, it will not be easy to represent, and therefore when we find the sparse representation, it will go away and the case of compressed sensing theory tells us approximately that quelaag North, which is again significantly smaller than North measurements, will suffice to reconstruct the right signal."
        ],
        [
            "So some relation of this to other problems.",
            "In the case of K = 1, The hard constraint for representation actually sort of retrieves K means clustering.",
            "The problem of finding the best Centers for representing for doing quantization, which carries with it a guarantee of NP hardness and also a gap between lower and upper bounds."
        ],
        [
            "For learning.",
            "In terms of the optimization problem itself, just finding the right representation is itself an NP hard problem unless we make some assumptions about the geometry of the dictionary D."
        ],
        [
            "And ignoring that, putting that aside for example, also in the case of learning dictionaries for the relaxed class of representations, we simply have a very high dimensional, pretty high dimensional space in which we have a nonconvex objective."
        ],
        [
            "Now we'll go back to the generalization."
        ],
        [
            "Um, about the optimization problem.",
            "There have been many algorithms people use this for various applications, so they care about getting good results and they do."
        ],
        [
            "Empirically, but there are no actual algorithmic guarantees for these for this procedure."
        ],
        [
            "Therefore, we opt for uniform convergence results."
        ],
        [
            "So let's set the ideas about the general generalization problem.",
            "So we have a dimension signal dimension in dictionary size P. Some particular sample size M and the coefficient set A and we denote with the empirical distribution, for example.",
            "Then we want to bound the difference or we want to bound the expected error for representation using this dictionary in terms of.",
            "The average performance on the sample and also.",
            "Some epsilon which we will try to bound and understand how it depends on the problem Param."
        ],
        [
            "There's.",
            "So the previous result that we already had is a I think beautiful set of results by modern puntel.",
            "Which are they mentioned free so.",
            "And then goes to Infinity.",
            "That does not.",
            "Sample complexity does not.",
            "Unfortunately, it is limited to the quadratic loss and also there is a dependent on.",
            "Lambda, which is quadratic in Lambda.",
            "Lambda, so we're talking about the case.",
            "The relaxed class R. Lambda, where Lambda is bound on the L1 coefficients.",
            "On the L1 norm of the coefficient vector.",
            "And, um.",
            "So we will see later that we care about large Lambda and so this is."
        ],
        [
            "Not quite sufficient for us, and therefore we propose a different proof proof method that deals with any Lipschitz Lawson gives us only log rhythmic dependence and Lambda at the cost of being mentioned."
        ],
        [
            "Dependent and also has a variation with fast rates."
        ],
        [
            "By standard."
        ],
        [
            "Standard methods and.",
            "These bounds are occasionally even vacuous."
        ],
        [
            "So what will go quickly through a proof for the R Lambda case?",
            "The idea is simply that the mapping between a dictionary and the function of distances from the polytope two points on the sphere is Lipschitz continuous uniformly, and this makes sense if we consider moving a particular point and they won't go into many more details because it's not that more that much interesting.",
            "And beyond that we use standard covering number arguments.",
            "So the covering numbers are basically preserved by live."
        ],
        [
            "Function.",
            "Now we go back to.",
            "We continue to the case of hard sparse representation, hard sparsity constraints on the representation, and here the situation is not quite as nice.",
            "So we remember our dictionary with these three dictionary elements here.",
            "There's no problem if we move one of the dictionary elements a bit, then the subspaces also move a little bit and then also the error function changes only slightly.",
            "However, if we have two dictionary elements, for example that are very close to one another, then one of them doesn't have to travel very far to go around the other one and thus have.",
            "The set of exactly representable signals move very quickly and also have error function change on much of the sphere, so to be Lipschitz we have to stay away from these singularities."
        ],
        [
            "This stability property that we require will come from definition that is reasonably commonly used in sparse representations literature called the Bell function and definition might seem a bit large.",
            "Basically we consider representations of size K, so we have one dictionary element, and then another K, and we consider the sum of the magnitudes of the inner products between this.",
            "One dictionary element and every other K. And this we will require to be."
        ],
        [
            "Other than one so other bounds or other requirements on this magnitude, allow us to find approximation ratios for algorithms and it is also has been shown to be sufficient for allow compressed sensing with random projection."
        ],
        [
            "We show that first that choosing the dictionary D at random from many copies of the sphere gives us.",
            "With high probability dictionaries that have this property of the Lobell function, and more Interestingly."
        ],
        [
            "We show that this is sufficient for stability, so if.",
            "If the Bell function is bounded away from one below, it is below one on the way from it, then actually any optimal representation we find is also in our Lambda.",
            "That is to say it has a bound on its L1 norm which has size at most K / 1 minus Delta.",
            "So we can consider the class of dictionaries that fulfill this and then use the results we showed before for our Lambda.",
            "And here we see that the dependence on Lambda.",
            "May be significant because of the nearness of Delta to one."
        ],
        [
            "So we consider a sketch of the proof for this stability in this case.",
            "So we consider we have some dictionary D and some representation potential representation A and it's supported on only these three columns of the, so the subset of those columns we call DK, we take the gram matrix that is the inner product of the used columns with one another."
        ],
        [
            "And we have two interesting properties for this gram matrix.",
            "First of all, on its diagonal we have the norms of the dictionary elements, which are one because they are in the sphere and the second one is that the sum of off diagonal elements on each row is bounded away from one.",
            "In fact it's low one because of the develop function restriction."
        ],
        [
            "By the girls going this this theorem, this implies that all the eigenvalues are within distance Delta of one, and in particular this implies they're not at zero.",
            "Therefore G is invertible and we have a bound on its norm.",
            "So this is the basic stability."
        ],
        [
            "And then we have some details.",
            "Basically, for every signal X we can find the optimal representation a under some particular support by using the pseudo inverse.",
            "The norm of this part has already been bounded and the norm of this is basically bounded by the fact that there are inner products of a dictionary column with a signal, both of which have norm one."
        ],
        [
            "And from there it's a few details and this applies of course to every a supported in every subset of size K therefore.",
            "Plays in general.",
            "Particularly for the optimal representation."
        ],
        [
            "To summarize, we've shown new convergence bounds for L1 dictionary learning, which do not depend too strongly on Lambda."
        ],
        [
            "We've generalized these bounds to apply also to the case of hard sparsity constraints."
        ],
        [
            "We found a little evidence that this restriction is reasonable in a sense that many dictionaries fulfill it, and in the paper we also consider a little bit dictionary learning."
        ],
        [
            "No spaces.",
            "Some open problems.",
            "Well, there are quite a few gaps in the in terms of the.",
            "Sample complexity there is first one we inherit from clustering problem, but also we have not shown that the Bell function requirement is necessary.",
            "There may be reasonable upper bounds without it, so lower bound or an upper bound that would close that would be interesting and also different problem that now becomes I think more interesting is finding algorithms with theoretical guarantees for performance for dictionary learning and.",
            "It may be useful, though not necessarily that interesting theoretically, to consider Bell function regularization for algorithms of the style that already exists.",
            "Thank you.",
            "Questions.",
            "So just to make sure I understand the results were only for dictionaries with where the double function was bounded or strictly smaller than one and bounded away by some number, so any dictionary is smaller than with the Bible function is smaller than 0.9 or so.",
            "We think this would imply if I only if I when I learned the dictionary, I minimize my reconstruction in also bound the Bible function.",
            "Do people do that?",
            "Not that I know of that despite the fact that there seems to be motivation for it anyway, since the algorithms used later for reconstruction already have guarantees only when the bubble function is bounded similarly.",
            "Other questions.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll be giving a talk about joint work with Seminole and fairly books thing from the technical also.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The motivation for the talk is that there is a significant proliferation of media of all kinds, and currently there is a common approach that is.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Somewhat uniform, and it might be better to do some learning.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we consider images, then the most successful or most successful standard into hopefully will be sometime very successful.",
                    "label": 0
                },
                {
                    "sent": "Standard of JPEG.",
                    "label": 0
                },
                {
                    "sent": "Use a particular basis to represent images and the idea here is that in these particular bases, most images that we know happen to have a sparse representation.",
                    "label": 0
                },
                {
                    "sent": "Many of the coefficients are small, we can simply throw them away and it still looks good and the question is whether in fact, since there are many images that actually look alike, have common elements, all of those pictures from that.",
                    "label": 0
                },
                {
                    "sent": "To the jungles of Bolivia.",
                    "label": 0
                },
                {
                    "sent": "Actually sort of look the same in some sense.",
                    "label": 0
                },
                {
                    "sent": "Can we make use of that in order to.",
                    "label": 0
                },
                {
                    "sent": "Gain something so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What kinds of things might we gain?",
                    "label": 0
                },
                {
                    "sent": "So the first application of course, is better compression for images from the trip, but there are also possibilities for relatively easy restoration of signals and also acquisition, by which I mean the area of compressed sensing.",
                    "label": 0
                },
                {
                    "sent": "Also depend on having this good basis in which few coefficients are not.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "And this does not apply only to images.",
                    "label": 0
                },
                {
                    "sent": "In fact, voice signals can be could be considered the same way, e.g signals have consist of measurements of 64 different electrodes, which actually are probably connected in.",
                    "label": 0
                },
                {
                    "sent": "There may be some useful basis for doing that and hyperspectral camera which at each pixel takes about hundreds, maybe 1000 of measurements at different frequencies as opposed to the three frequencies we are used to dealing with might also have useful.",
                    "label": 0
                },
                {
                    "sent": "Bases, which in this case maybe are not as well known to us as wavelets that have been very well studied analytic.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's consider this a bit more formally.",
                    "label": 0
                },
                {
                    "sent": "I will take some time with this is it's not necessarily the most common setting we have here, so we have a set of samples signals in RN which are normalized to the sphere.",
                    "label": 0
                },
                {
                    "sent": "We have N samples and they are distributed according to some unknown distribution so far.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So good.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                },
                {
                    "sent": "Instead of considering directly some classification loss, what we have is a distance between the best representation we can find, which is done by taking coefficient vector and some dictionary which has P elements on the unit sphere.",
                    "label": 0
                },
                {
                    "sent": "And the signal that we're trying to represent, and we take the representation with minimal Euclidean distance and allowed representations coefficients, we allow our from some set which enforces some form of simplicity.",
                    "label": 0
                },
                {
                    "sent": "So we will consider a hard sparsity constraint in the form of HK, which means that at most K coefficients are not zero and also a relaxed R Lambda concept of sparsity, where we bound.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point norm.",
                    "label": 0
                },
                {
                    "sent": "And just to get an idea of what these mean, what we have here is a diagram diagram for each of them, of the precisely representable signals.",
                    "label": 0
                },
                {
                    "sent": "Of course, in a very low dimensional setting that we can actually draw.",
                    "label": 0
                },
                {
                    "sent": "So at the top we have the polytope that comes from the convex Hull of the dictionary elements which lie on a sphere, so everything inside the polytope can be represented precisely and everything outside will have some error, and in the bottom case what we see is the dictionary elements.",
                    "label": 0
                },
                {
                    "sent": "In black, and if we assume here K is 2, then the signals we can represent precisely or exactly those great circles which are spanned by every two of those elements.",
                    "label": 0
                },
                {
                    "sent": "And I might ask you to imagine on top of these sphere and the distance from each point on the sphere to these two bodies we see here give us exactly the meaning of HA for the appropriate sparsity constraint for a single dictionary.",
                    "label": 0
                },
                {
                    "sent": "For every signal on the sphere.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On top of that, something more standard.",
                    "label": 0
                },
                {
                    "sent": "We may take a loss function.",
                    "label": 1
                },
                {
                    "sent": "For example, we actually care about the quadratic distance or just the distance as it is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And our goal is to learn a particular dictionary that minimizes the expected loss on our unseen distribution.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to fix our ideas in image restoration.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And or in denoising, EG pretty common setting is for end to be 64 corresponding to either Patch of images or simply a measurement of all the electrodes.",
                    "label": 1
                },
                {
                    "sent": "Dictionary size might be somewhat larger, and the sparsity coefficient would be significantly smaller than the dimension.",
                    "label": 0
                },
                {
                    "sent": "So this is what we mean by the sparse representation.",
                    "label": 0
                },
                {
                    "sent": "This is where compression for example would come from, and hundreds of thousands of examples are.",
                    "label": 0
                },
                {
                    "sent": "Reasonable data set.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we considered already how to.",
                    "label": 0
                },
                {
                    "sent": "Lossy compression comes about in the case of denoising, you might think that if images are sparse in this basis, however, the noise does not come from this basis, it will not be easy to represent, and therefore when we find the sparse representation, it will go away and the case of compressed sensing theory tells us approximately that quelaag North, which is again significantly smaller than North measurements, will suffice to reconstruct the right signal.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some relation of this to other problems.",
                    "label": 0
                },
                {
                    "sent": "In the case of K = 1, The hard constraint for representation actually sort of retrieves K means clustering.",
                    "label": 1
                },
                {
                    "sent": "The problem of finding the best Centers for representing for doing quantization, which carries with it a guarantee of NP hardness and also a gap between lower and upper bounds.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For learning.",
                    "label": 0
                },
                {
                    "sent": "In terms of the optimization problem itself, just finding the right representation is itself an NP hard problem unless we make some assumptions about the geometry of the dictionary D.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And ignoring that, putting that aside for example, also in the case of learning dictionaries for the relaxed class of representations, we simply have a very high dimensional, pretty high dimensional space in which we have a nonconvex objective.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we'll go back to the generalization.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, about the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "There have been many algorithms people use this for various applications, so they care about getting good results and they do.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Empirically, but there are no actual algorithmic guarantees for these for this procedure.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Therefore, we opt for uniform convergence results.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's set the ideas about the general generalization problem.",
                    "label": 0
                },
                {
                    "sent": "So we have a dimension signal dimension in dictionary size P. Some particular sample size M and the coefficient set A and we denote with the empirical distribution, for example.",
                    "label": 1
                },
                {
                    "sent": "Then we want to bound the difference or we want to bound the expected error for representation using this dictionary in terms of.",
                    "label": 0
                },
                {
                    "sent": "The average performance on the sample and also.",
                    "label": 0
                },
                {
                    "sent": "Some epsilon which we will try to bound and understand how it depends on the problem Param.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's.",
                    "label": 0
                },
                {
                    "sent": "So the previous result that we already had is a I think beautiful set of results by modern puntel.",
                    "label": 0
                },
                {
                    "sent": "Which are they mentioned free so.",
                    "label": 0
                },
                {
                    "sent": "And then goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "That does not.",
                    "label": 0
                },
                {
                    "sent": "Sample complexity does not.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, it is limited to the quadratic loss and also there is a dependent on.",
                    "label": 0
                },
                {
                    "sent": "Lambda, which is quadratic in Lambda.",
                    "label": 0
                },
                {
                    "sent": "Lambda, so we're talking about the case.",
                    "label": 0
                },
                {
                    "sent": "The relaxed class R. Lambda, where Lambda is bound on the L1 coefficients.",
                    "label": 0
                },
                {
                    "sent": "On the L1 norm of the coefficient vector.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                },
                {
                    "sent": "So we will see later that we care about large Lambda and so this is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not quite sufficient for us, and therefore we propose a different proof proof method that deals with any Lipschitz Lawson gives us only log rhythmic dependence and Lambda at the cost of being mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dependent and also has a variation with fast rates.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By standard.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Standard methods and.",
                    "label": 0
                },
                {
                    "sent": "These bounds are occasionally even vacuous.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what will go quickly through a proof for the R Lambda case?",
                    "label": 0
                },
                {
                    "sent": "The idea is simply that the mapping between a dictionary and the function of distances from the polytope two points on the sphere is Lipschitz continuous uniformly, and this makes sense if we consider moving a particular point and they won't go into many more details because it's not that more that much interesting.",
                    "label": 0
                },
                {
                    "sent": "And beyond that we use standard covering number arguments.",
                    "label": 1
                },
                {
                    "sent": "So the covering numbers are basically preserved by live.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "Now we go back to.",
                    "label": 0
                },
                {
                    "sent": "We continue to the case of hard sparse representation, hard sparsity constraints on the representation, and here the situation is not quite as nice.",
                    "label": 0
                },
                {
                    "sent": "So we remember our dictionary with these three dictionary elements here.",
                    "label": 0
                },
                {
                    "sent": "There's no problem if we move one of the dictionary elements a bit, then the subspaces also move a little bit and then also the error function changes only slightly.",
                    "label": 0
                },
                {
                    "sent": "However, if we have two dictionary elements, for example that are very close to one another, then one of them doesn't have to travel very far to go around the other one and thus have.",
                    "label": 0
                },
                {
                    "sent": "The set of exactly representable signals move very quickly and also have error function change on much of the sphere, so to be Lipschitz we have to stay away from these singularities.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This stability property that we require will come from definition that is reasonably commonly used in sparse representations literature called the Bell function and definition might seem a bit large.",
                    "label": 0
                },
                {
                    "sent": "Basically we consider representations of size K, so we have one dictionary element, and then another K, and we consider the sum of the magnitudes of the inner products between this.",
                    "label": 0
                },
                {
                    "sent": "One dictionary element and every other K. And this we will require to be.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other than one so other bounds or other requirements on this magnitude, allow us to find approximation ratios for algorithms and it is also has been shown to be sufficient for allow compressed sensing with random projection.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We show that first that choosing the dictionary D at random from many copies of the sphere gives us.",
                    "label": 0
                },
                {
                    "sent": "With high probability dictionaries that have this property of the Lobell function, and more Interestingly.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We show that this is sufficient for stability, so if.",
                    "label": 0
                },
                {
                    "sent": "If the Bell function is bounded away from one below, it is below one on the way from it, then actually any optimal representation we find is also in our Lambda.",
                    "label": 0
                },
                {
                    "sent": "That is to say it has a bound on its L1 norm which has size at most K / 1 minus Delta.",
                    "label": 0
                },
                {
                    "sent": "So we can consider the class of dictionaries that fulfill this and then use the results we showed before for our Lambda.",
                    "label": 1
                },
                {
                    "sent": "And here we see that the dependence on Lambda.",
                    "label": 1
                },
                {
                    "sent": "May be significant because of the nearness of Delta to one.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we consider a sketch of the proof for this stability in this case.",
                    "label": 0
                },
                {
                    "sent": "So we consider we have some dictionary D and some representation potential representation A and it's supported on only these three columns of the, so the subset of those columns we call DK, we take the gram matrix that is the inner product of the used columns with one another.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have two interesting properties for this gram matrix.",
                    "label": 0
                },
                {
                    "sent": "First of all, on its diagonal we have the norms of the dictionary elements, which are one because they are in the sphere and the second one is that the sum of off diagonal elements on each row is bounded away from one.",
                    "label": 0
                },
                {
                    "sent": "In fact it's low one because of the develop function restriction.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By the girls going this this theorem, this implies that all the eigenvalues are within distance Delta of one, and in particular this implies they're not at zero.",
                    "label": 0
                },
                {
                    "sent": "Therefore G is invertible and we have a bound on its norm.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic stability.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we have some details.",
                    "label": 0
                },
                {
                    "sent": "Basically, for every signal X we can find the optimal representation a under some particular support by using the pseudo inverse.",
                    "label": 0
                },
                {
                    "sent": "The norm of this part has already been bounded and the norm of this is basically bounded by the fact that there are inner products of a dictionary column with a signal, both of which have norm one.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And from there it's a few details and this applies of course to every a supported in every subset of size K therefore.",
                    "label": 0
                },
                {
                    "sent": "Plays in general.",
                    "label": 0
                },
                {
                    "sent": "Particularly for the optimal representation.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To summarize, we've shown new convergence bounds for L1 dictionary learning, which do not depend too strongly on Lambda.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've generalized these bounds to apply also to the case of hard sparsity constraints.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We found a little evidence that this restriction is reasonable in a sense that many dictionaries fulfill it, and in the paper we also consider a little bit dictionary learning.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No spaces.",
                    "label": 0
                },
                {
                    "sent": "Some open problems.",
                    "label": 0
                },
                {
                    "sent": "Well, there are quite a few gaps in the in terms of the.",
                    "label": 0
                },
                {
                    "sent": "Sample complexity there is first one we inherit from clustering problem, but also we have not shown that the Bell function requirement is necessary.",
                    "label": 0
                },
                {
                    "sent": "There may be reasonable upper bounds without it, so lower bound or an upper bound that would close that would be interesting and also different problem that now becomes I think more interesting is finding algorithms with theoretical guarantees for performance for dictionary learning and.",
                    "label": 1
                },
                {
                    "sent": "It may be useful, though not necessarily that interesting theoretically, to consider Bell function regularization for algorithms of the style that already exists.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "So just to make sure I understand the results were only for dictionaries with where the double function was bounded or strictly smaller than one and bounded away by some number, so any dictionary is smaller than with the Bible function is smaller than 0.9 or so.",
                    "label": 0
                },
                {
                    "sent": "We think this would imply if I only if I when I learned the dictionary, I minimize my reconstruction in also bound the Bible function.",
                    "label": 0
                },
                {
                    "sent": "Do people do that?",
                    "label": 0
                },
                {
                    "sent": "Not that I know of that despite the fact that there seems to be motivation for it anyway, since the algorithms used later for reconstruction already have guarantees only when the bubble function is bounded similarly.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}