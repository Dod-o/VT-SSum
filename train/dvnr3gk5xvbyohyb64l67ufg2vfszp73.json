{
    "id": "dvnr3gk5xvbyohyb64l67ufg2vfszp73",
    "title": "Confidence in nonparametric credible sets?",
    "info": {
        "author": [
            "Aad van der Vaart, Department of Mathematics, Faculty of Sciences, Vrije Universiteit Amsterdam (VU)"
        ],
        "introducer": [
            "Michael I. Jordan, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "recorded by": [
            "Kyoeisha"
        ],
        "published": "Aug. 22, 2012",
        "recorded": "June 2012",
        "category": [
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/isba2012_van_der_vaart_confidence/",
    "segmentation": [
        [
            "I'd like to welcome everybody to the first day of the IS but World Meeting.",
            "My name is Mike Jordan from University California, Berkeley.",
            "My job today is just to keep the speakers happy and so Luckily I was given this fan and I'm supposed to fan the speaker if he gets hot.",
            "So I will be standing here fanning the speaker.",
            "So these lectures are the traditional is but tutorial lectures, but they've been slightly renamed and repurposed there, being named the Isabelle Lectures on Bayesian foundations to give them a slightly more grand title, and these are being videotaped.",
            "They will appear on the website soon after the meeting and you'll be able to click there and enjoy them again and again and again.",
            "And hopefully people throughout the world will also be enjoying them.",
            "That's the whole point, really.",
            "There's hundreds of thousands of people behind you were sitting out there as a potential audience for these lectures, so I really think is a wonderful thing to do.",
            "For isba, this is the beginning.",
            "We're going to four of these lectures today and future is been meetings.",
            "We will also do this and soon I think will have a large library full of great lectures that will be watched out throughout the world and I can't think of anything we can do to get out in the world better than this.",
            "So our first speaker with no further ado add vendor of art from from light and he really needs no introduction.",
            "I teach at Berkeley the Asymptotical course and I use ads book.",
            "I think it's fair to say that it's changed the lives of many students at Berkeley and lead to higher quality research being done.",
            "It's A wonderful book.",
            "An odd writes beautifully and thinks beautifully.",
            "It's really the tradition of mathematical statistics going back to people like like naming and Walled Lake.",
            "Com and so on.",
            "And add is sort of in our generation.",
            "When the leading members of that grand tradition, so I'm more than delighted to invite up to the stage and have him give the first lecture.",
            "OK, thank you very much for this kind introduction is great, pleasure an great honor to speak here today and what I would like to speak about this nonparametric Bayesian statistics and then in particular I have my attention will go to credible sets and certainty quantification by nonparametric Bayesian methods."
        ],
        [
            "The layout of the talk is, well, there are four parts very brief part.",
            "I'll say what non parametric basis and then there will be parts setting the ground for the main part which is incredible.",
            "Sets in three and four and the ground parties on Gaussian process priors.",
            "Mostly what I be talking about as examples are priced at a Gaussian process is but I think the things I say.",
            "Will be valid for other priors as well, even though we don't have any mathematical results on those.",
            "Today you see four nice pictures here and I'll explain them in more detail later, But the idea there is that we see a true function which we trying to estimate and the true function is this black curve and there is a posterior distribution which has a posterior mean.",
            "That's the blue curve and then you see something Gray and that is some indication of the posterior distribution.",
            "How spread it is, and you see that in the 1st and the 4th picture it's reasonably good.",
            "Well, the first the first picture has a small signal to noise ratio, so the the curve the posterior mean is not good, it it gives only the global properties of the true curve.",
            "But at least the uncertainty margin looks OK.",
            "In the fourth there there is a little noise and both the posterior mean and the uncertainty quantification seems to be OK.",
            "In the 2nd and the third you see something goes wrong, the Gray doesn't.",
            "Capture the true curve, the black curve, and so somehow nonparametric base fails and like to get some insight in why that happens and when that happens."
        ],
        [
            "This is based on joint work with four kotis.",
            "First three are students Bartek and both want are here and also giving poster presentations.",
            "Suzanne's not here and for 4th is hard, even Stanton.",
            "Well also trying to look like a student here in this young days, but he's a bit older."
        ],
        [
            "I make a disclaimer that's also because this is taped in the old days.",
            "You just gave a talk and then after the talk everything was gone.",
            "You could say what you liked and you could make some mistakes.",
            "Then you had a time of slides and people asked your slides.",
            "Afterwards you had to be a bit careful about what you wrote and now I'm even being videotaped.",
            "So for years to come I understand this is website will show all my mistakes as I hope I won't make too many mistakes, but I certainly try to make the statements a bit simpler than they really are.",
            "So that it's better for this presentation."
        ],
        [
            "So no paramedic base.",
            "What's it about?"
        ],
        [
            "We would like to estimate some function and so if we do that to Bashan way, we need to prior on a function space.",
            "I don't want to have this, but.",
            "And that's a complicated object.",
            "We could visualize something like that by some draws.",
            "Do I go too fast.",
            "So here you see a regression problem.",
            "The dots are data and we would like to estimate a regression function for the dots.",
            "And you see three dashed curves, and they're actually three realizations from a prior on function space there rather straight here.",
            "But actually they are weekly functions, and in the resolution of the.",
            "The screen you they look a bit straight.",
            "They actually Gaussian process realizations and this is just three draws from the prior and you see, the prior law suggests that a little bit to the data and at the height of the curves is right.",
            "But it doesn't doesn't follow the regression curve in the data.",
            "Of course, we haven't seen the data yet.",
            "When we get to prior.",
            "The idea is then that we do get the data.",
            "These dots.",
            "We get this."
        ],
        [
            "The demand then we update our prior to a posterior, which again is a complicated object.",
            "It's a distribution of probability distribution on a function space, and you can also visualize test by taking some draws from it, which I did here and you see somewhat complicated crowd, but there are curved dashed curves and what they do is they sort of follow the data so the Bayesian analysis was successful in that the prior which had wrong realizations not following the data was updated by the data.",
            "To a posterior that produces the right sort of thing."
        ],
        [
            "The question will ask is does this indeed work?",
            "Does this give good reconstructions?",
            "So if you look at the sort of center of that cloud, does that follow the data?",
            "In this case it does, but more.",
            "An important question for today is thus the posterior distribution also give a correct sense of the remaining uncertainty in the data.",
            "Not only would be tax ID average of all those draws from the posterior, which would be the posterior mean, and that will be the estimated point estimate for the true curve.",
            "We would also look a little bit at the spread in these weekly things you see there and we would hope that somehow the width of the bands tells you something about how certain you can be of your.",
            "Reconstruction, that's what you use the full posterior distribution for, well, does that work?",
            "So these are the two users recovery and quantifying remaining uncertainty."
        ],
        [
            "And I'll do this in the case of Gaussian priors.",
            "So on this sheet the only thing changed that now I put Gaussian priors here and the prior is.",
            "A gaussian.",
            "Houston."
        ],
        [
            "Example.",
            "I take.",
            "The logistic regression model.",
            "So that's just we have data exist in Vice and we have any of them.",
            "And why is a 01 variable and the X is a covariate, we model the probability that wise one given is some value by the logistic transform of fatal fix and in the usual model you would take a linear function here fix with a few parameters and now we're doing this nonparametric.",
            "So this data is really unknown function that we only want to specify up to saying that it has some smoothness or something like that, but no.",
            "Specific shape the Bayesian analysis that we do here is we put a prior on the Theta and I took for these two pictures, integrated Brownian Motion Brownian motion is a well known Gaussian process.",
            "An integrated just means taking the primitive that makes it a little bit smoother and is also scaling factor in front of it, and then I produce these two pictures.",
            "Actually I in that package produced these two pictures.",
            "They have the same data these two pictures, so the same realization or fixes and wise I just use two different priors in both pictures.",
            "This red curve that is the true curve.",
            "I simulated this data so I knew what a true was.",
            "The black curve is to posterior mean and within like you can also easily produce so-called kredible bands and they made stats these blue things and they made in such a way that you fix some argument.",
            "XX is 1 dimensional here.",
            "And then you go to the.",
            "Posterior distribution of Theta attics so marginal posterior distribution.",
            "So 1 dimensional thing and you can take a central interval indebt marginal posterior distribution.",
            "Here I took 95%.",
            "And that gives you upper upper and lower bound, and that's exactly the values of the two curves.",
            "You do that for all lakes, and so you get 2 bands.",
            "I took two different priors both time it's integrated to Brownian motion, but the scaling that I used is different for the two and you see I get different results.",
            "Soda prior somehow influences what I get for the posterior.",
            "That's not a that is not a surprise, but it's even true if N is fairly large, you can have this effect now in the left picture the bands to contain the truth, so that's something we like.",
            "Somehow the prior the posterior.",
            "Captures the truth.",
            "The posterior mean is a bit off that information in the rate is not very high, but at least the posterior.",
            "The full posterior will tell you that the mistake you make is not bigger than your distance to the truth.",
            "Here the difficult part of the reconstruction sticks out of the band, so you are kind of off.",
            "The question is, can we understand why that happens and when will that happen?",
            "Will when will that not happen?"
        ],
        [
            "Here's the second example.",
            "This is so you could say example from physics.",
            "First we define some function U which is a solution of a partial differential equation.",
            "Here this is the heat equation and we have boundary conditions here and we have an initial condition at time 0.",
            "So we have U is a function of space and time at time zero the US 0 is some function takes and edits out parameter.",
            "In this problem the whole function at time zero what we will.",
            "Observe is the function you at time one?",
            "We don't see how it develops, but we know which starts at some unknown function at time zero that we want to know and at times zero we observe it with some noise that will be the data here will be the function you at.",
            "I'm sorry time one edit with some noise to it.",
            "Also, how can we do that?",
            "You can write this convenient way of doing this is to expand data in a serious with coefficient Theta I basis functions here, which could be the eigenbasis of the operator that's involved here.",
            "It gives me exactly the final thing and then you can say I just put a prior on Theta by putting.",
            "Gaussian priors on every coefficient in a series expansion are take them independent for different basis function.",
            "There's some parameters of course, in the Gaussians attacked him mean zero, but there's some parameters in the variance, so I'll come back to later.",
            "Then the data is as I said, the function of time one.",
            "OK if Theta I called it here plus some notes.",
            "And these are two pictures that come out and these are.",
            "Cases where we have small small enlarge ends and you see data reconstruction becomes better if N is larger.",
            "That's not surprising if you have less noise.",
            "We also have incredible bands.",
            "There's this green thing and you see the black one.",
            "That's the true one again, and in both cases it seems to work fine.",
            "The testings here again are some simulations from the posterior."
        ],
        [
            "Those were just simulation examples.",
            "People in practice of course do this all the time, and I took two more or less random examples.",
            "This is 1 from genomics where there was time here as the index for the function and its abundance of a transcription factor.",
            "That one is trying to estimate.",
            "The authors in the paper they produced the kredible bands on the picture and I don't think they asked the question whether that really works.",
            "If you just see this.",
            "We don't know the truth here.",
            "Then you think this is highly informative because we have a posterior mean and it is within the kredible vents.",
            "Well, that's by construction, so there's no guarantee that the truth is really in there unless we can prove some theorems about that, that it is generally the case."
        ],
        [
            "Here is another example where the function is the surface it comes from Earth sign and its travel times of surface waves.",
            "The left side gives the well.",
            "If you do a surface then it's hard to give this kredible bands in a picture, but what is done here at this left picture is the posterior mean.",
            "It's a heat map of the posterior mean, so the colors give the height of the surface in this thing and the right picture gives the posterior standard deviation.",
            "So that is your measure of uncertainty in the.",
            "Posterior mean or not really the posterior mean, but in the posterior distribution."
        ],
        [
            "OK, now in the rest of the talk I'll use the following notation.",
            "As usual, we have prior model, so Theta comes from some prior distribution.",
            "We have some data which given the Theta has some density P and then we form the posterior which is just a conditional Theta given by nothing special here.",
            "That's the same in nonparametric or parametric we always get the posterior by the usual formula relating the prior by the likelihood an.",
            "I'll look at 2 uses.",
            "One is recovery by the posterior distribution.",
            "So we take the mean of this posterior distribution and we see if that is close to something that we wanted to be close to, and the posterior is used as an expression of uncertainty.",
            "And then you're talking about the spreading the posterior distribution one beige and way of doing that is to look at incredible set which is the set which depends on the data in the parameter set.",
            "So could be these bands around the function and it's made in such a way that posterior mass of that set.",
            "Given why is 95%?",
            "Well, this doesn't say what sort of set it is, but typically you would take some set surrounding the posterior mean in some way."
        ],
        [
            "Now it's very hard to study that in in theory, and So what we're going to do is asymptotics, and I'll have an index end that will express DSM topics and will tend to Infinity, and end will have several meanings in several examples.",
            "Could be the noise level, could be the number of observations, and so on, and then the rest I make everything dependent on end, and even allowing a prior depends on it to see what it does.",
            "And everything is the same."
        ],
        [
            "And if you do some studies on this, then you have to become a frequentist, at least partly because.",
            "So yeah, the posterior is just what you get from your prior, and that is basically end of Bayesian analysis.",
            "You can marginalized it and extract all kinds of information, but you cannot say anything whether it is good or bad, unless I think you become at least for the moment of frequentist.",
            "And how do we do that?",
            "We assume that now the data is not generated as in our Beijing setup first parameter from prior.",
            "And then data from conditional given parameter.",
            "But actually there is a true function date are not out there.",
            "So I do notice later not.",
            "I call it the truth and the data is generated according to death date or not.",
            "And then you can study the posterior distribution under that sampling distribution of wire.",
            "I'll study two things here.",
            "The rate of contraction and will say that rate of contraction is at least some value.",
            "Epsilon N double depends on everything in particular under truth.",
            "If this equation holds and what that says is, I need to measure a distance to the truth somehow, and I took a metric D here could be a uniform distance out with the band or some L2 distance, some distance between Theta and Theta.",
            "Not, and I'm going to look for balls around Theta, not of radius epsilon in.",
            "And what I like is to find epsilon N such that those balls under the posterior distribution.",
            "Contain all the mass.",
            "Or in other words, the probability under the posterior that distance of the.",
            "Under the posterior from the truth is bigger than epsilon end posterior mass outside a ball of radius epsilon men around the truth.",
            "That should be very small should go to zero.",
            "Well, this is the asymptotics, so we get an error.",
            "We want that posterior probability to be small and then in asymptotics we can say we will look for the epsilon ends such that the expected value of that goes to 0, or which is the same data.",
            "Posterior probability goes to 0.",
            "In the picture there will be maybe something like if this red one for this picture would be the true curve then and all the realizations are here.",
            "Then the absolute end should be somewhere at the boundary of these things.",
            "The width of the curve somehow."
        ],
        [
            "Then the second thing is that's recovering.",
            "The second thing is uncertainty quantification.",
            "Incredible region.",
            "I defined on the previous slide some set of posterior mass 95%, but we can also look at the frequentist math of that and this is treating the credible set as a confidence set and I.",
            "Calculate its coverage in the frequentist sense.",
            "How I look if the YN comes from the distribution index by the state or not?",
            "Whether the state are not discovered by the kredible set.",
            "A frequentist would call CNY M to be a confidence set of."
        ],
        [
            "95% if that would be exactly 95%.",
            "In a more general sense, you might say that."
        ],
        [
            "The 95% would be nice if it would be exactly 95%, but we don't have to be frequentist, we but we would like to make this.",
            "Is disc readable set to have?",
            "Give it some meaning so."
        ],
        [
            "If the data gave incredible set like this, it's much more narrow around the curve then we would want that to mean something."
        ],
        [
            "And then this and so maybe we are not so much interested in in the 95% could say, oh that's frequentist."
        ],
        [
            "Constance but at least we would like to the posterior to to express the remaining uncertainties somehow.",
            "So up to a constant.",
            "Maybe it should be OK.",
            "It shouldn't be completely off."
        ],
        [
            "Now what do the frequentist say here?",
            "And it's nice to have a benchmark so that we can look at the posterior distributions.",
            "What I do and compare it to the frequentist world an you have to keep in mind.",
            "Then for the rest of the talk this rate in NN is by using topical parameter and I set up the models all the time that a typical rate becomes a power of N and the power is a bit funny with the parameter beta divided by two beta plus DDS.",
            "The dimension of the of the function of the curve.",
            "So for function would be one for Surface will be 2.",
            "And beta here is something like the smoothness of the.",
            "The truth later not curve.",
            "This is a typical rate that one gets in nonparametric estimation.",
            "Nonparametric theory is very much concerned with estimating smooth functions, so you assume that some true function has a certain smoothness and then you get this rate.",
            "If beta is the smoothness.",
            "Much more precisely, you get that a minimax rate of estimation of smooth functions has that order, and that I gave here.",
            "So we have this discrepancy.",
            "D as a distance I squared it, it's often done.",
            "And then I squared the rate here as well.",
            "You look at the frequency distribution of YN and arbitrary estimator.",
            "So we look for the best estimator T for the state or not.",
            "We take the risk which is this expectation and we soup it.",
            "Overall sets of truth that are beta smooth with the derivative bounded by something that's the little one here.",
            "So later not his beta times differentiable and the derivative is bounded by one and we take the soup over all those functions and then we look for the best estimator.",
            "That is the minimax rate of estimation, and that comes out well if you square, it comes out as this, so this is the rate.",
            "As beta is bigger than we assume or on the truth, we assume it's smoother and in fact it approaches enter minus 1/2 for beta is in infinite, but nonparametric situations beta will be finite and so this will tend to zero slower.",
            "It will be harder to estimate.",
            "Now the the nicest thing about frequentist analysis here is that one can get estimators T that attain this rate for any beta even without knowing those betas, and those are called adaptive estimators.",
            "Oak."
        ],
        [
            "Let me turn to Gaussian process."
        ],
        [
            "Prius.",
            "A Gaussian process is some stochastic process.",
            "Here you see a realization of a surface.",
            "It's a Brownian sheet in this case, and so stochastic process and its distribution can.",
            "Function as a prior.",
            "So realization is our model Ford unknown function.",
            "They're very useful because there are many different types of them.",
            "You can model almost any sort of thing with a Gaussian process, and also because you do can do computations easily in a number of examples with Gaussian processes, and every prior is reasonable in some way, although tuning by some hyperparameter over Gaussian process is often desirable.",
            "Come back to that."
        ],
        [
            "Here's some examples Brownian motion.",
            "I already showed you one example.",
            "This is 1 realization of Brownian motion, so you would say my true function.",
            "I think it looks a bit like this.",
            "Most people find it very quickly, and in fact a Brownian motion is not differentiable, so it's true.",
            "It's very quickly so one way of making it smoother would be to take the primitive function of this and that's this.",
            "You could also integrate it another time, then it becomes more differentiable and it's this one.",
            "And if you integrate it out of time, is this.",
            "The striking thing about these four pictures, I think, is that the first One, Brownian motion, looks very different from the second one.",
            "But when you go 234, if you integrate more and more, you eyeball no difference anymore.",
            "It's very hard to judge the nonparametric prior by just looking at a realization from it.",
            "These are all nice smooth curve.",
            "The shapes of these curves.",
            "They don't mean anything.",
            "There is just a particular realization.",
            "You sort of look how it changes in in space."
        ],
        [
            "The many other Gaussian process, these are two examples of stationary process.",
            "Is stationary means that if you take an interval here in space or time, then the distribution of the process here is the same as when you take an interval elsewhere around the motion doesn't have that because variance increases with time.",
            "Is an example of a stationary that's very smooth.",
            "In fact, this is an analytic function.",
            "You see it's it's basically, but it is very smooth and this is an example of a function with smoothness 3 / 2 so 1 1/2 derivative, whatever that may mean.",
            "And that's coming from the maternal class, which you can do for any smoothness except three with second and you see a visible difference here.",
            "If I would make this muther, you would not see so clearly that there is a difference."
        ],
        [
            "And here are some other examples.",
            "Fractional Brownian motion, Brownian sheet and a serious prior that I also looked at already.",
            "You take your favorite set of basis functions.",
            "Those are deterministic functions and you add Gaussian coefficients.",
            "Coefficients generated from Gaussian distributions that give a Gaussian process as well."
        ],
        [
            "Now here's the main result.",
            "A main result on posterior contraction.",
            "So the recovering if you use a Gaussian prior.",
            "Anne.",
            "I need to little bit notation I to make this theorem true.",
            "We need to prior to be Gaussian map in a band of space for Brownian motion.",
            "For instance, continuous functions could be something else, but in any case we have some norm that we can measure distances for the prior like the D, but not exactly the same as the D that I had before data not will be the true parameter in that space.",
            "Then the basic theorem is that rate epsilon N of contraction.",
            "Is given by this equation.",
            "Here you solve this equation and you go for the smallest solution epsilon N that satisfies this.",
            "Now we're looking for the best we can get for this thing and that goes for the smallest one that satisfies this.",
            "That is the rate you get where there is something.",
            "Of course I need to say something about what the sampling model is an for that.",
            "At the moment I have this line if the statistical distances on the model on the likelihood combine appropriately with this setting of the Gaussian.",
            "Process then we get this equation.",
            "What is this equation?",
            "Well, this is a.",
            "This is about a bull around the true function in this, this particular norm could be a uniform ball around late or not, and this is the probability that a prior gives to that ball that a realization of the W gets into that ball.",
            "The probability should be big enough, which is quite understandable that you get something like that 'cause you trying to recover Theta, not so you have to put some math.",
            "At state are not in the neighborhood of South, or not, so that next when you hit it with the likelihood, the likelihood can correct the prior to put a lot of mass next to state or not.",
            "If there's no math educator, not certainly.",
            "The posterior will also never target or not, so you need some math while you get exactly.",
            "This equation is a bit harder to see and I won't explain that.",
            "This is called to a centered small ball probability small because the epsilon N will tend to 0, so this is really about a tiny little book."
        ],
        [
            "I said if the statistical distances on a model combine appropriately within Norm and I can show you what that means for some."
        ],
        [
            "Examples don't want to do all the details here, but for instance, if you do density estimation, you would have a picture ID say in compact interval an you might make a density model for that with a Gaussian process prior by taking the Gaussian process, which is a thing that has Gaussian marginal distribution.",
            "So it's positive and negative.",
            "You want to make it a density you want to make it positive so you can put it in the exponent and then you have to re normalize to make it integrate to one.",
            "So this would be some.",
            "Prior model for the density.",
            "Then the theorem is true if the normal Gaussian process is the uniform norm and distance on the parameter on the on.",
            "The densities here is the challenge here.",
            "Classification I've shown you how to fit Gaussian processes in classification and well, there are some distance is here as well.",
            "On the Theta that combined with the distance on this and you can go through other meaningful statistical situations.",
            "Regression regarded diffusions and other ones where you have that is theorem is valid if you take the correct statistical distance.",
            "So he had a healthy distance here.",
            "Some Lt L2 distance and so on.",
            "But I skip the details.",
            "Put it."
        ],
        [
            "For inverse problems and the heat problem is an example of an inverse problem and I will come back to it.",
            "So let me say that the rate equation is somewhat different and we don't do not quite understand that in generality, but we have some calculations for special cases that I'll show you later."
        ],
        [
            "I'd like to 1st translate this general theorem on rate of contraction with Gaussian priors in the following way.",
            "I had one condition and I make it into two.",
            "That makes it more complicated, but it also helps illustrate something about Gaussian process priors.",
            "I introduced here the so called Smallball exponent and what is that?",
            "This is the prior mass of a ball around zero.",
            "That's the little zero.",
            "Here is the ball around 0.",
            "Before I had a ball around the true function state or not.",
            "And now I take it at zero and the prior mass of their balls here written as E to the minus five not epsilon, and it's useful to take that exponent because typically that's mobile probability is very small and so it's E to the minus something big.",
            "And the big thing is dentify not epsilon.",
            "Now that thing appears in the theorem, the rate of contraction epsilon N should satisfy this equation for the small ball probability is the same equation basically as before about the presence of prior mass.",
            "But now that the truth is being replaced by zero.",
            "There's a second equation.",
            "This equation does not involve the state or not.",
            "As you can see, it just involves the prior, and that's the thing I like to explain that you can pull this condition apart into 2 one only about a prior and the other will involve the truth.",
            "Here is the state or not, while the second one is a bit more complicated and I'm not going to go into details of this equation, but it involves the reproducing kernel Hilbert space of the Gaussian prior.",
            "Every Gaussian process has a thing like that.",
            "I'm not going to explain what it is because.",
            "Either you know it already or you will not understand it in 5 minutes, but let's just say there is some equation here that you have to solve which involved the state are not and then the Gaussian priors.",
            "The combination of the two.",
            "And you have to satisfy both, so you get a lower bound for the rate for both."
        ],
        [
            "Locations one from only.",
            "The prior and the other also from the combination of the truth and the prior."
        ],
        [
            "Let me show you in a few cases what gifts.",
            "Suppose I take the prior to be a Brownian motion.",
            "This very weekly thing.",
            "Then what is the small ball probability I try to explain that in this picture, if I take the uniform norm, then uniform norm, a smaller ball is a is a band and I hope you can see there are two red curves here and everything in between is a small bowl.",
            "It's a it's a ball around the midline of these two red curves, the small ball.",
            "Probability of Brownian motion is the probability that the Brownian sample pass stays inside this bent.",
            "That's not likely.",
            "The realization of Brownian motion that I show you here is not within the bent at all, and you can see that it will be unlikely that one will stay in there all the time at first has to start here.",
            "Of course, insight that's not so bad.",
            "That's a single Gaussian distribution, and it has small probability because it's a small interval here, but with some probability will start, but then it has to stay there all the time, and Brownian motion very quickly so wants to get out of the band all the time.",
            "Well, turns out one can compute.",
            "This small bowl probability and it is E to the minus one over epsilon squared, so the exponent is one over epsilon squared.",
            "Each of the minus one over epsilon squared.",
            "So if epsilon is very small, if you have a tiny band here then it is going to be very very small.",
            "Eat in a minus something very large.",
            "Now, the fact that that is so small will give you by the 1st equation in the rate theorem and enter the mind is 1/4 rate independent of the truth, whatever the truth is, you will never go faster than enter.",
            "The mind is 1/4 of Brownian motion.",
            "And that's what you see here.",
            "If I take a function Theta, not it's in C beta.",
            "That means it's beta times differentiable.",
            "It's smooth of order beta, then the rate is entered minus 1/4 as soon as beta is bigger than 1/2.",
            "So no matter how smooth that function was, I will never get better than tonight is 1/4.",
            "That's very disappointed because for a very smooth function I told you before, the rate in nonparametric estimation is entered minus beta over 1 + 2 beta.",
            "And that approach is enter.",
            "The mind is 1/2.",
            "You go to the parametric rate.",
            "If debate is really large, doesn't help in this case.",
            "To make the beta larger, you never get past this low rate.",
            "If the beta is happens to be very small, then you also punished.",
            "You get entered minus beta over 2, and in fact if you compare it to this minimax rate antonym minus beta over 1 + 2 beta, which is what you can get with a kernel estimator or wavelet estimator or whatever, then you get the optimal rate only if they die, so half.",
            "And a half anyway, is the smoothness of Brownian motion.",
            "So what it says then is simply that if your prior says truth is smooth of order half, then you do it correctly for a smooth or truth that is also smooth border half.",
            "But you don't do it well for other truth.",
            "In particular, smoothness doesn't help, which is very very disappointing from a nonparametric estimation point of view.",
            "OK."
        ],
        [
            "Brownian Motion is a very.",
            "Rough thing and we wouldn't like that as a prior anyway.",
            "If we wanted to estimate a smooth function.",
            "So let me take the integrated Brownian motion, let me integrate it Alpha minus 1/2 times, then becomes Alpha smooth.",
            "Brownian motion is 1/2 smooth.",
            "So if I integrated Alpha minus half times then I get Alpha smooth integrated Brownian motion.",
            "That's exactly right.",
            "If your true function is also of smoothness, Alpha if it matches, and otherwise you get something which is suboptimal.",
            "But it is like.",
            "Like this?",
            "Small ball probability is now bigger and you can see that from this picture right here is the sample pass.",
            "It's not in a small bowl, but you can easily imagine since the simple passes much smoother if it starts insight and there is more probability that it stays inside because it is smooth and it tries to go on smoothly."
        ],
        [
            "Other example are stationary processes.",
            "You could you can make stationary process by setting up the covariance function with some spectral measure like this.",
            "And I'll look at 2."
        ],
        [
            "Particular examples, one very smooth one which has a spectral measure, Gaussian measure, like this that gives very smooth sample path and then we get these rates here and then.",
            "Best stated in terms of the Fourier transform of the true parameter and I'm in D dimensions here.",
            "And the two cases if the true parameter is very smooth itself, like the prior, you get a very nice rate near one over root N, which is not nonparametric.",
            "Almost any more than ear means.",
            "There's a log factor missing there from one over root in, so you're doing very well as if you have a finite dimensional parameter that you're estimating.",
            "This is a condition on the Fourier transform to say that faith or not is very smooth.",
            "If, on the other end, the true function is beta smooth and in terms of the Fourier transform, that will be conditioned like this, then the rate is not even a power event, but that becomes a power of one over log in, so you're severely punished by taking a prior that is really good for smooth functions.",
            "Then you severely punished if the true function is actually not super smooth, then everything gets lost.",
            "You could say you get.",
            "Hardly any convergence.",
            "While you could have a power of end there, but you get a power of 1, overlook it."
        ],
        [
            "Then these other classes to maternelles were somewhat less smooth than you can adapt to smoothness.",
            "If you take the Alpha here, then it becomes Alpha smooth.",
            "This was the three and a half.",
            "Sorry 3 / 2 smoothness and that will be exactly right.",
            "We all have the exact match if Alphas.",
            "Beta addendum attorneys is right.",
            "Now, in practice, maybe you would scale the price."
        ],
        [
            "So let's look at scaling the priors in time and in space.",
            "You could, if you want to have a prior for functions on 01.",
            "What you could do is you could take a process and you could run it for a little while and then pull it out like this, scale it up so that it covers to complete time set."
        ],
        [
            "Or you could maybe run a prior Gaussian process for a long time and then scale it in so that you have a function on 01 and you see in the picture that.",
            "Well, certainly here you see very smooth here, and if you scale it becomes somewhat rougher.",
            "Not in an analytic sense, the number of derivatives doesn't change by this operation, but in a Bayesian sense it does in fact.",
            "And here also the.",
            "The smoothness."
        ],
        [
            "Increases so if we do integrate a time Brown integrated Brownian motion and we scale that in time, then it turns out that you can do a scaling factor depending on N here and the scaling factor.",
            "Depends also on the true target smoothness of the prior and then you get a prior scaled integrated Brownian motion which gives the optimal rate for any true function up to smoothness.",
            "Beta drawback is that you need debater to do this particular prior, but in any case that is a scaling that works for any Theta well up to K + 1/2.",
            "If this is K times integrated you can go to K plus 1/2.",
            "Sorry.",
            "K + 1 Northgate was a half not very smooth, but you can get."
        ],
        [
            "So stretching can help a little.",
            "You can go up two K plus 1/2 shrinking that goes all the way down to 0.",
            "For smoothness."
        ],
        [
            "You could also start with this very smooth Gaussian process and then basically you would only want to shrink it so that you make it rougher, and it turns out there is indeed also a scaling there that makes it right exactly for a beta smooth truth.",
            "With this scaling factor you get the right result, so scaling thus change everything.",
            "An shrinking a super smooth seems to be a very nice way of getting getting a good recovery with your prior."
        ],
        [
            "Now this scaling factors they depend on the true Smoothens beta, which we don't know and about maybe you're willing to guess.",
            "Say that it is too late or something else.",
            "But if you cannot guess, then you probably want to do something else.",
            "You might want to do the scaling based on the data.",
            "That's called adaptation.",
            "So we could try adapt prior to the data in two ways by the hierarchical base metadata.",
            "Empirical based methods by the hierarchical based methods.",
            "I would take a scale of priors.",
            "Maybe of several smoothness levels, and then I would put a prior on the smoothness level, or maybe on a scaling factor and then do full base with not a Gaussian prior anymore because it was a mixture of Gaussians.",
            "But any case that gives a full base method empirical base, you would get the regularity from the data and a typical method is to take the marginal distribution of the data.",
            "So integrating out the prior.",
            "Leaving into smoothness there and then optimize maximum likelihood estimator on the smoothness.",
            "And then we use that data determined smoothness.",
            "Well, the hierarchical base thing we know quite a bit already.",
            "It works in some generality, many different ways of setting it up.",
            "The empirical base.",
            "We actually know very little except some some examples I'll give you."
        ],
        [
            "Nice example of dialogical base approach.",
            "This works in this way we choose a gamma.",
            "Well, we choose a scaling factor, in fact these.",
            "Power of the scaling factor from a gamma distribution.",
            "I take this very smooth Gaussian process and then I scale that with the random scale coming from the.",
            "The gamma, so I take a deep root of a gamma distribution and it works in quite generality.",
            "If you have a smooth beta smooth truth beta derivatives, then indeed you get the minimax contraction rate, which is mixture of Gaussian processes.",
            "So nearly entered minus beta over 280 + D. Also you keep the nice properties of Satan, not a super smooth.",
            "Then the right scaling or no scaling got you nearly enter the minus half.",
            "You still get it with the scaling, so the scaling doesn't.",
            "The random skating doesn't confuse the full based.",
            "Random scaling doesn't confused the prior, and it's a very nice result in Bayesian nonparametric estimation because it means a full base approach can solve the bandwidth problem, because what we're choosing here is the smoothness that is the typical bandwidth.",
            "If you do a kernel estimator or a serious estimator, it's the truncation.",
            "And you can do that in a fully bashan way, in for instance, with this scheme."
        ],
        [
            "So summary of recovery, the recovery is best if the prior matches the truth and this match will slow down, but it will not prevent recovery.",
            "You will get some convergence rate, but maybe very slow.",
            "It can be prevented.",
            "This match match mismatch by using hyper parameters and then I get into the kredible sets."
        ],
        [
            "This is just my notation."
        ],
        [
            "I'll just remind you of what I called incredible set.",
            "I have the posterior distribution and then credible set is set in the posterior typically taken central around the posterior mean that has 95% of the posterior math."
        ],
        [
            "Anne will look at the coverage of this incredible set by treating the incredible set as a as if it were a confidence set in the frequentist sense.",
            "So we look at the coverage in the frequentist set."
        ],
        [
            "There's an earlier answer to this question, but it works or not and answer was it doesn't work.",
            "This is a paper by Dennis Cox from 1993 and he looks at a particular regression problem.",
            "It's a bit difficult to read this whole paper.",
            "It's very clever, very smart, but his conclusion is rather terrible.",
            "That's what he says.",
            "No invasions often find such basin procedures.",
            "That's the credible set attractively cause.",
            "The frequentist coverage probability of the basin regions tends to the interior coverage probability in typical cases.",
            "Was my hope that this would also halt in a non parametric setting so the paramedics setting with finite dimensional models everything is fine and we all knew that before 1993 and Cox settle I hoped it was also true in the non parametric setting.",
            "Unfortunately they hoped for result is false in about the worst possible way.",
            "Visibly this equation.",
            "And what does it say?",
            "It's here, it's the coverage and you see that the limb.",
            "The limit of the coverage is 0, which is terrible.",
            "There's no coverage at all, which means no correct uncertainty quantification.",
            "Not only that, you don't get in 95% which we could live it, I think, so 95% is rather arbitrary, but you just get zero.",
            "When do you get this?",
            "You get this for all the state are not that you.",
            "Sample from your prior, so that seems terrible.",
            "Nuessen it's strange that this paper hasn't been quoted before, and that we still doing nonparametric estimation by without really investigating this further."
        ],
        [
            "Cox model can actually be cast in a in a simpler form.",
            "It's a sequence model where the function is developed in an infinite serious with a fixed base set of base functions and the Theta ice come from Gaussian priors as I talked about before.",
            "So the data then become just independent observations on these coefficients in each coefficient is measured with a variance of 1 / N an error variance of 1 / N and I also have a multiplication here Ki that.",
            "That is, a known number could be one in the case of Cox it is 1.",
            "So the observation has mean Kappa times Theta I.",
            "We put a prior on Theta of which are just independent Gaussians with some variances that will tend to zero.",
            "Typically because we have infinitely many of those."
        ],
        [
            "Now to make that's a bit more easy to read, will just say that the data is some infinite dimensional Gaussian.",
            "It has a certain mean.",
            "That's the function of data, and it has a certain variance, which is essentially one over in the independent, and the prior is also an infinite dimensional Gaussian attesa certain covariance Lambda.",
            "Then if you see those two equations you see it's just conjugate, it's just the mean here.",
            "The case actually linear."
        ],
        [
            "So you can workout with the posterior wrist.",
            "In that case the posterior is also Gaussian, infinite dimensional and you have to multiply the data with some matrix operator.",
            "Here the formula is given here, but it's of no importance for us, just a linear transformation underway and it's going to be some posterior spread posterior covariance which are called S formulas.",
            "Dear but has no importance for the talk.",
            "What is may be interesting is that the but not surprising that this is not data dependent, so the data is in the posterior mean.",
            "The posterior spread is just some fixed covariance.",
            "Then the credible set.",
            "In this case you would wouldn't would make.",
            "That's what Cox also looks at is to make a ball around the posterior mean, and then you would give the ball of radius such that the radius makes that if you take a centered Gaussian, then the ball has exactly.",
            "Probability point 905 so that is a credible set for 95% level in this situation.",
            "So it's pretty simple setup."
        ],
        [
            "To see what we get, I'll show you.",
            "Results for truth and Prius the truth again, I will have the parameter beta which gives the smoothness of the function.",
            "In this case I'll measure smoothness with this over left space.",
            "That is, this is so Theta the true one is a serious expansion where the coefficients if you multiply by the square and multiply to betas fire.",
            "This basically regularity beta is before.",
            "And the prior then we have a variance here to play with in the prior will take the variance to be 1 / I two the 2A plus one and by doing these two things what I get is S before I can compare the Alpha and the beta.",
            "If the Alpha is the beta then roughly can say the prior and the truth match.",
            "If Alpha is bigger than beta then the coefficients under the prior code 20 faster the variance is 10 to 0 faster.",
            "So the prior over smooth this.",
            "And if Alpha smaller than beta, then the prior under Smith is the truth.",
            "So the interpretation of Alpha and beta are exactly as in the first part of the proof."
        ],
        [
            "I'm not being completely precise in the sense that this norm here, which is Sobolev norm.",
            "Sometimes you should take the soup, but I will not make that difference here, because then becomes."
        ],
        [
            "Very complicated.",
            "Now first we can talk about the contraction rate.",
            "In this case, there's nothing very special about it.",
            "You get the contraction rate, which is the optimal minimal Max rate.",
            "There's a little P in here now.",
            "Also, because it's an inverse problem, so the inverse nature is to K is 1P0 in the case I just one, but then the contraction rate is exactly as I had before.",
            "It's optimal if prior and.",
            "Truce, match and otherwise it's sub optimal.",
            "But you always get contraction, so whatever you miss, specify it still works in."
        ],
        [
            "Innocence.",
            "Now let's look at a credible sets, and let's look at some pictures.",
            "1st and I show you some picture for an inverse problem.",
            "This particular inverse problem, it's called Volterra operator, which is just a primitive functional Theta.",
            "So Theta is my parameter to function that I want to estimate.",
            "We take the primitive function and then what we observe is well, actually an integral of that.",
            "This is the physicist notation.",
            "The derivative of our data that's a little dot here is that primitive plus some noise.",
            "So we want to recover a function from noisy observation of its primitive.",
            "That's an inverse problem, very nonparametric."
        ],
        [
            "And it fits in the sequence serious, and you can work out what the expansions are, but let's not."
        ],
        [
            "Look at it, let's look at the picture.",
            "In pictures.",
            "The priors in five pictures on the left are the same and the prior in the five pictures on the right are also the same and I just repetitions with multiple data.",
            "So every five everyone at 5I generate the data and then data Bashan machine in the same way the same prior.",
            "The black one.",
            "Here is the truth.",
            "See number one before, but it was red but now it's black and the red one is the posterior mean and then you see the weekly things are things generated from the prior, so it seems to work quite well here.",
            "Well, in a sense we don't have enough data to get complete recovery.",
            "That's why the red one is not the black one, but the uncertainty margin given by the posterior suggested by the posterior is such that we know that we might may be off by that much, and that is true if you repeat it in the data every time on the right is different.",
            "You can see that, well, the estimate the red estimate is not that much worse than it is on the left, but the confidence bands have shrunk quite a bit.",
            "Everything I generated from the posterior is quite smooth, and here they well they failed to cover it here.",
            "Here there's a big part is covered here.",
            "This whole thing sticks out of the confidence band and so on.",
            "So here the priors very smooth and we do not cover the truth, which is somewhat.",
            "Comforting becausw the posterior is all we will have we generate from the posterior and then we hope that these dash things give you some sense of how far you might be wrong with your thing.",
            "Is it dependent on?"
        ],
        [
            "The amount of data I have.",
            "Well, if I take a bigger end so I less noise in the data then it doesn't change dramatically.",
            "In fact, here in the smooth case it gets very bad because the kredible bands here are just strong within the resolution of the picture to the posterior mean, and we missing completely the true curve.",
            "The posterior thinks that it's very sure that the truth is right on the posterior mean, but it wasn't.",
            "It's the black one, so we seem to have a problem."
        ],
        [
            "Can we do some math on that?",
            "Yeah, we have this particular theorem.",
            "So the situation is just linear inverse problem and we had a posterior which was Gaussian.",
            "And we made incredible set as a central bolinda Gaussian posterior.",
            "Then the theorem goes like this.",
            "We have a prior that had a smoothness, Alpha and a truth that has a smoothness beta.",
            "Now if Alpha smaller than beta.",
            "So if we under smooth the truth then we do have coverage and it's one uniformly.",
            "So it all works nicely.",
            "Maybe even two nice because we didn't get 95%.",
            "But actually we got one.",
            "If we have an exact match then becomes a bit complicated.",
            "Any coverage within 01 occurs.",
            "But if you don't take the 95% too serious, then nothing terrible is is happening.",
            "You sort of get the right thing, however, if the prior is smoother than the truth, the right set of pictures then thinks are disastrous for some beta smooth one, not for all one, because if it's beta smooth as can be much smoother.",
            "But there are things that are exactly beta smooth that are over smooth by this prior and then the asymptotical reaches 0.",
            "That is what you saw in the right picture.",
            "We have two smooth prior and everything goes to pieces.",
            "The posterior thinks it's can be very sure of where the truth is, but it is absolutely wrong.",
            "It's the prior that told it to be sure, but the prior was wrong relative to the truth.",
            "The good news is that in these two first cases the Kredible ball also has the correct order of magnitude, so drop this thing about a 95% and what the coverage is exactly.",
            "You could look at whether the size of the bull is sort of right.",
            "Up to constant, it's always right.",
            "If you do not over smooth.",
            "If you're over smooth then you get 2 narrow balls."
        ],
        [
            "Where does Cox result come in?",
            "Well, it doesn't follow from this theorem, but can be explained from the theorem that truth generated from an Alpha smooth prior.",
            "They belong with probability 12, smoothness things of smaller smoothness.",
            "But I don't belong to this Alpha smooth set and that's why it can be in the third case.",
            "That you get coverage 0."
        ],
        [
            "No, I'm skipping this for reasons of time.",
            "Well, this is about rescaling.",
            "We can I do very briefly.",
            "You can also re scale these price with the scaling parameter here and see if you can repair it in the contraction rate business.",
            "We could do that.",
            "I won't go through the theorem but I'll show."
        ],
        [
            "The pictures, if you do a rescaling then you can get it right after all, and this is for deterministic rescaling."
        ],
        [
            "So credible sets the first summary.",
            "In a non parametric set up the prior is not watched out by the data for recovery.",
            "We saw that the prior influences the posterior contraction rate but you get consistency for most priors.",
            "So it's it's gonna be bad.",
            "But it is not too bad.",
            "You always get something for the uncertainty quantification.",
            "It can be disastrous.",
            "And that happens if the prior if the prior takes mistakes.",
            "The truth for being more regular than it is.",
            "The posterior will then be 2 concentrated and center far away from the truth.",
            "So both ways it doesn't wrong.",
            "One solution to that would be to under smooth.",
            "So take a prior that is not smooth with the truth.",
            "That would have to adapt would require that you know something about the minimal smoothness of your true function, and then you can fix the prior to that.",
            "Now don't on the smooth too much, because then.",
            "The contraction rate, though it still will be something tending to 0.",
            "So you do get consistency, but it will.",
            "The recovery will be slow, so you should not under smooth too much.",
            "I I finished with this part by saying that much work to be done we have which I showed you results for the linear inverse Gaussian problem.",
            "We also have some results for Gaussian regression, but as many things to be asked here."
        ],
        [
            "And this is the heat equation problem.",
            "Just again to show you that there might be trouble you see prior stat increase in smoothness and here it's still OK. Years still OK.",
            "Here you get in the price that over smooth and here they vary over smooth.",
            "And this is for a bigger end, but the message is the same."
        ],
        [
            "My final part of the talk will be about.",
            "Adapt incredible sets to the day."
        ],
        [
            "ETA for recovery.",
            "We saw that if your prior is not matching the truth, you can make it match the truth by using a hyperparameter, and you could do that in your higher logical empirical base and set up.",
            "How does that work for kredible sets?",
            "Does it also work?"
        ],
        [
            "That well."
        ],
        [
            "Skip that.",
            "Again, I'll work in the linear Gaussian inverse problem and we actually have no results for the general Gaussian.",
            "Situation, though I don't think it will be very different in general, so the same setup as before.",
            "So we have the data from this infinite Gaussian.",
            "We have a prior, we have a covariance where I now make the Alpha explicit that is the smoothness of the prior.",
            "It's the rate of decrease of the eigenvalues of the Lambda Alpha.",
            "We get.",
            "The posterior were also made the Alpha explicit and we can forgive and Alpha take the kredible ball.",
            "Now suppose that we determine an Alpha based on the data using the empirical based methods.",
            "So for debt.",
            "You get an alphabet.",
            "The estimate by saying that YN forgiven Alpha.",
            "If you take data Scotian, Ryan, give and take discussion, then Ryan marginally has a Gaussian distribution, which is mean zero and has this covariance that contains the Alpha.",
            "And then you do maximum likelihood on the Alpha.",
            "In this model is you work that out.",
            "It's disting.",
            "It's kind of complicated with an infinite series, so it's not very obvious to analyze, but it can be done it quite explicit in this situation.",
            "Anyway, empirical base here takes the Alpha data dependent based on marginal maximum likelihood."
        ],
        [
            "This works for recovery, but I will not present results.",
            "The question is, does it also work for uncertainty quantification and more precisely then if I take the bull that you had forgiven Alpha, and if I stick in the Alpha heads there, does that cover so determined Alpha based on the data and then take them here I do empirical base we believe at the moment at hierarchical base where you put a prior on Alpha is will give similar results, but we haven't written that are completely out."
        ],
        [
            "Counterexample horrible doesn't work at all.",
            "Credible sets can be terribly wrong in this empirical base situation, and this is the pictures that I started with.",
            "Now we had a black truth.",
            "We had a blue posterior mean, and in Gray we had posterior credible dance 95%.",
            "First, last pictured works and what is different here is the informative of the data informativeness of the data.",
            "The noise level here is there any small letter and it's big.",
            "The truth is the same in all four pictures, and the data is regenerated, of course, and these are two intermediate values of in.",
            "You cannot.",
            "Well, it doesn't work.",
            "You can see that the two middle, once they are terrible, and it seemed to be interaction between the noise level and the truth.",
            "In this case for another truth for other values of N it might go wrong.",
            "What this is, is a counter example of a truth.",
            "There is a particular function that we took the black one and for that this particular procedure goes wrong for these values of N here.",
            "We can make other counterexamples, but they are also truthful, which it does work.",
            "Let me, for the last five minutes."
        ],
        [
            "Try to see where this comes from.",
            "First of all, do the frequentist say about this?",
            "They have a concept of honesty of confidence sets.",
            "Honesty means that the coverage is at least 95% for any possible through data, not for all so uniformly in the truth.",
            "So they're not here should contain all possible truth or truth you think might be valid.",
            "Could for instance be a Sobolev bull, then it's a known result that if you know the beta you want to be uniform.",
            "Overall functions with beta derivatives you can get confidence sets of.",
            "Honest confidence sets of the estimation rate enter minus beta."
        ],
        [
            "There is a problem though, if as soon as you allow more than one smoothness level.",
            "Here all smoothness levels bigger than some lower bound rate or not, then the diameter of the confidence sent necessarily we all have to involve this.",
            "Biggest their smallest biggest model.",
            "The smaller smoothness later not.",
            "So there is something nasty going on here.",
            "If you want to be honest over everything here, everything smooth and better not then you get a diameter, debt cannot.",
            "Adapt really to debater, it's determined by the biggest model and bit complicated thing.",
            "Here there are some other ways of describing this."
        ],
        [
            "Depends a bit on the metric, but I will skip all that."
        ],
        [
            "So there is a difference in frequentist literature between adaptive estimation and uncertainty quantification.",
            "If you do adaptive estimation, there is a success story that starts in the 1990s with wavelets and and so on.",
            "A more regular true function is easier to estimate.",
            "It should be, there's less degrees of freedom.",
            "Estimators can then be simultaneously optimal for multiple regularity's you don't have to know that it has 10 derivatives, you can just do that adapting from the data.",
            "And Bayesian estimators we now know since a couple of years can also achieve this by prior on the bench with on a bend with parameter of the problem.",
            "So that is a success story.",
            "You don't have to know smoothness, you can still do very nice things.",
            "Uncertainty quantification is very different.",
            "Honest uncertainty quantification must argue from the worst case, the smallest possible regularity level, and you cannot adapt so much to the smooth."
        ],
        [
            "As a Lucian Bersia phrased it in a discussion here, adaptive estimators from the success story they do the best that's possible in view of the properties of the underlying function to be estimated.",
            "That's quite satisfactory, but he estimated does not tell you how well it does.",
            "You have no idea about your order of magnitude of the distance, so you're doing well, but you don't know how well you're doing, and so you cannot really give the uncertainty margin on your estimator.",
            "And that seems to be in the nature of things."
        ],
        [
            "I'm.",
            "This is an attempt to solve that problem by looking only at particular truth and they called self similar here by this complicated equation you say something about the Sobolev norm high up at higher resolution levels should satisfy a lower bound.",
            "The interpretation is that the truth has the same character at any resolution level.",
            "An very vague idea is that if you have a noisy data set you can estimate.",
            "The function only up to a certain resolution level and you could say there is something like an affective dimension where you can still do it with their data.",
            "Above that you cannot do it and self similar sequences are the same at every possible affective dimension.",
            "An for dose you can get frequentist nice, uniform, honest confidence sets."
        ],
        [
            "And the good news is that you can also get good basean sets that adapt for those truth with empirical base.",
            "So for those sequences is a special class of sequences, it works."
        ],
        [
            "And you see that in this picture this one was just the prior matching the truth.",
            "And here I scaled the truth and it suddenly has kredible bands that do satisfied and this true function, the black one is a self similar one.",
            "That's a nice one."
        ],
        [
            "Where do then Lisa problems arise?",
            "Well, this is the Cox result again where you gotta O well the."
        ],
        [
            "Thing you might notice in the paper is that if you would blow up the kredible set a little bit, yes by log factor you suddenly get one instead of 00.",
            "Blowing it up works."
        ],
        [
            "And then I have to conjecture which is about we're going to finish that in the Bayesian adaptive sense.",
            "There is something similar true, and you can do it for every Alpha at the same time with the empirical base Alpha said honesty is questionable in this case."
        ],
        [
            "So then where does this go wrong?",
            "Well, this truth is not self similar, and what actually?",
            "This was a very smooth Troost debate.",
            "Aciro and.",
            "You could say that the black one is unlike anything that is sampled from some prior that I used.",
            "I used a whole scale of price with different smoothness is but this one was actually smoother than anyone.",
            "It had sequences that were non 0 up to some level and then all zeros and none of the things that come from the prior have that.",
            "And it seems that that is bad news.",
            "But I I have difficulty interpreting what it really means, so here are then conclusions."
        ],
        [
            "Conjectures nonparametric readable sets are never correct.",
            "Frequentist confidence regions."
        ],
        [
            "Priced at under smooth the truth.",
            "Give a reasonable idea of the uncertainty in the posterior mean."
        ],
        [
            "If the prior over smokes is the truth and spread in the posterior is very misleading about a remaining uncertainty."
        ],
        [
            "And that effect may disappear if the prior is scaled, for instance by hierarchical empirical Bayesian methods, but only for truth that resemble the prior."
        ],
        [
            "At least, so we think at a moment."
        ],
        [
            "And."
        ],
        [
            "Finally, it seems then that we must either under smooth or belief, to find details of the prior."
        ],
        [
            "And is that possible in nonparametrics?",
            "Then I make it open ended.",
            "I show you the picture again, Can you believe when you see these priors that one of those is really the best for your data?",
            "Or do these three, for instance look exactly the same?",
            "Is it really possible to know what you're doing in Nonparametrics?",
            "So I keep it with that question.",
            "Thank you very much.",
            "Thank you OD.",
            "So there are time.",
            "There's time for some questions.",
            "If there are any other microphones down in front.",
            "If you'd like to ask question, please come down.",
            "Maybe I could ask briefly about the the favorable case, the recovery, the full base handled the problem, and I noticed a gamma distribution means stuck in there at the top, and I assume that that this can't be true for all distributions, but I assume gamma is not needed and can you tell us what the story is there?",
            "Yeah I can.",
            "In fact it is true for all gammas, so all the parameters, then we have to realize it is asymptotics and in the asymptotics the parameters of the gamma will not come true.",
            "And you do you do select the correct bandwidth, but then maybe at higher order you would have to find 2 that for your data.",
            "Yeah.",
            "Thank you next.",
            "I had a question about one of your slides where you said shrinking with the Super smooth can adapt it to anything.",
            "I was wondering what's the relation between shrinking the Super smooth, by which I think you mean the squared exponential Gaussian process.",
            "What's the relationship between shrinking it horizontally and just taking a smaller horizontal length scale parameter L?",
            "In this case, I think it's very different in the in the integrated Gaussian case it would be the same because it's self similar.",
            "But here I think it is different.",
            "I actually I don't know what would happen if you would scale the vertical thing here.",
            "OK, I. I don't think it will work, but I don't really know.",
            "I shouldn't have said this because it's all taped, but.",
            "And I'll just quickly one of your main conclusions was about over smoothing.",
            "And can I take that to imply that if you use a squared exponential covariance which is infinitely smooth, then you in some sense for uncertainty quantification, always miss the truth unless you have, as you say, excellent reason to know that the truth really is beta Infinity.",
            "I think the answer is yes, so if you do not re scale the in the very smooth Gaussian process, I think that the credible sets will be completely off, but we have no mathematical proof.",
            "We only have some proofs of this fact for.",
            "A few typical examples, but this one we haven't handled yet.",
            "Thank you.",
            "OK, time for one more question.",
            "Hang on, I'm sorry.",
            "Line please, I'll try to keep it short.",
            "Your results made me made me wonder about the possibility of defining the rate of contraction and defining coverage, not relative to the truth data, but relative to a theater that's in a class that has smoothness as specified by the prior but is close to the truth data in some sense.",
            "And I wonder if you thought about about that.",
            "Yeah, you might have a very good idea.",
            "We have thought about it, but we don't know how to make that precise and to do with it yet.",
            "But it might be good in a contraction rate that is more or less included because you always look for something in the inside.",
            "The support of the prior that is at a certain distance with incredible sets.",
            "It is about the formulation of what do you.",
            "What can you derive really?",
            "What can you learn from the posterior distribution and maybe it should be phrased in the sense that some approximation of the truth.",
            "Is within the posterior support somehow?",
            "Yeah, but we have thought about it, but we don't know how to formulate it at the moment.",
            "OK, let's give it one more round of applause for add.",
            "Thanks for the great talk."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'd like to welcome everybody to the first day of the IS but World Meeting.",
                    "label": 1
                },
                {
                    "sent": "My name is Mike Jordan from University California, Berkeley.",
                    "label": 0
                },
                {
                    "sent": "My job today is just to keep the speakers happy and so Luckily I was given this fan and I'm supposed to fan the speaker if he gets hot.",
                    "label": 0
                },
                {
                    "sent": "So I will be standing here fanning the speaker.",
                    "label": 0
                },
                {
                    "sent": "So these lectures are the traditional is but tutorial lectures, but they've been slightly renamed and repurposed there, being named the Isabelle Lectures on Bayesian foundations to give them a slightly more grand title, and these are being videotaped.",
                    "label": 0
                },
                {
                    "sent": "They will appear on the website soon after the meeting and you'll be able to click there and enjoy them again and again and again.",
                    "label": 0
                },
                {
                    "sent": "And hopefully people throughout the world will also be enjoying them.",
                    "label": 0
                },
                {
                    "sent": "That's the whole point, really.",
                    "label": 0
                },
                {
                    "sent": "There's hundreds of thousands of people behind you were sitting out there as a potential audience for these lectures, so I really think is a wonderful thing to do.",
                    "label": 0
                },
                {
                    "sent": "For isba, this is the beginning.",
                    "label": 0
                },
                {
                    "sent": "We're going to four of these lectures today and future is been meetings.",
                    "label": 0
                },
                {
                    "sent": "We will also do this and soon I think will have a large library full of great lectures that will be watched out throughout the world and I can't think of anything we can do to get out in the world better than this.",
                    "label": 0
                },
                {
                    "sent": "So our first speaker with no further ado add vendor of art from from light and he really needs no introduction.",
                    "label": 0
                },
                {
                    "sent": "I teach at Berkeley the Asymptotical course and I use ads book.",
                    "label": 0
                },
                {
                    "sent": "I think it's fair to say that it's changed the lives of many students at Berkeley and lead to higher quality research being done.",
                    "label": 0
                },
                {
                    "sent": "It's A wonderful book.",
                    "label": 0
                },
                {
                    "sent": "An odd writes beautifully and thinks beautifully.",
                    "label": 0
                },
                {
                    "sent": "It's really the tradition of mathematical statistics going back to people like like naming and Walled Lake.",
                    "label": 0
                },
                {
                    "sent": "Com and so on.",
                    "label": 0
                },
                {
                    "sent": "And add is sort of in our generation.",
                    "label": 0
                },
                {
                    "sent": "When the leading members of that grand tradition, so I'm more than delighted to invite up to the stage and have him give the first lecture.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much for this kind introduction is great, pleasure an great honor to speak here today and what I would like to speak about this nonparametric Bayesian statistics and then in particular I have my attention will go to credible sets and certainty quantification by nonparametric Bayesian methods.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The layout of the talk is, well, there are four parts very brief part.",
                    "label": 0
                },
                {
                    "sent": "I'll say what non parametric basis and then there will be parts setting the ground for the main part which is incredible.",
                    "label": 0
                },
                {
                    "sent": "Sets in three and four and the ground parties on Gaussian process priors.",
                    "label": 1
                },
                {
                    "sent": "Mostly what I be talking about as examples are priced at a Gaussian process is but I think the things I say.",
                    "label": 0
                },
                {
                    "sent": "Will be valid for other priors as well, even though we don't have any mathematical results on those.",
                    "label": 0
                },
                {
                    "sent": "Today you see four nice pictures here and I'll explain them in more detail later, But the idea there is that we see a true function which we trying to estimate and the true function is this black curve and there is a posterior distribution which has a posterior mean.",
                    "label": 0
                },
                {
                    "sent": "That's the blue curve and then you see something Gray and that is some indication of the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "How spread it is, and you see that in the 1st and the 4th picture it's reasonably good.",
                    "label": 0
                },
                {
                    "sent": "Well, the first the first picture has a small signal to noise ratio, so the the curve the posterior mean is not good, it it gives only the global properties of the true curve.",
                    "label": 0
                },
                {
                    "sent": "But at least the uncertainty margin looks OK.",
                    "label": 0
                },
                {
                    "sent": "In the fourth there there is a little noise and both the posterior mean and the uncertainty quantification seems to be OK.",
                    "label": 0
                },
                {
                    "sent": "In the 2nd and the third you see something goes wrong, the Gray doesn't.",
                    "label": 0
                },
                {
                    "sent": "Capture the true curve, the black curve, and so somehow nonparametric base fails and like to get some insight in why that happens and when that happens.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is based on joint work with four kotis.",
                    "label": 0
                },
                {
                    "sent": "First three are students Bartek and both want are here and also giving poster presentations.",
                    "label": 0
                },
                {
                    "sent": "Suzanne's not here and for 4th is hard, even Stanton.",
                    "label": 0
                },
                {
                    "sent": "Well also trying to look like a student here in this young days, but he's a bit older.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I make a disclaimer that's also because this is taped in the old days.",
                    "label": 0
                },
                {
                    "sent": "You just gave a talk and then after the talk everything was gone.",
                    "label": 0
                },
                {
                    "sent": "You could say what you liked and you could make some mistakes.",
                    "label": 0
                },
                {
                    "sent": "Then you had a time of slides and people asked your slides.",
                    "label": 0
                },
                {
                    "sent": "Afterwards you had to be a bit careful about what you wrote and now I'm even being videotaped.",
                    "label": 0
                },
                {
                    "sent": "So for years to come I understand this is website will show all my mistakes as I hope I won't make too many mistakes, but I certainly try to make the statements a bit simpler than they really are.",
                    "label": 0
                },
                {
                    "sent": "So that it's better for this presentation.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So no paramedic base.",
                    "label": 0
                },
                {
                    "sent": "What's it about?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We would like to estimate some function and so if we do that to Bashan way, we need to prior on a function space.",
                    "label": 1
                },
                {
                    "sent": "I don't want to have this, but.",
                    "label": 0
                },
                {
                    "sent": "And that's a complicated object.",
                    "label": 1
                },
                {
                    "sent": "We could visualize something like that by some draws.",
                    "label": 0
                },
                {
                    "sent": "Do I go too fast.",
                    "label": 0
                },
                {
                    "sent": "So here you see a regression problem.",
                    "label": 0
                },
                {
                    "sent": "The dots are data and we would like to estimate a regression function for the dots.",
                    "label": 0
                },
                {
                    "sent": "And you see three dashed curves, and they're actually three realizations from a prior on function space there rather straight here.",
                    "label": 0
                },
                {
                    "sent": "But actually they are weekly functions, and in the resolution of the.",
                    "label": 0
                },
                {
                    "sent": "The screen you they look a bit straight.",
                    "label": 0
                },
                {
                    "sent": "They actually Gaussian process realizations and this is just three draws from the prior and you see, the prior law suggests that a little bit to the data and at the height of the curves is right.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't doesn't follow the regression curve in the data.",
                    "label": 0
                },
                {
                    "sent": "Of course, we haven't seen the data yet.",
                    "label": 0
                },
                {
                    "sent": "When we get to prior.",
                    "label": 0
                },
                {
                    "sent": "The idea is then that we do get the data.",
                    "label": 0
                },
                {
                    "sent": "These dots.",
                    "label": 0
                },
                {
                    "sent": "We get this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The demand then we update our prior to a posterior, which again is a complicated object.",
                    "label": 0
                },
                {
                    "sent": "It's a distribution of probability distribution on a function space, and you can also visualize test by taking some draws from it, which I did here and you see somewhat complicated crowd, but there are curved dashed curves and what they do is they sort of follow the data so the Bayesian analysis was successful in that the prior which had wrong realizations not following the data was updated by the data.",
                    "label": 1
                },
                {
                    "sent": "To a posterior that produces the right sort of thing.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The question will ask is does this indeed work?",
                    "label": 0
                },
                {
                    "sent": "Does this give good reconstructions?",
                    "label": 0
                },
                {
                    "sent": "So if you look at the sort of center of that cloud, does that follow the data?",
                    "label": 1
                },
                {
                    "sent": "In this case it does, but more.",
                    "label": 0
                },
                {
                    "sent": "An important question for today is thus the posterior distribution also give a correct sense of the remaining uncertainty in the data.",
                    "label": 0
                },
                {
                    "sent": "Not only would be tax ID average of all those draws from the posterior, which would be the posterior mean, and that will be the estimated point estimate for the true curve.",
                    "label": 0
                },
                {
                    "sent": "We would also look a little bit at the spread in these weekly things you see there and we would hope that somehow the width of the bands tells you something about how certain you can be of your.",
                    "label": 0
                },
                {
                    "sent": "Reconstruction, that's what you use the full posterior distribution for, well, does that work?",
                    "label": 1
                },
                {
                    "sent": "So these are the two users recovery and quantifying remaining uncertainty.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'll do this in the case of Gaussian priors.",
                    "label": 0
                },
                {
                    "sent": "So on this sheet the only thing changed that now I put Gaussian priors here and the prior is.",
                    "label": 0
                },
                {
                    "sent": "A gaussian.",
                    "label": 0
                },
                {
                    "sent": "Houston.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example.",
                    "label": 0
                },
                {
                    "sent": "I take.",
                    "label": 0
                },
                {
                    "sent": "The logistic regression model.",
                    "label": 0
                },
                {
                    "sent": "So that's just we have data exist in Vice and we have any of them.",
                    "label": 0
                },
                {
                    "sent": "And why is a 01 variable and the X is a covariate, we model the probability that wise one given is some value by the logistic transform of fatal fix and in the usual model you would take a linear function here fix with a few parameters and now we're doing this nonparametric.",
                    "label": 0
                },
                {
                    "sent": "So this data is really unknown function that we only want to specify up to saying that it has some smoothness or something like that, but no.",
                    "label": 0
                },
                {
                    "sent": "Specific shape the Bayesian analysis that we do here is we put a prior on the Theta and I took for these two pictures, integrated Brownian Motion Brownian motion is a well known Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "An integrated just means taking the primitive that makes it a little bit smoother and is also scaling factor in front of it, and then I produce these two pictures.",
                    "label": 0
                },
                {
                    "sent": "Actually I in that package produced these two pictures.",
                    "label": 0
                },
                {
                    "sent": "They have the same data these two pictures, so the same realization or fixes and wise I just use two different priors in both pictures.",
                    "label": 0
                },
                {
                    "sent": "This red curve that is the true curve.",
                    "label": 1
                },
                {
                    "sent": "I simulated this data so I knew what a true was.",
                    "label": 0
                },
                {
                    "sent": "The black curve is to posterior mean and within like you can also easily produce so-called kredible bands and they made stats these blue things and they made in such a way that you fix some argument.",
                    "label": 0
                },
                {
                    "sent": "XX is 1 dimensional here.",
                    "label": 0
                },
                {
                    "sent": "And then you go to the.",
                    "label": 0
                },
                {
                    "sent": "Posterior distribution of Theta attics so marginal posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "So 1 dimensional thing and you can take a central interval indebt marginal posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "Here I took 95%.",
                    "label": 0
                },
                {
                    "sent": "And that gives you upper upper and lower bound, and that's exactly the values of the two curves.",
                    "label": 0
                },
                {
                    "sent": "You do that for all lakes, and so you get 2 bands.",
                    "label": 0
                },
                {
                    "sent": "I took two different priors both time it's integrated to Brownian motion, but the scaling that I used is different for the two and you see I get different results.",
                    "label": 0
                },
                {
                    "sent": "Soda prior somehow influences what I get for the posterior.",
                    "label": 0
                },
                {
                    "sent": "That's not a that is not a surprise, but it's even true if N is fairly large, you can have this effect now in the left picture the bands to contain the truth, so that's something we like.",
                    "label": 0
                },
                {
                    "sent": "Somehow the prior the posterior.",
                    "label": 0
                },
                {
                    "sent": "Captures the truth.",
                    "label": 1
                },
                {
                    "sent": "The posterior mean is a bit off that information in the rate is not very high, but at least the posterior.",
                    "label": 0
                },
                {
                    "sent": "The full posterior will tell you that the mistake you make is not bigger than your distance to the truth.",
                    "label": 0
                },
                {
                    "sent": "Here the difficult part of the reconstruction sticks out of the band, so you are kind of off.",
                    "label": 0
                },
                {
                    "sent": "The question is, can we understand why that happens and when will that happen?",
                    "label": 0
                },
                {
                    "sent": "Will when will that not happen?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the second example.",
                    "label": 0
                },
                {
                    "sent": "This is so you could say example from physics.",
                    "label": 0
                },
                {
                    "sent": "First we define some function U which is a solution of a partial differential equation.",
                    "label": 0
                },
                {
                    "sent": "Here this is the heat equation and we have boundary conditions here and we have an initial condition at time 0.",
                    "label": 1
                },
                {
                    "sent": "So we have U is a function of space and time at time zero the US 0 is some function takes and edits out parameter.",
                    "label": 0
                },
                {
                    "sent": "In this problem the whole function at time zero what we will.",
                    "label": 0
                },
                {
                    "sent": "Observe is the function you at time one?",
                    "label": 0
                },
                {
                    "sent": "We don't see how it develops, but we know which starts at some unknown function at time zero that we want to know and at times zero we observe it with some noise that will be the data here will be the function you at.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry time one edit with some noise to it.",
                    "label": 0
                },
                {
                    "sent": "Also, how can we do that?",
                    "label": 0
                },
                {
                    "sent": "You can write this convenient way of doing this is to expand data in a serious with coefficient Theta I basis functions here, which could be the eigenbasis of the operator that's involved here.",
                    "label": 0
                },
                {
                    "sent": "It gives me exactly the final thing and then you can say I just put a prior on Theta by putting.",
                    "label": 0
                },
                {
                    "sent": "Gaussian priors on every coefficient in a series expansion are take them independent for different basis function.",
                    "label": 0
                },
                {
                    "sent": "There's some parameters of course, in the Gaussians attacked him mean zero, but there's some parameters in the variance, so I'll come back to later.",
                    "label": 0
                },
                {
                    "sent": "Then the data is as I said, the function of time one.",
                    "label": 0
                },
                {
                    "sent": "OK if Theta I called it here plus some notes.",
                    "label": 0
                },
                {
                    "sent": "And these are two pictures that come out and these are.",
                    "label": 0
                },
                {
                    "sent": "Cases where we have small small enlarge ends and you see data reconstruction becomes better if N is larger.",
                    "label": 0
                },
                {
                    "sent": "That's not surprising if you have less noise.",
                    "label": 0
                },
                {
                    "sent": "We also have incredible bands.",
                    "label": 0
                },
                {
                    "sent": "There's this green thing and you see the black one.",
                    "label": 0
                },
                {
                    "sent": "That's the true one again, and in both cases it seems to work fine.",
                    "label": 0
                },
                {
                    "sent": "The testings here again are some simulations from the posterior.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those were just simulation examples.",
                    "label": 0
                },
                {
                    "sent": "People in practice of course do this all the time, and I took two more or less random examples.",
                    "label": 0
                },
                {
                    "sent": "This is 1 from genomics where there was time here as the index for the function and its abundance of a transcription factor.",
                    "label": 1
                },
                {
                    "sent": "That one is trying to estimate.",
                    "label": 0
                },
                {
                    "sent": "The authors in the paper they produced the kredible bands on the picture and I don't think they asked the question whether that really works.",
                    "label": 0
                },
                {
                    "sent": "If you just see this.",
                    "label": 0
                },
                {
                    "sent": "We don't know the truth here.",
                    "label": 0
                },
                {
                    "sent": "Then you think this is highly informative because we have a posterior mean and it is within the kredible vents.",
                    "label": 0
                },
                {
                    "sent": "Well, that's by construction, so there's no guarantee that the truth is really in there unless we can prove some theorems about that, that it is generally the case.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is another example where the function is the surface it comes from Earth sign and its travel times of surface waves.",
                    "label": 1
                },
                {
                    "sent": "The left side gives the well.",
                    "label": 0
                },
                {
                    "sent": "If you do a surface then it's hard to give this kredible bands in a picture, but what is done here at this left picture is the posterior mean.",
                    "label": 0
                },
                {
                    "sent": "It's a heat map of the posterior mean, so the colors give the height of the surface in this thing and the right picture gives the posterior standard deviation.",
                    "label": 0
                },
                {
                    "sent": "So that is your measure of uncertainty in the.",
                    "label": 0
                },
                {
                    "sent": "Posterior mean or not really the posterior mean, but in the posterior distribution.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now in the rest of the talk I'll use the following notation.",
                    "label": 0
                },
                {
                    "sent": "As usual, we have prior model, so Theta comes from some prior distribution.",
                    "label": 1
                },
                {
                    "sent": "We have some data which given the Theta has some density P and then we form the posterior which is just a conditional Theta given by nothing special here.",
                    "label": 0
                },
                {
                    "sent": "That's the same in nonparametric or parametric we always get the posterior by the usual formula relating the prior by the likelihood an.",
                    "label": 0
                },
                {
                    "sent": "I'll look at 2 uses.",
                    "label": 0
                },
                {
                    "sent": "One is recovery by the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "So we take the mean of this posterior distribution and we see if that is close to something that we wanted to be close to, and the posterior is used as an expression of uncertainty.",
                    "label": 1
                },
                {
                    "sent": "And then you're talking about the spreading the posterior distribution one beige and way of doing that is to look at incredible set which is the set which depends on the data in the parameter set.",
                    "label": 0
                },
                {
                    "sent": "So could be these bands around the function and it's made in such a way that posterior mass of that set.",
                    "label": 0
                },
                {
                    "sent": "Given why is 95%?",
                    "label": 0
                },
                {
                    "sent": "Well, this doesn't say what sort of set it is, but typically you would take some set surrounding the posterior mean in some way.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now it's very hard to study that in in theory, and So what we're going to do is asymptotics, and I'll have an index end that will express DSM topics and will tend to Infinity, and end will have several meanings in several examples.",
                    "label": 0
                },
                {
                    "sent": "Could be the noise level, could be the number of observations, and so on, and then the rest I make everything dependent on end, and even allowing a prior depends on it to see what it does.",
                    "label": 0
                },
                {
                    "sent": "And everything is the same.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if you do some studies on this, then you have to become a frequentist, at least partly because.",
                    "label": 0
                },
                {
                    "sent": "So yeah, the posterior is just what you get from your prior, and that is basically end of Bayesian analysis.",
                    "label": 0
                },
                {
                    "sent": "You can marginalized it and extract all kinds of information, but you cannot say anything whether it is good or bad, unless I think you become at least for the moment of frequentist.",
                    "label": 0
                },
                {
                    "sent": "And how do we do that?",
                    "label": 0
                },
                {
                    "sent": "We assume that now the data is not generated as in our Beijing setup first parameter from prior.",
                    "label": 0
                },
                {
                    "sent": "And then data from conditional given parameter.",
                    "label": 0
                },
                {
                    "sent": "But actually there is a true function date are not out there.",
                    "label": 0
                },
                {
                    "sent": "So I do notice later not.",
                    "label": 0
                },
                {
                    "sent": "I call it the truth and the data is generated according to death date or not.",
                    "label": 0
                },
                {
                    "sent": "And then you can study the posterior distribution under that sampling distribution of wire.",
                    "label": 0
                },
                {
                    "sent": "I'll study two things here.",
                    "label": 0
                },
                {
                    "sent": "The rate of contraction and will say that rate of contraction is at least some value.",
                    "label": 1
                },
                {
                    "sent": "Epsilon N double depends on everything in particular under truth.",
                    "label": 0
                },
                {
                    "sent": "If this equation holds and what that says is, I need to measure a distance to the truth somehow, and I took a metric D here could be a uniform distance out with the band or some L2 distance, some distance between Theta and Theta.",
                    "label": 0
                },
                {
                    "sent": "Not, and I'm going to look for balls around Theta, not of radius epsilon in.",
                    "label": 0
                },
                {
                    "sent": "And what I like is to find epsilon N such that those balls under the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "Contain all the mass.",
                    "label": 0
                },
                {
                    "sent": "Or in other words, the probability under the posterior that distance of the.",
                    "label": 0
                },
                {
                    "sent": "Under the posterior from the truth is bigger than epsilon end posterior mass outside a ball of radius epsilon men around the truth.",
                    "label": 0
                },
                {
                    "sent": "That should be very small should go to zero.",
                    "label": 0
                },
                {
                    "sent": "Well, this is the asymptotics, so we get an error.",
                    "label": 0
                },
                {
                    "sent": "We want that posterior probability to be small and then in asymptotics we can say we will look for the epsilon ends such that the expected value of that goes to 0, or which is the same data.",
                    "label": 0
                },
                {
                    "sent": "Posterior probability goes to 0.",
                    "label": 0
                },
                {
                    "sent": "In the picture there will be maybe something like if this red one for this picture would be the true curve then and all the realizations are here.",
                    "label": 0
                },
                {
                    "sent": "Then the absolute end should be somewhere at the boundary of these things.",
                    "label": 0
                },
                {
                    "sent": "The width of the curve somehow.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the second thing is that's recovering.",
                    "label": 0
                },
                {
                    "sent": "The second thing is uncertainty quantification.",
                    "label": 0
                },
                {
                    "sent": "Incredible region.",
                    "label": 0
                },
                {
                    "sent": "I defined on the previous slide some set of posterior mass 95%, but we can also look at the frequentist math of that and this is treating the credible set as a confidence set and I.",
                    "label": 0
                },
                {
                    "sent": "Calculate its coverage in the frequentist sense.",
                    "label": 0
                },
                {
                    "sent": "How I look if the YN comes from the distribution index by the state or not?",
                    "label": 0
                },
                {
                    "sent": "Whether the state are not discovered by the kredible set.",
                    "label": 0
                },
                {
                    "sent": "A frequentist would call CNY M to be a confidence set of.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "95% if that would be exactly 95%.",
                    "label": 0
                },
                {
                    "sent": "In a more general sense, you might say that.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The 95% would be nice if it would be exactly 95%, but we don't have to be frequentist, we but we would like to make this.",
                    "label": 0
                },
                {
                    "sent": "Is disc readable set to have?",
                    "label": 0
                },
                {
                    "sent": "Give it some meaning so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If the data gave incredible set like this, it's much more narrow around the curve then we would want that to mean something.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then this and so maybe we are not so much interested in in the 95% could say, oh that's frequentist.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Constance but at least we would like to the posterior to to express the remaining uncertainties somehow.",
                    "label": 1
                },
                {
                    "sent": "So up to a constant.",
                    "label": 0
                },
                {
                    "sent": "Maybe it should be OK.",
                    "label": 0
                },
                {
                    "sent": "It shouldn't be completely off.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now what do the frequentist say here?",
                    "label": 1
                },
                {
                    "sent": "And it's nice to have a benchmark so that we can look at the posterior distributions.",
                    "label": 0
                },
                {
                    "sent": "What I do and compare it to the frequentist world an you have to keep in mind.",
                    "label": 0
                },
                {
                    "sent": "Then for the rest of the talk this rate in NN is by using topical parameter and I set up the models all the time that a typical rate becomes a power of N and the power is a bit funny with the parameter beta divided by two beta plus DDS.",
                    "label": 0
                },
                {
                    "sent": "The dimension of the of the function of the curve.",
                    "label": 0
                },
                {
                    "sent": "So for function would be one for Surface will be 2.",
                    "label": 0
                },
                {
                    "sent": "And beta here is something like the smoothness of the.",
                    "label": 0
                },
                {
                    "sent": "The truth later not curve.",
                    "label": 0
                },
                {
                    "sent": "This is a typical rate that one gets in nonparametric estimation.",
                    "label": 0
                },
                {
                    "sent": "Nonparametric theory is very much concerned with estimating smooth functions, so you assume that some true function has a certain smoothness and then you get this rate.",
                    "label": 1
                },
                {
                    "sent": "If beta is the smoothness.",
                    "label": 0
                },
                {
                    "sent": "Much more precisely, you get that a minimax rate of estimation of smooth functions has that order, and that I gave here.",
                    "label": 0
                },
                {
                    "sent": "So we have this discrepancy.",
                    "label": 0
                },
                {
                    "sent": "D as a distance I squared it, it's often done.",
                    "label": 0
                },
                {
                    "sent": "And then I squared the rate here as well.",
                    "label": 0
                },
                {
                    "sent": "You look at the frequency distribution of YN and arbitrary estimator.",
                    "label": 0
                },
                {
                    "sent": "So we look for the best estimator T for the state or not.",
                    "label": 0
                },
                {
                    "sent": "We take the risk which is this expectation and we soup it.",
                    "label": 0
                },
                {
                    "sent": "Overall sets of truth that are beta smooth with the derivative bounded by something that's the little one here.",
                    "label": 0
                },
                {
                    "sent": "So later not his beta times differentiable and the derivative is bounded by one and we take the soup over all those functions and then we look for the best estimator.",
                    "label": 0
                },
                {
                    "sent": "That is the minimax rate of estimation, and that comes out well if you square, it comes out as this, so this is the rate.",
                    "label": 1
                },
                {
                    "sent": "As beta is bigger than we assume or on the truth, we assume it's smoother and in fact it approaches enter minus 1/2 for beta is in infinite, but nonparametric situations beta will be finite and so this will tend to zero slower.",
                    "label": 0
                },
                {
                    "sent": "It will be harder to estimate.",
                    "label": 1
                },
                {
                    "sent": "Now the the nicest thing about frequentist analysis here is that one can get estimators T that attain this rate for any beta even without knowing those betas, and those are called adaptive estimators.",
                    "label": 0
                },
                {
                    "sent": "Oak.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me turn to Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prius.",
                    "label": 0
                },
                {
                    "sent": "A Gaussian process is some stochastic process.",
                    "label": 0
                },
                {
                    "sent": "Here you see a realization of a surface.",
                    "label": 0
                },
                {
                    "sent": "It's a Brownian sheet in this case, and so stochastic process and its distribution can.",
                    "label": 0
                },
                {
                    "sent": "Function as a prior.",
                    "label": 0
                },
                {
                    "sent": "So realization is our model Ford unknown function.",
                    "label": 0
                },
                {
                    "sent": "They're very useful because there are many different types of them.",
                    "label": 0
                },
                {
                    "sent": "You can model almost any sort of thing with a Gaussian process, and also because you do can do computations easily in a number of examples with Gaussian processes, and every prior is reasonable in some way, although tuning by some hyperparameter over Gaussian process is often desirable.",
                    "label": 1
                },
                {
                    "sent": "Come back to that.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's some examples Brownian motion.",
                    "label": 1
                },
                {
                    "sent": "I already showed you one example.",
                    "label": 0
                },
                {
                    "sent": "This is 1 realization of Brownian motion, so you would say my true function.",
                    "label": 0
                },
                {
                    "sent": "I think it looks a bit like this.",
                    "label": 0
                },
                {
                    "sent": "Most people find it very quickly, and in fact a Brownian motion is not differentiable, so it's true.",
                    "label": 0
                },
                {
                    "sent": "It's very quickly so one way of making it smoother would be to take the primitive function of this and that's this.",
                    "label": 0
                },
                {
                    "sent": "You could also integrate it another time, then it becomes more differentiable and it's this one.",
                    "label": 0
                },
                {
                    "sent": "And if you integrate it out of time, is this.",
                    "label": 0
                },
                {
                    "sent": "The striking thing about these four pictures, I think, is that the first One, Brownian motion, looks very different from the second one.",
                    "label": 0
                },
                {
                    "sent": "But when you go 234, if you integrate more and more, you eyeball no difference anymore.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to judge the nonparametric prior by just looking at a realization from it.",
                    "label": 0
                },
                {
                    "sent": "These are all nice smooth curve.",
                    "label": 0
                },
                {
                    "sent": "The shapes of these curves.",
                    "label": 0
                },
                {
                    "sent": "They don't mean anything.",
                    "label": 0
                },
                {
                    "sent": "There is just a particular realization.",
                    "label": 0
                },
                {
                    "sent": "You sort of look how it changes in in space.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The many other Gaussian process, these are two examples of stationary process.",
                    "label": 0
                },
                {
                    "sent": "Is stationary means that if you take an interval here in space or time, then the distribution of the process here is the same as when you take an interval elsewhere around the motion doesn't have that because variance increases with time.",
                    "label": 0
                },
                {
                    "sent": "Is an example of a stationary that's very smooth.",
                    "label": 1
                },
                {
                    "sent": "In fact, this is an analytic function.",
                    "label": 0
                },
                {
                    "sent": "You see it's it's basically, but it is very smooth and this is an example of a function with smoothness 3 / 2 so 1 1/2 derivative, whatever that may mean.",
                    "label": 0
                },
                {
                    "sent": "And that's coming from the maternal class, which you can do for any smoothness except three with second and you see a visible difference here.",
                    "label": 0
                },
                {
                    "sent": "If I would make this muther, you would not see so clearly that there is a difference.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here are some other examples.",
                    "label": 0
                },
                {
                    "sent": "Fractional Brownian motion, Brownian sheet and a serious prior that I also looked at already.",
                    "label": 1
                },
                {
                    "sent": "You take your favorite set of basis functions.",
                    "label": 0
                },
                {
                    "sent": "Those are deterministic functions and you add Gaussian coefficients.",
                    "label": 0
                },
                {
                    "sent": "Coefficients generated from Gaussian distributions that give a Gaussian process as well.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now here's the main result.",
                    "label": 0
                },
                {
                    "sent": "A main result on posterior contraction.",
                    "label": 0
                },
                {
                    "sent": "So the recovering if you use a Gaussian prior.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I need to little bit notation I to make this theorem true.",
                    "label": 0
                },
                {
                    "sent": "We need to prior to be Gaussian map in a band of space for Brownian motion.",
                    "label": 0
                },
                {
                    "sent": "For instance, continuous functions could be something else, but in any case we have some norm that we can measure distances for the prior like the D, but not exactly the same as the D that I had before data not will be the true parameter in that space.",
                    "label": 0
                },
                {
                    "sent": "Then the basic theorem is that rate epsilon N of contraction.",
                    "label": 1
                },
                {
                    "sent": "Is given by this equation.",
                    "label": 0
                },
                {
                    "sent": "Here you solve this equation and you go for the smallest solution epsilon N that satisfies this.",
                    "label": 0
                },
                {
                    "sent": "Now we're looking for the best we can get for this thing and that goes for the smallest one that satisfies this.",
                    "label": 0
                },
                {
                    "sent": "That is the rate you get where there is something.",
                    "label": 0
                },
                {
                    "sent": "Of course I need to say something about what the sampling model is an for that.",
                    "label": 0
                },
                {
                    "sent": "At the moment I have this line if the statistical distances on the model on the likelihood combine appropriately with this setting of the Gaussian.",
                    "label": 1
                },
                {
                    "sent": "Process then we get this equation.",
                    "label": 0
                },
                {
                    "sent": "What is this equation?",
                    "label": 0
                },
                {
                    "sent": "Well, this is a.",
                    "label": 0
                },
                {
                    "sent": "This is about a bull around the true function in this, this particular norm could be a uniform ball around late or not, and this is the probability that a prior gives to that ball that a realization of the W gets into that ball.",
                    "label": 0
                },
                {
                    "sent": "The probability should be big enough, which is quite understandable that you get something like that 'cause you trying to recover Theta, not so you have to put some math.",
                    "label": 0
                },
                {
                    "sent": "At state are not in the neighborhood of South, or not, so that next when you hit it with the likelihood, the likelihood can correct the prior to put a lot of mass next to state or not.",
                    "label": 0
                },
                {
                    "sent": "If there's no math educator, not certainly.",
                    "label": 0
                },
                {
                    "sent": "The posterior will also never target or not, so you need some math while you get exactly.",
                    "label": 0
                },
                {
                    "sent": "This equation is a bit harder to see and I won't explain that.",
                    "label": 0
                },
                {
                    "sent": "This is called to a centered small ball probability small because the epsilon N will tend to 0, so this is really about a tiny little book.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I said if the statistical distances on a model combine appropriately within Norm and I can show you what that means for some.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples don't want to do all the details here, but for instance, if you do density estimation, you would have a picture ID say in compact interval an you might make a density model for that with a Gaussian process prior by taking the Gaussian process, which is a thing that has Gaussian marginal distribution.",
                    "label": 0
                },
                {
                    "sent": "So it's positive and negative.",
                    "label": 0
                },
                {
                    "sent": "You want to make it a density you want to make it positive so you can put it in the exponent and then you have to re normalize to make it integrate to one.",
                    "label": 0
                },
                {
                    "sent": "So this would be some.",
                    "label": 0
                },
                {
                    "sent": "Prior model for the density.",
                    "label": 0
                },
                {
                    "sent": "Then the theorem is true if the normal Gaussian process is the uniform norm and distance on the parameter on the on.",
                    "label": 0
                },
                {
                    "sent": "The densities here is the challenge here.",
                    "label": 0
                },
                {
                    "sent": "Classification I've shown you how to fit Gaussian processes in classification and well, there are some distance is here as well.",
                    "label": 0
                },
                {
                    "sent": "On the Theta that combined with the distance on this and you can go through other meaningful statistical situations.",
                    "label": 0
                },
                {
                    "sent": "Regression regarded diffusions and other ones where you have that is theorem is valid if you take the correct statistical distance.",
                    "label": 0
                },
                {
                    "sent": "So he had a healthy distance here.",
                    "label": 0
                },
                {
                    "sent": "Some Lt L2 distance and so on.",
                    "label": 0
                },
                {
                    "sent": "But I skip the details.",
                    "label": 0
                },
                {
                    "sent": "Put it.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For inverse problems and the heat problem is an example of an inverse problem and I will come back to it.",
                    "label": 0
                },
                {
                    "sent": "So let me say that the rate equation is somewhat different and we don't do not quite understand that in generality, but we have some calculations for special cases that I'll show you later.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd like to 1st translate this general theorem on rate of contraction with Gaussian priors in the following way.",
                    "label": 0
                },
                {
                    "sent": "I had one condition and I make it into two.",
                    "label": 0
                },
                {
                    "sent": "That makes it more complicated, but it also helps illustrate something about Gaussian process priors.",
                    "label": 0
                },
                {
                    "sent": "I introduced here the so called Smallball exponent and what is that?",
                    "label": 0
                },
                {
                    "sent": "This is the prior mass of a ball around zero.",
                    "label": 0
                },
                {
                    "sent": "That's the little zero.",
                    "label": 0
                },
                {
                    "sent": "Here is the ball around 0.",
                    "label": 0
                },
                {
                    "sent": "Before I had a ball around the true function state or not.",
                    "label": 0
                },
                {
                    "sent": "And now I take it at zero and the prior mass of their balls here written as E to the minus five not epsilon, and it's useful to take that exponent because typically that's mobile probability is very small and so it's E to the minus something big.",
                    "label": 0
                },
                {
                    "sent": "And the big thing is dentify not epsilon.",
                    "label": 0
                },
                {
                    "sent": "Now that thing appears in the theorem, the rate of contraction epsilon N should satisfy this equation for the small ball probability is the same equation basically as before about the presence of prior mass.",
                    "label": 0
                },
                {
                    "sent": "But now that the truth is being replaced by zero.",
                    "label": 0
                },
                {
                    "sent": "There's a second equation.",
                    "label": 0
                },
                {
                    "sent": "This equation does not involve the state or not.",
                    "label": 0
                },
                {
                    "sent": "As you can see, it just involves the prior, and that's the thing I like to explain that you can pull this condition apart into 2 one only about a prior and the other will involve the truth.",
                    "label": 0
                },
                {
                    "sent": "Here is the state or not, while the second one is a bit more complicated and I'm not going to go into details of this equation, but it involves the reproducing kernel Hilbert space of the Gaussian prior.",
                    "label": 0
                },
                {
                    "sent": "Every Gaussian process has a thing like that.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to explain what it is because.",
                    "label": 0
                },
                {
                    "sent": "Either you know it already or you will not understand it in 5 minutes, but let's just say there is some equation here that you have to solve which involved the state are not and then the Gaussian priors.",
                    "label": 0
                },
                {
                    "sent": "The combination of the two.",
                    "label": 0
                },
                {
                    "sent": "And you have to satisfy both, so you get a lower bound for the rate for both.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Locations one from only.",
                    "label": 0
                },
                {
                    "sent": "The prior and the other also from the combination of the truth and the prior.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me show you in a few cases what gifts.",
                    "label": 0
                },
                {
                    "sent": "Suppose I take the prior to be a Brownian motion.",
                    "label": 0
                },
                {
                    "sent": "This very weekly thing.",
                    "label": 0
                },
                {
                    "sent": "Then what is the small ball probability I try to explain that in this picture, if I take the uniform norm, then uniform norm, a smaller ball is a is a band and I hope you can see there are two red curves here and everything in between is a small bowl.",
                    "label": 0
                },
                {
                    "sent": "It's a it's a ball around the midline of these two red curves, the small ball.",
                    "label": 0
                },
                {
                    "sent": "Probability of Brownian motion is the probability that the Brownian sample pass stays inside this bent.",
                    "label": 1
                },
                {
                    "sent": "That's not likely.",
                    "label": 0
                },
                {
                    "sent": "The realization of Brownian motion that I show you here is not within the bent at all, and you can see that it will be unlikely that one will stay in there all the time at first has to start here.",
                    "label": 0
                },
                {
                    "sent": "Of course, insight that's not so bad.",
                    "label": 0
                },
                {
                    "sent": "That's a single Gaussian distribution, and it has small probability because it's a small interval here, but with some probability will start, but then it has to stay there all the time, and Brownian motion very quickly so wants to get out of the band all the time.",
                    "label": 0
                },
                {
                    "sent": "Well, turns out one can compute.",
                    "label": 0
                },
                {
                    "sent": "This small bowl probability and it is E to the minus one over epsilon squared, so the exponent is one over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "Each of the minus one over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "So if epsilon is very small, if you have a tiny band here then it is going to be very very small.",
                    "label": 0
                },
                {
                    "sent": "Eat in a minus something very large.",
                    "label": 0
                },
                {
                    "sent": "Now, the fact that that is so small will give you by the 1st equation in the rate theorem and enter the mind is 1/4 rate independent of the truth, whatever the truth is, you will never go faster than enter.",
                    "label": 1
                },
                {
                    "sent": "The mind is 1/4 of Brownian motion.",
                    "label": 0
                },
                {
                    "sent": "And that's what you see here.",
                    "label": 0
                },
                {
                    "sent": "If I take a function Theta, not it's in C beta.",
                    "label": 0
                },
                {
                    "sent": "That means it's beta times differentiable.",
                    "label": 1
                },
                {
                    "sent": "It's smooth of order beta, then the rate is entered minus 1/4 as soon as beta is bigger than 1/2.",
                    "label": 0
                },
                {
                    "sent": "So no matter how smooth that function was, I will never get better than tonight is 1/4.",
                    "label": 0
                },
                {
                    "sent": "That's very disappointed because for a very smooth function I told you before, the rate in nonparametric estimation is entered minus beta over 1 + 2 beta.",
                    "label": 0
                },
                {
                    "sent": "And that approach is enter.",
                    "label": 0
                },
                {
                    "sent": "The mind is 1/2.",
                    "label": 0
                },
                {
                    "sent": "You go to the parametric rate.",
                    "label": 0
                },
                {
                    "sent": "If debate is really large, doesn't help in this case.",
                    "label": 0
                },
                {
                    "sent": "To make the beta larger, you never get past this low rate.",
                    "label": 0
                },
                {
                    "sent": "If the beta is happens to be very small, then you also punished.",
                    "label": 0
                },
                {
                    "sent": "You get entered minus beta over 2, and in fact if you compare it to this minimax rate antonym minus beta over 1 + 2 beta, which is what you can get with a kernel estimator or wavelet estimator or whatever, then you get the optimal rate only if they die, so half.",
                    "label": 0
                },
                {
                    "sent": "And a half anyway, is the smoothness of Brownian motion.",
                    "label": 0
                },
                {
                    "sent": "So what it says then is simply that if your prior says truth is smooth of order half, then you do it correctly for a smooth or truth that is also smooth border half.",
                    "label": 0
                },
                {
                    "sent": "But you don't do it well for other truth.",
                    "label": 0
                },
                {
                    "sent": "In particular, smoothness doesn't help, which is very very disappointing from a nonparametric estimation point of view.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Brownian Motion is a very.",
                    "label": 0
                },
                {
                    "sent": "Rough thing and we wouldn't like that as a prior anyway.",
                    "label": 0
                },
                {
                    "sent": "If we wanted to estimate a smooth function.",
                    "label": 0
                },
                {
                    "sent": "So let me take the integrated Brownian motion, let me integrate it Alpha minus 1/2 times, then becomes Alpha smooth.",
                    "label": 0
                },
                {
                    "sent": "Brownian motion is 1/2 smooth.",
                    "label": 0
                },
                {
                    "sent": "So if I integrated Alpha minus half times then I get Alpha smooth integrated Brownian motion.",
                    "label": 1
                },
                {
                    "sent": "That's exactly right.",
                    "label": 0
                },
                {
                    "sent": "If your true function is also of smoothness, Alpha if it matches, and otherwise you get something which is suboptimal.",
                    "label": 0
                },
                {
                    "sent": "But it is like.",
                    "label": 0
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "Small ball probability is now bigger and you can see that from this picture right here is the sample pass.",
                    "label": 0
                },
                {
                    "sent": "It's not in a small bowl, but you can easily imagine since the simple passes much smoother if it starts insight and there is more probability that it stays inside because it is smooth and it tries to go on smoothly.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other example are stationary processes.",
                    "label": 1
                },
                {
                    "sent": "You could you can make stationary process by setting up the covariance function with some spectral measure like this.",
                    "label": 0
                },
                {
                    "sent": "And I'll look at 2.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Particular examples, one very smooth one which has a spectral measure, Gaussian measure, like this that gives very smooth sample path and then we get these rates here and then.",
                    "label": 0
                },
                {
                    "sent": "Best stated in terms of the Fourier transform of the true parameter and I'm in D dimensions here.",
                    "label": 1
                },
                {
                    "sent": "And the two cases if the true parameter is very smooth itself, like the prior, you get a very nice rate near one over root N, which is not nonparametric.",
                    "label": 0
                },
                {
                    "sent": "Almost any more than ear means.",
                    "label": 0
                },
                {
                    "sent": "There's a log factor missing there from one over root in, so you're doing very well as if you have a finite dimensional parameter that you're estimating.",
                    "label": 0
                },
                {
                    "sent": "This is a condition on the Fourier transform to say that faith or not is very smooth.",
                    "label": 0
                },
                {
                    "sent": "If, on the other end, the true function is beta smooth and in terms of the Fourier transform, that will be conditioned like this, then the rate is not even a power event, but that becomes a power of one over log in, so you're severely punished by taking a prior that is really good for smooth functions.",
                    "label": 0
                },
                {
                    "sent": "Then you severely punished if the true function is actually not super smooth, then everything gets lost.",
                    "label": 0
                },
                {
                    "sent": "You could say you get.",
                    "label": 0
                },
                {
                    "sent": "Hardly any convergence.",
                    "label": 0
                },
                {
                    "sent": "While you could have a power of end there, but you get a power of 1, overlook it.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then these other classes to maternelles were somewhat less smooth than you can adapt to smoothness.",
                    "label": 0
                },
                {
                    "sent": "If you take the Alpha here, then it becomes Alpha smooth.",
                    "label": 0
                },
                {
                    "sent": "This was the three and a half.",
                    "label": 0
                },
                {
                    "sent": "Sorry 3 / 2 smoothness and that will be exactly right.",
                    "label": 0
                },
                {
                    "sent": "We all have the exact match if Alphas.",
                    "label": 0
                },
                {
                    "sent": "Beta addendum attorneys is right.",
                    "label": 0
                },
                {
                    "sent": "Now, in practice, maybe you would scale the price.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's look at scaling the priors in time and in space.",
                    "label": 0
                },
                {
                    "sent": "You could, if you want to have a prior for functions on 01.",
                    "label": 0
                },
                {
                    "sent": "What you could do is you could take a process and you could run it for a little while and then pull it out like this, scale it up so that it covers to complete time set.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or you could maybe run a prior Gaussian process for a long time and then scale it in so that you have a function on 01 and you see in the picture that.",
                    "label": 0
                },
                {
                    "sent": "Well, certainly here you see very smooth here, and if you scale it becomes somewhat rougher.",
                    "label": 0
                },
                {
                    "sent": "Not in an analytic sense, the number of derivatives doesn't change by this operation, but in a Bayesian sense it does in fact.",
                    "label": 0
                },
                {
                    "sent": "And here also the.",
                    "label": 0
                },
                {
                    "sent": "The smoothness.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Increases so if we do integrate a time Brown integrated Brownian motion and we scale that in time, then it turns out that you can do a scaling factor depending on N here and the scaling factor.",
                    "label": 0
                },
                {
                    "sent": "Depends also on the true target smoothness of the prior and then you get a prior scaled integrated Brownian motion which gives the optimal rate for any true function up to smoothness.",
                    "label": 1
                },
                {
                    "sent": "Beta drawback is that you need debater to do this particular prior, but in any case that is a scaling that works for any Theta well up to K + 1/2.",
                    "label": 0
                },
                {
                    "sent": "If this is K times integrated you can go to K plus 1/2.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "K + 1 Northgate was a half not very smooth, but you can get.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So stretching can help a little.",
                    "label": 0
                },
                {
                    "sent": "You can go up two K plus 1/2 shrinking that goes all the way down to 0.",
                    "label": 0
                },
                {
                    "sent": "For smoothness.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You could also start with this very smooth Gaussian process and then basically you would only want to shrink it so that you make it rougher, and it turns out there is indeed also a scaling there that makes it right exactly for a beta smooth truth.",
                    "label": 0
                },
                {
                    "sent": "With this scaling factor you get the right result, so scaling thus change everything.",
                    "label": 0
                },
                {
                    "sent": "An shrinking a super smooth seems to be a very nice way of getting getting a good recovery with your prior.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this scaling factors they depend on the true Smoothens beta, which we don't know and about maybe you're willing to guess.",
                    "label": 0
                },
                {
                    "sent": "Say that it is too late or something else.",
                    "label": 0
                },
                {
                    "sent": "But if you cannot guess, then you probably want to do something else.",
                    "label": 0
                },
                {
                    "sent": "You might want to do the scaling based on the data.",
                    "label": 0
                },
                {
                    "sent": "That's called adaptation.",
                    "label": 0
                },
                {
                    "sent": "So we could try adapt prior to the data in two ways by the hierarchical base metadata.",
                    "label": 0
                },
                {
                    "sent": "Empirical based methods by the hierarchical based methods.",
                    "label": 0
                },
                {
                    "sent": "I would take a scale of priors.",
                    "label": 0
                },
                {
                    "sent": "Maybe of several smoothness levels, and then I would put a prior on the smoothness level, or maybe on a scaling factor and then do full base with not a Gaussian prior anymore because it was a mixture of Gaussians.",
                    "label": 1
                },
                {
                    "sent": "But any case that gives a full base method empirical base, you would get the regularity from the data and a typical method is to take the marginal distribution of the data.",
                    "label": 1
                },
                {
                    "sent": "So integrating out the prior.",
                    "label": 0
                },
                {
                    "sent": "Leaving into smoothness there and then optimize maximum likelihood estimator on the smoothness.",
                    "label": 0
                },
                {
                    "sent": "And then we use that data determined smoothness.",
                    "label": 0
                },
                {
                    "sent": "Well, the hierarchical base thing we know quite a bit already.",
                    "label": 0
                },
                {
                    "sent": "It works in some generality, many different ways of setting it up.",
                    "label": 0
                },
                {
                    "sent": "The empirical base.",
                    "label": 0
                },
                {
                    "sent": "We actually know very little except some some examples I'll give you.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nice example of dialogical base approach.",
                    "label": 0
                },
                {
                    "sent": "This works in this way we choose a gamma.",
                    "label": 0
                },
                {
                    "sent": "Well, we choose a scaling factor, in fact these.",
                    "label": 0
                },
                {
                    "sent": "Power of the scaling factor from a gamma distribution.",
                    "label": 1
                },
                {
                    "sent": "I take this very smooth Gaussian process and then I scale that with the random scale coming from the.",
                    "label": 0
                },
                {
                    "sent": "The gamma, so I take a deep root of a gamma distribution and it works in quite generality.",
                    "label": 0
                },
                {
                    "sent": "If you have a smooth beta smooth truth beta derivatives, then indeed you get the minimax contraction rate, which is mixture of Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "So nearly entered minus beta over 280 + D. Also you keep the nice properties of Satan, not a super smooth.",
                    "label": 0
                },
                {
                    "sent": "Then the right scaling or no scaling got you nearly enter the minus half.",
                    "label": 0
                },
                {
                    "sent": "You still get it with the scaling, so the scaling doesn't.",
                    "label": 0
                },
                {
                    "sent": "The random skating doesn't confuse the full based.",
                    "label": 1
                },
                {
                    "sent": "Random scaling doesn't confused the prior, and it's a very nice result in Bayesian nonparametric estimation because it means a full base approach can solve the bandwidth problem, because what we're choosing here is the smoothness that is the typical bandwidth.",
                    "label": 0
                },
                {
                    "sent": "If you do a kernel estimator or a serious estimator, it's the truncation.",
                    "label": 0
                },
                {
                    "sent": "And you can do that in a fully bashan way, in for instance, with this scheme.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So summary of recovery, the recovery is best if the prior matches the truth and this match will slow down, but it will not prevent recovery.",
                    "label": 1
                },
                {
                    "sent": "You will get some convergence rate, but maybe very slow.",
                    "label": 0
                },
                {
                    "sent": "It can be prevented.",
                    "label": 0
                },
                {
                    "sent": "This match match mismatch by using hyper parameters and then I get into the kredible sets.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is just my notation.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll just remind you of what I called incredible set.",
                    "label": 0
                },
                {
                    "sent": "I have the posterior distribution and then credible set is set in the posterior typically taken central around the posterior mean that has 95% of the posterior math.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne will look at the coverage of this incredible set by treating the incredible set as a as if it were a confidence set in the frequentist sense.",
                    "label": 0
                },
                {
                    "sent": "So we look at the coverage in the frequentist set.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's an earlier answer to this question, but it works or not and answer was it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "This is a paper by Dennis Cox from 1993 and he looks at a particular regression problem.",
                    "label": 0
                },
                {
                    "sent": "It's a bit difficult to read this whole paper.",
                    "label": 0
                },
                {
                    "sent": "It's very clever, very smart, but his conclusion is rather terrible.",
                    "label": 0
                },
                {
                    "sent": "That's what he says.",
                    "label": 0
                },
                {
                    "sent": "No invasions often find such basin procedures.",
                    "label": 1
                },
                {
                    "sent": "That's the credible set attractively cause.",
                    "label": 0
                },
                {
                    "sent": "The frequentist coverage probability of the basin regions tends to the interior coverage probability in typical cases.",
                    "label": 1
                },
                {
                    "sent": "Was my hope that this would also halt in a non parametric setting so the paramedics setting with finite dimensional models everything is fine and we all knew that before 1993 and Cox settle I hoped it was also true in the non parametric setting.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately they hoped for result is false in about the worst possible way.",
                    "label": 0
                },
                {
                    "sent": "Visibly this equation.",
                    "label": 0
                },
                {
                    "sent": "And what does it say?",
                    "label": 0
                },
                {
                    "sent": "It's here, it's the coverage and you see that the limb.",
                    "label": 0
                },
                {
                    "sent": "The limit of the coverage is 0, which is terrible.",
                    "label": 0
                },
                {
                    "sent": "There's no coverage at all, which means no correct uncertainty quantification.",
                    "label": 0
                },
                {
                    "sent": "Not only that, you don't get in 95% which we could live it, I think, so 95% is rather arbitrary, but you just get zero.",
                    "label": 0
                },
                {
                    "sent": "When do you get this?",
                    "label": 0
                },
                {
                    "sent": "You get this for all the state are not that you.",
                    "label": 0
                },
                {
                    "sent": "Sample from your prior, so that seems terrible.",
                    "label": 0
                },
                {
                    "sent": "Nuessen it's strange that this paper hasn't been quoted before, and that we still doing nonparametric estimation by without really investigating this further.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cox model can actually be cast in a in a simpler form.",
                    "label": 1
                },
                {
                    "sent": "It's a sequence model where the function is developed in an infinite serious with a fixed base set of base functions and the Theta ice come from Gaussian priors as I talked about before.",
                    "label": 0
                },
                {
                    "sent": "So the data then become just independent observations on these coefficients in each coefficient is measured with a variance of 1 / N an error variance of 1 / N and I also have a multiplication here Ki that.",
                    "label": 1
                },
                {
                    "sent": "That is, a known number could be one in the case of Cox it is 1.",
                    "label": 0
                },
                {
                    "sent": "So the observation has mean Kappa times Theta I.",
                    "label": 0
                },
                {
                    "sent": "We put a prior on Theta of which are just independent Gaussians with some variances that will tend to zero.",
                    "label": 0
                },
                {
                    "sent": "Typically because we have infinitely many of those.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now to make that's a bit more easy to read, will just say that the data is some infinite dimensional Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It has a certain mean.",
                    "label": 0
                },
                {
                    "sent": "That's the function of data, and it has a certain variance, which is essentially one over in the independent, and the prior is also an infinite dimensional Gaussian attesa certain covariance Lambda.",
                    "label": 0
                },
                {
                    "sent": "Then if you see those two equations you see it's just conjugate, it's just the mean here.",
                    "label": 0
                },
                {
                    "sent": "The case actually linear.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can workout with the posterior wrist.",
                    "label": 0
                },
                {
                    "sent": "In that case the posterior is also Gaussian, infinite dimensional and you have to multiply the data with some matrix operator.",
                    "label": 0
                },
                {
                    "sent": "Here the formula is given here, but it's of no importance for us, just a linear transformation underway and it's going to be some posterior spread posterior covariance which are called S formulas.",
                    "label": 0
                },
                {
                    "sent": "Dear but has no importance for the talk.",
                    "label": 0
                },
                {
                    "sent": "What is may be interesting is that the but not surprising that this is not data dependent, so the data is in the posterior mean.",
                    "label": 0
                },
                {
                    "sent": "The posterior spread is just some fixed covariance.",
                    "label": 0
                },
                {
                    "sent": "Then the credible set.",
                    "label": 0
                },
                {
                    "sent": "In this case you would wouldn't would make.",
                    "label": 0
                },
                {
                    "sent": "That's what Cox also looks at is to make a ball around the posterior mean, and then you would give the ball of radius such that the radius makes that if you take a centered Gaussian, then the ball has exactly.",
                    "label": 0
                },
                {
                    "sent": "Probability point 905 so that is a credible set for 95% level in this situation.",
                    "label": 0
                },
                {
                    "sent": "So it's pretty simple setup.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To see what we get, I'll show you.",
                    "label": 0
                },
                {
                    "sent": "Results for truth and Prius the truth again, I will have the parameter beta which gives the smoothness of the function.",
                    "label": 0
                },
                {
                    "sent": "In this case I'll measure smoothness with this over left space.",
                    "label": 0
                },
                {
                    "sent": "That is, this is so Theta the true one is a serious expansion where the coefficients if you multiply by the square and multiply to betas fire.",
                    "label": 0
                },
                {
                    "sent": "This basically regularity beta is before.",
                    "label": 0
                },
                {
                    "sent": "And the prior then we have a variance here to play with in the prior will take the variance to be 1 / I two the 2A plus one and by doing these two things what I get is S before I can compare the Alpha and the beta.",
                    "label": 0
                },
                {
                    "sent": "If the Alpha is the beta then roughly can say the prior and the truth match.",
                    "label": 1
                },
                {
                    "sent": "If Alpha is bigger than beta then the coefficients under the prior code 20 faster the variance is 10 to 0 faster.",
                    "label": 0
                },
                {
                    "sent": "So the prior over smooth this.",
                    "label": 0
                },
                {
                    "sent": "And if Alpha smaller than beta, then the prior under Smith is the truth.",
                    "label": 0
                },
                {
                    "sent": "So the interpretation of Alpha and beta are exactly as in the first part of the proof.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not being completely precise in the sense that this norm here, which is Sobolev norm.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you should take the soup, but I will not make that difference here, because then becomes.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very complicated.",
                    "label": 0
                },
                {
                    "sent": "Now first we can talk about the contraction rate.",
                    "label": 0
                },
                {
                    "sent": "In this case, there's nothing very special about it.",
                    "label": 0
                },
                {
                    "sent": "You get the contraction rate, which is the optimal minimal Max rate.",
                    "label": 0
                },
                {
                    "sent": "There's a little P in here now.",
                    "label": 0
                },
                {
                    "sent": "Also, because it's an inverse problem, so the inverse nature is to K is 1P0 in the case I just one, but then the contraction rate is exactly as I had before.",
                    "label": 0
                },
                {
                    "sent": "It's optimal if prior and.",
                    "label": 0
                },
                {
                    "sent": "Truce, match and otherwise it's sub optimal.",
                    "label": 0
                },
                {
                    "sent": "But you always get contraction, so whatever you miss, specify it still works in.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Innocence.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at a credible sets, and let's look at some pictures.",
                    "label": 0
                },
                {
                    "sent": "1st and I show you some picture for an inverse problem.",
                    "label": 0
                },
                {
                    "sent": "This particular inverse problem, it's called Volterra operator, which is just a primitive functional Theta.",
                    "label": 1
                },
                {
                    "sent": "So Theta is my parameter to function that I want to estimate.",
                    "label": 0
                },
                {
                    "sent": "We take the primitive function and then what we observe is well, actually an integral of that.",
                    "label": 0
                },
                {
                    "sent": "This is the physicist notation.",
                    "label": 0
                },
                {
                    "sent": "The derivative of our data that's a little dot here is that primitive plus some noise.",
                    "label": 0
                },
                {
                    "sent": "So we want to recover a function from noisy observation of its primitive.",
                    "label": 0
                },
                {
                    "sent": "That's an inverse problem, very nonparametric.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it fits in the sequence serious, and you can work out what the expansions are, but let's not.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at it, let's look at the picture.",
                    "label": 0
                },
                {
                    "sent": "In pictures.",
                    "label": 0
                },
                {
                    "sent": "The priors in five pictures on the left are the same and the prior in the five pictures on the right are also the same and I just repetitions with multiple data.",
                    "label": 0
                },
                {
                    "sent": "So every five everyone at 5I generate the data and then data Bashan machine in the same way the same prior.",
                    "label": 0
                },
                {
                    "sent": "The black one.",
                    "label": 0
                },
                {
                    "sent": "Here is the truth.",
                    "label": 0
                },
                {
                    "sent": "See number one before, but it was red but now it's black and the red one is the posterior mean and then you see the weekly things are things generated from the prior, so it seems to work quite well here.",
                    "label": 0
                },
                {
                    "sent": "Well, in a sense we don't have enough data to get complete recovery.",
                    "label": 0
                },
                {
                    "sent": "That's why the red one is not the black one, but the uncertainty margin given by the posterior suggested by the posterior is such that we know that we might may be off by that much, and that is true if you repeat it in the data every time on the right is different.",
                    "label": 0
                },
                {
                    "sent": "You can see that, well, the estimate the red estimate is not that much worse than it is on the left, but the confidence bands have shrunk quite a bit.",
                    "label": 0
                },
                {
                    "sent": "Everything I generated from the posterior is quite smooth, and here they well they failed to cover it here.",
                    "label": 0
                },
                {
                    "sent": "Here there's a big part is covered here.",
                    "label": 0
                },
                {
                    "sent": "This whole thing sticks out of the confidence band and so on.",
                    "label": 0
                },
                {
                    "sent": "So here the priors very smooth and we do not cover the truth, which is somewhat.",
                    "label": 0
                },
                {
                    "sent": "Comforting becausw the posterior is all we will have we generate from the posterior and then we hope that these dash things give you some sense of how far you might be wrong with your thing.",
                    "label": 0
                },
                {
                    "sent": "Is it dependent on?",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The amount of data I have.",
                    "label": 0
                },
                {
                    "sent": "Well, if I take a bigger end so I less noise in the data then it doesn't change dramatically.",
                    "label": 0
                },
                {
                    "sent": "In fact, here in the smooth case it gets very bad because the kredible bands here are just strong within the resolution of the picture to the posterior mean, and we missing completely the true curve.",
                    "label": 0
                },
                {
                    "sent": "The posterior thinks that it's very sure that the truth is right on the posterior mean, but it wasn't.",
                    "label": 0
                },
                {
                    "sent": "It's the black one, so we seem to have a problem.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can we do some math on that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we have this particular theorem.",
                    "label": 0
                },
                {
                    "sent": "So the situation is just linear inverse problem and we had a posterior which was Gaussian.",
                    "label": 1
                },
                {
                    "sent": "And we made incredible set as a central bolinda Gaussian posterior.",
                    "label": 1
                },
                {
                    "sent": "Then the theorem goes like this.",
                    "label": 0
                },
                {
                    "sent": "We have a prior that had a smoothness, Alpha and a truth that has a smoothness beta.",
                    "label": 0
                },
                {
                    "sent": "Now if Alpha smaller than beta.",
                    "label": 0
                },
                {
                    "sent": "So if we under smooth the truth then we do have coverage and it's one uniformly.",
                    "label": 0
                },
                {
                    "sent": "So it all works nicely.",
                    "label": 0
                },
                {
                    "sent": "Maybe even two nice because we didn't get 95%.",
                    "label": 0
                },
                {
                    "sent": "But actually we got one.",
                    "label": 0
                },
                {
                    "sent": "If we have an exact match then becomes a bit complicated.",
                    "label": 0
                },
                {
                    "sent": "Any coverage within 01 occurs.",
                    "label": 0
                },
                {
                    "sent": "But if you don't take the 95% too serious, then nothing terrible is is happening.",
                    "label": 0
                },
                {
                    "sent": "You sort of get the right thing, however, if the prior is smoother than the truth, the right set of pictures then thinks are disastrous for some beta smooth one, not for all one, because if it's beta smooth as can be much smoother.",
                    "label": 0
                },
                {
                    "sent": "But there are things that are exactly beta smooth that are over smooth by this prior and then the asymptotical reaches 0.",
                    "label": 1
                },
                {
                    "sent": "That is what you saw in the right picture.",
                    "label": 1
                },
                {
                    "sent": "We have two smooth prior and everything goes to pieces.",
                    "label": 0
                },
                {
                    "sent": "The posterior thinks it's can be very sure of where the truth is, but it is absolutely wrong.",
                    "label": 1
                },
                {
                    "sent": "It's the prior that told it to be sure, but the prior was wrong relative to the truth.",
                    "label": 0
                },
                {
                    "sent": "The good news is that in these two first cases the Kredible ball also has the correct order of magnitude, so drop this thing about a 95% and what the coverage is exactly.",
                    "label": 1
                },
                {
                    "sent": "You could look at whether the size of the bull is sort of right.",
                    "label": 0
                },
                {
                    "sent": "Up to constant, it's always right.",
                    "label": 0
                },
                {
                    "sent": "If you do not over smooth.",
                    "label": 0
                },
                {
                    "sent": "If you're over smooth then you get 2 narrow balls.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where does Cox result come in?",
                    "label": 0
                },
                {
                    "sent": "Well, it doesn't follow from this theorem, but can be explained from the theorem that truth generated from an Alpha smooth prior.",
                    "label": 0
                },
                {
                    "sent": "They belong with probability 12, smoothness things of smaller smoothness.",
                    "label": 0
                },
                {
                    "sent": "But I don't belong to this Alpha smooth set and that's why it can be in the third case.",
                    "label": 0
                },
                {
                    "sent": "That you get coverage 0.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, I'm skipping this for reasons of time.",
                    "label": 0
                },
                {
                    "sent": "Well, this is about rescaling.",
                    "label": 0
                },
                {
                    "sent": "We can I do very briefly.",
                    "label": 0
                },
                {
                    "sent": "You can also re scale these price with the scaling parameter here and see if you can repair it in the contraction rate business.",
                    "label": 0
                },
                {
                    "sent": "We could do that.",
                    "label": 0
                },
                {
                    "sent": "I won't go through the theorem but I'll show.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The pictures, if you do a rescaling then you can get it right after all, and this is for deterministic rescaling.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So credible sets the first summary.",
                    "label": 0
                },
                {
                    "sent": "In a non parametric set up the prior is not watched out by the data for recovery.",
                    "label": 1
                },
                {
                    "sent": "We saw that the prior influences the posterior contraction rate but you get consistency for most priors.",
                    "label": 1
                },
                {
                    "sent": "So it's it's gonna be bad.",
                    "label": 0
                },
                {
                    "sent": "But it is not too bad.",
                    "label": 0
                },
                {
                    "sent": "You always get something for the uncertainty quantification.",
                    "label": 0
                },
                {
                    "sent": "It can be disastrous.",
                    "label": 1
                },
                {
                    "sent": "And that happens if the prior if the prior takes mistakes.",
                    "label": 0
                },
                {
                    "sent": "The truth for being more regular than it is.",
                    "label": 1
                },
                {
                    "sent": "The posterior will then be 2 concentrated and center far away from the truth.",
                    "label": 0
                },
                {
                    "sent": "So both ways it doesn't wrong.",
                    "label": 0
                },
                {
                    "sent": "One solution to that would be to under smooth.",
                    "label": 0
                },
                {
                    "sent": "So take a prior that is not smooth with the truth.",
                    "label": 0
                },
                {
                    "sent": "That would have to adapt would require that you know something about the minimal smoothness of your true function, and then you can fix the prior to that.",
                    "label": 0
                },
                {
                    "sent": "Now don't on the smooth too much, because then.",
                    "label": 0
                },
                {
                    "sent": "The contraction rate, though it still will be something tending to 0.",
                    "label": 0
                },
                {
                    "sent": "So you do get consistency, but it will.",
                    "label": 0
                },
                {
                    "sent": "The recovery will be slow, so you should not under smooth too much.",
                    "label": 0
                },
                {
                    "sent": "I I finished with this part by saying that much work to be done we have which I showed you results for the linear inverse Gaussian problem.",
                    "label": 0
                },
                {
                    "sent": "We also have some results for Gaussian regression, but as many things to be asked here.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the heat equation problem.",
                    "label": 0
                },
                {
                    "sent": "Just again to show you that there might be trouble you see prior stat increase in smoothness and here it's still OK. Years still OK.",
                    "label": 0
                },
                {
                    "sent": "Here you get in the price that over smooth and here they vary over smooth.",
                    "label": 0
                },
                {
                    "sent": "And this is for a bigger end, but the message is the same.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My final part of the talk will be about.",
                    "label": 0
                },
                {
                    "sent": "Adapt incredible sets to the day.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "ETA for recovery.",
                    "label": 0
                },
                {
                    "sent": "We saw that if your prior is not matching the truth, you can make it match the truth by using a hyperparameter, and you could do that in your higher logical empirical base and set up.",
                    "label": 0
                },
                {
                    "sent": "How does that work for kredible sets?",
                    "label": 1
                },
                {
                    "sent": "Does it also work?",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That well.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Skip that.",
                    "label": 0
                },
                {
                    "sent": "Again, I'll work in the linear Gaussian inverse problem and we actually have no results for the general Gaussian.",
                    "label": 1
                },
                {
                    "sent": "Situation, though I don't think it will be very different in general, so the same setup as before.",
                    "label": 0
                },
                {
                    "sent": "So we have the data from this infinite Gaussian.",
                    "label": 0
                },
                {
                    "sent": "We have a prior, we have a covariance where I now make the Alpha explicit that is the smoothness of the prior.",
                    "label": 0
                },
                {
                    "sent": "It's the rate of decrease of the eigenvalues of the Lambda Alpha.",
                    "label": 0
                },
                {
                    "sent": "We get.",
                    "label": 0
                },
                {
                    "sent": "The posterior were also made the Alpha explicit and we can forgive and Alpha take the kredible ball.",
                    "label": 0
                },
                {
                    "sent": "Now suppose that we determine an Alpha based on the data using the empirical based methods.",
                    "label": 0
                },
                {
                    "sent": "So for debt.",
                    "label": 0
                },
                {
                    "sent": "You get an alphabet.",
                    "label": 0
                },
                {
                    "sent": "The estimate by saying that YN forgiven Alpha.",
                    "label": 0
                },
                {
                    "sent": "If you take data Scotian, Ryan, give and take discussion, then Ryan marginally has a Gaussian distribution, which is mean zero and has this covariance that contains the Alpha.",
                    "label": 0
                },
                {
                    "sent": "And then you do maximum likelihood on the Alpha.",
                    "label": 0
                },
                {
                    "sent": "In this model is you work that out.",
                    "label": 0
                },
                {
                    "sent": "It's disting.",
                    "label": 0
                },
                {
                    "sent": "It's kind of complicated with an infinite series, so it's not very obvious to analyze, but it can be done it quite explicit in this situation.",
                    "label": 0
                },
                {
                    "sent": "Anyway, empirical base here takes the Alpha data dependent based on marginal maximum likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This works for recovery, but I will not present results.",
                    "label": 0
                },
                {
                    "sent": "The question is, does it also work for uncertainty quantification and more precisely then if I take the bull that you had forgiven Alpha, and if I stick in the Alpha heads there, does that cover so determined Alpha based on the data and then take them here I do empirical base we believe at the moment at hierarchical base where you put a prior on Alpha is will give similar results, but we haven't written that are completely out.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Counterexample horrible doesn't work at all.",
                    "label": 0
                },
                {
                    "sent": "Credible sets can be terribly wrong in this empirical base situation, and this is the pictures that I started with.",
                    "label": 1
                },
                {
                    "sent": "Now we had a black truth.",
                    "label": 0
                },
                {
                    "sent": "We had a blue posterior mean, and in Gray we had posterior credible dance 95%.",
                    "label": 0
                },
                {
                    "sent": "First, last pictured works and what is different here is the informative of the data informativeness of the data.",
                    "label": 0
                },
                {
                    "sent": "The noise level here is there any small letter and it's big.",
                    "label": 0
                },
                {
                    "sent": "The truth is the same in all four pictures, and the data is regenerated, of course, and these are two intermediate values of in.",
                    "label": 0
                },
                {
                    "sent": "You cannot.",
                    "label": 0
                },
                {
                    "sent": "Well, it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "You can see that the two middle, once they are terrible, and it seemed to be interaction between the noise level and the truth.",
                    "label": 0
                },
                {
                    "sent": "In this case for another truth for other values of N it might go wrong.",
                    "label": 0
                },
                {
                    "sent": "What this is, is a counter example of a truth.",
                    "label": 0
                },
                {
                    "sent": "There is a particular function that we took the black one and for that this particular procedure goes wrong for these values of N here.",
                    "label": 0
                },
                {
                    "sent": "We can make other counterexamples, but they are also truthful, which it does work.",
                    "label": 0
                },
                {
                    "sent": "Let me, for the last five minutes.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Try to see where this comes from.",
                    "label": 0
                },
                {
                    "sent": "First of all, do the frequentist say about this?",
                    "label": 0
                },
                {
                    "sent": "They have a concept of honesty of confidence sets.",
                    "label": 0
                },
                {
                    "sent": "Honesty means that the coverage is at least 95% for any possible through data, not for all so uniformly in the truth.",
                    "label": 0
                },
                {
                    "sent": "So they're not here should contain all possible truth or truth you think might be valid.",
                    "label": 0
                },
                {
                    "sent": "Could for instance be a Sobolev bull, then it's a known result that if you know the beta you want to be uniform.",
                    "label": 0
                },
                {
                    "sent": "Overall functions with beta derivatives you can get confidence sets of.",
                    "label": 0
                },
                {
                    "sent": "Honest confidence sets of the estimation rate enter minus beta.",
                    "label": 1
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is a problem though, if as soon as you allow more than one smoothness level.",
                    "label": 0
                },
                {
                    "sent": "Here all smoothness levels bigger than some lower bound rate or not, then the diameter of the confidence sent necessarily we all have to involve this.",
                    "label": 1
                },
                {
                    "sent": "Biggest their smallest biggest model.",
                    "label": 0
                },
                {
                    "sent": "The smaller smoothness later not.",
                    "label": 0
                },
                {
                    "sent": "So there is something nasty going on here.",
                    "label": 0
                },
                {
                    "sent": "If you want to be honest over everything here, everything smooth and better not then you get a diameter, debt cannot.",
                    "label": 0
                },
                {
                    "sent": "Adapt really to debater, it's determined by the biggest model and bit complicated thing.",
                    "label": 1
                },
                {
                    "sent": "Here there are some other ways of describing this.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Depends a bit on the metric, but I will skip all that.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there is a difference in frequentist literature between adaptive estimation and uncertainty quantification.",
                    "label": 1
                },
                {
                    "sent": "If you do adaptive estimation, there is a success story that starts in the 1990s with wavelets and and so on.",
                    "label": 0
                },
                {
                    "sent": "A more regular true function is easier to estimate.",
                    "label": 1
                },
                {
                    "sent": "It should be, there's less degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "Estimators can then be simultaneously optimal for multiple regularity's you don't have to know that it has 10 derivatives, you can just do that adapting from the data.",
                    "label": 0
                },
                {
                    "sent": "And Bayesian estimators we now know since a couple of years can also achieve this by prior on the bench with on a bend with parameter of the problem.",
                    "label": 0
                },
                {
                    "sent": "So that is a success story.",
                    "label": 1
                },
                {
                    "sent": "You don't have to know smoothness, you can still do very nice things.",
                    "label": 0
                },
                {
                    "sent": "Uncertainty quantification is very different.",
                    "label": 0
                },
                {
                    "sent": "Honest uncertainty quantification must argue from the worst case, the smallest possible regularity level, and you cannot adapt so much to the smooth.",
                    "label": 1
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a Lucian Bersia phrased it in a discussion here, adaptive estimators from the success story they do the best that's possible in view of the properties of the underlying function to be estimated.",
                    "label": 0
                },
                {
                    "sent": "That's quite satisfactory, but he estimated does not tell you how well it does.",
                    "label": 0
                },
                {
                    "sent": "You have no idea about your order of magnitude of the distance, so you're doing well, but you don't know how well you're doing, and so you cannot really give the uncertainty margin on your estimator.",
                    "label": 0
                },
                {
                    "sent": "And that seems to be in the nature of things.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "This is an attempt to solve that problem by looking only at particular truth and they called self similar here by this complicated equation you say something about the Sobolev norm high up at higher resolution levels should satisfy a lower bound.",
                    "label": 0
                },
                {
                    "sent": "The interpretation is that the truth has the same character at any resolution level.",
                    "label": 1
                },
                {
                    "sent": "An very vague idea is that if you have a noisy data set you can estimate.",
                    "label": 0
                },
                {
                    "sent": "The function only up to a certain resolution level and you could say there is something like an affective dimension where you can still do it with their data.",
                    "label": 0
                },
                {
                    "sent": "Above that you cannot do it and self similar sequences are the same at every possible affective dimension.",
                    "label": 0
                },
                {
                    "sent": "An for dose you can get frequentist nice, uniform, honest confidence sets.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the good news is that you can also get good basean sets that adapt for those truth with empirical base.",
                    "label": 0
                },
                {
                    "sent": "So for those sequences is a special class of sequences, it works.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you see that in this picture this one was just the prior matching the truth.",
                    "label": 0
                },
                {
                    "sent": "And here I scaled the truth and it suddenly has kredible bands that do satisfied and this true function, the black one is a self similar one.",
                    "label": 0
                },
                {
                    "sent": "That's a nice one.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where do then Lisa problems arise?",
                    "label": 0
                },
                {
                    "sent": "Well, this is the Cox result again where you gotta O well the.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing you might notice in the paper is that if you would blow up the kredible set a little bit, yes by log factor you suddenly get one instead of 00.",
                    "label": 0
                },
                {
                    "sent": "Blowing it up works.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then I have to conjecture which is about we're going to finish that in the Bayesian adaptive sense.",
                    "label": 0
                },
                {
                    "sent": "There is something similar true, and you can do it for every Alpha at the same time with the empirical base Alpha said honesty is questionable in this case.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then where does this go wrong?",
                    "label": 0
                },
                {
                    "sent": "Well, this truth is not self similar, and what actually?",
                    "label": 1
                },
                {
                    "sent": "This was a very smooth Troost debate.",
                    "label": 1
                },
                {
                    "sent": "Aciro and.",
                    "label": 0
                },
                {
                    "sent": "You could say that the black one is unlike anything that is sampled from some prior that I used.",
                    "label": 0
                },
                {
                    "sent": "I used a whole scale of price with different smoothness is but this one was actually smoother than anyone.",
                    "label": 0
                },
                {
                    "sent": "It had sequences that were non 0 up to some level and then all zeros and none of the things that come from the prior have that.",
                    "label": 1
                },
                {
                    "sent": "And it seems that that is bad news.",
                    "label": 0
                },
                {
                    "sent": "But I I have difficulty interpreting what it really means, so here are then conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conjectures nonparametric readable sets are never correct.",
                    "label": 0
                },
                {
                    "sent": "Frequentist confidence regions.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Priced at under smooth the truth.",
                    "label": 0
                },
                {
                    "sent": "Give a reasonable idea of the uncertainty in the posterior mean.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If the prior over smokes is the truth and spread in the posterior is very misleading about a remaining uncertainty.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that effect may disappear if the prior is scaled, for instance by hierarchical empirical Bayesian methods, but only for truth that resemble the prior.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At least, so we think at a moment.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, it seems then that we must either under smooth or belief, to find details of the prior.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And is that possible in nonparametrics?",
                    "label": 1
                },
                {
                    "sent": "Then I make it open ended.",
                    "label": 0
                },
                {
                    "sent": "I show you the picture again, Can you believe when you see these priors that one of those is really the best for your data?",
                    "label": 0
                },
                {
                    "sent": "Or do these three, for instance look exactly the same?",
                    "label": 0
                },
                {
                    "sent": "Is it really possible to know what you're doing in Nonparametrics?",
                    "label": 0
                },
                {
                    "sent": "So I keep it with that question.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Thank you OD.",
                    "label": 0
                },
                {
                    "sent": "So there are time.",
                    "label": 0
                },
                {
                    "sent": "There's time for some questions.",
                    "label": 0
                },
                {
                    "sent": "If there are any other microphones down in front.",
                    "label": 0
                },
                {
                    "sent": "If you'd like to ask question, please come down.",
                    "label": 0
                },
                {
                    "sent": "Maybe I could ask briefly about the the favorable case, the recovery, the full base handled the problem, and I noticed a gamma distribution means stuck in there at the top, and I assume that that this can't be true for all distributions, but I assume gamma is not needed and can you tell us what the story is there?",
                    "label": 0
                },
                {
                    "sent": "Yeah I can.",
                    "label": 0
                },
                {
                    "sent": "In fact it is true for all gammas, so all the parameters, then we have to realize it is asymptotics and in the asymptotics the parameters of the gamma will not come true.",
                    "label": 0
                },
                {
                    "sent": "And you do you do select the correct bandwidth, but then maybe at higher order you would have to find 2 that for your data.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Thank you next.",
                    "label": 0
                },
                {
                    "sent": "I had a question about one of your slides where you said shrinking with the Super smooth can adapt it to anything.",
                    "label": 0
                },
                {
                    "sent": "I was wondering what's the relation between shrinking the Super smooth, by which I think you mean the squared exponential Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "What's the relationship between shrinking it horizontally and just taking a smaller horizontal length scale parameter L?",
                    "label": 0
                },
                {
                    "sent": "In this case, I think it's very different in the in the integrated Gaussian case it would be the same because it's self similar.",
                    "label": 0
                },
                {
                    "sent": "But here I think it is different.",
                    "label": 0
                },
                {
                    "sent": "I actually I don't know what would happen if you would scale the vertical thing here.",
                    "label": 0
                },
                {
                    "sent": "OK, I. I don't think it will work, but I don't really know.",
                    "label": 0
                },
                {
                    "sent": "I shouldn't have said this because it's all taped, but.",
                    "label": 0
                },
                {
                    "sent": "And I'll just quickly one of your main conclusions was about over smoothing.",
                    "label": 0
                },
                {
                    "sent": "And can I take that to imply that if you use a squared exponential covariance which is infinitely smooth, then you in some sense for uncertainty quantification, always miss the truth unless you have, as you say, excellent reason to know that the truth really is beta Infinity.",
                    "label": 0
                },
                {
                    "sent": "I think the answer is yes, so if you do not re scale the in the very smooth Gaussian process, I think that the credible sets will be completely off, but we have no mathematical proof.",
                    "label": 0
                },
                {
                    "sent": "We only have some proofs of this fact for.",
                    "label": 0
                },
                {
                    "sent": "A few typical examples, but this one we haven't handled yet.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, time for one more question.",
                    "label": 0
                },
                {
                    "sent": "Hang on, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Line please, I'll try to keep it short.",
                    "label": 0
                },
                {
                    "sent": "Your results made me made me wonder about the possibility of defining the rate of contraction and defining coverage, not relative to the truth data, but relative to a theater that's in a class that has smoothness as specified by the prior but is close to the truth data in some sense.",
                    "label": 0
                },
                {
                    "sent": "And I wonder if you thought about about that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you might have a very good idea.",
                    "label": 0
                },
                {
                    "sent": "We have thought about it, but we don't know how to make that precise and to do with it yet.",
                    "label": 0
                },
                {
                    "sent": "But it might be good in a contraction rate that is more or less included because you always look for something in the inside.",
                    "label": 0
                },
                {
                    "sent": "The support of the prior that is at a certain distance with incredible sets.",
                    "label": 0
                },
                {
                    "sent": "It is about the formulation of what do you.",
                    "label": 0
                },
                {
                    "sent": "What can you derive really?",
                    "label": 0
                },
                {
                    "sent": "What can you learn from the posterior distribution and maybe it should be phrased in the sense that some approximation of the truth.",
                    "label": 0
                },
                {
                    "sent": "Is within the posterior support somehow?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but we have thought about it, but we don't know how to formulate it at the moment.",
                    "label": 0
                },
                {
                    "sent": "OK, let's give it one more round of applause for add.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the great talk.",
                    "label": 0
                }
            ]
        }
    }
}