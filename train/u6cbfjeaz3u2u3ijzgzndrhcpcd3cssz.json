{
    "id": "u6cbfjeaz3u2u3ijzgzndrhcpcd3cssz",
    "title": "Cross-lingual infobox alignment in Wikipedia using Entity-Attribute Factor Graph",
    "info": {
        "author": [
            "Yan Zhang, Knowledge Engineering Group, Tsinghua University"
        ],
        "published": "Nov. 28, 2017",
        "recorded": "October 2017",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2017_zhang_factor_graph/",
    "segmentation": [
        [
            "I'm changing from Qinghai University Beijing.",
            "It's an honor of May to give a presentation here.",
            "My topic is a learning based approach in Wikipedia.",
            "Cross lingual infobox alignment.",
            "The presentation is organized as follows.",
            "There are four parts.",
            "Firstly I will discuss."
        ],
        [
            "Now the problem of Wikipedia infobox alignment described motivation of our work and challenges and introduce some existing methods.",
            "Secondly, I will describe our approach in this paper, namely entity attribute factor graph, EFJ for short.",
            "Thirdly, there will be some experiments and analyze this in the third part as well as some conclusions derived from this result.",
            "At last I will discuss some future works.",
            "Here, let's start from the first part, an introduction of Wikipedia infobox alignment."
        ],
        [
            "The figure shows some shows an example of attribute matching says are the info boxes of Zuckerberg, the CEO of Facebook in English, Chinese and French.",
            "Wikipedia in this picture, born chosen in Chinese and not see I don't know how to pronounce it equivalent, invokes attributes which can be easily found according to the values using.",
            "Translator oh how and the however, for our tribute network and its Chinese corresponding attribute jeans, they have different values because of timeliness, so we cannot find alignment to using value based method and Furthermore English info box has an attribute relatives.",
            "This one in the green rectangle.",
            "Age does not exist in other two virgins, so we can complete the Chinese and French info box of Zuckerberg from English Info Box.",
            "As such"
        ],
        [
            "All the above there are some reasons why we do this work.",
            "Firstly, in Wikipedia each language version maintains its own set of info boxes with their own set of attributes as well as sometimes providing different values of corresponding attributes correspond across multiple sets.",
            "Attributes in different language must be matched if we want to get coherence knowledge and develop some applications.",
            "Secondly, English Wikipedia is obviously larger of higher quantities and low resources languages, which is why we use English attributes to expand and completing the box in other languages.",
            "In addition, the number of existing attributes mappings is limited.",
            "For example, there are more than 1000 thousand attributes in English Wikipedia, but only 5000 existing attribute mappings.",
            "Between English and Chinese."
        ],
        [
            "And why this work is challenging?",
            "Firstly, text information of attributes is not so much as weekly articles.",
            "Some text similarity based methods will not be effective in the.",
            "Secondly there are police may attribute that is a gating attribute, can have different semantics.",
            "For example country can mean nationality of one person or place of.",
            "All production and there are some synonyms attributes that is different attributes may have the same meaning such as alliance and other names.",
            "There are two problems also, late awards performance on days to Problem List awards performance, unlabeled similarity or translation based methods.",
            "Thirdly, there are also some problems in the values of attributes.",
            "Different measurement and timeliness.",
            "For example, population of Beijing is 21 million and 700,000 in English edition and 2117.",
            "10,000 in Chinese measurement, while in French edition population of Beijing is 21 million and 150,000.",
            "Actually it's the number four years ago.",
            "So these are the challenges.",
            "Although they do work."
        ],
        [
            "Challenging is there are several existing methods.",
            "These methods can be splitted into two categories, similarity based and learning based boamah and the research methods used.",
            "Similarity and adults work uses logistic regression in there.",
            "In these methods only research work leverages the relation among instances in different versions of Wikipedia, but they are ignored.",
            "Correlations among attributes within one knowledge base, which we think are very important to solve the problem."
        ],
        [
            "Next I will introduce our method.",
            "Here is a picture to describe our proposed model.",
            "This figure contains 2 parts, though one in left is correlation graph which presents several relations in two editions of Wikipedia, K1 and K2.",
            "Different language versions are separated by a diagonal line.",
            "This one and the upper layer is attribute layer.",
            "It contains attributes and templates relations among them.",
            "Similarly, the lower layer art is article layer an.",
            "It contains articles and category relations.",
            "The imaginary lines between the two layers denote the relation of usage between the articles and attributes.",
            "And the trend lines denote the existing cross lingo links.",
            "On the one end alright is factor graph.",
            "Do write notes are variables and there are two types of arrivals.",
            "Namely, don't.",
            "Although observe the rivals and hidden rivals.",
            "A lot of observed variables are XI and hidden variables are why I.",
            "And each consider attribute pair is mapped to an observed variable XI.",
            "The hidden variable.",
            "Why I represent present Bolan label which means equivalent or in equivalent of the observed variable XI?",
            "For example X2 is cross bonding with a candidate attribute pair.",
            "This one Pi sorry and PJ two and there exist across Lingo Link here we can see.",
            "So the hidden label YIY two is labeled one the black nodes in factor graph are factors these black nodes.",
            "And there are three types FG&H.",
            "Each type is associated with the kind of feature function which transforms relations into a computable feature as a the location at a local feature, which presents the posterior probability of Labor.",
            "Why I giving XC8 describes local information and similarity on.",
            "Observed variables in our model J denotes the correlation between hidden variables according to template information and seemingly HAH is factor to formulate the synonym am information.",
            "In short factor F is for ad hoc features and try to distinguish alignment using similarity features.",
            "While G&H are designed for modeling the correlations among attributes within Wikipedia."
        ],
        [
            "A for local days are the features we used in this paper for local features.",
            "We use four types.",
            "Similarity for label label similarity.",
            "We use Levenshtein distance, namely the edit distance and word embedding distance.",
            "Subject oriented similarity article usage feature and category similarity features and for template feature where you the template to build cooccurrence relations and synonym feature where you the semantic relatedness of attributes.",
            "Then according to the probability graph generated by the."
        ],
        [
            "Of factor graph we can get a joint distribution.",
            "PY thus giving a set of these traits are tribute mappings in the model learning.",
            "The model is to estimate an optimum paramenter configuration data to maximum the like like the log likelihood of P. Why we use log likehood function as the objective function where YL?",
            "Denotes the known labels.",
            "Zen will apply to a gradient decent method to estimate biomatters, and after learning the optimal parameter, we can infer the unknown labels by finding a set of labels which maximize maximize the joint probability PY.",
            "And the next are the experiments and conclusions."
        ],
        [
            "We construct two data set English Chinese and English French from existing cross lingual arch build links in Wikipedia.",
            "In each data set we randomly select 2000 crossing corresponding attribute pairs which are labeled as positive instances.",
            "For each positive instance, we generate five negative instances by randomly replacing one of the attribute in the pairs withdrawn one and.",
            "The below label there are four methods, namely.",
            "Label matching our M and similarity aggregation assay and support vector machine SVM and logistic regression by Adams work for comparison and two versions of our EFJ.",
            "The NT version is not translating, not not translation, and the table shows the performance of these five methods on English, Chinese and English.",
            "French data set.",
            "For English, Chinese data set according to the records to LMS code gets the highest precision, but at recall is only 12626 hour.",
            "I'm only use translations.",
            "Only use translations of models in the essay is the average of five features we introducted above, namely label similarity, subjective subject, object similarity and article usage.",
            "Similarity category similarity and word embedding similarity.",
            "As well, I measured first compute define similarity in SA and then trains as I model in RADAR method author design the 26 features and trained a logistic regression model to solve this problem.",
            "Apparently the variety of result and strict matching condition are the main reasons of the result.",
            "By using similarities on variety information, I say improves recall significantly on in comparison to our end, but it does not achieve good praises.",
            "Precision Becausw averaging strategy is simple strategy.",
            "And as we an hour and hour, are both learning based methods.",
            "SVM gets a presentation.",
            "Guys are good precision with a record 75 compared with the Aswan, our guides better precision but lower com and performs out as we end by 1% in terms of F1 score.",
            "Our method afj uses the same training data with SVM and outperforms.",
            "I swam by 4.6% in terms of F. One measure and our model get similar precision ways our AAD AR, but our model is able to discover more attribute mappings by considering the correlation between attribute pairs.",
            "And the next is summer."
        ],
        [
            "Examples way of the experiments."
        ],
        [
            "Apparently we can transfer info box of information that is missing in one language from other languages in which the information is already present.",
            "If we have the alignment of attributes in this paper, we try to complete Chinese and English Wikipedia infoboxes from each other using the attribute alignments obtained by our model.",
            "Firstly, we extract 223 thousand existing corresponding.",
            "English and Chinese article pairs.",
            "And finally we get 76,000 article pairs rapidly.",
            "All replenished by at least one attribute value.",
            "You can see the table of the result.",
            "This shows the number of added attribute values with respect to each article.",
            "The maximum the maximum number of added attribute values for one article is 34 and the average is 5.75.",
            "A which indicates the infoboxes in Chinese and English.",
            "Both benefit a lot from the attribute alignment.",
            "And in the future."
        ],
        [
            "Xplore's cross lingual cycle lob Cyclopaedia knowledge base.",
            "In our group an in the future we will do attribute alignment in acts law and also we will design A model to match entities and attributes itera tively to get better performance and that sort of thing."
        ],
        [
            "Two thank you.",
            "When you mention at the beginning that in some cases the values of the property are incorrect, like for measurements.",
            "So what do you do?",
            "I know I think a sample was for the population of China, So what do you do in this case is do you still align them with a margin of error or you don't align them all?",
            "You may way align what when you have to infobox properties you have different value like in the case, the formulation of China in the English.",
            "You be there versus the Chinese TV pedia.",
            "Yeah, wait.",
            "Wait, wait.",
            "All the my automatically and we design some features described above.",
            "Oh here we use these features.",
            "Yeah yeah OK so it depends on the gaze on the other features, not only on the value.",
            "Yeah, let's go this way.",
            "I consider much more information than the only label similarity, although value types.",
            "OK, yeah the there are two models.",
            "One is not motion translation, another is using translators and label similarity.",
            "The added distance is the distance between the label and.",
            "For example in English and we translate Chinese to English and within compared the added distance of the two labels the translated labels.",
            "And in the result this.",
            "EAPJ&T model we don't use the translate translation based features.",
            "Hi, thank you.",
            "I'll just wondering two parts.",
            "Have you contributed the new fax back to Wikipedia?",
            "Just wondering 1st and Secondly, have you looked at?",
            "Not all articles are equal so I imagine an article on London in in English so probably will be more up-to-date authorative than say the Beijing article which might be in in Mandarin.",
            "Do you look at the authorative nature of a document and decide?",
            "Well that might actually Trump which way the information needs to flow?",
            "If there's a soul dooming, or do we consider the article content or or the you know how authoritative and might be?",
            "So how many times it's been edited when lost was that edited?",
            "Has it been checked by a journalist?",
            "Was that article more likely to be authoritative in English and Chinese?",
            "'cause obviously you're trying to find a discrepancy between the boxes, but I imagine certain articles.",
            "Will be more authorative in different languages, so we just alignment the info box and we only used.",
            "Article usage information.",
            "Not the content.",
            "They how much.",
            "How much did it take the time to process all these 2000 attribute pairs?",
            "Scale scale?",
            "Yeah, yeah, like how much time did it take for the this experiment or the training training and training I didn't remember about the complete melting experiments.",
            "Spend about 10 hours.",
            "This is.",
            "Yeah, the complaint ING experiment.",
            "OK, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm changing from Qinghai University Beijing.",
                    "label": 1
                },
                {
                    "sent": "It's an honor of May to give a presentation here.",
                    "label": 0
                },
                {
                    "sent": "My topic is a learning based approach in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "Cross lingual infobox alignment.",
                    "label": 0
                },
                {
                    "sent": "The presentation is organized as follows.",
                    "label": 0
                },
                {
                    "sent": "There are four parts.",
                    "label": 0
                },
                {
                    "sent": "Firstly I will discuss.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the problem of Wikipedia infobox alignment described motivation of our work and challenges and introduce some existing methods.",
                    "label": 0
                },
                {
                    "sent": "Secondly, I will describe our approach in this paper, namely entity attribute factor graph, EFJ for short.",
                    "label": 0
                },
                {
                    "sent": "Thirdly, there will be some experiments and analyze this in the third part as well as some conclusions derived from this result.",
                    "label": 0
                },
                {
                    "sent": "At last I will discuss some future works.",
                    "label": 0
                },
                {
                    "sent": "Here, let's start from the first part, an introduction of Wikipedia infobox alignment.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The figure shows some shows an example of attribute matching says are the info boxes of Zuckerberg, the CEO of Facebook in English, Chinese and French.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia in this picture, born chosen in Chinese and not see I don't know how to pronounce it equivalent, invokes attributes which can be easily found according to the values using.",
                    "label": 0
                },
                {
                    "sent": "Translator oh how and the however, for our tribute network and its Chinese corresponding attribute jeans, they have different values because of timeliness, so we cannot find alignment to using value based method and Furthermore English info box has an attribute relatives.",
                    "label": 0
                },
                {
                    "sent": "This one in the green rectangle.",
                    "label": 0
                },
                {
                    "sent": "Age does not exist in other two virgins, so we can complete the Chinese and French info box of Zuckerberg from English Info Box.",
                    "label": 0
                },
                {
                    "sent": "As such",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All the above there are some reasons why we do this work.",
                    "label": 0
                },
                {
                    "sent": "Firstly, in Wikipedia each language version maintains its own set of info boxes with their own set of attributes as well as sometimes providing different values of corresponding attributes correspond across multiple sets.",
                    "label": 1
                },
                {
                    "sent": "Attributes in different language must be matched if we want to get coherence knowledge and develop some applications.",
                    "label": 0
                },
                {
                    "sent": "Secondly, English Wikipedia is obviously larger of higher quantities and low resources languages, which is why we use English attributes to expand and completing the box in other languages.",
                    "label": 1
                },
                {
                    "sent": "In addition, the number of existing attributes mappings is limited.",
                    "label": 0
                },
                {
                    "sent": "For example, there are more than 1000 thousand attributes in English Wikipedia, but only 5000 existing attribute mappings.",
                    "label": 0
                },
                {
                    "sent": "Between English and Chinese.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And why this work is challenging?",
                    "label": 0
                },
                {
                    "sent": "Firstly, text information of attributes is not so much as weekly articles.",
                    "label": 0
                },
                {
                    "sent": "Some text similarity based methods will not be effective in the.",
                    "label": 0
                },
                {
                    "sent": "Secondly there are police may attribute that is a gating attribute, can have different semantics.",
                    "label": 0
                },
                {
                    "sent": "For example country can mean nationality of one person or place of.",
                    "label": 0
                },
                {
                    "sent": "All production and there are some synonyms attributes that is different attributes may have the same meaning such as alliance and other names.",
                    "label": 0
                },
                {
                    "sent": "There are two problems also, late awards performance on days to Problem List awards performance, unlabeled similarity or translation based methods.",
                    "label": 0
                },
                {
                    "sent": "Thirdly, there are also some problems in the values of attributes.",
                    "label": 0
                },
                {
                    "sent": "Different measurement and timeliness.",
                    "label": 0
                },
                {
                    "sent": "For example, population of Beijing is 21 million and 700,000 in English edition and 2117.",
                    "label": 0
                },
                {
                    "sent": "10,000 in Chinese measurement, while in French edition population of Beijing is 21 million and 150,000.",
                    "label": 1
                },
                {
                    "sent": "Actually it's the number four years ago.",
                    "label": 0
                },
                {
                    "sent": "So these are the challenges.",
                    "label": 0
                },
                {
                    "sent": "Although they do work.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Challenging is there are several existing methods.",
                    "label": 0
                },
                {
                    "sent": "These methods can be splitted into two categories, similarity based and learning based boamah and the research methods used.",
                    "label": 0
                },
                {
                    "sent": "Similarity and adults work uses logistic regression in there.",
                    "label": 0
                },
                {
                    "sent": "In these methods only research work leverages the relation among instances in different versions of Wikipedia, but they are ignored.",
                    "label": 0
                },
                {
                    "sent": "Correlations among attributes within one knowledge base, which we think are very important to solve the problem.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next I will introduce our method.",
                    "label": 0
                },
                {
                    "sent": "Here is a picture to describe our proposed model.",
                    "label": 0
                },
                {
                    "sent": "This figure contains 2 parts, though one in left is correlation graph which presents several relations in two editions of Wikipedia, K1 and K2.",
                    "label": 0
                },
                {
                    "sent": "Different language versions are separated by a diagonal line.",
                    "label": 0
                },
                {
                    "sent": "This one and the upper layer is attribute layer.",
                    "label": 0
                },
                {
                    "sent": "It contains attributes and templates relations among them.",
                    "label": 0
                },
                {
                    "sent": "Similarly, the lower layer art is article layer an.",
                    "label": 0
                },
                {
                    "sent": "It contains articles and category relations.",
                    "label": 0
                },
                {
                    "sent": "The imaginary lines between the two layers denote the relation of usage between the articles and attributes.",
                    "label": 0
                },
                {
                    "sent": "And the trend lines denote the existing cross lingo links.",
                    "label": 0
                },
                {
                    "sent": "On the one end alright is factor graph.",
                    "label": 0
                },
                {
                    "sent": "Do write notes are variables and there are two types of arrivals.",
                    "label": 0
                },
                {
                    "sent": "Namely, don't.",
                    "label": 0
                },
                {
                    "sent": "Although observe the rivals and hidden rivals.",
                    "label": 0
                },
                {
                    "sent": "A lot of observed variables are XI and hidden variables are why I.",
                    "label": 0
                },
                {
                    "sent": "And each consider attribute pair is mapped to an observed variable XI.",
                    "label": 0
                },
                {
                    "sent": "The hidden variable.",
                    "label": 0
                },
                {
                    "sent": "Why I represent present Bolan label which means equivalent or in equivalent of the observed variable XI?",
                    "label": 0
                },
                {
                    "sent": "For example X2 is cross bonding with a candidate attribute pair.",
                    "label": 0
                },
                {
                    "sent": "This one Pi sorry and PJ two and there exist across Lingo Link here we can see.",
                    "label": 0
                },
                {
                    "sent": "So the hidden label YIY two is labeled one the black nodes in factor graph are factors these black nodes.",
                    "label": 0
                },
                {
                    "sent": "And there are three types FG&H.",
                    "label": 0
                },
                {
                    "sent": "Each type is associated with the kind of feature function which transforms relations into a computable feature as a the location at a local feature, which presents the posterior probability of Labor.",
                    "label": 1
                },
                {
                    "sent": "Why I giving XC8 describes local information and similarity on.",
                    "label": 0
                },
                {
                    "sent": "Observed variables in our model J denotes the correlation between hidden variables according to template information and seemingly HAH is factor to formulate the synonym am information.",
                    "label": 0
                },
                {
                    "sent": "In short factor F is for ad hoc features and try to distinguish alignment using similarity features.",
                    "label": 0
                },
                {
                    "sent": "While G&H are designed for modeling the correlations among attributes within Wikipedia.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A for local days are the features we used in this paper for local features.",
                    "label": 0
                },
                {
                    "sent": "We use four types.",
                    "label": 0
                },
                {
                    "sent": "Similarity for label label similarity.",
                    "label": 0
                },
                {
                    "sent": "We use Levenshtein distance, namely the edit distance and word embedding distance.",
                    "label": 1
                },
                {
                    "sent": "Subject oriented similarity article usage feature and category similarity features and for template feature where you the template to build cooccurrence relations and synonym feature where you the semantic relatedness of attributes.",
                    "label": 1
                },
                {
                    "sent": "Then according to the probability graph generated by the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of factor graph we can get a joint distribution.",
                    "label": 1
                },
                {
                    "sent": "PY thus giving a set of these traits are tribute mappings in the model learning.",
                    "label": 0
                },
                {
                    "sent": "The model is to estimate an optimum paramenter configuration data to maximum the like like the log likelihood of P. Why we use log likehood function as the objective function where YL?",
                    "label": 0
                },
                {
                    "sent": "Denotes the known labels.",
                    "label": 0
                },
                {
                    "sent": "Zen will apply to a gradient decent method to estimate biomatters, and after learning the optimal parameter, we can infer the unknown labels by finding a set of labels which maximize maximize the joint probability PY.",
                    "label": 0
                },
                {
                    "sent": "And the next are the experiments and conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We construct two data set English Chinese and English French from existing cross lingual arch build links in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "In each data set we randomly select 2000 crossing corresponding attribute pairs which are labeled as positive instances.",
                    "label": 1
                },
                {
                    "sent": "For each positive instance, we generate five negative instances by randomly replacing one of the attribute in the pairs withdrawn one and.",
                    "label": 0
                },
                {
                    "sent": "The below label there are four methods, namely.",
                    "label": 0
                },
                {
                    "sent": "Label matching our M and similarity aggregation assay and support vector machine SVM and logistic regression by Adams work for comparison and two versions of our EFJ.",
                    "label": 0
                },
                {
                    "sent": "The NT version is not translating, not not translation, and the table shows the performance of these five methods on English, Chinese and English.",
                    "label": 0
                },
                {
                    "sent": "French data set.",
                    "label": 0
                },
                {
                    "sent": "For English, Chinese data set according to the records to LMS code gets the highest precision, but at recall is only 12626 hour.",
                    "label": 0
                },
                {
                    "sent": "I'm only use translations.",
                    "label": 0
                },
                {
                    "sent": "Only use translations of models in the essay is the average of five features we introducted above, namely label similarity, subjective subject, object similarity and article usage.",
                    "label": 0
                },
                {
                    "sent": "Similarity category similarity and word embedding similarity.",
                    "label": 0
                },
                {
                    "sent": "As well, I measured first compute define similarity in SA and then trains as I model in RADAR method author design the 26 features and trained a logistic regression model to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "Apparently the variety of result and strict matching condition are the main reasons of the result.",
                    "label": 0
                },
                {
                    "sent": "By using similarities on variety information, I say improves recall significantly on in comparison to our end, but it does not achieve good praises.",
                    "label": 0
                },
                {
                    "sent": "Precision Becausw averaging strategy is simple strategy.",
                    "label": 0
                },
                {
                    "sent": "And as we an hour and hour, are both learning based methods.",
                    "label": 0
                },
                {
                    "sent": "SVM gets a presentation.",
                    "label": 0
                },
                {
                    "sent": "Guys are good precision with a record 75 compared with the Aswan, our guides better precision but lower com and performs out as we end by 1% in terms of F1 score.",
                    "label": 0
                },
                {
                    "sent": "Our method afj uses the same training data with SVM and outperforms.",
                    "label": 0
                },
                {
                    "sent": "I swam by 4.6% in terms of F. One measure and our model get similar precision ways our AAD AR, but our model is able to discover more attribute mappings by considering the correlation between attribute pairs.",
                    "label": 0
                },
                {
                    "sent": "And the next is summer.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples way of the experiments.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apparently we can transfer info box of information that is missing in one language from other languages in which the information is already present.",
                    "label": 0
                },
                {
                    "sent": "If we have the alignment of attributes in this paper, we try to complete Chinese and English Wikipedia infoboxes from each other using the attribute alignments obtained by our model.",
                    "label": 0
                },
                {
                    "sent": "Firstly, we extract 223 thousand existing corresponding.",
                    "label": 0
                },
                {
                    "sent": "English and Chinese article pairs.",
                    "label": 0
                },
                {
                    "sent": "And finally we get 76,000 article pairs rapidly.",
                    "label": 0
                },
                {
                    "sent": "All replenished by at least one attribute value.",
                    "label": 0
                },
                {
                    "sent": "You can see the table of the result.",
                    "label": 0
                },
                {
                    "sent": "This shows the number of added attribute values with respect to each article.",
                    "label": 0
                },
                {
                    "sent": "The maximum the maximum number of added attribute values for one article is 34 and the average is 5.75.",
                    "label": 0
                },
                {
                    "sent": "A which indicates the infoboxes in Chinese and English.",
                    "label": 0
                },
                {
                    "sent": "Both benefit a lot from the attribute alignment.",
                    "label": 0
                },
                {
                    "sent": "And in the future.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Xplore's cross lingual cycle lob Cyclopaedia knowledge base.",
                    "label": 0
                },
                {
                    "sent": "In our group an in the future we will do attribute alignment in acts law and also we will design A model to match entities and attributes itera tively to get better performance and that sort of thing.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two thank you.",
                    "label": 0
                },
                {
                    "sent": "When you mention at the beginning that in some cases the values of the property are incorrect, like for measurements.",
                    "label": 0
                },
                {
                    "sent": "So what do you do?",
                    "label": 0
                },
                {
                    "sent": "I know I think a sample was for the population of China, So what do you do in this case is do you still align them with a margin of error or you don't align them all?",
                    "label": 0
                },
                {
                    "sent": "You may way align what when you have to infobox properties you have different value like in the case, the formulation of China in the English.",
                    "label": 0
                },
                {
                    "sent": "You be there versus the Chinese TV pedia.",
                    "label": 0
                },
                {
                    "sent": "Yeah, wait.",
                    "label": 0
                },
                {
                    "sent": "Wait, wait.",
                    "label": 0
                },
                {
                    "sent": "All the my automatically and we design some features described above.",
                    "label": 0
                },
                {
                    "sent": "Oh here we use these features.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah OK so it depends on the gaze on the other features, not only on the value.",
                    "label": 0
                },
                {
                    "sent": "Yeah, let's go this way.",
                    "label": 0
                },
                {
                    "sent": "I consider much more information than the only label similarity, although value types.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah the there are two models.",
                    "label": 0
                },
                {
                    "sent": "One is not motion translation, another is using translators and label similarity.",
                    "label": 0
                },
                {
                    "sent": "The added distance is the distance between the label and.",
                    "label": 0
                },
                {
                    "sent": "For example in English and we translate Chinese to English and within compared the added distance of the two labels the translated labels.",
                    "label": 0
                },
                {
                    "sent": "And in the result this.",
                    "label": 0
                },
                {
                    "sent": "EAPJ&T model we don't use the translate translation based features.",
                    "label": 0
                },
                {
                    "sent": "Hi, thank you.",
                    "label": 0
                },
                {
                    "sent": "I'll just wondering two parts.",
                    "label": 0
                },
                {
                    "sent": "Have you contributed the new fax back to Wikipedia?",
                    "label": 0
                },
                {
                    "sent": "Just wondering 1st and Secondly, have you looked at?",
                    "label": 0
                },
                {
                    "sent": "Not all articles are equal so I imagine an article on London in in English so probably will be more up-to-date authorative than say the Beijing article which might be in in Mandarin.",
                    "label": 0
                },
                {
                    "sent": "Do you look at the authorative nature of a document and decide?",
                    "label": 0
                },
                {
                    "sent": "Well that might actually Trump which way the information needs to flow?",
                    "label": 0
                },
                {
                    "sent": "If there's a soul dooming, or do we consider the article content or or the you know how authoritative and might be?",
                    "label": 0
                },
                {
                    "sent": "So how many times it's been edited when lost was that edited?",
                    "label": 0
                },
                {
                    "sent": "Has it been checked by a journalist?",
                    "label": 0
                },
                {
                    "sent": "Was that article more likely to be authoritative in English and Chinese?",
                    "label": 0
                },
                {
                    "sent": "'cause obviously you're trying to find a discrepancy between the boxes, but I imagine certain articles.",
                    "label": 0
                },
                {
                    "sent": "Will be more authorative in different languages, so we just alignment the info box and we only used.",
                    "label": 0
                },
                {
                    "sent": "Article usage information.",
                    "label": 0
                },
                {
                    "sent": "Not the content.",
                    "label": 0
                },
                {
                    "sent": "They how much.",
                    "label": 0
                },
                {
                    "sent": "How much did it take the time to process all these 2000 attribute pairs?",
                    "label": 0
                },
                {
                    "sent": "Scale scale?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, like how much time did it take for the this experiment or the training training and training I didn't remember about the complete melting experiments.",
                    "label": 0
                },
                {
                    "sent": "Spend about 10 hours.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the complaint ING experiment.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}