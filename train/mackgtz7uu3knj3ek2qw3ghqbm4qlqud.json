{
    "id": "mackgtz7uu3knj3ek2qw3ghqbm4qlqud",
    "title": "Feature Selection via Detecting Ineffective Features",
    "info": {
        "author": [
            "Kris De Brabanter, Optimization in Engineering Center (OPTEC), KU Leuven"
        ],
        "published": "Aug. 26, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Compressed Sensing",
            "Top->Computer Science->Machine Learning->Regularization"
        ]
    },
    "url": "http://videolectures.net/roks2013_brabanter_feature/",
    "segmentation": [
        [
            "Good afternoon everybody.",
            "So as you already said, this is joint work with lots of deer free from hungry so he is not here today.",
            "But I will try to present it as good as possible.",
            "So today we're going to talk about feature selection via detecting ineffective features.",
            "So let's talk."
        ],
        [
            "With the outline of this talk.",
            "So first we're going to discuss the problem.",
            "I mean, I think everybody knows what feature selection or variable selection is well, and then I just give some applications and some methods that are used to do that.",
            "Then we go to our proposed methodology first.",
            "The theoretical aspects, the rate of convergence, and so on.",
            "Universal consistency, and then we get formulation of the hypothesis test, which is actually the final result.",
            "And to end we'll just have some simulations should be simulations.",
            "There's more than one, and there is a conclusion."
        ],
        [
            "So let's start with the first part introduction."
        ],
        [
            "So feature selection is something like you see here.",
            "So suppose behind or a, there is a nice nice fresh cold beer in this warm temperature today and here there is nothing and you just want to know.",
            "And you just want to know.",
            "Where is the beard, but you don't have any prior knowledge.",
            "Right, so of course you would choose a if you had prior knowledge, but you don't have to feature selection or something like that, so you don't have any prior knowledge in our case.",
            "So what are the good features?",
            "Are the good things in this case we need to determine door a.",
            "So here there are only two features, let's say, but normally you have like D features, so it's complicated a lot."
        ],
        [
            "So application so everybody knows this is a bad picture of a microarray."
        ],
        [
            "And the important thing is like finding important genes.",
            "They're ranked like this, right?",
            "And methods to."
        ],
        [
            "Do that are as before, a lasso or lasu you have come here which is used.",
            "You have a T test or variable selection.",
            "Via additive models you have large, you have many others and you see this is quite recently, but it has to be through additive models.",
            "OK."
        ],
        [
            "So let's start.",
            "So we have our data first here.",
            "XNNYN pairs XY can be a Vardy.",
            "An why Kim is an element of art, so they are iid.",
            "This is important and the model that we specify is just a linear model, as in lasso, linear model with an error term.",
            "The epsilon here, and the epsilons of course are IID and independent of the XI.",
            "The conditional mean of the errors are zero.",
            "This is the assumption of law.",
            "So so and then in standard case like before the previous talk already discussed that this is a standard lasso, right?",
            "So I'm not will not dwell on this further.",
            "So everybody knows this.",
            "This is a matrix notation.",
            "We need to minimize this and this is the penalty term and here this is a tuning parameter Lambda."
        ],
        [
            "OK, so as you see here I put this in red because it's a very important parameter if you don't tune it very carefully then you might up with wrong results.",
            "So what do we need to do there?",
            "Is there are two different aspects in law, so the first thing is what do you want to prove it?",
            "When you work with law?",
            "So do you want to use it for prediction too?",
            "That means standard regression.",
            "Let's say to predict and you point.",
            "Or do you want to use it for example for feature selection or variable selection?",
            "There are two different tasks, so the first thing standard regression.",
            "So what do we need consistency that means that the tuning parameter has to be of disorder, so D is the dimension of the."
        ],
        [
            "Problem and Art is a sample size, so it has to be of this order.",
            "Also the L1 norm of your coefficients have to be of this of small oh.",
            "And then then we can prove.",
            "See here Green Steiner it off that the that the lasso is consistency in prediction as one goes to Infinity that is 1.",
            "Now it totally changes if we want to use the law.",
            "So for example for variable selection."
        ],
        [
            "Because what's happening now?",
            "Suppose we define a set as hat that detects.",
            "All our estimated coefficients, beta which are non 0 right.",
            "Then you have here what is called a battermann condition.",
            "That means that it.",
            "Always detects the variables the betas, but they have to be larger than the cardinality of us, so the true set here, not the estimated set.",
            "And also depending on the logarithm of Z&M and also here.",
            "This is the important case the Lambda has to be of bigger order than the one for regression.",
            "But the huge problem is here.",
            "I can tune the Lambda for example for in with cross validation.",
            "No problem here.",
            "If I tune the Lambda with cross validation I end up with this order and not this one.",
            "So Whoops, we have a problem.",
            "So under these conditions we can prove this is consistency.",
            "That means that our estimated subset will be exactly the true subset in probability and would go to one's D is larger than an N goes to Infinity, so D will go to Infinity faster.",
            "Man, no problem.",
            "Huge problem this so you cannot do that with just simply akaiki by C."
        ],
        [
            "For cross validation, so there's there is something going on which is not that nice.",
            "So."
        ],
        [
            "Like I said, we have no.",
            "There are no theoretical results that say if you use be icy cross validation and so on that variable selection is consistent with law so.",
            "So if you use standard TV then lasso select more variables than needed.",
            "Well that means you will have your good subset plus some extras, so you will always have more.",
            "Another thing what you can do is for example choose is very close to 0 and you follow the regularization part of the Lars Till the end that you can do again.",
            "No theoretical justification to do that, but it's a possibility.",
            "So there are other ways to tune that.",
            "It's discussed in the book of Billman and Vonda here.",
            "It's stability selec."
        ],
        [
            "Action or hypothesis testing?",
            "So we're not going to dwell on that further, but just what can we?",
            "What can we actually say about that?",
            "So we only have this at our disposal.",
            "We don't have anything else.",
            "So what can we say?"
        ],
        [
            "Well, we have to be.",
            "Um?",
            "Happy with a less strong result.",
            "What's the less strong result instead of actually defining an asset, we will define as relevant, set the relevant set.",
            "Is that the true coefficients of beta in absolute value are larger than some C where C is between zero and Infinity.",
            "Note here it's not equal to 0 as we want to, but that's the theoretical part here and then we can prove very.",
            "It's not so difficult that our estimated that the relevant set here.",
            "Is a subset of the estimated one and it goes in probability to one friend to Infinity.",
            "Is it bad?",
            "No, that's also something that we need, and this we can use.",
            "Actually, this is just some theoretical theoretical thing here that the CN has to be some larger order than.",
            "Here.",
            "Again, it depends on the cardinality of the true subset, which you don't know.",
            "So how the case here?",
            "Again, how do I choose the C?",
            "Well, in practice you can't.",
            "Again, the longer end can be of this order, no problem.",
            "But then with high probability we go immediately to this one.",
            "The Lambda selected by cross validation here.",
            "Well, that will be here.",
            "We will have a relevant subset of that of our estimated ones.",
            "Are we happy with that?",
            "We have two, so there is nothing else to do.",
            "We just have to be happy with it.",
            "Now all this consistency results and all that is derived under the assumption that a linear model is present.",
            "That is very important.",
            "So it means before you start with law.",
            "So you just have to think is my total data set.",
            "Is it linear?",
            "Does it fit a linear model?",
            "If not, the lasso will give wrong results.",
            "So."
        ],
        [
            "Let's give a summary of Russell, the pros and the cons.",
            "Well, it's very easy to implement, right?",
            "There are fast algorithms available in model up and R. Also, the results are very easy.",
            "The beta is 0 or it's not zero and you have a solid theoretical foundation.",
            "If the model is correct.",
            "If the model is not correct, then you don't have any theoretical, so there you can have a worst case, but the worst case it selects oh so.",
            "No.",
            "So the actually the drawbacks of law."
        ],
        [
            "So is that model based?",
            "You have to agree with the linear form.",
            "Are not the model restriction is not always necessarily met, unfortunately, and you cannot.",
            "It's not always easy to check.",
            "So also no theoretical results if the model assumption does not hold.",
            "So what do we want to do in our thing in our methodology?"
        ],
        [
            "Well, we want to use this.",
            "Of course we want to keep it, but we want to make reductions on these ones.",
            "So you will see at the end this will turn green, but unfortunately they were turn 1 red and we're still busy on that.",
            "So what is our?"
        ],
        [
            "Proposed methodology well, the first theoretical aspects.",
            "Let's just consider it we have a Y again the same as element of R and the axis.",
            "So the Bolt X is our our matrix and are elements of our D. Alright, so this here is the minimum mean squared error as defined in the regression case standard regression.",
            "So I put it here.",
            "You can work this square out in the IID case.",
            "Of course an you will have this.",
            "Now if you look at that, this is easy to calculate, right?",
            "You can just use the strong law of large numbers.",
            "You have that this is a little bit more difficult.",
            "You cannot simply replace that by one over end of some because you don't have access to any of em of ex."
        ],
        [
            "So what do we do?",
            "We propose our estimator L star N. So all in this is with N. Here are estimators.",
            "So we propose here the strong law of large numbers, this one, and then we propose to estimate this second term by 1 / N The why and the Y I-1 is actually the value of its nearest neighbor of XI from the subset where XI is deleted.",
            "That's our estimator, actually of the variance.",
            "This works in D dimensions.",
            "It works in one dimension of course, but it works in D dimensions too.",
            "So this is very easy to implement, you don't need.",
            "There is no model assumption anything.",
            "All there is no modeler restriction, there is no parametric form.",
            "You don't need to know anything.",
            "Just calculate this is very easy, just a first nearest neighbor."
        ],
        [
            "So our theoretical aspects.",
            "The first thing that an estimator should have it's it should be consistent.",
            "If it's not, if you use an estimator that is not consistent, then well, you know you don't know what your estimated.",
            "So under the assumption that we have bounded Y, and I know in some theorists can be very angry with that because you exclude the Gaussian case.",
            "If you do that, you don't have bounded.",
            "Why in theory it's unbounded?",
            "But if you assume boundary Y?"
        ],
        [
            "Then our estimator will go to the true one in an almost sure sense.",
            "And because, yeah, some people don't like bounded why?",
            "Especially for those we have this condition of a second bounded moment which is.",
            "Less restrictive and then we have a universally consistent result in almost sure sense.",
            "So it means that our estimator if you if you use it, it estimates the correct quantity.",
            "Of course, for N going to Infinity as always.",
            "And then what you always need to do or what is interesting to do is yeah, I can estimate might ruin, but how fast does it converge to the true one called the rate of convergence?",
            "Well in this case we need we need to assume abounded X.",
            "So let's say the norm of X and the absolute value of Y is bounded and the regression function is Lipschitz continuous.",
            "That's the only thing.",
            "And then you have to assume in the nearest neighbour terminology that ties do not occur at all.",
            "So with probability zero, they occur.",
            "So that means that you have the same.",
            "You select the same nearest neighbors.",
            "This is true for on going to Infinity.",
            "Then in case for D and I will explain immediately why larger or equal than two.",
            "This is RL one rate of convergence, so C1 and C2 are just constants, not depending on N&D.",
            "And this is the result in case of D1 this drops.",
            "So you cannot fill in here D1?",
            "OK, the derivation is slightly different, so it drops and then you have the so called root N consistency, which are which is the best non parameter create you can have.",
            "So further in this case, if N goes to Infinity much faster than D. We have consistency.",
            "In almost sure sense, forgot that here.",
            "Of course, if D goes faster to Infinity then N if you fill it in here.",
            "We have a problem.",
            "It doesn't work.",
            "So that's again.",
            "I think if P is very very large and any small, our method does not work statistically.",
            "You can calculate it.",
            "Of course it's not difficult, but the results you cannot trust."
        ],
        [
            "OK.",
            "So how can we detect these features?",
            "Well, very simply, for example.",
            "Leave the Kate feature in your matrix.",
            "Take it out.",
            "That's what it's saying here.",
            "Take that one out.",
            "And then our corresponding minimum mean squared error we denoted by L star minus K is simply given by this.",
            "This is the regression function with the Kate feature out.",
            "Right in L2 cents.",
            "Here.",
            "So what do we need to do is now, well calculate or try to calculate this quantity.",
            "And calculate the quantity where the key is put back in into the matrix, so the full one in this case so and then you just see are they equal?",
            "Or are they?",
            "Not?",
            "Well, they're not never going to be exactly equal, but you make the difference between these two.",
            "That's the idea behind it.",
            "You make the difference and then you have some quantity and then you can if you have the limit distribution.",
            "For example of this difference, then you can assess whether that quantity is relevant or not, and that's exactly what we're going to do.",
            "So in this case, if they're equal, means that the leaving, leaving out decayed component doesn't increase the minimum mean squared error."
        ],
        [
            "Hence, it's not important to leave it out."
        ],
        [
            "That's the thing.",
            "So an under the null hypothesis, so we use it HK to just."
        ],
        [
            "To say the Kate variable is put out, but this is the null, very important.",
            "Then we have the regression function.",
            "This is by definition in L2, right?",
            "The conditional mean.",
            "Is equal because we assume this is the note is equal on the conditional mean, but with the Kate feature left out, meaning that the regression function, the original one is the same as the regression function with the K1 out in normal almost sure sense.",
            "So that means that you do not estimator regression function.",
            "They are equal and this is very important.",
            "It means that in our theoretical derivations you do not need to assume further smoothness on the regression function and the dimensionality does not count.",
            "Under this one, the null hypothesis, not the alternative.",
            "Then it does come, but under the note.",
            "But exactly, you never know when you're exactly under the note.",
            "The dimensionality and the smoothness of the regression function does not count, so you have it.",
            "You can have a discontinuous function no problem."
        ],
        [
            "So let's denote the data without the Kate component as this.",
            "So we just leave the K component out.",
            "This is the N -- K."
        ],
        [
            "We estimate the minimum variance by the cake out by using this form.",
            "Right, it's the same as before, but now it's just the nearest neighbor, just with the Kate component out.",
            "Our test statistic is as follows, calculate R."
        ],
        [
            "Estimator of this one right, minus the estimate of the of the full data set and then you reduce it to this.",
            "So what we need is our data.",
            "This we have and just the first nearest neighbor with the cat component out.",
            "You accept the hypothesis if this difference is small, but like I said before, define small.",
            "So we need actually the limit distribution of this guy.",
            "This is a random variable, so you can.",
            "It has a limit distribution.",
            "The problem is now find it well, OK, here it goes.",
            "So there is another problem that we need to assess."
        ],
        [
            "So this part is small even when the null hypothesis is true.",
            "Very rare, but I mean very annoying.",
            "But it's true.",
            "So why?",
            "Because this the why I the first nearest neighbor of the full is equal to the first nearest neighbor with Kate component out.",
            "So that means that and you will see in the."
        ],
        [
            "Graph with large probability that we have our first nearest neighbors are equal.",
            "So it means that the data do not count and as we show here."
        ],
        [
            "If you so this is tested for N, so the number of the sample size 2000 and you increase the dimension.",
            "So you see the probability that here it's for the first component doesn't matter when they are equal.",
            "If you go to larger dimension you have.",
            "Well, it goes to 100% that they are equal.",
            "So it means that you have no information.",
            "This will be 0 and you cannot say anything anymore.",
            "So that's a bad thing."
        ],
        [
            "The same boat, the opposite happens if we keep the dimension fixed here that I mentioned was 15 and you increase the sample size 2 here, try to 15,000 so.",
            "Then you see the opposite.",
            "It just goes down.",
            "So the probability that they are equal so that this will be zero will be roughly for large 18%.",
            "But here if the dimensionality is large we go to 100%, so that's not what we want, so it's perfect are let's say our estimator is good to estimate the variance, but in a test statistic we have a problem.",
            "So it's very simple now."
        ],
        [
            "We need to modify the test statistic.",
            "How can we?"
        ],
        [
            "Define well, very simply this.",
            "Couple is just this right this one and this one.",
            "So you place this couple here.",
            "Well, you don't replace.",
            "Actually if it's not zero, you don't replace anything, then it's fine, but you do replace it by convex combination here.",
            "If it is zero and with Y is just a 01 variable with probability and a half, so you don't change anything and this one here is the 2nd nearest neighbor of XI.",
            "So if you see a two, it's the second nearest neighbor, so you include an extra.",
            "Knowledge of the second nearest neighbor.",
            "It's very easy to implement.",
            "It doesn't cost much, so it goes very fast."
        ],
        [
            "So now we need to accept this hypothesis if the if the test statistic is small with and you test the modified one so."
        ],
        [
            "So what can we do?",
            "Define what a small as I told before, this is a random variable.",
            "So random variable."
        ],
        [
            "Have a limit distribution and you would say, well, this random variable.",
            "It has a it I didn't show, but I can show you later.",
            "Or if you believe me it's true.",
            "It has a first moment.",
            "It has a second moment.",
            "Well then you know the central limit theorem.",
            "Let's use it wrong.",
            "And the problem here is that this difference.",
            "Where is it?"
        ],
        [
            "Here.",
            "It's a sum of weakly dependent random variables, because here if you calculate this one, it depends on that one because you left one out.",
            "It's like sampling without replacement, so you have a bag.",
            "You put one ball out so the other one it's not IID anymore.",
            "So we introduce some kind of dependence.",
            "It's the same here, so it's not the central Limit theorem as everybody knows, so that you have dependence.",
            "The problem in there?",
            "Do you have central limit theorems for dependency?"
        ],
        [
            "Of course, but they all assume some kind of form of the dependence.",
            "We don't know the form of dependence, so can we progress or move on?",
            "Well, the key is yes, but we need.",
            "Well, as I said, we need something special for that.",
            "So and actually we look for a long thing because we needed to prove that that it's, well, you already know that it's done as an asymptotically normal, but we needed a concept and this is called exchangeable random variables or interchangeable or Inter exchangeable random variables.",
            "And it's an old concept, except it's actually from the early 30s.",
            "So, but then it was forgotten, too late 50s and then gone again.",
            "So what does it actually?",
            "It just simple random sampling.",
            "So don't read that it's not important, but it just if you have a triangular array.",
            "Just assume random variables.",
            "Right within this is an and one, so let's permute the one and then randomly.",
            "With some permutation row and then the the distribution of this equals distribution of that.",
            "When that is true, then you have exchangeable random variables.",
            "So this is just merely an extension of the ID concept.",
            "It allows for dependence.",
            "But you don't have to specify the dependence, and that is actually what we're looking for.",
            "This definition was given by.",
            "I forgot the name in Italian in 1932, so and then this is the art."
        ],
        [
            "So based on Bloom in 58 and this is ours.",
            "Our case we."
        ],
        [
            "Define the vny.",
            "Like this?",
            "Right so."
        ],
        [
            "It's basically the estimator minus the expectation of the estimated divided by the square root of the variance of the estimator blah blah blah, and then for this ugly conditions and I will immediately say what they are.",
            "It follows that this statistic.",
            "So this is our test statistic.",
            "This is the blowup factor as it called goes to asymptotic normal distribution with mean zero and variance, well, two times the minimum mean squared error and the expectation of the Y squared under H 0 so.",
            "Under the null hypothesis, again, so these ugly conditions just say that the variance needs to be.",
            "It can expand, but not too much.",
            "So that's actually the basic idea behind Central Limit Theorem.",
            "The variance has you can increase, but one additional observation cannot blow up the variance, so we need for that three ugly conditions.",
            "If you put that in there, it's very easy to prove it takes about a page, maybe a page and a half, but then we have our limit distribution, and that's what we want, so no assumption on the form.",
            "No tuning parameters, no additive model specification, nothing so."
        ],
        [
            "Like I said, the smoothness of the regression dimension does not count under HK of course."
        ],
        [
            "So.",
            "Maybe the previous theorem here many people in the audience say yeah, yeah, this is theory is not correct anyway, because it's asymptotic.",
            "When does it kick in?",
            "Maybe when you have a million data points, no?",
            "And this is what I wanted to show.",
            "So this is a simple."
        ],
        [
            "Sample here we have a simple linear model in five dimensions and only the first coefficient is zero and all the rest are one.",
            "So we have X generated uniform on the 01 to the power five.",
            "An epsilon is some just simple random noise.",
            "In this case it's normal with the variance 0.05.",
            "OK, so how can you replicate this distribution in empirically?",
            "Well, Bootstrap is one of the possible possibilities in there, so we just did it 10,000 times on this example.",
            "However, we see the blue thing here.",
            "So the histogram.",
            "In fact this is the result of the bootstrap.",
            "The red one.",
            "This is the result if you fit."
        ],
        [
            "A normal distribution with this so this you can calculate with our first estimator of the minimum mean squared error, the variance and this you can simply replace by one over end of some why I square.",
            "So you can ask."
        ],
        [
            "This is the red one, so we see it's perfectly nice and we had I here.",
            "We had 500 points, so asymptotic starts looking very fast because the limit distribution really is correct.",
            "So now we can use it very simple on our simulations.",
            "Well, they're not too big, but OK, so let's just."
        ],
        [
            "Assume that we have a Y and I didn't want to generate."
        ],
        [
            "An additive model, so this model is not additive is not linear, so if you use lasso on that one, lasso cannot detect it.",
            "If you use additive model selection, additive models won't detect it, so that's why I took this ugly thing.",
            "Um, just because we don't need to specify any model, so this is the first dimension, the 4th an we use 5 dimensions.",
            "For example.",
            "I know it's not that big, but still.",
            "So we have 1000 points we did 1000 time and the significance level of our test was 0.5% action.",
            "So the."
        ],
        [
            "Is the result?",
            "What does it mean?",
            "So we are consistent in the lasso sense.",
            "If you count this these two together.",
            "So this means how many times did I have the true subset, but the only true subset, nothing else.",
            "So in this case of a.",
            "85%.",
            "But then we have the true subset plus one component.",
            "So that means the 4th one, the first dimension, the fourth dimension plus some other.",
            "This is 50% so if the count them together, you have actually 9899% that you have the true thing here, so that's what you want.",
            "This one.",
            "I don't know what happened here.",
            "Then just selected everything, just not the true one.",
            "That is only happened once and then twice it selected all the variables.",
            "So because this was regenerated 1000 times again it was completely all random.",
            "So we it's.",
            "It's like our proof it should be consistent.",
            "Well it goes, it is.",
            "And."
        ],
        [
            "Then I included this earlier today because I thought well, then we have a theoretical example.",
            "Now everybody knows this busting housing data set or most of you, it's a very UCI clean data and so on and many people have used it, so it's like 506 points roughly and 13 dimensions, and the goal is actually to predict the median value of a house in Boston, OK?"
        ],
        [
            "So these are the explanations of the variables not important right now."
        ],
        [
            "So let's do it 500 times.",
            "So what we did was like randomly select 80% as actually training data as you would call it and 20% test LS do this.",
            "So then we see if you use.",
            "In this case we use in a least square support vector machine on the full data set the mean squared error on test.",
            "So averaged over 500 times was zero, point 22.",
            "If we do that on the reduced we have 0.21.",
            "So we see here it doesn't play a big role whether you use variable selection or not, but just.",
            "To have an idea what determines the price of a house?",
            "I think everybody can tell at least 5 six reasons.",
            "What determines the price?",
            "So the problem is here more.",
            "Can we detect those?"
        ],
        [
            "If you see this first 13 things look 1st at the blue ones, the blue ones are selected by our method.",
            "So by the hypothesis test.",
            "So that means what determines the price of your house?",
            "Well, you're in Boston, but anyway, so the crime rate.",
            "Of course I think so.",
            "If you live near to a big jail or something.",
            "I mean it decreases the value of your property.",
            "Well, nitric oxide concentration.",
            "Well if you live probably close to some big exhaust, I mean values go down if you live close to the airport, the noise and so on will property goes down still OK.",
            "The average number of rooms in your house well in the United States is very important.",
            "The number of rooms and the number of bathrooms.",
            "So this determines the price.",
            "And then also some kind of, let's say the distance to employment centers.",
            "How far are you from your work?",
            "So here they think I love and they would say well close to emic and close to the highway here.",
            "And also here the index.",
            "How close are you to the highway?",
            "Very important and the well the tax you have to pay on that.",
            "People don't want to spend too much money.",
            "If the tax rate goes up so.",
            "It is clear and also the percentage of lower status of the population.",
            "So if if you have a neighborhood full of criminals, let's say, well, the property goes down.",
            "The red one here.",
            "The two red ones.",
            "Plus the blue ones are selected by an additive approach."
        ],
        [
            "Another is here, so this is an additive approach and then you see here everything what is not a straight line.",
            "Roughly here is not selected, so this one is selected.",
            "This one is selected.",
            "This is important so but then you see actually the most important thing is the crime rate.",
            "An lower income people an I forgot #6."
        ],
        [
            "#6 so the average rooms you have."
        ],
        [
            "Stop."
        ],
        [
            "We have a can."
        ],
        [
            "And so we presented the feature selection method.",
            "Very easy to implement.",
            "It doesn't cost much.",
            "It takes like less than a second to do all this.",
            "The results are easy to interpret.",
            "You have a solid theoretical basis and it's a Model 3 methodology.",
            "The only."
        ],
        [
            "Disadvantage is that we don't have a theoretical reason.",
            "Well, we do have.",
            "It says it doesn't work.",
            "That if he grows faster than him, that's the thing, because even less so cannot do it.",
            "But you have to assume extra regulation, adaptivity, linear, Dan, yes, but here we cannot do, because we don't have a model.",
            "So."
        ],
        [
            "I would like to thank you for your attention.",
            "If there are any questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon everybody.",
                    "label": 0
                },
                {
                    "sent": "So as you already said, this is joint work with lots of deer free from hungry so he is not here today.",
                    "label": 0
                },
                {
                    "sent": "But I will try to present it as good as possible.",
                    "label": 0
                },
                {
                    "sent": "So today we're going to talk about feature selection via detecting ineffective features.",
                    "label": 1
                },
                {
                    "sent": "So let's talk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With the outline of this talk.",
                    "label": 0
                },
                {
                    "sent": "So first we're going to discuss the problem.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think everybody knows what feature selection or variable selection is well, and then I just give some applications and some methods that are used to do that.",
                    "label": 0
                },
                {
                    "sent": "Then we go to our proposed methodology first.",
                    "label": 1
                },
                {
                    "sent": "The theoretical aspects, the rate of convergence, and so on.",
                    "label": 0
                },
                {
                    "sent": "Universal consistency, and then we get formulation of the hypothesis test, which is actually the final result.",
                    "label": 1
                },
                {
                    "sent": "And to end we'll just have some simulations should be simulations.",
                    "label": 0
                },
                {
                    "sent": "There's more than one, and there is a conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start with the first part introduction.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So feature selection is something like you see here.",
                    "label": 1
                },
                {
                    "sent": "So suppose behind or a, there is a nice nice fresh cold beer in this warm temperature today and here there is nothing and you just want to know.",
                    "label": 0
                },
                {
                    "sent": "And you just want to know.",
                    "label": 0
                },
                {
                    "sent": "Where is the beard, but you don't have any prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "Right, so of course you would choose a if you had prior knowledge, but you don't have to feature selection or something like that, so you don't have any prior knowledge in our case.",
                    "label": 0
                },
                {
                    "sent": "So what are the good features?",
                    "label": 0
                },
                {
                    "sent": "Are the good things in this case we need to determine door a.",
                    "label": 0
                },
                {
                    "sent": "So here there are only two features, let's say, but normally you have like D features, so it's complicated a lot.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So application so everybody knows this is a bad picture of a microarray.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the important thing is like finding important genes.",
                    "label": 1
                },
                {
                    "sent": "They're ranked like this, right?",
                    "label": 0
                },
                {
                    "sent": "And methods to.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do that are as before, a lasso or lasu you have come here which is used.",
                    "label": 0
                },
                {
                    "sent": "You have a T test or variable selection.",
                    "label": 0
                },
                {
                    "sent": "Via additive models you have large, you have many others and you see this is quite recently, but it has to be through additive models.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start.",
                    "label": 0
                },
                {
                    "sent": "So we have our data first here.",
                    "label": 0
                },
                {
                    "sent": "XNNYN pairs XY can be a Vardy.",
                    "label": 0
                },
                {
                    "sent": "An why Kim is an element of art, so they are iid.",
                    "label": 0
                },
                {
                    "sent": "This is important and the model that we specify is just a linear model, as in lasso, linear model with an error term.",
                    "label": 0
                },
                {
                    "sent": "The epsilon here, and the epsilons of course are IID and independent of the XI.",
                    "label": 1
                },
                {
                    "sent": "The conditional mean of the errors are zero.",
                    "label": 0
                },
                {
                    "sent": "This is the assumption of law.",
                    "label": 0
                },
                {
                    "sent": "So so and then in standard case like before the previous talk already discussed that this is a standard lasso, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm not will not dwell on this further.",
                    "label": 0
                },
                {
                    "sent": "So everybody knows this.",
                    "label": 0
                },
                {
                    "sent": "This is a matrix notation.",
                    "label": 0
                },
                {
                    "sent": "We need to minimize this and this is the penalty term and here this is a tuning parameter Lambda.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so as you see here I put this in red because it's a very important parameter if you don't tune it very carefully then you might up with wrong results.",
                    "label": 0
                },
                {
                    "sent": "So what do we need to do there?",
                    "label": 0
                },
                {
                    "sent": "Is there are two different aspects in law, so the first thing is what do you want to prove it?",
                    "label": 0
                },
                {
                    "sent": "When you work with law?",
                    "label": 0
                },
                {
                    "sent": "So do you want to use it for prediction too?",
                    "label": 0
                },
                {
                    "sent": "That means standard regression.",
                    "label": 0
                },
                {
                    "sent": "Let's say to predict and you point.",
                    "label": 0
                },
                {
                    "sent": "Or do you want to use it for example for feature selection or variable selection?",
                    "label": 0
                },
                {
                    "sent": "There are two different tasks, so the first thing standard regression.",
                    "label": 0
                },
                {
                    "sent": "So what do we need consistency that means that the tuning parameter has to be of disorder, so D is the dimension of the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem and Art is a sample size, so it has to be of this order.",
                    "label": 0
                },
                {
                    "sent": "Also the L1 norm of your coefficients have to be of this of small oh.",
                    "label": 0
                },
                {
                    "sent": "And then then we can prove.",
                    "label": 0
                },
                {
                    "sent": "See here Green Steiner it off that the that the lasso is consistency in prediction as one goes to Infinity that is 1.",
                    "label": 0
                },
                {
                    "sent": "Now it totally changes if we want to use the law.",
                    "label": 0
                },
                {
                    "sent": "So for example for variable selection.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because what's happening now?",
                    "label": 0
                },
                {
                    "sent": "Suppose we define a set as hat that detects.",
                    "label": 0
                },
                {
                    "sent": "All our estimated coefficients, beta which are non 0 right.",
                    "label": 0
                },
                {
                    "sent": "Then you have here what is called a battermann condition.",
                    "label": 0
                },
                {
                    "sent": "That means that it.",
                    "label": 0
                },
                {
                    "sent": "Always detects the variables the betas, but they have to be larger than the cardinality of us, so the true set here, not the estimated set.",
                    "label": 0
                },
                {
                    "sent": "And also depending on the logarithm of Z&M and also here.",
                    "label": 0
                },
                {
                    "sent": "This is the important case the Lambda has to be of bigger order than the one for regression.",
                    "label": 0
                },
                {
                    "sent": "But the huge problem is here.",
                    "label": 0
                },
                {
                    "sent": "I can tune the Lambda for example for in with cross validation.",
                    "label": 0
                },
                {
                    "sent": "No problem here.",
                    "label": 0
                },
                {
                    "sent": "If I tune the Lambda with cross validation I end up with this order and not this one.",
                    "label": 0
                },
                {
                    "sent": "So Whoops, we have a problem.",
                    "label": 0
                },
                {
                    "sent": "So under these conditions we can prove this is consistency.",
                    "label": 0
                },
                {
                    "sent": "That means that our estimated subset will be exactly the true subset in probability and would go to one's D is larger than an N goes to Infinity, so D will go to Infinity faster.",
                    "label": 0
                },
                {
                    "sent": "Man, no problem.",
                    "label": 0
                },
                {
                    "sent": "Huge problem this so you cannot do that with just simply akaiki by C.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For cross validation, so there's there is something going on which is not that nice.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like I said, we have no.",
                    "label": 0
                },
                {
                    "sent": "There are no theoretical results that say if you use be icy cross validation and so on that variable selection is consistent with law so.",
                    "label": 1
                },
                {
                    "sent": "So if you use standard TV then lasso select more variables than needed.",
                    "label": 0
                },
                {
                    "sent": "Well that means you will have your good subset plus some extras, so you will always have more.",
                    "label": 0
                },
                {
                    "sent": "Another thing what you can do is for example choose is very close to 0 and you follow the regularization part of the Lars Till the end that you can do again.",
                    "label": 1
                },
                {
                    "sent": "No theoretical justification to do that, but it's a possibility.",
                    "label": 1
                },
                {
                    "sent": "So there are other ways to tune that.",
                    "label": 0
                },
                {
                    "sent": "It's discussed in the book of Billman and Vonda here.",
                    "label": 0
                },
                {
                    "sent": "It's stability selec.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Action or hypothesis testing?",
                    "label": 0
                },
                {
                    "sent": "So we're not going to dwell on that further, but just what can we?",
                    "label": 0
                },
                {
                    "sent": "What can we actually say about that?",
                    "label": 0
                },
                {
                    "sent": "So we only have this at our disposal.",
                    "label": 0
                },
                {
                    "sent": "We don't have anything else.",
                    "label": 0
                },
                {
                    "sent": "So what can we say?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, we have to be.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Happy with a less strong result.",
                    "label": 0
                },
                {
                    "sent": "What's the less strong result instead of actually defining an asset, we will define as relevant, set the relevant set.",
                    "label": 0
                },
                {
                    "sent": "Is that the true coefficients of beta in absolute value are larger than some C where C is between zero and Infinity.",
                    "label": 0
                },
                {
                    "sent": "Note here it's not equal to 0 as we want to, but that's the theoretical part here and then we can prove very.",
                    "label": 0
                },
                {
                    "sent": "It's not so difficult that our estimated that the relevant set here.",
                    "label": 0
                },
                {
                    "sent": "Is a subset of the estimated one and it goes in probability to one friend to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Is it bad?",
                    "label": 0
                },
                {
                    "sent": "No, that's also something that we need, and this we can use.",
                    "label": 0
                },
                {
                    "sent": "Actually, this is just some theoretical theoretical thing here that the CN has to be some larger order than.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Again, it depends on the cardinality of the true subset, which you don't know.",
                    "label": 0
                },
                {
                    "sent": "So how the case here?",
                    "label": 0
                },
                {
                    "sent": "Again, how do I choose the C?",
                    "label": 0
                },
                {
                    "sent": "Well, in practice you can't.",
                    "label": 0
                },
                {
                    "sent": "Again, the longer end can be of this order, no problem.",
                    "label": 1
                },
                {
                    "sent": "But then with high probability we go immediately to this one.",
                    "label": 1
                },
                {
                    "sent": "The Lambda selected by cross validation here.",
                    "label": 0
                },
                {
                    "sent": "Well, that will be here.",
                    "label": 0
                },
                {
                    "sent": "We will have a relevant subset of that of our estimated ones.",
                    "label": 0
                },
                {
                    "sent": "Are we happy with that?",
                    "label": 0
                },
                {
                    "sent": "We have two, so there is nothing else to do.",
                    "label": 0
                },
                {
                    "sent": "We just have to be happy with it.",
                    "label": 0
                },
                {
                    "sent": "Now all this consistency results and all that is derived under the assumption that a linear model is present.",
                    "label": 0
                },
                {
                    "sent": "That is very important.",
                    "label": 0
                },
                {
                    "sent": "So it means before you start with law.",
                    "label": 0
                },
                {
                    "sent": "So you just have to think is my total data set.",
                    "label": 0
                },
                {
                    "sent": "Is it linear?",
                    "label": 0
                },
                {
                    "sent": "Does it fit a linear model?",
                    "label": 0
                },
                {
                    "sent": "If not, the lasso will give wrong results.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's give a summary of Russell, the pros and the cons.",
                    "label": 0
                },
                {
                    "sent": "Well, it's very easy to implement, right?",
                    "label": 1
                },
                {
                    "sent": "There are fast algorithms available in model up and R. Also, the results are very easy.",
                    "label": 1
                },
                {
                    "sent": "The beta is 0 or it's not zero and you have a solid theoretical foundation.",
                    "label": 1
                },
                {
                    "sent": "If the model is correct.",
                    "label": 0
                },
                {
                    "sent": "If the model is not correct, then you don't have any theoretical, so there you can have a worst case, but the worst case it selects oh so.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So the actually the drawbacks of law.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So is that model based?",
                    "label": 0
                },
                {
                    "sent": "You have to agree with the linear form.",
                    "label": 0
                },
                {
                    "sent": "Are not the model restriction is not always necessarily met, unfortunately, and you cannot.",
                    "label": 1
                },
                {
                    "sent": "It's not always easy to check.",
                    "label": 0
                },
                {
                    "sent": "So also no theoretical results if the model assumption does not hold.",
                    "label": 1
                },
                {
                    "sent": "So what do we want to do in our thing in our methodology?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, we want to use this.",
                    "label": 0
                },
                {
                    "sent": "Of course we want to keep it, but we want to make reductions on these ones.",
                    "label": 0
                },
                {
                    "sent": "So you will see at the end this will turn green, but unfortunately they were turn 1 red and we're still busy on that.",
                    "label": 0
                },
                {
                    "sent": "So what is our?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Proposed methodology well, the first theoretical aspects.",
                    "label": 1
                },
                {
                    "sent": "Let's just consider it we have a Y again the same as element of R and the axis.",
                    "label": 0
                },
                {
                    "sent": "So the Bolt X is our our matrix and are elements of our D. Alright, so this here is the minimum mean squared error as defined in the regression case standard regression.",
                    "label": 0
                },
                {
                    "sent": "So I put it here.",
                    "label": 0
                },
                {
                    "sent": "You can work this square out in the IID case.",
                    "label": 0
                },
                {
                    "sent": "Of course an you will have this.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at that, this is easy to calculate, right?",
                    "label": 0
                },
                {
                    "sent": "You can just use the strong law of large numbers.",
                    "label": 0
                },
                {
                    "sent": "You have that this is a little bit more difficult.",
                    "label": 0
                },
                {
                    "sent": "You cannot simply replace that by one over end of some because you don't have access to any of em of ex.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "We propose our estimator L star N. So all in this is with N. Here are estimators.",
                    "label": 0
                },
                {
                    "sent": "So we propose here the strong law of large numbers, this one, and then we propose to estimate this second term by 1 / N The why and the Y I-1 is actually the value of its nearest neighbor of XI from the subset where XI is deleted.",
                    "label": 1
                },
                {
                    "sent": "That's our estimator, actually of the variance.",
                    "label": 0
                },
                {
                    "sent": "This works in D dimensions.",
                    "label": 0
                },
                {
                    "sent": "It works in one dimension of course, but it works in D dimensions too.",
                    "label": 0
                },
                {
                    "sent": "So this is very easy to implement, you don't need.",
                    "label": 0
                },
                {
                    "sent": "There is no model assumption anything.",
                    "label": 0
                },
                {
                    "sent": "All there is no modeler restriction, there is no parametric form.",
                    "label": 0
                },
                {
                    "sent": "You don't need to know anything.",
                    "label": 0
                },
                {
                    "sent": "Just calculate this is very easy, just a first nearest neighbor.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our theoretical aspects.",
                    "label": 0
                },
                {
                    "sent": "The first thing that an estimator should have it's it should be consistent.",
                    "label": 0
                },
                {
                    "sent": "If it's not, if you use an estimator that is not consistent, then well, you know you don't know what your estimated.",
                    "label": 0
                },
                {
                    "sent": "So under the assumption that we have bounded Y, and I know in some theorists can be very angry with that because you exclude the Gaussian case.",
                    "label": 0
                },
                {
                    "sent": "If you do that, you don't have bounded.",
                    "label": 0
                },
                {
                    "sent": "Why in theory it's unbounded?",
                    "label": 0
                },
                {
                    "sent": "But if you assume boundary Y?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then our estimator will go to the true one in an almost sure sense.",
                    "label": 0
                },
                {
                    "sent": "And because, yeah, some people don't like bounded why?",
                    "label": 0
                },
                {
                    "sent": "Especially for those we have this condition of a second bounded moment which is.",
                    "label": 0
                },
                {
                    "sent": "Less restrictive and then we have a universally consistent result in almost sure sense.",
                    "label": 0
                },
                {
                    "sent": "So it means that our estimator if you if you use it, it estimates the correct quantity.",
                    "label": 0
                },
                {
                    "sent": "Of course, for N going to Infinity as always.",
                    "label": 0
                },
                {
                    "sent": "And then what you always need to do or what is interesting to do is yeah, I can estimate might ruin, but how fast does it converge to the true one called the rate of convergence?",
                    "label": 0
                },
                {
                    "sent": "Well in this case we need we need to assume abounded X.",
                    "label": 0
                },
                {
                    "sent": "So let's say the norm of X and the absolute value of Y is bounded and the regression function is Lipschitz continuous.",
                    "label": 1
                },
                {
                    "sent": "That's the only thing.",
                    "label": 0
                },
                {
                    "sent": "And then you have to assume in the nearest neighbour terminology that ties do not occur at all.",
                    "label": 0
                },
                {
                    "sent": "So with probability zero, they occur.",
                    "label": 0
                },
                {
                    "sent": "So that means that you have the same.",
                    "label": 0
                },
                {
                    "sent": "You select the same nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "This is true for on going to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Then in case for D and I will explain immediately why larger or equal than two.",
                    "label": 0
                },
                {
                    "sent": "This is RL one rate of convergence, so C1 and C2 are just constants, not depending on N&D.",
                    "label": 0
                },
                {
                    "sent": "And this is the result in case of D1 this drops.",
                    "label": 0
                },
                {
                    "sent": "So you cannot fill in here D1?",
                    "label": 0
                },
                {
                    "sent": "OK, the derivation is slightly different, so it drops and then you have the so called root N consistency, which are which is the best non parameter create you can have.",
                    "label": 0
                },
                {
                    "sent": "So further in this case, if N goes to Infinity much faster than D. We have consistency.",
                    "label": 0
                },
                {
                    "sent": "In almost sure sense, forgot that here.",
                    "label": 0
                },
                {
                    "sent": "Of course, if D goes faster to Infinity then N if you fill it in here.",
                    "label": 0
                },
                {
                    "sent": "We have a problem.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work.",
                    "label": 0
                },
                {
                    "sent": "So that's again.",
                    "label": 0
                },
                {
                    "sent": "I think if P is very very large and any small, our method does not work statistically.",
                    "label": 0
                },
                {
                    "sent": "You can calculate it.",
                    "label": 0
                },
                {
                    "sent": "Of course it's not difficult, but the results you cannot trust.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So how can we detect these features?",
                    "label": 0
                },
                {
                    "sent": "Well, very simply, for example.",
                    "label": 0
                },
                {
                    "sent": "Leave the Kate feature in your matrix.",
                    "label": 0
                },
                {
                    "sent": "Take it out.",
                    "label": 0
                },
                {
                    "sent": "That's what it's saying here.",
                    "label": 0
                },
                {
                    "sent": "Take that one out.",
                    "label": 0
                },
                {
                    "sent": "And then our corresponding minimum mean squared error we denoted by L star minus K is simply given by this.",
                    "label": 0
                },
                {
                    "sent": "This is the regression function with the Kate feature out.",
                    "label": 0
                },
                {
                    "sent": "Right in L2 cents.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So what do we need to do is now, well calculate or try to calculate this quantity.",
                    "label": 0
                },
                {
                    "sent": "And calculate the quantity where the key is put back in into the matrix, so the full one in this case so and then you just see are they equal?",
                    "label": 0
                },
                {
                    "sent": "Or are they?",
                    "label": 0
                },
                {
                    "sent": "Not?",
                    "label": 0
                },
                {
                    "sent": "Well, they're not never going to be exactly equal, but you make the difference between these two.",
                    "label": 0
                },
                {
                    "sent": "That's the idea behind it.",
                    "label": 0
                },
                {
                    "sent": "You make the difference and then you have some quantity and then you can if you have the limit distribution.",
                    "label": 0
                },
                {
                    "sent": "For example of this difference, then you can assess whether that quantity is relevant or not, and that's exactly what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "So in this case, if they're equal, means that the leaving, leaving out decayed component doesn't increase the minimum mean squared error.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hence, it's not important to leave it out.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the thing.",
                    "label": 0
                },
                {
                    "sent": "So an under the null hypothesis, so we use it HK to just.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To say the Kate variable is put out, but this is the null, very important.",
                    "label": 0
                },
                {
                    "sent": "Then we have the regression function.",
                    "label": 0
                },
                {
                    "sent": "This is by definition in L2, right?",
                    "label": 0
                },
                {
                    "sent": "The conditional mean.",
                    "label": 0
                },
                {
                    "sent": "Is equal because we assume this is the note is equal on the conditional mean, but with the Kate feature left out, meaning that the regression function, the original one is the same as the regression function with the K1 out in normal almost sure sense.",
                    "label": 0
                },
                {
                    "sent": "So that means that you do not estimator regression function.",
                    "label": 0
                },
                {
                    "sent": "They are equal and this is very important.",
                    "label": 0
                },
                {
                    "sent": "It means that in our theoretical derivations you do not need to assume further smoothness on the regression function and the dimensionality does not count.",
                    "label": 0
                },
                {
                    "sent": "Under this one, the null hypothesis, not the alternative.",
                    "label": 0
                },
                {
                    "sent": "Then it does come, but under the note.",
                    "label": 0
                },
                {
                    "sent": "But exactly, you never know when you're exactly under the note.",
                    "label": 0
                },
                {
                    "sent": "The dimensionality and the smoothness of the regression function does not count, so you have it.",
                    "label": 0
                },
                {
                    "sent": "You can have a discontinuous function no problem.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's denote the data without the Kate component as this.",
                    "label": 0
                },
                {
                    "sent": "So we just leave the K component out.",
                    "label": 0
                },
                {
                    "sent": "This is the N -- K.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We estimate the minimum variance by the cake out by using this form.",
                    "label": 0
                },
                {
                    "sent": "Right, it's the same as before, but now it's just the nearest neighbor, just with the Kate component out.",
                    "label": 0
                },
                {
                    "sent": "Our test statistic is as follows, calculate R.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimator of this one right, minus the estimate of the of the full data set and then you reduce it to this.",
                    "label": 0
                },
                {
                    "sent": "So what we need is our data.",
                    "label": 0
                },
                {
                    "sent": "This we have and just the first nearest neighbor with the cat component out.",
                    "label": 0
                },
                {
                    "sent": "You accept the hypothesis if this difference is small, but like I said before, define small.",
                    "label": 0
                },
                {
                    "sent": "So we need actually the limit distribution of this guy.",
                    "label": 0
                },
                {
                    "sent": "This is a random variable, so you can.",
                    "label": 0
                },
                {
                    "sent": "It has a limit distribution.",
                    "label": 0
                },
                {
                    "sent": "The problem is now find it well, OK, here it goes.",
                    "label": 0
                },
                {
                    "sent": "So there is another problem that we need to assess.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this part is small even when the null hypothesis is true.",
                    "label": 0
                },
                {
                    "sent": "Very rare, but I mean very annoying.",
                    "label": 0
                },
                {
                    "sent": "But it's true.",
                    "label": 0
                },
                {
                    "sent": "So why?",
                    "label": 0
                },
                {
                    "sent": "Because this the why I the first nearest neighbor of the full is equal to the first nearest neighbor with Kate component out.",
                    "label": 0
                },
                {
                    "sent": "So that means that and you will see in the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graph with large probability that we have our first nearest neighbors are equal.",
                    "label": 0
                },
                {
                    "sent": "So it means that the data do not count and as we show here.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you so this is tested for N, so the number of the sample size 2000 and you increase the dimension.",
                    "label": 0
                },
                {
                    "sent": "So you see the probability that here it's for the first component doesn't matter when they are equal.",
                    "label": 0
                },
                {
                    "sent": "If you go to larger dimension you have.",
                    "label": 0
                },
                {
                    "sent": "Well, it goes to 100% that they are equal.",
                    "label": 0
                },
                {
                    "sent": "So it means that you have no information.",
                    "label": 0
                },
                {
                    "sent": "This will be 0 and you cannot say anything anymore.",
                    "label": 0
                },
                {
                    "sent": "So that's a bad thing.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same boat, the opposite happens if we keep the dimension fixed here that I mentioned was 15 and you increase the sample size 2 here, try to 15,000 so.",
                    "label": 0
                },
                {
                    "sent": "Then you see the opposite.",
                    "label": 0
                },
                {
                    "sent": "It just goes down.",
                    "label": 0
                },
                {
                    "sent": "So the probability that they are equal so that this will be zero will be roughly for large 18%.",
                    "label": 0
                },
                {
                    "sent": "But here if the dimensionality is large we go to 100%, so that's not what we want, so it's perfect are let's say our estimator is good to estimate the variance, but in a test statistic we have a problem.",
                    "label": 0
                },
                {
                    "sent": "So it's very simple now.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We need to modify the test statistic.",
                    "label": 0
                },
                {
                    "sent": "How can we?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Define well, very simply this.",
                    "label": 0
                },
                {
                    "sent": "Couple is just this right this one and this one.",
                    "label": 0
                },
                {
                    "sent": "So you place this couple here.",
                    "label": 0
                },
                {
                    "sent": "Well, you don't replace.",
                    "label": 0
                },
                {
                    "sent": "Actually if it's not zero, you don't replace anything, then it's fine, but you do replace it by convex combination here.",
                    "label": 0
                },
                {
                    "sent": "If it is zero and with Y is just a 01 variable with probability and a half, so you don't change anything and this one here is the 2nd nearest neighbor of XI.",
                    "label": 0
                },
                {
                    "sent": "So if you see a two, it's the second nearest neighbor, so you include an extra.",
                    "label": 0
                },
                {
                    "sent": "Knowledge of the second nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to implement.",
                    "label": 0
                },
                {
                    "sent": "It doesn't cost much, so it goes very fast.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we need to accept this hypothesis if the if the test statistic is small with and you test the modified one so.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what can we do?",
                    "label": 0
                },
                {
                    "sent": "Define what a small as I told before, this is a random variable.",
                    "label": 0
                },
                {
                    "sent": "So random variable.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have a limit distribution and you would say, well, this random variable.",
                    "label": 0
                },
                {
                    "sent": "It has a it I didn't show, but I can show you later.",
                    "label": 0
                },
                {
                    "sent": "Or if you believe me it's true.",
                    "label": 0
                },
                {
                    "sent": "It has a first moment.",
                    "label": 0
                },
                {
                    "sent": "It has a second moment.",
                    "label": 0
                },
                {
                    "sent": "Well then you know the central limit theorem.",
                    "label": 0
                },
                {
                    "sent": "Let's use it wrong.",
                    "label": 0
                },
                {
                    "sent": "And the problem here is that this difference.",
                    "label": 0
                },
                {
                    "sent": "Where is it?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "It's a sum of weakly dependent random variables, because here if you calculate this one, it depends on that one because you left one out.",
                    "label": 0
                },
                {
                    "sent": "It's like sampling without replacement, so you have a bag.",
                    "label": 0
                },
                {
                    "sent": "You put one ball out so the other one it's not IID anymore.",
                    "label": 0
                },
                {
                    "sent": "So we introduce some kind of dependence.",
                    "label": 0
                },
                {
                    "sent": "It's the same here, so it's not the central Limit theorem as everybody knows, so that you have dependence.",
                    "label": 0
                },
                {
                    "sent": "The problem in there?",
                    "label": 0
                },
                {
                    "sent": "Do you have central limit theorems for dependency?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, but they all assume some kind of form of the dependence.",
                    "label": 0
                },
                {
                    "sent": "We don't know the form of dependence, so can we progress or move on?",
                    "label": 0
                },
                {
                    "sent": "Well, the key is yes, but we need.",
                    "label": 0
                },
                {
                    "sent": "Well, as I said, we need something special for that.",
                    "label": 0
                },
                {
                    "sent": "So and actually we look for a long thing because we needed to prove that that it's, well, you already know that it's done as an asymptotically normal, but we needed a concept and this is called exchangeable random variables or interchangeable or Inter exchangeable random variables.",
                    "label": 0
                },
                {
                    "sent": "And it's an old concept, except it's actually from the early 30s.",
                    "label": 0
                },
                {
                    "sent": "So, but then it was forgotten, too late 50s and then gone again.",
                    "label": 0
                },
                {
                    "sent": "So what does it actually?",
                    "label": 0
                },
                {
                    "sent": "It just simple random sampling.",
                    "label": 0
                },
                {
                    "sent": "So don't read that it's not important, but it just if you have a triangular array.",
                    "label": 0
                },
                {
                    "sent": "Just assume random variables.",
                    "label": 0
                },
                {
                    "sent": "Right within this is an and one, so let's permute the one and then randomly.",
                    "label": 0
                },
                {
                    "sent": "With some permutation row and then the the distribution of this equals distribution of that.",
                    "label": 0
                },
                {
                    "sent": "When that is true, then you have exchangeable random variables.",
                    "label": 0
                },
                {
                    "sent": "So this is just merely an extension of the ID concept.",
                    "label": 0
                },
                {
                    "sent": "It allows for dependence.",
                    "label": 0
                },
                {
                    "sent": "But you don't have to specify the dependence, and that is actually what we're looking for.",
                    "label": 0
                },
                {
                    "sent": "This definition was given by.",
                    "label": 0
                },
                {
                    "sent": "I forgot the name in Italian in 1932, so and then this is the art.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So based on Bloom in 58 and this is ours.",
                    "label": 0
                },
                {
                    "sent": "Our case we.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Define the vny.",
                    "label": 0
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's basically the estimator minus the expectation of the estimated divided by the square root of the variance of the estimator blah blah blah, and then for this ugly conditions and I will immediately say what they are.",
                    "label": 0
                },
                {
                    "sent": "It follows that this statistic.",
                    "label": 0
                },
                {
                    "sent": "So this is our test statistic.",
                    "label": 0
                },
                {
                    "sent": "This is the blowup factor as it called goes to asymptotic normal distribution with mean zero and variance, well, two times the minimum mean squared error and the expectation of the Y squared under H 0 so.",
                    "label": 0
                },
                {
                    "sent": "Under the null hypothesis, again, so these ugly conditions just say that the variance needs to be.",
                    "label": 0
                },
                {
                    "sent": "It can expand, but not too much.",
                    "label": 0
                },
                {
                    "sent": "So that's actually the basic idea behind Central Limit Theorem.",
                    "label": 0
                },
                {
                    "sent": "The variance has you can increase, but one additional observation cannot blow up the variance, so we need for that three ugly conditions.",
                    "label": 0
                },
                {
                    "sent": "If you put that in there, it's very easy to prove it takes about a page, maybe a page and a half, but then we have our limit distribution, and that's what we want, so no assumption on the form.",
                    "label": 0
                },
                {
                    "sent": "No tuning parameters, no additive model specification, nothing so.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like I said, the smoothness of the regression dimension does not count under HK of course.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Maybe the previous theorem here many people in the audience say yeah, yeah, this is theory is not correct anyway, because it's asymptotic.",
                    "label": 0
                },
                {
                    "sent": "When does it kick in?",
                    "label": 0
                },
                {
                    "sent": "Maybe when you have a million data points, no?",
                    "label": 0
                },
                {
                    "sent": "And this is what I wanted to show.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sample here we have a simple linear model in five dimensions and only the first coefficient is zero and all the rest are one.",
                    "label": 0
                },
                {
                    "sent": "So we have X generated uniform on the 01 to the power five.",
                    "label": 1
                },
                {
                    "sent": "An epsilon is some just simple random noise.",
                    "label": 0
                },
                {
                    "sent": "In this case it's normal with the variance 0.05.",
                    "label": 0
                },
                {
                    "sent": "OK, so how can you replicate this distribution in empirically?",
                    "label": 0
                },
                {
                    "sent": "Well, Bootstrap is one of the possible possibilities in there, so we just did it 10,000 times on this example.",
                    "label": 0
                },
                {
                    "sent": "However, we see the blue thing here.",
                    "label": 0
                },
                {
                    "sent": "So the histogram.",
                    "label": 0
                },
                {
                    "sent": "In fact this is the result of the bootstrap.",
                    "label": 1
                },
                {
                    "sent": "The red one.",
                    "label": 0
                },
                {
                    "sent": "This is the result if you fit.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A normal distribution with this so this you can calculate with our first estimator of the minimum mean squared error, the variance and this you can simply replace by one over end of some why I square.",
                    "label": 0
                },
                {
                    "sent": "So you can ask.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the red one, so we see it's perfectly nice and we had I here.",
                    "label": 0
                },
                {
                    "sent": "We had 500 points, so asymptotic starts looking very fast because the limit distribution really is correct.",
                    "label": 0
                },
                {
                    "sent": "So now we can use it very simple on our simulations.",
                    "label": 0
                },
                {
                    "sent": "Well, they're not too big, but OK, so let's just.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assume that we have a Y and I didn't want to generate.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An additive model, so this model is not additive is not linear, so if you use lasso on that one, lasso cannot detect it.",
                    "label": 0
                },
                {
                    "sent": "If you use additive model selection, additive models won't detect it, so that's why I took this ugly thing.",
                    "label": 0
                },
                {
                    "sent": "Um, just because we don't need to specify any model, so this is the first dimension, the 4th an we use 5 dimensions.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "I know it's not that big, but still.",
                    "label": 0
                },
                {
                    "sent": "So we have 1000 points we did 1000 time and the significance level of our test was 0.5% action.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the result?",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "So we are consistent in the lasso sense.",
                    "label": 0
                },
                {
                    "sent": "If you count this these two together.",
                    "label": 0
                },
                {
                    "sent": "So this means how many times did I have the true subset, but the only true subset, nothing else.",
                    "label": 0
                },
                {
                    "sent": "So in this case of a.",
                    "label": 0
                },
                {
                    "sent": "85%.",
                    "label": 0
                },
                {
                    "sent": "But then we have the true subset plus one component.",
                    "label": 0
                },
                {
                    "sent": "So that means the 4th one, the first dimension, the fourth dimension plus some other.",
                    "label": 0
                },
                {
                    "sent": "This is 50% so if the count them together, you have actually 9899% that you have the true thing here, so that's what you want.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "I don't know what happened here.",
                    "label": 0
                },
                {
                    "sent": "Then just selected everything, just not the true one.",
                    "label": 0
                },
                {
                    "sent": "That is only happened once and then twice it selected all the variables.",
                    "label": 0
                },
                {
                    "sent": "So because this was regenerated 1000 times again it was completely all random.",
                    "label": 0
                },
                {
                    "sent": "So we it's.",
                    "label": 0
                },
                {
                    "sent": "It's like our proof it should be consistent.",
                    "label": 0
                },
                {
                    "sent": "Well it goes, it is.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I included this earlier today because I thought well, then we have a theoretical example.",
                    "label": 0
                },
                {
                    "sent": "Now everybody knows this busting housing data set or most of you, it's a very UCI clean data and so on and many people have used it, so it's like 506 points roughly and 13 dimensions, and the goal is actually to predict the median value of a house in Boston, OK?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the explanations of the variables not important right now.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's do it 500 times.",
                    "label": 0
                },
                {
                    "sent": "So what we did was like randomly select 80% as actually training data as you would call it and 20% test LS do this.",
                    "label": 0
                },
                {
                    "sent": "So then we see if you use.",
                    "label": 0
                },
                {
                    "sent": "In this case we use in a least square support vector machine on the full data set the mean squared error on test.",
                    "label": 0
                },
                {
                    "sent": "So averaged over 500 times was zero, point 22.",
                    "label": 0
                },
                {
                    "sent": "If we do that on the reduced we have 0.21.",
                    "label": 0
                },
                {
                    "sent": "So we see here it doesn't play a big role whether you use variable selection or not, but just.",
                    "label": 0
                },
                {
                    "sent": "To have an idea what determines the price of a house?",
                    "label": 0
                },
                {
                    "sent": "I think everybody can tell at least 5 six reasons.",
                    "label": 0
                },
                {
                    "sent": "What determines the price?",
                    "label": 0
                },
                {
                    "sent": "So the problem is here more.",
                    "label": 0
                },
                {
                    "sent": "Can we detect those?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you see this first 13 things look 1st at the blue ones, the blue ones are selected by our method.",
                    "label": 0
                },
                {
                    "sent": "So by the hypothesis test.",
                    "label": 0
                },
                {
                    "sent": "So that means what determines the price of your house?",
                    "label": 0
                },
                {
                    "sent": "Well, you're in Boston, but anyway, so the crime rate.",
                    "label": 0
                },
                {
                    "sent": "Of course I think so.",
                    "label": 0
                },
                {
                    "sent": "If you live near to a big jail or something.",
                    "label": 0
                },
                {
                    "sent": "I mean it decreases the value of your property.",
                    "label": 0
                },
                {
                    "sent": "Well, nitric oxide concentration.",
                    "label": 0
                },
                {
                    "sent": "Well if you live probably close to some big exhaust, I mean values go down if you live close to the airport, the noise and so on will property goes down still OK.",
                    "label": 0
                },
                {
                    "sent": "The average number of rooms in your house well in the United States is very important.",
                    "label": 0
                },
                {
                    "sent": "The number of rooms and the number of bathrooms.",
                    "label": 0
                },
                {
                    "sent": "So this determines the price.",
                    "label": 0
                },
                {
                    "sent": "And then also some kind of, let's say the distance to employment centers.",
                    "label": 0
                },
                {
                    "sent": "How far are you from your work?",
                    "label": 0
                },
                {
                    "sent": "So here they think I love and they would say well close to emic and close to the highway here.",
                    "label": 0
                },
                {
                    "sent": "And also here the index.",
                    "label": 0
                },
                {
                    "sent": "How close are you to the highway?",
                    "label": 0
                },
                {
                    "sent": "Very important and the well the tax you have to pay on that.",
                    "label": 0
                },
                {
                    "sent": "People don't want to spend too much money.",
                    "label": 0
                },
                {
                    "sent": "If the tax rate goes up so.",
                    "label": 0
                },
                {
                    "sent": "It is clear and also the percentage of lower status of the population.",
                    "label": 0
                },
                {
                    "sent": "So if if you have a neighborhood full of criminals, let's say, well, the property goes down.",
                    "label": 0
                },
                {
                    "sent": "The red one here.",
                    "label": 0
                },
                {
                    "sent": "The two red ones.",
                    "label": 0
                },
                {
                    "sent": "Plus the blue ones are selected by an additive approach.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another is here, so this is an additive approach and then you see here everything what is not a straight line.",
                    "label": 0
                },
                {
                    "sent": "Roughly here is not selected, so this one is selected.",
                    "label": 0
                },
                {
                    "sent": "This one is selected.",
                    "label": 0
                },
                {
                    "sent": "This is important so but then you see actually the most important thing is the crime rate.",
                    "label": 0
                },
                {
                    "sent": "An lower income people an I forgot #6.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "#6 so the average rooms you have.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stop.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a can.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we presented the feature selection method.",
                    "label": 0
                },
                {
                    "sent": "Very easy to implement.",
                    "label": 0
                },
                {
                    "sent": "It doesn't cost much.",
                    "label": 0
                },
                {
                    "sent": "It takes like less than a second to do all this.",
                    "label": 0
                },
                {
                    "sent": "The results are easy to interpret.",
                    "label": 0
                },
                {
                    "sent": "You have a solid theoretical basis and it's a Model 3 methodology.",
                    "label": 0
                },
                {
                    "sent": "The only.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Disadvantage is that we don't have a theoretical reason.",
                    "label": 0
                },
                {
                    "sent": "Well, we do have.",
                    "label": 0
                },
                {
                    "sent": "It says it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "That if he grows faster than him, that's the thing, because even less so cannot do it.",
                    "label": 0
                },
                {
                    "sent": "But you have to assume extra regulation, adaptivity, linear, Dan, yes, but here we cannot do, because we don't have a model.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I would like to thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "If there are any questions.",
                    "label": 0
                }
            ]
        }
    }
}