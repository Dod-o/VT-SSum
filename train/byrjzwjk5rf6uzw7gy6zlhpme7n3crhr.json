{
    "id": "byrjzwjk5rf6uzw7gy6zlhpme7n3crhr",
    "title": "Factored 3-way restricted Boltzmann machines for modeling natural images",
    "info": {
        "author": [
            "Marc\u2019Aurelio Ranzato, Department of Computer Science, University of Toronto"
        ],
        "published": "June 3, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/aistats2010_ranzato_f3wr/",
    "segmentation": [
        [
            "Hi, this is joint work with Alex Kryszewski and Jeff Hinton in Toronto.",
            "In this work we are we want to model natural images by using a generative model.",
            "We have a probability distribution over images and we want to train the parameters W to give higher likelihoods to natural images.",
            "Actually, we are interested in probabilistic models as away as a mean to learn representations of data because we want to use these representations to do for example, recognition, segmentation, denoising.",
            "Over the past few years, there has been a lot of excitement and work on some hierarchical models that are called deep belief networks that can learn a feature hierarchy.",
            "But these models are currently limited to binary variables that don't work very well with continuous data.",
            "So in this work we aim at mapping images into binary features that then can be used as input to a standard.",
            "Belief network, so for simplicity.",
            "Here the input is a small image Patch, so we want to model small image patches an."
        ],
        [
            "This is denoted by V, so the input.",
            "Perhaps it's too loud, so the input is vectorized.",
            "Image Patch denoted by B and then we have a set of letting variables or hidden units that are denoted by 8 and this H is our binary.",
            "The goal is to model the joint distribution between the visibles we and hidden's age.",
            "Before we proceed, let me warn you that I won't be as formalizing the paper, and I won't derive all the formulas.",
            "And if you are interested in.",
            "In that you should refer to the paper.",
            "Here I'm trying to give you the intuition why we chose this model and how it works.",
            "And this interpretation goes back to the work by Jim Anjema in the early 80s on the line process.",
            "So it's a very different interpretation, but.",
            "So the goal."
        ],
        [
            "Is to model the joint distribution between the visibles, we and the hidden stage an we want this joint distribution to capture the properties of natural images.",
            "So what are these properties?"
        ],
        [
            "The key property is smoothness, so most of the times the value of 1 pixel can be very well predicted from its neighbors and just by looking at a bunch."
        ],
        [
            "Image patches you can see that most of them are very smooth.",
            "Quantify we can look at the statistics how natural images as measured by."
        ],
        [
            "I linear filter for example by this vertical Gabor wavelet.",
            "So if you take the inner product of this filter with many image patches, you can compute the histogram of the filter response is now here I'm showing the negative logarithm of that histogram and this is showing that most of the images give zero response to this filter.",
            "Because it is very rare to find vertical ads exactly at this position and disorientation, and this is another way of saying that natural images tend to be very smooth.",
            "But this is showing us another characteristic which is that.",
            "Naturally images so that there are outliers.",
            "So because you can see that details tend to bend, so natural signal, so sometimes violations to this smoothness property do occur."
        ],
        [
            "And the model that I'm going to talk about actually gives a very nice fit to this empirical distribution."
        ],
        [
            "And by contrast, if you were going to use the Gaussian model, if it wouldn't be so good because it is true that it gives higher likelihood to smooth images because you can see that the minimum is around 0, where that is zero OS.",
            "Bonds between this filter and in the images, but on the other hand he greatly underestimates the mass on the tails.",
            "So in other words, natural signals tend to have many more outliers that you would expect by using a Gaussian model, so."
        ],
        [
            "The key idea of this work is the following.",
            "We have hidden units that are binary and that work like a switch.",
            "So if the input images smooth, then the hidden unit is on and basically the negative log probability is like the one of a Gaussian.",
            "It's a very tight Gaussian, but as soon as the input images structure then the hidden unit detects that and switches off.",
            "It turns the lock prop into one of the uniform distribution.",
            "An of course, if you have a mixture of Gaussians, two Gaussians and with that are centered and with different variants, the result is the habitat distribution.",
            "This is basically what we have in our model with the red curve so."
        ],
        [
            "To be more formal, we want to define a joint distribution over Visibles and Hedons an we define the joint distribution in terms of an energy function.",
            "So the joint distribution is proportional to the exponential of a negative energy.",
            "And so basically we want to assign higher probability to smooth images or lower energy to smooth images to natural images an we can make the energy function depend on a measure of UN smoothness or smoothness dissatisfaction.",
            "So in this plot.",
            "Smooth images, those that have zero dissatisfaction because they're smooth, have lower energy, and as the dissatisfaction increases because the images structure, then we want to increase the energy to make them more unlikely.",
            "But we want to be robust to outliers to match the marginal statistics that we've seen before.",
            "Um, so everything depends on how we."
        ],
        [
            "We find the energy function and how we measure this smoothness, dissatisfaction and so related question is how?",
            "So what is this smoothness dissatisfaction?",
            "I'm in particular what are edges and because edges are not smooth so I remember once Jeff asked me what is a vertical ads and I said it is something like bright dark but."
        ],
        [
            "She mentioned that it could be also dark bright, or it could be the boundary between regions of different color.",
            "Different tax are different disparity and so and so forth.",
            "And the short answer was a vertical ads is just the lack of horizontal interpolation.",
            "No."
        ],
        [
            "Whether it is color interpolation, brightness interpolation, or tax interpolation?",
            "And so ideally we want to measure smoothness dissatisfaction in terms of lack of interpolation, since as I mentioned, there are many different kinds of interpolation."
        ],
        [
            "It is difficult to set it by hand, and therefore we learn it and we learn it by using Acetylene air filters.",
            "So this linear filters will end up being highly unsmooth because they want to detect.",
            "Broken smoothness in images.",
            "So as I mentioned, we measure smoothness dissatisfaction by using linear filters that are denoted by my."
        ],
        [
            "Wii.",
            "And I define."
        ],
        [
            "The energy is the sum of this squared smoothness.",
            "Dissatisfaction with the bias I square because I don't care whether the dissatisfaction is positive or negative.",
            "I just got to detect whether it is zero or non zero an this does almost what we want because this would make smooth images more likely.",
            "They have lower energy here.",
            "But we would penalize too much structured images, so the idea is to allow for violation by introducing hidden units.",
            "So now we have hidden units that multiply both the smoothness, dissatisfaction term and a bias."
        ],
        [
            "Esther and the way it works is the following.",
            "So when the emails is smooth."
        ],
        [
            "Then the hidden unit turns out to be on and so the energy is a quadratic as a function of the smoothness dissatisfaction as a function of this filter output.",
            "And so that's what you see in the lower part of this graph.",
            "But when the so if the bias is positive, you can imagine that the smoothness satisfaction is almost zero, and so the energy is negative.",
            "But when?",
            "The image is structure then and so the smoothness, smoothness, dissatisfaction is very high.",
            "Then the hidden unit turns off and so the energy becomes constant as a function of the smoothness dissatisfaction and becomes 0.",
            "So the energy increased from something negative to zero, which means that a structure image now is less likely than smooth image.",
            "But we don't pay a quadratic penalty on this structured image, so we don't make it.",
            "So unlikely so it is unlikely, but not too much.",
            "So I just read this car."
        ],
        [
            "Except through a toy example.",
            "So here I have just two hidden units and two filters.",
            "So the first filter is W one and it is detecting lack of horizontal smoothness while the second filter is detecting lack of vertical smoothness.",
            "And we have Tobias is that equal to 10?",
            "So now.",
            "Dynasty defines a joint distribution over Visibles and Hedons, and from that you can derive the conditional distributions.",
            "If you do the math, it turns out that.",
            "The conditional distribution over the Hidden's is just a inspectorial, so you can.",
            "It's hidden unit is independent from the others given the visible and the value, the probability of 1 hidden unit being on is just the logistic of the negative square filter output plus the bias.",
            "Now since images are smooth and if you remember the histogram.",
            "We heard that the square filter output is most of the time zero, and so the hidden unit is most of the time on and so I drew this little histogram on the side to indicate that.",
            "Another thing to observe is that the energy function as a function.",
            "So for a given set of hidden units, it defines a quadratic over the visible.",
            "So basically for any given configuration, the hidden units specifies a set of pairwise constraint pairwise.",
            "Relations between the visible units or in other words, any given configuration.",
            "The hidden units specify an image specific covariance matrix.",
            "A Gaussian with certain covariance matrix.",
            "So let's read about the formula this is."
        ],
        [
            "The example, so if the input image is smooth, then both hidden units are on.",
            "And you can find that by using the formula that I showed before, and so it's one, it's two are equal to 1.",
            "But now the 1st and 2nd square filter outputs are basically zero, and so the energy function is approximately just the negative.",
            "Some of the biases, which is minus 20 because you got 1 * 0 + 1 * 0 and minus B 1 * 1 -- 2 * 1.",
            "So it is plus 20.",
            "Now for the given.",
            "For this given hidden units, we have that the energy defines also a quadratic function over the filter outputs.",
            "And since H1 is equal to one, H2 is equal to 1.",
            "Then you have that this condition distribution is a spherical Gaussian.",
            "So this is what you see on your right hand side and if you draw samples from this distribution you will get smooth patches like the ones that I show below.",
            "But if the improvement is not smooth."
        ],
        [
            "For example, is a line along the first filter then.",
            "The first hidden unit has overwhelming probability to be 0, and so that lot is showing that the probability of being one is very small and therefore the energy function is now something like basically zero times a very large number.",
            "Plus 1 * 0 and minus B1 time 0 -- B two, so the energy is approximately equal to minus P2, so before it was it was minus 20.",
            "Now it is minus 10, which is saying that this structure demands is less likely than the smoothie mix that we saw before.",
            "However, by setting each one equal to zero, we got a very good discount because we don't pay the penalty given by their first square filter output.",
            "That could be very large, so this image is less likely than the smooth image, but it's not so unlikely.",
            "Now if we look at the conditional distribution over the visible.",
            "So now the energy function defines an inverse covariance matrix for for a given set of hidden units, you got the quadratic over the visibles right?",
            "And so this defines an inverse covariance matrix an.",
            "As a function of the filter output, so you got that when you invert this thing, you got a very large.",
            "You got the negative vector with a very large again value in the direction of the first filter output, because each one is very small.",
            "So when you invert it, that becomes very large and so now about this is a bit counter intuitive.",
            "So when you detect that there is an edge, for example this vertical edge, it's not that the distribution becomes more confident that and, so it's not that the Gaussian distribution becomes narrower, but the model becomes more uncertain in that direction.",
            "And so if you sample, you got a samples that could look like the ones in the bottom that all lack to some extent horizontal interpolation.",
            "So this is another words.",
            "There are many.",
            "There is a whole subspace of images that all lack horizontal interpolation that are mapped into this hidden configuration."
        ],
        [
            "Since the hidden units are binary, we can marginalized them out an.",
            "So here I'm showing the negative logarithm or the marginal distribution for I'm showing the contribution of 1 hidden unit an.",
            "Basically we recovered the robust function we were looking before cause this energy function is assigned is assigning lower energy to smooth images.",
            "Those that have zero filter response by the same time it is allowing for outliers.",
            "Becaused details are very banned a lot.",
            "So in general."
        ],
        [
            "The energy function of our model can be written in this way as before.",
            "We have square filter outputs that multiply hidden units, but this time there is also a matrix P that linearly combines hold square filter outputs into each hidden unit before it was just one hidden unit.",
            "Multiplying one square filter output, and then we have the biases.",
            "So the goal of learning is to find the parameters W that detect directions, or smoothness dissatisfaction.",
            "And the metrics P. That is pulling this square filter outputs into each hidden unit and the biases, and we would like to maximize the likelihood.",
            "But of course we cannot, because it's intractable to compute the derivative.",
            "So the log partition function that normalizes this distribution and so we do crude approximation, which is called contrastive divergent, and to draw samples from the model we use Monte Carlo methods that is called.",
            "Monte Carlo, you can find the details in the paper you sharp.",
            "We do some approximation maximum likelihood and that's what matters."
        ],
        [
            "So if we apply this model to natural images, we find that the directions are smoothness dissatisfactions.",
            "Basically the filters W look like localized and oriented Gabor wavelets.",
            "So for example, if you take I don't know this filter.",
            "This is trying to detect.",
            "Image patches where there is a breakdown or smoothness at this position.",
            "This orientation.",
            "So basically this is saying it wants to detect image patches where on this side of the emails pixels are strongly correlated on this side of the image.",
            "Pixels are also strongly correlated, but there is strong anti correlation between pixels on either side of that region.",
            "So since these features look very much like the word wavelets, we can fit the boards and then.",
            "In the cement."
        ],
        [
            "We can see how these features tiled the image space so."
        ],
        [
            "These filters are 16 by 16 because I trained on 16 by 16 image patches and this all images 16 by 16 and each bar correspond to."
        ],
        [
            "One filter that we saw before we can use this."
        ],
        [
            "If it also to visualize what the hidden units are doing, so here each tier shows one in the unit and each bar is a feature that is strongly connected to that hidden unit and basically each hidden unit unit is strongly connected to filters that are in similar position and have similar orientation.",
            "And the idea is that the hidden unit is going to switch off.",
            "Whenever any of the filters that is connected to are strongly activated, so there is a lot of invariants that is achieved because you got the same hidden configuration no matter which of these filters are on.",
            "We did also expect."
        ],
        [
            "Points on object recognition using the CIFAR 10 data set.",
            "This is a data set of very low resolution images.",
            "The resolution is treated better to pixels and there are 10 classes.",
            "5000 samples per class in the training set and 1000 samples per class in the test set in each column.",
            "Here you can see some examples of images belonging to the same category so you can see that it's pretty challenging because there are very different viewpoints and occlusion and variability.",
            "What we did was training on 8 by 8 image patches and then to represent the whole image.",
            "We step the model every four pixels in both the vertical and horizontal direction and then we got a feature vector and then we used that as input to a standard belief network that is a sequence of restricted Boltzmann machine and then basically we got.",
            "At representation that we used as input to standard logistic logistic regression classifier for discrimination.",
            "So deep belief network was train unsupervised and then we got feature vector and then we train a classifier on it.",
            "So here I'm reporting just few results.",
            "You can find many more."
        ],
        [
            "In the paper.",
            "So in the first row is the performance that we got by using this model on the top, just one GBM in the 2nd row.",
            "It's the comparison with standard debrief network, where the first layer is a Gaussian RBM and so we do much better than that.",
            "And also we compare very well to this descriptors which are standard computer vision, kind of holistic descriptors of images.",
            "So to."
        ],
        [
            "Summarize I proposed generative model on natural images that produces binary features.",
            "These features detect breakdowns or smoothness in images and they contribute to define an image specific covariance matrix for each image.",
            "They are good for recognition and also they provide a probabilistic model for the simple, complex and model where division and they can be integrated in deep belief networks to produce.",
            "Feature hierarchy and I just want to mention that you can find more material and some code or."
        ],
        [
            "On my website, thank you."
        ],
        [
            "We have time for a couple of questions.",
            "Can you go back at the results section and what was the difference between the two 3 where again?"
        ],
        [
            "OK, so the only difference, so it is the same deep belief network, but just the first layer is different than in the first case in the first row."
        ],
        [
            "First layer is the model that I'm proposing this 3 way.",
            "This factorized way PBM in the 2nd row you instead the first layer is a Gaussian RBM which is another kind of GBM which is a more standard an.",
            "So basically this is showing that this model provides a better description of images and that the binary features that we produce are more suitable for learning or deep learning.",
            "So the 3rd row is basically.",
            "So the belief network where the first layer is our model and then we have basically 12 PBM's on the top, and so we produce.",
            "So it goes from input to use our model to produce a 9800 dimensional representation and then there is another behind.",
            "It produces on the top of this 4096 dimensional representation on the top another beyond that produces a 384 dimensional representation.",
            "That representation goes in the classifier.",
            "In the linear classifier and now we compare this to just.",
            "So you applied just descriptors to the emails to produce a 384 dimensional representation.",
            "So we compare things with the same dimensionality and then this descriptor goes into the same linear classifier and so this is showing that with the same representation we are able to produce.",
            "Something that gives higher accuracy than just."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, this is joint work with Alex Kryszewski and Jeff Hinton in Toronto.",
                    "label": 0
                },
                {
                    "sent": "In this work we are we want to model natural images by using a generative model.",
                    "label": 0
                },
                {
                    "sent": "We have a probability distribution over images and we want to train the parameters W to give higher likelihoods to natural images.",
                    "label": 0
                },
                {
                    "sent": "Actually, we are interested in probabilistic models as away as a mean to learn representations of data because we want to use these representations to do for example, recognition, segmentation, denoising.",
                    "label": 0
                },
                {
                    "sent": "Over the past few years, there has been a lot of excitement and work on some hierarchical models that are called deep belief networks that can learn a feature hierarchy.",
                    "label": 0
                },
                {
                    "sent": "But these models are currently limited to binary variables that don't work very well with continuous data.",
                    "label": 0
                },
                {
                    "sent": "So in this work we aim at mapping images into binary features that then can be used as input to a standard.",
                    "label": 0
                },
                {
                    "sent": "Belief network, so for simplicity.",
                    "label": 0
                },
                {
                    "sent": "Here the input is a small image Patch, so we want to model small image patches an.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is denoted by V, so the input.",
                    "label": 0
                },
                {
                    "sent": "Perhaps it's too loud, so the input is vectorized.",
                    "label": 0
                },
                {
                    "sent": "Image Patch denoted by B and then we have a set of letting variables or hidden units that are denoted by 8 and this H is our binary.",
                    "label": 1
                },
                {
                    "sent": "The goal is to model the joint distribution between the visibles we and hidden's age.",
                    "label": 1
                },
                {
                    "sent": "Before we proceed, let me warn you that I won't be as formalizing the paper, and I won't derive all the formulas.",
                    "label": 0
                },
                {
                    "sent": "And if you are interested in.",
                    "label": 0
                },
                {
                    "sent": "In that you should refer to the paper.",
                    "label": 0
                },
                {
                    "sent": "Here I'm trying to give you the intuition why we chose this model and how it works.",
                    "label": 0
                },
                {
                    "sent": "And this interpretation goes back to the work by Jim Anjema in the early 80s on the line process.",
                    "label": 0
                },
                {
                    "sent": "So it's a very different interpretation, but.",
                    "label": 0
                },
                {
                    "sent": "So the goal.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is to model the joint distribution between the visibles, we and the hidden stage an we want this joint distribution to capture the properties of natural images.",
                    "label": 0
                },
                {
                    "sent": "So what are these properties?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The key property is smoothness, so most of the times the value of 1 pixel can be very well predicted from its neighbors and just by looking at a bunch.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Image patches you can see that most of them are very smooth.",
                    "label": 0
                },
                {
                    "sent": "Quantify we can look at the statistics how natural images as measured by.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I linear filter for example by this vertical Gabor wavelet.",
                    "label": 0
                },
                {
                    "sent": "So if you take the inner product of this filter with many image patches, you can compute the histogram of the filter response is now here I'm showing the negative logarithm of that histogram and this is showing that most of the images give zero response to this filter.",
                    "label": 0
                },
                {
                    "sent": "Because it is very rare to find vertical ads exactly at this position and disorientation, and this is another way of saying that natural images tend to be very smooth.",
                    "label": 0
                },
                {
                    "sent": "But this is showing us another characteristic which is that.",
                    "label": 0
                },
                {
                    "sent": "Naturally images so that there are outliers.",
                    "label": 0
                },
                {
                    "sent": "So because you can see that details tend to bend, so natural signal, so sometimes violations to this smoothness property do occur.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the model that I'm going to talk about actually gives a very nice fit to this empirical distribution.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And by contrast, if you were going to use the Gaussian model, if it wouldn't be so good because it is true that it gives higher likelihood to smooth images because you can see that the minimum is around 0, where that is zero OS.",
                    "label": 0
                },
                {
                    "sent": "Bonds between this filter and in the images, but on the other hand he greatly underestimates the mass on the tails.",
                    "label": 0
                },
                {
                    "sent": "So in other words, natural signals tend to have many more outliers that you would expect by using a Gaussian model, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The key idea of this work is the following.",
                    "label": 1
                },
                {
                    "sent": "We have hidden units that are binary and that work like a switch.",
                    "label": 0
                },
                {
                    "sent": "So if the input images smooth, then the hidden unit is on and basically the negative log probability is like the one of a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It's a very tight Gaussian, but as soon as the input images structure then the hidden unit detects that and switches off.",
                    "label": 0
                },
                {
                    "sent": "It turns the lock prop into one of the uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "An of course, if you have a mixture of Gaussians, two Gaussians and with that are centered and with different variants, the result is the habitat distribution.",
                    "label": 0
                },
                {
                    "sent": "This is basically what we have in our model with the red curve so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To be more formal, we want to define a joint distribution over Visibles and Hedons an we define the joint distribution in terms of an energy function.",
                    "label": 0
                },
                {
                    "sent": "So the joint distribution is proportional to the exponential of a negative energy.",
                    "label": 0
                },
                {
                    "sent": "And so basically we want to assign higher probability to smooth images or lower energy to smooth images to natural images an we can make the energy function depend on a measure of UN smoothness or smoothness dissatisfaction.",
                    "label": 1
                },
                {
                    "sent": "So in this plot.",
                    "label": 0
                },
                {
                    "sent": "Smooth images, those that have zero dissatisfaction because they're smooth, have lower energy, and as the dissatisfaction increases because the images structure, then we want to increase the energy to make them more unlikely.",
                    "label": 0
                },
                {
                    "sent": "But we want to be robust to outliers to match the marginal statistics that we've seen before.",
                    "label": 0
                },
                {
                    "sent": "Um, so everything depends on how we.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We find the energy function and how we measure this smoothness, dissatisfaction and so related question is how?",
                    "label": 0
                },
                {
                    "sent": "So what is this smoothness dissatisfaction?",
                    "label": 1
                },
                {
                    "sent": "I'm in particular what are edges and because edges are not smooth so I remember once Jeff asked me what is a vertical ads and I said it is something like bright dark but.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "She mentioned that it could be also dark bright, or it could be the boundary between regions of different color.",
                    "label": 0
                },
                {
                    "sent": "Different tax are different disparity and so and so forth.",
                    "label": 0
                },
                {
                    "sent": "And the short answer was a vertical ads is just the lack of horizontal interpolation.",
                    "label": 1
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Whether it is color interpolation, brightness interpolation, or tax interpolation?",
                    "label": 0
                },
                {
                    "sent": "And so ideally we want to measure smoothness dissatisfaction in terms of lack of interpolation, since as I mentioned, there are many different kinds of interpolation.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is difficult to set it by hand, and therefore we learn it and we learn it by using Acetylene air filters.",
                    "label": 0
                },
                {
                    "sent": "So this linear filters will end up being highly unsmooth because they want to detect.",
                    "label": 0
                },
                {
                    "sent": "Broken smoothness in images.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned, we measure smoothness dissatisfaction by using linear filters that are denoted by my.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wii.",
                    "label": 0
                },
                {
                    "sent": "And I define.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The energy is the sum of this squared smoothness.",
                    "label": 0
                },
                {
                    "sent": "Dissatisfaction with the bias I square because I don't care whether the dissatisfaction is positive or negative.",
                    "label": 0
                },
                {
                    "sent": "I just got to detect whether it is zero or non zero an this does almost what we want because this would make smooth images more likely.",
                    "label": 0
                },
                {
                    "sent": "They have lower energy here.",
                    "label": 0
                },
                {
                    "sent": "But we would penalize too much structured images, so the idea is to allow for violation by introducing hidden units.",
                    "label": 0
                },
                {
                    "sent": "So now we have hidden units that multiply both the smoothness, dissatisfaction term and a bias.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Esther and the way it works is the following.",
                    "label": 0
                },
                {
                    "sent": "So when the emails is smooth.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the hidden unit turns out to be on and so the energy is a quadratic as a function of the smoothness dissatisfaction as a function of this filter output.",
                    "label": 0
                },
                {
                    "sent": "And so that's what you see in the lower part of this graph.",
                    "label": 0
                },
                {
                    "sent": "But when the so if the bias is positive, you can imagine that the smoothness satisfaction is almost zero, and so the energy is negative.",
                    "label": 0
                },
                {
                    "sent": "But when?",
                    "label": 0
                },
                {
                    "sent": "The image is structure then and so the smoothness, smoothness, dissatisfaction is very high.",
                    "label": 0
                },
                {
                    "sent": "Then the hidden unit turns off and so the energy becomes constant as a function of the smoothness dissatisfaction and becomes 0.",
                    "label": 0
                },
                {
                    "sent": "So the energy increased from something negative to zero, which means that a structure image now is less likely than smooth image.",
                    "label": 0
                },
                {
                    "sent": "But we don't pay a quadratic penalty on this structured image, so we don't make it.",
                    "label": 0
                },
                {
                    "sent": "So unlikely so it is unlikely, but not too much.",
                    "label": 0
                },
                {
                    "sent": "So I just read this car.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Except through a toy example.",
                    "label": 0
                },
                {
                    "sent": "So here I have just two hidden units and two filters.",
                    "label": 0
                },
                {
                    "sent": "So the first filter is W one and it is detecting lack of horizontal smoothness while the second filter is detecting lack of vertical smoothness.",
                    "label": 0
                },
                {
                    "sent": "And we have Tobias is that equal to 10?",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "Dynasty defines a joint distribution over Visibles and Hedons, and from that you can derive the conditional distributions.",
                    "label": 0
                },
                {
                    "sent": "If you do the math, it turns out that.",
                    "label": 0
                },
                {
                    "sent": "The conditional distribution over the Hidden's is just a inspectorial, so you can.",
                    "label": 0
                },
                {
                    "sent": "It's hidden unit is independent from the others given the visible and the value, the probability of 1 hidden unit being on is just the logistic of the negative square filter output plus the bias.",
                    "label": 0
                },
                {
                    "sent": "Now since images are smooth and if you remember the histogram.",
                    "label": 0
                },
                {
                    "sent": "We heard that the square filter output is most of the time zero, and so the hidden unit is most of the time on and so I drew this little histogram on the side to indicate that.",
                    "label": 0
                },
                {
                    "sent": "Another thing to observe is that the energy function as a function.",
                    "label": 0
                },
                {
                    "sent": "So for a given set of hidden units, it defines a quadratic over the visible.",
                    "label": 0
                },
                {
                    "sent": "So basically for any given configuration, the hidden units specifies a set of pairwise constraint pairwise.",
                    "label": 0
                },
                {
                    "sent": "Relations between the visible units or in other words, any given configuration.",
                    "label": 0
                },
                {
                    "sent": "The hidden units specify an image specific covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "A Gaussian with certain covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So let's read about the formula this is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The example, so if the input image is smooth, then both hidden units are on.",
                    "label": 0
                },
                {
                    "sent": "And you can find that by using the formula that I showed before, and so it's one, it's two are equal to 1.",
                    "label": 0
                },
                {
                    "sent": "But now the 1st and 2nd square filter outputs are basically zero, and so the energy function is approximately just the negative.",
                    "label": 0
                },
                {
                    "sent": "Some of the biases, which is minus 20 because you got 1 * 0 + 1 * 0 and minus B 1 * 1 -- 2 * 1.",
                    "label": 0
                },
                {
                    "sent": "So it is plus 20.",
                    "label": 0
                },
                {
                    "sent": "Now for the given.",
                    "label": 0
                },
                {
                    "sent": "For this given hidden units, we have that the energy defines also a quadratic function over the filter outputs.",
                    "label": 0
                },
                {
                    "sent": "And since H1 is equal to one, H2 is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Then you have that this condition distribution is a spherical Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So this is what you see on your right hand side and if you draw samples from this distribution you will get smooth patches like the ones that I show below.",
                    "label": 0
                },
                {
                    "sent": "But if the improvement is not smooth.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, is a line along the first filter then.",
                    "label": 0
                },
                {
                    "sent": "The first hidden unit has overwhelming probability to be 0, and so that lot is showing that the probability of being one is very small and therefore the energy function is now something like basically zero times a very large number.",
                    "label": 0
                },
                {
                    "sent": "Plus 1 * 0 and minus B1 time 0 -- B two, so the energy is approximately equal to minus P2, so before it was it was minus 20.",
                    "label": 0
                },
                {
                    "sent": "Now it is minus 10, which is saying that this structure demands is less likely than the smoothie mix that we saw before.",
                    "label": 0
                },
                {
                    "sent": "However, by setting each one equal to zero, we got a very good discount because we don't pay the penalty given by their first square filter output.",
                    "label": 0
                },
                {
                    "sent": "That could be very large, so this image is less likely than the smooth image, but it's not so unlikely.",
                    "label": 0
                },
                {
                    "sent": "Now if we look at the conditional distribution over the visible.",
                    "label": 0
                },
                {
                    "sent": "So now the energy function defines an inverse covariance matrix for for a given set of hidden units, you got the quadratic over the visibles right?",
                    "label": 0
                },
                {
                    "sent": "And so this defines an inverse covariance matrix an.",
                    "label": 0
                },
                {
                    "sent": "As a function of the filter output, so you got that when you invert this thing, you got a very large.",
                    "label": 0
                },
                {
                    "sent": "You got the negative vector with a very large again value in the direction of the first filter output, because each one is very small.",
                    "label": 0
                },
                {
                    "sent": "So when you invert it, that becomes very large and so now about this is a bit counter intuitive.",
                    "label": 0
                },
                {
                    "sent": "So when you detect that there is an edge, for example this vertical edge, it's not that the distribution becomes more confident that and, so it's not that the Gaussian distribution becomes narrower, but the model becomes more uncertain in that direction.",
                    "label": 0
                },
                {
                    "sent": "And so if you sample, you got a samples that could look like the ones in the bottom that all lack to some extent horizontal interpolation.",
                    "label": 0
                },
                {
                    "sent": "So this is another words.",
                    "label": 0
                },
                {
                    "sent": "There are many.",
                    "label": 0
                },
                {
                    "sent": "There is a whole subspace of images that all lack horizontal interpolation that are mapped into this hidden configuration.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since the hidden units are binary, we can marginalized them out an.",
                    "label": 0
                },
                {
                    "sent": "So here I'm showing the negative logarithm or the marginal distribution for I'm showing the contribution of 1 hidden unit an.",
                    "label": 0
                },
                {
                    "sent": "Basically we recovered the robust function we were looking before cause this energy function is assigned is assigning lower energy to smooth images.",
                    "label": 0
                },
                {
                    "sent": "Those that have zero filter response by the same time it is allowing for outliers.",
                    "label": 0
                },
                {
                    "sent": "Becaused details are very banned a lot.",
                    "label": 0
                },
                {
                    "sent": "So in general.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The energy function of our model can be written in this way as before.",
                    "label": 0
                },
                {
                    "sent": "We have square filter outputs that multiply hidden units, but this time there is also a matrix P that linearly combines hold square filter outputs into each hidden unit before it was just one hidden unit.",
                    "label": 0
                },
                {
                    "sent": "Multiplying one square filter output, and then we have the biases.",
                    "label": 0
                },
                {
                    "sent": "So the goal of learning is to find the parameters W that detect directions, or smoothness dissatisfaction.",
                    "label": 0
                },
                {
                    "sent": "And the metrics P. That is pulling this square filter outputs into each hidden unit and the biases, and we would like to maximize the likelihood.",
                    "label": 0
                },
                {
                    "sent": "But of course we cannot, because it's intractable to compute the derivative.",
                    "label": 0
                },
                {
                    "sent": "So the log partition function that normalizes this distribution and so we do crude approximation, which is called contrastive divergent, and to draw samples from the model we use Monte Carlo methods that is called.",
                    "label": 0
                },
                {
                    "sent": "Monte Carlo, you can find the details in the paper you sharp.",
                    "label": 1
                },
                {
                    "sent": "We do some approximation maximum likelihood and that's what matters.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we apply this model to natural images, we find that the directions are smoothness dissatisfactions.",
                    "label": 1
                },
                {
                    "sent": "Basically the filters W look like localized and oriented Gabor wavelets.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you take I don't know this filter.",
                    "label": 0
                },
                {
                    "sent": "This is trying to detect.",
                    "label": 0
                },
                {
                    "sent": "Image patches where there is a breakdown or smoothness at this position.",
                    "label": 0
                },
                {
                    "sent": "This orientation.",
                    "label": 0
                },
                {
                    "sent": "So basically this is saying it wants to detect image patches where on this side of the emails pixels are strongly correlated on this side of the image.",
                    "label": 0
                },
                {
                    "sent": "Pixels are also strongly correlated, but there is strong anti correlation between pixels on either side of that region.",
                    "label": 0
                },
                {
                    "sent": "So since these features look very much like the word wavelets, we can fit the boards and then.",
                    "label": 0
                },
                {
                    "sent": "In the cement.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can see how these features tiled the image space so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These filters are 16 by 16 because I trained on 16 by 16 image patches and this all images 16 by 16 and each bar correspond to.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One filter that we saw before we can use this.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If it also to visualize what the hidden units are doing, so here each tier shows one in the unit and each bar is a feature that is strongly connected to that hidden unit and basically each hidden unit unit is strongly connected to filters that are in similar position and have similar orientation.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that the hidden unit is going to switch off.",
                    "label": 0
                },
                {
                    "sent": "Whenever any of the filters that is connected to are strongly activated, so there is a lot of invariants that is achieved because you got the same hidden configuration no matter which of these filters are on.",
                    "label": 0
                },
                {
                    "sent": "We did also expect.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Points on object recognition using the CIFAR 10 data set.",
                    "label": 1
                },
                {
                    "sent": "This is a data set of very low resolution images.",
                    "label": 0
                },
                {
                    "sent": "The resolution is treated better to pixels and there are 10 classes.",
                    "label": 0
                },
                {
                    "sent": "5000 samples per class in the training set and 1000 samples per class in the test set in each column.",
                    "label": 0
                },
                {
                    "sent": "Here you can see some examples of images belonging to the same category so you can see that it's pretty challenging because there are very different viewpoints and occlusion and variability.",
                    "label": 0
                },
                {
                    "sent": "What we did was training on 8 by 8 image patches and then to represent the whole image.",
                    "label": 0
                },
                {
                    "sent": "We step the model every four pixels in both the vertical and horizontal direction and then we got a feature vector and then we used that as input to a standard belief network that is a sequence of restricted Boltzmann machine and then basically we got.",
                    "label": 0
                },
                {
                    "sent": "At representation that we used as input to standard logistic logistic regression classifier for discrimination.",
                    "label": 0
                },
                {
                    "sent": "So deep belief network was train unsupervised and then we got feature vector and then we train a classifier on it.",
                    "label": 0
                },
                {
                    "sent": "So here I'm reporting just few results.",
                    "label": 0
                },
                {
                    "sent": "You can find many more.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the paper.",
                    "label": 0
                },
                {
                    "sent": "So in the first row is the performance that we got by using this model on the top, just one GBM in the 2nd row.",
                    "label": 0
                },
                {
                    "sent": "It's the comparison with standard debrief network, where the first layer is a Gaussian RBM and so we do much better than that.",
                    "label": 0
                },
                {
                    "sent": "And also we compare very well to this descriptors which are standard computer vision, kind of holistic descriptors of images.",
                    "label": 0
                },
                {
                    "sent": "So to.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Summarize I proposed generative model on natural images that produces binary features.",
                    "label": 1
                },
                {
                    "sent": "These features detect breakdowns or smoothness in images and they contribute to define an image specific covariance matrix for each image.",
                    "label": 1
                },
                {
                    "sent": "They are good for recognition and also they provide a probabilistic model for the simple, complex and model where division and they can be integrated in deep belief networks to produce.",
                    "label": 0
                },
                {
                    "sent": "Feature hierarchy and I just want to mention that you can find more material and some code or.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On my website, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have time for a couple of questions.",
                    "label": 0
                },
                {
                    "sent": "Can you go back at the results section and what was the difference between the two 3 where again?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the only difference, so it is the same deep belief network, but just the first layer is different than in the first case in the first row.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First layer is the model that I'm proposing this 3 way.",
                    "label": 0
                },
                {
                    "sent": "This factorized way PBM in the 2nd row you instead the first layer is a Gaussian RBM which is another kind of GBM which is a more standard an.",
                    "label": 0
                },
                {
                    "sent": "So basically this is showing that this model provides a better description of images and that the binary features that we produce are more suitable for learning or deep learning.",
                    "label": 0
                },
                {
                    "sent": "So the 3rd row is basically.",
                    "label": 0
                },
                {
                    "sent": "So the belief network where the first layer is our model and then we have basically 12 PBM's on the top, and so we produce.",
                    "label": 0
                },
                {
                    "sent": "So it goes from input to use our model to produce a 9800 dimensional representation and then there is another behind.",
                    "label": 0
                },
                {
                    "sent": "It produces on the top of this 4096 dimensional representation on the top another beyond that produces a 384 dimensional representation.",
                    "label": 0
                },
                {
                    "sent": "That representation goes in the classifier.",
                    "label": 0
                },
                {
                    "sent": "In the linear classifier and now we compare this to just.",
                    "label": 0
                },
                {
                    "sent": "So you applied just descriptors to the emails to produce a 384 dimensional representation.",
                    "label": 0
                },
                {
                    "sent": "So we compare things with the same dimensionality and then this descriptor goes into the same linear classifier and so this is showing that with the same representation we are able to produce.",
                    "label": 0
                },
                {
                    "sent": "Something that gives higher accuracy than just.",
                    "label": 0
                }
            ]
        }
    }
}