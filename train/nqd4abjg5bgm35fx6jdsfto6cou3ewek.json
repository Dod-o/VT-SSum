{
    "id": "nqd4abjg5bgm35fx6jdsfto6cou3ewek",
    "title": "Bayesian Adaptation for Statistical Machine Translation",
    "info": {
        "author": [
            "German Sanchis-Trilles, Technical University of Valencia (UPV)"
        ],
        "published": "Sept. 13, 2010",
        "recorded": "August 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/ssspr2010_sanchis_trilles_basm/",
    "segmentation": [
        [
            "My name is Carmen 23.",
            "Yes, I'm from the Polytechnic University of Valencia.",
            "And I am here to present you the word added together with Francisco Casa Covert about Bayesian adaptation for statistical machine translation loops.",
            "And sorry for that.",
            "Right?"
        ],
        [
            "OK, so first I will perform a brief introduction to the problem of adaptation.",
            "Then I will explain you how weight optimization is typically done in statistical machine translation.",
            "After that I will give some insight about how we have we have applied by Asian adaptation to so the bias in learning paradigm to statistical machine translation.",
            "And then I will explain you the experiments performed together with the practical approximations needed in order to apply this nice theoretic paradigm to our particular case.",
            "And finally, I will explain you the conclusions of the work than what future work can be done."
        ],
        [
            "So first of all, what is adaptation, adaptation and pattern recognition is typically the task of taking a system trade on a relatively large amount of data.",
            "And putting it in order to be able to use it in a different domain, say for instance, if we have trained a system in order to classify roses, different kinds of roses, and now we want to classify different kinds of.",
            "I don't some other kind of flower then the problem would be how to extract the information available in the train system in order to be able to apply it to another case.",
            "For instance in speech recognition if we have system which has been trained on a large amount of data, but now we intend to recognize only woman voices, then most probably what we will have to do is take this amount of this training data or dis system trained on the.",
            "Complete amount of training data and modify it in some sense in order to be able to recognize.",
            "To recognize what the female speakers are saying in an appropriate way.",
            "So the problem of adaptation is basically how to take advantage of a large collection of data which has been used to train our system in order to run this system on another domain which has a lot less of data available.",
            "This is particularly challenging and natural language processing.",
            "Because it is very costly to label new new data.",
            "So for instance in machine translation you have very large amount of data coming from the parliamentary domain.",
            "Say for instance data from the European Parliament we have from the European Parliament proceedings.",
            "We have data from 1996 so.",
            "Millions and millions of sentences.",
            "But however maybe this data is not good enough in order to translate, maybe manual printers or medical data.",
            "Or even maybe Canadian hansard's data.",
            "Because the way the speakers speak is different, and there are some differences in there."
        ],
        [
            "So.",
            "Now a bit more about machine translation about the field.",
            "This work has been done.",
            "So lately log linear models were introduced into statistical machine translation and they implied are very important breakthrough in which the translation quality the systems were able to deliver increased significantly.",
            "Moreover, log linear models have also been used in other natural language processing tasks with success, namely, for instance handwritten text recognition or speech recognition as well.",
            "They have also been able to provide improvements over the previous state of the art.",
            "So this work which has been applied here to statistical machine translation could also be applied without much effort to these other tasks where pattern recognition is implemented with log linear models.",
            "So in machine translation, the basic decision rule which we have when using log linear models is given by this formula here.",
            "Which means that the sentence the system will output as best translation is given by a maximization over the weighted sum.",
            "Over a Series A collection of features and these features are the features which are considered important in order to translate from the source language into the target language.",
            "So here X would be the source sentence and why would be the target sentence an example of these features can be, for instance language model, translation model.",
            "Maybe even are reordering model distortion, model, whatever we think is a good idea to plug into the system so that it works.",
            "And typically this set of weights.",
            "Is optimized by means of an adaptation set by sorry by means of development set.",
            "An by means of an algorithm which I will explain in the next slide so as to maximize the translation quality that the system is able to deliver on this specific development set.",
            "OK.",
            "In this paper, we are presenting a Bayesian technique for adapting these weights.",
            "Without forgetting about the generality provided by the initial set of weights, and as I said, this could be easily applied to other fields where log linear models are present."
        ],
        [
            "OK, so the algorithm I said before is named merged, which means minimum error rate training.",
            "An it's basically more than algorithm.",
            "It's a family of algorithms and which consists of three steps.",
            "First, we extract set of N best hypothesis.",
            "From the current development set.",
            "With the current set of weights, then we rerank these hypothesis.",
            "By playing with these weights so that the translation quality over the whole development set is the best we are able to obtain from these N best lists.",
            "And finally, these two steps are repeated until the desired convergence is achieved.",
            "Of course, since changing the set of weights.",
            "May imply that the list that the N best list changes.",
            "We will need to repeat these steps until until the set of weights remains unchanged.",
            "This this algorithm, when applied into adaptation, has basically 2 problems.",
            "The first one that it relies heavily on the on having a relatively large amount of adaptation data available.",
            "Secondly, it has an additional problem, which is that it only relies on this development data.",
            "This means that whenever the amount of development data is not very, very big.",
            "This will lead to heavily overtrained solutions of the optimization problem.",
            "In which.",
            "We will be able to translate the development data quite well, will have very high translation quality in the development data, but in the test data, the translation quality will drop significantly.",
            "Which means that this algorithm becomes very unstable whenever the amount of development data is low.",
            "And in addition, if we have this system set up to maybe help a human translator for online translation, then the human translator is not willing to wait for more to finish.",
            "As you might imagine.",
            "This takes translating the development set several times, which may range from maybe a couple of minutes to maybe several hours, depending on the amount of development data.",
            "At even for several minutes, this is not acceptable if the user is waiting.",
            "Actively, we do not want this."
        ],
        [
            "OK, so this scenario is very appropriate for Biasion adaptation.",
            "Since what we want is to take up the set of nicely trained weights and and sometimes shift them or bias them towards the adaptation data.",
            "This is what the Bayesian learning paradigm does.",
            "We bias the weight vector towards the adaptation data and we prevent overtraining by introducing the prior over the over the weight para meters."
        ],
        [
            "So now for some scary formula.",
            "And as I said, the main idea behind by using adaptation is to consider the para meters the model para meters.",
            "As random variables with underlying distributions.",
            "This means that we will need to consider the integral over all the parametric space.",
            "And that we will introduce our prior over the para meters.",
            "So.",
            "That will be the probability of the output sentence given an input sentence, the training data and the adaptation data.",
            "And operating with this and making some assumptions and throwing in several normalizations constants into this Z.",
            "Constant, we obtain this formula here.",
            "Which is divided into 3 parts.",
            "The first part this product is.",
            "The probability of the adaptation data.",
            "Then we have this normal, which is the probability of the current model parameters given the parameters submitted in training time.",
            "And finally, the probability of the current output sentence.",
            "And well, the whole integral over the whole parametric space."
        ],
        [
            "Tips?",
            "OK, so the decision rule is redefined as the maximization over the probability of the output sentence given the input sentence, the training data and the adaptation data now.",
            "And then plugging in the log linear models.",
            "Use them statistical machine translation.",
            "We end up with this huge formula here, which is basically what I said before.",
            "First we have the probability of the adaptation data, then the probability of the current model parameters given the ones in obtaining training time and finally the probability of the current test sentence.",
            "This means that the first part, this product will account for biasing the whole integral towards the adaptation data.",
            "The second part is the probabilities prior.",
            "Will account for keeping the para meters the model parameters within a reasonable range so avoid overtraining.",
            "And while finally this is a necessary part since it's the probability of the current test sentence.",
            "If you look at this whole thing, the difference with the traditional decision rule in statistical machine translation.",
            "It's basically this and this and well integrated, but this part.",
            "Is the one we already had, so we're introducing a couple of terms there.",
            "A couple of terms which can be precomputed so it does not imply further time in translation time."
        ],
        [
            "OK.",
            "So as I said in the at the beginning.",
            "And all this theory is very nice for formulas, but it's very nice.",
            "However, we have a big problem, which is that some of the assumptions that have been made are not applicable in practice, so we need to perform several practical approximations.",
            "To start with, current state of the art statisical matching trade translation models are not able to explain the.",
            "A given sentence pair.",
            "For instance, given if this sentence pair has some out of vocabulary words, then the statistical machine translation system will fail in explaining that sentence, so it will fail in in giving that sentence pair a probability.",
            "For that reason, what we did is instead of using the true label of the sentence, we use the best hypothesis which the system is able to deliver.",
            "Then another problem is that the sum over all the world possible output sentences is unfeasible because the amount of sentences.",
            "In a language is fine it, but it's very very big.",
            "Then what we did is to consider only a set of best.",
            "So 500 best in this case.",
            "And finally, since the tree integral over the parametric space is not feasible either, what we did is a random sampling over the other para meters."
        ],
        [
            "OK, so for the experimental setup as I said, what we want to do is to perform adaptation.",
            "This means that we need system trained on out of the main corpus and in this case this out of the main corpus will be your pool, which is a fairly big corpus obtained from parliamentary proceedings.",
            "It has about 7700 thirty thousand sentences and it's training partition.",
            "Then we also have a development partition which will be the one we will use to estimate the initial set of weights for the log linear model.",
            "And then in order to test how well this system adapts.",
            "We have selected sentences from other two corpora, which is the sharrocks corpus which is made out of printer manuals and the EU corpus which is made out of parliamentary speeches, not proceedings.",
            "In this presentation I will focus on the EU corpus, although and in the paper you can look at the Sharks results as well.",
            "OK, so for performing adaptation what we did is to extract randomly sentences from the training corpus and then test the system on the final Test data.",
            "And we will be assessing the translation quality by means of blue and TR, which are common metrics and statistical machine translation.",
            "An blue is a measures precision of N grams.",
            "So it measures the average position of 1 unigrams bigrams, trigrams programs without penalty for two long sentences.",
            "And TR is more or less the edit distance between the two sentences, with allowing shifts of word sequences.",
            "But it's basically the edit distance.",
            "And word base."
        ],
        [
            "So as I said, we will be extracting sentences from the training data of the EU corpus in order to obtain this adaptation data.",
            "This data will be used either format or within our Bayesian adaptation technique, and we will extract from ten of two 140 sentences in order to assess how good the system behaves with increasing amount of samples.",
            "And in order to provide stability to the results displayed here, we performed 15 random samplings of for each one of these up, this data amounts."
        ],
        [
            "And how well does it work?",
            "Well, here you can see how well it works and these results are for the EU corpus we have here.",
            "The baseline is the black line here baseline.",
            "Then we have merged, which is this one which means re estimation of the model para meters.",
            "This one here and here we have the Baiyun adaptation results.",
            "What can we see here?",
            "We can see two things.",
            "The first thing we can see is that by SNR application is over, is able to improve the baseline consistently and both metrics.",
            "And in both corpora, although the second one, you cannot see it here.",
            "And we can also see that bias annotation has very low confidence intervals, so.",
            "Even with 10 samples here you can see that.",
            "Then the confidence intervals, which are these black lines are very low, less than a point, whereas for the murder case the confidence intervals range from 6 to 7 points with low adaptation data sizes.",
            "And then turn a bit more stable.",
            "But if the amount of adaptation data is very low, then it runs very very unstable and this is not desirable if we want to set the system online.",
            "So if we only have 40 sentences for instance, or 30 maybe six or seven points difference means a huge difference and whenever the translator.",
            "Is trying to post it to to use this sentence for some purpose?",
            "Whenever the amount of adaptation data increases, then we can see that the translation Quality Mart is able to deliver.",
            "It is over the Bayesian adaptation quality in average because their confidence intervals are still very are still quite big.",
            "Of course.",
            "If the amount of adaptation data is very big, then adaptation has no sense.",
            "So if we have enough sentences in order to re estimate the model parameters, why would we do any kind of adaptation?",
            "It must be noted that in this context we only have.",
            "Seven weights and a log linear model.",
            "So seven parameters are estimated here."
        ],
        [
            "OK. Anne.",
            "In addition, at the computational cost of running merge each time is still too high in order to have this system produce online.",
            "In some cases it could be maybe four 100 sentences.",
            "This could be maybe an hour or so, so this is not desirable.",
            "And improvements obtained with the Bayesian adaptation technique proved to be consistent and with all adaptation sizes and with both Cobra and with both metrics."
        ],
        [
            "So I ask for conclusions.",
            "And the Bayesian adaptation technique presented here is over is able to provide stable improvements over the baseline, and if the amount of adaptation data is low.",
            "Also overestimating the model para meters with murder.",
            "The improvements obtained are both consistent and stable.",
            "Which means that.",
            "They allegedly may be true improvements.",
            "And even for smaller additions later these these improvements are stable, which is very important in our case.",
            "And of course, for larger amounts of data, then let's re estimate the parameters."
        ],
        [
            "So I put the formula here so that you don't forget it so that you can all have any side of this big thing.",
            "As for future work.",
            "Of course, if you look at this integral here.",
            "And Z normalization constant Z normalization constant is not taken into account since what we intend to do is a decision rule which is a maximum operation argmax operation on this.",
            "Does he constant is forgotten completely?",
            "Taken outside.",
            "What does this mean?",
            "This means that sends no days notice.",
            "This is no notice.",
            "Notice.",
            "Neither of them are probabilities, because the normalizations have been taken out.",
            "They may have very different numeric ranges, which may imply that maybe in this product, if the amount of adaptation data increases.",
            "This term here will drop.",
            "Will drop quite heavily and will have and in the long run absolutely no effect on the final integral.",
            "And hence the effect of adapting the system may fade completely.",
            "So the idea we had is introduced here.",
            "An exponent exponent, which would be the same here and another one here.",
            "So Delta Delta 1 minus Delta in order to leverage these terms and return them into the same numeric ranges.",
            "Of course this steps quite a lot aside the theory, but it has been proven to behave well.",
            "Another in other fields.",
            "And this would imply a sort of compromise between stability and improvement potential.",
            "Of course, if the if the exponent went through, here is very high.",
            "Then is magnifies the impact a lot?",
            "Then it will turn unstable into it.",
            "Will biased too much towards the adaptation data.",
            "We also intend to do to perform a 2 Monte Carlo sampling on the on the sampling over on the integral instead of using random sampling we used.",
            "We intend to perform 2 Monte Carlo sampling.",
            "And finally, we also intend to extend this formulation in order to adapt these feature functions instead of the set of the log linear weights we intend to adapt the feature functions.",
            "Allowed that is still working progress everything."
        ],
        [
            "And that's all.",
            "If you would have any questions or suggestions.",
            "Thank you for the presentation."
        ],
        [
            "Any questions?",
            "You said that you use some prior knowledge on.",
            "My question is, how do you learn the prior parameters?",
            "OK, so as I said, the incestral machine translation.",
            "These parameters are typically trained by using our development set.",
            "So what we did is to use the development set of the Europol data so the out of the main data in order to estimate the this prior.",
            "This well if in that case the development data is there a pool.",
            "And that would.",
            "Give us.",
            "The estimation for the list training Theta, which is the average of which is the mean of this quotient, and that would be the prior knowledge.",
            "Your prior parameters.",
            "Actually, during the application of your system are really different for.",
            "Yes, in some cases they are relatively different.",
            "Why do you think?",
            "Are so different?",
            "Well, one reason, for instance, is the sentence length and something I did not say, but which is true about statistical machine translation state of the art systems is that they introduce a word penalty and a phrase penalty which is.",
            "If the sentence becomes too long, typically.",
            "It loses probability because it's a product.",
            "So in order to compensate this fact, there is a leveraging term which enables that this effect is not so strict.",
            "This sentence, this phrase, or word penalty, which which is not really a penalty but a bonus.",
            "It also has a weight.",
            "So of course, if the the main changes if the corpus changes, the average sentence length also changes sometime, in some cases dramatically so.",
            "This weight, for instance, is changed quite a lot.",
            "And maybe also the language model Weight loses potential versus the translation model weight or the distortion model.",
            "OK, this is another question, mines very simple.",
            "It's why is it so obvious that the.",
            "System.",
            "Convergence of the adaptation is not converged.",
            "You don't have to impose some conditions on the number of.",
            "Each step, the number of actual examples need to draw to be able to.",
            "Do you mean this convergence here or?",
            "App."
        ],
        [
            "OK this year.",
            "Well, this is.",
            "This is more state of the art machine translation and my work, but I can explain it anyway.",
            "While the amount of hypothesis the system is able to deliver is finite, of course.",
            "Anne.",
            "There is one of the hypothesis which is the best.",
            "So in the long run, after several iterations.",
            "You will have seen enough different Lambda samples in order to have seen the best hypothesis in the different.",
            "Cases.",
            "So that answer your question or not really.",
            "OK. Alright, so.",
            "Maybe if you want I can refer you to the papers of 2000, four 2003 of Ohio and others who did this work.",
            "OK, so this finishes the presentation.",
            "Thank you, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is Carmen 23.",
                    "label": 0
                },
                {
                    "sent": "Yes, I'm from the Polytechnic University of Valencia.",
                    "label": 0
                },
                {
                    "sent": "And I am here to present you the word added together with Francisco Casa Covert about Bayesian adaptation for statistical machine translation loops.",
                    "label": 1
                },
                {
                    "sent": "And sorry for that.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so first I will perform a brief introduction to the problem of adaptation.",
                    "label": 0
                },
                {
                    "sent": "Then I will explain you how weight optimization is typically done in statistical machine translation.",
                    "label": 0
                },
                {
                    "sent": "After that I will give some insight about how we have we have applied by Asian adaptation to so the bias in learning paradigm to statistical machine translation.",
                    "label": 0
                },
                {
                    "sent": "And then I will explain you the experiments performed together with the practical approximations needed in order to apply this nice theoretic paradigm to our particular case.",
                    "label": 1
                },
                {
                    "sent": "And finally, I will explain you the conclusions of the work than what future work can be done.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, what is adaptation, adaptation and pattern recognition is typically the task of taking a system trade on a relatively large amount of data.",
                    "label": 0
                },
                {
                    "sent": "And putting it in order to be able to use it in a different domain, say for instance, if we have trained a system in order to classify roses, different kinds of roses, and now we want to classify different kinds of.",
                    "label": 1
                },
                {
                    "sent": "I don't some other kind of flower then the problem would be how to extract the information available in the train system in order to be able to apply it to another case.",
                    "label": 0
                },
                {
                    "sent": "For instance in speech recognition if we have system which has been trained on a large amount of data, but now we intend to recognize only woman voices, then most probably what we will have to do is take this amount of this training data or dis system trained on the.",
                    "label": 0
                },
                {
                    "sent": "Complete amount of training data and modify it in some sense in order to be able to recognize.",
                    "label": 0
                },
                {
                    "sent": "To recognize what the female speakers are saying in an appropriate way.",
                    "label": 1
                },
                {
                    "sent": "So the problem of adaptation is basically how to take advantage of a large collection of data which has been used to train our system in order to run this system on another domain which has a lot less of data available.",
                    "label": 1
                },
                {
                    "sent": "This is particularly challenging and natural language processing.",
                    "label": 0
                },
                {
                    "sent": "Because it is very costly to label new new data.",
                    "label": 1
                },
                {
                    "sent": "So for instance in machine translation you have very large amount of data coming from the parliamentary domain.",
                    "label": 0
                },
                {
                    "sent": "Say for instance data from the European Parliament we have from the European Parliament proceedings.",
                    "label": 0
                },
                {
                    "sent": "We have data from 1996 so.",
                    "label": 0
                },
                {
                    "sent": "Millions and millions of sentences.",
                    "label": 0
                },
                {
                    "sent": "But however maybe this data is not good enough in order to translate, maybe manual printers or medical data.",
                    "label": 0
                },
                {
                    "sent": "Or even maybe Canadian hansard's data.",
                    "label": 0
                },
                {
                    "sent": "Because the way the speakers speak is different, and there are some differences in there.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now a bit more about machine translation about the field.",
                    "label": 0
                },
                {
                    "sent": "This work has been done.",
                    "label": 0
                },
                {
                    "sent": "So lately log linear models were introduced into statistical machine translation and they implied are very important breakthrough in which the translation quality the systems were able to deliver increased significantly.",
                    "label": 1
                },
                {
                    "sent": "Moreover, log linear models have also been used in other natural language processing tasks with success, namely, for instance handwritten text recognition or speech recognition as well.",
                    "label": 0
                },
                {
                    "sent": "They have also been able to provide improvements over the previous state of the art.",
                    "label": 0
                },
                {
                    "sent": "So this work which has been applied here to statistical machine translation could also be applied without much effort to these other tasks where pattern recognition is implemented with log linear models.",
                    "label": 0
                },
                {
                    "sent": "So in machine translation, the basic decision rule which we have when using log linear models is given by this formula here.",
                    "label": 0
                },
                {
                    "sent": "Which means that the sentence the system will output as best translation is given by a maximization over the weighted sum.",
                    "label": 0
                },
                {
                    "sent": "Over a Series A collection of features and these features are the features which are considered important in order to translate from the source language into the target language.",
                    "label": 0
                },
                {
                    "sent": "So here X would be the source sentence and why would be the target sentence an example of these features can be, for instance language model, translation model.",
                    "label": 0
                },
                {
                    "sent": "Maybe even are reordering model distortion, model, whatever we think is a good idea to plug into the system so that it works.",
                    "label": 0
                },
                {
                    "sent": "And typically this set of weights.",
                    "label": 1
                },
                {
                    "sent": "Is optimized by means of an adaptation set by sorry by means of development set.",
                    "label": 0
                },
                {
                    "sent": "An by means of an algorithm which I will explain in the next slide so as to maximize the translation quality that the system is able to deliver on this specific development set.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In this paper, we are presenting a Bayesian technique for adapting these weights.",
                    "label": 1
                },
                {
                    "sent": "Without forgetting about the generality provided by the initial set of weights, and as I said, this could be easily applied to other fields where log linear models are present.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the algorithm I said before is named merged, which means minimum error rate training.",
                    "label": 0
                },
                {
                    "sent": "An it's basically more than algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's a family of algorithms and which consists of three steps.",
                    "label": 0
                },
                {
                    "sent": "First, we extract set of N best hypothesis.",
                    "label": 0
                },
                {
                    "sent": "From the current development set.",
                    "label": 1
                },
                {
                    "sent": "With the current set of weights, then we rerank these hypothesis.",
                    "label": 0
                },
                {
                    "sent": "By playing with these weights so that the translation quality over the whole development set is the best we are able to obtain from these N best lists.",
                    "label": 0
                },
                {
                    "sent": "And finally, these two steps are repeated until the desired convergence is achieved.",
                    "label": 0
                },
                {
                    "sent": "Of course, since changing the set of weights.",
                    "label": 0
                },
                {
                    "sent": "May imply that the list that the N best list changes.",
                    "label": 0
                },
                {
                    "sent": "We will need to repeat these steps until until the set of weights remains unchanged.",
                    "label": 0
                },
                {
                    "sent": "This this algorithm, when applied into adaptation, has basically 2 problems.",
                    "label": 0
                },
                {
                    "sent": "The first one that it relies heavily on the on having a relatively large amount of adaptation data available.",
                    "label": 1
                },
                {
                    "sent": "Secondly, it has an additional problem, which is that it only relies on this development data.",
                    "label": 1
                },
                {
                    "sent": "This means that whenever the amount of development data is not very, very big.",
                    "label": 0
                },
                {
                    "sent": "This will lead to heavily overtrained solutions of the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "In which.",
                    "label": 0
                },
                {
                    "sent": "We will be able to translate the development data quite well, will have very high translation quality in the development data, but in the test data, the translation quality will drop significantly.",
                    "label": 0
                },
                {
                    "sent": "Which means that this algorithm becomes very unstable whenever the amount of development data is low.",
                    "label": 1
                },
                {
                    "sent": "And in addition, if we have this system set up to maybe help a human translator for online translation, then the human translator is not willing to wait for more to finish.",
                    "label": 0
                },
                {
                    "sent": "As you might imagine.",
                    "label": 0
                },
                {
                    "sent": "This takes translating the development set several times, which may range from maybe a couple of minutes to maybe several hours, depending on the amount of development data.",
                    "label": 0
                },
                {
                    "sent": "At even for several minutes, this is not acceptable if the user is waiting.",
                    "label": 0
                },
                {
                    "sent": "Actively, we do not want this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this scenario is very appropriate for Biasion adaptation.",
                    "label": 0
                },
                {
                    "sent": "Since what we want is to take up the set of nicely trained weights and and sometimes shift them or bias them towards the adaptation data.",
                    "label": 0
                },
                {
                    "sent": "This is what the Bayesian learning paradigm does.",
                    "label": 0
                },
                {
                    "sent": "We bias the weight vector towards the adaptation data and we prevent overtraining by introducing the prior over the over the weight para meters.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now for some scary formula.",
                    "label": 0
                },
                {
                    "sent": "And as I said, the main idea behind by using adaptation is to consider the para meters the model para meters.",
                    "label": 0
                },
                {
                    "sent": "As random variables with underlying distributions.",
                    "label": 1
                },
                {
                    "sent": "This means that we will need to consider the integral over all the parametric space.",
                    "label": 1
                },
                {
                    "sent": "And that we will introduce our prior over the para meters.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That will be the probability of the output sentence given an input sentence, the training data and the adaptation data.",
                    "label": 0
                },
                {
                    "sent": "And operating with this and making some assumptions and throwing in several normalizations constants into this Z.",
                    "label": 0
                },
                {
                    "sent": "Constant, we obtain this formula here.",
                    "label": 0
                },
                {
                    "sent": "Which is divided into 3 parts.",
                    "label": 0
                },
                {
                    "sent": "The first part this product is.",
                    "label": 0
                },
                {
                    "sent": "The probability of the adaptation data.",
                    "label": 0
                },
                {
                    "sent": "Then we have this normal, which is the probability of the current model parameters given the parameters submitted in training time.",
                    "label": 0
                },
                {
                    "sent": "And finally, the probability of the current output sentence.",
                    "label": 0
                },
                {
                    "sent": "And well, the whole integral over the whole parametric space.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tips?",
                    "label": 0
                },
                {
                    "sent": "OK, so the decision rule is redefined as the maximization over the probability of the output sentence given the input sentence, the training data and the adaptation data now.",
                    "label": 0
                },
                {
                    "sent": "And then plugging in the log linear models.",
                    "label": 0
                },
                {
                    "sent": "Use them statistical machine translation.",
                    "label": 0
                },
                {
                    "sent": "We end up with this huge formula here, which is basically what I said before.",
                    "label": 0
                },
                {
                    "sent": "First we have the probability of the adaptation data, then the probability of the current model parameters given the ones in obtaining training time and finally the probability of the current test sentence.",
                    "label": 0
                },
                {
                    "sent": "This means that the first part, this product will account for biasing the whole integral towards the adaptation data.",
                    "label": 1
                },
                {
                    "sent": "The second part is the probabilities prior.",
                    "label": 1
                },
                {
                    "sent": "Will account for keeping the para meters the model parameters within a reasonable range so avoid overtraining.",
                    "label": 0
                },
                {
                    "sent": "And while finally this is a necessary part since it's the probability of the current test sentence.",
                    "label": 0
                },
                {
                    "sent": "If you look at this whole thing, the difference with the traditional decision rule in statistical machine translation.",
                    "label": 0
                },
                {
                    "sent": "It's basically this and this and well integrated, but this part.",
                    "label": 0
                },
                {
                    "sent": "Is the one we already had, so we're introducing a couple of terms there.",
                    "label": 0
                },
                {
                    "sent": "A couple of terms which can be precomputed so it does not imply further time in translation time.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So as I said in the at the beginning.",
                    "label": 0
                },
                {
                    "sent": "And all this theory is very nice for formulas, but it's very nice.",
                    "label": 0
                },
                {
                    "sent": "However, we have a big problem, which is that some of the assumptions that have been made are not applicable in practice, so we need to perform several practical approximations.",
                    "label": 0
                },
                {
                    "sent": "To start with, current state of the art statisical matching trade translation models are not able to explain the.",
                    "label": 1
                },
                {
                    "sent": "A given sentence pair.",
                    "label": 0
                },
                {
                    "sent": "For instance, given if this sentence pair has some out of vocabulary words, then the statistical machine translation system will fail in explaining that sentence, so it will fail in in giving that sentence pair a probability.",
                    "label": 0
                },
                {
                    "sent": "For that reason, what we did is instead of using the true label of the sentence, we use the best hypothesis which the system is able to deliver.",
                    "label": 0
                },
                {
                    "sent": "Then another problem is that the sum over all the world possible output sentences is unfeasible because the amount of sentences.",
                    "label": 0
                },
                {
                    "sent": "In a language is fine it, but it's very very big.",
                    "label": 1
                },
                {
                    "sent": "Then what we did is to consider only a set of best.",
                    "label": 1
                },
                {
                    "sent": "So 500 best in this case.",
                    "label": 0
                },
                {
                    "sent": "And finally, since the tree integral over the parametric space is not feasible either, what we did is a random sampling over the other para meters.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so for the experimental setup as I said, what we want to do is to perform adaptation.",
                    "label": 1
                },
                {
                    "sent": "This means that we need system trained on out of the main corpus and in this case this out of the main corpus will be your pool, which is a fairly big corpus obtained from parliamentary proceedings.",
                    "label": 0
                },
                {
                    "sent": "It has about 7700 thirty thousand sentences and it's training partition.",
                    "label": 0
                },
                {
                    "sent": "Then we also have a development partition which will be the one we will use to estimate the initial set of weights for the log linear model.",
                    "label": 0
                },
                {
                    "sent": "And then in order to test how well this system adapts.",
                    "label": 0
                },
                {
                    "sent": "We have selected sentences from other two corpora, which is the sharrocks corpus which is made out of printer manuals and the EU corpus which is made out of parliamentary speeches, not proceedings.",
                    "label": 1
                },
                {
                    "sent": "In this presentation I will focus on the EU corpus, although and in the paper you can look at the Sharks results as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so for performing adaptation what we did is to extract randomly sentences from the training corpus and then test the system on the final Test data.",
                    "label": 0
                },
                {
                    "sent": "And we will be assessing the translation quality by means of blue and TR, which are common metrics and statistical machine translation.",
                    "label": 1
                },
                {
                    "sent": "An blue is a measures precision of N grams.",
                    "label": 0
                },
                {
                    "sent": "So it measures the average position of 1 unigrams bigrams, trigrams programs without penalty for two long sentences.",
                    "label": 0
                },
                {
                    "sent": "And TR is more or less the edit distance between the two sentences, with allowing shifts of word sequences.",
                    "label": 0
                },
                {
                    "sent": "But it's basically the edit distance.",
                    "label": 0
                },
                {
                    "sent": "And word base.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I said, we will be extracting sentences from the training data of the EU corpus in order to obtain this adaptation data.",
                    "label": 0
                },
                {
                    "sent": "This data will be used either format or within our Bayesian adaptation technique, and we will extract from ten of two 140 sentences in order to assess how good the system behaves with increasing amount of samples.",
                    "label": 1
                },
                {
                    "sent": "And in order to provide stability to the results displayed here, we performed 15 random samplings of for each one of these up, this data amounts.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And how well does it work?",
                    "label": 0
                },
                {
                    "sent": "Well, here you can see how well it works and these results are for the EU corpus we have here.",
                    "label": 0
                },
                {
                    "sent": "The baseline is the black line here baseline.",
                    "label": 0
                },
                {
                    "sent": "Then we have merged, which is this one which means re estimation of the model para meters.",
                    "label": 0
                },
                {
                    "sent": "This one here and here we have the Baiyun adaptation results.",
                    "label": 0
                },
                {
                    "sent": "What can we see here?",
                    "label": 0
                },
                {
                    "sent": "We can see two things.",
                    "label": 0
                },
                {
                    "sent": "The first thing we can see is that by SNR application is over, is able to improve the baseline consistently and both metrics.",
                    "label": 0
                },
                {
                    "sent": "And in both corpora, although the second one, you cannot see it here.",
                    "label": 0
                },
                {
                    "sent": "And we can also see that bias annotation has very low confidence intervals, so.",
                    "label": 0
                },
                {
                    "sent": "Even with 10 samples here you can see that.",
                    "label": 0
                },
                {
                    "sent": "Then the confidence intervals, which are these black lines are very low, less than a point, whereas for the murder case the confidence intervals range from 6 to 7 points with low adaptation data sizes.",
                    "label": 0
                },
                {
                    "sent": "And then turn a bit more stable.",
                    "label": 0
                },
                {
                    "sent": "But if the amount of adaptation data is very low, then it runs very very unstable and this is not desirable if we want to set the system online.",
                    "label": 0
                },
                {
                    "sent": "So if we only have 40 sentences for instance, or 30 maybe six or seven points difference means a huge difference and whenever the translator.",
                    "label": 0
                },
                {
                    "sent": "Is trying to post it to to use this sentence for some purpose?",
                    "label": 0
                },
                {
                    "sent": "Whenever the amount of adaptation data increases, then we can see that the translation Quality Mart is able to deliver.",
                    "label": 0
                },
                {
                    "sent": "It is over the Bayesian adaptation quality in average because their confidence intervals are still very are still quite big.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "If the amount of adaptation data is very big, then adaptation has no sense.",
                    "label": 0
                },
                {
                    "sent": "So if we have enough sentences in order to re estimate the model parameters, why would we do any kind of adaptation?",
                    "label": 0
                },
                {
                    "sent": "It must be noted that in this context we only have.",
                    "label": 0
                },
                {
                    "sent": "Seven weights and a log linear model.",
                    "label": 0
                },
                {
                    "sent": "So seven parameters are estimated here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Anne.",
                    "label": 0
                },
                {
                    "sent": "In addition, at the computational cost of running merge each time is still too high in order to have this system produce online.",
                    "label": 0
                },
                {
                    "sent": "In some cases it could be maybe four 100 sentences.",
                    "label": 0
                },
                {
                    "sent": "This could be maybe an hour or so, so this is not desirable.",
                    "label": 0
                },
                {
                    "sent": "And improvements obtained with the Bayesian adaptation technique proved to be consistent and with all adaptation sizes and with both Cobra and with both metrics.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I ask for conclusions.",
                    "label": 0
                },
                {
                    "sent": "And the Bayesian adaptation technique presented here is over is able to provide stable improvements over the baseline, and if the amount of adaptation data is low.",
                    "label": 1
                },
                {
                    "sent": "Also overestimating the model para meters with murder.",
                    "label": 1
                },
                {
                    "sent": "The improvements obtained are both consistent and stable.",
                    "label": 0
                },
                {
                    "sent": "Which means that.",
                    "label": 1
                },
                {
                    "sent": "They allegedly may be true improvements.",
                    "label": 1
                },
                {
                    "sent": "And even for smaller additions later these these improvements are stable, which is very important in our case.",
                    "label": 0
                },
                {
                    "sent": "And of course, for larger amounts of data, then let's re estimate the parameters.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I put the formula here so that you don't forget it so that you can all have any side of this big thing.",
                    "label": 0
                },
                {
                    "sent": "As for future work.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you look at this integral here.",
                    "label": 0
                },
                {
                    "sent": "And Z normalization constant Z normalization constant is not taken into account since what we intend to do is a decision rule which is a maximum operation argmax operation on this.",
                    "label": 0
                },
                {
                    "sent": "Does he constant is forgotten completely?",
                    "label": 0
                },
                {
                    "sent": "Taken outside.",
                    "label": 0
                },
                {
                    "sent": "What does this mean?",
                    "label": 0
                },
                {
                    "sent": "This means that sends no days notice.",
                    "label": 0
                },
                {
                    "sent": "This is no notice.",
                    "label": 0
                },
                {
                    "sent": "Notice.",
                    "label": 0
                },
                {
                    "sent": "Neither of them are probabilities, because the normalizations have been taken out.",
                    "label": 0
                },
                {
                    "sent": "They may have very different numeric ranges, which may imply that maybe in this product, if the amount of adaptation data increases.",
                    "label": 1
                },
                {
                    "sent": "This term here will drop.",
                    "label": 0
                },
                {
                    "sent": "Will drop quite heavily and will have and in the long run absolutely no effect on the final integral.",
                    "label": 1
                },
                {
                    "sent": "And hence the effect of adapting the system may fade completely.",
                    "label": 0
                },
                {
                    "sent": "So the idea we had is introduced here.",
                    "label": 0
                },
                {
                    "sent": "An exponent exponent, which would be the same here and another one here.",
                    "label": 0
                },
                {
                    "sent": "So Delta Delta 1 minus Delta in order to leverage these terms and return them into the same numeric ranges.",
                    "label": 0
                },
                {
                    "sent": "Of course this steps quite a lot aside the theory, but it has been proven to behave well.",
                    "label": 1
                },
                {
                    "sent": "Another in other fields.",
                    "label": 0
                },
                {
                    "sent": "And this would imply a sort of compromise between stability and improvement potential.",
                    "label": 0
                },
                {
                    "sent": "Of course, if the if the exponent went through, here is very high.",
                    "label": 0
                },
                {
                    "sent": "Then is magnifies the impact a lot?",
                    "label": 1
                },
                {
                    "sent": "Then it will turn unstable into it.",
                    "label": 0
                },
                {
                    "sent": "Will biased too much towards the adaptation data.",
                    "label": 0
                },
                {
                    "sent": "We also intend to do to perform a 2 Monte Carlo sampling on the on the sampling over on the integral instead of using random sampling we used.",
                    "label": 0
                },
                {
                    "sent": "We intend to perform 2 Monte Carlo sampling.",
                    "label": 0
                },
                {
                    "sent": "And finally, we also intend to extend this formulation in order to adapt these feature functions instead of the set of the log linear weights we intend to adapt the feature functions.",
                    "label": 0
                },
                {
                    "sent": "Allowed that is still working progress everything.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's all.",
                    "label": 0
                },
                {
                    "sent": "If you would have any questions or suggestions.",
                    "label": 0
                },
                {
                    "sent": "Thank you for the presentation.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "You said that you use some prior knowledge on.",
                    "label": 0
                },
                {
                    "sent": "My question is, how do you learn the prior parameters?",
                    "label": 0
                },
                {
                    "sent": "OK, so as I said, the incestral machine translation.",
                    "label": 0
                },
                {
                    "sent": "These parameters are typically trained by using our development set.",
                    "label": 1
                },
                {
                    "sent": "So what we did is to use the development set of the Europol data so the out of the main data in order to estimate the this prior.",
                    "label": 1
                },
                {
                    "sent": "This well if in that case the development data is there a pool.",
                    "label": 0
                },
                {
                    "sent": "And that would.",
                    "label": 0
                },
                {
                    "sent": "Give us.",
                    "label": 0
                },
                {
                    "sent": "The estimation for the list training Theta, which is the average of which is the mean of this quotient, and that would be the prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "Your prior parameters.",
                    "label": 0
                },
                {
                    "sent": "Actually, during the application of your system are really different for.",
                    "label": 0
                },
                {
                    "sent": "Yes, in some cases they are relatively different.",
                    "label": 0
                },
                {
                    "sent": "Why do you think?",
                    "label": 0
                },
                {
                    "sent": "Are so different?",
                    "label": 0
                },
                {
                    "sent": "Well, one reason, for instance, is the sentence length and something I did not say, but which is true about statistical machine translation state of the art systems is that they introduce a word penalty and a phrase penalty which is.",
                    "label": 0
                },
                {
                    "sent": "If the sentence becomes too long, typically.",
                    "label": 0
                },
                {
                    "sent": "It loses probability because it's a product.",
                    "label": 0
                },
                {
                    "sent": "So in order to compensate this fact, there is a leveraging term which enables that this effect is not so strict.",
                    "label": 0
                },
                {
                    "sent": "This sentence, this phrase, or word penalty, which which is not really a penalty but a bonus.",
                    "label": 0
                },
                {
                    "sent": "It also has a weight.",
                    "label": 0
                },
                {
                    "sent": "So of course, if the the main changes if the corpus changes, the average sentence length also changes sometime, in some cases dramatically so.",
                    "label": 0
                },
                {
                    "sent": "This weight, for instance, is changed quite a lot.",
                    "label": 0
                },
                {
                    "sent": "And maybe also the language model Weight loses potential versus the translation model weight or the distortion model.",
                    "label": 0
                },
                {
                    "sent": "OK, this is another question, mines very simple.",
                    "label": 0
                },
                {
                    "sent": "It's why is it so obvious that the.",
                    "label": 0
                },
                {
                    "sent": "System.",
                    "label": 0
                },
                {
                    "sent": "Convergence of the adaptation is not converged.",
                    "label": 1
                },
                {
                    "sent": "You don't have to impose some conditions on the number of.",
                    "label": 0
                },
                {
                    "sent": "Each step, the number of actual examples need to draw to be able to.",
                    "label": 0
                },
                {
                    "sent": "Do you mean this convergence here or?",
                    "label": 0
                },
                {
                    "sent": "App.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK this year.",
                    "label": 0
                },
                {
                    "sent": "Well, this is.",
                    "label": 0
                },
                {
                    "sent": "This is more state of the art machine translation and my work, but I can explain it anyway.",
                    "label": 0
                },
                {
                    "sent": "While the amount of hypothesis the system is able to deliver is finite, of course.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "There is one of the hypothesis which is the best.",
                    "label": 0
                },
                {
                    "sent": "So in the long run, after several iterations.",
                    "label": 0
                },
                {
                    "sent": "You will have seen enough different Lambda samples in order to have seen the best hypothesis in the different.",
                    "label": 0
                },
                {
                    "sent": "Cases.",
                    "label": 0
                },
                {
                    "sent": "So that answer your question or not really.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Maybe if you want I can refer you to the papers of 2000, four 2003 of Ohio and others who did this work.",
                    "label": 0
                },
                {
                    "sent": "OK, so this finishes the presentation.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you.",
                    "label": 0
                }
            ]
        }
    }
}