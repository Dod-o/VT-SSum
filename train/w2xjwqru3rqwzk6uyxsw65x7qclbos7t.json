{
    "id": "w2xjwqru3rqwzk6uyxsw65x7qclbos7t",
    "title": "Automatic Labeling of Multinomial Topic Models",
    "info": {
        "author": [
            "Qiaozhu Mei, University of Illinois at Urbana-Champaign"
        ],
        "published": "Aug. 14, 2007",
        "recorded": "August 2007",
        "category": [
            "Top->Computer Science->Text Mining",
            "Top->Computer Science->Machine Learning->Human Language Technology"
        ]
    },
    "url": "http://videolectures.net/kdd07_mei_alm/",
    "segmentation": [
        [
            "OK hello everyone, my name is Georgia May I am a PhD student with University of Illinois Anapana champagne and this is actually a joint work with share passion and my analyzer chance I think."
        ],
        [
            "So as the outline of this talk, I will first give an introduction about the background for examples of statistic topic models.",
            "And then I will introduce why we want to label such a topic model with the criteria of a good label and the challenge of this task.",
            "After that I will introduce our approach, which is the probabilistic framework to label polynomial topic models followed by our experiments and then."
        ],
        [
            "Henry.",
            "So let's first look under Statistique topic models for text mining.",
            "The idea is actually quite simple given a collection of texts where will use some probabilistic topic models to fit the data, which will give us a number of topic models.",
            "Each topic model is essentially a multinomial distribution of the words.",
            "For example, in this topic model you can see each word is associated with the probability, and you can rank those terms with probability.",
            "You can also get other topic models with different words ranked atop an all such topic.",
            "Models can be used to discover the subtopics to analyze the topic patterns.",
            "For example, the change of topics over time and location.",
            "And we can also use them to summarize disk as well as compare the appearance in the text.",
            "And indeed, there's a large body of works on the topic modeling, including two most famous work, softens.",
            "Peers, is a an David plays LDA and recently there is also a large body of extensions to the basic models.",
            "Actually, this morning we also have another property model by college men universe."
        ],
        [
            "So.",
            "However, there is the problem that all such topic model suffers one, which is it is really hard to interpret the meaning of a topic model.",
            "So given our public distribution like this, it's really hard to guess the meaning.",
            "If you don't have a background from information retrieval.",
            "So most work use the top words in the probability distribution to label the topic.",
            "Of course this is automatic, but still it's very hard to make sense and instead some people will use human generated labels to label the topic.",
            "This makes sense, but this cannot be scaled up.",
            "If we look at the right on column, which is the top extracted from the biology literature, we will understand how hard it is to generate a good label.",
            "An hour question is, can we automatically generate such meaningful labels for us?"
        ],
        [
            "Topic model.",
            "So to answer this question, we should first learn that what is the good label for topic?",
            "Let's use the same example.",
            "We believe that retrieval models this phrase is a good label for this topic distribution.",
            "And why is that?",
            "We believe that a good label of a topic should first be semantically close to the meaning of the topic itself, which means that we cannot use the phrase I, Fernando, to label this topic.",
            "Then, uh, we expect that a good table should be understandable for the common audience, and that's why we use phrase the retrieval model to label this topic.",
            "Some models, some labels are reasonable, but are not understandable, audience want to avoid it.",
            "And the third criteria is that we expect a good label to cover the information in this topic distribution as much as possible.",
            "Although this topic distribution indeed mentioned something about feedback, we don't want to use the phrase social feedback to label this topic because it only partially covers the topic.",
            "And finally, since every public model outputs more than one topics from the same collection of text, we expect the good label to be discriminated across these topics.",
            "We don't want to use a general phrase like information retrieval to unlabeled this topic, although it's relevant because all topics extracted from signal processing."
        ],
        [
            "About the information table.",
            "So now we know that a good label should be understandable.",
            "Should be relevant should be high courage inside topic and should be discriminative across topics list can all measure to generate such good labels given a collection of text.",
            "For example cigar proceeding, we will have a number of topics and if we want to label this topic distribution, the first step we want to do is to generate a pool of candidate labels which are guaranteed to be understandable and our approach is to use an LP, trunker or.",
            "N gram statistic tool to extract the meaningful phrases and use these phrases to be candid labels.",
            "Once we have this pool of candid labels, we will introduce a relevance score which will give us are activist of this candidate.",
            "Labels to this topic.",
            "So a phrase ranked at the top should be more relevant to this topic.",
            "Once we have this ranked list, the next step, the third task is to guarantee the discrimination of of this ranked list to this topic.",
            "Specifically, we will discount the phrases which are too general and push them to the bottom of this list.",
            "And then the next step is to guarantee the high coverage of the top ranked phrases to the topic.",
            "So specifically we will discount the phrases which are redundant to the labels which are ranked above it and push this to the bottom.",
            "For example, in this case the phrase IR model is redundant to retrieval models.",
            "You want to push it back.",
            "And finally, we will give the reasonable ranked list of labels on the user can select either the top one label on top, several labels to label this."
        ],
        [
            "Distribution.",
            "So since we are using the previous standard method for the first task, we want to focus on the 2nd, 3rd and 4th task, so to measure the relevance of our candidate label and the topic model, we introduce two different measures of relevance.",
            "The first one is called their_The intuition is quite simple if we can rent the words in the vocabulary based on the probability of this word in this topic distribution.",
            "In this case we use the size.",
            "Of the cycle to indicate the probability.",
            "We expect a good level 2 will cover the top ranked terms, so in this case the phrase clustering algorithm is the good label because it will covers the Top Rank terms and the label on body shape is not a good label 'cause the world it covers all ranked loading.",
            "And this relevance can be easily captured by the normalized likelihood of this phrase to be generated from this topic distribution.",
            "So in this case, we know that clustering algorithms are good one.",
            "But the zero_has some problems because it only covers several top terms and we don't know what about other words.",
            "Can they be covered by this label with no idea?"
        ],
        [
            "So to solve this challenge, we introduce another relevant score which is called the 1st order score.",
            "The intuition is separate.",
            "Instead of looking for a label which will cover the only a few of the top terms, we want to find a label which will cover the whole distribution.",
            "So suppose we can come up with another model distribution from the label itself.",
            "We can easily compute the relevance of the label to the topic by comparing those two distributions.",
            "And can we achieve that?",
            "Given a label like this, the hash join.",
            "We can look at the context of on this phrase in some tax collections, like the SIGMOD proceedings.",
            "If we look at the words Co occurring with this phrase, we can easily estimate another multinomial distribution which we call the context distribution of the label as joint.",
            "And if we can't compare these two distributions, we see that they are quite different, which indicates that her showing may not be a good label for this topic.",
            "Whereas for this label clustering algorithm, we can also estimate a context distribution from the world preparing with this label, and we see that these two distributions are quite similar, which means that clustering accent is much better label.",
            "Informally, the score this relevance score can be written as the care divergent of the on topic word distribution and the context distribution of the label.",
            "With some realisation, we can rank the candidates labels with the expected pointwise mutual information of the label and the topic.",
            "So if you are interested about detail, you can either come to our poster tonight an or."
        ],
        [
            "In our paper.",
            "So then we want to worry about the task three and four.",
            "How can we guarantee that the label is discriminative across topic as well as a high coverage inside the topic as we introduced that?",
            "To guarantee a unlabel is discriminating across topic, we want to have this label to have high relevance to the target public as well as no relevance to other topics.",
            "Then we can use this re formalized scoring function and to guarantee that topic has high coverage inside the topic, we can as well use the Merge strategy.",
            "The basic idea is simple, so when we want to select a label two we will.",
            "Both guarantee that this label has the largest relevance to the topic as well as the smallest redundancy to the labels we already selected, and we're actually using this formula."
        ],
        [
            "OK, so now that we know that how we can generate meaningful label, this slide shows that our method is not only useful to label a multi nomial distribution of words.",
            "Actually some variations of our can be used to label a document cluster.",
            "This is because from a document cluster we can easily estimate unigram language model or essentially a multinomial distribution upwards.",
            "And we can use our method to generate meaningful labels for this model distribution, which can be also used as label for original document clusters, and indeed our method is applicable to any tasks with unigram language model involved.",
            "So on the other hand, our method could also be used to generate content context sensitive labels, which is essentially using different collection of texts as the context.",
            "This is because the label of the topic is sensitive to the context where different people can use from different backgrounds can use different labels for the same topic.",
            "For example, when we see a topic with this top words, a computer science person will.",
            "Label this topic as Tree Adams.",
            "But what about people in horticulture?",
            "They may have different labels.",
            "And what about marketing people?",
            "The words like Rudan branch may have totally different meaning to marketing people, which means that we can use our method as now positive way to approach contextual text mining."
        ],
        [
            "So to evaluate our approach with desire or experiments with three different datasets, the sigmoid asked abstract, cigar abstract, and AP news data, and we used candidate labels as significant bigrams and LB chunks.",
            "We apply our accent on top of two unfamous topic models.",
            "A, an LDA and we ask human allocators to compare the labels generated from anonymous systems randomly perturbed."
        ],
        [
            "So here are our findings from the experiments.",
            "First, based on the human judge of the automatic generated space, labels are much better than the top words in terms of catching the meaning of a topic model.",
            "And if we use the 1st order relevance, we got far better results than if we use the zero order relevance.",
            "We also notice that.",
            "In general bigrams outperformed LP trends in terms of in terms of generating the cat labels.",
            "And the background actually works better with literature data.",
            "While LG Trans Works a little bit better with use data.",
            "We also notice that the system generated labels are still not as good as human generated labels in general, although in many cases they are really competitive and this.",
            "This gap is much smaller when we are dealing with scientific literature, which means that labeling topics in scientific literature is an easier task than labeling topic."
        ],
        [
            "Can you sleep?",
            "And in this slide, we show that on some sample topic labels.",
            "For example, this topic distribution is extracted from the AP News data, which is labeled as Iran.",
            "Contra Iran Contra is actually an event happened to me years ago.",
            "It's interesting to say that word counter doesn't necessarily appear in the Top Rank terms.",
            "And this topic is extracted from sigmoid with with LDA.",
            "If we don't prove the stop words, we will see that zero order relevance will give us many labels which are too general and dominated by the top words.",
            "But first order relevance will still give us meaningful labels such as clustering algorithm.",
            "And finally, this topic is generated from Sigma two which was labeled as Archie and B tree there meaningful labels, but if we look at the gold standard creative vacuum and we see that the human can generate even better labels such as indexing measures, which will generalize this Archie NBG if we want to get such label automatically.",
            "We need at least consider even higher, higher order relevant."
        ],
        [
            "And this slide shows the results of the context sensitive labeling.",
            "If we want to label this topic model from the context of database, we will have labels such as selectivity, estimation, random sampling and approximate answers which are meaningful if we want to label this topic from the context of information table, we will get much different results such as distributed retrieval, parameter estimation and mixture models.",
            "Interestingly, these are the word mixture or model.",
            "Appears in the top distribution, but indeed when people are talking about mixture models, information retrieval that tends to mention the parameter estimation approximation and simply measured.",
            "So this shows that indeed our method can explore the different meanings of a topic with different contexts, and we can use it as an alternative."
        ],
        [
            "Approach to Contacts or text mining.",
            "Summer.",
            "We introduced automatic labeling of multinomial topic models, which is the processing post processing step of all statistique topic models.",
            "We introduced a probabilistic approach to generate good labels which are understandable, relevant, high courage, and discriminate if we expect this work to be broadly applicable to any mining test involving multinomial distributions, and some variations can generate labels for clusters or context.",
            "And in the future of our work, we want to first look at how we can label hierarchical topic models as well as how we can incorporate priors into this living process.",
            "For example, can we make use of the gene ontology to label them?"
        ],
        [
            "Biology topics.",
            "So thank you and please come to our poster tonight and start number body.",
            "Perimeter.",
            "Voice, yeah, so The thing is, all the parameter is or if you wait the prices on parameters then you can come.",
            "If you want the labels for being more discriminating you is satisfied for the power edge.",
            "It changes with the vintages literature or use data by the way.",
            "Things are different data.",
            "It won't change much.",
            "OK, let's fix it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK hello everyone, my name is Georgia May I am a PhD student with University of Illinois Anapana champagne and this is actually a joint work with share passion and my analyzer chance I think.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as the outline of this talk, I will first give an introduction about the background for examples of statistic topic models.",
                    "label": 0
                },
                {
                    "sent": "And then I will introduce why we want to label such a topic model with the criteria of a good label and the challenge of this task.",
                    "label": 1
                },
                {
                    "sent": "After that I will introduce our approach, which is the probabilistic framework to label polynomial topic models followed by our experiments and then.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Henry.",
                    "label": 0
                },
                {
                    "sent": "So let's first look under Statistique topic models for text mining.",
                    "label": 1
                },
                {
                    "sent": "The idea is actually quite simple given a collection of texts where will use some probabilistic topic models to fit the data, which will give us a number of topic models.",
                    "label": 0
                },
                {
                    "sent": "Each topic model is essentially a multinomial distribution of the words.",
                    "label": 0
                },
                {
                    "sent": "For example, in this topic model you can see each word is associated with the probability, and you can rank those terms with probability.",
                    "label": 0
                },
                {
                    "sent": "You can also get other topic models with different words ranked atop an all such topic.",
                    "label": 0
                },
                {
                    "sent": "Models can be used to discover the subtopics to analyze the topic patterns.",
                    "label": 1
                },
                {
                    "sent": "For example, the change of topics over time and location.",
                    "label": 0
                },
                {
                    "sent": "And we can also use them to summarize disk as well as compare the appearance in the text.",
                    "label": 0
                },
                {
                    "sent": "And indeed, there's a large body of works on the topic modeling, including two most famous work, softens.",
                    "label": 0
                },
                {
                    "sent": "Peers, is a an David plays LDA and recently there is also a large body of extensions to the basic models.",
                    "label": 0
                },
                {
                    "sent": "Actually, this morning we also have another property model by college men universe.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "However, there is the problem that all such topic model suffers one, which is it is really hard to interpret the meaning of a topic model.",
                    "label": 0
                },
                {
                    "sent": "So given our public distribution like this, it's really hard to guess the meaning.",
                    "label": 0
                },
                {
                    "sent": "If you don't have a background from information retrieval.",
                    "label": 0
                },
                {
                    "sent": "So most work use the top words in the probability distribution to label the topic.",
                    "label": 0
                },
                {
                    "sent": "Of course this is automatic, but still it's very hard to make sense and instead some people will use human generated labels to label the topic.",
                    "label": 1
                },
                {
                    "sent": "This makes sense, but this cannot be scaled up.",
                    "label": 0
                },
                {
                    "sent": "If we look at the right on column, which is the top extracted from the biology literature, we will understand how hard it is to generate a good label.",
                    "label": 1
                },
                {
                    "sent": "An hour question is, can we automatically generate such meaningful labels for us?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Topic model.",
                    "label": 0
                },
                {
                    "sent": "So to answer this question, we should first learn that what is the good label for topic?",
                    "label": 0
                },
                {
                    "sent": "Let's use the same example.",
                    "label": 0
                },
                {
                    "sent": "We believe that retrieval models this phrase is a good label for this topic distribution.",
                    "label": 1
                },
                {
                    "sent": "And why is that?",
                    "label": 0
                },
                {
                    "sent": "We believe that a good label of a topic should first be semantically close to the meaning of the topic itself, which means that we cannot use the phrase I, Fernando, to label this topic.",
                    "label": 0
                },
                {
                    "sent": "Then, uh, we expect that a good table should be understandable for the common audience, and that's why we use phrase the retrieval model to label this topic.",
                    "label": 0
                },
                {
                    "sent": "Some models, some labels are reasonable, but are not understandable, audience want to avoid it.",
                    "label": 0
                },
                {
                    "sent": "And the third criteria is that we expect a good label to cover the information in this topic distribution as much as possible.",
                    "label": 0
                },
                {
                    "sent": "Although this topic distribution indeed mentioned something about feedback, we don't want to use the phrase social feedback to label this topic because it only partially covers the topic.",
                    "label": 0
                },
                {
                    "sent": "And finally, since every public model outputs more than one topics from the same collection of text, we expect the good label to be discriminated across these topics.",
                    "label": 0
                },
                {
                    "sent": "We don't want to use a general phrase like information retrieval to unlabeled this topic, although it's relevant because all topics extracted from signal processing.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About the information table.",
                    "label": 0
                },
                {
                    "sent": "So now we know that a good label should be understandable.",
                    "label": 0
                },
                {
                    "sent": "Should be relevant should be high courage inside topic and should be discriminative across topics list can all measure to generate such good labels given a collection of text.",
                    "label": 0
                },
                {
                    "sent": "For example cigar proceeding, we will have a number of topics and if we want to label this topic distribution, the first step we want to do is to generate a pool of candidate labels which are guaranteed to be understandable and our approach is to use an LP, trunker or.",
                    "label": 0
                },
                {
                    "sent": "N gram statistic tool to extract the meaningful phrases and use these phrases to be candid labels.",
                    "label": 0
                },
                {
                    "sent": "Once we have this pool of candid labels, we will introduce a relevance score which will give us are activist of this candidate.",
                    "label": 0
                },
                {
                    "sent": "Labels to this topic.",
                    "label": 0
                },
                {
                    "sent": "So a phrase ranked at the top should be more relevant to this topic.",
                    "label": 0
                },
                {
                    "sent": "Once we have this ranked list, the next step, the third task is to guarantee the discrimination of of this ranked list to this topic.",
                    "label": 0
                },
                {
                    "sent": "Specifically, we will discount the phrases which are too general and push them to the bottom of this list.",
                    "label": 0
                },
                {
                    "sent": "And then the next step is to guarantee the high coverage of the top ranked phrases to the topic.",
                    "label": 0
                },
                {
                    "sent": "So specifically we will discount the phrases which are redundant to the labels which are ranked above it and push this to the bottom.",
                    "label": 0
                },
                {
                    "sent": "For example, in this case the phrase IR model is redundant to retrieval models.",
                    "label": 0
                },
                {
                    "sent": "You want to push it back.",
                    "label": 0
                },
                {
                    "sent": "And finally, we will give the reasonable ranked list of labels on the user can select either the top one label on top, several labels to label this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distribution.",
                    "label": 0
                },
                {
                    "sent": "So since we are using the previous standard method for the first task, we want to focus on the 2nd, 3rd and 4th task, so to measure the relevance of our candidate label and the topic model, we introduce two different measures of relevance.",
                    "label": 0
                },
                {
                    "sent": "The first one is called their_The intuition is quite simple if we can rent the words in the vocabulary based on the probability of this word in this topic distribution.",
                    "label": 0
                },
                {
                    "sent": "In this case we use the size.",
                    "label": 0
                },
                {
                    "sent": "Of the cycle to indicate the probability.",
                    "label": 0
                },
                {
                    "sent": "We expect a good level 2 will cover the top ranked terms, so in this case the phrase clustering algorithm is the good label because it will covers the Top Rank terms and the label on body shape is not a good label 'cause the world it covers all ranked loading.",
                    "label": 1
                },
                {
                    "sent": "And this relevance can be easily captured by the normalized likelihood of this phrase to be generated from this topic distribution.",
                    "label": 0
                },
                {
                    "sent": "So in this case, we know that clustering algorithms are good one.",
                    "label": 0
                },
                {
                    "sent": "But the zero_has some problems because it only covers several top terms and we don't know what about other words.",
                    "label": 0
                },
                {
                    "sent": "Can they be covered by this label with no idea?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to solve this challenge, we introduce another relevant score which is called the 1st order score.",
                    "label": 0
                },
                {
                    "sent": "The intuition is separate.",
                    "label": 0
                },
                {
                    "sent": "Instead of looking for a label which will cover the only a few of the top terms, we want to find a label which will cover the whole distribution.",
                    "label": 0
                },
                {
                    "sent": "So suppose we can come up with another model distribution from the label itself.",
                    "label": 0
                },
                {
                    "sent": "We can easily compute the relevance of the label to the topic by comparing those two distributions.",
                    "label": 0
                },
                {
                    "sent": "And can we achieve that?",
                    "label": 0
                },
                {
                    "sent": "Given a label like this, the hash join.",
                    "label": 1
                },
                {
                    "sent": "We can look at the context of on this phrase in some tax collections, like the SIGMOD proceedings.",
                    "label": 0
                },
                {
                    "sent": "If we look at the words Co occurring with this phrase, we can easily estimate another multinomial distribution which we call the context distribution of the label as joint.",
                    "label": 1
                },
                {
                    "sent": "And if we can't compare these two distributions, we see that they are quite different, which indicates that her showing may not be a good label for this topic.",
                    "label": 0
                },
                {
                    "sent": "Whereas for this label clustering algorithm, we can also estimate a context distribution from the world preparing with this label, and we see that these two distributions are quite similar, which means that clustering accent is much better label.",
                    "label": 1
                },
                {
                    "sent": "Informally, the score this relevance score can be written as the care divergent of the on topic word distribution and the context distribution of the label.",
                    "label": 0
                },
                {
                    "sent": "With some realisation, we can rank the candidates labels with the expected pointwise mutual information of the label and the topic.",
                    "label": 0
                },
                {
                    "sent": "So if you are interested about detail, you can either come to our poster tonight an or.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our paper.",
                    "label": 0
                },
                {
                    "sent": "So then we want to worry about the task three and four.",
                    "label": 0
                },
                {
                    "sent": "How can we guarantee that the label is discriminative across topic as well as a high coverage inside the topic as we introduced that?",
                    "label": 1
                },
                {
                    "sent": "To guarantee a unlabel is discriminating across topic, we want to have this label to have high relevance to the target public as well as no relevance to other topics.",
                    "label": 1
                },
                {
                    "sent": "Then we can use this re formalized scoring function and to guarantee that topic has high coverage inside the topic, we can as well use the Merge strategy.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is simple, so when we want to select a label two we will.",
                    "label": 0
                },
                {
                    "sent": "Both guarantee that this label has the largest relevance to the topic as well as the smallest redundancy to the labels we already selected, and we're actually using this formula.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now that we know that how we can generate meaningful label, this slide shows that our method is not only useful to label a multi nomial distribution of words.",
                    "label": 0
                },
                {
                    "sent": "Actually some variations of our can be used to label a document cluster.",
                    "label": 0
                },
                {
                    "sent": "This is because from a document cluster we can easily estimate unigram language model or essentially a multinomial distribution upwards.",
                    "label": 1
                },
                {
                    "sent": "And we can use our method to generate meaningful labels for this model distribution, which can be also used as label for original document clusters, and indeed our method is applicable to any tasks with unigram language model involved.",
                    "label": 1
                },
                {
                    "sent": "So on the other hand, our method could also be used to generate content context sensitive labels, which is essentially using different collection of texts as the context.",
                    "label": 0
                },
                {
                    "sent": "This is because the label of the topic is sensitive to the context where different people can use from different backgrounds can use different labels for the same topic.",
                    "label": 1
                },
                {
                    "sent": "For example, when we see a topic with this top words, a computer science person will.",
                    "label": 0
                },
                {
                    "sent": "Label this topic as Tree Adams.",
                    "label": 0
                },
                {
                    "sent": "But what about people in horticulture?",
                    "label": 0
                },
                {
                    "sent": "They may have different labels.",
                    "label": 1
                },
                {
                    "sent": "And what about marketing people?",
                    "label": 0
                },
                {
                    "sent": "The words like Rudan branch may have totally different meaning to marketing people, which means that we can use our method as now positive way to approach contextual text mining.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to evaluate our approach with desire or experiments with three different datasets, the sigmoid asked abstract, cigar abstract, and AP news data, and we used candidate labels as significant bigrams and LB chunks.",
                    "label": 0
                },
                {
                    "sent": "We apply our accent on top of two unfamous topic models.",
                    "label": 0
                },
                {
                    "sent": "A, an LDA and we ask human allocators to compare the labels generated from anonymous systems randomly perturbed.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are our findings from the experiments.",
                    "label": 0
                },
                {
                    "sent": "First, based on the human judge of the automatic generated space, labels are much better than the top words in terms of catching the meaning of a topic model.",
                    "label": 0
                },
                {
                    "sent": "And if we use the 1st order relevance, we got far better results than if we use the zero order relevance.",
                    "label": 0
                },
                {
                    "sent": "We also notice that.",
                    "label": 0
                },
                {
                    "sent": "In general bigrams outperformed LP trends in terms of in terms of generating the cat labels.",
                    "label": 0
                },
                {
                    "sent": "And the background actually works better with literature data.",
                    "label": 1
                },
                {
                    "sent": "While LG Trans Works a little bit better with use data.",
                    "label": 0
                },
                {
                    "sent": "We also notice that the system generated labels are still not as good as human generated labels in general, although in many cases they are really competitive and this.",
                    "label": 0
                },
                {
                    "sent": "This gap is much smaller when we are dealing with scientific literature, which means that labeling topics in scientific literature is an easier task than labeling topic.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can you sleep?",
                    "label": 0
                },
                {
                    "sent": "And in this slide, we show that on some sample topic labels.",
                    "label": 1
                },
                {
                    "sent": "For example, this topic distribution is extracted from the AP News data, which is labeled as Iran.",
                    "label": 1
                },
                {
                    "sent": "Contra Iran Contra is actually an event happened to me years ago.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to say that word counter doesn't necessarily appear in the Top Rank terms.",
                    "label": 0
                },
                {
                    "sent": "And this topic is extracted from sigmoid with with LDA.",
                    "label": 0
                },
                {
                    "sent": "If we don't prove the stop words, we will see that zero order relevance will give us many labels which are too general and dominated by the top words.",
                    "label": 0
                },
                {
                    "sent": "But first order relevance will still give us meaningful labels such as clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "And finally, this topic is generated from Sigma two which was labeled as Archie and B tree there meaningful labels, but if we look at the gold standard creative vacuum and we see that the human can generate even better labels such as indexing measures, which will generalize this Archie NBG if we want to get such label automatically.",
                    "label": 0
                },
                {
                    "sent": "We need at least consider even higher, higher order relevant.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this slide shows the results of the context sensitive labeling.",
                    "label": 0
                },
                {
                    "sent": "If we want to label this topic model from the context of database, we will have labels such as selectivity, estimation, random sampling and approximate answers which are meaningful if we want to label this topic from the context of information table, we will get much different results such as distributed retrieval, parameter estimation and mixture models.",
                    "label": 1
                },
                {
                    "sent": "Interestingly, these are the word mixture or model.",
                    "label": 0
                },
                {
                    "sent": "Appears in the top distribution, but indeed when people are talking about mixture models, information retrieval that tends to mention the parameter estimation approximation and simply measured.",
                    "label": 0
                },
                {
                    "sent": "So this shows that indeed our method can explore the different meanings of a topic with different contexts, and we can use it as an alternative.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approach to Contacts or text mining.",
                    "label": 0
                },
                {
                    "sent": "Summer.",
                    "label": 0
                },
                {
                    "sent": "We introduced automatic labeling of multinomial topic models, which is the processing post processing step of all statistique topic models.",
                    "label": 1
                },
                {
                    "sent": "We introduced a probabilistic approach to generate good labels which are understandable, relevant, high courage, and discriminate if we expect this work to be broadly applicable to any mining test involving multinomial distributions, and some variations can generate labels for clusters or context.",
                    "label": 1
                },
                {
                    "sent": "And in the future of our work, we want to first look at how we can label hierarchical topic models as well as how we can incorporate priors into this living process.",
                    "label": 0
                },
                {
                    "sent": "For example, can we make use of the gene ontology to label them?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Biology topics.",
                    "label": 0
                },
                {
                    "sent": "So thank you and please come to our poster tonight and start number body.",
                    "label": 1
                },
                {
                    "sent": "Perimeter.",
                    "label": 0
                },
                {
                    "sent": "Voice, yeah, so The thing is, all the parameter is or if you wait the prices on parameters then you can come.",
                    "label": 0
                },
                {
                    "sent": "If you want the labels for being more discriminating you is satisfied for the power edge.",
                    "label": 0
                },
                {
                    "sent": "It changes with the vintages literature or use data by the way.",
                    "label": 0
                },
                {
                    "sent": "Things are different data.",
                    "label": 0
                },
                {
                    "sent": "It won't change much.",
                    "label": 0
                },
                {
                    "sent": "OK, let's fix it.",
                    "label": 0
                }
            ]
        }
    }
}