{
    "id": "ob3h75naefj3mdioulux4zvogpmgw4o4",
    "title": "Information Theoretical Kernels for Generative Embeddings Based on Hidden Markov Models",
    "info": {
        "author": [
            "Manuele Bicego, University of Verona"
        ],
        "published": "Sept. 13, 2010",
        "recorded": "August 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/ssspr2010_bicego_itk/",
    "segmentation": [
        [
            "Hey, my name is Amalia Bituin from University of Verona.",
            "This is work done jointly with the Group of Matter Figueredo in Lisbon and the main context of this work is a generative embedding, so generative embeddings are can be considered as hybrid metal so their meters able to merge the descriptive capabilities of generative models with discriminative capabilities of discriminative approach.",
            "And there is this so we have a set of objects sequences in this case.",
            "And then we have a generative model and through this generative model we are projecting these objects into a feature space and this feature space is called generative embedded space.",
            "So in this way we can have a Victoria representation from objects we can be possibly not Victoria.",
            "This generative embedding involved 3 three steps.",
            "So the first is to learn a generative model, the second is to define the mapping and the third one is just to.",
            "Computer similarity, individual space.",
            "So the idea is that the idea of this poster is that in most of the cases, the main focuses in on this step the mapping step.",
            "But here we focus on the third step, so we try to find out a reasonable similarity measure and this has been done with the information theoretic kernels which are.",
            "Recently introduced by the Group of Amanda Figueredo, and here, the idea is that we can compute the distance between two probability distributions and in the colors they introduced, they just use the Jensen solace entropy, which is a generalization of the Shannon entropy.",
            "And with this new measure in the new space in the industry in the generative space, then we can a little bit improve.",
            "The performance is over the of the generative embeddings, and this is what we have shown in our experiment.",
            "Experimental evaluation, and we did some experiment with the Shape recognition task within HMM and we used for different generative embeddings.",
            "You can find all the details in the paper and the idea is that we compare the simple linear kernel in this space with the all these kernels and the idea is that we can have quite impressive improvements in some cases, so they take home messages that if you are using generative embedding so don't focus only on the mapping stash but also on the similarity.",
            "Finishing the future space.",
            "So this is what we have shown here."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hey, my name is Amalia Bituin from University of Verona.",
                    "label": 1
                },
                {
                    "sent": "This is work done jointly with the Group of Matter Figueredo in Lisbon and the main context of this work is a generative embedding, so generative embeddings are can be considered as hybrid metal so their meters able to merge the descriptive capabilities of generative models with discriminative capabilities of discriminative approach.",
                    "label": 1
                },
                {
                    "sent": "And there is this so we have a set of objects sequences in this case.",
                    "label": 1
                },
                {
                    "sent": "And then we have a generative model and through this generative model we are projecting these objects into a feature space and this feature space is called generative embedded space.",
                    "label": 1
                },
                {
                    "sent": "So in this way we can have a Victoria representation from objects we can be possibly not Victoria.",
                    "label": 0
                },
                {
                    "sent": "This generative embedding involved 3 three steps.",
                    "label": 0
                },
                {
                    "sent": "So the first is to learn a generative model, the second is to define the mapping and the third one is just to.",
                    "label": 0
                },
                {
                    "sent": "Computer similarity, individual space.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that the idea of this poster is that in most of the cases, the main focuses in on this step the mapping step.",
                    "label": 1
                },
                {
                    "sent": "But here we focus on the third step, so we try to find out a reasonable similarity measure and this has been done with the information theoretic kernels which are.",
                    "label": 0
                },
                {
                    "sent": "Recently introduced by the Group of Amanda Figueredo, and here, the idea is that we can compute the distance between two probability distributions and in the colors they introduced, they just use the Jensen solace entropy, which is a generalization of the Shannon entropy.",
                    "label": 1
                },
                {
                    "sent": "And with this new measure in the new space in the industry in the generative space, then we can a little bit improve.",
                    "label": 1
                },
                {
                    "sent": "The performance is over the of the generative embeddings, and this is what we have shown in our experiment.",
                    "label": 1
                },
                {
                    "sent": "Experimental evaluation, and we did some experiment with the Shape recognition task within HMM and we used for different generative embeddings.",
                    "label": 0
                },
                {
                    "sent": "You can find all the details in the paper and the idea is that we compare the simple linear kernel in this space with the all these kernels and the idea is that we can have quite impressive improvements in some cases, so they take home messages that if you are using generative embedding so don't focus only on the mapping stash but also on the similarity.",
                    "label": 0
                },
                {
                    "sent": "Finishing the future space.",
                    "label": 0
                },
                {
                    "sent": "So this is what we have shown here.",
                    "label": 0
                }
            ]
        }
    }
}