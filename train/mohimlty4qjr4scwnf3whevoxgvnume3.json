{
    "id": "mohimlty4qjr4scwnf3whevoxgvnume3",
    "title": "Designing Frameworks for Automatic Affect Prediction and Classification in Dimensional Space",
    "info": {
        "author": [
            "Maja Pantic, Intelligent Behaviour Understanding Group (iBUG), Department of Computing, Imperial College London"
        ],
        "published": "Aug. 24, 2011",
        "recorded": "June 2011",
        "category": [
            "Top->Computer Science->Computer Vision->Face & Gesture Analysis"
        ]
    },
    "url": "http://videolectures.net/gesturerecognition2011_pantic_designing/",
    "segmentation": [
        [
            "So my name is my appendix, and in fact this whole thing was a little bit of mass becausw I was not supposed to be here and at the end it turned out that I can be here.",
            "So in the meantime, my colleague was supposed to present this talk, but now I'm presenting so everything is just like 5 last five minutes arrangement so.",
            "Alright, so the talk will actually be about facial behavior understanding and the work done at the intelligent Behavior Understanding Group, which I had at Imperial College London."
        ],
        [
            "And there are several points I want to make in this talk point.",
            "Number one would be that temporal analysis of facial behavior or Templer analysis of any behavior that you want to analyze is of importance."
        ],
        [
            "The second point that I want to make is that.",
            "Temporal analysis of behavior is also extremely important when you want to analyze the differences between active behavior and spontaneous behavior, and in fact you just have to really take into account that many of the current methods are trained on the active behavior and hence cannot actually deal with spontaneous behavior."
        ],
        [
            ".3 that I want to make is that in principle discrete analysis of human behavior such as analysis of six basic emotions, is not something we should try because these emotions in these expressions are not something that you will encounter in real world applications and hence another kind of analysis or behavior in terms of.",
            "Continuous dimensional space is something what we should."
        ],
        [
            "What to do?",
            "And the last point that I will try to make is that instead of classical multimodal Fusion that most of the people attempt when doing multimodal behavior analysis is maybe not the way forward, that instead we should maybe go for prediction."
        ],
        [
            "So the whole idea of human behavior analysis and the reason why it became quite popular topic in the past years is due to the applications that you can have.",
            "So human computer interaction and analysis of behavior in terms of whether a person is interested in what he or she is doing.",
            "Whether something is boring, whether somebody is tired, this kind of feed that you can use to enhance human computer interaction in vehicle computing where you can measure.",
            "Sleepiness of or tiredness of the driver would be a good application.",
            "Very important games, but also security things like authentic behavior analysis or in fact our typical behavior analysis, including the deception detection would be of interest.",
            "So there are many applications and detect this actually raised interest into the."
        ],
        [
            "Into the whole field.",
            "So machine analysis of human behavior is usually approached in the following way.",
            "You first analyze things like facial expressions or body gestures, audio expressions at the same time, you have to reason about the context.",
            "So who the user is, where that person is, what that person is doing and the reason is that actually the very same behavior could have completely different interpretation given the context.",
            "So for example.",
            "I mean, think about this situation.",
            "I'm currently the speaker and if I would throw, that might mean that this topic that I'm currently discussing might be of importance to me or might be difficult.",
            "However, if you thrown, it might be what she's talking about.",
            "So actually the whole understanding of the very same gesture would be very different given the context.",
            "So context sensitive behavior.",
            "Interpretation would be something that we would like to achieve.",
            "However most of the current methods do not do that.",
            "And finally, if you have human computer interaction, you would also like to give a context sensitive response to the user."
        ],
        [
            "However, this talk mostly focuses on facial behavior analysis, so I will not talk about the context and about giving the feedback to the user.",
            "So when it comes to facial behavior analysis, usually you do that in a couple of steps.",
            "The first step would be to detect the face in the scene.",
            "The next step will be to analyze the face and the changes in the phase due to the facial expressions, you can do that in several ways.",
            "You can do that by.",
            "Detecting a certain certain number of characteristic points and then tracking those points throughout the sequence and reason about the movement of those points to reason about the facial expressions.",
            "Or you can do the appearance analysis and the changes in the appearance due to facial expressions and then of course you want to classify that in a set of classes.",
            "As I said, a very common way is to do 6 basic emotions.",
            "However, this is something which."
        ],
        [
            "We do not do so.",
            "Let me say a couple of words about the methods that we're using in in our work.",
            "This is the face tracker that we are using this.",
            "Actually what you see here are the results on the Ross Video an on the your right hand side you will see the regional Ross Video while on the left hand side you will see the video with the results of our tracker.",
            "The tracker that we're using uses incremental, robust kernel PCA.",
            "At actually what does it mean is that it calculates in the detected phase region.",
            "The principle components in an incremental way robust incremental way and for the features you can use either the intensity pixel intensity in the face region in our current work we use different features and those are the gradients of orientation.",
            "The work has been published at Face and Gesture Recognition Conference, so you can.",
            "You can read about."
        ],
        [
            "It.",
            "The next step would be.",
            "Once you have this face region to either analyze the changes, appearance changes that in that region or to detect a number of characteristic features like facial points and then track those in the subsequent frames.",
            "We did actually both approaches, so in the geometric feature based approach we detect a number of points.",
            "Those are the 20 points illustrated here and.",
            "How we do that?",
            "We, once you have the face region, we learn where the different parts or computational components could be.",
            "And out of these regions of interest we extract features like hard features and then we learn a regressor which actually predicts for the whole region where all the points related to that region should be.",
            "And we have the second step which is based on Markov random fields, which is actually graphical model that gives to you accuracy of that prediction.",
            "So say for example that one of the points that's actually in this case.",
            "That this point was detected here, however, the good location would be here.",
            "So what you do you predict based on all combination of three points, the 4th point in each time when you would use this point, the prediction of the fourth point will be very much of given your training data.",
            "So you use this information to actually reason about which point has not been predicted well, and then you adjust your prediction.",
            "Based on the output of the Markov random field.",
            "The result."
        ],
        [
            "Out of this method are quite encouraging that this is the red line that you see.",
            "How this, how you should read this graphic is down is actually the distance metric which is given in percentage of the Inter ocular distance.",
            "So if you want that the error is the 0% of the of the Inter ocular distance, so it means absolutely accurately detected point.",
            "Then you have on your epsilon axis will be the amount of percentage of the images which will be correctly recognized.",
            "SO40 error there will be almost 0 examples.",
            "However if you go to 2%, that's this.",
            "2% of the Interocular distance.",
            "So for example, in the typical databases it is usually 8200 pixels.",
            "So if we talk about 2%, so that would be something.",
            "Like let me think.",
            "It will be 8 zero, 8 * 2 so it will be less than two pixels error and then you will have something like 20% correctly recognize images.",
            "So at one point you will go to 5% of the interactual distance.",
            "So something like four to five pixels, and in that case we will have already 85 to 90% correctly recognized examples.",
            "So this is the other lines are.",
            "Actually other state of the art methods.",
            "So at this point we can say that we perform.",
            "Comparably or outperform the currently existing methods?"
        ],
        [
            "The next step in the process is to track these points throughout the video, and you can use any kind of facial feature tracker for that.",
            "There are many, for example trackers developed by a team at us, but you can also use the one that we are using.",
            "We like this one very much.",
            "It is backed actually based on particle filtering with factorized likelihoods and the whole idea of this framework.",
            "Is that we that instead of using a classical condensation algorithm or classical auxiliary particle field filtering, we extended auxiliary particle filtering so that each particle is actually one constellation of the points.",
            "In our case, the constellation of the points are for example the points of the eye or the points of the eyebrow.",
            "The points of the mouth, and then you use the training data which are actually expressive data that will say to you which constellations.",
            "They are feasible, possible anatomically possible constellation.",
            "So for example, if one of the points go completely off the classical constellation of the mouth, you will simply not find it in the training data and the distance to the training data will be very large.",
            "Hence you will reject this possibility.",
            "This is the way.",
            "Also the reason why the tracker can.",
            "Handle pretty well the occlusions and at least temporary short occlusions."
        ],
        [
            "So the next step would be the facial expression recognition based on the say the tracked points or actually the trajectories of the of the tracked points.",
            "So as I said, one of the way, and this is a very typical way, is to classify the movements of the points in the six basic emotion categories.",
            "However, the problem with this approach is that as I said in the real world.",
            "Scenarios people don't show this kind of expressions, hence our preference was to go to so called action unit recognition where."
        ],
        [
            "Where the action units are actually directly related to the facial muscle activations.",
            "So each action you need can be related to the activation of a specific muscle.",
            "So for example, action which one would be the activation of the inner portion of the forehead muscle and will result in the inner eyebrow raise the actually two will be related to the author.",
            "The activation of the outer portion of the eyebrows, where you will have the outer eyebrow raise actually four will be related to the activation of the muscles running around the eyebrow.",
            "So if you activate that muscle you will have a throne and so forth and so on.",
            "So there are in total 40 three such action units, of which less than 30 or cure frequently in daily life.",
            "However, the combination of.",
            "Different intensities and different combinations of action units account for 7000 or 10,000 facial expressions that we are anatomically capable of making.",
            "So think about the dimensionality reduction.",
            "Instead of having this 10,000 different classes, you actually have 43.",
            "If you go to action units.",
            "Another very good point of action units is that they are agnostic.",
            "They are independent of an interpretation and you can interpret the activated action units as emotions.",
            "As any mental state or social."
        ],
        [
            "Signal.",
            "What you want to achieve is automatic annotation of face videos.",
            "In this way, where for each frame you will say which action units have been activated.",
            "But as I said one."
        ],
        [
            "One of the points that I want to make it is in this talk is that you do not want only to recognize action units, but also the temporal dynamics, or at least the onset.",
            "The apex and the offset of these actions for each of the activated action."
        ],
        [
            "Units, let me explain why.",
            "So an let me first talk about this geometr."
        ],
        [
            "Based approach, so these are the results that you get if you apply so you have the features which are the distances between different points in frame by frame basis and the angles between different points.",
            "And you can even have kind of temporal features which are the trajectory's of the points and you use that for the input to support vector machine.",
            "The second column shows actually a combination of gentle boost with support vector machine.",
            "Where you use the gentle boost for the feature reduction and the support vector machine for the classification, and you train.",
            "Actually for each separate action unit a separate classifier.",
            "What you get this is F1 measure.",
            "You get something like 06 average in F1."
        ],
        [
            "Measure, however, if you take into account that action units cannot be in one frame activated, and then in the other faith frame not activated to be again activated in the 2nd frame, or that actually it is impossible if one action unit was in all set in one frame that it will not exist or be in immediately.",
            "In offsetting this in the next frame.",
            "This kind of temporal reasoning will help."
        ],
        [
            "So if you use an output of the support vector machines.",
            "If you go through sigmoid function an you use this as the input to the hidden Markov models such that you have the states which are.",
            "You must go through first from onset and then through apex and then through offset of the off each action unit which is feasible.",
            "It is normal because as I said it's impossible to have onset and immediately noting or to have at once apex without onset or after the apex without offset.",
            "Not to have the action unit, so this kind of errors.",
            "You can smooth and you can see also the improvement in the F1 measure.",
            "Look at the last column you have the average F1 measure of .66 and this is statistically significant."
        ],
        [
            "The similar results have been obtained by another method that we developed.",
            "This is the method which detects first phase in an input image and.",
            "Then overlay grid of points over the image and then calculates in a frame by frame basis the movements and the movement of the of the face and the transfer the actually the formation of these grid points.",
            "So what you see here is the original video, then the grid points and the changes in a frame by frame basis and the last image shows you actually accumulated motion, so the changes accumulated for the whole sequence.",
            "And then this image shows actually the static data.",
            "This is if you apply this motion onto the 1st frame of the video.",
            "So of course, since Giannis was not laughing in the 1st frame of the video, you have this black in between the mouse because you cannot model that if you don't see them right.",
            "So the whole idea here you can calculate based on this.",
            "Direction of the motion.",
            "The magnitude of the motion.",
            "The Divergens, the kurland."
        ],
        [
            "These are the features that we use as the input to the gentle boost classifiers and what you see here in the top graph.",
            "The Green Line is the onset class, the red line is the offset class.",
            "We of course do not model the apex because in the apex you don't have any motion.",
            "It is apex, right?",
            "And this method is based on motion, so we only model the onset and the offset and the apex.",
            "Is assumed to be between the onset and offset, so the green labels and red labels at some point if you do it in a frame by frame basis, you have certain noise, so you will at the same time we have scored the onset and at the same time the offset of the very same action.",
            "Of course this is impossible, so if you feed that into the hidden Markov models you will get very smooth labeling.",
            "That's the.",
            "That's the graph down.",
            "Where you have the true labels are green and the red labels are to the predicted labels."
        ],
        [
            "So these are the results overall in terms of F1 measure for different action units.",
            "What is really interesting here is that with this method overall you can recognize also very difficult action units, which are like this where you go your exactly your mouth, corners go inwards and this is an action that actually based on just feature based approaches you cannot.",
            "Recognize because there is no movement of which is visible into the images.",
            "So if you have this kind of approach where you model just the motion, you will see you will observe the motion of the mouth corners, which in terms of points movements will not be observable.",
            "So however my main point is just this that if you use temporal analysis then this will help your overall overall recognition rate.",
            "Of the facial behavior, and in this case sorry.",
            "They also can help you in and."
        ],
        [
            "Alice is of the differences between active and spontaneous behavior in active behavior.",
            "What you have are very short lived expressions which are very strong.",
            "While in spontaneous behavior what you have are very subtle expressions and.",
            "Which actually are quite long."
        ],
        [
            "So let me ask you question those are the two ladies.",
            "They were very young ladies.",
            "They were entertained by a clown.",
            "One of them was entertained by a clown and the other was actually asked to produce an expression on command.",
            "My question to you is which one of these ladies has been entertained by a clown?",
            "In other words, which of these laughters is spontaneous, and which one is active?",
            "So which one is continuous?",
            "Or OK, who says that this young lady has a spontaneous laughter?",
            "All right, who says that the other lady has spontaneous laughter?",
            "OK, this is absolutely very typical result of this question.",
            "Whenever I pose it to computer scientists, they always say that the other one is spontaneous, but actually that's not true.",
            "This lady has a spontaneous smile, so."
        ],
        [
            "Let me show you why.",
            "Look at what you see here are actually the temporal dynamics of the movement of the mouth corners only.",
            "So what you observe by this lady is actually that she has this typical pattern of spontaneous behavior.",
            "Oh, this is funny.",
            "Should I smile?",
            "Let me think yes, I should smile.",
            "Let me smile a little bit more.",
            "The other one says, OK, somebody said to her smile now so she smiled.",
            "OK, let me smile, smile, smile.",
            "OK now it's enough.",
            "He's a very sharp offset of of the smile.",
            "This is also when you go to any of the airplanes which observes this lady that sees you.",
            "She smiles.",
            "You noted here and then you pass.",
            "And if you happen to turn at that time and you look at her, she will be like you know, never smiling, right?",
            "So this is.",
            "These are the very typical expressions of either polite smile of a or any other active smiles so."
        ],
        [
            "The dynamics of behavior are extremely important if you want to analyze the differences between things like polite smile versus spontaneous smiles, smiles of joy.",
            "So we did the analysis of that based on different cues, including the facial expressions, the temporal analysis we we took into account, the amplitude of the smile, but also the intensity of only onset or only offset.",
            "As well as the duration of onset and offset, and we also use the head gestures and the shoulder gestures.",
            "In our case, we had two clearly different databases.",
            "One was a spontaneous at the other one was affected data an you will observe that in spontaneous data there is a lot of facial motion, facial and head motion as well as shoulder motion, while inactive data people tend to be rigid not to move the head a lot.",
            "And not to move shoulders a lot.",
            "All of this is actually the reason why the results are quite high, because we have in classification rate of 94% for the distinction between the two classes active versus spontaneous smile."
        ],
        [
            "The last point, because I will not go to the prediction, I don't have time for the last point that I will make in this presentation is that we should not do the discrete emotion detection, but that we should go into the analysis of behavior in continuous and multi dimensional space.",
            "The main reason for that is actually."
        ],
        [
            "That things like this.",
            "You don't hear this which is spitting.",
            "So let's try to do this again.",
            "Oh sorry, sorry sorry.",
            "He said something extremely rude to you.",
            "Silly, useless gift you really are, yet only one thing on your mind.",
            "A mask.",
            "So he he smiles a lot.",
            "So based on the facial expressions you would say he's happy, right?",
            "And that his attitude is positive.",
            "But however, if you take into account what he's saying, his attitude is not positive at all.",
            "He's actually smiling and saying to this other person or whomever he is coming.",
            "That he's just very stupid, right?",
            "So what you want is a continuous analysis of of this kind of expressions, where you will be able to reason over on in a continuous space on whether an expression is positive or not.",
            "But what is more important, you don't have to think about emotions.",
            "Think about anything like personality.",
            "Personalities are also dimensions.",
            "Also, continuous space, right?",
            "In personality how we express our personnel.",
            "Is something that we continuously do so there are different.",
            "It is really if you if you think about discrete classes, there are those six basic emotions or any categories that you want to use.",
            "This is very limiting if you have a number of dimensions you can do actually a much richer analysis of human behavior so."
        ],
        [
            "That's the idea, and we developed a number of regression techniques to address this problem.",
            "The latest one that we developed is an extension of relevance vector machine regression, which is output associative relevance vector machine machine regression which actually takes into account the movement of the points in facial points in an input video in a certain window, right and then?",
            "It takes also the prediction made by the regular relevance vector machine and what it gets at the end is the output prediction within that in that video of the output associative relevance vector machine where the output is actually the prediction of the initial relevance vector machine.",
            "The result."
        ],
        [
            "Sir, pretty encourageing.",
            "So what we did is test to different tests.",
            "One was you train on one person that output associative relevance vector machine outperforms.",
            "The relevance vector machine regression in both cases an for both arousal and valence, so there's there are quite some considerations actually.",
            "About these results because for example, when you have a negative balance an.",
            "You have the negative arousal.",
            "You will have much higher errors than if you have, for example, a positive balance and positive arousal, and the reason is actually that negative emotions and low arousal are very difficult to detect in facial expressions, audio features are much better for for that case, so this actually proves correct.",
            "This is what psychologists have said as well.",
            "So at this point I will end.",
            "There is a. I don't have the time to make my 4th point, but you can read."
        ],
        [
            "About it, it's it's about the prediction rather than for."
        ],
        [
            "Version and you can read about it in the face and gesture paper where we actually do not do the diffusion of audiovisual, but we actually predict the visual features from the audio features and the audio features from the visual features an achieve much, much higher recognition rate than if we do classical Fusion.",
            "So this concludes my talk.",
            "Thank you very much for your attention.",
            "Any questions Tamia?",
            "One question I don't know do is do people study facial expressions in relation to reactions or how do people react?",
            "So I guess the defining the class of expression is not very.",
            "Always easier, so do people study expression as a result of reaction to some particular event, and would this be interesting?",
            "Yes, actually.",
            "What most of our data is currently is those how to elicit it.",
            "Expressions where you show for example video to person and then that person react to that.",
            "So that's interesting.",
            "For example for retrieval because you can do.",
            "So called implicit tagging and then multimedia content retrieval based on the reactions of the user and the similar tags which are already associated with retrieval.",
            "So that's that's one way.",
            "But actually everything what you saw are the reactions of the person to another communicator.",
            "Tell.",
            "Now the questions.",
            "I've not Albert.",
            "Do you believe that 3D information can add something in your research or not?",
            "Yes, certainly.",
            "So there is a new research that we started in the lab.",
            "It is done by the research Fellow in our Group 7 as a video who is standing behind so.",
            "We believe that 3D facial expression analysis will bring.",
            "A lot insight of how actually the texture changes and also we think that certain things will be easier like you know dealing with the variances in head pose, which is one of the biggest problem with you when you do to the analysis.",
            "But most importantly for us we will be able to analyze very subtle expressions and how the small changes in texture can be detected.",
            "So there is also quite a lot of research in for example.",
            "Done by Lee Joon are in, for example or by Thomas Kwang, so they started this research on 3D facial expression analysis.",
            "OK, if there are no other questions, let's thank my again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So my name is my appendix, and in fact this whole thing was a little bit of mass becausw I was not supposed to be here and at the end it turned out that I can be here.",
                    "label": 0
                },
                {
                    "sent": "So in the meantime, my colleague was supposed to present this talk, but now I'm presenting so everything is just like 5 last five minutes arrangement so.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the talk will actually be about facial behavior understanding and the work done at the intelligent Behavior Understanding Group, which I had at Imperial College London.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there are several points I want to make in this talk point.",
                    "label": 0
                },
                {
                    "sent": "Number one would be that temporal analysis of facial behavior or Templer analysis of any behavior that you want to analyze is of importance.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second point that I want to make is that.",
                    "label": 0
                },
                {
                    "sent": "Temporal analysis of behavior is also extremely important when you want to analyze the differences between active behavior and spontaneous behavior, and in fact you just have to really take into account that many of the current methods are trained on the active behavior and hence cannot actually deal with spontaneous behavior.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": ".3 that I want to make is that in principle discrete analysis of human behavior such as analysis of six basic emotions, is not something we should try because these emotions in these expressions are not something that you will encounter in real world applications and hence another kind of analysis or behavior in terms of.",
                    "label": 0
                },
                {
                    "sent": "Continuous dimensional space is something what we should.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What to do?",
                    "label": 0
                },
                {
                    "sent": "And the last point that I will try to make is that instead of classical multimodal Fusion that most of the people attempt when doing multimodal behavior analysis is maybe not the way forward, that instead we should maybe go for prediction.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the whole idea of human behavior analysis and the reason why it became quite popular topic in the past years is due to the applications that you can have.",
                    "label": 0
                },
                {
                    "sent": "So human computer interaction and analysis of behavior in terms of whether a person is interested in what he or she is doing.",
                    "label": 0
                },
                {
                    "sent": "Whether something is boring, whether somebody is tired, this kind of feed that you can use to enhance human computer interaction in vehicle computing where you can measure.",
                    "label": 0
                },
                {
                    "sent": "Sleepiness of or tiredness of the driver would be a good application.",
                    "label": 0
                },
                {
                    "sent": "Very important games, but also security things like authentic behavior analysis or in fact our typical behavior analysis, including the deception detection would be of interest.",
                    "label": 0
                },
                {
                    "sent": "So there are many applications and detect this actually raised interest into the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Into the whole field.",
                    "label": 0
                },
                {
                    "sent": "So machine analysis of human behavior is usually approached in the following way.",
                    "label": 1
                },
                {
                    "sent": "You first analyze things like facial expressions or body gestures, audio expressions at the same time, you have to reason about the context.",
                    "label": 0
                },
                {
                    "sent": "So who the user is, where that person is, what that person is doing and the reason is that actually the very same behavior could have completely different interpretation given the context.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "I mean, think about this situation.",
                    "label": 0
                },
                {
                    "sent": "I'm currently the speaker and if I would throw, that might mean that this topic that I'm currently discussing might be of importance to me or might be difficult.",
                    "label": 0
                },
                {
                    "sent": "However, if you thrown, it might be what she's talking about.",
                    "label": 1
                },
                {
                    "sent": "So actually the whole understanding of the very same gesture would be very different given the context.",
                    "label": 0
                },
                {
                    "sent": "So context sensitive behavior.",
                    "label": 0
                },
                {
                    "sent": "Interpretation would be something that we would like to achieve.",
                    "label": 0
                },
                {
                    "sent": "However most of the current methods do not do that.",
                    "label": 0
                },
                {
                    "sent": "And finally, if you have human computer interaction, you would also like to give a context sensitive response to the user.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, this talk mostly focuses on facial behavior analysis, so I will not talk about the context and about giving the feedback to the user.",
                    "label": 0
                },
                {
                    "sent": "So when it comes to facial behavior analysis, usually you do that in a couple of steps.",
                    "label": 0
                },
                {
                    "sent": "The first step would be to detect the face in the scene.",
                    "label": 0
                },
                {
                    "sent": "The next step will be to analyze the face and the changes in the phase due to the facial expressions, you can do that in several ways.",
                    "label": 0
                },
                {
                    "sent": "You can do that by.",
                    "label": 0
                },
                {
                    "sent": "Detecting a certain certain number of characteristic points and then tracking those points throughout the sequence and reason about the movement of those points to reason about the facial expressions.",
                    "label": 0
                },
                {
                    "sent": "Or you can do the appearance analysis and the changes in the appearance due to facial expressions and then of course you want to classify that in a set of classes.",
                    "label": 0
                },
                {
                    "sent": "As I said, a very common way is to do 6 basic emotions.",
                    "label": 0
                },
                {
                    "sent": "However, this is something which.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We do not do so.",
                    "label": 0
                },
                {
                    "sent": "Let me say a couple of words about the methods that we're using in in our work.",
                    "label": 0
                },
                {
                    "sent": "This is the face tracker that we are using this.",
                    "label": 0
                },
                {
                    "sent": "Actually what you see here are the results on the Ross Video an on the your right hand side you will see the regional Ross Video while on the left hand side you will see the video with the results of our tracker.",
                    "label": 1
                },
                {
                    "sent": "The tracker that we're using uses incremental, robust kernel PCA.",
                    "label": 0
                },
                {
                    "sent": "At actually what does it mean is that it calculates in the detected phase region.",
                    "label": 0
                },
                {
                    "sent": "The principle components in an incremental way robust incremental way and for the features you can use either the intensity pixel intensity in the face region in our current work we use different features and those are the gradients of orientation.",
                    "label": 0
                },
                {
                    "sent": "The work has been published at Face and Gesture Recognition Conference, so you can.",
                    "label": 0
                },
                {
                    "sent": "You can read about.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "The next step would be.",
                    "label": 0
                },
                {
                    "sent": "Once you have this face region to either analyze the changes, appearance changes that in that region or to detect a number of characteristic features like facial points and then track those in the subsequent frames.",
                    "label": 0
                },
                {
                    "sent": "We did actually both approaches, so in the geometric feature based approach we detect a number of points.",
                    "label": 0
                },
                {
                    "sent": "Those are the 20 points illustrated here and.",
                    "label": 0
                },
                {
                    "sent": "How we do that?",
                    "label": 0
                },
                {
                    "sent": "We, once you have the face region, we learn where the different parts or computational components could be.",
                    "label": 0
                },
                {
                    "sent": "And out of these regions of interest we extract features like hard features and then we learn a regressor which actually predicts for the whole region where all the points related to that region should be.",
                    "label": 0
                },
                {
                    "sent": "And we have the second step which is based on Markov random fields, which is actually graphical model that gives to you accuracy of that prediction.",
                    "label": 0
                },
                {
                    "sent": "So say for example that one of the points that's actually in this case.",
                    "label": 0
                },
                {
                    "sent": "That this point was detected here, however, the good location would be here.",
                    "label": 0
                },
                {
                    "sent": "So what you do you predict based on all combination of three points, the 4th point in each time when you would use this point, the prediction of the fourth point will be very much of given your training data.",
                    "label": 0
                },
                {
                    "sent": "So you use this information to actually reason about which point has not been predicted well, and then you adjust your prediction.",
                    "label": 0
                },
                {
                    "sent": "Based on the output of the Markov random field.",
                    "label": 0
                },
                {
                    "sent": "The result.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Out of this method are quite encouraging that this is the red line that you see.",
                    "label": 0
                },
                {
                    "sent": "How this, how you should read this graphic is down is actually the distance metric which is given in percentage of the Inter ocular distance.",
                    "label": 0
                },
                {
                    "sent": "So if you want that the error is the 0% of the of the Inter ocular distance, so it means absolutely accurately detected point.",
                    "label": 0
                },
                {
                    "sent": "Then you have on your epsilon axis will be the amount of percentage of the images which will be correctly recognized.",
                    "label": 0
                },
                {
                    "sent": "SO40 error there will be almost 0 examples.",
                    "label": 0
                },
                {
                    "sent": "However if you go to 2%, that's this.",
                    "label": 0
                },
                {
                    "sent": "2% of the Interocular distance.",
                    "label": 0
                },
                {
                    "sent": "So for example, in the typical databases it is usually 8200 pixels.",
                    "label": 0
                },
                {
                    "sent": "So if we talk about 2%, so that would be something.",
                    "label": 0
                },
                {
                    "sent": "Like let me think.",
                    "label": 0
                },
                {
                    "sent": "It will be 8 zero, 8 * 2 so it will be less than two pixels error and then you will have something like 20% correctly recognize images.",
                    "label": 0
                },
                {
                    "sent": "So at one point you will go to 5% of the interactual distance.",
                    "label": 0
                },
                {
                    "sent": "So something like four to five pixels, and in that case we will have already 85 to 90% correctly recognized examples.",
                    "label": 0
                },
                {
                    "sent": "So this is the other lines are.",
                    "label": 0
                },
                {
                    "sent": "Actually other state of the art methods.",
                    "label": 0
                },
                {
                    "sent": "So at this point we can say that we perform.",
                    "label": 0
                },
                {
                    "sent": "Comparably or outperform the currently existing methods?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next step in the process is to track these points throughout the video, and you can use any kind of facial feature tracker for that.",
                    "label": 0
                },
                {
                    "sent": "There are many, for example trackers developed by a team at us, but you can also use the one that we are using.",
                    "label": 0
                },
                {
                    "sent": "We like this one very much.",
                    "label": 0
                },
                {
                    "sent": "It is backed actually based on particle filtering with factorized likelihoods and the whole idea of this framework.",
                    "label": 1
                },
                {
                    "sent": "Is that we that instead of using a classical condensation algorithm or classical auxiliary particle field filtering, we extended auxiliary particle filtering so that each particle is actually one constellation of the points.",
                    "label": 0
                },
                {
                    "sent": "In our case, the constellation of the points are for example the points of the eye or the points of the eyebrow.",
                    "label": 0
                },
                {
                    "sent": "The points of the mouth, and then you use the training data which are actually expressive data that will say to you which constellations.",
                    "label": 0
                },
                {
                    "sent": "They are feasible, possible anatomically possible constellation.",
                    "label": 0
                },
                {
                    "sent": "So for example, if one of the points go completely off the classical constellation of the mouth, you will simply not find it in the training data and the distance to the training data will be very large.",
                    "label": 0
                },
                {
                    "sent": "Hence you will reject this possibility.",
                    "label": 0
                },
                {
                    "sent": "This is the way.",
                    "label": 0
                },
                {
                    "sent": "Also the reason why the tracker can.",
                    "label": 0
                },
                {
                    "sent": "Handle pretty well the occlusions and at least temporary short occlusions.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next step would be the facial expression recognition based on the say the tracked points or actually the trajectories of the of the tracked points.",
                    "label": 1
                },
                {
                    "sent": "So as I said, one of the way, and this is a very typical way, is to classify the movements of the points in the six basic emotion categories.",
                    "label": 0
                },
                {
                    "sent": "However, the problem with this approach is that as I said in the real world.",
                    "label": 0
                },
                {
                    "sent": "Scenarios people don't show this kind of expressions, hence our preference was to go to so called action unit recognition where.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where the action units are actually directly related to the facial muscle activations.",
                    "label": 0
                },
                {
                    "sent": "So each action you need can be related to the activation of a specific muscle.",
                    "label": 0
                },
                {
                    "sent": "So for example, action which one would be the activation of the inner portion of the forehead muscle and will result in the inner eyebrow raise the actually two will be related to the author.",
                    "label": 0
                },
                {
                    "sent": "The activation of the outer portion of the eyebrows, where you will have the outer eyebrow raise actually four will be related to the activation of the muscles running around the eyebrow.",
                    "label": 0
                },
                {
                    "sent": "So if you activate that muscle you will have a throne and so forth and so on.",
                    "label": 0
                },
                {
                    "sent": "So there are in total 40 three such action units, of which less than 30 or cure frequently in daily life.",
                    "label": 0
                },
                {
                    "sent": "However, the combination of.",
                    "label": 0
                },
                {
                    "sent": "Different intensities and different combinations of action units account for 7000 or 10,000 facial expressions that we are anatomically capable of making.",
                    "label": 0
                },
                {
                    "sent": "So think about the dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "Instead of having this 10,000 different classes, you actually have 43.",
                    "label": 0
                },
                {
                    "sent": "If you go to action units.",
                    "label": 0
                },
                {
                    "sent": "Another very good point of action units is that they are agnostic.",
                    "label": 0
                },
                {
                    "sent": "They are independent of an interpretation and you can interpret the activated action units as emotions.",
                    "label": 0
                },
                {
                    "sent": "As any mental state or social.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Signal.",
                    "label": 0
                },
                {
                    "sent": "What you want to achieve is automatic annotation of face videos.",
                    "label": 0
                },
                {
                    "sent": "In this way, where for each frame you will say which action units have been activated.",
                    "label": 0
                },
                {
                    "sent": "But as I said one.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the points that I want to make it is in this talk is that you do not want only to recognize action units, but also the temporal dynamics, or at least the onset.",
                    "label": 0
                },
                {
                    "sent": "The apex and the offset of these actions for each of the activated action.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Units, let me explain why.",
                    "label": 0
                },
                {
                    "sent": "So an let me first talk about this geometr.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Based approach, so these are the results that you get if you apply so you have the features which are the distances between different points in frame by frame basis and the angles between different points.",
                    "label": 0
                },
                {
                    "sent": "And you can even have kind of temporal features which are the trajectory's of the points and you use that for the input to support vector machine.",
                    "label": 0
                },
                {
                    "sent": "The second column shows actually a combination of gentle boost with support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Where you use the gentle boost for the feature reduction and the support vector machine for the classification, and you train.",
                    "label": 0
                },
                {
                    "sent": "Actually for each separate action unit a separate classifier.",
                    "label": 0
                },
                {
                    "sent": "What you get this is F1 measure.",
                    "label": 0
                },
                {
                    "sent": "You get something like 06 average in F1.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Measure, however, if you take into account that action units cannot be in one frame activated, and then in the other faith frame not activated to be again activated in the 2nd frame, or that actually it is impossible if one action unit was in all set in one frame that it will not exist or be in immediately.",
                    "label": 0
                },
                {
                    "sent": "In offsetting this in the next frame.",
                    "label": 0
                },
                {
                    "sent": "This kind of temporal reasoning will help.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you use an output of the support vector machines.",
                    "label": 0
                },
                {
                    "sent": "If you go through sigmoid function an you use this as the input to the hidden Markov models such that you have the states which are.",
                    "label": 0
                },
                {
                    "sent": "You must go through first from onset and then through apex and then through offset of the off each action unit which is feasible.",
                    "label": 0
                },
                {
                    "sent": "It is normal because as I said it's impossible to have onset and immediately noting or to have at once apex without onset or after the apex without offset.",
                    "label": 0
                },
                {
                    "sent": "Not to have the action unit, so this kind of errors.",
                    "label": 0
                },
                {
                    "sent": "You can smooth and you can see also the improvement in the F1 measure.",
                    "label": 0
                },
                {
                    "sent": "Look at the last column you have the average F1 measure of .66 and this is statistically significant.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The similar results have been obtained by another method that we developed.",
                    "label": 0
                },
                {
                    "sent": "This is the method which detects first phase in an input image and.",
                    "label": 0
                },
                {
                    "sent": "Then overlay grid of points over the image and then calculates in a frame by frame basis the movements and the movement of the of the face and the transfer the actually the formation of these grid points.",
                    "label": 0
                },
                {
                    "sent": "So what you see here is the original video, then the grid points and the changes in a frame by frame basis and the last image shows you actually accumulated motion, so the changes accumulated for the whole sequence.",
                    "label": 0
                },
                {
                    "sent": "And then this image shows actually the static data.",
                    "label": 0
                },
                {
                    "sent": "This is if you apply this motion onto the 1st frame of the video.",
                    "label": 0
                },
                {
                    "sent": "So of course, since Giannis was not laughing in the 1st frame of the video, you have this black in between the mouse because you cannot model that if you don't see them right.",
                    "label": 0
                },
                {
                    "sent": "So the whole idea here you can calculate based on this.",
                    "label": 0
                },
                {
                    "sent": "Direction of the motion.",
                    "label": 0
                },
                {
                    "sent": "The magnitude of the motion.",
                    "label": 0
                },
                {
                    "sent": "The Divergens, the kurland.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the features that we use as the input to the gentle boost classifiers and what you see here in the top graph.",
                    "label": 0
                },
                {
                    "sent": "The Green Line is the onset class, the red line is the offset class.",
                    "label": 0
                },
                {
                    "sent": "We of course do not model the apex because in the apex you don't have any motion.",
                    "label": 0
                },
                {
                    "sent": "It is apex, right?",
                    "label": 0
                },
                {
                    "sent": "And this method is based on motion, so we only model the onset and the offset and the apex.",
                    "label": 0
                },
                {
                    "sent": "Is assumed to be between the onset and offset, so the green labels and red labels at some point if you do it in a frame by frame basis, you have certain noise, so you will at the same time we have scored the onset and at the same time the offset of the very same action.",
                    "label": 0
                },
                {
                    "sent": "Of course this is impossible, so if you feed that into the hidden Markov models you will get very smooth labeling.",
                    "label": 0
                },
                {
                    "sent": "That's the.",
                    "label": 0
                },
                {
                    "sent": "That's the graph down.",
                    "label": 0
                },
                {
                    "sent": "Where you have the true labels are green and the red labels are to the predicted labels.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the results overall in terms of F1 measure for different action units.",
                    "label": 0
                },
                {
                    "sent": "What is really interesting here is that with this method overall you can recognize also very difficult action units, which are like this where you go your exactly your mouth, corners go inwards and this is an action that actually based on just feature based approaches you cannot.",
                    "label": 0
                },
                {
                    "sent": "Recognize because there is no movement of which is visible into the images.",
                    "label": 0
                },
                {
                    "sent": "So if you have this kind of approach where you model just the motion, you will see you will observe the motion of the mouth corners, which in terms of points movements will not be observable.",
                    "label": 0
                },
                {
                    "sent": "So however my main point is just this that if you use temporal analysis then this will help your overall overall recognition rate.",
                    "label": 0
                },
                {
                    "sent": "Of the facial behavior, and in this case sorry.",
                    "label": 0
                },
                {
                    "sent": "They also can help you in and.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alice is of the differences between active and spontaneous behavior in active behavior.",
                    "label": 0
                },
                {
                    "sent": "What you have are very short lived expressions which are very strong.",
                    "label": 0
                },
                {
                    "sent": "While in spontaneous behavior what you have are very subtle expressions and.",
                    "label": 0
                },
                {
                    "sent": "Which actually are quite long.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me ask you question those are the two ladies.",
                    "label": 0
                },
                {
                    "sent": "They were very young ladies.",
                    "label": 0
                },
                {
                    "sent": "They were entertained by a clown.",
                    "label": 0
                },
                {
                    "sent": "One of them was entertained by a clown and the other was actually asked to produce an expression on command.",
                    "label": 0
                },
                {
                    "sent": "My question to you is which one of these ladies has been entertained by a clown?",
                    "label": 0
                },
                {
                    "sent": "In other words, which of these laughters is spontaneous, and which one is active?",
                    "label": 0
                },
                {
                    "sent": "So which one is continuous?",
                    "label": 0
                },
                {
                    "sent": "Or OK, who says that this young lady has a spontaneous laughter?",
                    "label": 0
                },
                {
                    "sent": "All right, who says that the other lady has spontaneous laughter?",
                    "label": 0
                },
                {
                    "sent": "OK, this is absolutely very typical result of this question.",
                    "label": 0
                },
                {
                    "sent": "Whenever I pose it to computer scientists, they always say that the other one is spontaneous, but actually that's not true.",
                    "label": 0
                },
                {
                    "sent": "This lady has a spontaneous smile, so.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me show you why.",
                    "label": 0
                },
                {
                    "sent": "Look at what you see here are actually the temporal dynamics of the movement of the mouth corners only.",
                    "label": 0
                },
                {
                    "sent": "So what you observe by this lady is actually that she has this typical pattern of spontaneous behavior.",
                    "label": 0
                },
                {
                    "sent": "Oh, this is funny.",
                    "label": 0
                },
                {
                    "sent": "Should I smile?",
                    "label": 0
                },
                {
                    "sent": "Let me think yes, I should smile.",
                    "label": 0
                },
                {
                    "sent": "Let me smile a little bit more.",
                    "label": 0
                },
                {
                    "sent": "The other one says, OK, somebody said to her smile now so she smiled.",
                    "label": 0
                },
                {
                    "sent": "OK, let me smile, smile, smile.",
                    "label": 0
                },
                {
                    "sent": "OK now it's enough.",
                    "label": 0
                },
                {
                    "sent": "He's a very sharp offset of of the smile.",
                    "label": 0
                },
                {
                    "sent": "This is also when you go to any of the airplanes which observes this lady that sees you.",
                    "label": 0
                },
                {
                    "sent": "She smiles.",
                    "label": 0
                },
                {
                    "sent": "You noted here and then you pass.",
                    "label": 0
                },
                {
                    "sent": "And if you happen to turn at that time and you look at her, she will be like you know, never smiling, right?",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "These are the very typical expressions of either polite smile of a or any other active smiles so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The dynamics of behavior are extremely important if you want to analyze the differences between things like polite smile versus spontaneous smiles, smiles of joy.",
                    "label": 0
                },
                {
                    "sent": "So we did the analysis of that based on different cues, including the facial expressions, the temporal analysis we we took into account, the amplitude of the smile, but also the intensity of only onset or only offset.",
                    "label": 0
                },
                {
                    "sent": "As well as the duration of onset and offset, and we also use the head gestures and the shoulder gestures.",
                    "label": 0
                },
                {
                    "sent": "In our case, we had two clearly different databases.",
                    "label": 0
                },
                {
                    "sent": "One was a spontaneous at the other one was affected data an you will observe that in spontaneous data there is a lot of facial motion, facial and head motion as well as shoulder motion, while inactive data people tend to be rigid not to move the head a lot.",
                    "label": 0
                },
                {
                    "sent": "And not to move shoulders a lot.",
                    "label": 0
                },
                {
                    "sent": "All of this is actually the reason why the results are quite high, because we have in classification rate of 94% for the distinction between the two classes active versus spontaneous smile.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last point, because I will not go to the prediction, I don't have time for the last point that I will make in this presentation is that we should not do the discrete emotion detection, but that we should go into the analysis of behavior in continuous and multi dimensional space.",
                    "label": 0
                },
                {
                    "sent": "The main reason for that is actually.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That things like this.",
                    "label": 0
                },
                {
                    "sent": "You don't hear this which is spitting.",
                    "label": 0
                },
                {
                    "sent": "So let's try to do this again.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, sorry sorry.",
                    "label": 0
                },
                {
                    "sent": "He said something extremely rude to you.",
                    "label": 0
                },
                {
                    "sent": "Silly, useless gift you really are, yet only one thing on your mind.",
                    "label": 0
                },
                {
                    "sent": "A mask.",
                    "label": 0
                },
                {
                    "sent": "So he he smiles a lot.",
                    "label": 0
                },
                {
                    "sent": "So based on the facial expressions you would say he's happy, right?",
                    "label": 0
                },
                {
                    "sent": "And that his attitude is positive.",
                    "label": 0
                },
                {
                    "sent": "But however, if you take into account what he's saying, his attitude is not positive at all.",
                    "label": 0
                },
                {
                    "sent": "He's actually smiling and saying to this other person or whomever he is coming.",
                    "label": 0
                },
                {
                    "sent": "That he's just very stupid, right?",
                    "label": 0
                },
                {
                    "sent": "So what you want is a continuous analysis of of this kind of expressions, where you will be able to reason over on in a continuous space on whether an expression is positive or not.",
                    "label": 0
                },
                {
                    "sent": "But what is more important, you don't have to think about emotions.",
                    "label": 0
                },
                {
                    "sent": "Think about anything like personality.",
                    "label": 0
                },
                {
                    "sent": "Personalities are also dimensions.",
                    "label": 0
                },
                {
                    "sent": "Also, continuous space, right?",
                    "label": 0
                },
                {
                    "sent": "In personality how we express our personnel.",
                    "label": 0
                },
                {
                    "sent": "Is something that we continuously do so there are different.",
                    "label": 0
                },
                {
                    "sent": "It is really if you if you think about discrete classes, there are those six basic emotions or any categories that you want to use.",
                    "label": 0
                },
                {
                    "sent": "This is very limiting if you have a number of dimensions you can do actually a much richer analysis of human behavior so.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the idea, and we developed a number of regression techniques to address this problem.",
                    "label": 0
                },
                {
                    "sent": "The latest one that we developed is an extension of relevance vector machine regression, which is output associative relevance vector machine machine regression which actually takes into account the movement of the points in facial points in an input video in a certain window, right and then?",
                    "label": 0
                },
                {
                    "sent": "It takes also the prediction made by the regular relevance vector machine and what it gets at the end is the output prediction within that in that video of the output associative relevance vector machine where the output is actually the prediction of the initial relevance vector machine.",
                    "label": 0
                },
                {
                    "sent": "The result.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sir, pretty encourageing.",
                    "label": 0
                },
                {
                    "sent": "So what we did is test to different tests.",
                    "label": 0
                },
                {
                    "sent": "One was you train on one person that output associative relevance vector machine outperforms.",
                    "label": 0
                },
                {
                    "sent": "The relevance vector machine regression in both cases an for both arousal and valence, so there's there are quite some considerations actually.",
                    "label": 0
                },
                {
                    "sent": "About these results because for example, when you have a negative balance an.",
                    "label": 0
                },
                {
                    "sent": "You have the negative arousal.",
                    "label": 0
                },
                {
                    "sent": "You will have much higher errors than if you have, for example, a positive balance and positive arousal, and the reason is actually that negative emotions and low arousal are very difficult to detect in facial expressions, audio features are much better for for that case, so this actually proves correct.",
                    "label": 0
                },
                {
                    "sent": "This is what psychologists have said as well.",
                    "label": 0
                },
                {
                    "sent": "So at this point I will end.",
                    "label": 0
                },
                {
                    "sent": "There is a. I don't have the time to make my 4th point, but you can read.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About it, it's it's about the prediction rather than for.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Version and you can read about it in the face and gesture paper where we actually do not do the diffusion of audiovisual, but we actually predict the visual features from the audio features and the audio features from the visual features an achieve much, much higher recognition rate than if we do classical Fusion.",
                    "label": 0
                },
                {
                    "sent": "So this concludes my talk.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for your attention.",
                    "label": 0
                },
                {
                    "sent": "Any questions Tamia?",
                    "label": 0
                },
                {
                    "sent": "One question I don't know do is do people study facial expressions in relation to reactions or how do people react?",
                    "label": 0
                },
                {
                    "sent": "So I guess the defining the class of expression is not very.",
                    "label": 0
                },
                {
                    "sent": "Always easier, so do people study expression as a result of reaction to some particular event, and would this be interesting?",
                    "label": 0
                },
                {
                    "sent": "Yes, actually.",
                    "label": 0
                },
                {
                    "sent": "What most of our data is currently is those how to elicit it.",
                    "label": 0
                },
                {
                    "sent": "Expressions where you show for example video to person and then that person react to that.",
                    "label": 0
                },
                {
                    "sent": "So that's interesting.",
                    "label": 0
                },
                {
                    "sent": "For example for retrieval because you can do.",
                    "label": 0
                },
                {
                    "sent": "So called implicit tagging and then multimedia content retrieval based on the reactions of the user and the similar tags which are already associated with retrieval.",
                    "label": 1
                },
                {
                    "sent": "So that's that's one way.",
                    "label": 0
                },
                {
                    "sent": "But actually everything what you saw are the reactions of the person to another communicator.",
                    "label": 0
                },
                {
                    "sent": "Tell.",
                    "label": 0
                },
                {
                    "sent": "Now the questions.",
                    "label": 0
                },
                {
                    "sent": "I've not Albert.",
                    "label": 0
                },
                {
                    "sent": "Do you believe that 3D information can add something in your research or not?",
                    "label": 0
                },
                {
                    "sent": "Yes, certainly.",
                    "label": 0
                },
                {
                    "sent": "So there is a new research that we started in the lab.",
                    "label": 0
                },
                {
                    "sent": "It is done by the research Fellow in our Group 7 as a video who is standing behind so.",
                    "label": 0
                },
                {
                    "sent": "We believe that 3D facial expression analysis will bring.",
                    "label": 0
                },
                {
                    "sent": "A lot insight of how actually the texture changes and also we think that certain things will be easier like you know dealing with the variances in head pose, which is one of the biggest problem with you when you do to the analysis.",
                    "label": 0
                },
                {
                    "sent": "But most importantly for us we will be able to analyze very subtle expressions and how the small changes in texture can be detected.",
                    "label": 0
                },
                {
                    "sent": "So there is also quite a lot of research in for example.",
                    "label": 0
                },
                {
                    "sent": "Done by Lee Joon are in, for example or by Thomas Kwang, so they started this research on 3D facial expression analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, if there are no other questions, let's thank my again.",
                    "label": 0
                }
            ]
        }
    }
}