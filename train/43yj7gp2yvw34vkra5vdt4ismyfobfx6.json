{
    "id": "43yj7gp2yvw34vkra5vdt4ismyfobfx6",
    "title": "Analyzing and Escaping Local Optima in Planning as Inference for Partially Observable Domains",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Pascal Poupart, School of Computer Science, University of Waterloo"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Decision Support",
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_poupart_domains/",
    "segmentation": [
        [
            "OK, so I'm going to talk about analyzing an escaping local optimum planning as in French for partial observable domains.",
            "OK, and this is also joint work with copious flag and map to stand for the Free University of Berlin in Germany."
        ],
        [
            "OK, so in this talk when I consider one particular approach and complete planning that has become popular in recent years, which is pending as inference and we're going to focus on partial observable domain.",
            "So that means we're going to look at palm DP.",
            "So partial observable Markov decision processes.",
            "Now when we do planning as inference, gay is that we're going to convert the palm DP into and make sure that any invasion networks an as a result, planning to now be framed as simply.",
            "Our problem of maximizing likelihood.",
            "I'll explain that in details, but the key is that by doing this then it opens the door to using many entrance algorithms.",
            "Do planning, so this is all nice, except that the original problem, the original planning problem was non convex and doing this transformation actually doesn't change its nature, so it remains a nonconvex optimization problem.",
            "So even though we have not many in France, algorithms that can solve this, we will all suffer from local Optima as well OK.",
            "I mean, machine learning this is also an issue, but in general and panning is a more severe issue, so that's why we're looking at that.",
            "OK, so in this talk there will be 2 contributions, so the first one will be an analysis of EMS local optimize.",
            "What I mean by an analysis is I'm going to interpret what it means to be stuck into local optimal in terms of time, so we'll give him meaning to this, and then there will be 2 escaped."
        ],
        [
            "OK, so here's the outline.",
            "1st I'll just review quickly, whereupon the peas and playing is inference.",
            "Then we'll go into the meat of the subjects, will pull up.",
            "To my interpretation.",
            "The main thing here.",
            "So if you don't understand the talk or if you don't remember watch, then the main thing to remember is that when you get stuck into a local optimum will show is that if we just correspond to doing it one step, look ahead.",
            "OK, so that's what young guys.",
            "And then I'll explain how to escape from local Optima using two techniques or for research and not splitting, and then I'll show you some experiment."
        ],
        [
            "OK, So what is the palm DP?",
            "So it's a regular Markov decision process where we don't get to observe this state, so we've got some observations here that are correlated with the States and I'll do this as part of the reinforcement learning session.",
            "Here.",
            "We're really looking at a planning problem, so we're assuming that we know what are the transitions as well as the reward function and the observation function, so we know everything we don't have to get some samples.",
            "OK, so we just need to optimize the calls."
        ],
        [
            "OK, so very quickly high is also optimization done.",
            "Traditionally you would be fine, believes that our distributions over states because we don't get to observe them.",
            "You could have made those beliefs by basophil trying.",
            "So is it just based their own and then a policy with him mapping from beliefs to actions and you could evaluate a policy simply by taking a discounted some of the expected reward and the best policy is obviously the one that will give you.",
            "The highest expected sum of rewards and a certificate to verify optimality is Bellman's equation, which is right here.",
            "OK so."
        ],
        [
            "That being said, let's look now at one class of policies.",
            "I find some controllers, so find that controllers are very interesting because of their simplicity, and then they can be downloaded in very very small devices.",
            "So here the idea is that we've got a bunch of nodes and for each node there is an action.",
            "So if you wanna know you executive action then you receive an observation that will tell you which edge to follow, so you get 01.",
            "You go to this node, execute a one again and keep on going.",
            "So it's really just a final state on that we executed.",
            "So then the problem possible optimization in this context is really a problem.",
            "Simply defining what would be the best action mapping and next node mapping and general, we're going to let those things be probabilistic.",
            "So we're going to have conditional distributions.",
            "So."
        ],
        [
            "So in 1999 Nickelodeon and some coauthors actually realize that we could fold this controller type of policy into the graphical model of funding.",
            "So here I've got again the graphical model that I showed you a few slides ago.",
            "I can augment it now with one more set of variables on the labeled MIND correspond to the node of the controller that the agent will be in at any point in time.",
            "And now the policy is re encoded by those red marks simply because the choice of the action is conditioned on the current node and the next node that you would end up in this condition on their current note as well as the observation received.",
            "OK, so now you can see where we're going.",
            "So in terms of I guess we planning as inference ideas that were slowly starting to convert from deep into it a regular dynamic Bayesian network where the policy is is essentially some initial distributions.",
            "As part of Asian Network.",
            "Alright."
        ],
        [
            "So.",
            "Then in 2006 Mac to say propose that since we've got this view, perhaps you could simply ask how we close time.",
            "What is the best conditional distribution in this graphical model that with maximized our expected reward?",
            "But let's try to do this in a way that would really have a dynamic Bayesian network.",
            "'cause of problems that we still have some rewards here that are utilities are not random variables and we've got some discount."
        ],
        [
            "Factor so one ideas that we could simply normalize the rewards so the rewards normally are any real number, and if we simply re normalize by subtracting the minimum reward and dividing by the range, then everything will be between zero and one and that can be interpreted as a probability.",
            "And now we can make the rewards just be some binary variables with probability disproportional to the original reward function.",
            "So what's left now is how to deal with the discount factor, and so this is where."
        ],
        [
            "When we could do is simply introduce a mixture of an invasion networks simply because rewards need to be added so mixed chairs or mixture models or traditionally falls that are decompose additively, so this makes sense here, and each dynamic vision network is going to have a probability that's proportional to the discount factor race to the number of time steps that's included in that dynamic Bayesian network.",
            "So now I've got a conversion where everything is essentially proportional.",
            "And now the value of the policy is nothing more than simply the probability that our bar would be equal to 1."
        ],
        [
            "OK, so given that now, finding an optimal policy can be phrased as an inference problem, where we would simply find what is what are the initial distributions here that would maximize the probability that power bar is equal to 1.",
            "Now how do we do this?",
            "There's lots of algorithms as I mentioned before, but let's start with the simplest one expectation maximization."
        ],
        [
            "The problem with any of the algorithms including yeah, is that they're going to get stuck into some local Optima, and So what I'm going to do now is go through analysis an essentially showing you that what PM ensures is just one step lookahead optimality.",
            "And based on that, then the natural thing to do if you want to escape is to look at multiple step to look ahead.",
            "An another approach would be to split some goals.",
            "So I find these things in more details."
        ],
        [
            "OK, so let's go into the details of.",
            "Yeah, it is going to be a few equations, but I'll get back to some high level ideas in a minute.",
            "If you're not comfortable with recent.",
            "OK, so YM&R context here is used to update the powders.",
            "And here I've got the distribution over actions for Michelle and the distribution over next nodes.",
            "Given the current hold, an observation, these parameters are only updated in a way that we multiply there.",
            "Setting by some expression.",
            "So we doing multiplicative update.",
            "That's what PM does.",
            "And if we're stuck into a local optimum, let it means that the multiplicative update here won't change what those values are.",
            "OK so far."
        ],
        [
            "Yes, we can.",
            "Formally the first theorem then essentially says that if we've got a policy that's a stable fixed point of PM, then those two conditions have to hold and essentially specifying webtools expr."
        ],
        [
            "Actions that I had on the previous slide.",
            "So G here engage here when it should be."
        ],
        [
            "Be like an essentially they must be equal to the largest possible value that they can take for any action here and for any next node here.",
            "OK, so that in itself is not too interesting."
        ],
        [
            "Xbox Now, let's see how we can use that to really understand what it means to be stuck into local Optima.",
            "So I've got the equations for YM right here and also global optimality as specified by the man's equation.",
            "So most here are nothing more than balance equation that's been split in two in the context of controllers.",
            "So when this holds, I know that I've got an optimal pulse an when this holds here.",
            "I know that I'm stuck.",
            "If I'm using him.",
            "Now these equations are actually very similar, but we've got some differences.",
            "So in green I've circled samples differences and you'll notice that here I've got some terms that are bounded functions and hear some terms that are bad.",
            "So what are those bad as if I?"
        ],
        [
            "Go back to my slide here for the details of.",
            "Yeah, beta is nothing more than the backward term that you would compute in India.",
            "Now it wasn't clear to us what it meant, but from those in."
        ],
        [
            "Visions actually we can show that when better really is just as rescaled version of the value function, so you can actually interpret the backward terms in EM as just the value function.",
            "So now because it's a rescaled version, it doesn't change anything, because here we've got a Max and here in our maximum when you maximized, if you just re scale things by a constant factor, it doesn't change anything.",
            "OK, the next differences that here I'm going to copy that are very close one and here with the reward function as we explained before, these things were just rescaled version of each other, so again it doesn't change anything.",
            "The last differences are that you've got Alpha and some beliefs.",
            "Alpha corresponds to the forward terms in EM, and those bees are beliefs or or distributions overstates.",
            "It turns out that the forward terms any M can be interpreted as the occupancy frequencies of the controller, and if you re normalize these things, to sum up to one, they correspond to beliefs.",
            "So it looks like, again, there's no difference, but it turns out that here for global optimality those equations have to hold for every belief, and actually we know that if we have an optimal policy then we should be executing the optimal thing in every single belief right where PM says is that you're going to be picking the best action only in the beliefs that are proportional to the occupancy frequencies of the nodes of the controller.",
            "So it's really just answering.",
            "Optimality without respect for a subset of the movie or another way to interpret this, is that because it's only with respect to the nose of the controllers that we're really just looking at one step ahead optimality."
        ],
        [
            "So if you got a second theorem, that essentially confirms that the condition that I had before, or indeed the ones that are necessary but not sufficient for global optimality an as I explained, we can interpret that as just saying that PM ensures once that look ahead of time.",
            "Alright, so based on that now we can look at escaped techniques.",
            "The first one will simply generalize this one step to multiple step.",
            "And the second one."
        ],
        [
            "Is on sale so for multi step when we do is we look at every node in our control room.",
            "So let's take this one for example.",
            "And what we can do is just look ahead search meaning that I simulate all possible trajectories of actions and observations and check for each belief is.",
            "Or am I picking the optimal action or not?",
            "If I realize that there's a belief where I'm suboptimal, then all I have to do is add some notes to the controller.",
            "And corresponds to the path where I found something suboptimal.",
            "And you see, normally I am only ensures one step lookahead optimality from each belief that's proportional to the occupancy frequencies of each node.",
            "But now we're going to do this search science for each node, but for multiple steps right?",
            "And so in the limit we can guarantee you know global optimality if we go far enough in the search."
        ],
        [
            "OK, the second approach is actually inspired by some work with respect to hidden Markov models, where the idea is that we could simply take each node one at a time and see if we could split it and perhaps re optimize the counters of the new nodes in such a way that perhaps we're going to improve the controller.",
            "And you can actually interpret this as looking for.",
            "I guess the optimal two step.",
            "Look ahead that would be possible from the beliefs that are normally encountered in that node.",
            "So it's.",
            "So different approach, but it has its advantages in the sense that it will generally be optimal to expect the two step look ahead.",
            "And it's also fairly simple, because we're simply just reusing them as well."
        ],
        [
            "OK, so let's see how they compare first in terms of complexity.",
            "So M by itself the complexity with respect to the sense of the controller with quadratic.",
            "Now if we include node splitting to get out of local Optima, the complexity becomes quiet.",
            "If an intuition here is that for every known, I'm going to try to do a split and then the split means that I'm going to run them so and I'm gonna have to do this as as I grow the controller so there's an additional quadratic term that creeps in as a result of that.",
            "So what it means that no spinning is going to have a hard time to scale."
        ],
        [
            "With respect."
        ],
        [
            "With respect to the number of nodes in the controller.",
            "Now for forward search we have better complexity with respect to the side of the controller, but then we have to do a search and that's exponential with respected in depth.",
            "So that could be bad.",
            "On the other hand, I mean a lot of people have been looking recently at doing forward search for online techniques and there are ways of I guess need to be mitigating this complexity by doing a branch and bound or sampling etc.",
            "So here I'm just showing all the worst case complexity."
        ],
        [
            "So nine practice we tested this on 6 problems.",
            "I'm going to show you some graphs where two of them at a table.",
            "I'm in the first problem here.",
            "Cheese taxi.",
            "This is a benchmark where the optimal policy actually has a sequence of action that has to be executed exactly, and if you give me just a little bit from it, then you're going to have you're going to miss the goal and have very little reward.",
            "As a result, most of the techniques that are doing some kind of gradient descent or policy search are subject to local Optima or just going to fail because they have to go through a Canyon before they reach that.",
            "Precise sequence of actions and the never get to find the optimal pause.",
            "So here what we see is that four search eventually manages to escape, whereas no splitting an random restart is just stuck no matter what.",
            "In this graph I have two versions of Forward Search.",
            "One is 4 search from each one of the nodes an otherwise the auto type.",
            "Of course we could do that is the obvious one with me to just to afford college from the initial belief as opposed to each one of those.",
            "So the tradeoff is that you could do one big forward search from the initial belief or several small for search from each belief from each node, and in general it will be better to just with from.",
            "Each note and this is where this graph shows here."
        ],
        [
            "OK, the second problem that I'm going to show you doesn't have a long sequence of actions that we should not deviate from, but I'm getting ahead.",
            "It has a lot of small, local optimal and you can see here that actually it's an alternate technique that works best and then we've got the two variants of four search an random restarts."
        ],
        [
            "OK. We tested this on 6 problems and we wanted to compare as well with other approaches that work on with controllers.",
            "So we included here.",
            "Source of bias about Idpol situation.",
            "Press escape.",
            "SLP is quadratically constrained linear programming.",
            "PBS LS is stochastic linear, stochastic local search.",
            "We've got our four search technique and old splitting.",
            "The highlight is that forward search OK was best in two problems and came close for the other one.",
            "And you can see how good would be the alcohol policy by looking at an upper bound that was computed based on.",
            "I think it was HSV I2 as well as the policy found by source off.",
            "If we just let it run for 100,000 seconds.",
            "OK, so it means that the alcohol policy is somewhere in between those values here.",
            "So when they match then we know what is the optimal value.",
            "I'm OK in parantheses.",
            "We've got the size of the controller and all the techniques that are in light blue here are generally meant to work on small controller, so they'll be fairly good in that sense, so our sakes I should point based approach so it's not meant per say to look for the smallest representation, But I think it's included in since it's one of the leading techniques, and here we can see that it works well as long as you just let it happen.",
            "A lot of Alpha factors that are essentially the same as the number of nodes in a controller, but if we restrict that too small number of off of vectors, just like all the other techniques, then it doesn't work as well.",
            "No spinning this best on tree problems, but it actually failed miserably on two of the problems that actually had some local Optima that required more than two step look ahead."
        ],
        [
            "OK, so to conclude what I talked about today is how we can allies on the local Optima when we're using.",
            "Yeah for planning as inference and this analysis I tried to convey to you know what it means and how to interpret those local Optima and with that.",
            "Then we also talked about two techniques to escape those local Optima and they each have their pros and cons.",
            "And our future work.",
            "Now that we can deal with local Optima, the next thing to do is to scale this.",
            "So in fact we had.",
            "I guess some issues, sometimes with Thunder reviews where they would prefer us to scale things instead of.",
            "Deal with local optimal, but I feel in general is that there's no point scaling something that produces garbage.",
            "It's better to just I get good results 1st and then after that scale things.",
            "So that's our next step.",
            "And there's also a lot of interest in using a controller based approaches in Essentialized hungry peaks and they are they have another type of local Optima issues with this analysis would not apply directly, but it would be interesting to extend it.",
            "That's it, thank you.",
            "OK questions for skull.",
            "So.",
            "The.",
            "Instead of waiting to do something.",
            "So afterwards.",
            "Something sure.",
            "So yes, we we could do this.",
            "So in fact, as I mentioned, any interest algorithm could be used when I sing it I guess is that EM is a deterministic algorithm, so it's easier to analyze.",
            "Give something being sarcastic with me a lot harder climbs, but yeah, so other people have use particle field training gives something to do the inference, specially in the case of continuous problems.",
            "Escape the problem.",
            "Unfortunately, I don't see how we could like.",
            "I think for each algorithm you have to remove the analysis because the conditions under which it gets stock may be different.",
            "More questions.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to talk about analyzing an escaping local optimum planning as in French for partial observable domains.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is also joint work with copious flag and map to stand for the Free University of Berlin in Germany.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in this talk when I consider one particular approach and complete planning that has become popular in recent years, which is pending as inference and we're going to focus on partial observable domain.",
                    "label": 0
                },
                {
                    "sent": "So that means we're going to look at palm DP.",
                    "label": 0
                },
                {
                    "sent": "So partial observable Markov decision processes.",
                    "label": 0
                },
                {
                    "sent": "Now when we do planning as inference, gay is that we're going to convert the palm DP into and make sure that any invasion networks an as a result, planning to now be framed as simply.",
                    "label": 0
                },
                {
                    "sent": "Our problem of maximizing likelihood.",
                    "label": 0
                },
                {
                    "sent": "I'll explain that in details, but the key is that by doing this then it opens the door to using many entrance algorithms.",
                    "label": 0
                },
                {
                    "sent": "Do planning, so this is all nice, except that the original problem, the original planning problem was non convex and doing this transformation actually doesn't change its nature, so it remains a nonconvex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So even though we have not many in France, algorithms that can solve this, we will all suffer from local Optima as well OK.",
                    "label": 0
                },
                {
                    "sent": "I mean, machine learning this is also an issue, but in general and panning is a more severe issue, so that's why we're looking at that.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this talk there will be 2 contributions, so the first one will be an analysis of EMS local optimize.",
                    "label": 1
                },
                {
                    "sent": "What I mean by an analysis is I'm going to interpret what it means to be stuck into local optimal in terms of time, so we'll give him meaning to this, and then there will be 2 escaped.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's the outline.",
                    "label": 0
                },
                {
                    "sent": "1st I'll just review quickly, whereupon the peas and playing is inference.",
                    "label": 0
                },
                {
                    "sent": "Then we'll go into the meat of the subjects, will pull up.",
                    "label": 0
                },
                {
                    "sent": "To my interpretation.",
                    "label": 0
                },
                {
                    "sent": "The main thing here.",
                    "label": 0
                },
                {
                    "sent": "So if you don't understand the talk or if you don't remember watch, then the main thing to remember is that when you get stuck into a local optimum will show is that if we just correspond to doing it one step, look ahead.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what young guys.",
                    "label": 0
                },
                {
                    "sent": "And then I'll explain how to escape from local Optima using two techniques or for research and not splitting, and then I'll show you some experiment.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what is the palm DP?",
                    "label": 0
                },
                {
                    "sent": "So it's a regular Markov decision process where we don't get to observe this state, so we've got some observations here that are correlated with the States and I'll do this as part of the reinforcement learning session.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "We're really looking at a planning problem, so we're assuming that we know what are the transitions as well as the reward function and the observation function, so we know everything we don't have to get some samples.",
                    "label": 0
                },
                {
                    "sent": "OK, so we just need to optimize the calls.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so very quickly high is also optimization done.",
                    "label": 0
                },
                {
                    "sent": "Traditionally you would be fine, believes that our distributions over states because we don't get to observe them.",
                    "label": 0
                },
                {
                    "sent": "You could have made those beliefs by basophil trying.",
                    "label": 0
                },
                {
                    "sent": "So is it just based their own and then a policy with him mapping from beliefs to actions and you could evaluate a policy simply by taking a discounted some of the expected reward and the best policy is obviously the one that will give you.",
                    "label": 0
                },
                {
                    "sent": "The highest expected sum of rewards and a certificate to verify optimality is Bellman's equation, which is right here.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That being said, let's look now at one class of policies.",
                    "label": 0
                },
                {
                    "sent": "I find some controllers, so find that controllers are very interesting because of their simplicity, and then they can be downloaded in very very small devices.",
                    "label": 0
                },
                {
                    "sent": "So here the idea is that we've got a bunch of nodes and for each node there is an action.",
                    "label": 0
                },
                {
                    "sent": "So if you wanna know you executive action then you receive an observation that will tell you which edge to follow, so you get 01.",
                    "label": 0
                },
                {
                    "sent": "You go to this node, execute a one again and keep on going.",
                    "label": 0
                },
                {
                    "sent": "So it's really just a final state on that we executed.",
                    "label": 0
                },
                {
                    "sent": "So then the problem possible optimization in this context is really a problem.",
                    "label": 0
                },
                {
                    "sent": "Simply defining what would be the best action mapping and next node mapping and general, we're going to let those things be probabilistic.",
                    "label": 0
                },
                {
                    "sent": "So we're going to have conditional distributions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in 1999 Nickelodeon and some coauthors actually realize that we could fold this controller type of policy into the graphical model of funding.",
                    "label": 0
                },
                {
                    "sent": "So here I've got again the graphical model that I showed you a few slides ago.",
                    "label": 0
                },
                {
                    "sent": "I can augment it now with one more set of variables on the labeled MIND correspond to the node of the controller that the agent will be in at any point in time.",
                    "label": 0
                },
                {
                    "sent": "And now the policy is re encoded by those red marks simply because the choice of the action is conditioned on the current node and the next node that you would end up in this condition on their current note as well as the observation received.",
                    "label": 0
                },
                {
                    "sent": "OK, so now you can see where we're going.",
                    "label": 0
                },
                {
                    "sent": "So in terms of I guess we planning as inference ideas that were slowly starting to convert from deep into it a regular dynamic Bayesian network where the policy is is essentially some initial distributions.",
                    "label": 0
                },
                {
                    "sent": "As part of Asian Network.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Then in 2006 Mac to say propose that since we've got this view, perhaps you could simply ask how we close time.",
                    "label": 0
                },
                {
                    "sent": "What is the best conditional distribution in this graphical model that with maximized our expected reward?",
                    "label": 0
                },
                {
                    "sent": "But let's try to do this in a way that would really have a dynamic Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "'cause of problems that we still have some rewards here that are utilities are not random variables and we've got some discount.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Factor so one ideas that we could simply normalize the rewards so the rewards normally are any real number, and if we simply re normalize by subtracting the minimum reward and dividing by the range, then everything will be between zero and one and that can be interpreted as a probability.",
                    "label": 0
                },
                {
                    "sent": "And now we can make the rewards just be some binary variables with probability disproportional to the original reward function.",
                    "label": 0
                },
                {
                    "sent": "So what's left now is how to deal with the discount factor, and so this is where.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we could do is simply introduce a mixture of an invasion networks simply because rewards need to be added so mixed chairs or mixture models or traditionally falls that are decompose additively, so this makes sense here, and each dynamic vision network is going to have a probability that's proportional to the discount factor race to the number of time steps that's included in that dynamic Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "So now I've got a conversion where everything is essentially proportional.",
                    "label": 0
                },
                {
                    "sent": "And now the value of the policy is nothing more than simply the probability that our bar would be equal to 1.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so given that now, finding an optimal policy can be phrased as an inference problem, where we would simply find what is what are the initial distributions here that would maximize the probability that power bar is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Now how do we do this?",
                    "label": 0
                },
                {
                    "sent": "There's lots of algorithms as I mentioned before, but let's start with the simplest one expectation maximization.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem with any of the algorithms including yeah, is that they're going to get stuck into some local Optima, and So what I'm going to do now is go through analysis an essentially showing you that what PM ensures is just one step lookahead optimality.",
                    "label": 1
                },
                {
                    "sent": "And based on that, then the natural thing to do if you want to escape is to look at multiple step to look ahead.",
                    "label": 0
                },
                {
                    "sent": "An another approach would be to split some goals.",
                    "label": 0
                },
                {
                    "sent": "So I find these things in more details.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's go into the details of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it is going to be a few equations, but I'll get back to some high level ideas in a minute.",
                    "label": 0
                },
                {
                    "sent": "If you're not comfortable with recent.",
                    "label": 0
                },
                {
                    "sent": "OK, so YM&R context here is used to update the powders.",
                    "label": 0
                },
                {
                    "sent": "And here I've got the distribution over actions for Michelle and the distribution over next nodes.",
                    "label": 0
                },
                {
                    "sent": "Given the current hold, an observation, these parameters are only updated in a way that we multiply there.",
                    "label": 0
                },
                {
                    "sent": "Setting by some expression.",
                    "label": 0
                },
                {
                    "sent": "So we doing multiplicative update.",
                    "label": 0
                },
                {
                    "sent": "That's what PM does.",
                    "label": 0
                },
                {
                    "sent": "And if we're stuck into a local optimum, let it means that the multiplicative update here won't change what those values are.",
                    "label": 0
                },
                {
                    "sent": "OK so far.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, we can.",
                    "label": 0
                },
                {
                    "sent": "Formally the first theorem then essentially says that if we've got a policy that's a stable fixed point of PM, then those two conditions have to hold and essentially specifying webtools expr.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actions that I had on the previous slide.",
                    "label": 0
                },
                {
                    "sent": "So G here engage here when it should be.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be like an essentially they must be equal to the largest possible value that they can take for any action here and for any next node here.",
                    "label": 0
                },
                {
                    "sent": "OK, so that in itself is not too interesting.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Xbox Now, let's see how we can use that to really understand what it means to be stuck into local Optima.",
                    "label": 0
                },
                {
                    "sent": "So I've got the equations for YM right here and also global optimality as specified by the man's equation.",
                    "label": 1
                },
                {
                    "sent": "So most here are nothing more than balance equation that's been split in two in the context of controllers.",
                    "label": 0
                },
                {
                    "sent": "So when this holds, I know that I've got an optimal pulse an when this holds here.",
                    "label": 0
                },
                {
                    "sent": "I know that I'm stuck.",
                    "label": 0
                },
                {
                    "sent": "If I'm using him.",
                    "label": 0
                },
                {
                    "sent": "Now these equations are actually very similar, but we've got some differences.",
                    "label": 0
                },
                {
                    "sent": "So in green I've circled samples differences and you'll notice that here I've got some terms that are bounded functions and hear some terms that are bad.",
                    "label": 0
                },
                {
                    "sent": "So what are those bad as if I?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go back to my slide here for the details of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, beta is nothing more than the backward term that you would compute in India.",
                    "label": 0
                },
                {
                    "sent": "Now it wasn't clear to us what it meant, but from those in.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Visions actually we can show that when better really is just as rescaled version of the value function, so you can actually interpret the backward terms in EM as just the value function.",
                    "label": 0
                },
                {
                    "sent": "So now because it's a rescaled version, it doesn't change anything, because here we've got a Max and here in our maximum when you maximized, if you just re scale things by a constant factor, it doesn't change anything.",
                    "label": 0
                },
                {
                    "sent": "OK, the next differences that here I'm going to copy that are very close one and here with the reward function as we explained before, these things were just rescaled version of each other, so again it doesn't change anything.",
                    "label": 0
                },
                {
                    "sent": "The last differences are that you've got Alpha and some beliefs.",
                    "label": 0
                },
                {
                    "sent": "Alpha corresponds to the forward terms in EM, and those bees are beliefs or or distributions overstates.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the forward terms any M can be interpreted as the occupancy frequencies of the controller, and if you re normalize these things, to sum up to one, they correspond to beliefs.",
                    "label": 0
                },
                {
                    "sent": "So it looks like, again, there's no difference, but it turns out that here for global optimality those equations have to hold for every belief, and actually we know that if we have an optimal policy then we should be executing the optimal thing in every single belief right where PM says is that you're going to be picking the best action only in the beliefs that are proportional to the occupancy frequencies of the nodes of the controller.",
                    "label": 0
                },
                {
                    "sent": "So it's really just answering.",
                    "label": 0
                },
                {
                    "sent": "Optimality without respect for a subset of the movie or another way to interpret this, is that because it's only with respect to the nose of the controllers that we're really just looking at one step ahead optimality.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you got a second theorem, that essentially confirms that the condition that I had before, or indeed the ones that are necessary but not sufficient for global optimality an as I explained, we can interpret that as just saying that PM ensures once that look ahead of time.",
                    "label": 1
                },
                {
                    "sent": "Alright, so based on that now we can look at escaped techniques.",
                    "label": 0
                },
                {
                    "sent": "The first one will simply generalize this one step to multiple step.",
                    "label": 0
                },
                {
                    "sent": "And the second one.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is on sale so for multi step when we do is we look at every node in our control room.",
                    "label": 0
                },
                {
                    "sent": "So let's take this one for example.",
                    "label": 0
                },
                {
                    "sent": "And what we can do is just look ahead search meaning that I simulate all possible trajectories of actions and observations and check for each belief is.",
                    "label": 0
                },
                {
                    "sent": "Or am I picking the optimal action or not?",
                    "label": 0
                },
                {
                    "sent": "If I realize that there's a belief where I'm suboptimal, then all I have to do is add some notes to the controller.",
                    "label": 0
                },
                {
                    "sent": "And corresponds to the path where I found something suboptimal.",
                    "label": 0
                },
                {
                    "sent": "And you see, normally I am only ensures one step lookahead optimality from each belief that's proportional to the occupancy frequencies of each node.",
                    "label": 0
                },
                {
                    "sent": "But now we're going to do this search science for each node, but for multiple steps right?",
                    "label": 0
                },
                {
                    "sent": "And so in the limit we can guarantee you know global optimality if we go far enough in the search.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the second approach is actually inspired by some work with respect to hidden Markov models, where the idea is that we could simply take each node one at a time and see if we could split it and perhaps re optimize the counters of the new nodes in such a way that perhaps we're going to improve the controller.",
                    "label": 0
                },
                {
                    "sent": "And you can actually interpret this as looking for.",
                    "label": 0
                },
                {
                    "sent": "I guess the optimal two step.",
                    "label": 0
                },
                {
                    "sent": "Look ahead that would be possible from the beliefs that are normally encountered in that node.",
                    "label": 0
                },
                {
                    "sent": "So it's.",
                    "label": 0
                },
                {
                    "sent": "So different approach, but it has its advantages in the sense that it will generally be optimal to expect the two step look ahead.",
                    "label": 0
                },
                {
                    "sent": "And it's also fairly simple, because we're simply just reusing them as well.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's see how they compare first in terms of complexity.",
                    "label": 0
                },
                {
                    "sent": "So M by itself the complexity with respect to the sense of the controller with quadratic.",
                    "label": 0
                },
                {
                    "sent": "Now if we include node splitting to get out of local Optima, the complexity becomes quiet.",
                    "label": 1
                },
                {
                    "sent": "If an intuition here is that for every known, I'm going to try to do a split and then the split means that I'm going to run them so and I'm gonna have to do this as as I grow the controller so there's an additional quadratic term that creeps in as a result of that.",
                    "label": 0
                },
                {
                    "sent": "So what it means that no spinning is going to have a hard time to scale.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With respect.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With respect to the number of nodes in the controller.",
                    "label": 0
                },
                {
                    "sent": "Now for forward search we have better complexity with respect to the side of the controller, but then we have to do a search and that's exponential with respected in depth.",
                    "label": 1
                },
                {
                    "sent": "So that could be bad.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, I mean a lot of people have been looking recently at doing forward search for online techniques and there are ways of I guess need to be mitigating this complexity by doing a branch and bound or sampling etc.",
                    "label": 0
                },
                {
                    "sent": "So here I'm just showing all the worst case complexity.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So nine practice we tested this on 6 problems.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you some graphs where two of them at a table.",
                    "label": 0
                },
                {
                    "sent": "I'm in the first problem here.",
                    "label": 0
                },
                {
                    "sent": "Cheese taxi.",
                    "label": 0
                },
                {
                    "sent": "This is a benchmark where the optimal policy actually has a sequence of action that has to be executed exactly, and if you give me just a little bit from it, then you're going to have you're going to miss the goal and have very little reward.",
                    "label": 0
                },
                {
                    "sent": "As a result, most of the techniques that are doing some kind of gradient descent or policy search are subject to local Optima or just going to fail because they have to go through a Canyon before they reach that.",
                    "label": 0
                },
                {
                    "sent": "Precise sequence of actions and the never get to find the optimal pause.",
                    "label": 0
                },
                {
                    "sent": "So here what we see is that four search eventually manages to escape, whereas no splitting an random restart is just stuck no matter what.",
                    "label": 0
                },
                {
                    "sent": "In this graph I have two versions of Forward Search.",
                    "label": 0
                },
                {
                    "sent": "One is 4 search from each one of the nodes an otherwise the auto type.",
                    "label": 0
                },
                {
                    "sent": "Of course we could do that is the obvious one with me to just to afford college from the initial belief as opposed to each one of those.",
                    "label": 0
                },
                {
                    "sent": "So the tradeoff is that you could do one big forward search from the initial belief or several small for search from each belief from each node, and in general it will be better to just with from.",
                    "label": 0
                },
                {
                    "sent": "Each note and this is where this graph shows here.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the second problem that I'm going to show you doesn't have a long sequence of actions that we should not deviate from, but I'm getting ahead.",
                    "label": 0
                },
                {
                    "sent": "It has a lot of small, local optimal and you can see here that actually it's an alternate technique that works best and then we've got the two variants of four search an random restarts.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. We tested this on 6 problems and we wanted to compare as well with other approaches that work on with controllers.",
                    "label": 0
                },
                {
                    "sent": "So we included here.",
                    "label": 0
                },
                {
                    "sent": "Source of bias about Idpol situation.",
                    "label": 0
                },
                {
                    "sent": "Press escape.",
                    "label": 0
                },
                {
                    "sent": "SLP is quadratically constrained linear programming.",
                    "label": 0
                },
                {
                    "sent": "PBS LS is stochastic linear, stochastic local search.",
                    "label": 0
                },
                {
                    "sent": "We've got our four search technique and old splitting.",
                    "label": 0
                },
                {
                    "sent": "The highlight is that forward search OK was best in two problems and came close for the other one.",
                    "label": 0
                },
                {
                    "sent": "And you can see how good would be the alcohol policy by looking at an upper bound that was computed based on.",
                    "label": 0
                },
                {
                    "sent": "I think it was HSV I2 as well as the policy found by source off.",
                    "label": 0
                },
                {
                    "sent": "If we just let it run for 100,000 seconds.",
                    "label": 0
                },
                {
                    "sent": "OK, so it means that the alcohol policy is somewhere in between those values here.",
                    "label": 0
                },
                {
                    "sent": "So when they match then we know what is the optimal value.",
                    "label": 0
                },
                {
                    "sent": "I'm OK in parantheses.",
                    "label": 0
                },
                {
                    "sent": "We've got the size of the controller and all the techniques that are in light blue here are generally meant to work on small controller, so they'll be fairly good in that sense, so our sakes I should point based approach so it's not meant per say to look for the smallest representation, But I think it's included in since it's one of the leading techniques, and here we can see that it works well as long as you just let it happen.",
                    "label": 0
                },
                {
                    "sent": "A lot of Alpha factors that are essentially the same as the number of nodes in a controller, but if we restrict that too small number of off of vectors, just like all the other techniques, then it doesn't work as well.",
                    "label": 0
                },
                {
                    "sent": "No spinning this best on tree problems, but it actually failed miserably on two of the problems that actually had some local Optima that required more than two step look ahead.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so to conclude what I talked about today is how we can allies on the local Optima when we're using.",
                    "label": 0
                },
                {
                    "sent": "Yeah for planning as inference and this analysis I tried to convey to you know what it means and how to interpret those local Optima and with that.",
                    "label": 0
                },
                {
                    "sent": "Then we also talked about two techniques to escape those local Optima and they each have their pros and cons.",
                    "label": 0
                },
                {
                    "sent": "And our future work.",
                    "label": 0
                },
                {
                    "sent": "Now that we can deal with local Optima, the next thing to do is to scale this.",
                    "label": 1
                },
                {
                    "sent": "So in fact we had.",
                    "label": 0
                },
                {
                    "sent": "I guess some issues, sometimes with Thunder reviews where they would prefer us to scale things instead of.",
                    "label": 0
                },
                {
                    "sent": "Deal with local optimal, but I feel in general is that there's no point scaling something that produces garbage.",
                    "label": 0
                },
                {
                    "sent": "It's better to just I get good results 1st and then after that scale things.",
                    "label": 0
                },
                {
                    "sent": "So that's our next step.",
                    "label": 0
                },
                {
                    "sent": "And there's also a lot of interest in using a controller based approaches in Essentialized hungry peaks and they are they have another type of local Optima issues with this analysis would not apply directly, but it would be interesting to extend it.",
                    "label": 0
                },
                {
                    "sent": "That's it, thank you.",
                    "label": 0
                },
                {
                    "sent": "OK questions for skull.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Instead of waiting to do something.",
                    "label": 0
                },
                {
                    "sent": "So afterwards.",
                    "label": 0
                },
                {
                    "sent": "Something sure.",
                    "label": 0
                },
                {
                    "sent": "So yes, we we could do this.",
                    "label": 0
                },
                {
                    "sent": "So in fact, as I mentioned, any interest algorithm could be used when I sing it I guess is that EM is a deterministic algorithm, so it's easier to analyze.",
                    "label": 0
                },
                {
                    "sent": "Give something being sarcastic with me a lot harder climbs, but yeah, so other people have use particle field training gives something to do the inference, specially in the case of continuous problems.",
                    "label": 0
                },
                {
                    "sent": "Escape the problem.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, I don't see how we could like.",
                    "label": 0
                },
                {
                    "sent": "I think for each algorithm you have to remove the analysis because the conditions under which it gets stock may be different.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}